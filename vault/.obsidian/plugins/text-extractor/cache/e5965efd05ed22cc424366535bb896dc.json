{"path":"sem1/LinAlg/PV/cheatsheets/LinAlg-cheatsheet-gassmann.pdf","text":"1 Lizenziert unter CC BY-SA 4.0. Für Urheber, Quellen und Lizenzinformationen, siehe: github.com/thomasgassmann/eth-cheatsheets Dieses Cheatsheet ist inhaltlich inspiriert von Danny Camen- isch’s Cheatsheet. Stylistisch basiert das Cheatsheet auf dem Ana- lysis I Cheatsheet von xyquadrat. 1 Vorwissen 1.1 Komplexe Zahlen Definition Ein Ausdruck der Form z = a + ib, wobei i2 = \u00001. a = Re(z) ist der Realteil, b = Im(z) ist der Imaginärteil. Addition erfolgt komponentenweise, Multiplikation erfolgt unter Annahme des Binomialgesetzes und i2 = \u00001 (i.e. zw = (ac \u0000 bd) + i(ad + bc)). Für Division gilt z w = c+id a+ib = zw ww = (ca+bd)+i(ad−cb) a2+b2 . Die Norm ist definiert als jzj = pRe(z)2 + Im(z)2 = pz \u0001 z. Für z = x + iy ist z = x \u0000 iy konjugiert-komplex. zz = Re(z)2 + Im(z)2 Eine komplexe Zahl kann in Polarkoordinaten dargestellt wer- den. Es gilt z = reiϕ = r(cos(ϕ) + i sin(ϕ)). Radizieren: npa = z () a = zn () jajeiα = rneiϕn wobei r = npjaj und ϕ = α+2kπ n . Fundamentalsatz der Algebra Sei p(z) = anzn + \u0001 \u0001 \u0001 + a0 ein Polynom mit an 6= 0 und reellen oder komplexen Koeﬀizienten ai 2 C. Dann hat p(z) genau n Nullstellen (mit ihren Vielfachen gezählt). Es gilt z \u0006 w = z \u0006 w, zw = zw, ( z w ) = z w , jzj = jzj, jz + wj \u0014 jzj + jwj, jzwj = jzjjwj. θ = 8 >>>< >>>: arctan( y x ) if z on positive x-axis π 2 if x = 0, y > 0 π + arctan( y x ) if z on negative x-axis 3π 2 if x = 0, y < 0 1.2 Polynome Bei Polynomen mit reellen Nullstellen treten die Nullstellen als komplex-konjugiertes Paar auf. Für Grad 2, verwende z = −b±pb2−4ac 2a um ein Polynom p(z) = 0 zu lösen. Für azn + c = 0, verwende z = n q\u0000 c a . Bei einem Polynom über C mit ungeradem Grad gibt es min- destens eine reelle Nullstelle. 2 LGS / Gauss-Elimination Ein homogenes LGS hat die Form Ax = 0. 1. Wir können Zeilen vertauschen, eine Zeile mit a 2 R n f0g multiplizieren und Zeilen voneinander subtrahieren bzw. ad- dieren. 2. Wir schreiben das lineare Gleichungssystem (LGS) in Ma- trixform. 3. Wir transformieren das LGS in die Zeilenstufenform. 4. Wir lösen das LGS von unten nach oben. Verträglichkeitsbedingungen (VTB): Falls die Verträglich- keitsbedingungen cr+1 = \u0001 \u0001 \u0001 = cm = 0 nicht erfüllt sind, so gibt es keine Lösung. Bei homogenen LGS sind die Verträglichkeitsbe- dingungen immer erfüllt. Nur wenn r < n gibt es also nicht-triviale Lösungen für das homogene LGS. Lösungen Ax = b hat mindestens eine Lösung gdw. (r = m) oder (r < m und VTB sind erfüllt). In diesem Fall gibt es 1 Lösung falls r = n, andernfalls eine n \u0000 r Schar an Lösungen. • r = m: (r = m = n: eindeutige Lösung, regulär r < n: 1 Lösungen mit n \u0000 r freien Variablen • r < m: (r = n: eindeutige Lösung r < n: 1 Lösungen mit n \u0000 r freien Variablen Es gilt immer r \u0014 min(m, n). 3 Matrizen & Vektoren Eine m \u0002 n Matrix hat m Zeilen und n Spalten wobei das i, j Ele- ment mit ai,j oder (A)i,j bezeichnet wird. Ein m \u0002 1 Vektor ist ein Spaltenvektor und ein 1 \u0002 n Vektor ist ein Zeilenvektor (n-Tupel). Die Elemente ajj , j = 1, 2, \u0001 \u0001 \u0001 , min(m, n) heissen Dia- gonalelemente. Für eine Diagonalmatrix A gilt (A)ij = 0 für i 6= j. Wir bezeichnen die Matrix dann durch die Elemente auf der Diagonale: A = diag(d11, \u0001 \u0001 \u0001 , dnn). Für eine obere Dreiecksma- trix gilt: (R)ij = 0 für i > j. Für eine untere Dreiecksmatrix gilt: (L)ij = 0 für i < j. Wir definieren als Tr(A) die Summe der Diagonalelemente. Multiplikation Für eine m \u0002 n Matrix A und eine n \u0002 p Matrix B gilt (AB)ij = Pn k=1 aikbkj wobei AB eine m \u0002 p Matrix ist. Generell nicht kommutativ. Falls zwei Matrizen A, B 2 En×n kommutieren (AB = BA), dann gilt (AB)k = AkBk. Rechenregeln für Matrizen i: (αβ)A = α(βA) · ii: (αA)B = α(AB) = A(αB) · iii: (α + β)A = αA + βA · iv: α(A + B) = αA + αB · v: A + B = B + A · vi: A + (B + C) = (A + B) + C · vii: (AB)C = A(BC) · viii: (A + B)C = AC + BC · ix: A(B + C) = AB + AC Eine Linearkombination der Vektoren a1, \u0001 \u0001 \u0001 , an ist α1a1 + \u0001 \u0001 \u0001 + αnan. Falls AB = 0, so sind A und B Nullteiler. Transponierte Matrizen Transponiert: A⊤ wird definiert als (A⊤)ij = (A)ji. Hermitesch-transponiert: AH = (A)⊤ = A⊤. Eine Matrix ist symmetrisch falls A⊤ = A und hermitesch falls AH = A (Diagonale somit reell). Schiefsymmetrisch ist sie falls A⊤ = \u0000A. Sind A und B hermitesche Matrizen, so ist AB genau dann hermitesch falls AB = BA. AHA und AAH sind für alle Matrizen A hermitesch. Rechenregeln für hermitesche Matrizen i: (A⊤)⊤ = A · ii: (AH)H = A · iii: (αA)⊤ = αA⊤ · iv: (αA)H = αAH · v: (A + B)⊤ = A⊤ + B⊤ · vi: (A + B)H = AH + BH · vii: (AB)⊤ = B⊤A⊤ · viii: (AB)H = BHAH Spalten- und Reihensichtweise Ist A = \u0012 a1 ··· am \u0013 eine m \u0002 n Matrix, B = ( b1 b2 ··· bp ) eine n \u0002 p Matrix, dann gilt AB = ( Ab1 Ab2 ··· Abp ) = \u0012 a1B ··· amB \u0013. Ist umgekehrt A = ( a1 a2 ··· an ) eine m \u0002 n Matrix, B =\u0012 b1 ··· bn \u0013 eine n \u0002 p Matrix, dann gilt AB = Pn i=1 aibi. Multipliziert man eine Diagonalmatrix von links, dann skaliert dies die Zeilen. Multipliziert man eine Diagonalmatrix von rechts, dann multipliziert dies die Spalten. 2 Definite Eine Matrix A ist positiv-definit (positiv-semidefinit), falls 8x 2 En, xHAx > 0 (\u0015 0). Eine symmetrische Matrix A ist genau dann positiv-definit (positiv-semidefinit) falls alle Eigenwerte strikt positiv (positiv) sind. Um zu zeigen, dass eine Matrix A indefinit ist, zeigt man, dass es ein x mit x⊤Ax > 0 und ein y mit y⊤Ay < 0 gibt. Es gilt e⊤ i Aej = aij . Folglich ist A indefinit, falls es auf der Diagonalen von A verschiedene Vorzeichen gibt (vorrausgesetzt det(A) 6= 0). Marlene Trick: Sei M eine symmetrische Matrix und Ak die k \u0002 k Blockmatrix der oberen, linken Ecke. Mit ∆k bezeichnen wir die Determinante von Ak. Nun ist M pos. def. wenn ∆k > 0 für alle k, weiter ist M neg. def. wenn (\u00001)k∆k > 0 für alle k. A ist indefinit falls das erste ∆k das keiner der beiden Regeln entspricht ein falsches Vorzeichen hat. Das äussere Produkt für zwei Vektoren x und y ist definiert als xy⊤. Die orthogonale Projektion von x auf y ist Pyx = 1 ∥y∥2 yyHx. Entsprechend ist die Orthogonalprojektion PA := A(AHA)−1AH (mit A 2 Em×n und Rank A = n) die Projektion auf den Kolon- nenraum R(A). Für diese Orthogonalprojektion gilt ky \u0000 P yk2 = minz∈Im P ky \u0000 zk2 (siehe Least Squares). Eine Matrix P heisst Projektor falls P 2 = P . 3.1 Inverse Eine n\u0002n Matrix ist invertierbar wenn eine Matrix A−1 existiert, so dass AA−1 = In = A−1A. Die Inverse ist eindeutig bestimmt und existiert genau dann wenn Rank A = n. Rechenregeln für Inverse Sind A, B regulär. Dann: • A−1 regulär und (A−1)−1 = A. • AB regulär und (AB)−1 = B−1A−1. • AH regulär und (AH)−1 = (A−1)H. Die Inverse findet man durch Gauss-Jordan Elimination. \u0002AjIn\u0003 Zeilen- ========) operationen \u0002InjA−1\u0003 Für A = \u0000 a b c d \u0001 und det(A) 6= 0 (also A invertierbar) gilt A−1 = 1 ad−bc \u0010 d −b −c a \u0011. 3.2 Orthogonale und unitäre Matrizen Eine n \u0002 n Matrix heisst unitär falls AHA = In. Eine Matrix heisst orthogonal falls A⊤A = In. Die Matrix ist genau dann unitär/or- thogonal wenn alle Spalten orthonormal sind. Für A, B unitär (dasselbe für orthogonal) gilt: i: A regulär und A−1 = AH · ii: AAH = In = AHA · iii: A−1, AB unitär Für unitäre Matrizen gilt j det(A)j = 1, für orthogonale Matri- zen somit det(A) = \u00061. Ausserdem gilt jλj = 1 für alle Eigenwerte λ einer unitären Matrix. Entsprechend gilt λ = \u00061 für alle Eigen- werte einer orthogonalen Matrix. Eine Rotationsmatrix R ist eine unitäre Matrix. In 2D ist R =\u0000 cos θ − sin θ sin θ cos θ \u0001. Verallgemeinert wird dies durch Givens-Rotationen. In 4D gilt bspw. \u0012 cos θ 0 − sin θ 0 0 1 0 0 sin θ 0 cos θ 0 0 0 0 1 \u0013. Die Householder-Matrix Qu = I \u0000 2uu⊤ stellt eine Spiege- lung an der Hyperebene welche orthogonal zu u ist dar. 4 LR-Zerlegung Die LR-Zerlegung ist ein weiteres Verfahren zum lösen von LGS. Wir erhalten dadurch P A = LR. Es ist besonders effektiv wenn wir mehrere LGS mit gleichem A haben. 2 4 I z }| { 1 0 0 A z }| { 2 1 2 I z }| { 1 0 0 0 1 0 1 2 3 0 1 0 0 0 1 2 2 2 0 0 1 3 5 \u0000 1 2 \u00001 =) 2 41 0 0 2 1 2 1 0 0 0 1 0 0 3 2 2 1 2 1 0 0 0 1 0 1 0 1 0 1 3 5 \u0000 2 3 =) 2 41 0 0 2 1 2 1 0 0 0 1 0 0 3 2 2 1 2 1 0 | {z } P 0 0 1 | {z } R 0 0 \u0000 4 3 | {z } L 1 2 3 1 3 5 Wenn wir Zeilen in R vertauschen, dann vertauschen wir auch die selben Zeilen in P und L (die 1 Einträge auf der Diagonalen in L werden nicht vertauscht!). Im Allgemeinen gilt A 2 Em×n sowie P, L 2 Em×m und R 2 Em×n. Die Matrix L hat immer 1 auf der Diagonalen und Einträge unten links sind nur ungleich 0 für Spalten kleiner oder gleich Rank A. Die i-te Spalte in L entspricht also dem i-ten Pivot von A. Haben wir eine LR-Zerlegung von A können wir Ax = b eﬀi- zienter lösen. Zuerst lösen wir dazu P A = LR und lösen Lc = P b nach c. Dann lösen wir Rx = c nach x. 5 Vektorräume Eine Vektorraum V über K ist eine nichtleere Menge auf der eine Vektoraddition und Skalarmultiplikation definiert sind. V1: x + y = y + x · V2: (x + y) + z = x + (y + z) · V3: 90 2 V : x + 0 = x (8x 2 V ) · V4: 8x 2 V existiert \u0000x: x + (\u0000x) = 0 · V5: α(x + y) = αx + αy · V6: (α + β)x = αx + βx · V7: (αβ)x = α(βx) · V8: 91 2 K so dass 8x 2 V : 1x = x i: 0x = 0 · ii: α0 = 0 · iii: αx = 0 =) x = 0 _ α = 0 · iv: (\u0000α)x = α(\u0000x) = \u0000(αx) 5.1 Unterräume Ein Unterraum U ist eine nichtleere Teilmenge von V der abge- schlossen bzgl. Addition und Multiplikation ist. U beinhaltet immer den Nullvektor. Jeder Unterraum ist ein Vektorraum. Für zwei Un- terräume U, W \u0012 V ist U [ W genau dann ein Unterraum von V falls U \u0012 W oder W \u0012 U . U \\W ist hingegen immer ein Unterraum. Für eine lineare Abbildung F : X 7! Y und einen Unterraum U \u0012 X ist F U \u0012 Y auch ein Unterraum. Ist W \u0012 Im F ein Unter- raum so ist auch F −1W \u0012 X ein Unterraum. Lineare Hülle (span) Die Menge der Linearkombinationen der Vektoren v1, \u0001 \u0001 \u0001 , vn ist der Unterraum aufgespannt durch diese Vektoren. Man bezeichnet ihn mit spanfv1, \u0001 \u0001 \u0001 , vng. Falls spanfv1, \u0001 \u0001 \u0001 , vng = V , dann sind v1, \u0001 \u0001 \u0001 , vn ein Erzeu- gendensystem von V . Um zu zeigen dass U \u0012 V ein Unterraum von V ist zeigt man i: 0V 2 U · ii: 8x, y 2 U gilt x + y 2 U · iii: 8x 2 U, 8α 2 K gilt αx 2 U . Unterräume sind bspw. auch Ker(A) und Im(A). 5.2 Lineare Abhängigkeit, Basen und Dimensionen Lineare Unabhängigkeit Vektoren v1, \u0001 \u0001 \u0001 , vn sind linear unabhängig genau dann wenn: nX k=1 αkvk = 0 =) α1 = \u0001 \u0001 \u0001 = αn = 0 Andernfalls sind die Vektoren linear abhängig, d.h. es exis- tiert eine nicht-triviale Nullsumme bzw. ein Vektor lässt sich als Linearkombination der anderen schreiben. Basis Ein Erzeugendensystem v1, \u0001 \u0001 \u0001 , vn von V ist eine Basis von V genau dann wenn v1, \u0001 \u0001 \u0001 , vn linear unabhängig sind. Jeder Vektor in V lässt sich eindeutig als Linearkombination der Basisvektoren schreiben. Jede Menge fv1, \u0001 \u0001 \u0001 , vmg \u0012 V mit dim V < m ist linear ab- hängig. In jedem endlichen Vektorraum V mit dim V = n ist eine Menge von n linear unabhängigen Vektoren eine Basis. In einem Vektorraum V beeinflusst die Wahl der Skalare die Dimension. Cn über C hat Dimension n, Cn über R hat Dimension 2n. Die Koeﬀizienten ξ = (ξ1, \u0001 \u0001 \u0001 , ξn)⊤ sind Koordinaten von x bzgl. einer Basis B falls x = Pn i=1 ξibi gilt. Die Summe wird Koordinatendarstellung genannt. Zwei Unterräume U, W \u0012 V heissen komplementär falls jedes x 2 V eine eindeutige Darstellung x = u+w mit u 2 U und w 2 W 3 hat (V ist dann direkte Summe von U und W ). Man schreibt V = U \b W . Falls zwei Unterräume komplementär sind, folgt U \\ W = f0V g. Um zu zeigen dass U \b W = V gilt zeigt man dim V = dim U + dim W und U \\ W = f0V g. Falls f : U ! V und g : V ! W lineare Abbildungen zwischen Vektorräumen sind so dass g \u000e f ein Isomorphismus ist, dann gilt V = Im f \b Ker g. 5.3 Basiswechsel und Koordinatentransformation Wenn wir von einer alten Basis B zu einer neuen Basis B′ wechseln, können wir die neue Basis mit der alten darstellen. Es gilt dann b′ k = Pn i=1 τikbi mit der Basiswechselmatrix T = (τik). Es gilt ξ = T ξ′ sowie ξ′ = T −1ξ, da T regulär ist. Dabei sind ξ′ die Koordinaten in der Basis B′ und ξ sind die Koordinaten in der Basis B. Es gilt dann B′ = BT , was aus Bξ = x = B′ξ′ und ξ = T ξ′ folgt. Wir schreiben auch T = Mat(B′)B. Wir können T zwischen B und B′ auch durch Gauss-Elimination finden (denn T = B−1B′): \u0002BjB′\u0003 Zeilen- ========) operationen \u0002InjT \u0003 6 Lineare Abbildungen Eine Abbildung F : V 7! W ist linear, wenn F (v+w) = F (v)+F (w) sowie F (αv) = αF (v) für alle v, w 2 V und α 2 K gilt. Einfacher ausgedrückt ist F linear falls F (αv + w) = αF (v) + F (w). Für eine Funktion F : X 7! Y gilt: • injektiv: 8x, x′ 2 X f (x) = f (x′) =) x = x′ • surjektiv: 8y 2 Y gibt es x 2 X mit f (x) = y • bijektiv: injektiv und surjektiv, F −1 existiert Sei F : X 7! Y eine lineare Abbildung ist wobei spanfb1, \u0001 \u0001 \u0001 , bng = X und spanfc1, \u0001 \u0001 \u0001 , cmg = Y Basen sind. Dann lässt sich F (bl) = Pm k=1 aklck schreiben. Die Matrix Am×n mit Elementen akl heisst die Abbildungsmatrix bezüglich X, Y . Jede lineare Abbildung lässt sich also durch eine Matrix dar- stellen. Ist F bijektiv ist es ein Isomorphismus, ist zusätzlich X = Y so ist F ein Automorphismus. Wenn F ein Isomorphismus ist, existiert ein Isomorphismus F −1. 6.1 Kern, Bild und Rang Sei F : X 7! Y eine lineare Abbildung. Es gilt Rank F := dim Im F . Kern und Bild Wir definieren den Kern als Ker F := fx 2 XjF (x) = 0g. Der Kern ist ein Unterraum von X und F ist injektiv genau dann wenn Ker F = f0g Wir definieren das Bild als Im F := fF (x)jx 2 Xg. Das Bild ist ein Unterraum von Y und F ist surjektiv genau dann wenn Im F = Y . Rangsatz dim X \u0000 dim Ker F = dim Im F = Rank F Zwei Vektorräume endlicher Dimension sind genau dann iso- morph wenn sie die gleiche Dimension haben. Foldergungen des Rangsatzes Seien F : X 7! Y, G : Y 7! Z lineare Abbildungen: • F injektiv () Rank F = dim X • F surjektiv () Rank F = dim Y • F bijektiv () Rank F = dim X = dim Y • Rank G \u000e F \u0014 min(Rank F, Rank G) • G injektiv =) Rank GF = Rank F • F surjektiv =) Rank GF = Rank G 6.2 Matrizen als lineare Abbildungen Sei A 2 Em×n. Der Kolonnenraum oder Wertebereich von A ist der Unterraum R(A) = Im A = spanfa1, \u0001 \u0001 \u0001 , ang \u0012 Em. Der Nullraum von A ist der Unterraum N (A) = Ker A \u0012 En. Rangsatz für Matrizen Sei r := Rank A = dim Im A. Dann ist dim Ker A = n \u0000 r und r entspricht der Anzahl Pivotelemente in REF. Zusätzlich: Rank A⊤ = Rank AH = Rank A Somit gilt für Matrizen A 2 Em×n und B 2 Ep×m auch: • Rank BA \u0014 min(Rank B, Rank A) • Rank B = m \u0014 p =) Rank BA = Rank A • Rank A = m \u0014 n =) Rank BA = Rank B Für quadratische Matrizen A 2 En×n sind also folgende Aussa- gen äquivalent: i: A ist regulär · ii: Rank A = n · iii: Spalten (oder Zeilen) sind linear unabhängig · iv: Ker A = f0g · v: Im A = En Falls x0 eine Lösung für Ax = b ist, so ist die Lösungsmenge Lb = x0 + Ker A ein aﬀiner Unterraum. 6.3 Zusammenfassende Eigenschaften von A Sei A 2 M m×n mit r := Rank A: i: dim Im A = dim Im AH = r · ii: dim Ker A = n \u0000 r · iii: dim Ker AH = m \u0000 r r = n , Ker A = f0g r = m , Ker AH = f0g , Spalten von A l.u. , Spalten von A erzeugend , Zeilen von A erzeugend , Zeilen von A l.u. , A injektiv , A surjektiv Basis für Im A und Ker A finden Zuerst bringt man A auf Zeilenstufenform. • Basis für Im A: Die Spalten in der originalen Ma- trix welche den Pivotspalten in der Zeilenstufenform entsprechen sind Basisvektoren für Im A. • Basis für Ker A: Zuerst parameterisiert man die frei- en Variablen und findet dann die Lösungesmenge L0 von Ax = 0. Die Zeile i des Lösungsvektors enthält dann den Wert der i-ten Variable. Man wählt dann die Koeﬀizienten der freien Variablen entsprechend um Basisvektoren zu finden (e.g. man setzt einen der Koef- fizienten auf 1 und alle anderen auf 0 pro Basisvektor). 6.4 Abbildungen von Koordinatentransformation x 2 X y 2 Y ξ 2 En η 2 Em ξ′ 2 En η′ 2 Em F κX κY A κ−1 X T −1 κ−1 Y S−1T B S • A = SBT −1 • B = S−1AT • Rank F = Rank A = Rank B Ist F Rank F = r, so besitzt F bzgl. geeigneten Basen X, Y die Abbildungsmatrix: \u0014 Ir 0 0 0 \u0015 7 Vektorräume mit Skalarprodukt Skalarprodukt Skalarprodukt in einem Vektorraum V über E ist eine Funk- tion h\u0001, \u0001i : V \u0002 V 7! E mit folgenden Eigenschaften: • Linear im zweiten Faktor: hx, y + zi = hx, yi + hx, zi und hx, αyi = αhx, yi. Für reelle Skalare auch linear im ersten Faktor. • Symmetrisch wenn E = R und hermitesch wenn E = C: hx, yi = hy, xi • Positiv definit: 8x 2 V , hx, xi 2 R (auch für komplexe Vektorräume!), hx, xi \u0015 0, hx, xi = 0 () x = 0 Falls E = C, nennt man V auch einen unitären Vektorraum, für E = R auch euklidischer oder orthogonaler Vektorraum. Für ein Skalarprodukt definiert man die induzierte Norm als kxk =phx, xi. 4 Norm Norm in Vektorraum V über E ist eine Funktion k\u0001k : V 7! R mit: • positiv definit: 8x 2 V , kxk \u0015 0 und kxk = 0 () x = 0 • homogen: kαxk = jαj \u0001 kxk 8x 2 V, 8α 2 E • Dreiecksungleichung: kx + yk \u0014 kxk + kyk 8x, y 2 V Ein Vektorraum mit einer Norm heisst normierter Vektor- raum. Euklidischer Raum Das euklidische Skalarprodukt ist definiert als: hx, yi := xHy Die euklidische 2-Norm ist entsprechend definiert als: kxk2 =pxHx Die p-Norm ist definiert als kxkp = (jx1jp + \u0001 \u0001 \u0001 + jxnjp) 1 p . Skalarprodukte hx, yiV ist ein Skalarprodukt genau dann wenn hx, yiV := xHAy wobei A eine hermitesche Matrix mit strikt positiven Eigenwerten ist (positiv-definit). Es gilt dann aij = hei, ej i. Cauchy-Schwarz Sei V ein Vektorraum über E mit beliebigem Skalarprodukt. jhx, yij \u0014 kxkkyk () jhx, yij2 \u0014 hx, xihy, yi Das Gleichheitszeichen gilt genau dann wenn x und y linear abhängig sind. Winkel Seien x, y, 2 V . Der Winkel zwischen x und y ist gegeben als: φ := arccos Rehx, yi kxkkyk Zwei Vektoren sind orthogonal, falls hx, yi = 0 (x ? y). Zwei Teilmengen sind orthogonal, wenn 8x 2 M, 8y 2 N gilt: hx, yi = 0 (M ? N ). Eine Basis ist orthogonal wenn hbk, bli = δkl für alle k, l, wobei das Kroneckerdelta definiert ist als δkl = (0 falls k 6= l 1 falls k = l . Gilt zusätzlich kbik = 1 8i ist sie orthonormal. Pythagoras Falls x ? y, so folgt kx \u0006 yk2 = kxk2 + kyk2. Allgemein gilt kx \u0000 yk2 = kxk2 + kyk2 \u0000 2kxkkyk cos(φ). Eine Menge von paarweise orthogonalen Vektoren ist linear un- abhängig wenn alle Vektoren ungleich null sind. Der Nullvektor ist orthogonal zu allen Vektoren. Für eine Orthonormalbasis fb1, \u0001 \u0001 \u0001 , bng und x 2 V gilt x =Pn k=1hbk, xibk. Parsevalsche Formel Seien x, y 2 V , fb1, \u0001 \u0001 \u0001 , bng eine Orthonormalbasis, so dass ξk := hbk, xiV und ηk := hbk, yiV . Für k = 1, \u0001 \u0001 \u0001 , n sind ξk und ηk die Koordinatenvektoren. hx, yiV = Σn k=1ξkηk = ξHη = hξ, ηiEn i: kxkV = kξkEn · ii: ∠(x, y)V = ∠(ξ, η)En · iii: x ? y () ξ ? η 7.1 Gram-Schmidt-Orthonormalisierungsverfahren Für einen Unterraum U von V ist U ⊥ := fy 2 V jfyg?U g das orthogonale Komplement. U und U ⊥ sind komple- mentäre Unterräume in direkter Summe zu V . Algorithmus Sei fa1, \u0001 \u0001 \u0001 , ang eine Menge linear unabhängiger Vektoren. Wir definieren rekursiv: • b1 := a1 ∥a1∥ • ebk := ak \u0000 Pk−1 j=1 hbj , akibj bk := fbk ∥fbk∥ Nach k Schritten sind fb1, \u0001 \u0001 \u0001 bkg paarweise orthonormal und spanfa1, \u0001 \u0001 \u0001 , akg = spanfb1, \u0001 \u0001 \u0001 , bkg. Wenn fa1, \u0001 \u0001 \u0001 , ang ei- ne Basis von V ist, ist fb1, \u0001 \u0001 \u0001 bng auch eine. Jeder endlich- dimensionale Vektorraum hat eine Orthonormalbasis. Fundamentale Unterräume einer Matrix Sei A 2 Em×n mit r := Rank A. N (A) = R(AH)⊥ \u0012 En N (A) \b R(AH) = En N (AH) = R(A)⊥ \u0012 Em N (AH) \b R(A) = Em dim R(A) = r dim N (A) = n \u0000 r dim R(AH) = r dim N (AH) = m \u0000 r 7.2 Basiswechsel und Koordinatentransformation von Orthonormalbasen Wir wollen von B nach B′, wobei beides Orthonormalbasen sind. Wir können b′ k = Pn j=1 τjkbj schreiben und erhalten die Basis- wechselmatrix T . Da B, B′ orthonormal sind gilt T H = T −1. Daher gilt ξ = T ξ′ und ξ′ = T Hξ. Zudem ist B = B′T und B′ = BT H, wobei alle Matrizen unitär/orthogonal sind. Die Trans- formationsmatrix einer Basistransformation zwischen Orthonormal- basen ist unitär/orthogonal. T ist ausserdem längen- und winkeltreu. Definition Eine lineare Abbildung F : X 7! Y ist unitär/orthogonal falls hF (v), F (w)iY = hv, wiX . Es gilt dann dass i: F ist längen- und winkeltreu · ii: Ker F = f0g, injektiv. Haben wir zusätzlich dim X = dim Y gilt i: F ist ein Isomor- phismus · ii: fb1, \u0001 \u0001 \u0001 , bng ist eine Orthonormalbasis von X () fF (b1), \u0001 \u0001 \u0001 , F (bn)g ist eine Orthonormalbasis von Y · iii: F −1 unitär/orthogonal · iv: Abbildungsmatrix A ist unitär/orthogonal. 8 QR-Zerlegung Eine Matrix kann dargestellt werden als A = QR wobei Q orthogo- nal und R eine obere Dreiecksmatrix ist. • Wende Gram-Schmidt auf den Spalten von A an ( =) Q) • R = Q⊤A lösen und R zu erhalten (alternativ r11 = ka1k, rjk = hqj , aki, rkk = k eqkk) Im allgemeinen Fall ist dabei für A 2 Em×n Q 2 Em×m und R 2 Em×n. Man definiert dann: A = QR = \u0002 eQj eQ⊥\u0003 \u0014 eR 0 \u0015 = eQ eR wobei eQ 2 Em×r, eR 2 Er×n. 9 Least Squares Sei A 2 Em×n Ax = b ein überbestimmtes LGS (m > n). Im Allgemeinen gibt es keine Lösung, wir möchten deshalb kAx \u0000 bk2 2 minimieren. Wir definieren also x∗ = argminx∈En kAx \u0000 bk2 2. Dies trifft für alle x 2 En für welche Ax \u0000 b senkrecht auf R(A) steht. Ax∗ \u0000 b ? R(A) () Ax∗ \u0000 b 2 N (AH) () AH(Ax∗ \u0000 b) = 0 Es gilt N (AHA) = N (A), denn AHAx = 0 =) xHAHAx = 0 =) kAxk2 2 = 0 () Ax = 0 und Ax = 0 =) AHAx = 0. 5 Normalengleichungen Eine Lösung des Least Squares Problems findet man durch lösen der Normalengleichungen: AHAx = AHb Ist Rank A = Rank AHA = n \u0014 m, so ist AHA invertierbar und x∗ = (AHA)−1AHb. Man nennt A+ = (AHA)−1AH auch die Pseudoinverse von A, da A+A = I. Haben wir eine QR-Zerlegung von A, gilt Rx∗ = QT b für A = QR. Für A 2 Em×n mit Rank A = n \u0014 m ist die Least Squares Lö- sung eindeutig bestimmt. Wenden wir Gram-Schmidt auf die Spal- ten von A a1, \u0001 \u0001 \u0001 , an und den Vektor an+1 := y an ergibt sich die eindeutige Lösung Ax∗ = y \u0000ebn+1 durch das Lot ebn+1 = y \u0000 Ax∗ ? R(A). 10 Determinanten Determinante Die Determinante einer quadratischen Matrix A 2 En×n ist definiert als: det(A) = X p∈Sn sign(p)a1,p(1) \u0001 \u0001 \u0001 an,p(n) Sn ist dabei die symmetrische Gruppe n mit Ordnung n!. Jede Permutation p kann als Produkt von Transpositionen (Permutation bei welcher nur zwei Elemente vertauscht wer- den) dargestellt werden. sign(p) = (1 p Produkt gerader Anzahl Transpositionen \u00001 p Produkt ungerader Anzahl Transpositionen Für 3 \u0002 3 Matrizen gilt (Sarrus): a11 a12 a13 a11 a12 a21 a22 a23 a21 a22 a31 a32 a33 a31 a32 + + + \u0000 \u0000 \u0000 Für 1 \u0002 1 Matrizen gilt det(a11) = a11 und für 2 \u0002 2 Matrizen gilt det\u0000 a11 a12 a21 a22 \u0001 = a11a22 \u0000 a12a21. Für Blockdreiecksmatrizen gilt: det \u0014 A C 0 B \u0015 = det(A) det(B) Eigenschaften der Determinante Für die Determinante einer Matrix A 2 En×n gilt: 1. det ist eine lineare Funktion in jeder Zeile. det 0 B B B B @ a11 a12 ··· a1n ... ... . . . ... γal1+γ′a′ l1 γal2+γ′a′ l2 ··· γaln+γ′a′ ln ... ... . . . ... an1 an2 ··· ann 1 C C C C A = γ det 0 B B B B @ a11 a12 ··· a1n ... ... . . . ... al1 al2 ··· aln ... ... . . . ... an1 an2 ··· ann 1 C C C C A+γ′ det 0 B B B B @ a11 a12 ··· a1n ... ... . . . ... a′ l1 a′ l2 ··· a′ ln ... ... . . . ... an1 an2 ··· ann 1 C C C C A 2. Vertauscht man zwei Zeilen, dann wechselt det(A) das Vorzeichen 3. det(I) = 1 Die Determinante ist die einzige auf En×n definierte Funktion mit den obigen drei Eigenschaften. Folgerungen für det(A), A 2 En×n • det(A) = 0 () A ist singulär – hat A also eine Nullzeile oder Nullspalte, so ist det(A) = 0 – hat A zwei gleiche Spalten oder zwei gleiche Zei- len, so ist det(A) = 0 • Addiert man zu einer Zeile (oder Spalte) eine ande- re Zeile (oder Spalte) multipliziert mit einem Skalar, dann ändert sich det(A) nicht • det(γA) = γn det(A) • ist A eine Diagonalmatrix oder Dreiecksmatrix, so ist det(A) = Qn i=1 aii • wenden wir Gauss auf A an so gilt det(A) = (\u00001)v Qn k=1 rkk wobei v die Anzahl Zeilenvertau- schungen ist und rkk die Pivotwerte der REF sind • det(AH) = det(A) sowie det(A⊤) = det(A) (auch für komplexe Matrizen) – wenn man zwei Spalten vertauscht, dann wech- selt det(A) das Vorzeichen – det ist eine lineare Funktion in jeder Spalte • det(AB) = det(A) det(B) und det(A−1) = 1 det(A) falls A−1 existiert Kofaktor Zu jedem Element akl einer n\u0002n Matrix A ist A[k,l] definiert als die (n \u0000 1) \u0002 (n \u0000 1) Matrix mit der Zeile k und Spalte l gestrichen. Für den Kofaktor κkl von akl gilt dann: κkl = (\u00001)k+l det(A[k,l]) Dann gilt für jedes feste k, l (Entwicklung nach Zeile k bzw. Spalte l): det(A) = nX i=1 akiκki det(A) = nX i=1 ailκil 11 Eigenwerte und Eigenvektoren Definition Eine Zahl λ 2 K heisst Eigenwert der linearen Abbildung F : V 7! V falls es v 2 V mit v 6= 0 gibt so dass F (v) = λv. v ist dabei ein Eigenvektor. Eine lineare Abbildung F und ihre Matrixdarstellung haben die gleichen Eigenwerte und die Eigenvektoren sind über die Koordina- tenabbildung κV verbunden. Spektrum Die Menge aller Eigenwerte von F heisst Spektrum von F und wird mit σ(F ) bezeichnet. Die Menge aller Eigenvektoren die zu einem Eigenwert λ gehö- ren bilden einen Unterraum Eλ = fv 2 V jF (v) = λvg. λ ist ein Eigenwert von A genau dann wenn A \u0000 λI einen nicht- trivialen Nullraum hat. Es gilt dann Eλ = Ker(A \u0000 λI). Die geo- metrische Vielfachheit von λ entspricht dann dim Eλ. Charakteristisches Polynom Das charakteristische Polynom von A ist das Polynom χA(λ) = det(A \u0000 λI). χA(λ) = 0 ist dabei die charakte- ristische Gleichung. χA(λ) = (\u00001)nλn + (\u00001)n−1 Tr(A)λn−1 + \u0001 \u0001 \u0001 + det(A) Die algebraische Vielfachheit eines Eigenwertes λ ist die Vielfachheit von λ als Nullstelle von χA über C. Sei A 2 E2×2. Dann ist das charakteristische Polynom χA(λ) = λ2 \u0000 Tr(A)λ + det(A). Für schiefsymmetrische Matrizen sind alle Eigenwerte ent- weder 0 oder imaginär. Bei Dreiecksmatrizen entsprechen die Ei- genwerte den Werten auf der Diagonalen. 6 Eigenschaften von Eigenwerten und Eigenvektoren • Für jede Matrix A 2 En×n gilt: det(A) = nY i=1 λi Tr(A) = Σn i=1λi • λ ist ein Eigenwert von A genau dann wenn λ eine Nullstelle von χA ist • Für einen Eigenwert λ gilt: 1 \u0014 geo. Vfh. von λ \u0014 alg. Vfh. von λ \u0014 n • Eigenvektoren zu verschiedenen Eigenwerten sind li- near unabhängig • Eine Matrix ist genau dann singulär wenn sie 0 als Eigenwert hat Eigenwerte und Eigenvektoren finden 1. Bestimmte das charakteristische Polynom χA(λ) = det(A \u0000 λI) 2. Bestimme die Nullstellen λ1, λ2, \u0001 \u0001 \u0001 , λn von χA (Ei- genwerte) 3. Für jedes λk bestimme eine Basis für Ker(A \u0000 λkI) (Eigenvektoren) Ähnliche Matrizen Zwei Matrizen A, C 2 En×n sind ähnlich falls es ein T 2 En×n gibt so dass C = T −1AT . Ähnliche Matrizen haben das gleiche charakteristische Poly- nom, die gleiche Spur, die gleiche Determinante, den selben Rang und die gleichen Eigenwerte inkl. algebraischer und geo- metrischer Vielfachheit (im Allgemeinen aber nicht die selben Eigenvektoren). 12 Spektral- und Eigenwertzerlegung Eine Matrix A 2 En×n besitzt genau dann eine Eigenbasis wenn es eine ähnliche Diagonalmatrix Λ gibt. Man nennt die Matrix dann diagonalisierbar. A \u0014 | | | v1 ··· vn | | | \u0015 | {z } V = \u0014 | | | v1 ··· vn | | | \u0015 | {z } V 2 4 λ1 ··· 0 ... . . . ... 0 ··· λn 3 5 | {z } Λ A = V ΛV −1, Avj = vj λj , j = 1, \u0001 \u0001 \u0001 , n A wird durch V diagonalisiert. Die Kolonnen von V sind Ei- genvektoren, die Diagonalelemente von Λ Eigenwerte. Eine Matrix ist genau dann diagonalisierbar wenn für alle Eigenwerte die geo- metrische Vielfachheit der algebraischen Vielfachheit ent- spricht. Dies trifft immer zu wenn alle n Eigenwerte verschieden sind. Spektralsatz Sei A 2 En×n hermitesch (AH = A). Dann gilt: • Alle Eigenwerte sind reell • Die Eigenvektoren sind paarweise orthogonal • Es gibt eine orthonormale Basis aus Eigenvektoren U so dass A = U ΛU H Dasselbe gilt für reell-symmetrische Matrizen. In diesem Fall sind die Eigenvektoren reell. Für Matrizen mit reellen Eigen- werten und einer reellen Orthonormalbasis aus Eigenvektoren gilt auch die Umkehrung. Normale Matrizen Eine Matrix A 2 En×n ist normal, falls AHA = AAH. A ist genau dann diagonalisierbar durch eine unitäre Matrix über C falls A normal ist. 13 Singulärwertszerlegung Die Singulärwertszerlegung existiert für jede Matrix A 2 Em×n. AHA ist immer hermitesch und positiv-definit. Es existiert also eine Spektralzerlegung: AHAV = V Λ λ=σ2 ========) Umordnung AHAVr = VrΣ2 r =) V H r AHAVr = Σ2 r =) (Σ−1 r V H r AH) | {z } U H r (r×m) (AVrΣ−1 r ) | {z } Ur (m×r) = I Ur kann zu einer unitären m\u0002m Matrix U ergänzt werden. U, V sind unitär, Σ ist nicht-negativ, reell und diagonal mit σ1 \u0015 \u0001 \u0001 \u0001 σr > σr+1 = \u0001 \u0001 \u0001 = σn = 0 wobei r = Rank A = Rank AHA = Rank AAH. V diagonalisiert AHA, U diagonalisiert AAH. Singulärwertszerlegung A = U ΣV H = rX k=1 ukσkvH k AAH = U Σ2 mU H, AHA = V Σ2 nV H Ausserdem gilt AH = V Σ⊤U H und falls A invertierbar ist A−1 = V Σ−1U H. Aus AV = U Σ und AHU = V Σ⊤ folgt: • fu1, \u0001 \u0001 \u0001 , urg: Orthonormalbasis von Im A = R(A) • fur+1, \u0001 \u0001 \u0001 , umg: Orthonormalbasis von Ker AH = N (AH) • fv1, \u0001 \u0001 \u0001 , vrg: Orthonormalbasis von Im AH = R(AH) • fvr+1, \u0001 \u0001 \u0001 , vng: Orthonormalbasis von Ker A = N (A) Bei einer Selbstabbildung gilt: A = U ΣV H = V V HU| {z } R ΣV H = V RΣV H wobei R eine Rotation/Spiegelung ist und Σ die Hauptachsen ska- liert. AVr = UrΣr ist die reduzierte Form der Singulärwertszerle- gung. Es gilt dabei U H r Ur = I sowie V H r Vr = I. Aus der Singulärwertszerlegung folgt auch dass die r positiven Eigenwerte von AHA und AAH gleich sind, die Vielfachheit des Eigenwertes 0 aber n \u0000 r respektive m \u0000 r ist. 14 Synthetische Division Berechne 6x3+5x2−7 3x2−2x−1 = 2x + 3 + 8x−4 3x2−2x−1 : 15 Trigonometrie • sin(z) = e iz −e −iz 2i • cos(z) = e iz +e−iz 2 • sin(x) \u0000 sin(y) = 2 sin( x−y 2 ) cos( x+y 2 ) • cos(x) \u0000 cos(y) = \u00002 sin( x−y 2 ) sin( x+y 2 ) • sin(α + β) = sin(α) cos(β) + cos(α) sin(β) • cos(α + β) = cos(α) cos(β) \u0000 sin(α) sin(β) deg 0° 30° 45° 60° 90° 180° rad 0 π 6 π 4 π 3 π 2 π cos 1 √ 3 2 √ 2 2 1 2 0 -1 sin 0 1 2 √ 2 2 √3 2 1 0 tan 0 1√ 3 1 p3 +1 0","libVersion":"0.3.1","langs":""}