{"path":"sem3/LinAlg/UE/s/LinAlg-s-u01.pdf","text":"D-INFK Linear Algebra HS 2024 Bernd G¨artner Robert Weismantel Solution for Assignment 1 1. a) By definition of L, there exists w ∈ Rm such that L = {λw : λ ∈ R}. In particular, we can write u = λuw for some λu ∈ R. Observe that we have λu ̸= 0 because u ̸= 0. Now consider an arbitrary vector v ∈ L. There must be λv ∈ R such that v = λvw. Putting these together, we get v = λv λu u. This already proves L ⊆ {λu : λ ∈ R} and it remains to prove {λu : λ ∈ R} ⊆ L. For this, consider an arbitrary vector v′ = λv′u ∈ {λu : λ ∈ R}. Combining v′ = λv′u with u = λuw we get v′ = λv′λuw and hence v′ ∈ L. We conclude that L = {λu : λ ∈ R}. b) Let L1 and L2 be two lines of Rm. By definition, there exist w1 and w2 such that L1 = {λw1 : λ ∈ R} and L2 = {λw2 : λ ∈ R}. In order to see that 0 ∈ L1 ∩ L2, it suffices to observe 0 ∈ L1 and 0 ∈ L2 since we have 0 = 0w1 = 0w2. Now assume L1 ∩ L2 ̸= {0}. Because L1 ∩ L2 is not empty, there exists a non-zero vector u ∈ L1 ∩ L2. By u ∈ L1, we know from part a) that L1 = {λu : λ ∈ R}. Analogously, we have L2 = {λu : λ ∈ R} and hence L1 = L2. 2. a) By definition, there must be a vector w = [w1 w2 ] ∈ R2 such that L = {λw : λ ∈ R}. We want to find a vector d = [d1 d2 ] ∈ R2 such that L = {v ∈ R2 : v · d = 0}. In particular, we want w · d = w1d1 + w2d2 ! = 0 since w ∈ L. Choosing d1 := −w2 and d2 := w1 would certainly work, so let this be our “guess” for d. It remains to prove that with this choice of d, we have L = {v ∈ R2 : v · d = 0}. ⊆: Consider an arbitrary element u = λuw ∈ L. We have u · d = (λuw) · d = λuw1d1 + λuw2d2 = λu(w1d1 + w2d2) = 0 and hence u ∈ {v ∈ R2 : v · d = 0}. ⊇: Consider an arbitrary element v ∈ {v ∈ R2 : v · d = 0}. In particular, we have v1d1 + v2d2 = −v1w2 + v2w1 = 0. Our goal is to find λ such that v = λw. Recall from the definition of a line that we must have w ̸= 0 and hence either w1 ̸= 0 or w2 ̸= 0. Assume first w1 ̸= 0 and observe that we can rewrite −v1w2 + v2w1 = 0 to v2 = w2 w1 v1. Choosing λ = v1 w1 we can see that indeed, we have v1 = λw1 and v2 = λw2, as desired. If we have w1 = 0, then it must be the case that w2 ̸= 0. But then we can rewrite −v1w2 + v2w1 = 0 to v1 = w1 w2 v2 and choose λ = v2 w2 . b) In order to prove that S′ is a subset of a hyperplane of Rm+1, we need to find a non-zero vector d′ ∈ Rm+1 such that w · d′ = 0 for all w ∈ S′. To achieve this, consider what we know about an arbitrary w ∈ S′: It must be of the form w =        v1 v2 ... vm 1        1 d = [ 1 3 ] 0 [ −3 1 ] xy Figure 1: This figure illustrates the situation in task a). The yellow line given by {λ [ −3 1 ] : λ ∈ R} is equal to the hyperplane {v : v · [ 1 3 ] = 0}. for some v ∈ S. So for the scalar product w · d′ we get w · d′ = w1d ′ 1 + w2d ′ 2 + · · · + wm+1d ′ m+1 = v1d ′ 1 + v2d ′ 2 + · · · + vmd ′ m + d ′ m+1. By v ∈ S we know that v · d = c. So if we now choose d′ =        d1 d2 ... dm −c        we get w · d ′ = v1d ′ 1 + v2d ′ 2 + · · · + vmd ′ m + d ′ m+1 = (v · d) − c = (v · d − c) = 0 as desired. This works for any w ∈ S′ and hence S′ is a subset of the hyperplane {v ∈ Rm+1 : v · d′}. 3. a) Let 1 ∈ Rm denote the vector whose entries are all 1. Now observe that m∑ i=1 vi = m∑ i=1 1vi = 1 · v ≤ ∥ 1 ∥ ∥ v ∥ where we used Cauchy-Schwarz in the inequality. It remains to observe that ∥ 1 ∥ = √m. b) Let w ∈ Rm denote the vector whose i-th entry is √i for all i ∈ [m] := {1, . . . , m}. Observe that m∑ i=1 √ivi = w · v ≤ ∥ w ∥ ∥ v ∥ where we used Cauchy-Schwarz in the inequality. For the length ∥ w ∥ of w, we get ∥ w ∥ = √ √ √ √ m∑ i=1(√i)2 = √ √ √ √ m∑ i=1 i = √ (m + 1)m 2 = √ m2 2 + m 2 ≤ √ m2 2 + m2 2 = m. 2 4. a) This set of vectors is not linearly independent as v can always be written as a linear combi- nation of any other vector, e.g. 0u = v. b) This set of vectors is linearly independent. To see this, we first check that v cannot be obtained as a linear combination of u: indeed, observe that we cannot obtain the 1 in the third and fourth coordinate of v from u as u contains 0 in both of those coordinates. Next, we check that w cannot be obtained as a linear combination of u and v. Such a linear combination would require scalars λ and µ such that λu + µv = λ     1 1 0 0     + µ     0 0 1 1     ! =     0 1 1 0     = w. But notice that the 1 in the second coordinate of w can only be obtained by setting λ = 1. Similarly, the 1 in the third coordinate of w can only be obtained with µ = 1. But with this choice of λ and µ, we would get 1 (instead of 0) in the first and forth coordinate. So we conclude that there is no such linear combination. From the lecture we know that checking the three vectors in any order (we did it in the order they were given) suffices to check linear independence. Hence, we conclude that the three vectors are linearly independent. 5. a) With λi := (−1)i for all i ∈ {1, . . . , m}, we get m∑ i=1 λivi = m−1∑ i=1 (−1) i(ei + ei+1) + (−1) m(em + e1). Now observe that we have m−1∑ i=1 (−1) i(ei + ei+1) = −(e1 + e2) + (e2 + e3) − · · · − (em−1 + em) which reduces to −e1 − em. Plugging this into the previous equation, we therefore obtain m∑ i=1 λivi = −e1 − em + (−1) m(em + e1) = −e1 − em + (em + e1) = 0. By Lemma 1.19, we conclude that the vectors are linearly dependent. b) Consider arbitrary λ1, . . . , λm ∈ R such that λ1v1 + · · · + λmvm = 0. By definition of the vectors v1, . . . , vm, the i-th coordinate is non-zero only in vi−1 and vi for all i ∈ {2, . . . , m}, and the first coordinate is non-zero only in v1 and vm. Moreover, every non- zero entry is exactly 1. Hence, we must have λi = −λi−1 for all i ∈ {2, . . . , m}, and λ1 = −λm. Therefore, we get λ1 = −λ2 = −(−λ3) = · · · = (−1) m−1λm = (−1) mλ1, which implies that λ1 = λ2 = · · · = λm = 0 since m is odd. By Lemma 1.19, we conclude that the vectors are linearly independent. 6. We first observe that we have ∥ v ∥ = √ x2 + y2 + z2 = √ z2 + x2 + y2 = ∥ w ∥. In particular, this implies ∥ v ∥ ∥ w ∥ = x2 +y2 +z2. Using the formula from the lecture for the angle α between v and w, we calculate cos(α) = v · w ∥ v ∥ ∥ w ∥ = xz + yx + zy x2 + y2 + z2 . 3 Next, observe that we can rewrite xz + yx + zy = 1 2 (x + y + z)2 − 1 2 (x2 + y2 + z2). By our assumption x + y + z = 0, the first term vanishes and we obtain cos(α) = xz + yx + zy x2 + y2 + z2 = − 1 2 (x2 + y2 + z2) x2 + y2 + z2 = − 1 2 . To find α, it remains to look up (or remember) cos−1(− 1 2 ) = 2 3 π ( = 120◦). 7. We follow roughly the proof of Fact 1.5 and write down the two equations v1λ + w1µ = u1 v2λ + w2µ = u2 that we want to solve for λ, µ ∈ R. Since we assumed v ̸= 0, we know that either v1 ̸= 0 or v2 ̸= 0. Without loss of generality, assume that v1 ̸= 0. This allows us to rewrite the first equation as λ = u1 − w1µ v1 . By plugging this into the second equation and re-grouping, we get µ(w2 − v2w1 v1 ) + u1v2 v1 = u2. Now observe that w2 − v2w1 v1 = 0 would imply that w1 v1 v = w, which contradicts our assumption. Thus, we conclude that w2 − v2w1 v1 ̸= 0 and that we can solve for µ as µ = u2 − u1v2 v1 w2 − v2w1 v1 . In other words, we proved that choosing µ = u2− u1v2 v1 w2− v2w1 v1 and λ = u1−w1µ v1 is possible (we had to argue that the denominator is non-zero), and with this choice we get λv + µw = u, as desired. 4","libVersion":"0.5.0","langs":""}