{"path":"sem3/LinAlg/LinAlg-plan-w04.pdf","text":"Lecture plan Linear Algebra (401-0131-00L, HS24), ETH Z ¨urich Numbering of Sections, Definitions, Figures, etc. as in the Lecture Notes Week 4 Gauss elimination (Section 3.2) Algorithm for solving Ax = b with square matrix (m × m). Back substitution: if A is upper triangular   2 3 4 0 5 6 0 0 7     x1 x2 x3   =   19 17 14   equation before substitution after substitution solution 1 2x1 + 3x2 + 4x3 = 19 2x1 + 11 = 19 x1 = 4 2 5x2 + 6x3 = 17 5x2 + 12 = 17 x2 = 1 3 7x3 = 14 x3 = 2 ↑        Case m × m: Equation i = m, m − 1, . . . , 1: m∑ j=i aijxj = bi. Already know xi+1, . . . , xm: xi = bi − ∑m j=i+1 aijxj aii . Needs aii ̸= 0! 1 f o r ( i = m− 1 ; i >= 0 ; i − −) { 2 sum = b [ i ] ; 3 f o r ( j = i + 1 ; j < m; j ++) 4 sum −= A[ i ] [ j ] * x [ j ] ; 5 x [ i ] = sum / A[ i ] [ i ] ; 6 } Back substitution code Elimination: If A not upper triangular • Ax = b → U x = c (same solutions, U upper triangular) • solve U x = c with back substitution A =   2 3 4 4 11 14 2 8 17   , b =  19 55 50   1 Get rid of 4: subtract 2 · (equation 1) from equation 2: (equation 2) : 4x1 + 11x2 + 14x3 = 55 − 2 · (equation 1) : 4x1 + 6x2 + 8x3 = 38 (equation 2’) : 5x2 + 6x3 = 17 → A ′x = b′: A ′ =  2 3 4 0 5 6 2 8 17   , b′ =   19 17 50   . This was a row subtraction: linear transformation applied to all columns of A and b: TE21     x1 x2 x3     =   x1 x2 − 2x1 x3   =   1 0 0 −2 1 0 0 0 1   ︸ ︷︷ ︸ E21   x1 x2 x3   . E21: elimination matrix A ′ = E21A, b′ = E21b E21: subtract 2·(row 1) from (row 2) Can undo it: A = E′ 21A ′, b = E′ 21b′ E′ 21: add 2·(row 1) to (row 2) Ax = b and A ′x = b′ have the same solutions: • If Ax = b, then A ′x = E21Ax = E21b = b′ • If A ′x = b′, then Ax = E′ 21A ′x = E′ 21b′ = b Column by column, eliminate all red entries (→ upper triangular system from before): fat number: the pivot A =   2 3 4 4 11 14 2 8 17   b =  19 55 50   subtract 2·(row 1) from (row 2): ↓ ↓ E21 =   1 0 0 −2 1 0 0 0 1   E21A =  2 3 4 0 5 6 2 8 17   E21b =   19 17 50   subtract 1·(row 1) from (row 3): ↓ ↓ E31 =   1 0 0 0 1 0 −1 0 1   E31E21A =  2 3 4 0 5 6 0 5 13   E31E21b =   19 17 31   subtract 1·(row 2) from (row 3): ↓ ↓ E32 =  1 0 0 0 1 0 0 −1 1   E32E31E21A ︸ ︷︷ ︸ U =  2 3 4 0 5 6 0 0 7   E32E31E21b ︸ ︷︷ ︸ c =   19 17 14   ↑ elimination matrices done! Now back substitution. . . 2 Row exchanges: also undoable, solutions stay the same A =   2 3 4 4 6 14 2 8 17   b = · · · elimination in first column: ↓ ↓ E31E21A =   2 3 4 0 0 6 0 5 13   E31E21b = · · · pivot 0: exchange (row 2) and (row 3): ↓ ↓ P23 =   1 0 0 0 0 1 0 1 0   P23E31E21A ︸ ︷︷ ︸ U =   2 3 4 0 5 13 0 0 6   P23E31E21b ︸ ︷︷ ︸ c = · · · ↑ permutation matrix done! Failure: no row exchange helps, give up for now! A =   2 3 4 4 6 14 2 3 17   b = · · · elimination in first column: ↓ ↓ E31E21A =   2 3 4 0 0 6 0 0 13   E31E21b = · · ·   2 3 4 0 5 6 0 0 0   we also fail here! 1 f o r ( j = 0 ; j < m; j ++) { 2 / / e l i m i n a t e i n column j 3 i f (A[ j ] [ j ] == 0 ) { 4 / / z e r o p i v o t , t r y row e x c h a n g e 5 k = j + 1 ; 6 while ( k < m && (A[ k ] [ j ] == 0 ) ) k ++; 7 i f ( k == m) 8 r e t u r n f a l s e ; / / no row e x c h a n g e i s p o s s i b l e , g i v e up 9 e l s e { 10 / / e x c h a n g e rows j and k . . . 11 row A = A[ j ] ; A[ j ] = A[ k ] ; A[ k ] = row A ; / / . . . o f A 12 row b = b [ j ] ; b [ j ] = b [ k ] ; b [ k ] = row b ; / / . . . o f b 13 } 14 } 15 / / c r e a t e z e r o s b e l o w A[ j ] [ j ] 16 f o r ( i = j + 1 ; i < m; i ++) { 17 / / s u b t r a c t c * row j f r o m row i . . . 18 c = A[ i ] [ j ] / A[ j ] [ j ] ; 19 A[ i ] [ j ] = 0 ; 20 f o r ( k = j + 1 ; k < m; k++) 21 A[ i ] [ k ] −= c * A[ j ] [ k ] ; / / . . . o f A 22 b [ i ] −= c * b [ j ] ; / / . . . o f b 23 } 24 } 25 r e t u r n t r u e ; Elimination code 3 Success and failure “Solutions stay the same”, general case m × n: subtract c · (row j) from (row i) row operation exchange (row j) and (row k) Eij = m×m ︷ ︸︸ ︷      ⧹ 1 ⧹ −c 1 ⧹       ← j ← i ↑ ↑ j i Pjk = m×m ︷ ︸︸ ︷      ⧹ 0 1 ⧹ 1 0 ⧹       ← j ← k ↑ ↑ j k elimination matrix row operation matrix permutation matrix Lemma 3.3: Let Ax = b, A ∈ R m×n, M ∈ R m×m a row operation matrix, A ′ = M A and b′ = M b. Ax = b and A′x = b′ have the same solutions. Proof. Undo M with M ′: add c · (row j) to (row i), or exchange (row j) and (row k) again: A ′ = M ′A, b = M ′b′. Ax = b ⇔ A ′x = b′ follows as in 3 × 3 case. Corollary 3.4: Let A ∈ R m×n, M ∈ R m×m a row operation matrix, A′ = M A. A has linearly independent columns if and only if A ′ has linearly independent columns. Proof. Lemma 3.3 with b = 0: Ax = 0 and A ′x = 0 have the same solutions. Only x = 0 Another x : both have linearly independent dependent columns (Observation 3.2). Theorem 3.5: Let Ax = b be a system of m linear equations in m variables. The following two statements are eqivalent. (i) Gauss elimination succeeds. (ii) The columns of A are linearly independent. We prove (i)⇒(ii) and ¬(i)⇒ ¬(ii): if not (i), then not (ii). This is the contraposition of (ii)⇒(i) and logically equivalent. Proof. (i)⇒(ii): If Gauss elimination succeeds: A → upper triangular U with all ujj ̸= 0. U has linearly independent columns by Corollary 1.20 (iii): no column is a linear combination of the previous ones. We get (ii) by Corollary 3.4, repeatedly applied (A → A′ . . . → U ). ¬(i)⇒ ¬(ii):If Gauss elimination fails in column j, A → A ′, A ′ = [ U v · · · 0 0 · · · ] , U ∈ R (j−1)×(j−1) upper triangular, all uℓℓ ̸= 0, v ∈ R j−1. 4 Construct x ̸= 0 : Ax = 0. Then A ′ has linearly dependent columns (Observation 3.2). This gives ¬(ii) by Corollary 3.4 (A → . . . → A′). Construction: xj+1, xj+2, . . . , xm = 0, U      x1 x2 ... xj−1      ︸ ︷︷ ︸ y +xjv = 0 ⇒ Ax = 0. ↑ xj = −1, solve U y = v (back substitution!) Runtime: Count arithmetic steps (−, ·, /)! Theorem 3.6 Let Ax = b be a system of m linear equations in m variables, m ≥ 1. Gauss elimination with back substitution solves Ax = b (or gives up) with at most g(m) = 2 3m 3 + 3 2 m 2 − 7 6m arithmetic steps and therefore in time O(m 3). Proof. Count arithmetic steps in elimination: e(m) steps 1 f o r ( j = 0 ; j < m; j ++) { 2 / / e l i m i n a t e i n column j 3 i f (A[ j ] [ j ] == 0 ) { 4 / / z e r o p i v o t , t r y row e x c h a n g e 5 k = j + 1 ; 6 while ( k < m && (A[ k ] [ j ] == 0 ) ) k ++; 7 i f ( k == m) 8 r e t u r n f a l s e ; / / no row e x c h a n g e i s p o s s i b l e , g i v e up 9 e l s e { 10 / / e x c h a n g e rows j and k . . . 11 row A = A[ j ] ; A[ j ] = A[ k ] ; A[ k ] = row A ; / / . . . o f A 12 row b = b [ j ] ; b [ j ] = b [ k ] ; b [ k ] = row b ; / / . . . o f b 13 } 14 } 15 / / c r e a t e z e r o s b e l o w A[ j ] [ j ] 16 f o r ( i = j + 1 ; i < m; i ++) { 17 / / s u b t r a c t c * row j f r o m row i . . . 18 c = A[ i ] [ j ] / A[ j ] [ j ] ; 19 A[ i ] [ j ] = 0 ; 20 f o r ( k = j + 1 ; k < m; k++) 21 A[ i ] [ k ] −= c * A[ j ] [ k ] ; / / . . . o f A 22 b [ i ] −= c * b [ j ] ; / / . . . o f b 23 } 24 } 25 r e t u r n t r u e ; 1 / 1 ·, 1 − 1 ·, 1 − 5 Count arithmetic steps in back substitution: b(m) steps 1 f o r ( i = m− 1 ; i >= 0 ; i − −) { 2 sum = b [ i ] ; 3 f o r ( j = i + 1 ; j < m; j ++) 4 sum −= A[ i ] [ j ] * x [ j ] ; 5 x [ i ] = sum / A[ i ] [ i ] ; 6 } 1 ·, 1 − 1 / Counting formulas in the lecture notes! Dominating term: 2 3m3: Gauss elimination is very slow on large systems. O(m 3): 3 nested loops in elimination (lines 1, 16, 20) Inverse matrices (Section 3.3) Row operation matrix M : do undo do & undo A = I undo & do A ′ = I A ′ = M A A = M ′A′ A = M ′ M A︸︷︷︸ A′ M ′M = I A′ = M M ′A ′ ︸ ︷︷ ︸ A M M ′ = I Definition 3.7 Let M be an m × m matrix. M is called invertible if there exists an m × m matrix M −1 (called the inverse of M ) such that M M −1 = M −1M = I. Case 1 × 1: M = [a ] ⇒ M −1 = [ 1 a] (if a ̸= 0). Case 2 × 2: check it! M = [a b c d ] , ⇒ M −1 = 1 ad − bc [ d −b −c a ] (if ad − bc ̸= 0). Inverse is unique: Lemma 3.8: Let M be an m × m matrix with two inverses A and B. Then A = B. Proof. A = IA = (BM )A = B(M A) = BI = B. Lemma 3.9: Let A and B be invertible m × m matrices. Then AB is invertible, and (AB) −1 = B−1A−1. Proof sketch. Undo in reverse order (put socks on, then shoes; take shoes off, then socks). Lemma 3.10: Let A be an invertible m × m matrix. Then the transpose A ⊤ is invertible, and (A ⊤)−1 = (A −1)⊤ . The Inverse Theorem: “good” matrices Theorem 3.11: Let A be an m × m matrix. The following statements are equivalent. 6 (i) A is invertible. (ii) For every b ∈ R m, Ax = b has a unique solution x. (iii) The columns of A are linearly independent. Success of Gauss elimination ⇔ (iii), Theorem 3.5. Proof plan: (i) ⇒ (ii) ⇑ ⇓ (ii) ⇐ (iii) Proof. (i) ⇒ (ii): if A is invertible, x = A−1b is a solution of Ax = b: A(A −1b︸ ︷︷ ︸ x ) = (AA−1)b = Ib = b. Uniqueness: take any x satisfying Ax = b: x = A−1Ax = A−1b. (ii) ⇒ (iii): if Ax = b has a unique solution for every b, then also for b = 0. By Observa- tion 3.2, the columns of A are linearly independent. (iii) ⇒ (ii): Let x, x′ be two solutions: Ax = Ax′ = b. Then A(x − x′) = 0. If the columns of A are linearly independent, then x − x′ must be 0 (Observation 3.2), so x = x′. (ii) ⇒ (i): If Ax = b has a unique solution for all b, we find v1, v2, . . . , vm ∈ R m such that Av1 =      1 0 ... 0      ︸︷︷︸ e1 , Av2 =      0 1 ... 0      ︸︷︷︸ e2 , . . . , Avm =      0 0 ... 1      ︸︷︷︸ em ⇒ A   | | | v1 v2 · · · vm | | |   ︸ ︷︷ ︸ B =      1 0 · · · 0 0 1 · · · 0 ... ... . . . ... 0 0 · · · 1      ︸ ︷︷ ︸ I . Still need BA = I: • AI = IA = (AB)A = A(BA), so A(I − BA) = 0. • w1, w2, . . . , wm: columns of I − BA. Then A(I − BA) = 0 means Awj = 0 for all j. • The columns of A are linearly independent by (ii) ⇒ (iii). Hence wj = 0 for all j: by Observation 3.2, 0 is the only solution of Ax = 0. • All columns of I − BA are 0, so BA = I. Exercise 3.12: For all A, B: If AB = I, then BA = I, so we only need one condition in Definition 3.7 of the inverse. 7","libVersion":"0.3.2","langs":""}