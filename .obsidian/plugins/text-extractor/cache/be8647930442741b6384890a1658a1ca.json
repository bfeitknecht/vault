{"path":"sem3/LinAlg/UE/s/LinAlg-s-u11.pdf","text":"D-INFK Linear Algebra HS 2024 Bernd G¨artner Robert Weismantel Solution for Assignment 11 1. a) Let Cij be the co-factors of A where i, j ∈ [5]. Note that by combining Propositions 6.0.16 and 6.0.9, we get detA 6.0.9 = detA⊤ 6.0.16 = 5∑ j=1(A⊤)3,j(C⊤)3,j = 5∑ i=1 Ai,3Ci,3 = 0C1,3 + 0C2,3 + bC3,3 + 0C4,3 + 0C5,3 = b · (−1) (3+3) · ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ 0 1 4 c a 5 4 −1 0 −2 1 0 0 −4 3 1 ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ . This is also sometimes called expansion of the determinant along the third column. In par- ticular, we chose the third column because it contains many zeroes and hence many terms disappeared. In order to compute ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ 0 1 4 c a 5 4 −1 0 −2 1 0 0 −4 3 1 ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ we use the same trick again for the first column. In this way we obtain ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ 0 1 4 c a 5 4 −1 0 −2 1 0 0 −4 3 1 ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ = a · (−1) (2+1) ∣ ∣ ∣ ∣ ∣ ∣ 1 4 c −2 1 0 −4 3 1 ∣ ∣ ∣ ∣ ∣ ∣ . We repeat this one more time for the third column of ∣ ∣ ∣ ∣ ∣ ∣ 1 4 c −2 1 0 −4 3 1 ∣ ∣ ∣ ∣ ∣ ∣ to get ∣ ∣ ∣ ∣ ∣ ∣ 1 4 c −2 1 0 −4 3 1 ∣ ∣ ∣ ∣ ∣ ∣ = c · (−1) (1+3) · ∣ ∣ ∣ ∣ −2 1 −4 3 ∣ ∣ ∣ ∣ + 1 · (−1) (3+3) · ∣ ∣ ∣ ∣ 1 4 −2 1 ∣ ∣ ∣ ∣ . We can compute these 2 × 2 determinants directly with the formula from the lecture as ∣ ∣ ∣ ∣−2 1 −4 3 ∣ ∣ ∣ ∣ = −2 and ∣ ∣ ∣ ∣ 1 4 −2 1 ∣ ∣ ∣ ∣ = 9. Putting everything together, we obtain detA = b·(−1) (3+3) (a · (−1) (2+1) (c · (−1) (1+3) · (−2) + 1 · (−1) (3+3) · 9)) = ab(2c−9). We conclude that detA = 0 if and only if a = 0, or b = 0, or c = 9 2 . 1 b) As it turns out, we only need to perform one step of Gauss elimination on B to obtain U : B =   1 2 −3 2 6 0 −1 −2 2   →   1 2 −3 0 2 6 0 0 −1   =: U. Using Proposition 6.0.8, we see that det(U ) = −2. By using Proposition 6.0.22 (and the discussion in Section 6.0.4), we know that the determinant of U is the same as the determinant of B (we did not swap any rows). Hence, we conclude det(B) = −2. 2. a) This is a solution for a) that uses the decomposition in the hint. Further below, we provide an alternative solution that does not use the hint. Observe that, as suggested in the hint, we can decompose M as M = [A B 0 C ] = [I B 0 C ] [ A 0 0 I ] where we used identity matrices I and zero matrices 0 of the appropriate dimensions. By Proposition 6.0.12, we have det(M ) = det([ I B 0 C ])det( [A 0 0 I ] ). Consider first the matrix M ′ := [ I B 0 C ]. Observe that there is a unique non-zero entry in each of its first m columns. Thus, every permutation σ that contributes to the determinant of M ′ in the formula detM ′ = ∑ σ∈Πn sign(σ) n∏ i=1 M ′ i,σ(i) must select these non-zero entries, i.e. σ(i) = i for all i ∈ [m]. The formula then simplyfies to detM ′ = ∑ σ∈Πn−m sign(σ) n−m∏ i=1 Ci,σ(i) = detC as the non-zero entries in the first m columns (or rows) are all one. Analogously, we get det [ A 0 0 I ] = ∑ σ∈Πm sign(σ) m∏ i=1 Ai,σ(i) = detA and thus we conclude detM = det(A)det(C). a’) Here is a solution that ignores the decomposition in the hint: We start by using the definition of the determinant for M , i.e. we have detM = ∑ σ∈Πn sign(σ) n∏ i=1 Mi,σ(i) where Πn is the set of all permutations on n elements. The key observation for this exercise is that only those permutations σ ∈ Πn that satisfy σ(1), . . . , σ(m) ∈ {1, . . . , m} will contribute to this sum. To see this, let σ ∈ Πn be a permutation with σ(i) > m for some i ∈ [m]. By the pigeonhole principle, there must exist j ∈ [n] \\ [m] with σ(j) ∈ [m]. But by the shape of M , we must have Mj,σ(j) = 0 and hence the contribution of σ to the sum is 0. In particular, the relevant (those that contribute non-zero terms to the sum) permutations σ ∈ Πn satisfy σ(i) ∈ [m] for all i ∈ [m] and σ(j) ∈ [n] \\ [m] for all j ∈ [n] \\ [m]. In 2 other words, restricting such a permutation σ to [m] yields a permutation on m elements, and restricting σ to [n] \\ [m] yields a permutation on n − m elements. Conversely, any two permutations σ1 ∈ Πm and σ2 ∈ Πn−m yield a permutation σ ∈ Πn that contributes to the sum (define σ(i) = σ1(i) for i ∈ [m] and σ(j) = m + σ2(j − m) for j ∈ [n] \\ [m]). Observe that the number of inversions in σ is exactly the number of inversions in σ1 plus the number of inversions in σ2. Hence, we always have sign(σ) = sign(σ1)sign(σ2) in this correspondence. We conclude that we can rewrite the sum as detM = ∑ σ∈Πn sign(σ) n∏ i=1 Mi,σ(i) = ∑ σ1∈Πm ∑ σ2∈Πn−m sign(σ1)sign(σ2) m∏ i=1 Mi,σ1(i) n∏ j=m+1 Mj, j+σ2(j−m). Next, observe that the terms Mi,σ1(i) are always in the A-part of M , i.e. we have Mi,σ1(i) = Ai,σ1(i). Similarly, the terms Mj, j+σ2(j−m) are always in the C-part of M , i.e. we have Mj, j+σ2(j−m) = Cj−m,σ2(j−m). Hence, we can further rewrite the sum as detM = ∑ σ1∈Πm ∑ σ2∈Πn−m sign(σ1)sign(σ2) m∏ i=1 Mi,σ1(i) n∏ j=m+1 Mj, j+σ2(j−m) = ∑ σ1∈Πm ∑ σ2∈Πn−m sign(σ1)sign(σ2) m∏ i=1 Ai,σ1(i) n∏ j=m+1 Cj−m, σ2(j−m) = ∑ σ1∈Πm sign(σ1) m∏ i=1 Ai,σ1(i)   ∑ σ2∈Πn−m sign(σ2) n∏ j=m+1 Cj−m, σ2(j−m)   =   ∑ σ1∈Πm sign(σ1) m∏ i=1 Ai,σ1(i)     ∑ σ2∈Πn−m sign(σ2) n∏ j=m+1 Cj−m, σ2(j−m)   =   ∑ σ1∈Πm sign(σ1) m∏ i=1 Ai,σ1(i)     ∑ σ2∈Πn−m sign(σ2) n−m∏ j=1 Cj, σ2(j)   = det(A)det(C) which concludes the proof. b) In order to calculate the determinant of M using the previous result, we must first bring it into the right form. Clearly, M already contains a lot of zero entries. In the end, we want to have a block of zeroes in the bottom left corner. We can use that transposing the matrix does not change its determinant. Moreover, by Proposition 6.0.21, swapping two rows of a matrix negates its determinant. Hence we proceed as follows: we first transpose M and then swap the second row and fourth row, as well as the third and sixth row of the resulting matrix. In this way, we obtain the matrix M ′ =         2 9 1 3 2 8 4 0 0 5 5 3 7 4 0 7 2 1 0 0 0 2 3 8 0 0 0 0 0 2 0 0 0 0 1 7         . Using the result from the previous subtask and some more row swaps as well as the formula 3 for the determinant of triangular matrices, we get detM = (−1) 2detM ′ = detM ′ = ∣ ∣ ∣ ∣ ∣ ∣ 2 9 1 4 0 0 7 4 0 ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ 2 3 8 0 0 2 0 1 7 ∣ ∣ ∣ ∣ ∣ ∣ = (−1) 2 ∣ ∣ ∣ ∣ ∣ ∣ 4 0 0 7 4 0 2 9 1 ∣ ∣ ∣ ∣ ∣ ∣ (−1) ∣ ∣ ∣ ∣ ∣ ∣ 2 3 8 0 1 7 0 0 2 ∣ ∣ ∣ ∣ ∣ ∣ = −16 · 4 = −64. 3. a) Using the rules we learned in the lecture, we calculate u + v + w = (u + v) + w = (4 + 2i) + (3 − 4i) = (4 + 3) + (2 − 4)i = 7 − 2i u · v = (3 + i) · (1 + i) = 3 + 3i + i − 1 = 2 + 4i v · w · i = (1 + i) · (3 − 4i) · i = (3 − 4i + 3i + 4) · i = 3i + 4 − 3 + 4i = 1 + 7i w v = w v · v v = (3 − 4i)(1 − i) (1 + i)(1 − i) = 3 − 3i − 4i − 4 1 + 1 = − 1 2 − 7 2 i v u = v u · u u = (1 + i)(3 − i) (3 + i)(3 − i) = 3 − i + 3i + 1 9 + 1 = 2 5 + 1 5 i |v| = √ 12 + 12 = √2. 4. In this exercise we want to exploit Proposition 6.0.22 which says that the determinant is linear in each row. In particular, using this on the second row of A and B, we get det(A) − det(B) = det  |v⊤ 1||u⊤ 1|M   − det  |v⊤ 1||u⊤ 2|M   = det  |v⊤ 1||(u1 − u2)⊤|M   . Analogously, we can use it on the second row of C and D to get det(C) − det(D) = det  |v⊤ 2||u⊤ 1|M   − det  |v⊤ 2||u⊤ 2|M   = det  |v⊤ 2||(u1 − u2)⊤|M   . Finally, using linearity in the first row of those two resulting matrices yields det  |v⊤ 1||(u1 − u2)⊤|M   − det  |v⊤ 2||(u1 − u2)⊤|M   = det  |(v1 − v2)⊤||(u1 − u2)⊤|M   and thus det(A) − det(B) − det(C) + det(D) = det(E). 5. a) Let λ ∈ R be an arbitrary real eigenvalue of M with corresponding real eigenvector v ∈ Rn, i.e. we have M v = λv. Now let’s see what happens to v if we apply M + cI instead of M to it: (M + cI)v = M v + cv = λv + cv = (λ + c)v. 4 As we have observed, v is a real eigenvector of M + cI with corresponding real eigenvalue c + λ. This is exactly what we wanted to prove. b) Consider the matrix B =         1 3 5 7 9 11 1 3 5 7 9 11 1 3 5 7 9 11 1 3 5 7 9 11 1 3 5 7 9 11 1 3 5 7 9 11         . We observe that A = B + 2I. Hence, our plan is to find two distinct real eigenvalues of B, and then use the result from the previous subtask. Since all rows of B are equal, the matrix has rank 1. Thus, 0 is an eigenvalue of B. It remains to find another real eigenvalue. For this, let us try to guess a real eigenvector of B that does not correspond to eigenvalue 0. This is not as hard as it may sound: every row of B is the same, hence any eigenvector of B that does not correspond to eigenvalue 0 should have the same value in each coordinate. Indeed, we have B         1 1 1 1 1 1         = 36         1 1 1 1 1 1         . Therefore, the vector 1 = [1 1 1 1 1 1]⊤ is an eigenvector of B with corresponding eigenvalue 36. By the result from the previous subtask, it follows that λ1 = 2 and λ2 = 38 are two distinct real eigenvalues of A. 6. a) Let x denote the vector of x-coordinates x = [ px,1 . . . px,n]⊤ and let y denote the vector of y-coordinates y = [py,1 . . . py,n]⊤. The smoothness property can be rewritten as pj − 1 2 (pj−1 + pj+1) = 0 ∀ j ∈ {2, . . . , n − 1} p1 − 1 2 (pn + p2) = 0 pn − 1 2 (pn−1 + p1) = 0 which translates to Ax = 0 and Ay = 0 with A =           1 − 1 2 0 · · · 0 − 1 2 − 1 2 1 − 1 2 · · · 0 0 0 . . . . . . . . . 0 0 ... . . . . . . . . . . . . ... 0 · · · 0 − 1 2 1 − 1 2 − 1 2 0 · · · 0 − 1 2 1           . The matrix A can also be written as A = I − 1 2 (T + E1,n) − 1 2 (T + E1,n) ⊤ 5 where T is the matrix with ones on the first strict upper diagonal (i.e. the entries where the row coefficient i and column coefficient j satisfy j = i + 1 for i ∈ {1, ..., n − 1}) and zeroes everywhere else, and E1,n has a single non-zero entry in row 1 and column n that is equal to 1. We also want to satisfy the constraints pjs = cs for all s ∈ [k]. Let xc denote the vector of x-coordinates of the locations, i.e. xc = [ cx,1 . . . cx,k]⊤ and let yc denote the vector of y-coordinates yc = [ cy,1 . . . cy,n]⊤. Then, the location constraints can be written as Bx = xc and By = yc where the matrix B ∈ Rk×n is given by Bs,r = δr,js for all s ∈ [k] and r ∈ [n] (recall that the Kronecker-Delta δr,js is one if r = js and zero otherwise). In other words, an entry Bs,r is one whenever the vertex pr should match location cs according to the prescribed correspondence C, and Bs,r is zero otherwise. The final systems of linear equations hence are [A B ] x = [0n xc ] and [A B ] y = [ 0n yc ] where 0n denotes the n dimensional all-zero vector. b) Let S = [A B ] denote the system matrix. Indeed, the system matrix is the same for both linear systems. Since A ∈ Rn×n and B ∈ Rk×n, the system matrix S is in R(n+k)×n. This implies that S has rank at most n. c) We are solving for the curve vertex positions in the least squares sense for the values n = 6, k = 3, C = {j1 = 1, j2 = 3, j3 = 5} and c1 = [cx,1 cy,1 ] = [2 2 ] c2 = [cx,2 cy,2 ] = [6 2 ] c3 = [cx,3 cy,3 ] = [4 0 ] . Our strategy is to first combine the two linear systems in one larger system and then solve this using the least squares method. Observe that the two systems [A B ] x = [0n xc ] and [A B ] y = [ 0n yc ] can be rewritten as M [x y ] =     A 0n,n B 0k,n 0n,n A 0k,n B     [x y ] =     0n xc 0n yc     where M is a 2(n + k) × 2n matrix block matrix (meaning that we put it together from smaller matrices) and 0n,n and 0k,n are zero-matrices of corresponding dimensions. The normal equations hence yield M ⊤M [x y ] = M ⊤     0n xc 0n yc     . 6 Plugging in the values of this specific example for A, B, xc, and yc, we get S = [A B ] =                1 −1/2 0 0 0 −1/2 −1/2 1 −1/2 0 0 0 0 −1/2 1 −1/2 0 0 0 0 −1/2 1 −1/2 0 0 0 0 −1/2 1 −1/2 −1/2 0 0 0 −1/2 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0                , [ 0 xc ] =                0 0 0 0 0 0 2 6 4                , [ 0 yc ] =                0 0 0 0 0 0 2 2 0                . The exact final solution is (obtained by solving the normal equations with a computer) x = [76/29 4 156/29 148/29 4 84/29 ]⊤ and y = [52/29 60/29 52/29 28/29 12/29 28/29 ]⊤ . A drawing of this solution is provided in Figure 1 below. Figure 1: A drawing of the solution. 7","libVersion":"0.5.0","langs":""}