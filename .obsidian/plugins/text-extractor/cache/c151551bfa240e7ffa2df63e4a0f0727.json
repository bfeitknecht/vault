{"path":"sem3/LinAlg/VRL/LinAlg-script-2.pdf","text":"Linear Algebra for Computer Scientists Lecture Notes Part II A. S. Bandeira and R. Weismantel ETH Z¨urich Last update on November 1, 2024 1 2 “READ ME” FOR PART II These lecture notes serve as a continuation1 of Part I, taught by Prof. Bernd G¨artner, available at https://ti.inf.ethz.ch/ew/courses/LA23/notes_part_I.pdf. Please read the Preface there. Please note there may be some changes in notation. Furthermore, we will try to stay close to the notation in [Str23], but there also be some differences. [Str23] Gilbert Strang. Introduction to Linear Algebra. Wellesley - Cambridge Press, 6th ed., 2023. The course page has relevant information for the course: https://ti.inf.ethz.ch/ew/ courses/LA23. There are countless excellent Linear Algebra books with the material covered in this course. For Part II we will roughly continue to follow, in structure and content, [Str23], with some small devi- ations. I will try to keep the numbering of Chapters/Sections and Sections/Subsections consistent with [Str23] (as far as the deviations allow). See Appendix A for some important preliminaries and some remarks on notation. Throughout the notes, and the lectures, we will try to motivate some of the material with Guid- ing Questions. For students who would like to explore the topic further, I will include some Exploratory Challenges and Further Reading, these often will include difficult problems or topics. I will also take some opportunities to share some active Research Questions related to the topics we covered (we are still discovering new phenomena in Linear Algebra today and for many years to come!). After deriving a result, we will often do some Sanity Checks, and some things we will leave as a Challenge: these should be accessible and of difficulty comparable to homework questions, a ⋆ indicates a harder problem (but still within the scope). On the other hand, Exploratory Challenges are generally outside the scope of the course or of substantial higher difficulty. Linear Algebra is a beautiful topic, connecting Algebra with Geometry2, and has countless applications making it a key component of almost all quantitative pursuits. We sincerely hope you will enjoy the course as much as we enjoy teaching this beautiful subject! 1If you are reading these notes and did not follow Part I, please read Appendix A. 2and Analysis, as you will likely see later in your academic life. For example, when Joseph Fourier invented Fourier Series to develop a theory of heat transfer he was essentially finding good orthonormal bases for functions. 3 We believe the Questions, Sanity Checks, Challenges, etc are very useful to learn the material, but when you want to review the material, or do a last read before the exam, you can focus on the Definitions, Propositions, Theorems, etc (and focus less on the blue parts). As your mathematical level matures over the semester, the notes will have less illustrations and more definitions and mathematical statements. Our recommendation is to read the notes with pen & paper next to you and to draw the picture yourself, this “translation” you will be doing — from mathematical statement to picture — will help you greatly in the learning of Mathematics! There are also countless high-quality videos and other content online about Linear Algebra, for example there is also an excellent series of videos by Gil Strang filmed ∼15 years ago: https: //www.youtube.com/playlist?list=PLE7DDD91010BC51F8. Strang actually retired just a few months ago, at almost 90 years of age! You can see his last lecture online: https://www.youtube.com/watch?v=lUUte2o2Sn8 Moreover, there are many excellent animations online giving lots of great intuition on several Linear Algebra topics and phenomena. While it is a great idea to take advantage of this, I would recommend first trying yourself to develop an intuition of the concept/phenomenon (e.g. by drawing a picture) and using these tools only after — use them to improve your intuition, not to create it! As these Lecture Notes are being continuously updated, and sometimes the discussion in lectures leads us into proving an extra result, or suggests a remark, etc, I will try to add then and not change the numbering of things downstream, I do this by numbering them with +1000. After each lecture, we post the handwritten notes from lecture on the course website https: //ti.inf.ethz.ch/ew/courses/LA23/index.html. My suggestion would be to use the Lecture Notes to review the material, not the handwritten notes (which are mainly meant to support my oral exposition). 4 CONTENTS “Read me” for Part II 2 5. Orthogonality 5 5.1. Orthogonality of vectors and subspaces 5 5.2. Projections 9 5.3. Least Squares Approximation 13 5.4. Orthonormal Bases and Gram Schmidt 17 5.5. The Pseudoinverse, also known as Moore–Penrose Inverse 23 5.6. Projections of sets and the Farkas lemma 27 6. The determinant 32 7. Eigenvalues and Eigenvectors 41 7.0. Complex Numbers 42 7.1. Introduction to Eigenvalues and Eigenvectors 45 7.2. Diagonalizing a Matrix and Change of Basis of a Linear Transformation 56 7.3. Symmetric Matrices and the Spectral Theorem 59 8. Singular Value Decomposition; and some open questions in Linear Algebra 65 8.1. The Singular Value Decomposition 65 8.2. Vector and Matrix Norms 69 8.3. Some Mathematical Open Problems 70 Acknowledgements 72 Appendix A. Some Important Preliminaries and Remarks on Notation 72 Appendix B. A “Simple proof” of the Fundamental Theorem of Algebra 73 73 5 5. ORTHOGONALITY 5.1. Orthogonality of vectors and subspaces. Guiding Question 1. Our task is to explore orthogonality as a geometric and algebraic tool in order to be able to decompose a space into subspaces. Among many results our knowledge will then put us in position to understand how to solve systems of linear equations. Let us begin by introducing orthogonality. Definition 5.1.1. Two vectors v, w ∈ Rn are called orthogonal if vT w = ∑ n i=1 viwi = 0. Two sub- spaces V and W are orthogonal if for all v ∈ V and w ∈ W , the vectors v and w are orthogonal. In order to check whether two subspaces V and W are orthogonal it is enough to verify it for the vectors forming a basis of V and W , respectively. Lemma 5.1.2. Let v1, . . . , vk be a basis of subspace V . Let w1, . . . , wl be a basis of subspace W . V and W are orthogonal if and only if vi and w j are orthogonal for all i ∈ {1, . . . , k} and j ∈ {1, . . . , l}. Proof. Let us begin by proving the statement from left to right: Suppose V and W are orthogonal. Since vi ∈ V for all i ∈ {1, . . . , k} and w j ∈ W for all j ∈ {1, . . . , l}, we have that vT i w j = 0 for all i ∈ {1, . . . , k} and j ∈ {1, . . . , l}. For the converse direction, assume that vT i w j = 0 for all i ∈ {1, . . . , k} and j ∈ {1, . . . , l}. Let v ∈ V and w ∈ W . Then, there exist real multipliers such that v = k ∑ i=1 λivi and w = l ∑ j=1 µ jv j. Then vT w = k ∑ i=1 λiv T i w = k ∑ i=1 l ∑ j=1 µ jλiv T i w j = 0. □ Lemma 5.1.3. Let V and W be two orthogonal subspaces of Rn. Let v1, . . . , vk be a basis of subspace V . Let w1, . . . , wl be a basis of subspace W . The set of vectors {v1, . . . , vk, w1, . . . , wl} are linearly independent. 6 Proof. Consider the linear combination k ∑ i=1 λivi + l ∑ j=1 µ jw j = 0. We want to show that λi = 0 for all i ∈ {1, . . . , k} and µ j = 0 for all j ∈ {1, . . . , l}. Let v = ∑ k i=1 λivi. The linear combination is equivalent to v = − ∑ l j=1 µ jw j. We obtain vT v = − ∑ l j=1 µ jvT w j = 0. Hence, v = 0. This implies that λi = 0 for all i ∈ {1, . . . , k} since v1, . . . , vk is a basis of V . The same argument can be applied to show that w = ∑ l j=1 µ jw j must equal the all-zero vector and hence, µ j = 0 for all j ∈ {1, . . . , l}. This is the result. □ Lemma 5.1.3 allows us to derive an important fact about orthogonal subspaces. Namely we can take bases of the two subspaces V and W and their union gives a basis for the subspace {λ v + µw | λ , µ ∈ R, v ∈ V, w ∈ W }. Corollary 5.1.4. Let V and W be orthogonal subspaces. Then V ∩ W = {0}. Moreover, if dim(V ) = k and dim(W ) = l, then dim({λ v + µw | λ , µ ∈ R, v ∈ V, w ∈ W }) = k + l ≤ n. 5.1.1. The orthogonal complement of a subspace. So far we have explored general subspaces V and W that are orthogonal. Next consider a subspace V . Then there is a special orthogonal subspace attached to V . Definition 5.1.5. Let V be a subspace of Rn. We define the orthogonal complement of V as V ⊥ = {w ∈ R n | w T v = 0 for all v ∈ V }. Challenge 2. Prove that V ⊥ is a subspace of Rn. This concept of orthogonal subspaces allows us to decompose the space. Let us first see an important example of how this idea can be used. Theorem 5.1.6. Let A ∈ Rm×n be a matrix. N(A) = C(A T ) ⊥ = R(A) ⊥. Proof. Let us show that N(A) ⊆ C(AT )⊥. 7 Let x ∈ N(A). Take any b ∈ R(A). By definition, b = AT y for some y ∈ Rm. Then bT x = yT Ax = 0. Hence, x ∈ C(AT ). Conversely, we want to show that C(AT )⊥ ⊆ N(A). To this end let x ∈ C(AT )⊥. Then bT x = 0 for all b ∈ C(AT ). Take y := Ax ∈ Rm Then b := AT y ∈ C(AT ) and hence, xT b = 0. We obtain 0 = xT b = xT AT y = xT AT Ax = ∥Ax∥2. This implies that Ax = 0, i.e., x ∈ N(A). □ From the lecture in Chapter 3.5 we know already that if r = dim(R(A)), then n − r = dim(N(A). This fact together with Theorem 5.1.6 allows us to prove a decomposition theorem of the space Rn. Theorem 5.1.7. Let V,W be orthogonal subspaces of Rn. The following statements are equivalent. (i) W = V ⊥. (ii) dim(V ) + dim(W ) = n. (iii) Every u ∈ Rn can be written as u = v + w with unique vectors v ∈ V , w ∈ W . Proof. Let v1, . . . , vk be a basis of V and w1, . . . , wl a basis of W . From Lemma 5.1.2 V and W are orthogonal if and only if vT i w j = 0 for all i ∈ {1, . . . , k} and j ∈ {1, . . . , l}. (i) implies (ii): Define A ∈ Rk×n to be the matrix with row vectors v1, . . . , vk. Then V = R(A) = C(AT ). Moreover, W = V ⊥ = N(A) from Theorem 5.1.6. From our remark before dim(V ) = k and hence, dim(W ) = n − k. (ii) implies (iii): From Lemma 5.1.3 the vectors in the set {v1, . . . , vk, w1, . . . , wl} are linearly independent. Since by assumption l = n − k, this set is a basis of Rn. Hence, every vector u ∈ Rn has a unique representation in form of u = k ∑ i=1 λivi + l ∑ j=1 µ jw j, where λ1, . . . , λk, µ1, . . . , µl ∈ R. Define the unique vectors v := ∑ k i=1 λivi, w := ∑ l j=1 µ jw j. This gives the statement. (iii) implies (i): We need to show that W = V ⊥. Note that W ⊆ V ⊥ since W is orthogonal to V . To show the reverse inclusion, take any vector u ∈ V ⊥ ⊆ Rn Hence, from our assumption in (iii) 8 we know that u = v + w where v ∈ V and w ∈ W . Then 0 = uT v = v T v + v T w = v T v = ∥v∥ 2. Hence, v = 0 and it follows that u = w ∈ W . □ Indeed Theorem 5.1.7 allows us to decompose the space Rn according to a given subspace V ⊆ Rn. We write R n = V +V ⊥ = {v + w | v ∈ V, w ∈ V ⊥}. This decomposition is symmetric in the sense that we can also take the subspace V ⊥ and then write R n = V ⊥ + (V ⊥) ⊥ = V ⊥ +V. This follows from the following observation. Lemma 5.1.8. Let V be a subspace of Rn. Then V = (V ⊥)⊥. Proof. Let v1, . . . , vk be a basis of V and w1, . . . , wl a basis of V ⊥. It follows that l = n − k. From Lemma 5.1.2 we conclude that vT i w j = 0 for all indices i and j. Then by definition (V ⊥) ⊥ = {x ∈ R n | xT w j = 0 for all j = 1, . . . , n − k}. Since vT i w j = 0 for all j = 1, . . . , n − k we obtain that V ⊆ (V ⊥)⊥. From Theorem 5.1.7 we obtain that dim((V ⊥)⊥) = n − (n − k) = k. Since {v1, . . . , vk} ⊆ V ⊆ (V ⊥)⊥ are linearly independent, they form a basis of (V ⊥)⊥. Hence V = (V ⊥)⊥. □ This lemma in combination with Theorem 5.1.6 allows us to conclude that for a matrix A we have that C(AT ) = N(A)⊥. Corollary 5.1.9. Let A ∈ Rm×n. N(A) = C(AT )⊥ and C(AT ) = N(A)⊥. 5.1.2. The set of all solutions to a system of linear equations. The machinery developed in the previous chapters allows us to understand the set of solutions to a system of linear equations over the reals. To make it precise, let A ∈ Rm×n. There are two important subspaces associated with A: 9 N(A) = {x ∈ R n | Ax = 0} and R(A) = C(A T ) = {x ∈ R n | ∃y ∈ R m such that x = A T y}. We have learned that N(A) is the orthogonal complement of R(A). Vice versa, R(A) is the or- thogonal complement of N(A). Hence all of Rn can be written as the sum of two elements: one is from N(A) and the other one from R(A). In other words: ∀x ∈ R n there exist x0 ∈ N(A) and x1 ∈ R(A) such that x = x0 + x1 and xT 1 x0 = 0. We summarize below how the set of all solutions to a system of equations is described. Theorem 5.1.10. {x ∈ R n | Ax = b} = x1 + N(A) where x1 ∈ R(A) such that Ax1 = b. There is one final link between the nullspace of a matrix A and the nullspace of the matrix AT A that we will need in our analysis of projections later on. Lemma 5.1.11. Let A ∈ Rm×n. Then N(A) = N(AT A) and C(AT ) = C(AT A). Proof. If x ∈ N(A) then Ax = 0 and so A⊤Ax = 0, thus x ∈ N(A⊤A). The other implication is more interesting. If x ∈ N(A⊤A) then A⊤Ax = 0. This implies that x⊤A⊤Ax = x⊤0 = 0. But x⊤A⊤Ax = (Ax)⊤(Ax) = ∥Ax∥2 so Ax must be a vector with norm 0 which implies that Ax = 0 and so x ∈ N(A). For the second statement we utilize Corollary 5.1.9. We have C(A T ) = N(A) ⊥ = N(AT A) ⊥ = C((AT A) T ) = C(AT A). 2 5.2. Projections. Guiding Question 3. If we have a system of linear equations that has no solution, how do we find the “solution” that has the smallest error? This question is central in countless applications 3. 3as you will see later on, it is in a sense what Machine Learning is all about. 10 Before diving into systems of equations, we will study Projections of vectors in a subspace. Definition 5.2.1 (Projection of a vector onto a subspace). The projection of a vector b ∈ Rm on a subspace S (of Rm) is the point in S that is closest to b. In other words (1) projS(b) = argmin p∈S ∥b − p∥. Sanity Check 4. This is only a proper definition if the minimum exists and is unique. Can you show it exists and is unique? (perhaps at the end of the lecture?) Let us build us some intuition by starting with one-dimensional subspaces. 5.2.1. The one-dimensional case. Let S be the subspace corresponding to the line that goes through the vector a ∈ Rm \\ {0}, i.e. S = {λ a | λ ∈ R} = C(a). By drawing a two dimensional example one can see that the projection p is the vector in the subspace S such that the “error vector” e = b − p is perpendicular to a (i.e. b − p ⊥ a). This geometric intuition turns out to be correct. We will verify it algebraically. FIGURE 1. Projection on a line. Lemma 5.2.2. Let a ∈ Rm \\ {0}. The projection of b ∈ Rm on S = {λ a | λ ∈ R} = C(a) is given by projS(b) = aaT aT ab. 11 Proof. Let p ∈ S, p = λ a for λ ∈ R. Then ∥b − p∥2 = (b − p) T (b − p) = bT b − 2b T p + pT p = ∥b∥2 − 2λ b T a + λ 2∥a∥2 = g(λ ). g is a convex, quadratic function in one variable λ . Hence, the minimizer is obtained at the point λ ∗ where the derivative vanishes. We obtain g′(λ ) = −2bT a + 2λ ∥a∥2 = 0 ⇐⇒ λ ∗ = bT a aT a. Hence, we have shown that projS(b) = λ ∗a = abT a aT a = aaT b aT a = aaT aT ab. □ □ Let us next verify that our initial geometric understanding is indeed correct: the projection p should be the vector in the subspace S such that the “error vector” e = b − p is perpendicular to a, i.e., (b − projS(b)) ⊥ projS(b). Indeed by substituting what we just computed we obtain (b − aaT aT ab) T aaT aT ab = bT aaT b aT a − b T aaT aT a aaT aT ab = (aT b)2 aT a − bT aaT b aT a = 0. The projection of a vector that is already a multiple of a should be the identity operation. This is indeed true. Check that this is the case! 5.2.2. The general case. For general subspaces the idea is precisely the same as with dimension one. Let S be a subspace in Rm with dimension n. Let a1, . . . , an be a basis of S, meaning that S = span(a1, . . . , an) = C(A) = {Aλ | λ ∈ R n}, where A is the matrix with column vectors a1, . . . , an. Lemma 5.2.3. The projection of a vector b ∈ Rm to the subspace S = C(A) can be written as projS(b) = A ˆx, where ˆx satisfies the normal equations A T A ˆx = A T b. 12 Proof. Let b ∈ Rm. The vector b can be written as b = p + e where p ∈ S and e ∈ S⊥, i.e., pT e = 0. Now consider another point p′ ∈ S. Then p − p′ ∈ S and hence, eT (p − p′) = 0. This gives ∥p′ − b∥2 = ∥p′ − p + p − b∥2 = ∥p′ − p − e∥2 = ∥p ′ − p∥2 + ∥e∥ 2 ≥ ∥e∥ 2 = ∥p − b∥2. Hence, we have shown that projS(b) = p = A ˆx ∈ S where b = p + e with e ∈ S⊥. This shows us that (b − projS(b)) ⊥ ai for all i = 1, . . . , n ⇐⇒ aT i (b − projS(b)) = 0 for all i = 1, . . . , n. This is equivalent to saying that A T (b − projS(b)) = 0 ⇐⇒ A T (b − A ˆx) = 0 ⇐⇒ A T A ˆx = AT b. □ □ If we can show that A⊤A is invertible then we would have p = A ˆx = A ( A⊤A )−1 A⊤b. Let’s make a detour to show that it is indeed invertible. Lemma 5.2.4. A⊤A is invertible if and only if A has linearly independent columns. Proof. This follows essentially from Lemma 5.1.11 where we proved that A⊤A and A have the same nullspace. This is enough because, since A⊤A is a square matrix it is invertible if and only if its nullspace only has the 0 vector, and A has linearly independent columns if and only if its nullspace only has the 0 vector. 4 2 Corollary 5.2.5. If A has linearly independent columns then A⊤A is a square matrix, it is invertible and symmetric. 5 Now back to deriving a formula for projections: Since the columns of A are a basis they are linearly independent and so A⊤A is indeed invertible. We just proved the following. Theorem 5.2.6. Let S be a subspace in Rm and A a matrix whose columns are a basis of S. The projection of b ∈ Rm to S is given by projS(b) = Pb, 4We usually call a nullspace with only the zero vector, a trivial nullspace. 13 where P = A ( A⊤A)−1 A⊤ is the projection matrix. The matrix P = A ( A⊤A)−1 A⊤ is known as a Projection Matrix, it maps a vector b to its projection Pb on a subspace S. For the case of lines, P was given by P = aa⊤ a⊤a = a 1 a⊤a a⊤. Caution! 5. The matrix A (and A⊤) are not necessarily square, and so they don’t have inverses. The expression A ( A⊤A)−1 A⊤ cannot be simplified by expanding ( A⊤A)−1 (which would yield I = P, this would only make sense if S was all of Rm and note that, unsurprisingly, this would correspond exactly to the case when A is invertible). Let us summarize a few facts about the projection matrix P. P can be viewed as a mapping: for a given vector b its projection is given by projS(b) = Pb. Remark 5.2.7. • If b ∈ Rm, then projS(projs(b)) = projS(b) by definition. This requires us to have that PPb = Pb, i.e., we should have P2 = P. Indeed P 2 = ( A ( A⊤A )−1 A ⊤)2 = A ( A⊤A )−1 A ⊤A ( A⊤A)−1 A⊤ = A ( A⊤A )−1 A ⊤ = P. • Let S⊥ be the orthogonal complement of S and P the projection matrix onto the subspace S, i.e., projS(b) = Pb. Then I − P is the projection matrix that maps b ∈ Rm to projS⊥(b). This follows since b = e + projS(b) = e + Pb where e ∈ S⊥. Hence, (I − P)b = b − Pb = e = projS⊥(b). • Note that – as it should be – we have that (I − P)2 = I − 2P + P2 = I − P. 5.3. Least Squares Approximation. We go back to the guiding question of what to do when we want to “solve” a linear system that does not have an exact solution. More precisely let us suppose we have a linear system Ax = b, for which no solution x exists (for example, with too many equations, which would happen if A ∈ Rm×n and m > n). A natural approach is to try to find x for which Ax is as close as possible to b (2) min ˆx∈Rn ∥A ˆx − b∥ 2. 14 Further Remark 6. This seemingly simple observation is key to countless technologies. Mea- surement systems often have errors and so it is impossible to find the target object/signal x that satisfies them all exactly, and we look for the one that satisfies them the best possible. In Data Science and Learning Theory we often want to find a predictor that best describes a set of training data, but usually no predictor described the data exactly, so we look for the best possible, etc etc. We’ll see a couple of applications later. We can solve this problem using the ideas we developed above. What we are looking for is a vector ˆx for which the error e = b − A ˆx is as small as possible. Since the set of possible vectors y = A ˆx is exactly C(A), A ˆx is precisely the projection of b on C(A). As we saw in the Section above, this means that A ⊤(b − A ˆx) = 0. These are known as the normal equations and can be rewritten as (3) A⊤A ˆx = A ⊤b. Recall that we had shown in Lemma 5.1.11 that for any matrix A, C(A⊤) = C(A⊤A). Hence, the system (3) always has a solution. We also know that if A has linearly independent columns, then A⊤A is invertible and so we can write ˆx = (A⊤A)−1A⊤b. We will address the case in which A has dependent columns shortly. Fact 5.3.1. A minimizer of (2) is also a solution of (3). When A has independent columns the unique minimizer ˆx of (2) is given by (4) ˆx = (A⊤A) −1A⊤b 5.3.1. Linear Regression — fitting a line to data points. One of the most common tasks in data analysis is linear regression, to fit a line through data points. Let us consider data points (t1, b1), (t2, b2), . . . , (tm, bm), perhaps representing some attribute b over time t. If the relation between t and b is (at least partly) explained by a linear relationship then it makes sense to search for constants α0 ∈ R and α1 ∈ R such that bk ≈ α0 + α1tk. 15 FIGURE 2. Fitting a line to points See Figure 2. In particular, it is natural to search for α0 and α1 that minimize the sum of squares of the errors (“least squares”), min α0,α1 m ∑ k=1 (bk − [α0 + α1tk]) 2 . In matrix-vector notation (5) min α0,α1 ∥ ∥ ∥ ∥ ∥ b − A [ α0 α1 ]∥ ∥ ∥ ∥ ∥ 2 , where b =         b1 b2 ... bm−1 bm         and A =         1 t1 1 t2 ... ... 1 tm−1 1 tm         . We can assume w.l.o.g. that A has independent columns, see Lemma 5.3.2. Hence, the solution to (5) is given by [ α0 α1 ] = (A⊤A) −1A⊤b = [ m ∑ m k=1 tk ∑ m k=1 tk ∑ m k=1 t2 k ]−1 [ ∑ m k=1 bk ∑ m k=1 tkbk ] 16 Lemma 5.3.2. The columns of the m × 2 matrix A defined before are linearly dependent if and only if ti = t j for all i ̸= j. Proof. Suppose that there are two indices i ̸= j such that ti ̸= t j. Let 1 be the all ones-vector in Rm and t the vector with components t1, . . . ,tm. Consider the system in variables λ , µ λ 1 + µt = 0. Since ti ̸= t j we can subtract row j from row i to obtain λ 0 + µ(ti − t j) = 0 ⇐⇒ µ = 0 since ti − t j ̸= 0. This implies that λ = 0 and hence A has full column rank. Conversely, if ti = t j for all i and j, then t = t11. Then the two columns of A are linearly dependent. □ Remark 5.3.3. If the columns of A are pairwise orthogonal, then A⊤A is a diagonal matrix, which is easy to invert. In this example, the columns of A being orthogonal corresponds to ∑ m k=1 tk = 0. We could simply do a change of variables to a new time tnew k = tk − 1 m ∑ m i=1 ti to achieve this. If indeed ∑ m k=1 tk = 0 then the equation above could be easily simplified: [ α0 α1 ] = [ m 0 0 ∑ m k=1 t2 k ]−1 [ ∑ m k=1 bk ∑ m k=1 tkbk ] = [ 1 m 0 0 1 ∑ m k=1 t2 k ] [ ∑ m k=1 bk ∑ m k=1 tkbk ] = [ 1 m ∑ m k=1 bk (∑ m k=1 tkbk) / ( ∑ m k=1 t2 k ) ] , this is an instance where having orthogonal vectors is beneficial. In this next Section we will see how to build orthonormal basis for subspaces, and some of the many benefits they have. Challenge 7. Try to work out the actual change of variables that makes the tk’s add up to zero and derive a formula for fitting a line to points without the assumption in Remark 5.3.3 Example 5.3.4 (Fitting a Parabola). We can use Linear Algebra to do fits of many other curves (or functions), not just lines. If we believe the relationship between tk and bk is quadratic we could attempt to fit a Parabola: bk ≈ α0 + α1tk + α2t2 k . While this isn’t a linear function in tk, this is still a linear function on the coefficients α0, α1, and α2, and this is what is important. Similarly as with linear regression, it is natural to attempt to 17 minimze (6) min α0,α1,α2 ∥ ∥ ∥ ∥ ∥ ∥ ∥ b − A    α0 α1 α2    ∥ ∥ ∥ ∥ ∥ ∥ ∥ 2 , where b =         b1 b2 ... bm−1 bm         and A =         1 t1 t2 1 1 t2 t2 2 ... ... 1 tm−1 t2 m−1 1 tm t2 m         , and we can use the technology we developed in this section to solve this problem as well. Challenge 8. Try to work out the example of fitting a parabola further. What is A⊤A? When is A⊤A diagonal? Further Reading 9. There is a whole (beautiful) area of Mathematics related to studying so- called Orthogonal Polynomials. The basic idea can be already hinted at from these examples: In the example of the parabola we wrote a function of t as a linear combination of the polynomials 1, t, and t2. But we could have picked other polynomials, we could have e.g. written something like b ≈ α ′ 0 + α ′ 1(t − 2023) + α2(t2 +t), and a particularly good choice (that would depend on the distribution of the points tk) might have resulted in a diagonal matrix A⊤A... search “orthogonal polynomials” online to learn more. Further Reading 10. A lot of Machine Learning includes Linear Regression as a key component. The idea is to create, find, or learn features of the data points. Given n data points t1, . . .tn (which now can be perhaps pixel images, rather than just time points) we might want to do classification (for example, in the case of images, maybe we want a function that is large when the picture has a dog in it and small when it has a cat in it). It is hard to imagine that this can be done with a linear fit, but if we build good feature vectors ϕ(tk) ∈ Rp for very large p then the function can depend on all coordinates of ϕ(tk) (the p features) and this is incredible powerful. There are several ways to construct features, a bit over a decade ago they were sometimes handmade, now they are often learned (this is in a sense what Deep Learning does). Another important way to build (or compute with) features are the so-called Kernel Methods. 5.4. Orthonormal Bases and Gram Schmidt. When we think of (or draw) a basis of a subspace, we tend to think of (or draw) vectors that are orthogonal (have an angle of 90◦) and that have the same length (length 1). Indeed, these bases 18 have many advantages, this section is about these bases, some of their advantages, and how to find them. Definition 5.4.1 (Orthonormal vectors). Vectors q1, . . . , qn ∈ Rm are orthonormal if they are or- thogonal and have norm 1. In other words, for all i, j ∈ {1, . . . , n} qT i q j = δi j, where δi j is the Kronecker delta (7) δi j = { 0 if i ̸= j 1 if i = j. If Q is the matrix whose columns are the vectors qi’s, then the condition that the vectors are orthonormal can be rewritten as Q⊤Q = I. Caution! 11. Q may not be a square matrix, and so it is not necessarily the case that QQ⊤ = I. Example 5.4.2. A classical example of an orthonormal set of vectors is the canonical basis, e1, . . . , en ∈ Rn where ei is the vector with a 1 in the i-th entry and 0 in all other entries, i.e., (ei) j = δi j. When Q is a square matrix then Q⊤Q = I implies also that QQ⊤ = I and so Q−1 = Q⊤. We call such matrices orthogonal matrices. This corresponds to the case when the qi’s are an orthonormal basis for all of Rn. Definition 5.4.3 (Orthogonal Matrix). A square matrix Q ∈ Rn×n is an orthogonal matrix when Q⊤Q = I. In this case, QQ⊤ = I, Q−1 = Q⊤, and the columns of Q form an orthonormal basis for Rn. Example 5.4.4. The 2 × 2 matrix Q that corresponds to rotating, counterclockwise, the plane by θ , Rθ = [ cos θ − sin θ sin θ cos θ ] is an orthogonal matrix. Indeed, RT θ Rθ = [ cos θ sin θ − sin θ cos θ ] [ cos θ − sin θ sin θ cos θ ] = I. Example 5.4.5. Permutation matrices are another example of orthogonal matrices. A permuta- tion is a map π : {1, . . . , n} ↦→ {1, . . . , n} such that π(i) ̸= π( j) for i ̸= j. 19 The permutation matrix A ∈ Rn×n associated with π has entries Ai j = 1 if π(i) = j and Ai j = 0, otherwise. From this definition one can derive that AT is the permutation matrix associated with the permutation σ defined as σ ( j) = i for π(i) = j. Hence, AT A = I, i.e., A is an orthogonal matrix. Challenge 12 (⋆). Show that for every permutation matrix P there exists a positive integer k such that Pk = I. Proposition 5.4.6. Orthogonal matrices preserve norm and inner product of vectors. In other words, if Q ∈ Rn×n is orthogonal then, for all x, y ∈ Rn ∥Qx∥ = ∥x∥ and (Qx) ⊤(Qy) = x⊤y Proof. To show the second inequality note that, for x, y ∈ Rn we have that (Qx)⊤(Qy) = x⊤Q⊤Qy = x⊤Iy = x⊤y. To show the first equality note that, since for x ∈ Rn we have that ∥Qx∥ ≥ 0 and ∥x∥ ≥ 0, it suffices to show that the squares are equal and indeed ∥Qx∥2 = (Qx)⊤(Qx) = x⊤x = ∥x∥2. 2 5.4.1. Projections with Orthonormal Basis. One advantage of having access to an orthonormal basis is that projections become much simpler. The reason is easy to explain. When we discussed projections and least squares, many of the expressions we derived included A⊤A, but in the case when A has orthonormal columns, these all simplify as A⊤A = I. We collect these observations in the following proposition. Proposition 5.4.7. Let S be a subspace of Rm and q1, . . . , qn be an orthonormal basis for S. Let Q be the m × n matrix whose columns are the qi’s; Q = [ q1 , · · · , qn ] . Then the Projection Matrix that projects to S is given by QQ⊤ and the Least Squares solution to Qx = b is given by ˆx = Q⊤b. Remark 5.4.8. When Q is a square matrix then the projection QQ⊤ is simply the identity (corre- sponding to projecting to the entire ambient space Rn. Even in this seemingly trivial instance, it is useful to look closer at what this operation does: For a vector x ∈ Rn it gives x = q1 ( q⊤ 1 x) + q2 ( q⊤ 2 x) + · · · + qn ( q ⊤ n x) . 20 It is writing x as a linear combination of the orthonormal basis {qi}n i=1 (as we will see later this is sometimes referred to as a change of basis).6 5.4.2. Gram-Schmidt Process. By now we have given some evidence that orthonormal bases are useful. Fortunately, there is a relatively simple process to construct orthonormal bases, that will also suggest a new matrix factorization. The idea is simple: If we have 2 linearly independent vectors a1 and a2 which span a subspace S, it is straightforward to transform them into an orthonormal basis of S: we first normalize a1: q1 = a1 ∥a1∥ , then subtract from a2 a multiple of q1 so that it becomes orthogonal to q1, followed by a normalization step: q2 = a2 − (a⊤ 2 q1)q1∥ ∥a2 − (a⊤ 2 q1)q1∥ ∥. Let us check that indeed these vectors are orthonormal: By construction they have unit norm, and q⊤ 1 q2 = q⊤ 1 a2 − (a⊤ 2 q1)q1∥ ∥a2 − (a⊤ 2 q1)q1∥ ∥ = q⊤ 1 a2 − (a⊤ 2 q1)q⊤ 1 q1∥ ∥a2 − (a⊤ 2 q1)q1∥ ∥ = 0 ∥ ∥a2 − (a⊤ 2 q1)q1∥ ∥ = 0. Note that the denominator is not zero because a1 and a2 are linearly independent; and that, since q1 has unit norm, (a⊤ 2 q1)q1 = projSpan(q1)(a2). For more vectors, the idea is to apply this process recursively, by removing from a vector ak+1 the projection of it on the subspace spanned by the k vectors before it. More formally: Algorithm 5.4.9. [Gram-Schmidt Process] Given n linearly independent vectors a1, . . . , an that span a subspace S, the Gram-Schmidt process constructs q1, . . . qn in the following way: • q1 = a1 ∥a1∥ . • For k = 2, . . . , n set q′ k = ak − ∑ k−1 i=1 (a⊤ k qi)qi qk = q′ k ∥q′ k∥ . Theorem 5.4.10 (Correctness of Gram-Schmidt). Given n linearly independent vectors a1, . . . , an, the Gram-Schmidt process returns an orthonormal basis for the span of a1, . . . , an. 6There are countless instances in which doing this operation is beneficial, for example one of the most important algorithms, the Fast Fourier Transform, is an instance of this operation. 21 Proof. Let Sk be the subspace spanned by a1, . . . , ak. Then S = Sn. We will show, by induction, that q1, . . . , qk are an orthonormal basis for Sk. It is enough to show that they are orthonormal and are in Sk since orthonormality implies linearly independence and Sk has dimension k. For the base case, note that ∥q1∥ = 1 and q1 is a multiple of a1 and so q1 ∈ S1. Now we assume the hypothesis for i = 1, . . . k −1 and prove it for k. By the hypothesis q1, . . . , qk−1 are orthonormal, so we have to show that ∥qk∥ = 1 and that q⊤ i qk = 0 for all 1 ≤ i ≤ k − 1. • Since ak is linearly independent from the other original vectors it is not in Sk−1 and so q′ k ̸= 0. Thus ∥qk∥ = 1. • By construction ak ∈ Sk and so qk ∈ Sk. • Let 1 ≤ j ≤ k − 1. Since q1, . . . , qk−1 are orthonormal, we have q ⊤ j ( ak − k−1 ∑ i=1(a ⊤ k qi)qi ) = q⊤ j ak − k−1 ∑ i=1(a⊤ k qi)q ⊤ j qi = q ⊤ j ak − (a ⊤ k q j) = 0, and q⊤ j qk = 1 ∥q′ k∥ q⊤ j q′ k = 0. 2 Challenge 13. Try to do the Gram-Schmidt process for the columns of      1 2 3 0 0 4 5 6 0 0 7 8 0 0 0 9      . Is it the case that the Gram-Schmidt process of the columns of an upper triangular matrix (with non-zero diagonal elements) is always a subset of the canonical basis? Can you come up with an example of a set of vectors for which Gram-Schmidt does not output elements of the canonical basis? Gram-Schmidt actually provides us with a new matrix factorization. Let A be an m × n matrix with linearly independent columns a1, . . . , an and Q the m×n matrix whose columns are q1, . . . , qn as outputted by Algorithm 5.4.9. Let R = Q⊤A, since each qk is orthogonal to every ai for i < k we have that R is upper triangular. Q is not necessarily a square matrix, and so not necessarily invertible. But QQ⊤ is the projection on the span of the qi’s and thus also on the ai’s, this means that QQ⊤A = A and so we have that QR = QQ⊤A = A. We call A = QR the QR decomposition. 22 Definition 5.4.11 (QR decomposition). Let A be an m × n matrix with linearly independent columns. The QR decomposition is given by A = QR, where Q is an m × n matrix with orthonormal columns (they are the output of Gram Schmidt, Algorithm 5.4.9, on the columns of A) and R is an upper triangular matrix given by R = Q⊤A. It requires us to show that indeed this is a proper definition. We need to convince ourselves that R is upper triangluar. Lemma 5.4.12. The matrix R defined in Definition 5.4.11 is upper triangular and invertible. Moreover, QQT A = A and hence, A = QR is well defined. Proof. qT k qi = 0 for all i = 1, . . . k − 1. Since q1, . . . , qk−1 and a1, . . . , ak−1 span the same subspace Sk−1 we have that qT k ai = 0 for all i = 1, . . . , k − 1. Hence R = QT A is upper triangular. Moreover, QQT is the projection onto the subspace C(Q) = C(A). Hence, for every index i, projSn(ai) = ai = QQT ai. This is equivalent to QQT A = QR = A. Finally, N(A) = {0} and since A = QR, we must have that N(R) = {0}. Since R is an n by n matrix R is invertible and hence, RT as well. 2 Fact 5.4.13. The QR decomposition greatly simplifies calculations involving Projections and Least Squares. • Since C(A) = C(Q) then projections on C(A) can be done with Q which means they are given by projC(A)(b) = QQ⊤b. • The least squares solution to Ax = b denoted by ˆx is defined as a solution of the normal equations (recall (3)) A ⊤A ˆx = A ⊤b. Furthermore, A⊤A = (QR)⊤(QR) = R⊤Q⊤QR = R⊤R, and so we can write (8) R⊤R ˆx = R ⊤Q⊤b. Since R has independent columns (is full column rank) then N(R) = {0} and so we can simplify (8) to (9) R ˆx = Q ⊤b, 23 which can be efficiently solved by back-substitution since R is a triangular matrix. 5.5. The Pseudoinverse, also known as Moore–Penrose Inverse. The goal of this Section is to construct an analogue to the inverse of a matrix A for matrices that have no inverse. Such an analogue is called a pseudoinverse, or the Moore-Penrose Inverse, and we will denote it by A†. It is also commonly denoted by A+. Guiding Question 14. While not all matrices are A invertible, we saw that we can still aim to find the (or a) vector x such that Ax is as close as possible to a target vector b. Can we develop this idea to define a “pseudoinverse” for any matrix A, a matrix that is, in a sense, closest to being an inverse for A? What should “closest to being an inverse” even mean? There are (at least) three issues we need to overcome to try to define a pseudoinverse for a non- invertible matrix A: (i) For some vectors b there might not be a vector x such that Ax = b, (ii) For some vectors b there may be more than one x such that Ax = b and we would have to pick one, and (iii) even if we make such choices, it is not clear that such an operation will correspond to multiplying by a matrix A†. Let A ∈ Rm×n be an m × n matrix. There are a couple of different ways we could try to define a pseudoinverse A† for a non-invertible matrix A. Let us start by building on what we discussed in Section 5.3 (Least Squares Approximations), if the columns of A are linearly independent that it would make sense to build A† such that A†b is the Least Squares Solution ˆx = (A⊤A)−1A⊤b (the vector ˆx such that A ˆx is as close as possible to b), and so for matrices A with independent columns we will define A† = (A⊤A)−1A⊤. This motivates the following definition. Definition 5.5.1 (Pseudoinverse for matrices of full column rank). For A ∈ Rm×n with rank(A) = n we define the pseudo-inverse A† ∈ Rn×m of A as A † = (A⊤A) −1A⊤. Proposition 5.5.2. For A ∈ Rm×n with rank(A) = n, the pseudoinverse A† is a left inverse of A, meaning that A†A = I. Proof. Since rank(A) = n, A⊤A is invertible. Furthermore, A†A = (A⊤A)−1A⊤A = I. 2 24 Let us know consider the case for which the rows are linearly independent (in other words, A ∈ Rm×n is full row rank; or equivalently rank(A) = m). One natural way to define a pseudoinverse is based on the observation that A⊤ has full column rank and to define A† as (( A⊤)†)⊤ = ((( A⊤)⊤ ( A⊤))−1 ( A⊤)⊤)⊤ = (( AA ⊤)−1 A )⊤ = A⊤ ( AA⊤)−1 . Definition 5.5.3 (Pseudoinverse for matrices of full row rank). For A ∈ Rm×n with rank(A) = m we define the pseudo-inverse A† ∈ Rn×m of A as A † = A ⊤(AA ⊤) −1. Lemma 5.5.4. For A ∈ Rm×n with rank(A) = m, the pseudoinverse A† is a right inverse of A, meaning that AA† = I. Proof. Since rank(A) = m, AA⊤ is invertible. Furthermore, AA† = AA⊤(AA⊤)−1 = I. 2 Let us try to understand what A† is achieving for full row rank matrices A. Since A is full row rank, for all b ∈ Rm, there exists x ∈ Rn such that Ax = b. The issue is that there are potentially many such vectors. A natural strategy in this case is to pick, among all such vectors, the one with smallest norm. 7 In other words to solve min x∈Rn ∥x∥2(10) s.t. Ax = b, where s.t. stands for “subject to” or “such that”. If x1 and x2 are vectors such that Ax1 = Ax2 = b then x1 − x2 ∈ N(A), and conversely, if Ax = b and y ∈ N(A) then A(x + y) = b. Thus, given one vector x1 such that Ax1 = b the set of solutions to Ax = b are all vectors of the form x1 + y where y ∈ N(A). So we would like to find the minimum ∥x1 + y∥ among all vectors y ∈ N(A). Let us write x1 = ( x1 − projN(A)(x1) ) + projN(A)(x1). Since y ∈ N(A) we have that ( x1 − projN(A)(x1) ) ⊥ ( y + projN(A)(x1) ) and so, by Pythagoras, ∥x1 + y∥2 = ∥ ∥ ∥ ( x1 − projN(A)(x1) ) + projN(A)(x1) + y ∥ ∥ ∥ 2 = ∥ ∥ ∥x1 − projN(A)(x1) ∥ ∥ ∥ 2 + ∥ ∥ ∥projN(A)(x1) + y ∥ ∥ ∥ 2 , 7This idea, of picking the smallest (or simplest) solution among many possibilities goes far beyond Linear Algebra and is known as “regularization” in Statistics, Machine Learning, Signal Processing, and Image Processing, etc. It can be viewed as a mathematical version of the famous “Occam’s razor” principle in Philosophy. 25 and so picking y = − projN(A)(x1) yields the smallest norm solution. Since the vectors orthogonal to N(A) are precisely the vectors that are in C(A⊤), i.e., the row space of A. We just proved. Lemma 5.5.5. For any matrix A and a vector b ∈ C(A), the (unique) solution to (10) is given by the vector ˆx ∈ C(A⊤) that satisfies the constraint A ˆx = b. A† is precisely the matrix that maps b to a point ˆx that corresponds to a solution of (10). Proposition 5.5.6. For a full row rank matrix A, the (unique) solution to (10) is given by the vector ˆx = A†b. Proof. By using Lemma 5.5.5 we just need to show that ˆx = A†b satisfies A ˆx = b and that ˆx = A†b is in C(A⊤). Both these are easy to verify: A ˆx = AA†b = AA⊤(AA⊤)−1b = b and ˆx = A†b = A⊤ ( (AA⊤)−1b) and so ˆx ∈ C(A⊤). 2 Our next task is to define A† for all matrices, not just full rank matrices. The idea is to write A as a product of two matrices, one which is of full column rank and one which is of full row rank. Recall that in Part I of the lecture we achieved this task by introducing the CR-decomposition. For A ∈ Rm×n, with rank(A) = r, the CR decomposition writes A = CR where C ∈ Rm×r has the first r linearly independent columns of A and R ∈ Rr×n is upper triangular. Note that C has full column rank and R full row rank. Definition 5.5.7 (Pseudoinverse for all matrices). For A ∈ Rm×n with rank(A) = r and CR de- composition A = CR we define the pseudoinverse A† as A † = R†C†, which can be rewritten as A† = R ⊤ ( RR ⊤)−1 ( C⊤C)−1 C⊤ = R⊤ ( C⊤CRR ⊤)−1 C⊤ = R ⊤ ( C⊤AR ⊤)−1 C⊤. The following lemma characterizes what the matrix A† achieves for us. 26 Lemma 5.5.8. Given A ∈ Rm×n and a vector b ∈ Rn, the (unique) solution to min x∈Rn ∥x∥2(11) s.t. A⊤Ax = A⊤b, is given by ˆx = A†b. Proof. Let r be the rank of A and A = CR with C ∈ Rm×r and R ∈ Rr×n. Then ˆx = A†b = R⊤ ( C⊤AR⊤)−1 C⊤b. Thus, A⊤A ˆx = A ⊤AR ⊤ ( C⊤AR ⊤)−1 C⊤b = R ⊤C⊤AR⊤ ( C⊤AR ⊤)−1 C⊤b = R ⊤C⊤b = A⊤b. It remains to show that ˆx is the smallest norm solution. To verify this we use Lemma 5.5.5, i.e., we need to verify that ˆx ∈ C(A⊤A). From Lemma 5.1.11 we conclude that C(AT A) = C(AT ). Hence it is enough to show that ˆx ∈ C(A⊤) and since C(A⊤) = C(R⊤) we have that ˆx = R⊤ ( C⊤AR⊤)−1 C⊤b ∈ C(R⊤) from which the statement follows. 2 In this proof, the only property of the matrices CR we used is that A = CR and both C and R are full rank. So we have actually shown that we can compute the pseudoinverse from any full rank factorization, not just specifically the CR decomposition. We write it here as a proposition. Proposition 5.5.9. For A ∈ Rm×n, with rank(A) = r, and let S ∈ Rm×r and T ∈ Rr×n such that A = ST . Then, A† = T †S†. Remark 5.5.10. Note that If A = ST and rank(A) = r then rank(S) ≥ r and rank(T ) ≥ r and so the matrices ST in Proposition 5.5.9 are indeed full rank (either full column rank or full row rank). Let us finally summarize a few important properties about the matrix A and its pseudoinverse A†. Theorem 5.5.11. Let A ∈ Rm×n. (1) AA†A = A (2) A†AA† = A†. (3) AA† is symmetric. It is the projection matrix for projection on C(A), 27 (4) A†A is symmetric. It is the projection matrix for projection on C(A⊤). (5) ( A⊤)† = ( A†)⊤, Proof. (1) We calculate AA †A = CRR T (CTCRR T ) −1CTCR = CRR T (RR T ) −1(CTC) −1CTCR = CR = A. (3) To see that AA† is symmetric we calculate AA † = CRRT (RR T ) −1(CTC) −1CT = C(CTC) −1CT = ( C(CTC) −1CT )T = (AA †) T . Since the column space of A, C(A) and the column space of C coincide and since C is a basis for C(A) Theorem 5.2.6 applies and shows us that AA† = C(CTC)−1CT is the projection matrix for projecting onto C(A). □ Challenge 15. Prove Statements (2), (4) and (5) of Theorem 5.5.11. Proposition 5.5.12. Let A ∈ Rm×n be a matrix and recall that C(A) and C(A⊤) denote re- spectively its column and row spaces. When A : x → Ax is viewed as a function from C(A⊤) to C(A) it is a bijection. In other words, for all b ∈ C(A) there is one and only one x ∈ C(A⊤) such that Ax = b. Challenge 16. Prove Proposition 5.5.12 5.6. Projections of sets and the Farkas lemma. We have so far seen that a single point can be projected to a subspace by solving a least squares problem. Now we want to consider entire sets of points and understand their projections. Guiding Question 17. Suppose we are given a set of linear inequalities in Rn. How can we certify that the set is nonempty? An answer to this question is absolutely fundamental and has numerous applications in other areas of science. In order to attack this question we remark that projections are a way to reduce the dimension of the initial question about feasibility of a set in Rn to a question about feasibility of a set in smaller dimension. We will not consider here arbitrary sets, but focus on sets described by linear inequalities whose coefficients are all rational. Let us make this precise 28 Definition 5.6.1. Let A ∈ Qm×n, b ∈ Qm and P = {x ∈ Rn | Ax ≤ b}. P is called a polyhedron. Let S = {1, . . . , s}. The projection of P on the subspace Rs associated with the variables in the subset S is projS(P) := {x ∈ R s | ∃y ∈ R n−s such that (x, y) ∈ P}. . From the definition it is clear that P ̸= /0 if and only if projS(P) ̸= /0. The question though is whether the set projS(P) also has a description in the form of a finite system of linear inequalities. If so, then we can indeed reduce the question of whether P ̸= /0 to a question of the same form in smaller dimension. Let us build some intuition by analyzing the situation when we project a one-dimensional set of inequalities to dimension 0. Let a ∈ Qm, ai ̸= 0 for all i and b ∈ Qm. We consider P = {x ∈ R | ax ≤ b} ⊆ R. We first notice that we can rewrite the constrains in P as follows. Set u := min{bi ai | ai > 0}, l := max{bi ai | ai < 0}. Then P = {x ∈ R | x ≤ bi ai if ai > 0, x ≥ bi ai if ai < 0} = {x ∈ R | x ≤ u, x ≥ l}. Now we are in the position to clarify when P = /0. Proposition 5.6.2. P ̸= /0 ⇐⇒ l ≤ u ⇐⇒ 0 ≤ u − l ⇐⇒ 0 ≤ y T b for all y ≥ 0 such that y T a = 0. This idea can be applied in general. 5.6.1. Elimination of one variable. Let A ∈ Qm×n, b ∈ Qm and P = {x ∈ Rn | Ax ≤ b}. In what follows we denote the entries of the matrix A by ai j. Hence, row i gives us an inequality of the form n ∑ j=1 ai jx j ≤ bi. Let ¯x = (x1, . . . , xn−1) and define the matrix consisting of the first n − 1 columns by ¯A = [A·1 . . . A·n−1]. Consider now the algorithm that applies the following steps. 29 Step 1 Partition the indices M = {1, . . . , m} of the rows of the matrix A into three subsets M0 = {i ∈ M | ai,n = 0}, M+ = {i ∈ M | ai,n > 0} and M− = {i ∈ M | ai,n < 0}. Step 2 – For every row with index i ∈ M+ multiply the corresponding constraint by 1 ain . This gives a new representation of the i-th. row as follows xn ≤ di + f T i ¯x for i ∈ M+ where di = bi ain , fi j = − ai j ain . – Every row with index k ∈ M0 can be rewritten as 0 ≤ dk + f T k ¯x for k ∈ M0 where dk = bk, fk j = −ak j. – For every row with index i ∈ M− multiply the corresponding constraint by 1 ain . This gives a new representation of the i-th. row as follows xn ≥ di + f T i ¯x for i ∈ M− where di = bi ain , fi j = − ai j ain . Step 3 Return Q = { ¯x ∈ Rn−1 | 0 ≤ dk + f T k ¯x for all k ∈ M0, dl + f T l ¯x ≤ di + f T i ¯x for all l ∈ M−, i ∈ M+} . Theorem 5.6.3. The set Q returned in Step 3 is a polyhedron. Moreover, Q = projS(P), where S = {1, . . . , n − 1}. Proof. Q is a polyhedron, because we can define a matrix F ∈ Qk×n−1 and a right hand side vector δ such that Q = { ¯x ∈ R n−1 | F ¯x ≤ δ } . Indeed, k = |M0| + |M−||M+|. The rows of F contain all rows of A with index i ∈ M0 and the corresponding right hand side vector satisfies that δi = bi. The other rows of F are of the form ( fl − fi)T for indices l ∈ M− and i ∈ M+. The corresponding right hand side entry of δ is then δi = di − dl. Let us show next that projS(P) ⊆ Q. Take any ¯x ∈ projS(P). By definition there exists z ∈ R such that ( ¯x, z) ∈ P. Hence z satisfies the constraints presented in Step 2. In particular, dl + f T l ¯x ≤ z ≤ di + f T i ¯x for all l ∈ M, i ∈ M+. This shows that ¯x ∈ Q. 30 To see why Q ⊆ projS(P), take any ¯x ∈ Q. It follows that 0 ≤ dk + f T k ¯x for all k ∈ M0, dl + f T l ¯x ≤ di + f T i ¯x for all l ∈ M−, i ∈ M+. Let L := max{dl + f T l ¯x | l ∈ M−} and U := min{di + f T i ¯x | i ∈ M+}. Take any value z ∈ [L,U]. Then ( ¯x, z) ∈ P. Hence, ¯x ∈ projS(P). □ Our next task is to use these projections repeatedly. Lemma 5.6.4. Let A ∈ Qm×n, b ∈ Qm and P = {x ∈ Rn | Ax ≤ b}. Let S1 = {1, . . . , n − 1} and S2 = {1, . . . , n − 2}. Then projS2(P) = projS2(projS1(P)). Proof. Let z ∈ projS2(P). Hence, there exist real values (xn−1, xn) such that (z, xn−1, xn) ∈ P. In particular, there exists a value xn such that (z, xn−1, xn) ∈ P. Hence, (z, xn−1) ∈ projS1(P) and hence, z ∈ projS2(projS1(P). Conversely, take any z ∈ projS2(projS1(P)). By definition, there exists a real value xn−1 such that (z, xn−1) ∈ projS1(P). Hence there also exists a real value xn such that (z, xn−1, xn) ∈ P. Hence, z ∈ projS2(P). □ It is an inductive argument to show the following generalization of Lemma 5.6.4. This is left as an exercise. Challenge 18. Let A ∈ Qm×n, b ∈ Qm and P = {x ∈ Rn | Ax ≤ b}. Let S1 = {1, . . . , n − k} and S2 = {1, . . . , n − j} for indices 1 ≤ k < j < n. Then projS2(P) = projS2(projS1(P)). 5.6.2. A compact algebraic description of projections and the Farkas Lemma. In view of Challenge 18 we can start with a polyhedron P in dimension n and eliminate variable by variable. At the end there is no variable left and we simply reduced the question of whether P is empty or not to a logical statement that is either true or false, see Proposition 5.6.2. The question emerges how to describe this elimination process algebraically? This requires us to set up some terminology. 31 Definition 5.6.5. Let A ∈ Qm×n, b ∈ Qm and P = {x ∈ Rn | Ax ≤ b}. For k ∈ {1, . . . , j} let A( j) to be the submatrix of A with column vectors A·k. Let P(0) = P and C(0) = Rm +. Define for i ∈ {1, . . . , n} C(i) = { y ∈ R m + | y T A·k = 0 for all k = n − i + 1, . . . , n} . P(i) = { ¯x ∈ R n−i | y T A(n−i) ¯x ≤ y T b for all y ∈ C(i)} . With this notation we can now elegantly describe the projection of a polyhedron. Theorem 5.6.6. projSn−i(P) = P(i). Proof. We first verify that projSn−i(P) ⊆ P(i). Indeed, take any ¯x ∈ projSn−i(P). By definition there exists a real vector z of dimension i such that ( ¯x, z) ∈ P. Hence, ( ¯x, z) satisfies the following inequalities n−i ∑ k=1 A·k ¯xk + n ∑ k=n−i+1 A·kzk ≤ b. This implies that for all y ∈ C(i) we obtain that n−i ∑ k=1 y T A·k ¯xk + n ∑ k=n−i+1 y T A·kzk = n−i ∑ k=1 y T A·k ¯xk = y T A(n−i) ¯x ≤ y T b. Hence, ¯x ∈ P(i). For the converse direction, we apply induction. Let i = 1. We have C(1) = { y ∈ R m + | y T A·n = 0} and P (1) = { ¯x ∈ R n−1 | y T A (n−1) ¯x ≤ y T b for all y ∈ C(1)} . Our task is to show that P(1) ⊆ projSn−1(P). This follows from Theorem 5.6.3. To see this, let ei be the i-th. unit vector in Rm. Take as y the unit vector ek for k ∈ M0. Then ek ∈ C(1). Moreover, pick two indices l ∈ M− and i ∈ M+. Then − 1 aln el + 1 ain ei ∈ C(1). This gives precisely the description of Q in Theorem 3. This argument can be adapted to show the inductive step. The details are left as an exercise. □ Challenge 19. Prove that P(i) ⊆ projSn−i(P). We are now in the position to prove the famous Farkas Lemma. 32 Theorem 5.6.7 (The Farkas Lemma). Let A ∈ Qm×n, b ∈ Qm. Either there exists a vector x ∈ Rn such that Ax ≤ b or there exists a vector y ∈ Rm such that y ≥ 0, yT A = 0 and yT b < 0. Proof. We refer to the notation introduced in Definition 5.6.5. C(n) = {y ∈ R m + | y T A· j = 0 for all j = 1, . . . , n} = {y ≥ 0 | y T A = 0}. P(n) = {0 ≤ yT b for all y ∈ C(n)}. We conclude with Proposition 5.6.2 that P ̸= /0 ⇐⇒ P (1) ̸= /0 ⇐⇒ . . . ⇐⇒ P(n) ̸= /0 ⇐⇒ for all y ≥ 0 with y T A = 0 we have y T b ≥ 0. This leads to the following conclusion. Either P ̸= /0 or P = /0. This is equivalent to saying: either there exists a vector x ∈ Rn such that Ax ≤ b or there exists a vector y ∈ Rm such that y ≥ 0, yT A = 0 and yT b < 0. □ The power of Theorem 5.6.7 is that it allows us to give a nice certificate of when a polyhedron is empty. Indeed, if P ̸= /0, then we can simply find a point in P that is feasible and convince ourselves that P ̸= /0. However, when P = /0, then it is not clear how to convince someone that this is indeed the case. In the CS lenz we will see several important applications of Theorem 5.6.7. 6. THE DETERMINANT We will now introduce the notion of determinant det(A) of a square matrix A. While this has a somewhat involved definition for n × n matrices, it is useful to first discuss what the determinant geometrically corresponds to, and to focus on small matrices. In a nutshell, the determinant of a matrix is a number that corresponds to how much the associated linear transformation inflates space, it corresponds precisely to the volume (or area, in R2) of the image of the unit cube (the red square in the pictures above in R2); with a negative sign when the orientation changes (in the pictures above in R2, when the order of the colored dots, on the red square, changed). If we think about the determinant this way, then many of the properties we will list below can be intuitively understood (while it is hard to do so from the formula for the n × n determinant). For this reason, this section will be somewhat less proof-based, and rather focus on the most relevant properties of the determinant. 33 FIGURE 3. Calculation in 3Blue1Brown’s video (see Remark 6.0.1) computing the determinant of a 2 × 2 matrix as the area of the image of the unit square after a linear transformation (that does not change orientation). Remark 6.0.1. Grant Sanderson has a website https://www.3blue1brown.com/ and Youtube channel https://www.youtube.com/3blue1brown with excellent animation- heavy explanations of topics in Mathematics, including Linear Algebra. I particularly recom- mend the video on Determinants, it has also 3 dimensional visualizations that are harder to do on a static medium. You can find it here https://youtu.be/Ip3X9LOh2dk or here https://www.3blue1brown.com/lessons/determinant. See also Figure 3. A calculation of the area of the image of the unit square by left-multiplication by a 2 × 2 matrix shows (see Figure 3) that ∣ ∣ ∣ ∣ ∣ a b c d ∣ ∣ ∣ ∣ ∣ := det [ a b c d ] = ad − bc. Before we actually formally define the determinant for general n × n matrices we will first focus on the special case of 2 × 2- matrices to derive several important properties of the determinant. 6.0.1. The 2 × 2 - case. 34 Let us first understand how the determinant changes when we multiply matrices. To this end, let A = [ a c b d ] and W = [ x z y w ] . Next we multiply these two matrices and obtain an explicit representation of the coefficients AW = [ ax + cy az + cw bx + dy bz + dw ] . Using this representation we obtain the following result. Lemma 6.0.2. Let A,W ∈ R2×2. Then det(AW ) = det(A) det(W ). Proof. det(AW ) = (ax + cy)(bz + dw) − (az + cw)(bx + dy) = axbz + axdw + cybz + cydw − azbx − azdy − cwbx − cwdy = axdw + cybz − azdy − cwbx = ad(xw − zy) + cb(zy − xw) = det(A) det(W ). □ This computation allows us to derive a characterization of when a 2 × 2-matrix is invertible. Lemma 6.0.3. A matrix A ∈ R2×2 is invertible if and only if det(A) ̸= 0. Proof. Let A = [ a c b d ] . If A is invertible, then A−1 exists and hence, AA−1 = I implies together with the previous lemma that det(A) det(A−1) = 1. Hence, det(A) ̸= 0. Conversely, if det(A) ̸= 0, then a ̸= 0 or b ̸= 0. Without loss of generality we can assume that a ̸= 0. Consider now the system of linear equations AW = I. ax + cy = 1 implies that x = 1−cy a az + cw = 0 implies that z = −cw a . 35 By substituting these expressions into the other two equations bx + dy = 0 and bz + dw = 1 we obtain b a − cyb a + dy = 0 ⇐⇒ b + y(ad − bc) = 0 ⇐⇒ y = −b det(A). −bcw a + dw = 1 ⇐⇒ −bcw + adw = a ⇐⇒ w = a det(A). This then gives us a formula for the parameters z and x in form of z = −c det(A) and x = 1 + cd det(A) a = d det(A). These calculations show that A−1 exists whenever det(A) ̸= 0. □ Notice that our calculations give us an explicit formula for the inverse of matrix A and its deter- minant: (12) A−1 = 1 det(A) [ d −c −b a ] . 6.0.2. The n × n - case. It turns out that what we have verified in dimension two carries over to general dimensions. It is, however much more involved to verify it algebraically. We now give the definition of a determinant for n × n matrices. This requires us to discuss permutations. Definition 6.0.4 (Sign of a permutation). Given a permutation σ : {1, . . . , n} → {1, . . . , n} of n elements, its sign sgn(σ ) can be 1 or −1. The sign counts the parity of the number of pairs of elements that are out of order (sometimes called inversions) after applying the permutation. In other words, sgn(σ ) =    1 if |(i, j) ∈ {1, . . . , n} × {1, . . . , n} such that i < j and σ (i) > σ ( j)| is even, −1 if |(i, j) ∈ {1, . . . , n} × {1, . . . , n} such that i < j and σ (i) > σ ( j)| is odd. Example 6.0.5. Let n = 4. Consider the permutation π defined as π(1) = 1, π(2) = 3, π(3) = 2, π(4) = 4. The pairs (i, j) such that i < j are (1, 2), (1, 3), (1, 4), (2, 3), (2, 4), (3, 4). For all these listed pairs (i, j) we have that π(i) < π( j) except for the pair (2, 3). Hence, sgn(π) = −1. 36 Exploratory Challenge 20. The sign of a permutation has many nice properties. Try to prove a couple of them: (1) The sign of a permutation is multiplicative, i.e.: for two permutations σ , γ we have that sgn(σ ◦ γ) = sgn(σ )sgn(γ). (2) For all n ≥ 2, exactly half of the permutations have sign 1 and exactly half have sign −1. The identity has a sign of 1, the sign of a transposition (a permutation that only swaps two elements) is −1 and for two permutations σ , γ we have that sgn(σ ◦ γ) = sgn(σ )sgn(γ). We are now in position to introduce the general notion of a determinant of a square matrix. Definition 6.0.6. Given a square matrix A ∈ Rn×n the determinant det(A) is defined as det(A) = ∑ σ ∈Πn sgn(σ ) n ∏ i=1 Ai,σ (i), where Πn is the set of all permutations of n elements. If A is a 1 × 1 matrix, since there is only one permutation of 1 element (the permutation σ (1) = 1, which has sign 1). It follows that det(A) = A. For 2 × 2 matrices we observe that here are two permutations. Let us call σ1 the identity permu- tation (that doesn’t move any element, which has sign 1) and σ2 the permutation that swaps the two elements (which has sign −1). Hence, for a 2 × 2 matrix A with entries Ai j we have det(A) = ∑ σ ∈Π2 sgn(σ ) 2 ∏ i=1 Ai,σ (i) = (+1) 2 ∏ i=1 Ai,σ1(i) + (−1) 2 ∏ i=1 Ai,σ2(i) = A11A22 − A12A21. This corresponds precisely to. ∣ ∣ ∣ ∣ ∣ a b c d ∣ ∣ ∣ ∣ ∣ = ad − bc. Definition 6.0.6 allows us to derive a few results. Proposition 6.0.7. Given a permutation matrix P ∈ Rn×n corresponding to a permutation σ , then det(P) = sgn(σ ). We sometimes also write sgn(P). Proposition 6.0.8. Given a triangular (either upper- or lower-) matrix T ∈ Rn×n we have det(T ) = n ∏ k=1 Tkk, 37 in particular, det(I) = 1. Theorem 6.0.9. Given a matrix A ∈ Rn×n we have det(A⊤) = det(A). Proof. For a permutation σ let σ −1 denote the inverse permutation, i.e., σ (i) = j ⇐⇒ σ −1( j) = i for all i, j. From Challenge 20 it follows that sgn(σ ) = sgn(σ −1). The conclusion det(A⊤) = det(A) follows from observing ∑ σ ∈Πn sgn(σ ) n ∏ i=1 Ai,σ (i) = ∑ σ −1∈Πn sgn(σ −1) n ∏ i=1 Aσ −1(i),i = ∑ σ ∈Πn sgn(σ ) n ∏ i=1 Aσ (i),i. □ The following is a consequence of the propositions above. Proposition 6.0.10. If Q ∈ Rn×n is an orthogonal matrix then det(Q) = 1 or det(Q) = −1. Proof. By Propositions 6.0.8 and 6.0.12 we have 1 = det(I) = det(Q⊤Q) = det(Q⊤) det(Q). by Proposition 6.0.9 we have 1 = det(Q)2 and so det(Q) is 1 or -1. 2 Proposition 6.0.11. A matrix A ∈ Rn×n is invertible if and only if det(A) ̸= 0. We can also multiply matrices as we did in the 2 dimensional case and see the effect on their determinants. 38 Proposition 6.0.12. Given matrices A, B ∈ Rn×n we have det(AB) = det(A) det(B). Following the same line of argument we also have Proposition 6.0.13. Given a matrix A ∈ Rn×n such that det(A) ̸= 0, then A is invertible and det(A−1) = 1 det(A). Example 6.0.14. For 3 × 3 matrices there are 3! = 6 permutations, so there will be 6 terms. For A a 3 × 3 matrix, we can write its determinant as (where an empty entry corresponds to a zero entry) det(A) = ∣ ∣ ∣ ∣ ∣ ∣ ∣ A11 A12 A13 A21 A22 A23 A31 A32 A33 ∣ ∣ ∣ ∣ ∣ ∣ ∣ = ∣ ∣ ∣ ∣ ∣ ∣ ∣ A11 A22 A33 ∣ ∣ ∣ ∣ ∣ ∣ ∣ + ∣ ∣ ∣ ∣ ∣ ∣ ∣ A12 A21 A33 ∣ ∣ ∣ ∣ ∣ ∣ ∣ + ∣ ∣ ∣ ∣ ∣ ∣ ∣ A12 A23 A31 ∣ ∣ ∣ ∣ ∣ ∣ ∣ + ∣ ∣ ∣ ∣ ∣ ∣ ∣ A13 A22 A31 ∣ ∣ ∣ ∣ ∣ ∣ ∣ + ∣ ∣ ∣ ∣ ∣ ∣ ∣ A13 A21 A32 ∣ ∣ ∣ ∣ ∣ ∣ ∣ + ∣ ∣ ∣ ∣ ∣ ∣ ∣ A11 A23 A32 ∣ ∣ ∣ ∣ ∣ ∣ ∣ = A11A22A33 − A12A21A33 + A12A23A31 − A13A22A31 + A13A21A32 − A11A23A32. There is another convenient way of writing this determinant (13) ∣ ∣ ∣ ∣ ∣ ∣ ∣ A11 A12 A13 A21 A22 A23 A31 A32 A33 ∣ ∣ ∣ ∣ ∣ ∣ ∣ = A11 ∣ ∣ ∣ ∣ ∣ A22 A23 A32 A33 ∣ ∣ ∣ ∣ ∣ − A12 ∣ ∣ ∣ ∣ ∣ A21 A23 A31 A33 ∣ ∣ ∣ ∣ ∣ + A13 ∣ ∣ ∣ ∣ ∣ A21 A22 A31 A32 ∣ ∣ ∣ ∣ ∣ . In general, these terms are called the co-factors of A. Definition 6.0.15. Given A ∈ Rn×n, for each 1 ≤ i, j ≤ n let Ai j denote the (n − 1) × (n − 1) matrix obtained by removing row i and column j from A. Then we define the co-factors of A as Ci j = (−1) i+ j det(Ai j). 39 Just as in (13), the determinant can be written in terms of the co-factors. Proposition 6.0.16. Let A ∈ Rn×n, for any 1 ≤ i ≤ n, det(A) = n ∑ j=1 Ai jCi j. The formula we derived above for the inverse of 2 × 2 matrices (Equation 12), also has an ana- logue in n dimensions. Proposition 6.0.17. Given A ∈ Rn×n with det(A) ̸= 0 we have A −1 = 1 det(A)C⊤, where C is the n × n matrix with the co-factors of A as entries. The formula in Proposition 6.0.17 can be rewritten as AC⊤ = det(A)I. Remark 6.0.18. Computationally speaking, this is not a good way to compute the inverse, as it involves computing many determinants. Challenge 21. Verify that Proposition 6.0.17 indeed corresponds to the formula we derived for A−1 when n = 2. Exploratory Challenge 22. Try to prove Proposition 6.0.17 by showing that AC⊤ = det(A)I. Perhaps start with n = 3. You can also use Cramer’s Rule (below) to prove this. 6.0.3. Cramer’s Rule. The determinant also allows us to write a formula for the solution of the linear system of the type Ax = b when A ∈ Rn×n and det(A) ̸= 0. The idea is simple, we will illustrate it here for n = 3. 40 If    A11 A12 A13 A21 A22 A23 A31 A32 A33       x1 x2 x3    =    b1 b2 b3   , then we have    A11 A12 A13 A21 A22 A23 A31 A32 A33       x1 0 0 x2 1 0 x3 0 1    =    b1 A12 A13 b2 A22 A23 b3 A32 A33    . Since the determinant is multiplicative, and the determinant of the second matrix in the expression is x1, we have det(A)x1 = det(B1), where B1 is the matrix obtained by A by replacing the first column of A with the vector b. Since we can do this for any of the columns, we have x j = det(B j)/ det(A). In general Proposition 6.0.19 (Cramer’s Rule). Let A ∈ Rn×n such that det(A) ̸= 0 and b ∈ Rn then the solution x ∈ Rn of Ax = b is given by x j = det(B j) det(A) , where B j is the matrix obtained by A by replacing the j-th column of A with the vector b. Remark 6.0.20. As with the formula for the inverse: computationally speaking, this is not a good way to solve linear systems, as it involves computing many determinants. 6.0.4. Several further comments on the determinant. The definition we used for the determinant of a square matrix involves a formula with n! terms. This formula is computational infeasible for even moderate levels of n (it is faster than exponen- tial! For example, 100! has almost 160 digits!). In practice, the determinant of a matrix A is computed by Gaussian Elimination and the matrix decomposition PA = LU (P permutation and so det(P) = sgn(P), U is upper triangular and L is lower triangular with only 1s in the diagonal, and so det(L) = 1) . This gives us the formula (14) det(A) = 1 det(P) det(L) det(U) = sgn(P) det(U), and since U is a triangular matrix its determinants can be easily computed by Proposition 6.0.8. Alternatively, one can also think of Gaussian Elimination as directly computing the determinant via the following two propositions 41 Proposition 6.0.21. If A is an n × n matrix and P is a permutation that swaps two elements, meaning that PA corresponds to swapping two rows of A then det(PA) = − det(A). Proposition 6.0.22. The determinant is linear in each row (or each column). In other words, for any a0, a1, a2 . . . , an ∈ Rn and α0, α1 ∈ R we have ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ — α0a⊤ 0 + α1a⊤ 1 — — a⊤ 2 — ... — a⊤ n — ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ = α0 ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ — a⊤ 0 — — a⊤ 2 — ... — a⊤ n — ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ + α1 ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ — a⊤ 1 — — a⊤ 2 — ... — a⊤ n — ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ , and ∣ ∣ ∣ ∣ ∣ ∣ ∣ | | | α0a0 + α1a1 a2 · · · an | | | ∣ ∣ ∣ ∣ ∣ ∣ ∣ = α0 ∣ ∣ ∣ ∣ ∣ ∣ ∣ | | | a0 a2 · · · an | | | ∣ ∣ ∣ ∣ ∣ ∣ ∣ + α1 ∣ ∣ ∣ ∣ ∣ ∣ ∣ | | | a1 a2 · · · an | | | ∣ ∣ ∣ ∣ ∣ ∣ ∣ . Exploratory Challenge 23. The more mathematical way of presenting this material is to define a determinant as a function that goes from n × n matrices to R with the following properties: (1) it is linear in each column, (2) det(I) = 1 and (3) det(A) = 0 whenever A has two identical columns. It is then possible to prove that the only function satisfying these three properties is the determi- nant as we defined it. 7. EIGENVALUES AND EIGENVECTORS We are (almost) ready for one of the most important concepts (if not the most important one) in Linear Algebra, eigenvalues and eigenvectors. In a sense, it has all been building up to this! Guiding Strategy 24. Given a square matrix A, as we will see below, an eigenvalue λ and eigen- vector v will be, respectively, a scalar and a non-zero vector satisfying Av = λ v. This means that (A − λ I)v = 0 and so (A − λ I) is not invertible, or equivalently det(A − λ I) = 0. We can look for eigenvalues as solutions of det(A − λ I) = 0 which is a polynomial8 in λ but unfortunately, not 8This is one of the main reasons we had to cover determinants. 42 all polynomials have real zeros.9 For example if A = [ 0 −1 1 0 ] , det(A − λ I) = 0 corresponds to λ 2 + 1 = 0 which only has solutions in C, the Complex Numbers. For this reason we will start this Chapter with a brief introduction to Complex Numbers. It all starts with asking for a number λ such that λ 2 + 1 = 0. Further Reading 25. Complex Analysis is a beautiful topic in Mathematics, what we will cover here is just a tiny peak at it, there is a all bookshelf of excellent books in this topic in our li- brary. I have personally taught a course at ETH on Complex Analysis, and since it was during the COVID pandemic I made videos available online, which are still available at https://www. youtube.com/playlist?list=PLiud-28tsatLRRGqO_Eg_x0S4LVyxuV5p (In par- ticular, the first lecture covers roughly the content here). 7.0. Complex Numbers. If we start with the natural numbers N and want to solve equations like x + 10 = 1, we need negative numbers. This motivates considering the integers Z. Similarly, rational numbers Q are needed to solve equations like 10x = 1 and real numbers R are needed to solve x2 = 2.10 Similarly, the Complex Numbers are needed to solve equations such as x2 + 1 = 0. It starts with the introduction of an imaginary number i ∈ C such that i2 = −1. You can think of i as √ −1. The complex numbers are numbers of the form z = a + ib for a ∈ R and b ∈ R. C = {a + ib : a, b ∈ R}. Keeping in mind that i2 = −1 we can do operations with complex numbers: • (a + ib) + (x + iy) = (a + x) + i(b + y), • (a + ib)(x + iy) = ax + i(ay + bx) + i2by = ax + i(ay + bx) − by = (ax − by) + i(ay + bx), • (a + ib)(a − ib) = a2 + b2, • a+ib x+iy = (x−iy)(a+ib) (x−iy)(x+iy) = (ax+by)+i(bx−ay) x2+y2 = ( ax+by x2+y2 ) + i ( bx−ay x2+y2 ) . Given z ∈ C with z = a + ib we have the following notation ℜ(a + ib) := a called the real part of z = a + ib,(15) ℑ(a + ib) := b called the imaginary part of z = a + ib,(16) |z| := √ a2 + b2 called the modulus of z = a + ib,(17) a + ib := a − ib called the complex conjugate of z = a + ib.(18) 9A zero of a polynomial P is a point x such that P(x) = 0, this is also called a root of the polynomial. In German, it’s a “Nullstelle”. In fact, a (rather deep) multidimensional version of Theorem 7.0.3, and one of the most important facts in Algebraic Geometry, is called “Hilbert’s Nullstellensatz”. 10If you have never seen the proof that there exists no x ∈ Q such that x2 = 2 I highly recommend trying to do it: set x = a/b for a, b ∈ Z and try to count how many times 2 divides both a and b and find a contradiction. 43 Note that for z1, z2 ∈ C, we have |z|2 = zz, z1z2 = z2z1, z1 + z2 = z1 + z2, and 1 z = z |z|2 . Fact 7.0.1 (Euler’s Formula). Given θ ∈ R, we have (19) e iθ = cos θ + i sin θ . This means, in particular, that eiπ = −1. This is usually written as eiπ + 1 = 0. Further Reading 26. In order to prove Euler’s Formula, we need to first define what we mean by eiθ , this can be done, for example, by the Taylor series of the exponential, but this is outside the scope of this course (see Further Reading 25). Fact 7.0.2 (Polar Coordinates). A complex number z ∈ C can be written as (20) z = re iθ , where r ≥ 0 is the modulus of z and θ ∈ R (we can restrict to θ ∈ [0, 2π[) is an angle, also called the argument of z. FIGURE 4. A complex number z = 4 + 3i in the Complex plane. The most important property of Complex Numbers, and what makes them a very natural mathe- matical object, is that any univariate polynomial equation with complex number coefficients has a (complex) solution, in a certain sense we don’t need to extend numbers further, C is Alge- braically closed. 44 Theorem 7.0.3 (Fundamental Theorem of Algebra). Any degree n non-constant (n ≥ 1) poly- nomial P(z) = αnzn + αn−1zn−1 + · · · + α1z + α0 (with αn ̸= 0) has a zero: λ ∈ C such that P(λ ) = 0. Further Reading 27. As the name suggests, Theorem 7.0.3 is a central result in Complex Anal- ysis. Proving it is outside the scope of this course.11 Complex analysis (which leads to the proof of this theorem) is a beautiful example of interaction between analysis, algebra, and geometry. In a nutshell the idea for the classical proof is that differentiable functions in the complex plane f : C → C are very special and, in a sense, need to behave like polynomials (this is a deep state- ment that needs a significant amount of background to properly state and prove). If a polynomial P(z) doesn’t have a zero then 1/P(z) is a differentiable function that cannot behave like a non- constant polynomial because it does not grow sufficiently far away from zero, and so it must be a constant function which means that P(z) had to be constant, so any non-constant polynomial has a zero. For more on Complex Analysis see Further Reading 25. Further Remark 28. Once we have λ a zero of P(z), we can divide P(z) by (z − λ ) to get P(z) = (z − λ )P1(z), then use a zero of P1 to reiterate, and so on. This argument (carried out carefully) gives the following corollary. Corollary 7.0.4. Any degree n non-constant (n ≥ 1) polynomial P(z) = αnzn + αn−1zn−1 + · · · + α1z + α0 (with αn ̸= 0) has n zeros: λ1, . . . , λn ∈ C, perhaps with repetitions, such that (21) P(z) = αn(z − λ1)(z − λ2) · · · (z − λn). The number of times λ ∈ C appears in this expansion is called the algebraic multiplicity of the zero. 7.0.1. Complex-valued Matrices and Vectors. Analogously to Rn we also define Cn as the set of n-dimentional complex valued vectors. We can have complex valued vectors v ∈ Cn and matrices A ∈ Cm×n. The natural operation of “transpos- ing” for complex vectors and matrices is that of “conjugate transpose” or “hermitian transpose” denoted by A∗, or sometimes AH, (22) A∗ = A T . 11But you can see Appendix B for a relatively elementary proof. 45 Given v ∈ Cn we have ∥v∥ 2 = v ∗v = v T v = n ∑ i=1 vivi = n ∑ i=1 |vi| 2. The inner-product (or dot-product) in Cn is given by ⟨v, w⟩ = w∗v. Similarly to the situation in Rn, we say v1, . . . , vk ∈ Cn are linearly independent if there is no (complex valued) non-zero linear combination giving zero, meaning that if α1v1 + · · · + αkvk = 0 for α1, . . . , αk ∈ C we must have α1 = · · · = αk = 0. Also, the span of v1, . . . , vk ∈ Cn is the set of possible linear combinations α1v1 + · · · + αkvk for α1, . . . , αk ∈ C. If v1, . . . , vk is a spanning set of a subspace and linearly independent we say it is a basis of that subspace. As with Rn if we have v1, . . . , vn ∈ Cn that are either a spanning set of Cn or linearly independent then they must actually be both (and so are a basis). Further Reading 29. With these definitions you can already understand the Discrete Fourier Transform (which is the linear transformation corresponding to the DFT matrix, one of the most important complex valued matrices). This is the key object behind signal processing, you can read more about it on the lecture notes of another course I usually teach [BM23]. You can also see a discussion of Fourier Transform, circulant matrices, and signal convolutions in [Str23] (end of Section 6.4). 7.1. Introduction to Eigenvalues and Eigenvectors. Even though the theory can be analo- gously developed for complex valued matrices, we will focus on real valued matrices. Guiding Example 30. We will use a guiding example to illustrate both some of the power, and some of the properties, of eigenvalues and eigenvectors. In Guiding Example numbers 30 through 36 we will derive a formula for the n-th Ficonacci Number. The Fibonacci numbers are defined by the recurrence: (23) F0 = 0, F1 = 1, and, for n ≥ 2, Fn = Fn−1 + Fn−2. The recurrence can be rewritten in linear algebraic notation as, for n ≥ 2, (24) [ Fn+1 Fn ] = [ 1 1 1 0 ] [ Fn Fn−1 ] . Defining (25) M = [ 1 1 1 0 ] and gn = [ Fn+1 Fn ] , the recurrence can be rewritten as g0 = [ 1 0 ] and gn = Mgn−1, 46 meaning that (26) gn = Mng0. Definition 7.1.1. Given A ∈ Rn×n, we say λ ∈ C is an eigenvalue of A and v ∈ Cn \\ {0} is an eigenvector of A, associated with with the eigenvalue λ , when the following holds: Av = λ v. We call them an eigenvalue-eigenvector pair. If λ ∈ R then we will call λ a real eigenvalue, and the associated eigenvalue-eigenvector pair a real eigenvalue-eigenvector pair. Guiding Example 31. Let us try to find eigenvalues (and later the eigenvectors) of M = [ 1 1 1 0 ] . We are looking for v ∈ R2 \\ {0} and λ ∈ R such that Mv = λ v, but this can be rewritten as (M − λ I)v = 0 and since v ̸= 0 it means that M − λ I is non-invertible (also called singular).12 This is equivalent to det(M − λ I) = 0 and so we can find the eigenvalues λ with this equation: (27) 0 = det(M − λ I) = ∣ ∣ ∣ ∣ ∣ 1 − λ 1 1 0 − λ ∣ ∣ ∣ ∣ ∣ = (1 − λ )(0 − λ ) − 1 = λ 2 − λ − 1. By the quadratic formula, 13 the solutions to (27) are given by (28) λ1 = 1 + √ 5 2 and λ2 = 1 − √ 5 2 . Further Reading 32 (Golden Ratio). The number ϕ = 1+ √ 5 2 is the celebrated Golden Ratio; believed, since the ancient Greeks, to be the ideal aspect ratio for a rectangle. “Some of the greatest mathematical minds of all ages, from Pythagoras and Euclid in ancient Greece, through the medieval Italian mathematician Leonardo of Pisa and the Renaissance astronomer Johannes Kepler, to present-day scientific figures such as Ox- ford physicist Roger Penrose, have spent endless hours over this simple ratio and its properties. [. . . ] Biologists, artists, musicians, historians, architects, psychologists, and even mystics have pondered and debated the basis of its ubiquity and appeal. In fact, it is probably fair to say that the Golden Ratio has inspired thinkers of all disciplines like no other number in the history of mathematics.” — The Golden Ratio: The Story of Phi, the World’s Most Astonishing Number 12Normally, we would have to look for λ ∈ C and v ∈ Cn but in this case the eigenvalues, as we will see, are real. 13Recall that the quadratic formula says that the zeros of ax2 + b + c are given by x = −b± √ b2−4ac 2a . 47 The following is the original definition which dates back to Euclid around 2300 years ago (they called the number “extreme and mean ratio” back then) “A straight line is said to have been cut in extreme and mean ratio when, as the whole line is to the greater segment, so is the greater to the lesser” Guiding Example 33. Now we can try to find the eigenvectors v1 and v2 such that Av1 = λ1v1 and Av2 = λ2v2. Let us start with v1. We are looking for a non-zero element of N ( A − 1+ √ 5 2 I) . In other words [ 0 0 ] = [ 1 − 1+ √ 5 2 1 1 − 1+ √ 5 2 ] [ (v1)1 (v1)2 ] . This is an under-determined system and we are looking for a non-zero solution, so let us start by setting (v1)2 = 1. The second equation gives us (v1)1 = 1+ √ 5 2 . Indeed v1 = [ 1+ √ 5 2 1 ] is an eigenvector of M associated to the eigenvalue λ1 = 1+ √ 5 2 . A similar calculation for λ2 = 1− √ 5 2 gives that v2 = [ 1− √ 5 2 1 ] . Indeed (29) [ 1 1 1 0 ] [ 1+ √ 5 2 1 ] = 1 + √ 5 2 [ 1+ √ 5 2 1 ] and [ 1 1 1 0 ] [ 1− √ 5 2 1 ] = 1 − √ 5 2 [ 1− √ 5 2 1 ] Challenge 34. Carry out the calculations in Guiding Example 33 and confirm that we have indeed found two eigenvectors (check the two equalities in (29)). Further Remark 35. The v1 and v2 we constructed in 33 are not the only possible choices, for example any non-zero scalar multiples of these would have also been a possible choice. Normally one picks a unit-norm representative, but in this case we picked vectors that make the calculations the cleanest. What we carried out in the example above is very general and we now develop the theory for general matrices. Let λ and v be an eigenvalue-eigenvector pair of a matrix A. Since v ̸= 0 and (A − λ I)v = Av − λ v = 0 we have that det(A − λ I) = 0. Conversely, if det(A − λ I) = 0 for some λ , then there exists v ∈ N(A − λ I) \\ {0} and so λ is an eigenvalue. This gives a procedure to find eigenvalues and eigenvectors: (i) eigenvalues are the solution of det(A − λ I) = 0, which is a polynomial equation, and (ii) an associated eigenvector is a non-zero element of N(A − λ I). 48 Let us first formulate this for real eigenvalues and eigenvectors. Proposition 7.1.2. Let A ∈ Rn×n. λ ∈ R is a (real) eigenvalue of A if and only if det(A−λ I) = 0. A vector v is an eigenvector associated with the eigenvalue λ if (and only if) it is a non-zero element of N(A − λ I). A direct inspection of the formula for the determinant (Definition 6.0.6) gives the following. Proposition 7.1.3. det(A − λ I) is a polynomial, in λ , of degree n. The coefficient of the λ n term is (−1)n. The Fundamental Theorem of Algebra (Theorem 7.0.3) immediately implies Theorem 7.1.4. Every matrix A ∈ Rn×n has an eigenvalue (perhaps complex-valued). Remark 7.1.5. For now we will focus on real eigenvalues, and address complex valued ones later on. Essentially all the properties we will describe below also hold for complex valued eigenvalues (just by replacing R by C and doing the appropriate adjustments). For example, Proposition 7.1.2 also holds for complex-valued eigenvalues, one just needs to think of N(A − λ I) as a subspace of Cn, meaning the vectors v ∈ Cn such that (A − λ I)v = 0. Guiding Example 36. Let us return to our guiding example. Notice that v1 and v2 are linearly independent, and so they are a basis for R2. We can write g0 = α1v1 + α2v2. [ 1 0 ] = g0 = α1v1 + α2v2 = [ α1 1+ √ 5 2 + α2 1− √ 5 2 α1 + α2 ] = [ (α1 + α2) 1 2 + (α1 − α2) √ 5 2 α1 + α2 ] , and so α1 = 1√ 5 and α2 = − 1√ 5. Recall that gn = Ang0 and so gn = A n ( 1 √ 5v1 − 1 √ 5v2 ) = 1 √ 5Anv1 − 1 √ 5A nv2 = 1 √ 5 (Anv1 − Anv2) . 49 Since Av1 = λ1v1 we have that A2v1 = A(λ1v1) = λ 2 1 v1 and iterating this procedure 14 gives Anv1 = λ n 1 v1. This means that gn = Anv1 − Anv2√ 5 = ( 1+ √ 5 2 )n v1 − ( 1− √ 5 2 )n v2 √ 5 = ( 1+ √ 5 2 )n √ 5 [ 1+ √ 5 2 1 ] − ( 1− √ 5 2 )n √ 5 [ 1− √ 5 2 1 ] . Since Fn is the second coordinate of gn, we derived a closed formula for the n-th terms of the Fibonacci sequence: (30) Fn = 1 √ 5 (1 + √ 5 2 )n − 1 √ 5 (1 − √ 5 2 )n . An important property that allowed us to do the calculation above was that applying a power of a matrix to an eigenvector was a simple operation, this is the next proposition. Proposition 7.1.6. If λ and v are an eigenvalue-eigenvector pair of a matrix A, then, for k ≥ 1, λ k and v are an eigenvalue-eigenvector pair of the matrix Ak. Proof. Proof by Induction: The base case k = 1 is trivial. For the induction step, since λ k and v are an eigenvalue-eigenvector pair then Akv = A ( Ak−1v ) = A ( λ k−1v ) = λ kv. 2 Proposition 7.1.7. Let A be an invertible matrix. If λ and v are an eigenvalue-eigenvector pair of the matrix A, then, 1 λ and v are an eigenvalue-eigenvector pair of the matrix A−1. Proof. A is invertible and hence, in the statement λ ̸= 0. Since Av = λ v we have A−1(λ v) = v and so λ A−1v = v, which (since λ ̸= 0) is equivalent to A−1v = 1 λ v. 2 Another important property was that we were able to write a vector as a linear combination of eigenvectors, which was possible because the eigenvectors were linearly independent. Proposition 7.1.8. Let A ∈ Rn×n and let v1, . . . , vk ∈ Rn be eigenvectors corresponding to eigenvalues λ1, . . . , λk ∈ R. If λ1, . . . , λk are all distinct, the eigenvectors v1, . . . , vk are linearly independent. 14A formal proof would use induction 50 Proof. We will prove this by contradiction. Assume that v1, . . . , vk are linearly dependent. For i = 1, . . . , k, let di denote the dimension of the span of v1, . . . , vi. Since v1 ̸= 0 we have d1 = 1. By the hypothesis dk < k. Let j be the smallest positive integer for which d j < j. Note that, by construction, d j−1 = d j = j − 1, this means that v1, . . . , v j−1 are linearly independent but that v j is in the span of v1, . . . , v j−1. We can then write (31) v j = α1v1 + · · · α j−1v j−1. If we multiply by A both sides we get λ jv j = Av j = A ( α1v1 + · · · α j−1v j−1) = α1λ1v1 + · · · λ j−1α j−1v j−1. Replacing the v j in the left hand side with the right hand side of (31) we get λ j ( α1v1 + · · · α j−1v j−1) = α1λ1v1 + · · · λ j−1α j−1v j−1, which we can rearrange as (32) α1 ( λ j − λ1) v1 + α2 ( λ j − λ2) v2 + · · · + α j−1 ( λ j − λ j−1) v j−1 = 0. Since λ j − λi ̸= 0 for all i ≤ j − 1 and not all αi’s are zero, this is a non-zero linear combination of v1, . . . , v j−1 adding to zero, which would be a contradiction with d j−1 = j − 1. 2 A very important consequence of this is that if a matrix has n distinct real eigenvalues then the eigenvectors form a basis for Rn. Theorem 7.1.9. Let A ∈ Rn×n with n distinct real eigenvalues (meaning that the n zeros of det(A − λ I), as described in Corollary 7.0.4, are all distinct) then there is a basis of Rn, v1, . . . , vn, made up of eigenvectors of A. Guiding Example 37. Guiding Example 30 is yet to stop providing us with insight into properties of eigenvalues and eigenvectors! Here are a couple of observations, which although outside of the core scope of this course, have significant impact in several areas: • Notice that since |λ2| < |λ1|, the contribution of λ n 2 α2v2 becomes negligible (when com- pared to λ n 1 α1v1) as n → ∞. This observation can be used in a clever way: we can approximate the eigenvector v1 by Ang0 and so if we have a fast way to do matrix-vector multiply, we can approximate eigenvalues and eigenvectors. This is often referred to as the Power Method. In a CS Lens I plan to show you how Google’s celebrated PageR- ank algorithm is based on the idea of how eigenvectors can be used for ranking (you can 51 also read more about it here [BSS]. 15), calculating the eigenvector using a version of the Power Method is a crucial part of the algorithm. 16 • The vector gn gets larger and larger as n → ∞ because |λ1| > 1. If both eigenvalues satisfied |λ | < 1 then gn → 0 as n → ∞. This illustrates the importance of the largest absolute values of the eigenvalues of a matrix in understanding the long term behaviour of systems of the form Ang0 for some A. If it represents a dynamical system it is related to stability or instability/chaos, if it represents e.g. the evolution of an economical system over time (or the finances of a company) it can be the difference between growth or ruin.17 A few properties of the eigenvalues follow from the fact that, by Corollary 7.0.4, (33) (−1) n det(A − zI) = det(zI − A) = (z − λ1)(z − λ2) · · · (z − λn). The polynomial (33) is called the Characteristic Polynomial of the matrix A.18 Proposition 7.1.10. Given A ∈ Rn×n the eigenvalues of A are the same as the ones of A⊤. Proof. This follows from (33), and the fact that, for det(A − zI) = det((A − zI)⊤) = det(A⊤ − zI). 2 Definition 7.1.11. Given a matrix A ∈ Rn×n, the trace of A is defined as Tr(A) = n ∑ i=1 Aii. A link between eigenvalues and the trace o a matrix is given below. Proposition 7.1.12. Let A ∈ Rn×n and λ1, . . . , λn its n eigenvalues as they show up in (33) (meaning that a value λ may be repeated, the number of times it shows up is the algebraic 15Take a look also at “Landau on Chess Tournaments and Google’s PageRank” by Rainer Sinn and G¨unter M. Ziegler (https://arxiv.org/pdf/2210.17300.pdf). 16An important advantage is that if we already have a good approximation of v1, e.g. the page ranks from last week, we can compute a better approximation of v1 (of this week’s rankings) with very few matrix multiplies, you can read more about it here [BSS] and in the references therein. 17Try to modify the Fibonacci recurrence rule so that the new numbers go to zero as n → ∞. Can you pick a recurrence such that they stabilize as n → ∞ (without going to ∞ or 0)? Maybe linear algebra students in 2823 years will be studying your sequence! 18There is a converse to this in the sense that any monic polynomial can be written as a characteristic polynomial of a matrix, there is a particularly elegant way to build the matrix, if you are interested in learning more, look-up “companion matrix”. 52 multiplicity of the eigenvalue) then (34) Tr(A) = n ∑ i=1 λi, (35) det(A) = n ∏ i=1 λi. Remark 7.1.13. When calculating eigenvalues, Proposition 7.1.12 is very useful to check com- putations. It is not difficult to verify that one can calculate with the trace-operator. Lemma 7.1.14. For matrices A, B,C ∈ Rn×n one has (i) Tr(AB) = Tr(BA) (ii) Tr(ABC) = Tr(BCA) = Tr(CAB). Proof. (i) Tr(AB) = n ∑ i=1 n ∑ j=1 Ai jB ji = n ∑ j=1 n ∑ i=1 B jiAi j = Tr(BA). (ii) Tr(ABC) = Tr(A(BC)) = Tr((BC)A) = Tr(B(CA)) = Tr(CAB). 2 Proof. [of Proposition 7.1.12] Let us consider the characteristic polynomial established in (33). (−1)n det(A − zI) = (z − λ1)(z − λ2) · · · (z − λn) = zn + (− ∑ n i=1 λi)zn−1 + ∑ n−2 k=1 bkzk + (−1)n ∏ n i=1 λi, where bk ∈ C. Set z = 0 in the expression above. It gives (−1)n det(A) = (−1)n ∏ n i=1 λi as claimed in (35). For (34) note that the coefficient of zn−1 in the characteristic polynomial (33) is given in the right hand side by (− ∑ n i=1 λi). On the left hand side the coefficient of zn−1 can only come from the permutation that takes all diagonal elements in the matrix zI − A. Hence it is the coefficient of 53 zn−1 of ∏ n i=1 (z − Aii) which is − ∑ n i=1 Aii = − Tr(A). 2 Caution! 38. We write this caution as Remark 7.1.15 below given how important it is (so that it appears in black font). Remark 7.1.15. A few important words of caution: (1) Even though the eigenvalues of A and A⊤ are the same, the eigenvectors are not! (2) The eigenvalues of A + B are not easily computed from the eigenvalues of A and the ones of B, in particular they are not their sum! (3) The eigenvalues of AB or BA are not easily computed from the eigenvalues of A and the ones of B, in particular they are not their product! 19 (4) Gaussian Elimination doesn’t preserve eigenvalues and eigenvectors. The eigenvalues are not the diagonal elements of the U matrix in the PA = LU factorization. 20 While we focus on real eigenvalues on this course, let us see an example of a matrix that has no real eigenvalues and only complex valued ones. Example 7.1.16. The eigenvalues of the matrix A = [ 0 −1 1 0 ] , corresponding to a 90o counter- clockwise rotation, are the solutions to 0 = det(A − λ I) = λ 2 + 1, which are λ1 = i and λ2 = −i. The eigenvectors are given by v1 = [ i 1 ] and v2 = [ −i 1 ] . Challenge 39. Try to work out an example for another rotation of R2. This is a particular case of an orthogonal matrix, whose eigenvalues have a special property. Proposition 7.1.17. Let Q ∈ Rn×n be an orthogonal matrix.21 If λ ∈ C is an eigenvalue of Q, then |λ | = 1. 19Interesting, there is a deep connection between A and B commuting (meaning AB = BA) and having the same eigenvectors. This is important in a few fields, in particular in Quantum Physics. If you want to learn more look-up “simultaneously diagonalizable”. 20How to actually compute eigenvalues efficiently is outside of the scope of this course, it turns out that one can do it using the QR decomposition that we learned here as a subroutine. If you want to learn more look-up “QR algorithm”. 54 Proof. Let λ ∈ C be an eigenvalue of Q and v ∈ Cn an associated eigenvector. Then Qv = λ v. Since Q is an orthogonal matrix we have ∥v∥2 = ∥Qv∥2 = ∥λ v∥2 = |λ |2∥v∥2. Since v ̸= 0 we have |λ | = 1. 2 Further Fact 40. If λ ∈ C is an eigenvalue of a matrix A with real entries, then λ is also an eigenvalue of A. This can be shown by noticing that det ( A − λ I) = det (A − λ I) (due to the polynomial representation of the determinant in (33), the fact that A has real entries). 7.1.1. Repeated eigenvalues. An important part of the success of the strategy we took in Guiding Example 30 was the fact that we were able to build a basis of R2 with eigenvectors of the matrix M. In Theorem 7.1.9 we showed that we can always build a basis of Rn with eigenvectors of an n × n matrix A if A has n distinct real eigenvalues. One obstacle could be if some of the eigenvalues are not real valued but, even though we have not focused in complex valued eigenvalues, a straightforward adaption of the proof shows that if A has n distinct eigenvalues (not necessarily real) then there is a basis of Cn made up of eigenvectors of A. However, repeated eigenvalues can (but doesn’t have to) pose a real obstacle to building a basis. Example 7.1.18. The matrix A = [ 0 1 0 0 ] does not have two linearly independent eigenvectors. Indeed, det(A − λ I) = λ 2 which means that λ = 0 is the only eigenvalue and has algebraic multiplicity 2. However, N(A − 0I) = N(A) only has dimension 1, so there is only one eigenvector (and multiples of it). with Example 7.1.19. The zero matrix A = [ 0 0 0 0 ] does have two linearly independent eigenvectors. Indeed, det(A − λ I) = λ 2 which means that λ = 0 is the only eigenvalue and has algebraic multiplicity 2. But, unlike in Example 7.1.18, N(A − 0I) = N(A) has dimension 2, so there is a basis made up of two eigenvectors (in fact any two linearly independent vectors will be such a basis). Further Remark 41. Notice that in Example 7.1.18, N ( A2) does have dimension 2. When there exist a positive integer k such that Ak = 0 we call A Nilpotent. There is a (rather deep) Theorem that essentially says nilpotency is the only obstacle to getting a complete set of eigenvectors. It roughly says that when there are “missing” eigenvectors they can be found in the Nullspace of powers of A − λ I, and this gives rise to something called “Jordan Normal Form”. 55 Definition 7.1.20. If, given a matrix A ∈ Rn×n, we can build a basis of Rn with eigenvectors of A we say that A has a complete set of real eigenvectors. 22 Theorem 7.1.9 states that a matrix with n distinct eigenvalues always has a complete set of real eigenvectors. Proposition 7.1.21 (Eigenvalues and Eigenvectors of a Projection Matrix). Let P be the pro- jection matrix on the subspace U ⊆ Rn. Then P has two eigenvalues, 0 and 1, and a complete set of real eigenvectors. Proof. Let m be the dimension of U. Let u1, . . . , um be a orthonormal basis of U, and w1, . . . , wn−m an orthonormal basis of U ⊥. It is easy to see that Puk = 1uk for any 1 ≤ k ≤ m and Pwk = 0wk for any 1 ≤ k ≤ n − m, so all n vectors are eigenvectors of P (with eigenvalues either 1 or 0). By construction of U ⊥, they form an orthonormal basis. 2 In general when there is an eigenvalue λ with algebraic multiplicity larger than 1, it can be that N(A − λ I) is of large enough dimension to find enough linearly independent eigenvectors (as it is the case in projection matrices above, but not in the nilpotent example). Definition 7.1.22. Given a matrix A ∈ Rn×n and an eigenvalue λ of A we call the dimension of N(A − λ I) the geometric multiplicity of λ . Further Fact 42. A matrix has a complete set of eigenvectors when the geometric multiplicities are the same as the algebraic multiplicites of all eigenvalues. Exploratory Challenge 43. Prove Further Fact 42 Example 7.1.23. For D ∈ Rn×n a diagonal matrix, the eigenvalues of D are the diagonal entries of D. The canonical basis e1, . . . , en is a set of eigenvectors of D. Challenge 44. Prove the statement in Example 7.1.23 Further Fact 45. The eigenvalues of an n × n triangular matrix are the n values in the diagonal. However, triangular matrices may not have a complete set of eigenvectors. 22If the matrix A has complex valued eigenvalues and we can instead build a basis of Cn we say it has a complete set of eigenvectors. Essentially everything we do below can be (straightforwardly) extended to this case but we will focus on real eigenvalues and eigenvectors for ease of exposition. 56 Challenge 46. Prove this fact. Hint: For the positive part use (33), for the negative part recall Example 7.1.18. Challenge 47. Let’s say a matrix A ∈ Rn×n has an LU decomposition (without the need for P in PA = LU). Remark 7.1.15 says the eigenvalues of A ∈ Rn×n are not the ones of U in the LU decomposition PA = LU. The eigenvalues of U are indeed their diagonal entries, and the eigenvalues of L are all 1 (by Further Fact 7.1.1). Why is it the case that the eigenvalues of A are not the diagonal entries of U? 7.2. Diagonalizing a Matrix and Change of Basis of a Linear Transformation. Let us continue dissecting Guiding Example 30. Essentially, what we did was to write g0 in the basis v1, v2 of eigenvectors of M and then exploit the fact that linear transformation given M had a very simple behaviour when written in the basis v1, v2 (the coefficients simply were multiplied by the eigenvalues of M). This motivates us to take a detour in briefly studying linear transformations written in different basis, and to discuss “change of bases”. 7.2.1. Change of basis. For this detour we will briefly consider m × n matrices, before returning to square matrices when discussing eigenvalues and eigenvectors. Let Am×n be a matrix representing a linear transformation L : Rn → Rm given by x ∈ Rn → Ax ∈ Rm, with both input and output written in the canonical bases as x = ∑ n j=1 x je j and Ax = ∑ m i=1(Ax)iei. Recall (Example 5.4.2) that (ei) j = δi j, and that (Ax)i is the i-th entry of the vector Ax. Now, let’s say we have a basis for Rn given by u1, . . . , un and one for Rm given by v1, . . . , vm (neither being necessarily the canonical basis) and we want to understand the the linear transfor- mation L written in this basis. Then L takes a vector x = ∑ n j=1 α ju j and outputs L(x) = ∑ n j=1 βivi. We want to compute the matrix B that takes α =    α1 ... αn    to    β1 ... βm   . In other words, such that Bα = β . Let U ∈ Rn×n be the matrix whose columns are the basis elements u1, . . . , un and V ∈ Rm×m the matrix whose columns are the basis elements v1, . . . , vm. Then, x = Uα and L(x) = V β and so β = V −1AUα, the matrix B, corresponding to the linear transformation L writ- ten in the new bases is B = V −1AU. Note that we can do change of basis between any pair of basis, it needs not be from the canonical basis to another basis, in that case the role of U and V would be played by the change of basis matrix (the matrix that maps the coefficients of a vector written in the old basis, to its coefficients when written in the new basis). 57 L : R n → R m linear transformation L ( n ∑ j=1 x je j ) = n ∑ i=1 (Ax)iei x =    x1 ... xn   (36) L ( n ∑ j=1 α ju j ) = n ∑ i=1 (Bα)ivi α =    α1 ... αn    where B = V −1AU ∈ R m×n, U = [ u1 · · · un ] ∈ R n×n, V = [ v1 · · · vm ] ∈ Rm×m. 7.2.2. Diagonalizing a Matrix. Let us focus back on square matrices A ∈ Rn×n. In particular, let A be a matrix with a complete set of real eigenvectors (in the sense of Definition 7.1.20) and let v1, . . . , vn ∈ Rn×n be a basis formed with eigenvectors of A. The crucial fact we used in Guiding Example 30 also holds in this general situation: if we write a vector x ∈ Rn as x = ∑ n i=1 αivi then Ax = ∑ n i=1 λiαivi (and also Akx = ∑ n i=1 λ k i αivi, for k ≥ 1, where λi is the eigenvalue associated with the eigenvector vi). One way to think about this is that the linear transformation corresponding to the matrix A, when written in the basis V is simply a diagonal matrix/transformation. This is the key idea behind Matrix Diagonalization. This is one of the most important facts in Linear Algebra. Theorem 7.2.1. Let A ∈ Rn×n be a matrix with a complete set of real eigenvectors (in the sense of Definition 7.1.20) and let v1, . . . , vn ∈ Rn×n be a basis formed with eigenvectors of A and let λ1,. . . ,λn be the associated eigenvalues (λi associated to vi). Let V be the matrix whose columns are the eigenvectors vi, V = [ v1 · · · vm ] ∈ Rn×n. Then, (37) A = V ΛV −1, where Λ is a diagonal matrix with Λii = λi (and Λi j = 0 for all i ̸= j). Proof. Since v1, . . . , vn is a basis, V is an invertible matrix, so it suffices to prove that (38) V −1AV = Λ. This can be done by direct calculation: For any 1 ≤ j ≤ n, the j-th column of the matrix V −1AV is given by ( V −1AV ) · j := ( V −1AV ) e j = V −1Av j = V −1λ jv j = λ jV −1v j = λ je j, 58 since V −1v j = V −1Ve j = e j. Recall that e j is the vector in Rn with a 1 in j-th entry and zero else- where. Since for any 1 ≤ j ≤ n, λ je j is also the j-th column of Λ, we have that V −1AV = Λ. 2 Definition 7.2.2 (Diagonalizable Matrix). A matrix A ∈ Rn×n is called a diagonalizable matrix if there exists an invertible matrix V such that V −1AV = Λ, where Λ is a diagonal matrix. Challenge 48. Most properties of the eigenvalues are very easy to prove by using Theorem 7.2.1 (for the matrices that have a complete set of eigenvectors). Try it! The eigenvalues of Λ are also λ1, . . . , λn (recall Example 7.1.23). More generally, for an invertible matrix S we always have that A and S−1AS have the same eigenvalues. Definition 7.2.3 (Similar Matrices). 23 We say that A ∈ Rn×n and B ∈ Rn×n are similar matrices if there exists an invertible matrix S such that B = S−1AS. Proposition 7.2.4. Similar matrices have the same eigenvalues. Challenge 49. Try to show Proposition 7.2.4 via (33). Challenge 50. Try to show that if λ is an eigenvalue of S−1AS (with associated eigenvector v) then it is also an eigenvalue of A and compute the associated eigenvector in terms of v and S. (without using Proposition 7.2.4 or (33). Remark 7.2.5 (Diagonalizing a matrix and finding a good basis for a linear transformation). If we have a matrix A ∈ Rn×n with a complete set of real eigenvectors then Theorem 7.2.1 tells us that the corresponding linear transformation, when viewed in the bases v1, . . . , vn is simply a diagonal matrix (recall that in this case B = Λ, see (36)). This is a remarkable fact: since most matrices have a full set of eigenvectors (in particular all for which the eigenvalues are all distinct do) this says that all the corresponding linear combinations, regardless of how complicated they might seem, are actually just a diagonal operation when viewed in the basis v1, . . . , vn. 23The operation A → S−1AS is sometimes called conjugation but this is not to be confused with complex conju- gation z → z, one term comes from “conjugation” in group theory, the other from “conjugation” in complex analysis. 59 7.3. Symmetric Matrices and the Spectral Theorem. This section is devoted to real symmetric matrices, 24 meaning matrices A ∈ Rn×n for which A⊤ = A (see Further Remark 54 for a brief discussion of how symmetric matrices appear naturally in several settings). The main goal of this section is to prove the Spectral Theorem. Theorem 7.3.1 (Spectral Theorem). Any symmetric25 matrix A ∈ Rn×n has n real eigenvalues and an orthonormal basis made of eigenvectors of A. Together with Theorem 7.2.1 this implies the following corollary. Corollary 7.3.2. For any symmetric matrix A ∈ Rn×n there exists an orthogonal matrix V ∈ Rn×n (whose columns are eigenvectors of A) such that A = V ΛV ⊤, where Λ ∈ Rn×n is a diagonal matrix with the eigenvalues of A in its diagonal (and V ⊤V = I). Remark 7.3.3 (Eigendecomposition). The decompositions in Corollary 7.3.2 and Theorem 7.2.1 are called Eigendecompositions. The following follows easily from the Spectral Theorem. Corollary 7.3.4. The rank of a real symmetric matrix A is the number of non-zero eigenvalues (counting repetitions). Remark 7.3.5. For general n × n (non-symmetric) matrices, the rank is n minus the dimension of the nullspace, so it is n minus the geometric multiplicity of λ = 0. Since symmetric matrices always have a complete set of eigenvalues and eigenvectors, the geometric multiplicities are always the same as the algebraic multiplicities. 24The same theory can be developed (by a straightforward adaption) to complex matrices but the property of being symmetric is replaced by being Hermitian, which means that a matrix A = A∗ = A ⊤ . In both situations we say A is self-adjoint. 60 Proposition 7.3.6. Let A be a real n×n symmetric matrix and let v1, . . . , vn be an orthonormal basis of eigenvectors of A (the columns of the matrix V in Corollary 7.3.2) and λ1, . . . , λn the associated eigenvalues. Then A = n ∑ k=1 λiviv ⊤ i Proof. Follows directly by Corollary 7.3.2. 2 We “build up” to the proof of Theorem 7.3.1 with a few propositions. Proposition 7.3.7. Let A ∈ Rn×n be a symmetric matrix and λ ∈ C an eigenvalue of A, then λ ∈ R. Proof. Let v ∈ Cn be an eigenvector associated with the eigenvalue λ . We have Av = λ v. Recall that, for a matrix (or vector) M, its Hermitian conjugate is given by M∗ = M⊤. Since A is real symmetric we have A∗ = A. Thus, we have λ ∥v∥ 2 = λ v ∗v = (λ v) ∗v = (Av) ∗v = v ∗A∗v = v ∗Av = v ∗λ v = λ ∥v∥ 2. Since v ̸= 0, then ∥v∥ ̸= 0 and so λ = λ . This implies that λ ∈ R. 2 This, together with Theorem 7.1.4, immediately implies the following. Corollary 7.3.8. Every symmetric matrix A ∈ Rn×n has a real eigenvalue λ . Remark 7.3.9. The fact that two eigenvectors of a real symmetric matrix are orthogonal follows from Theorem 7.3.1 but it is useful to see a simple argument of that (the main difficulty of proving Theorem 7.3.1 is proving that the matrix indeed has a complete set of eigenvectors). Let’s say we have λ1 ̸= λ2 eigenvalues of a real symmetric matrix A and v1, v2 ∈ Rn \\ {0} corresponding eigenvectors. Then λ1v ⊤ 1 v2 = (Av1) ⊤ v2 = v ⊤ 1 A ⊤v2 = v ⊤ 1 Av2 = v ⊤ 1 (Av2) = λ2v ⊤ 1 v2, since λ1 ̸= λ2 we must have that v⊤ 1 v2 = 0 61 Further Remark 51. Corollary 7.3.8 is a great example of the usefulness of complex numbers. Even though it is a statement just about real matrices and real eigenvalues we proved it by going through the complex numbers and using results in Complex Analysis. There is an alternative proof without going through the complex numbers but it would need more background, and I find this one more transparent. 26 Proof. [of Theorem 7.3.1] Let A ∈ Rn×n be a symmetric matrix. We will prove the following by induction, which for k = n implies the theorem we want to show: • For any 1 ≤ k ≤ n there are k orthogonal eigenvectors of A. The base case k = 1 follows directly by Corollary 7.3.8 as we can always normalize the eigen- vector to have norm 1. We now assume that the statement is true for k and show it for k + 1. We will show that if a real symmetric matrix A has k (with 1 ≤ k < n) orthonormal eigenvectors then we can build an extra one, orthogonal to the others (to achieve norm 1 we simply need to normalize it). 27 Let v1, . . . , vk denote k orthonormal eigenvectors of A and λ1, . . . , λk the respective eigenvalues. Let uk+1, . . . , un be an orthonormal basis of the orthogonal complement of the span of v1, . . . , vk. Let Vk be the n × n matrix whose i-th column is vi if i ≤ k and ui if i > k. Vk is an orthogonal 26If you would like to see a nice example of how improving knowledge can lead to much simpler and more transparent methods/models, search “Ptolemaic Epicycle Machine”. 27In a first reading of the proof I recommend taking k = 1 in the induction step, as it is simpler while already containing all the relevant ideas. 62 matrix. Moreover, let us define B ∈ Rn×n as B = V ⊤AV , then: B = V ⊤AV =            — v⊤ 1 — ... — v⊤ k — — u⊤ k+1 — ... — u⊤ n —                    | | | | Av1 · · · Avk Auk+1 · · · Aun | | | |         =            — v⊤ 1 — ... — v⊤ k — — u⊤ k+1 — ... — u⊤ n —                    | | | | λ v1 · · · λ vk Auk+1 · · · Aun | | | |         = [ Λk 0k×(n−k) 0(n−k)×k C ] , where Λk is a diagonal matrix with λ1, . . . , λk in the diagonal, 0(n−k)×k and 0k×(n−k) are zero matrices of size respectively (n − k) × k and k × (n − k). C is a (n − k) × (n − k) symmetric matrix. Since C is a (n − k) × (n − k) symmetric matrix, Theorem 7.3.8 implies it has a real eigenvalue λk+1 and a real eigenvector y ∈ Rn−k. Let w ∈ Rn be the vector with 0 in the first k coordinates and y in the remaining n − k, in other words wi = { 0 if i ≤ k yi−k if i > k. We have Bw = [ Λk 0k×(n−k) 0(n−k)×k C ] [ 0k×1 y ] = [ 0k×1 Cy ] = [ 0k×1 λk+1y ] = λk+1w. Let vk+1 := V w. Since V is orthogonal we have that A = V BV ⊤. Thus, Avk+1 = V BV ⊤vk+1 = V Bw = V λk+1w = λk+1vk+1, so vk+1 is an eigenvector of A. To see that it is orthogonal to v1, . . . , vk note that the inner products v⊤ i vk+1 for i ≤ k appear in the first k entries of V ⊤vk+1 = w and that w has its first k coordinates 0 by construction. By normalizing the vector we can have it have unit norm. 63 2 Proposition 7.3.10 (Rayleigh Quotient). Given a symmetric matrix A ∈ Rn×n the Rayleigh Quotient, defined for x ∈ Rn \\ {0}, as R(x) = x⊤Ax x⊤x attains its maximum at R(vmax) = λmax and its minimum at R(vmin) = λmin where λmax and λmin are respectively the largest and smallest eigenvalues of A and vmax, vmin their associated eigenvectors. Proof. It is easy to see that R(vmax) = λmax and R(vmin) = λmin so it suffices to show that, for all x ∈ Rn \\ {0} we have λmin ≤ R(x) ≤ λmax. Using Proposition 7.3.6 we can write, for x ∈ Rn \\ {0}, R(x) = x⊤ ( ∑ n i=1 λiviv⊤ i ) x ∥x∥2 = ∑ n i=1 λi ( x⊤vi)2 ∥x∥2 , where v1, . . . , vn form an orthonormal basis of eigenvectors of A and λ1, . . . , λn are the associated eigenvalues. Since ( x⊤vi)2 ≥ 0 for all 1 ≤ i ≤ n we have that, for all 1 ≤ i ≤ n, λmin ( x⊤vi)2 ≤ λi ( x⊤vi)2 ≤ λmax ( x⊤vi)2 . Collecting all these inequalities we get λmin ∑ n i=1 ( x⊤vi)2 ∥x∥2 ≤ ∑ n i=1 λi ( x⊤vi)2 ∥x∥2 ≤ λmax ∑ n i=1 ( x⊤vi)2 ∥x∥2 . To conclude the proof note that, since the vi’s are orthonormal, the matrix V with the vi’s as columns is orthogonal and ∑ n i=1 ( x⊤vi)2 = ∥V x∥2 = ∥x∥2 and so ∑ n i=1(x⊤vi) 2 ∥x∥2 = 1. 2 Definition 7.3.11 (Positive Definite and Positive Semidefinite matrix). A symmetric matrix A ∈ Rn×n is said to be Positive Semidefinite (PSD) if all its eigenvalues are non-negative. If all the eigenvalues of A are strictly positive then we say A is Positive Definite (PD). Exploratory Challenge 52. Even though the eigenvalues of A + B are not easily described by the eigenvalues of A and the ones of B it turns out that if both are PSD (or PD) then so is the sum. Can you show that? The following follows directly from Proposition 7.3.10. 64 Proposition 7.3.12. A symmetric matrix A ∈ Rn×n is Positive Semidefinite if and only if x⊤Ax ≥ 0 for all x ∈ Rn. Analogously, a symmetric matrix A ∈ Rn×n is Positive Definite if and only if x⊤Ax > 0 for all x ∈ Rn \\ {0}. Fact 7.3.13. If two n × n matrices A and B are PSD (or PD) then their sum is PSD (or PD). Challenge 53. Exploratory Challenge 52 looked pretty difficult, but now with Proposition 7.3.12 it is much easier, try to prove Fact 7.3.13. Definition 7.3.14 (Gram Matrix). Given n vectors, v1, . . . , vn in Rm we call their Gram Matrix the n × n matrix of inner products Gi j = v ⊤ i v j. Note that if V ∈ Rm×n is the matrix whose columns are the n vectors, then G = V ⊤V is the Gram matrix of V . Remark 7.3.15. Given a matrix A ∈ Rm×n, as an abuse of notation, we sometimes also call AA⊤ a Gram matrix of A. Notice that, if a1, . . . , an ∈ Rm are the columns of A then AA⊤ is m × m and (39) AA ⊤ = n ∑ i=1 aia ⊤ i . Proposition 7.3.16. Given a real matrix A ∈ Rm×n, the non-zero eigenvalues of A⊤A ∈ Rn×n are the same as the ones of AA⊤ ∈ Rm×m. Both matrices are symmetric and positive semidef- inite. Proof. Let r be the rank of A. We know rank(A) = rank(A⊤) = rank(A⊤A) = rank(AA⊤) (recall Proposition ?? and Challenge ??). It is straightforward that both A⊤A and AA⊤ are symmetric. Let us prove they are positive semidefinite. We have x⊤A⊤Ax = ∥Ax∥2 ≥ 0 for all x which implies A⊤A is PSD, and the same argument can be used for AA⊤. Now, both AA⊤ and A⊤A have a complete set of real eigenvalues and orthogonal eigenvectors. Let λ1, . . . , λr be the r non-zero eigenvalues of A⊤A and v1 . . . , vr be the corresponding eigenvec- tors. We have, for 1 ≤ k ≤ r, A⊤Avk = λkvk, multiplying by A both sides we get AA⊤Avk = λkAvk and so λk is an eigenvalue of AA⊤ with eigenvector Avk (note that Avk ̸= 0). Furthermore, For j ̸= k we have (Av j)⊤(Avk) = v⊤ j A⊤Avk = v⊤ j λkvk = λkv⊤ j vk = 0 and so the r eigenvectors of AA⊤ built this way are orthogonal, and so λ1, . . . , λr are the nonzero eigenvectors of AA⊤. 2 65 Proposition 7.3.17. [Cholesky decomposition] Every symmetric positive semidefinite matrix M is a gram matrix of an upper triangular matrix C. M = C⊤C is known as the Cholesky Decomposition. 28 Proof. Let M be a symmetric positive semidefinite matrix. Corollary 7.3.2 gives us a de- composition M = V ΛV ⊤ with Λ a diagonal matrix with the eigenvalues of M in the diagonal. Since M is PSD, the diagonal entries of Λ are non-negative and so we can build Λ1/2 by tak- ing the square root of each diagonal entry of Λ. Then M = ( V Λ1/2) ( V Λ1/2)⊤. To make the matrices in the decomposition be upper triangular, simply take the QR decomposition (re- call Definition 5.4.11) ( V Λ1/2)⊤ = QR with Q such that Q⊤Q = I and R upper triangular. We have M = ( V Λ1/2) ( V Λ1/2)⊤ = (QR) ⊤ (QR) = R⊤Q⊤QR = R⊤R. Taking C = R establishes the Proposition. 29 2 Further Remark 54. At first glance, Symmetric matrices look very special (since we must have A⊤ = A) but they they actually appear very often in both applications and pure mathematics. For example, in my own work, I rarely encounter non-symmetric matrices. There are (at least) two reasons for this: (i) For any matrix B we can form a symmetric matrix B⊤B from which we can study B, this is going to be the key idea behind the Singular Value Decomposition. (ii) In many instances, matrices represent relationship between objects — for example, Ai j can represent a friendship connection (or a similarity measure) between person (or data point) i and j and in many cases such relationships are symmetric. Exploratory Challenge 55. Recall the symmetric LU decomposition from the first part of the course, what can you say of such a decomposition for PSD matrices? How is it related to the Cholesky Decomposition? 8. SINGULAR VALUE DECOMPOSITION; AND SOME OPEN QUESTIONS IN LINEAR ALGEBRA 8.1. The Singular Value Decomposition. 29This is not the classical construction of the Cholesky Decomposition. The classical construction is with Gauss- ian Elimination, but at this stage of the course I think this is more transparent. Note also that when using Gaussian Elimination C will be a square matrix, while here R can be rectangular if M is not full rank (which makes it a more economical decomposition). 66 We are now reaching “the ultimate theorem of our class”, the Singular Value Decomposition (SVD). The SVD is a way to generalize the eigendecomposition to non-symmetric, and even non- square, matrices. Instead of eigenvalues we will have singular values and instead of eigenvectors we will have (right and left) singular vectors. Definition 8.1.1 (SVD — Singular Value Decomposition). Let A ∈ Rm×n. There exist orthogonal matrices U ∈ Rm×m and V ∈ Rn×n such that (40) A = UΣV ⊤, where Σ ∈ Rm×n is a diagonal matrix, in the sense that Σi j = 0 when i ̸= j, and the diagonal elements are non-negative and ordered in descending order. U ⊤U = I and V ⊤V = I. The columns u1, . . . um of U are called the left singular vectors of A and are orthonormal. The columns v1, . . . vn of V are called the right singular vectors of A and are orthonormal. The diagonal elements of Σ, σi = Σii are called the singular values of A and are ordered as σ1 ≥ · · · ≥ σmin{m,n}. Remark 8.1.2. If A has rank r we can write the SVD in a more compact form: (41) A = UrΣrV ⊤ r , where Ur ∈ Rm×r contains the first r left singular vectors, Vr ∈ Rn×r contains the first r right singular vectors and Σr ∈ Rr×r is a diagonal matrix with the first r singular values. Notice that storing such a decomposition in the computer requires storing r ×(m+n+1) real numbers rather than m × n real numbers which would be required to store A naively. When a matrix has small rank these are crucial savings. 30. Oftentimes the subscript is omitted and the compact SVD is simply written as UΣV ⊤ while spec- ifying the dimensions of the matrices involved to specify which form of the SVD is being consid- ered. Remark 8.1.3. Let A ∈ Rm×n and A = UΣV ⊤ be its SVD (as in (40)) then AA⊤ = U ( ΣΣ⊤)U ⊤, and so the left singular vectors of A, the columns of U, are the eigenvectors of AA⊤ and the singular values of A are the square-root of the eigenvalues of AA⊤ (note that ΣΣ⊤ is m × m diagonal). If m > n, A has n singular values and AA⊤ has m eigenvalues (which is larger than n), but the “missing” ones are 0. 30Taking this one step forward, when a matrix is well approximated by a low rank matrix, oftentimes one stores only a small rank approximation of a matrix A, this is a crucial idea in tasks ranging from Image Compressions, Numerical Analysis, and Machine Learning, see Section ??. 67 Analogously, A ⊤A = V ( Σ ⊤Σ)V ⊤, and so the right singular vectors of A, the columns of V , are the eigenvectors of A⊤A and the sin- gular values of A are the square-root of the eigenvalues of A⊤A (note that Σ⊤Σ is n × n diagonal). If n > m, A has m singular values and A⊤A has n eigenvalues (which is larger than m), but the “missing” ones are 0. This observation makes it easier to write the singular values and singular vectors of A in terms of eigenvalues and eigenvectors of AA⊤ and A⊤A, which are symmetric matrices (and directly implies, e.g., uniqueness of singular values; and the fact that the rank of a matrix is the number of nonzero singular values). In fact, the proof of the existence SVD will heavily rely on the Spectral Theorem. An important direct consequence of the SVD, and in particular of (41) is that we can write any rank-r matrix A ∈ Rm×n as a sum of r rank-1 matrices: Proposition 8.1.4. Let A ∈ Rm×n be a matrix with rank r. Let σ1, . . . , σr be the non-zero singular values of A, u1, . . . , ur the corresponding left singular vectors and v1, . . . , vr the cor- responding right singular vectors. Then (42) A = r ∑ k=1 σkukv ⊤ k . Challenge 56. The SVD is a powerful tool. Many of the things we did in this course become significantly simpler with the SVD. Now that you have the SVD, try to reread these notes and try to re-interpret the results we derived in terms of the SVD. For example, the Moore-Penrose Pseudoinverse has a very simple description of the SVD, it corresponds to swapping U and V and replacing the non-zero singular values by their inverses, while keeping the zero ones zero. Try to derive this! Theorem 8.1.5. [The SVD – the Ultimate Theorem of ETHZ 401-0131-00L] Every matrix A ∈ Rm×n has an SVD decomposition of the form (40). In other words: Every linear transformation is diagonal when viewed in the bases of the singular vectors. 68 Proof. Let Am×n. Let r be the rank of Am×n. We will build a compact SVD as in (41). It is easy to see that we can get an SVD in the sense of (40) from a compact one by adding singular values that are zero and extending the singular vectors in both Ur and Vr to orthonormal bases. By Theorem 7.3.1 and Corollary 7.3.2 the matrix AA⊤ has a complete set of orthonormal eigen- vectors and can be written as (43) AA ⊤ = UΛU ⊤, where U ∈ Rm×m is orthogonal and Λ is diagonal. Let us write (43) ordering the diagonal entries of Λ in decreasing order. Furthermore, let us write (43) also in a compact form, by keeping only the r non-zero eigenvalues (and corresponding eigenvectors), so AA ⊤ = UrΛrU ⊤ r for Ur ∈ Rm×r such that U ⊤ r Ur = I and Λr is r × r diagonal with the non-zero eigenvalues of AA⊤. By Proposition 7.3.16 the eigenvalues of AA⊤ are non-negative and so the diagonal entries of Λr are positive. Let Σr ∈ Rr×r be the diagonal matrix with diagonal entries σi := (Σr)ii = √ Λii. Our goal is to show that there is a n × r matrix Vr, with orthonormal columns, such that A = UrΣrV ⊤ r . We would have Σ−1 r U ⊤ r A = Σ−1 r U ⊤ r UrΣrV ⊤ r = V ⊤ r , or equivalently Vr = A⊤UrΣ−1 r . Motivated by this, let’s set Vr := A⊤UrΣ −1 r , this corresponds to a matrix with columns v1, . . . , vr given by vk = 1 σk A⊤uk. To conclude we need to show that this construction indeed gives a compact SVD, for this we still need to show two things: (1) V ⊤ r Vr = I. This can be verified by direct computation, while recalling that AA⊤ = UrΛrU ⊤ r : V ⊤ r Vr = ( A⊤UrΣ −1 r )⊤ A ⊤UrΣ−1 r = Σ −1 r U ⊤ r AA ⊤UrΣ −1 r = Σ−1 r U ⊤ r UrΛrU ⊤ r UrΣ−1 r = Σ −1 r ΛrΣ−1 r = I (2) A = UrΣrV ⊤ r . Note that UrΣrV ⊤ r = UrΣr ( A⊤UrΣ −1 r )⊤ = UrU ⊤ r A. Let us simply verify that A = UrU T r A. • Let x ∈ N(A). Then Ax = 0 = UrU T r x • Let x ∈ C(AT ). It follows that x = AT y for y ∈ Rm and hence, Ax = AA T y = UrΛrU T r y = UrIΛrU T r y = UrU T r UrΛrU T r y = UrU T r AA T y = UrU T r Ax. Hence, for all x ∈ Rn we have verified that Ax = UrU T r Ax. Then A = UrU T r A follows. 69 2 8.2. Vector and Matrix Norms. A short section on vector and matrix norms. So far, the norm of a vector x ∈ Rn was simply given by ∥x∥ = x⊤x but there are instances where it makes sense to measure the “lenght” of vectors in other ways. One popular way is called the “Manhattan distance” since when traveling in Manhattan one cannot take advantage of Pythagoras Theorem because that would involve cutting through buildings, that norm is given by ∥x∥1 = ∑ n i=1 |xi|. In general, for 1 ≤ p ≤ ∞ the ℓp norm is given by (44) ∥x∥p = ( n ∑ i=1 |xi|p)1/p , for p < ∞, and ∥x∥∞ = maxi |xi|. Notice that ∥ · ∥2 corresponds to the Euclidean norm that we have used in this course. 31 Challenge 57 (⋆). Prove that, for all x ∈ Rn, we have ∥x∥2 ≤ ∥x∥1 ≤ √ n∥x∥2. In several situations one also needs to “measure” the size of matrices (for example, when talking about a matrix being close to another one, we need a notion of distance, or norm of the difference). Definition 8.2.1 (Two matrix norms). Given a matrix A ∈ Rm×n we define two matrix norms: • ∥A∥F , known as the Frobenius norm, is defined as ∥A∥F = √ m ∑ i=1 n ∑ j=1 A2 i j, • ∥A∥op, known as operator or specral norm, is defined as ∥A∥op = max x∈Rn s.t.∥x∥=1 ∥Ax∥. Further Proposition 58. Given A ∈ Rm×n with singular values σ1 ≥ · · · ≥ σmin{m,n}. We have (1) ∥A∥ 2 F = Tr ( AT A) (2) ∥A∥ 2 F = min{m,n} ∑ i=1 σ 2 i (3) ∥A∥op = σ1 31The ℓ1 norm is notable for promoting sparsity when one attempts to minimize it to solve underdetermined linear systems. This is the key idea behind “Compressed Sensing”, and plays a crucial role in many imaging/sensing technologies. You can read more about it in Section 12 of [BM23] or Chapter 10 of [BSS] and references therein. 70 (4) ∥A∥op ≤ ∥A∥F ≤ √ min{m, n}∥A∥op. Challenge 59 (⋆). Prove Further Proposition 58. 32 8.3. Some Mathematical Open Problems. Now that we have covered the notion of eigenvalues and eigenvectors, there are a few fascinating open questions we can state. These are questions (or conjectures), that we currently do now know the answer to (or that we are not sure they are true). I have a list of 42 open problems in some lecture notes [Ban16] I wrote almost a decade ago, some have been solved in the meantime, but many remain open. I write below a few for which you have all the necessary background to understand: Let me know if you solve any of them; regardless of whether its days or decades from now, I will be very happy to hear about your solution! Conjecture 60 (Hadamard conjecture). For any n multiple of 4 there exists an Hadamard matrix H that is n × n. An Hadamard matrix H ∈ Rn×n is a matrix with only entries 1 or −1 that is a multiple of an orthogonal matrix. In other words Hi, j = ±1 for all i, j and H⊤H = nI. Yet in other words: the columns of H are an orthogonal basis for Rn formed with only vectors with entries ±1. Open Problem 61 (Mutually Unbiased Bases). See Open Problem 6.2 in [Ban16]. Conjecture 62 (Zauner’s Conjecture). See Open Problem 6.3 in [Ban16]. Conjecture 63 (Komlos Conjecture). See Open Problem 0.1 in [Ban16]. Conjecture 64 (Matrix Spencer Conjecture). See Open Problem 4.3 in [Ban16]. Open Problem 65 (Rank of the Matrix Multiplication Tensor). What is the rank of the Matrix Multiplication Tensor corresponding to multiplication of 3 × 3 matrices. A d1 × d2 × d3 tensor T is what we can think of as a cubic matrix. It has d1d2d3 entries given by Ti jl. We say T has rank r if r is the smallest integer such that we can write T = r ∑ k=1 ak ⊗ bk ⊗ ck, 32Hint: The order I chose for Further Proposition 58 was so that it is easiest to prove these properties in this order. This is a good exercise to help consolidate your understanding of the SVD, and other concepts in this course. 71 for ak ∈ Rd1, bk ∈ Rd2, ck ∈ Rd3, for k = 1, . . . , r. In other words Ti jl = r ∑ k=1(ak)i(bk) j(ck)l. Recall Proposition 8.1.4 to see why for matrices this corresponds to the notion of rank we have been using. While computing the rank of a matrix is computationally easy, doing so for tensors is notoriously difficult (because they lack a spectral theory of eigenvalues and eigenvectors). There is a way to think of Strassen’s algorithm (that you saw in a CS Lens in Part I of the course) in terms of a decomposition of a certain Tensor in terms of rank-1 tensors. In this description we focus on n × n matrices, but the same thing can be done for rectangular matrices. The n × n matrix multiplication tensor is a n2 × n2 × n2 tensor, where each dimension is indexed by pairs (i1, i2), ( j1, j2), (l1, l2) and T is given by T(i1,i2),( j1, j2),(l1,l2) = { 1 if i1 = j1, j2 = l1, l2 = i2 0 o.w. . Strassen’s algorithm can be viewed as the fact that the rank of the 2 × 2 matrix multiplication tensor (a 4 × 4 × 4 tensor) is ≤ 7. The rank of the 3 × 3 matrix multiplication tensor (a 9 × 9 × 9 tensor) remains unknown. 33 34 Conjecture 66 (The Paley ETF Conjecture). See Open Problem 6.4 in [Ban16] (see also Open Problem 5.1. in the same reference). 35 Conjecture 67 (Ellipsoid Problem). See Conjecture 1.1. in either https://arxiv.org/ pdf/2307.01181.pdf or https://arxiv.org/pdf/2310.05787.pdf.36 33To the best of my knowledge, the current “world records” for lower and upper bounds are 19 and 23, see for example https://mathoverflow.net/questions/249256/ best-known-bounds-on-border-ranks-of-small-matrix-multiplication-tensors? noredirect=1&lq=1 or https://mathoverflow.net/questions/151058/ best-known-bounds-on-tensor-rank-of-matrix-multiplication-of-3x3-matrices. 34See https://www.youtube.com/watch?v=fDAPJ7rvcUw for a very nice description of how AI methods found a better low rank decomposition for the matrix multiplication tensor for some dimensions, and https://www.nature.com/articles/s41586-022-05172-4 for the paper the video discusses (make sure to take a look at the table in Figure 3 in that article). 35This one needs some (light) Number Theory background to understand. I posed this one (with collaborators) and spent many hours trying to make progress on it. . . 36This one needs some (light) Probability Theory to understand 72 Acknowledgements. Many thanks to all the Linear Algebra teaching team! A particularly shout out to many of you that caught countless typos on these notes and gave countless suggestions that improve them! Thanks (a partial list): Bernd G¨artner, Sebastian Haslebacher, Stefan Kuhlmann, Felix Breuer, Till Schnabel, Mattia Taiana, Tanguiy Magne, Aviv Segall, Matthieu Croci, Sergey Prokudin, Mia Filic, Sofia Giampietro, Seyedmorteza Sadat, Mark Sosman, Elia Trachsel, Silvan Bolt. APPENDIX A. SOME IMPORTANT PRELIMINARIES AND REMARKS ON NOTATION To follow these notes the reader needs to be familiar with basics of vector and matrix operations and manipulations; understand what is dimension of a subspace, and in particular that is well- defined (that every basis of a subspace has the same size); and understand what is the rank of a matrix (and in particular that the dimension of the column space and the row space are the same). Even though Gaussian Elimination is not a core ingredient of this part of the course, we still assume that the reader is familiar with it. The students of 401-0131-00L are familiar with all this via Part I of this course. Some further important preliminaries and/or remarks: (1) The dot product x · y between two real valued vectors is sometimes also called inner product and written as ⟨x, y⟩ (it is equal to x⊤y). For Cn the inner product is given by ⟨x, y⟩ = y∗x. (2) Matrix Factorization for A an m × n matrix with rank r: A = CR, C is m × r with linearly independent columns (they are the first r linearly independent columns of A). R is r × n, it is upper triangular (i.e. Ri j = 0 if i > j), and it has an r × r identity as a submatrix, corresponding to the locations of the first r linearly independent columns of A. (3) For V a subspace (or a vector space) with dimension n the following holds: • Any basis of V has size n. • Any spanning set of V has size ≥ n. • Any spanning set of V with size n is also a basis. • Any set of linearly independent vectors in V has size ≤ n. • Any set of linearly independent vectors in V with size n is also a basis. 73 APPENDIX B. A “SIMPLE PROOF” OF THE FUNDAMENTAL THEOREM OF ALGEBRA In this appendix we present a brief sketch of a (relatively) simple proof of the Fundamental Theorem of Algebra I learned from Alessio Figalli. Let P(z) be a polynomial of degree n. Without loss of generality we can assume it is monic P(z) = zn + αn−1zn−1 + · · · + α0. Suppose P(z) has no zeros/roots. There is a r ∈ R large enough such that the infimum of |P(z)| inside the close disc Dr of radius r centered at zero is smaller than that outside the disc Dr (because far from the origin the term zn dominates and forces |P(z)| to be large outside of Dr. Since Dr is a compact set and |P(z)| is continuous it needs to attain its minimum 37 at a point z0 ∈ Dr. Note that P(z0) ̸= 0. Write Q(z) = P(z−z0), it is also a polynomial of degree n, Q(z) = β0 + β1z + β2z2 + · · · + zn. Notice that β0 = P(z0). let k be the first coefficient of Q(z) (not including β0) that is nonzero (meaning that βk ̸= 0 but βi = 0 for all 0 < i < k. Then Q(z) = β0 + βkzk + βk+1zk + · · · . Take ε > 0 arbitrarily small and consider Q ( ε ( − β0 βk ) 1 k ) . It is not difficult to see that for ε small enough the higher order terms are negligible and the term β0 + βkzk has smaller modulus and so one can pick ε such that ∣ ∣ ∣ ∣Q ( ε ( − β0 βk ) 1 k )∣ ∣ ∣ ∣ < |Q(0)| which is a contradiction with the fact that |P(z0)| was minimum. References 38 [Ban16] Afonso S. Bandeira. Ten lectures and forty-two open problems in the mathematics of data science. Available online at: https://people.math.ethz.ch/˜abandeira/ TenLecturesFortyTwoProblems.pdf. See also https://ocw.mit.edu/courses/ 18-s096-topics-in-mathematics-of-data-science-fall-2015/, 2016. [BM23] Afonso S. Bandeira and Antoine Maillard. Mathematics of signals, networks, and learning. Available online at: https://anmaillard.github.io/teaching/msnl_spring_2023.pdf. Videos from an earlier version of the course available at https://youtube.com/playlist?list= PLiud-28tsatL0MbfJFQQS7MYkrFrujCYp, 2023. [BSS] A. S. Bandeira, A. Singer, and T. Strohmer. Mathematics of data science. Book draft available at https: //people.math.ethz.ch/˜abandeira/BandeiraSingerStrohmer-MDS-draft. pdf. Videos available at: https://www.youtube.com/playlist?list= PLiud-28tsatIKUitdoH3EEUZL-9i516IL. [Str23] Gilbert Strang. Introduction to Linear Algebra. (Table of contents available at https://math.mit. edu/˜gs/linearalgebra/ila6/indexila6.html). Wellesley - Cambridge Press., sixth edi- tion, 2023. 37This part needs some extra analysis/topology background: the fact that continuous functions on a compact (think closed and bounded) set needs to attain a minimum. You will learn about this in Analysis. Perhaps not surprisingly, the “other proof” of Corollary 7.3.8 that does not involve complex numbers, also needs this fact. 38In some PDF viewers the ∼ in the urls above does not show as the correct character, if the link appears broken delete the ˜ and write a new ∼ on the url.","libVersion":"0.5.0","langs":""}