{"path":"sem3/LinAlg/VRL/extra/plans/LinAlg-plan-w02.pdf","text":"Lecture plan Linear Algebra (401-0131-00L, HS24), ETH Z ¨urich Numbering of Sections, Definitions, Figures, etc. as in the Lecture Notes Week 2 Matrices and linear combinations (Section 2.1) Matrix: Notation for sequence of vectors:     1 3 5   ,   2 4 6     →   1 2 3 4 5 6   → ( [1 2] , [3 4] , [5 6]) Gives another sequence of (row) vectors. Definition 2.1: An m × n matrix is a rectangular array of real numbers with m rows and n columns. A =      a11 a12 · · · a1n a21 a22 · · · a2n ... ... . . . ... am1 am2 · · · amn      aij: entry in row i, column j Dot-free notation: A = [aij]m n i=1,j=1 R m×n: set of m × n matrices Column notation: A =   | | | v1 v2 · · · vn | | |   Row notation: A =     |u1||u2|...|um|      Column vector v ∈ R m: m × 1 matrix Row vector u ∈ R n: 1 × n matrix Matrix addition, matrix scalar multiplication: Definition 2.2 [1 2 3 4 ] + [5 6 7 8 ] = [ 6 8 10 12 ] , 2 [ 1 2 3 4 ] = [2 4 6 8 ] . 0: zero matrix (all entries are 0) 1 Matrix shapes: mnnmmntall and skinnysquareshort and widem > nm = nm < n Square matrices: Definition 2.3  1 0 0 0 1 0 0 0 1    2 0 0 0 4 0 0 0 5     2 1 0 0 4 7 0 0 5     2 0 0 1 4 0 0 7 5     2 1 0 1 4 7 0 7 5   identity I diagonal upper triangular lower triangular symmetric aij = δij j ̸= i : aij = 0 j < i : aij = 0 j > i : aij = 0 aij = aji Kronecker delta: δij = 1 if i = j and 0 otherwise. m × m identity matrix I = [δij] m m i=1,j=1 Matrix-vector multiplication: Notation for linear combination of the columns 7   1 3 5   + 8   2 4 6   ︸ ︷︷ ︸ linear combination =   1 2 3 4 5 6   [ 7 8 ] . ︸ ︷︷ ︸ matrix-vector product Definition 2.4: Let A =   | | | v1 v2 · · · vn | | |   ∈ R m×n, x =      x1 x2 ... xn      ∈ R n. The vector Ax := n∑ j=1 xjvj ∈ R m is the product of A and x. Direct definition (without columns): Observation 2.5 u1 u2 ... um      a11 a12 · · · a1n a21 a22 · · · a2n ... ... . . . ... am1 am2 · · · amn           x1 x2 ... xn      =      a11x1 + a12x2 + · · · + a1nxn a21x1 + a22x2 + · · · + a2nxn ... am1x1 + am2x2 + · · · + amnxn      u1 · x u2 · x ... um · x v1 v2 · · · vn x1v1 + x2v2 + · · · + xnvn 2 Corollary 2.6: Ix = x for all x ∈ R m. Definition in row notation: Observation 2.7 A =     |u1||u2|...|um|      , Ax =      u1 · x u2 · x ... um · x      ︸ ︷︷ ︸ scalar products . Pictorial view: AxAxnm=mn Column space and rank: Definition 2.8: Let A be an m × n matrix. The column space or image C(A) of A is the span (set of all linear combinations) of the columns, C(A) := {Ax : x ∈ R n} ⊆ R m. x = 0 ⇒ 0 ∈ C(A). Fact 1.5: C ([ 2 3 3 −1 ]) = Span ([ 2 3 ] , [ 3 −1 ]) = R 2. Independent columns: Definition 2.9: Let A =   | | | v1 v2 · · · vn | | |   . Column vj is independent if vj is not a linear combination of v1, v2, . . . , vj−1. Otherwise, vj is dependent. The rank of A, rank(A), is the number of independent columns. rank(A) = n: linearly independent columns rank(A) = 0: zero matrix rank ([ 2 4 3 1 ]) = 2 xy [ 2 3 ][ 4 1 ] rank ([ 2 4 3 6 ]) = 1 xy [ 2 3 ][ 4 6 ] both columns are independent only first column is independent 3 Later: Reordering columns does not change the rank. The independent columns span the column space: Lemma 2.10: Let A be an m×n matrix with r independent columns, and let C be the m×r submatrix containing the independent columns. Then C(A) = C(C). Proof. u1, u2, . . . , ur: the independent columns. w1, w2, . . . , wn−r: the dependent columns (order as in A) For all j, wj is a linear combination of u1, u2, . . . , ur, w1, w2, . . . wj−1: sequence contains all previous columns. Take Span(u1, u2, . . . , ur) ︸ ︷︷ ︸ C(C) , add w1, w2, . . . , wn−r → Span(u1, u2, . . . , ur, w1, w2, . . . , wn−r) ︸ ︷︷ ︸ C(A) . Adding linear combinations of previous vectors never changes the span (Lemma 1.23)! Row space and transpose: Row space: span of the rows R ([ 2 4 3 1 ]) xy [ 2 4 ][ 3 1 ] R ([ 2 4 3 6 ]) xy [ 2 4 ][ 3 6 ] Later: rank, number of independent columns = row rank, number of independent rows To define row space, independent row, (row) rank: • Copy& Paste from column definitions? • Use transpose matrices! Mirroring a matrix along the diagonal: ↔    1 2 3 4 5 6       1 2 3 4 5 6    A = [1 2 3 4 5 6 ] ↔ A⊤ =  1 4 2 5 3 6   Definition 2.11: Let A = [aij]m n i=1,j=1 be an m × n matrix. The transpose of A is the n × m matrix A ⊤ := [aji] n m i=1,j=1. 4 Row vector ↔ column vector:  1 3 5   ⊤ = [ 1 3 5 ] (A⊤)⊤ = A. Observation 2.12 A symmetric ⇔ A = A ⊤. Definition 2.13: Let A be an m × n matrix. The row space R(A) of A is the column space of the transpose, R(A) := C(A ⊤). Matrix multiplication (Section 2.2) Matrix multiplication: Notation for several linear combinations of the columns Definition 2.16: Let A be an a × n matrix and B =   | | | x1 x2 · · · xb | | |   an n × b matrix. The a × b matrix AB :=   | | | Ax1 Ax2 · · · Axb | | |   is the product of A and B. AB is defined exactly if number of columns of A = number of rows of B. v = [2 3 ] , w = [ 3 −1 ] : λ µ λv + µw −3 2 [ 0 −11 ] 1 −1 [−1 4 ] 3 0 [ 6 9 ] [2 3 3 −1 ] ︸ ︷︷ ︸ A [−3 1 3 2 −1 0 ] ︸ ︷︷ ︸ B = [ 0 −1 6 −11 4 9 ] ︸ ︷︷ ︸ AB Direct definition: “Rows of A times columns of B” Observation 2.17     |u1||u2|...|ua|      ︸ ︷︷ ︸ A   | | | x1 x2 · · · xb | | |   ︸ ︷︷ ︸ B =      u1 · x1 u1 · x2 · · · u1 · xb u2 · x1 u2 · x2 · · · u2 · xb ... ... . . . ... ua · x1 ua · x2 · · · ua · xb      ︸ ︷︷ ︸ AB (ab scalar products) = [ui · xj]a b i=1,j=1 5 AB = [ 1 2 3 4 ] [ 0 1 1 0 ] = [1 · 0 + 2 · 1 1 · 1 + 2 · 0 3 · 0 + 4 · 1 3 · 1 + 4 · 0 ] = [2 1 4 3 ] (”column exchange in A”) BA = [0 1 1 0 ] [ 1 2 3 4 ] = [0 · 1 + 1 · 3 0 · 2 + 1 · 4 1 · 1 + 0 · 3 1 · 2 + 0 · 4 ] = [3 4 1 2 ] (”row exchange in A”) AB ̸= BA, matrix multiplication is not commutative. (AB) ⊤ = B⊤A ⊤ Lemma 2.19 Corollary 2.20: Let I be the m × m identity matrix. Then IA = A for all m × n matrices, and AI = A for all n × m matrices. Everything is matrix multiplication: Matrix-vector multiplication: [1 2 3 4 ] ︸ ︷︷ ︸ 2×2 [1 1 ] ︸︷︷︸ 2×1 = [3 7 ] ︸︷︷︸ 2×1 . Vector-matrix multiplication: [1 1] ︸ ︷︷ ︸ 1×2 [1 2 3 4 ] ︸ ︷︷ ︸ 2×2 = [4 6] ︸ ︷︷ ︸ 1×2 . Scalar product: [ 1 2] ︸ ︷︷ ︸ 1×2 [3 4 ] ︸︷︷︸ 2×1 = [ 11 ] ︸︷︷︸ 1×1 = 11. ⇒ Another scalar product notation: v · w = v⊤w Outer product: [ 3 4 ] ︸︷︷︸ 2×1 [ 1 2] ︸ ︷︷ ︸ 1×2 = [ 3 6 4 8 ] ︸ ︷︷ ︸ 2×2 . Lemma 2.21 Let A be an m × n matrix. The following two statements are equivalent. (i) rank(A) = 1. (ii) There are nonzero vectors v ∈ R m, w ∈ R n such that A is their outer product, A = vw⊤. 6 Distributivity and associativity: Lemma 2.22: Let A, B, C be three matrices Whenever the sums and products are defined, then (i) A(B + C) = AB + AC and (A + B)C = AB + AD (distributivity); (ii) (AB)C = A(BC) (associativity). Generalized associativity: brackets don’t matter, also with more matrices (needs a sepa- rate proof): (AB)(CD) = A((BC)D) = · · · = ABCD CR decomposition: Lemma 2.23: Let A be an m × n matrix of rank r (Definition 2.9). Let C be the m × r submatrix of A containing the independent columns. Then there exists a unique r × n matrix R such that A = CR. Example, r = 1 (we get outer product form, see Lemma 2.21): [ 2 4 6 3 6 9 ] ︸ ︷︷ ︸ A,2×3 = [2 3 ] ︸︷︷︸ C,2×1 [1 2 3 ] ︸ ︷︷ ︸ R,1×3 . Proof. A and C have the same column space (Lemma 2.10) ⇒ Column vj of A is a linear combination of the columns of C: vj = Cxj (xj ∈ R r) A =   | | | v1 v2 · · · vn | | |   = C   | | | x1 x2 · · · xn | | |   ︸ ︷︷ ︸ R∈Rr×n = CR. C has linearly independent columns (Corollary 1.20) ⇒ The vectors xj and hence R is unique (Lemma 1.21). Example, r = 2: columns of A v1 v2 v3 v4==== A =   1 2 0 3 2 4 1 4 3 6 2 5    1 2 3     2 4 6     0 1 2     3 4 5  ====v1 1v1 2v1 3v1 v2 v3 1v3 −2v3 v4 independent? yes no yes no   1 2 0 3 2 4 1 4 3 6 2 5   ︸ ︷︷ ︸ A, 3×4 =   1 0 2 1 3 2   ︸ ︷︷ ︸ C, 3×2 [1 2 0 3 0 0 1 −2 ] ︸ ︷︷ ︸ R, 2×4 7","libVersion":"0.5.0","langs":""}