{"path":"sem2/DDCA/PV/summaries/DDCA-FS19-summary-lbürgi.pdf","text":"1 Verilog 1.0.0.1 case sensitive 1.0.0.2 whitespace is ignored 1.0.0.3 indenti˝ers may not start with number 1.0.0.4 single line comments // 1.0.0.5 multiline comments /* */ 1.0.0.6 continuous assignment \u0010assign\u0011 describes wires, not used in always block or other procedural blocks. assign bla = blu; deassign bla. latest assign wins, other assignments are ignored. 1.0.0.7 blocking assignment = assignment is done before next statement 1.0.0.8 nonblocking assignment <= assignments are done at end of block all at once, i.e. values can not propagate 1.0.0.9 multiple declarations can declare multiple things of same type on same line: r e g [ 3 : 0 ] a , b , c ; 1.0.0.10 parameter type constant values that are evaluated at compile-time 1.0.0.11 types physical: nets (wire and others, default z), reg- isters (reg, default x), charge storage node. abstract: integer, time, ˛oating point (real), parameter, event 1.0.0.12 literals 1, 0, z, x or N'Bxx for number of bits N. N is optional and defaults to 32. base B. xx is the number in the base, it can have x and z values and may contain _ for formatting. \u0010string literal\u0011 can be assigned to physical types (8bit char?) 1.0.0.13 signal strength 8 strength levels, stronger one domi- nates in case of contention, x if none wins. 1.0.0.14 arrays reg [n:0] a[m:0]; creates (m + 1) long array of (n + 1) long registers. there are no true multi-dimensional arrays. start/end index arbitrary but consistent. module < i d e n t i f i e r > #(parameter < i d e n t i f i e r > = <d e f a u l t value >) (<params >); code ; endmodule params may have direction in param list or later in code or even missing. i n p u t a , b , c ; valid. <module name> <new i d e n t i f i e r > ( params ) ; params can either be ordered and without the names of the params in the module, or unordered with .<module param name>(<what we c o n n e c t t o i t >) params separated by commas. or with parameters: module name> #(<params >) <new i d e n t i f i e r > ( params ) ; and again with the same syntax for named/unnamed parameters 1.0.0.17 blocks sequential: begin+end, parallel: fork+join. always @ (< s e n s i t i v i t y l i s t >) s t a t e m e n t ; always @ ∗ s t a t e m e n t ; sensitivity list can be * for everything that matters or for example p os e dg e c l k , negedge r e s e t can only assign to registers, should use <= nonblocking assign- ments. describes combinational logic if all right hand signals are in sensitivity list (*) and all left hand side signals are assigned in every condition. otherwise latches are implied for unassigned left hand side signals 1.0.0.19 case blocks only inside always blocks c a s e ( data ) < l i t e r a l >: <statement >; <... > d e f a u l t : <statement >; // r e q u i r e d e n d c a s e 1.0.0.20 casex like case but can check for x 1.0.0.21 if-else blocks only in always blocks 1.0.0.22 loops while ( expr ) \\\\ statement; for ( init ; expr ; step ) \\\\ statement; repeat ( constant ) \\\\ statement; forever \\\\ statement; 1.0.0.23 initial block for testing, runs once at beginning. if multiple, run concurrently. 1.0.0.24 testing $display(\u0010printf() style message!\"); #10 // wait 10 ns $readmemb(\"example.tv\", testvectors); // text ˝le containing bi- nary $readmemh(\"example.tv\", testvectors); // hexadecimal 1.0.0.25 gate primitives and, or, nand, nor, xor, xnor, buf, not, strange ones 1.0.0.26 operators and precedence ~ not *, /, % arithmetic +, - arithmetic <\u0017<, >\u0017> shift <\u0017<\u0017<, >\u0017>\u0017> arithmetic shift <, <=, >=, > comparison ==, != equality &, ~& and, nand ^, ~^ xor, xnor |, ~| or, nor a ? b : c if a then b else c binary logical operators can also be used as unary operators and then operate on all bits of the operand. left to right. 1.0.0.27 buses wire [n:0] <identi˝er>; higher number ˝rst for msb bus. indexing as <indeti˝er>[i] or <identi˝er>[i:j] with both i and j inclusive. concatenation using {}. 1.0.0.28 structural vs behavioural structural hdl is the de- scription of the structure of the building blocks, behavioural is the description of the function of the circuit and is thus more abstract 1.0.0.29 synchronous sequential logic use always@(posedge clk) and non-blocking assignments 1.0.0.30 combinational logic use assign for simple logic and always@(*) with blocking assignments otherwise 2 By abstraction level 2.0.0.1 programming model how the programmer speci˝es the program, for example sequential, von neumann, spmd, simd, data˛ow, etc. 2.0.0.2 hardware executing model how the hardware exe- cutes the program, for example ooo, vector, data˛ow, etc. 2.1 programming level 2.1.0.1 pragma keyword in programming language that can be used to specify information about how to execute a piece of code (branch prediction, parallelization, etc.) 2.2 isa level 2.2.1 isa instruction set architecture, the language of the computer, for ex- ample mips, x86, arm. usually von neumann model, rarely data ˛ow model. speci˝es: memory organization, register set, instruction set, data types (2's complement integers, unsigned, ˛oating point, ...) 2.2.2 von neumann model dominant isa model. works sequentially, with a pc, data and in- struction in same memory, etc. details below in microarchitecture section 2.2.3 data ˛ow model other isa model. no instruction pointer, instruction specify what data they need and produce and are executed as soon as possible. data ˛ow nodes can be unary/binary computations, conditional, re- lational, barrier synch (produce result when all inputs are available), etc. not successful at isa level, but very successful on microarchitecture level (hidden from programmer). 2.2.3.0.1 good at exploiting irregular parallelism, everything than can be done in parallel is done in parallel 2.2.3.0.2 bad no precise state semantics (debugging, inter- rupt+exception handling di˚cult) parallelism is not controlled high bookkeeping overhead 2.2.4 types of instructions (lc-3+mips) operate (use alu, from/to registers), data movement (memory ↔registers), control ˛ow (do something with pc) 2.2.5 ieee ˛oating point exception requires status bits to be set on exception and only reset on demand 2.2.6 endianness order of bytes or bits, big endian is the ordering of a decimal digits in a number, little endian the reverse 2.2.7 virtual memory most modern isas have support for virtual memory. it requires both software and hardware support. it's basically a fully associative cache and has similar issues to caching 2.2.7.1 physical memory only managing memory manually would be hard because programmers/compilers would need to man- age the address space especially between multiple processes (written by di˙erent people at di˙erent times) and the addresses would be ˝xed in the program at compile time. 2.2.7.2 mmu memory management unit the hardware part of virtual memory support 2.2.7.3 address translation translates from addresses in a program to real addresses, can be implemented by hardware and software together 2.2.7.4 page consecutive piece of virtual memory. a virtual page is mapped to a frame or a location on disk 2.2.7.5 page size speci˝ed by isa, usually a few sizes are avail- able. 2.2.7.6 virtual address most signi˝cant bits are the virtual page number and the least signi˝cant bits the page o˙set 2.2.7.7 vpn virtual page number virtual addresses beginning with this number are mapped to a continuous space in memory or on disk 2.2.7.8 page o˙set the part of the virtual addresses that maps to physical memory unchanged 2.2.7.9 physical address consists of physical page number and o˙set 2.2.7.10 pfn/ppn physical frame/page number the most signi˝cant bits of the physical address. shorter (or equal) to vpn 2.2.7.11 frame consecutive piece of physical memory, same size as page 2.2.7.12 page fault miss, page not in memory but on disk. causes an exception handled by the os. 2.2.7.13 paged virtual memory virtual addresses are trans- lated to physical ones by the mmu. whole pages are mapped to contiguous physical memory. 2.2.7.14 page table maps virtual addresses to physical ones per process. somewhat like tag store. if an address is not in main memory it only contains a note that it is not available. the os is completely responsible for maintaining it. has additional bits for validity, dirtiness, replacement policy. is pretty large. leaves o˙set untouched, just translates vpn to ppn. 2.2.7.15 pte page table entry 2.2.7.16 tlb translation lookaside bu˙er hardware cache for the page table. small, high associativity. because of relatively large page sizes high hit rate. index is lower bits of vpn, tag the rest of vpn and process id. can be hardware managed (x86) or software managed (mips). hardware is less ˛exible but much faster. 2.2.7.17 memory protection by mapping virtual addresses only to physical addresses that a program may access, prevent it from accessing memory it shouldn't. provide further access control per page, for example to enable read only memory shared between processes. 2.2.7.18 copy on write optimization os's job, save memory that is identical in multiple processes 2.2.7.19 page replacement algorithms modern systems use approximations of lru least recently used and more sophisticated frequency based algorithms 2.2.7.19.1 clock ~ circular list of physical frames. pointer/- hand to the last examined frame. when accessing a frame, set R bit in frame. when searching for eviction victim traverse list clockwise, reset passed R bits and evict the ˝rst frame which doesn't have R bit set. 2.2.7.20 multi level page tables ˝rst level in physical mem- ory, second not necessarily 2.2.7.21 ptbr page table base register has physical address of page table for current virtual address space 2.2.7.22 translating before/after cache there is the possi- bility to index caches with virtual or physical addresses. virtually addresses cache can lead to inconsistencies if same physical address is present multiple times or if the same virtual address is used for di˙erent physical addresses by di˙erent processes. virtually-indexed physically tagged: index cache and tlb using vir- tual address bits but use physical address bits for tag in cache. query cache and tlb concurrently and check if tag from cache matches tag from tlb. 2.2.7.23 synonym problem when translating after cache and two virtual addresses map to the same physical address. solutions: • use page o˙set for cache index • ensure index(VA)=index(PA) • on write, search all cache indices that could contain same phys- ical block and invalidate 2.2.7.24 hybrid physical memory systems combine DRAM and NVM SSD into single virtual memory 2.2.7.25 dma direct memory access when a device like a hard disk is allowed to write it response directly into main mem- ory. happens on page faults. 2.2.8 word group of bits, size usually a power of two 2.2.9 word-adressable if the addresses refer to words and whole words at a time are read- /written. pc usually incremented by one 2.2.10 byte-addressable if the addresses refer to bytes. pc usually incremented by wordsize in bytes 2.2.11 address space total number of addresses 2.2.12 addressing modes 2.2.12.0.1 immediate/literal address in instruction 2.2.12.0.2 register address in register 2.2.12.0.3 pc-relative address relative to incremented pc 2.2.12.0.4 indirect ˝nal address is at other address in mem- ory given in pc-relative form 2.2.12.0.5 base(+o˙set) base is in a register, o˙set is an immediate. in mips, base is in parenthesis and o˙set before 2.2.13 storage of matrix 2.2.13.0.1 row major row elements laid out consecutively 2.2.13.0.2 column major column elements laid out consec- utively 2.2.14 condition code bits that are written/cleared additionally when a result is written, for example if positive, zero, negative. 2.2.15 mips 2.2.15.0.1 r-type instructions 3 register operands, format: opcode=0, rs (source), rt (source), rd (destination), shamt (0 except for shift ops), funct (which r-type instr) 2.2.15.0.2 i-type 2 register operands, one literal, format: op, rs, rt 2.2.15.0.3 j-type opcode, address 2.2.16 data dependence types raw read after write, ˛ow dependence, real dependence war write after read, anti dependence, can't overwrite old value because we still need it waw write after write, output dependence 2.2.17 loop unrolling put multiple loop iterations in loop body together. unroll factor is amount of repetitions. reduces loop control overhead, enlarges basic block whose schedule can more easily be optimized. if the unroll factor doesn't divide the amount of overall executions needs additional logic for that and it increases code size. 2.2.18 speedup speedup × newtime = oldtime 2.2.19 exception caused by thread itself. handled immediately (if not caused by spec- ulative execution). handled in context of own thread, with priority of that thread. 2.2.20 interrupt caused by something external to the current thread. handled when convenient (except for high priority ones like power failure). handled in context of whole system, with varying priorities. 2.2.21 precise exceptions and interrupts being precise means that they leave the computer in the state it should be in according to isa and hide anything else that might have been going on under the hood. this is important because it's required by the von Neumann model, it aids in software debugging, makes recovery easier, makes restarting a process easier, enables traps into software (software implemented opcodes) 2.2.22 ˛ynn taxonomy of computers categorized into {single instruction , multiple instructions } × {single data , multiple data }. SISD, SIMD, MISD, MIMD. 2.3 microarchitecture level 2.3.0.1 microarchitecture way a given ISA is implemented in a particular processor 2.3.0.2 von neumann architecture data and instructions in same memory and sequential instruction processing. components: memory (address register mar and data register mdr), process- ing unit (alu, temp), control unit (program counter or instruction pointer for current instruction, instruction register contains current instruction). to read memory write address to mar and read data from mdr, to write memory write address and data to mar and mdr and drive write enable line. 2.3.0.3 instruction cycle implemented by fsm in control unit 1. fetch: instruction from memory into instruction register. put pc into mar, increment pc, interrogate memory, put mdr into instruction register 2. decode: decoder that takes op bits from instruction and enables the matching instruction circuitry 3. evaluate address: calculate the address of the data word that needs to be read from memory (for example load with base address and o˙set) 4. fetch operands: load operand either from memory or register, the latter usually happens at the same time as the decode 5. execute: execute instruction (use alu, change pc, ...) 6. store result: write to destination or alternatively IF instruction fetch ID/RF instruction decode and register operand fetch EX/AG execute/evaluate memory address MEM memory operand fetch WB store/writeback result which are the pipeline stages from the pipeline that is most discussed in the lecture 2.3.1 single cycle microarchitecture each instruction takes one cycle to execute 2.3.1.1 single cycle processor diagram https://safari.ethz.ch/digitaltechnik/spring2019/lib/exe/fetch.php?media=onur- digitaldesign-2019-lecture11-microarchitecture-i-afterlecture.pdf page 103 2.3.1.2 single cycle mips state elements 2.3.1.2.1 program counter register 2.3.1.2.2 instruction memory input address, output the in- struction 2.3.1.2.3 register ˝le registers, three address inputs, two read outputs, one write input, one write enable input 2.3.1.2.4 data memory address input, write enable input, write data input, read data output 2.3.2 multi cycle microarchitecture each stage in the instruction cycle takes one clock cycle to execute 2.3.2.1 multi cycle processor diagram https://safari.ethz.ch/digitaltechnik/spring2019/lib/exe/fetch.php?media=onur- digitaldesign-2019-lecture12-microarchitecture-ii-afterlecture.pdf page 92 2.3.3 multi cycle execution have multiple execution units in the instruction cycle, they can ei- ther be pipelined or not, and have some of them use multiple cycles 2.3.4 instruction level concurrency overview 2.3.4.0.1 pipelining 2.3.4.0.2 out-of-order execution 2.3.4.0.3 data˛ow (isa level) see above and also ooo is ba- sically local data ˛ow 2.3.4.0.4 superscalar execution handle multiple instruc- tions per cycle (i.e. multiple instructions in each stage of instruction cycle). can be in-order or out-of-order. can provide higher ipc, but at cost of more dependence checking and other hardware resources 2.3.4.0.5 vliw very long instruction word is an instruc- tion that consists of multiple other instructions that were packed together by the compiler/programmer. they are independent and aligned so that the hardware can skip checks and executes them con- currently and in lock-step (i.e. they are made to all start and ˝nish at the same time). eli enormously longword instruction (512bits). basically moves complexity from cpu to compiler, leads to simple hardware, but compiler needs to be able to make suitable packs of instructions (independent and similar latency), the compilation product is very speci˝c to the exact processor, latency needs to be known at compile time as much a possible 2.3.4.0.6 fgmt ˝ne-grained multithreading use hardware threads (each has pc and registers), switch between threads every cy- cle so that after an instruction from one thread there is enough time before the next instruction of the same thread to avoid stalls. good: no need for either dependence checking nor branch prediction, better utilization, throughput, latency tolerance. bad: extra complexity, reduced single/few thread performance, resource contention between threads (cache/memory), dependency checking between threads. 2.3.4.0.7 simd single instruction multiple data 2.3.4.0.8 simt single instruction multiple threads simd implemented using threads. 2.3.4.0.9 spmd single program multiple data very much like simt 2.3.4.0.10 systolic arrays the basic idea is to have multiple pe processing elements arranged in n dimensions that load one unit of data from memory, do a chain of computations on it and then write the result back. the pe are as regularly arranged and simple as possible and their processing time is matched to memory latency so that memory is not always the bottleneck. the pe preferably but not necessarily have predictable latency. each pe can have local memory and execute its own kernel, but many do not. if each pe is powerful and ˛exible leads to • staged execution in general. • stream processing in particular. • pipeline parallelism in particular. here each pipeline stage is basically a whole processor. loop iterations are divided into parts called stages and the stage executed on di˙erent cores using threads example applications: • convolution. given weights w1, · · · , wk and input x1, · · · , xn compute yi = ∑k j=1 w1xi+j−1 for 1 ≤ i ≤ n + 1 − k. used for ˝ltering, pattern matching, correlation, polynomial evalu- ation, image processing tasks, convolutional neural networks. implemented using one pe per weight for addition and multipli- cation, passing through input and output in opposite directions through a linear array. • least-squares ˝t on-the-˛y as data arrives • orthogonal triangularization • solving triangular linear systems good: computation and memory speed balanced, high performance, concurrency and e˚ciency, simple design bad: specialized, not good at exploiting irregular parallelism 2.3.4.0.11 decoupled access execute separate program into E execution and A (memory) access parts and run both parts on separate hardware in parallel. A end E communicate using isa- visible queues. invented in time when tomasulo's algorithm was too complex to implement. good: E and A can run ahead of each other, queues reduce amount of needed registers, limited ooo execution at cheap cost bad: the E and A streams need to be synchronized on branches. compiler support to achieve as much decoupling as possible (amount of ˛exibility between A and E) 2.3.5 pipelines 2.3.5.1 pipeline timing for n stages and original delay T the bandwidth is: ideal ( T n )−1, latch delay S ( T n + S)−1 2.3.5.2 pipeline design problems • balancing: distributing work evenly among pipeline stages • keep pipeline correct, moving and full in the face of data and control dependences, resource contention and long latency op- erations • exceptions, interrupts • minimizing stalls 2.3.5.3 interlocking detection of dependences in a pipelined processor to guarantee correctness. can be done in software or hard- ware. without data forwarding, there needs to be an empty cycle between the write cycle of the dependence and the dependent exe- cute cycle. 2.3.5.4 scoreboarding interlocking algorithm. stall until safe. while an instruction is in the pipeline that writes a register, that reg- ister is marked. instructions that read/write this register are stalled until the register is unmarked. wikipedia has a more complicated algorithm with this name. 2.3.5.5 combinational dependence check logic interlocking method. during decoding, have logic that checks whether any in- struction in later stages is going to write to a source register of cur- rent instruction. if yes, stall. more complicated than scoreboarding and becomes more complicated the deeper the pipeline is 2.3.5.6 data forwarding/bypassing forward data produced in pipeline stage (memory or writeback) to other instruction in ear- lier stage (execute). concept from data ˛ow model. the dependent execute stage happens in the cycle after the stage which does for- warding. done if the later stage is going to write to a register that the earlier stage reads, if both memory and writeback stages match then the value from the memory stage is taken because that value is the output of a more recent instruction that is going to overwrite the earlier value. 2.3.5.7 internal register ˝le forwarding instructions write to registers in ˝rst half of cycle and other instructions can access the value in the second half of the same cycle 2.3.5.8 pipeline stall hardware fetch and decode stages need a pause switch and the execute stage needs a clear input so that it executes a nop (sends a bubble along the pipeline) instead of the data from the previous stage (which is supposed to stall) 2.3.5.9 humps (in a pipeline) there are two, ˝rst consists of reservation stations (also: scheduling window), second consists of the rob (also: instruction window, active window) 2.3.6 pipeline stall prevention 2.3.6.1 compile time scheduling/reordering/dependence elimination remove dependence at compile time by inserting nops or equivalently reordering instructions 2.3.6.2 value prediction 2.3.6.3 fgmt ˝ne-grained multithreading 2.3.6.4 rob reorder bu˙er, out of order completion a bu˙er where results of instructions go before being applied (writ- ten to registers, memory). with rob and in-order pipeline, there is no need to stall to make instructions complete in order, or can even do ooo dispatch order too (see below). each entry contains everything needed to apply the results later in the correct oder or to discard it. entry: V, DestRegID, DestRegVal, StoreAddr, Store- Data, PC, valid bits for reg/data and control bits, exception status. implemented among other things by storing ids of rows in register ˝le next to the registers which belong to the rows. 2.3.6.4.1 rob tradeo˙s good conceptually simple, can also eliminate false depen- dences (register renaming) bad rob needs to be accessed which implies content address- able memory or indirection 2.3.6.4.2 register renaming with rob a rob can be used to rename registers. the register id maps to the rob entry that contains or will contain the register content. then the rob entry id is used to refer to this register. 2.3.6.4.3 in order pipeline with rob in-order dispatch, out-of-order completion, in-order retirement. pipeline steps: decode Access reg˝le/ROB, allocate entry in ROB, check if in- struction can execute, if so dispatch instruction execute Instructions can complete out-of-order completion Write result to reorder bu˙er retirement/commit Check for exceptions; if none, write result to architectural register ˝le or memory; else, ˛ush pipeline and start from exception handler 2.3.6.5 ooo/out-of-order dispatch/execution see below, not just useful in pipelines but to eliminate waits in general 2.3.7 ooo/out-of-order dispatch/execution put dependent instructions into reservation stations to wait while executing independent later instructions. also needs rob to restore order at the end. improves latency tolerance (few long latency oper- ations don't necessarily block program ˛ow). note similarity to data ˛ow model (basically the data ˛ow graph is built for the instructions in the instruction window). image: https://safari.ethz.ch/digitaltechnik/spring2019/lib/exe/fetch.php?media=onur- digitaldesign-2019-lecture15b-out-of-order-execution- afterlecture.pdf page 16 2.3.7.0.1 reservation station place for dependent instruc- tions to wait. separate per execition unit (e.g. add, mul). connected to cdb. each entry is indexed by a tag and contains a section for each data source. a data source section contains a valid bit, tag, data content. if valid, then data content contains valid content and tag is empty. if not valid, tag is a valid tag and data content is empty. if a broadcasted tag+data matches a src tag, then the data is saved and the valid bit set. if an instruction ˝nishes its index tag and result data are broadcast instead of writing it to a register directly. this can potentially become the critical path (tag broadcast →value capture →instruction wake up) 2.3.7.0.2 register alias table register ˝le with support for reservation stations, connected to cdb. each register also has a valid bit and a tag value. valid registers have register content as usual and no tag. invalid registers have a tag and if a tag+data broadcast matches the tag, the data is saved and set to valid. 2.3.7.0.3 cdb common data bus bus connecting all reser- vation stations and the rat, transmits pairs of tag and data content. data written to it is \u0010broadcast\u0011, i.e. read by all other connected units. there can be multiple buses instead (e.g. one per reservation station), then everyone listens on all busses and multiple reservation stations can broadcast results at the same time 2.3.7.0.4 tomasulo's algorithm for out-of-order execu- tion use reservation stations, rat and cdb as described. 2.3.7.0.5 modern ooo execution with precise exceptions most modern processors have a rob, one register ˝le (for each of in- t/˛oat) which contains both speculative and architectural (isa visi- ble) registers and two register maps, one for renaming and one for maintaining precise state (no more details given, doesn't seem to exactly match the descriptions of the parts that were given) 2.3.7.0.6 instruction window the set of decoded but not yet retired (˝nished) instructions. mostly important in context of rob and ooo execution. has a ˝xed size or at least size limit. 2.3.8 handling of exceptions and interrupts architectural state should be precise/consistent for handling. this means all previous instructions should be retired and no later in- structions should be retired. retired (commited) means ˝nished so that the result is visible on the isa level. 2.3.8.1 exception handling in pipeline done by control logic. ensures architectural state is precise (register ˝le, pc, memory as re- quired by instructions up to the one causing the exception). ˛ushes younger instructions from pipeline (undoes any side e˙ects they may have had). saves pc and registers as speci˝ed by isa and redirects fetch engine to appropriate exception handling routine. 2.3.8.2 ways to make precise exception possible in pipeline • make each operation take the same amount of time • rob reorder bu˙er • history bu˙er • future register ˝le • checkpointing 2.3.9 control dependence/branches if the value of pc has a dependence on the instruction (i.e. instruc- tion is a control ˛ow instruction). such dependences are frequent and if they cause stalls or are mispredicted the penalty is large (even 10% wrong predictions/stalls can triple amount of cycles). 2.3.9.1 branch types 2.3.9.1.1 conditional direct 2 possible next addresses, re- solved at execution 2.3.9.1.2 unconditional direct 1 possible next address, re- solved at decode 2.3.9.1.3 call unconditional direct jump into subroutine. 1 possible next address, resolved at decode 2.3.9.1.4 unconditional indirect jump to address con- tained in register. many possible next addresses, resolved at exe- cution 2.3.9.1.5 return special case of unconditional indirect where register contains address of parent rountine which called the current routine 2.3.9.2 solutions for all branch types 2.3.9.2.1 stall pipeline until dependence is resolved 2.3.9.2.2 early branch resolution on branch instructions, as soon as possible, bypass the rest of the pipeline, ˛ush poten- tially wrong instructions and start work on the correct branch. can increase clock cycle, costs additional hardware 2.3.9.2.3 branch prediction guess if branch is going to be taken. on success, pipeline keeps fetching uninterrupted. prediction is latency critical because the next address needs to be available at the end of the cycle for the next fetch. using btb to recognize recurring branches early can help this. more accurate predictors are more complex, bigger, slower. 2.3.9.2.4 delayed branching/branch delay slot the isa speci˝es that n instructions after a branch instruction are concep- tually moved in front of the branch instruction, i.e. they will be executed in either case. this moves the problem to the compiler, which has to add nops if it can't ˝nd any useful instructions. hard to ˝nd such instructions for unconditional branches, need as many delay slots as the pipeline is deep (i.e. potentially many), changes isa. 2.3.9.2.5 delayed branching with squashing the isa spec- i˝es that n instructions after a branch instruction are executed only when the branch is taken, but not if it's not taken. 2.3.9.2.6 fgmt ˝ne grained multi threading 2.3.9.3 solutions for conditional direct branches 2.3.9.3.1 predicate combining detect composite predi- cates/conditions that were turned into multiple branches by com- piler and turn it back into single predicate by evaluating whole ex- pression ˝rst and then doing single branch dependent on result. 2.3.9.3.2 predicated execution/predication change isa to add instructions which combine evaluating a predicate and executing an instruction only when the predicate evaluates to true. somewhat similar to multipath execution chosen by compiler. eliminates espe- cially small branches, compiler has more freedom for optimization. useless work especially if branch would have been easy to predict, requires isa support, not all branches can sensibly be taken care of this way. 2.3.9.3.3 multipath execution follow both paths, discard the one that is not taken. paths followed can increase exponentially, each followed path needs its own context, duplicate work if paths merge. 2.3.9.4 branch prediction 2.3.9.4.1 branch misprediction penalty amount instruc- tions ˛ushed when branch di˙erent from guessed one is taken 2.3.9.4.2 branch con˝dence estimation estimate likeli- ness of correct prediction. used for example to choose predictor or decide to execute both branches, etc. 2.3.9.4.3 btb branch target bu˙er aka branch target ad- dress cache. on branch instructions save the computed target ad- dress together with pc. next time use the precomputed value. mostly useful for direct branches, around 50% accurate for indirect branches. can also be used to identify branch instructions before they are decoded if their address is already in the btb. 2.3.9.4.4 bht branch history table (local) saves whether each branch was last taken/not taken, or a saturating counter for multibit predictions. for last/previous times branch prediction, can be part of same hardware as btb 2.3.9.4.5 ghr global branch history register saves direc- tion of n previous branches using n bits 2.3.9.4.6 pht pattern history table a table which maps history patterns of taken/not taken to individual saturating counters 2.3.9.5 branch prediction types 2.3.9.5.1 static with isa support. done in advance, recorded in instruction or implied by instruction and isa. 2.3.9.5.2 dynamic with microarchitecture support. done by hardware during execution. no need for isa support. 2.3.9.6 static branch prediction methods (mostly for di- rect branches) 2.3.9.6.1 pc+=4 always very simple, can be aided by com- piler if compiler tries to lay out program that way. 2.3.9.6.2 not taken simple, usually less than 50% accurate, but compiler could adjust code layout to improve 2.3.9.6.3 taken usually more than 50% accurate, uses btb to save target address 2.3.9.6.4 btfn backwards taken forwards not taken for- ward/backwards refers to jump address in relation to pc. usually more accurate than taken/not taken. 2.3.9.6.5 pro˝le based program is run (\u0010pro˝le run\u0011), data gathered and predictions saved in program. can be very accurate if pro˝le run is representative 2.3.9.6.6 program-(analysis)-based compiler guesses based on simple rules like \u0010pointer and fp comparisons: not equal\u0011, \u0010loops are executed\u0011, \u0010blez (branch less equal zero) not taken\u0011 2.3.9.6.7 programmer based programmer speci˝es likely direction. needs programming language support on top of isa sup- port, burdens programmer. 2.3.9.7 dynamic branch prediction methods for direct branches 2.3.9.7.1 last time (single bit) assume same as last time for each branch. saved in btb. implementation schematic https://safari.ethz.ch/digitaltechnik/spring2019/lib/exe/fetch.php?media=onur- digitaldesign-2019-lecture18-branch-prediction-ii-afterlecture.pdf page 25 2.3.9.7.2 saturating counters/previous times (two bit/bimodal prediction) four states: strongly taken, weakly taken, weakly not taken, strongly not taken. guess taken/not taken based on state, adjust state in direction of taken/not taken based on real outcome. saved in btb. can often be as good as 85%-90% 2.3.9.7.3 two level adaptive branch predictor use an n- bit history as the index for a pht table of 2n saturating counters and return prediction based on that counter. 2.3.9.7.4 two level global branch prediction use ghr global branch history register with global pht table as two level adaptive branch predictor (above). implementation schematic https://safari.ethz.ch/digitaltechnik/spring2019/lib/exe/fetch.php?media=onur- digitaldesign-2019-lecture18-branch-prediction-ii-afterlecture.pdf page 26 2.3.9.7.5 two level gshare branch predictor works like two level global branch prediction (above), but in- stead of using the current global history as an index di- rectly, ˝rst xor the global history with the pc (or some bits of the pc, or do some other hash algorithm). schematic https://safari.ethz.ch/digitaltechnik/spring2019/lib/exe/fetch.php?media=onur- digitaldesign-2019-lecture18-branch-prediction-ii-afterlecture.pdf page 27 2.3.9.7.6 two level local branch predictor two level adap- tive branch predictor using local branch history for each branch and global pht table (or maybe local too?) 2.3.9.7.7 per-branch history register save the last n out- comes for this branch and assume the pattern is cyclic, make pre- diction based on that. 2.3.9.7.8 loop branch predictor try to detect loops (which are executed often) and guess accordingly 2.3.9.7.9 perceptron ˝nds direction correlations between branches. models neuron, is binary classi˝er. learns linear function of input vector element to output. can not learn all types of correla- tions and is expensive to implement. prediction =    w0 ... wn    ·      1 ghr 1 ... ghr n      with ghr j = ±1 for taken/not taken and weights determined by perceptron. 2.3.9.7.10 hybrid use multiple predictors and a meta- predictor/selector that chooses which predictor to use when. for example can train one predictor for a while without using it and then switch to using it once it is warmed up. can have 90-97% accuracy on average. 2.3.9.7.11 hybrid history length multiple history tables of di˙erent length 2.3.9.8 dynamic branch target predictors for uncondi- tional indirect branches 2.3.9.8.1 indirect branch prediction using btb save tar- get in btb. simple, but only around 50% success 2.3.9.8.2 indirect branch prediction using history keep a ghr and instead of indexing btb with pc, index it with hash (xor) of ghr and pc so that more than one prediction per branch is saved. but there are too many possible targets which causes con˛ict misses and it's ine˚cient if the branch really only has few targets our of the many possible 2.3.9.8.3 return address stack for return branches: keep a stack of caller addresses. when doing a call, the return address is pushed to the stack, when doing a return a value is popped from the stack and used as return address. the problem is to recognize calls/returns (need some isa support or guesswork) and that it fails if the logical calling stack is bigger than the return address stack. for an 8-entry stack, success rate is over 95%. 2.3.10 memory dependence handling memory address not know until instruction partly executed. renam- ing di˚cult, dependences known late, when a ld/st has its address ready there could be other ld/st where address is not yet known 2.3.10.1 approaches 2.3.10.1.1 conservative stall load until all previous stores have computed their addresses (need to compare addresses to know whether to continue stalling) or even until they are ˝nished (no need to compare addresses). easy, but stalls a lot. 2.3.10.1.2 aggressive assume load is independent and sched- ule it right away. still somewhat simple, but needs recovery on mis- prediction. 2.3.10.1.3 intelligent try to predict whether the load is in- dependent. most complicated and accurate, still need recovery on misprediction. 2.3.10.2 load and store queues modern processors use them for memory dependence handling, either combined or separate. each load/store searches the store/load (i.e. the other) queue after its address is calculated. 2.3.10.3 store to load forwarding a load gets its value either from store queue, rob or memory so that it gets the value that was stored by the latest instruction before itself. complicated, needs content addressabel memory 2.3.11 simd single instruction multiple data do the same instruction multiple times for the same data. gpus do this using warps (see below), some isas have support for doing an instruction on multiple short operands instead of one long (use one word as multiple numbers instead of one). modern simd proces- sors including gpus are a combination of array processor and vector processor. 2.3.11.1 array processors do the instruction multiple times at the same time. 2.3.11.2 vector processors do the same instruction multiple times sequentially, pipelined. good: • the pipelines can be deeper than usual because of the regular nature of vector processing (see following points) • no interlocking needed • no control ˛ow • addresses known in advance so vectors can be prefetched • there are less instructions to be loaded for the same amount of work and less branches in particular • regular memory access pattern bad: • specialized • parallelizability of code limited (Amdahl's Law) • memory bandwidth can easily become bottleneck. 2.3.11.2.1 vector registers store entire vectors 2.3.11.2.2 vlen vector length register stores length of vec- tor, limited by size of vector registers 2.3.11.2.3 vector stripmining if the logical vectors are longer than the vector registers and the logical vectors are processed in chunks of the maximal amount of entries at a time. 2.3.11.2.4 vstr vector stride register stride is the distance in memory between entries of a vector (1 if the vector is stored without other data in between). if there are enough banks and the number of banks is relatively prime to stride length, then one element per cycle throughput can be sustained. 2.3.11.2.5 vmask register select only some entries of vec- tor to operate on by having a toggle bit for each entry. kind of predicated execution. simple implementation: turn o˙ writeback for the disabled entries, but do the computations anyway. density-time implementation: only execute elements with non- zero masks. 2.3.11.2.6 memory banking memory is put into banks with independent access. vector elements are distributed over the banks so that as many memory accesses can run in parallel as there are banks. if there are more banks than they have latency then the memory can supply the one element per second that the pipeline can operate on. 2.3.11.2.7 bank con˛ict when the same bank is accessed again before the last access is ˝nished. avoided by having enough banks, optimizing data layout and mapping of addresses to banks (for example pseudo-random) 2.3.11.2.8 vectorizable loop if each iteration is independent of others 2.3.11.2.9 vector chaining forward vector entries to other pipelined functional unit before all entries are available. 2.3.11.2.10 gather/scatter operations get/put vectors from/in irregular memory locations using an index vector. often implemented in hardware with sparse vectors in mind. 2.3.11.2.11 automatic code vectorization compile time reordering, is di˚cult 2.3.12 simt+spmd simd using threads. each thread has the same instructions but dif- ferent data and its own context so that it can be treated indepen- dently. the threads use their ids to index data. advantage is that the threads can do mimd processing sometimes and are dynamically grouped into simd warps when possible. 2.3.12.1 warp/wavefront dynamically created group of threads which (at the moment) has the same sequence of instruc- tions (i.e. same pc) on di˙erent data. one warp works like one simd instruction, but the warps are not exposed to gpu programmers. 2.3.12.2 warp size/vector length/wavefront size number of threads in lockstep on one simd unit 2.3.12.3 warp interleaving is basically fgmt. in gpus the reg- ister values of all threads stay in the register ˝le so switching is fast. 2.3.12.4 branch divergence when threads in the same warp branch di˙erently. hardware reacts by masking (as in vector pro- cessors) all the threads in the warp except for those which are the same pc and interleaving the sub-warps formed this way. 2.3.12.5 merging / dynamic warp formation when warps fragment too much because of branch divergence, they can be split up and the threads rearranged in new (fewer) warps. this works best when the threads stay on the same simd lane so they keep access to their registers so the register contents don't need to be moved. 2.3.12.6 two-level round robin to deal with long latency op- erations it's possible to interleave two groups of warps so that one group of warps is succeeded by the next group just after issueing long latency operations which should have completed by the time it's the group's time again. 2.3.13 memory architecture 2.3.13.1 locality 2.3.13.1.1 temporal if an access just happened, it's likely it will happen again 2.3.13.1.2 spatial if a particular address was accessed, it's likely this and nearby addresses will be accessed again 2.3.13.2 cache automatically managed fast (sram) storage of recently accessed memory and memory that is close to recently ac- cessed memory 2.3.13.3 cache size/capacity compromise of speed and size 2.3.13.4 block/line ˝xed size unit of storage. smaller blocks have higher overhead and use spatial locality less. larger blocks waste space and there might be not enough blocks instead. 2.3.13.5 block size 2.3.13.6 number of blocks 2.3.13.7 set the cache locations one memory address can go to 2.3.13.8 number of sets number of blocks associativity , each memory address maps to exactly one set 2.3.13.9 critical word because fetching a whole cache block can take a long time, fetch the critical word that was actually re- quested ˝rst 2.3.13.10 insertion/placement where and how to put/˝nd block 2.3.13.11 eviction/replacement what data to remove to free space. rules: invalid blocks ˝rst. then for example: random, ˝fo, least recently used, not most recently used, least frequently used, least costly to re-fetch, hybrid, optimal 2.3.13.11.1 least frequently used pretty expensive, for n- way associative need a log n bit counter to keep track of when the entry was used. 2.3.13.11.2 optimal belady's: replace the block that is going to be referenced the furthest in the future by the program. not optimal, miss cost not constant 2.3.13.12 compulsory miss the address wasn't accessed be- fore, so it can't be cached 2.3.13.13 capacity miss the cache isn't large enough, those misses that would also happen in a fully associative cache of the same size. can be avoided by software by organizing data into working sets that are needed at the same time and that ˝t into cache. 2.3.13.14 con˛ict miss neither compulsory nor capacity, be- cause of too low associativity. can be avoided using software hints, better/randomized indexing or a victim cache 2.3.13.15 miss penalty time it takes to retrieve a block from a lower level 2.3.13.16 overall miss cost depends on whether (how long) the processor will stall on a miss, how long it will take to resolve the miss 2.3.13.17 performance miss rate, miss cost, hit cost 2.3.13.18 victim cache 2.3.13.19 software optimizations for cache 2.3.13.19.1 loop interchange exchange indexes in multidi- mensional loops to keep access more localized 2.3.13.19.2 blocking/tiling work on data (matrices) in blocks at a time so that each block ˝ts into memory, for example blocked matrix multiplication 2.3.13.19.3 separate data structures if only part of a data structure is accessed frequently, it makes sense to store those parts separately because otherwise the infrequently used parts will get into the cache 2.3.13.20 granularity of management block size 2.3.13.21 caches in multi core system 2.3.13.21.1 private and shared caches private caches be- long to one core only, shared ones to multiple cores. shared caches good: avoid duplicate and incoherent data in mul- tiple caches, allow better space utilization, reduce communcation latency between cores. shared caches bad: resource contention (not everyone can use cache at the same time), cache farther away from core (latency), threads impact each others performance, security problems (˝nd out if another thread used this memory location) 2.3.13.21.2 cache coherence if there are private caches (and it's hard to do without any private caches), need to make sure they don't contain contradicting/outdated information. can provide cash ˛ush instruction in isa and move burden to programmer/com- piler. or do it in hardware, one possibility is to invalidate a block in all other core's caches if one core writes to it 2.3.13.21.3 write propagation guarantee that updates of all kinds will propagate 2.3.13.21.4 write serialization ensure correct and consis- tent order as seen by all cores for some memory location 2.3.13.22 promotion whether and how to change priority of cached block on hit 2.3.13.23 write policy what happens on writes 2.3.13.23.1 write-back write to next level when block is evicted. can combine multiple writes to same block, so can be more e˚cient and save energy. but need a dirty bit to keep track of whether a write still needs to happen. 2.3.13.23.2 write-through write to next level immediately. simpler, easier consistency, but more bandwidth intensive 2.3.13.23.3 allocate on write miss put the value in the cache. can combine writes and handle write misses the same as read misses, but need to fetch the whole block from the next level. 2.3.13.23.4 no-allocate on write miss don't put written values in cache on miss. conserves space, locality of writes may be low so that this increases hit rate 2.3.13.23.5 allocate sub block on write miss have indi- vidual valid+dirty bits for smaller sections of cache blocks. no need to transfer additional data and can still cache the write, but more complex and may not optimally exploit spatiality. 2.3.13.24 instruction cache and data cache uni˝ed/sepa- rate caches. uni˝ed is good to make full use of cache size, but instructions and data may evict/thrash each other and the opti- mal place on the pipeline for instruction and data caches is not the same. this is why ˝rst level caches are usually separate and later levels uni˝ed. 2.3.13.25 address layout |address | = |index | + |tag | + block size 2.3.13.26 tag store hardware part that maps the index part of addresses to a valid bit for that cache block, replacement policy bits, the tag part of the address of the valid block and the address of the data store which contains the valid cache block (if there are multiple data stores. could also have a tag store for each data store instead). if the valid bit is set and the saved tag matches the tag of the current address, then the current address is cached in the saved data store 2.3.13.27 data store hardware part that takes the index part of addresses and outputs data. it takes a part of the address matching its size as the index into its storage array and discards the rest. there is one for direct-mapped cache and n for n-way associative cache. 2.3.13.28 cache hierarchy it's not possible to make the cache large because it would otherwise be too slow, so a hierarchy of caches is used where the fastest ideally has a 1 cycle access time 2.3.13.29 manual cache programmer explicitly manage cache. too much work for large programs. exists in some embedded pro- cessors and gpus 2.3.13.30 hierarchical latency analysis level i, level access time ti, expected total access time Ti, hit-rate (for this level) hi, miss-rate (for this level) mi. Ti = hiti+mi (ti + Ti+1) = ti+miTi+1. 2.3.13.31 direct-mapped cache each address/block can go into exactly one place in the cache. addresses have three parts: the least signi˝cant part that has the same length as the blocks is not directly used by the caching infrastructure. the index part in- dexes into the cache and thus has length determined by cache size. whatever rests is the tag. good: simple, often works ok bad: depending on memory access pattern can have 0% hit rate 2.3.13.32 set associativity a cache is n-way set associative if it has n places where an address could go in the cache. n is practically always a power of two. there are n data stores and one matching tag store (or one for each data store). higher associativity reduces con˛ict misses but is slower and more expensive 2.3.13.33 full associativity each block could go anywhere in the cache, n-way set associative for maximal n. usually not worth it because the returns from higher associativity are diminishing 2.3.13.34 ˝rst level cache usually tag and data store are ac- cess in parallel and has low associativity because latency is critical 2.3.13.35 higher level caches large and highly associative, tag and data store accessed serially 2.3.13.36 serial and parallel access of level it's possible to search multiple levels of cache at the same time, or go through them in order 2.3.13.37 prefetching guess which block will be needed soon and put them in cache 2.3.14 uncategorized points TODO 2.3.14.1 instruction issueing an instruction is issued when the processor starts working on executing it 2.3.14.2 issue width the number of issue slots 2.3.14.3 issue slot for an instruction to be issued it needs a free issue slot, each running instruction presumably occupies one issue slot 2.4 logic level 2.4.0.1 minterm of a set of variables is a product of all variables which may optionally be inverted. Often encoded as binary numbers with 0s denoting the inverted terms and assuming an ordering of the variables 2.4.0.2 maxterm like a minterm but with sums 2.4.0.3 minterm/maxterm conversion switch from minterms to maxterms with the indices not yet used or the other way around 2.4.0.4 inversion of minterms/maxterms replace indices used with all indices not beforehand used 2.4.0.5 sum of products form, DNF, minterm form ob- tained by taking rows that evaluate to 1 2.4.0.6 product of sums form, CNF, maxterm form ob- tained by taking the inverse of rows that evaluate to 0 2.4.0.7 mealy ˝nite state machine output is expression of state and input 2.4.0.8 moore ˝nite state machine output is expession of state exclusively 2.4.0.9 two's complement positive numbers and zero are rep- resented the same as unsigned, a negative number x is represented by z = 2 n − |x|. to multiply a number with −1, invert its digits and add one. 2.4.0.10 gray code permutation of n-bit binary numbers so that consecutive numbers di˙er in one bit only. construct n + 1 bit gray code from n bit gray code by enumerating it ˝rst in order and then again backwards and pre˝x bit b to the ˝rst half of entries and b to the second half. 2.4.0.11 karnaugh map visualizes adjacencies, up to four vari- ables. groups must be rectangular and have the size of a power of two. enumerate possibilites in gray code 2.4.0.12 logical completeness whether all logic functions could be implemented using a set of gates/circuits. pla, {and , or , not }, nand, nor are logically complete. 2.5 circuit level 2.5.0.1 functional speci˝cation what outputs correspond to inputs 2.5.0.2 timing speci˝cation how long it takes for the input to propagate to the output 2.5.0.3 combination logic stateless, output depends only on current input 2.5.0.4 sequential logic stateful, output may also depend on past input 2.5.0.5 critical path path a signal can take so that propagation delay is maximal 2.5.0.6 contamination delay delay until a value starts chang- ing. minimum possible delay. 2.5.0.7 propagation delay delay until a value ˝nishes chang- ing. maximum possible delay. 2.5.0.8 glitch when one input transition causes multiple output transitions. happen when there are paths of di˙erent lengths that lead to di˙erent outputs. glitches happen when moving between prime implicants on k-maps. can add additional implicants to cover transitions to avoid glitches. 2.5.0.9 sequencing overhead time wasted to meet timing re- quirements, per cycle 2.5.0.10 timing analysis checking whether timing require- ments are met. TODO look at this more closely 2.5.0.11 clock skew time di˙erence between the same clock edge at di˙erent locations on the circuit 2.5.0.12 testing functional correctness at high level (C, HDL), timing and power at low level (circuit) and equivalency of low level model with high level model 2.5.0.13 timing design principles • critical path design: minimize maximal delay • balanced design: balance maximal logic delays accross di˙erent parts of the system • bread and butter design: optimize for common case, worst cases still ok 2.5.0.14 level triggered vs edge triggered an element (es- pecially memory cell) can be level triggered (changes according to level) or edge triggered (changes only on rising or falling edge) 2.5.0.15 n-bit lut, lookup table just directly implements the truth table of a function {0, 1} n → {0, 1}, basic building block of fpga for 4 ≤ n ≤ 6 2.5.0.16 pla programmable logic array n inputs, m outputs, 2 n and gates (for each combination of input values), m or gates (for each output), wires from some of the and gates to the or gates 2.5.0.17 full adder adder with carry in and out, sum = a⊕b⊕c, carry = ab + bc + ca 2.5.0.18 n-bit adder can bet constructed from n full-adders, but has delay linear in n when constructed that way 2.5.0.19 decoder n inputs, 2 n outputs, exactly one output is on at a time 2.5.0.20 multiplexer n + 2n inputs, 1 output, connects one of the 2 n inputs to the output depending on the n other inputs 2.5.0.21 register usually multiple ˛ip˛ops (edge triggered) 2.5.0.22 memory array organization commonly 2d array consisting of addressable words 2.5.0.22.1 bitline there is one bitline for each bit in a word which supplies this bit 2.5.0.22.2 wordline there is a wordline for each word in the memory and when it is activated the bitlines are connected to the 1-bit storage elements for this word 2.5.0.22.3 decoder decodes the address onto the wordlines 2.5.0.22.4 memory division because memory becomes slower the larger it is (propagation delay), it is split into many sub- arrays. for DRAM the hierarchy is channel, rank, bank, subarray, mat 2.5.0.23 fsm consists of next state logic, state register, output logic. state transition on clock signal only. 2.5.0.24 state transition table table for function {state , input } → next state 2.5.0.25 state encoding 1. fully encoded: just a binary number 2. 1-hot-encoded: exactly one one in each state 3. output encoded: state equals the output in that state, can add bits to make encoding unique 2.6 gate level 2.6.0.1 signal levels low 0, high 1, ˛oating z, don't care x 2.6.0.2 cross-coupled inverters two inverters cross couple have two stable output signal Q and Q but no way to change it 2.6.0.3 sram cell basically cross-coupled inverters together with the infrastructure to set a value (enable/wordline signal and the data signal and the inverted data signal). 4 transistors for storage (two inverters) and 2 for access 2.6.0.4 r-s-latch cross-coupled nand gates. when both inputs are one, outputs saved value. when exactly one input is one, the output is one or zero. it's not allowed for both inputs to be zero. 2.6.0.5 gated d-latch r-s-latch with gate (consisting of two nand gates and an inverter) to ensure correct usage. input d, input write enable. 2.6.0.6 d ˛ip-˛op two d-latches in series with opposite enable signals so that the output changes exclusively at rising edge of clock signal. can also have enable signal that needs to be on for the transition to happen. can have reset signal that forces output to zero (either asynchronously=immediately or synchronously at next clock edge). can also have set signal that works like reset signal. setup time is time before clock edge that signal must be stable, hold time the time after the clock edge that signal must be stable, aperture time is sum of both. otherwise output is non-deterministic. 2.6.0.7 not gate inverter, uses one p- and one n-type transistor 2.6.0.8 nand gate and gate with output inverted. Uses 2 p- and 2 n-type transistors 2.6.0.9 and gate harder to implement than nand, uses nand and not gate to implement 2.6.0.10 bu˙er gate the just forwards, strengthens signal 2.6.0.11 tri-state bu˙er bu˙ers only when control signal on, otherwise output is ˛oating z. 2.6.0.12 or one or the other or both 2.6.0.13 xor exactly one 2.6.0.14 xnor both or neither 2.6.0.15 nor neither input 2.7 physics level 2.7.0.1 n-type transistor normally open, closes connection when voltage applied to gate, good at pulling down 2.7.0.2 p-type transistor normally closed, good at pulling up 2.7.0.3 cmos complementary metal oxide semiconductors, i.e. uses both n-type and p-type transistors 2.7.0.4 cmos gate structure Vin \u000f\u000f pMOS network \u000f\u000f I 88 && · // \u000f\u000f O nMOS network \u000f\u000f GND 2.7.0.5 power consumption dynamic CV 2f , static V I. C ca- pacitance, V voltage, f frequency, I leakage current 2.7.0.6 dram cell a capacitor and a transistors which controls access to the capacitor. the capacitor is always emptied when read- ing, so needs to be refreshed is the data is still needed. the capacitor also empties itself with time and needs to be refreshed on the order of every 10ms.","libVersion":"0.3.2","langs":""}