{"path":"sem1/DMath/PV/extra/pvw/DMath-pvw-script-HS22.pdf","text":"Discrete Mathematics PVW Script Last Updated: June, 2022 Authors: Christian Mitsch, Friedrich Ginnold, Simon Yuan, Olivier Bitter, Dimitri Stanojevic, Robin Staab, Mikl´os Horv´ath, Marc Himmelberger diskmat-pvw-skript@vis.ethz.ch Disclaimer: This script only serves as additional material for practice purposes and should not serve as a substitute for the lecture material. We neither guarantee that this script covers all relevant topics for the exam, nor that it is correct. If an attentive reader finds any mistakes or has any suggestions on how to improve the script, they are encouraged to contact the authors under the indicated email address or, preferably, through a gitlab issue on https://gitlab.ethz.ch/vis/luk/pvw script diskmath. Contents 1 Sets, Relations and Functions 3 1.1 Set fundamentals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 1.2 Relations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 1.3 Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7 1.4 Countability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8 1.4.1 Primes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8 1.4.2 Geometric arguments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9 1.4.3 Important countable sets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9 1.4.4 Proving countability and uncountability . . . . . . . . . . . . . . . . . . . . . . . . 10 1.5 Additional Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12 1.6 Solutions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13 1.6.1 Solutions for Hands-On 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13 1.6.2 Solutions for Hands-On 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16 1.6.3 Solutions for Hands-On 3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19 1.6.4 Solutions for Hands-On 4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22 1.6.5 Solutions for Hands-On 5 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 2 Number Theory 27 2.1 Divisibility . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27 2.2 Greatest Common Divisor . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27 2.3 Euclid’s Extended GCD Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27 2.4 GCD Tableau . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29 2.5 Ideals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29 2.6 Irrationality of Roots and Logarithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29 2.7 Least Common Multiples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30 2.8 Modular Congruences and Arithmetic . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31 2.9 Multiplicative Inverses . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32 2.10 Chinese Remainder Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32 2.11 Diffie-Hellman protocol . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33 2.12 Solutions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35 2.12.1 Solution for Hands-On 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35 2.12.2 Solution for Hands-On 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36 2.12.3 Solution for Hands-On 3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37 3 Algebra 40 3.1 Algebras . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40 3.2 Groups . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41 3.2.1 Subgroups . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42 3.2.2 Order of groups and elements - Cyclic groups and generators . . . . . . . . . . . . 43 3.3 Morphisms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47 3.4 Z ∗ m and Euler’s totient function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50 3.5 Rings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52 3.6 Fields . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53 3.7 Polynomials over Rings and Fields . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54 3.7.1 Factorization of Polynomials . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55 3.7.2 Polynomial Interpolation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56 3.8 Finite Fields . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56 3.9 Solutions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58 3.9.1 Solutions for Hands-On 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58 3.9.2 Solutions for Hands-On 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66 3.9.3 Solutions for Hands-On 3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71 3.9.4 Solutions for Hands-On 4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 72 3.9.5 Solutions for Hands-On 5 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 74 3.9.6 Solutions for Hands-On 6 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 75 1 3.9.7 Solutions for Hands-On 7 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77 4 Logic 80 4.1 Proof Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80 4.2 Logical calculi . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 81 4.3 Propositional Logic . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 84 4.3.1 Concepts: Syntax, Semantics, Interpretation, Model . . . . . . . . . . . . . . . . . 84 4.3.2 Concepts: Satisfiability, Tautology, Consequence, Equivalence . . . . . . . . . . . . 86 4.3.3 Normal Forms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 87 4.4 Resolution Calculus . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 87 4.5 Predicate logic . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 90 4.5.1 Syntax, Free Variables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 90 4.5.2 Semantics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 91 4.5.3 Summary of the General Concepts in Logic . . . . . . . . . . . . . . . . . . . . . . 92 4.5.4 Some basic Equivalences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 93 4.5.5 Substitution of (Bound) Variables, Normal Forms . . . . . . . . . . . . . . . . . . . 94 4.6 Solutions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 98 4.6.1 Solutions for Hands-On 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 98 4.6.2 Solutions for Hands-On 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 102 4.6.3 Solutions for Hands-On 3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 105 2 1 Sets, Relations and Functions This chapter covers the fundamentals of the crucial mathematical concepts sets, relations and functions. Many exercises are solved, which not only helps understanding the theory better but also revises some basic proof techniques. 1.1 Set fundamentals Definition 1.1. A set S is a well-defined collection of distinct objects. We call these objects ele- ments and denote x ∈ S for saying that x is an element of S. A set is considered to be an object in its own right. To say that an object x is not an element of a set S, we write x /∈ S. We can describe a set by either listing all its elements (if it is finite) or by giving a predicate that fits every element in the set, i.e. {x ∈ A | P (x)}. For example, {2, 3, 5} denotes the set which contains the numbers 2, 3 and 5. We can also describe this set with {x ∈ N | prime(x) ∧ x < 6} where prime(x) = 1 :⇔ x is a prime number. Note that the set representations are not unique, e.g. {2, 3, 3, 5, 5} and {x ∈ N | prime(x) ∧ x < 7} describe the same set as above. With the first method, only finite sets can be represented while the second one also enables the description of infinite sets. Two crucial concepts in set theory are subsets and set equality. Definition 1.2. A set A is a subset of a set B, denoted as A ⊆ B, if and only if each element of A is also an element of B. Formally A ⊆ B :⇔ ∀a(a ∈ A → a ∈ B). Definition 1.3. Two sets A and B are equal, denoted as A = B, if and only if they contain the same elements. Formally A = B :⇔ ∀a(a ∈ A ↔ a ∈ B). Furthermore there are some important operations on a set. The power set, union, intersection, difference and cartesian product. For the following table, let A and B be two sets. Operation Notation Definition Meaning power set P(A) P(A) := {x | x ⊆ A} The set which contains all subsets of A. union A ∪ B A ∪ B := {x | x ∈ A ∨ x ∈ B} The set of all objects which are elements of A or B. intersection A ∩ B A ∩ B := {x | x ∈ A ∧ x ∈ B} The set of all objects which are elements of A and B. difference A \\ B A \\ B := {x | x ∈ A ∧ x /∈ B} The set of all objects which are elements of A and are not elements of B. cartesian product A × B A × B := {(a, b) | a ∈ A ∧ b ∈ B} The set of all ordered pairs whose first component is an element of A and its second component is an element of B. complement A A := {x ∈ U | x ̸∈ A} The set of all elements of some universe U that are not elements of A. (Only makes sense w.r.t. a universe!) 3 Note that it immediately follows from these definitions that two sets are equal if and only if they are subsets of each other. This property is usually the easiest way of proving that two sets are equal. Our general strategy for proving that two sets are equal is to prove that they are both subsets of each other. To show that A ⊆ B, we have to show that each element of A is also an element of B (which is simply done by applying the definition of a subset). The standard way of doing this is to choose an arbitrary element x ∈ A and to show that also x ∈ B. Since x is arbitrary, this implies that each element of A is also an element of B. The following exercises use these definitions and strategies in practice, and should help you to understand them better. Keep in mind that ∅ (or {}) is the empty set, the set which has no elements, and that the cardinality |A| of a finite set A is the number of elements contained in A. Hands-On 1 1.1. Describe the set of all even positive natural numbers which have one digit using both represen- tations that were mentioned. 1.2. Describe the set of all even positive natural numbers using the predicate representation. 1.3. Let A = {∅, {∅}, {∅, {∅}}} and B = {{∅}, ∅, {∅}} be two sets. Specify each of the following sets (by listing their elements) and give their cardinality. 1. A ∪ B 2. A ∩ B 3. P(B) 4. A \\ P(B) 5. B × P(∅) 1.4. Let A and B be two sets. Show that P(A) = P(B) ⇒ A = B. 1.5. Let A, B and C be three sets. For which A is A × B = A × C ⇒ B = C? 1.6. Show that (A ∪ B) \\ (A ∩ B) = (A ∪ C) \\ (A ∩ C) ⇒ B = C. 1.7. Prove that there exist infinitely many sets S such that S ⊆ P(S). 4 1.2 Relations Definition 1.4. A (binary) relation ρ from a set A to a set B is a subset of A × B. If A = B, then ρ is called a relation on A. We write (a, b) ∈ ρ or a ρ b to say that (a, b) ∈ A × B is an element of the relation ρ. There are a few crucial properties that relations on a set can have. The following table contains the property, its concrete definition and a graph theoretic interpretation for illustration purposes. Let ρ be a relation on a set A. Property Definition Meaning (graph theory) reflexivity ∀a (a ρ a) Each vertex has a loop (an edge to itself). irreflexivity ∀a (a \u0001ρ a) No vertex has a loop. symmetry ∀a∀b (a ρ b ↔ b ρ a) It can be represented as an undirected graph, possibly with loops. antisymmetry ∀a∀b ((a ρ b ∧ b ρ a) → a = b) There are no cycles of length 2 in the graph representation. transitivity ∀a∀b∀c ((a ρ b ∧ b ρ c) → a ρ c) If a vertex can be reached from another vertex, then it can also be directly reached from that vertex. The inverse, composition and the transitive closure are operations on relations. Exercise 2.2 shows how these work. Additionally there are also some connections between the properties, which are (for the most part) highlighted in the exercise 2.3. There are two relations of special interest on a single set A, namely equivalence relations and partial order relations. They are defined through a few concrete properties. Definition 1.5. An equivalence relation is a relation on a set A that is reflexive, symmetric and transitive. The definition immediately gives us a method of proving whether some relation is an equivalence relation or not: to prove that a relation is an equivalence relation, we prove that it is reflexive, symmetric and transitive. To prove that a relation is not an equivalence relation, disprove one of these properties. Definition 1.6. A partial order is a relation on a set A that is reflexive, antisymmetric and transitive. A set A together with a partial order ⪯ on A is called a partially ordered set (poset) and is denoted as (A; ⪯). Again the definition gives us a proof strategy: To prove that a relation is a partial order, we prove that it is reflexive, antisymmetric and transitive. To prove that a relation is not a poset, disprove one of these properties. Additionally, we can represent posets by Hasse diagrams and they can have special elements like least/greatest or maximal/minimal elements, which will occur in exercises 2.7 and 2.8. Definition 1.7. The composition of two relations is a quite natural concept using the existence of one ”connecting element”. While we can generalize the concept to an arbitrary amount of relations we will now only look at the composition of two relations ρ and σ. Let ρ ⊆ A × B and σ ⊆ B × C, then their composition is ρσ ⊆ A × C and (a, c) ∈ ρσ ⇔ ∃b ∈ B : (a ρ b ∧ b σ c) 5 Furthermore we can compose a relation with itself (which only makes sense if we talk about a relation on a set i.e. ρ ⊆ A × A) and by repeating this we get ρ k containing all tuples (a, a ′) ⊆ A × A that fulfill ∃a1, ..., ak−1 ∈ A : (a ρ a1 ∧ a1 ρ a2 ∧ ... ∧ ak−2 ρ ak−1 ∧ ak−1 ρ a ′). Definition 1.8. We get the transitive closure ρ∗ of a relation ρ using the definition ρ∗ = ∞⋃ n=1 pn Hands-On 2 2.1. Decide whether the relation ρ on {a, b, c, d} with ρ = {(a, b), (b, a), (b, c), (b, d), (c, b), (d, b)} is reflexive, irreflexive, symmetric, antisymmetric, transitive. 2.2. Let ρ be the same relation as in the previous exercise. Determine the set representation of the following relations ̂ρ, ρ2 and ρ ∗. 2.3. Let σ be a relation on a set A. Prove or disprove the following statements: 1. If σ is not reflexive, then it is irreflexive. 2. If σ is irreflexive and A ̸= ∅, then σ is not reflexive. 3. If σ is symmetric, then it is not anti-symmetric. 2.4. Let ∼ be a relation on R2 with (x, y) ∼ (a, b) := x2 + b = a 2 + y Prove that ∼ is an equivalence relation. 2.5. Let (A, ⪯) be a poset. Prove or disprove that (A, ̂⪯) is a poset. 2.6. Prove or disprove that there exists a poset on N which is also an equivalence relation. 2.7. Draw the Hasse Digramm of the poset (P({a, b, c}) ∪ {{d}}, ⊆). Determine its minimal, maxi- mal, least and greatest elements. 2.8. Let’s look at the poset (P({a, b, c}) ∪ {{d}}, ⊆) from the previous exercise. Find the lower bounds of the subset {{a, b}, {a, c}} and find its greatest lower bound. Is this poset a lattice? 2.9. Let ρ ⊆ A × B and σ ⊆ B × C. Prove that the inverse of ρσ fulfills ̂ρσ = ̂σ ̂ρ. 2.10. For each of the following relations on Z decide whether or not it is reflexive, symmetric and transitive. 1. x ∼ y ⇐⇒ x + y is even 2. x ∼ y ⇐⇒ xy is odd 3. x ∼ y ⇐⇒ x + x · y is even 2.11. Let define ρ ⊆ N+ × N+ as aρb ⇐⇒ gcd(a, b) = a (N+ = N \\ {0}). Decide whether or not ρ is reflexive, antisymmetric and transitive. 6 1.3 Functions A crucial mathematical concept is the one of functions, which can be described elegantly with relations. Definition 1.9. A function f : A → B from a domain A to a codomain B is a relation from A to B with the following properties: 1. f is totally defined, i.e. ∀a ∈ A ∃b ∈ B(a f b). 2. f is well-defined, i.e. ∀a ∈ A ∀b, b ′ ∈ B((a f b ∧ a f b′) → b = b ′). The set of all functions from A to B is denoted as BA. A partial function is a function where condition 1 does not necessarily have to hold. It is a convenient and acceptable standard to write f (a) = b instead of a f b when it is clear that f is a function. The notation BA is motivated by the fact that if A and B are both finite, then there are |B||A| such functions f : A → B. Definition 1.10. For a function f : A → B and a set S ⊆ A, we call the image of S under the function f the set f (S) := {f (a) | a ∈ S}. Injectivity, surjectivity and bijectivity are crucial properties of functions. Definition 1.11. A function f : A → B is injective if x ̸= y ⇒ f (x) ̸= f (y) for every x, y ∈ A. It is surjective if for every b ∈ B, there exists an a ∈ A such that b = f (a). It is bijective if it is injective and surjective. The definition immediately gives us methods to prove injectivity, surjectivity or bijectivity. In order to prove that a function f : A → B is injective, prove that for any x, y ∈ A with f (x) = f (y) we have x = y. To prove that f is surjective, prove that for any b ∈ B, there exists an a ∈ A such that f (a) = b. To prove bijectivity, prove injectivity and surjectivity seperately. Two functions can also be composed if the first function’s codomain matches the second function’s domain. Definition 1.12. The composition of two functions f : A → B and g : B → C is defined as g ◦ f where (g ◦ f )(a) = g(f (a)) ∈ C for any a ∈ A. Hands-On 3 3.1. Let f : X → Y be an arbitrary function and A, B subsets of X. Prove or disprove that f (A \\ B) = f (A) \\ f (B). 3.2. Let f : X → Y be an arbitrary function and assume X ̸= ∅ ̸= Y . Prove the following statement: ∀A, B ⊆ X : f (A ∩ B) = f (A) ∩ f (B) ⇐⇒ f injective 3.3. Let A, B be two arbitrary non-empty sets. Prove the following statement: There exists an injective function f : A → B if and only if there exists a surjective function g : B → A. 3.4. (Harder) Let F : NN → NN be defined as F (g) = g ◦ g. Show that F is neither injective nor surjective. 7 1.4 Countability Definition 1.13 (Countability). Let A, B be two arbitrary sets (of any cardinality). We define: • A ∼ B : A and B are of equal cardinality iff there exists a bijective mapping f : A → B. • A ⪯ B : B has at least cardinality of A iff there exists an injective function f : A → B. • A set A is called countable iff A ⪯ N, otherwise we call it uncountable. These definitions (together with our existing knowledge) imply a variety of basic conclusions which shall be listed here. As an additional exercise one can prove them. Corollary 1.1 • For any finite set A and infinite set B we have that A ⪯ B and B ̸⪯ A. • The by ⪯ defined relation is reflexive and transitive. • Countably infinite and uncountable are the only two ”options” for infinite sets, there exists nothing ”in between”. A proof of this can be found in the script. • For any countable set (finite and infinite) we are able to leverage its mapping to the natural numbers to apply the concept of ”well-foundedness”, i.e. every subset has a smallest element. The same holds in reverse i.e. if there exists a non-well-founded subset, the set has to be uncountable. Now we want to look at a variety of ways how we can construct such injections onto N (and for uncount- ability surjections onto R). 1.4.1 Primes This is probably the most used and most helpful way to quickly create an injection onto N. It relies on the fact, that any n ∈ N has unique prime factorization, a fact which will be proven in chapter 2. We proceed as follows: Given n countable sets A1, ..., An and their respective injections f1, ..., fn with fi : Ai ↦→ N, we can choose any n different primes p1, .., pn and construct the following injection: F : A1 × A2 × ... × An → N (a1, a2, ..., an) ↦→ pf1(a1) 1 · p f2(a2) 2 · .... · pfn(an) n Injectivity follows from the unique prime factorization theorem. Example. • N × N is countable with e.g. the following injection (a, b) ↦→ 2 a · 3 b • Q is countable with e.g. f ( p q ) = 2p · 3q under the assumption that p and q are relative primes. This prevents us from mapping both 1 2 and 2 4 (Food for thought: What would we actually be mapping if we wouldn’t make this assumption and how would it influence the correctness?). • Important Let A1, A2 be countable sets with A1 ∪ A2 = A and their respective injections f1, f2 onto N − {0}. For A we can define the following injective mapping F : A → N: F (a) = { 2 f1(a), for a ∈ A1 3 f2(a), for a ∈ A2, a ̸∈ A1 } 8 1.4.2 Geometric arguments Another useful way of constructing injections is to use a ”geometric argument” i.e. convering an entire space (usually constructed as a product space e.g. N × N or Z × N) by moving along a single trace. Example. A simple example is the following injection f : N+ × Z ↦→ N in which we move from inner circles to outer ones. Simply listed it would be (0, 1), (1, 1), (2, 1), ..., (1, 2), (2, 2), (3, 2), (3, 1), ... Injection f : N+ × Z ↦→ N 1.4.3 Important countable sets The following table provides a quick overview over common countable sets and how we prove their countability: A1 × ... × An Sets of finite tupels over countable sets We create an injection from Ai × Aj to N × N (they are countable). From there we create an injection onto N (either via primes or geometrically). We generalize the argument via induction (or primes as above) Z, Q Integers and rationals Simple geometric argument or primes for Q ⋃n i=1 Ai Union of a finite amount of countable sets Proof is in the exercises ⋃∞ i=1 Ai Union of a countably infinite amount of countable sets Proof is in the exercises {f : N → N | ∀n ∈ Nf (n) ≥ f (n + 1)} Set of all monotonically decreasing functions from N to N Proof is in the exercises 9 1.4.4 Proving countability and uncountability Sometimes your task is to prove that a given set is countable or uncountable. Therefore we will recap the most common ideas used. Using injections To prove that some set A is countable : construct an injection from A to a countable set, transitivity of the ”dominated by” relation implies that A is countable too. To prove that set B is uncountable construct an injection from an uncountable set C to B. If B was countable, transitivity would imply that C is too, which is a contradiction. Hence B is uncountable. Cantor’s diagonal argument to show uncountability Assume we want to show that the set of infinite strings over a set A with at least 2 elements is uncountable. We can do so by assuming the contrary (i.e. it is countable) and arriving at a contradiction. Let us for exemplary reasons look at the open interval (0, 1) ⊂ [0, 1] ⊂ R and let us assume it’s countable. We already know that real numbers can have an infinitely long decimal representation and given the countability assumption we can arrange them as follows: z1 = 0, a11 a12 a13 a14 . . . z2 = 0, a21 a22 a23 a24 . . . z3 = 0, a31 a32 a33 a34 . . . z4 = 0, a41 a42 a43 a44 . . . z5 = . . . ... x = 0, x1 x2 x3 x4 . . . with ∀j, k ∈ N : aj,k ∈ [0, 9] ⊂ N. Now we define xi as follows: xi = R10(aii + 1). The important idea is the following: By assumption we start with a well-defined countable sequence of numbers. Therefore all xi are also well-defined, and thereby x is too (even if it is infinitely long). Via our construction our x differs from all zi at least in position i. As this is true for all i ∈ N we have that x cannot be part of our initial ”ordered and complete sequence”. As x is a well-defined real number, our sequence therefore can’t be complete! This is a contradiction and (0, 1) has to be uncountable. Power sets and Cantor’s theorem Theorem 1: Cantor’s Theorem For any set A we have that there exists no surjective mapping f : A ↦→ P(A) onto its power set. This directly implies that P(A) always has a strictly greater cardinality than A. In particular, one can show that for any infinite set A the set P(A) is uncountable. A proof of this theorem used to be an exam question. In order to encourage you to try it yourself it’s part of the last hands-on. 10 Hands-On 4 4.1. Let Ai be a countable set for any i ∈ N. Is A = n⋃ i=1 Ai countable for n ∈ N? Prove your claim. 4.2. Now look at A = ∞⋃ i=1 Ai Is it countable? Prove your claim. Can you extend your proof from exercise 1? If not, explain why. 4.3. Let A = {f : N → N | ∀n ∈ N f (n) ≥ f (n + 1)} Is A countable? Prove your claim. 4.4. Let A = {f : N → N | ∀n ∈ N (n even → f (n) > f (n + 1) ∧ n odd → f (n) < f (n + 1))} i.e. the set of alternating functions on N. Is A countable? Prove your claim. 11 1.5 Additional Exercises Here, some additional challenging exercises are added, where various concepts are mixed. Especially lots of exercises about functions are covered. Hands-On 5 5.1. Let A be an arbitrary non-empty set and let f : A → P(A) be a function. Prove that Zf /∈ f (A) for Zf := {x ∈ A|x /∈ f (x)}. 5.2. Let ρ ⊆ NN × NN be a relation on the set of all functions from N to N with f ρ g :⇔ |{x ∈ N | f (x) ̸= g(x)}| ≤ 1. Prove or disprove that ρ is an equivalence relation. 5.3. Let ρ be a relation on the set A of all functions from N \\ {0} to N \\ {0} with f ρ g :⇔ ∀x (f (x) | g(x)). Prove that (A, ρ) is a poset and determine all minimal and least elements of it. 5.4. Let f : A → B be a function. Let’s look at the following conditions: • f is injective • f is surjective • f is bijective Show that one of these conditions is sufficient and necessary for the statement: for all X ⊆ A we have f −1(f (X)) = X. 12 1.6 Solutions 1.6.1 Solutions for Hands-On 1 1.1 Proof. {2, 4, 6, 8} = {x ∈ N | ∃k (2k = x) ∧ x > 0 ∧ x < 10}. 1.2 Proof. {x ∈ N | ∃k (2k = x) ∧ x > 0}. 1.3 Proof. For simplicity, notice that B = {∅, {∅}}. For 5., keep in mind that P(∅) = {∅}. 1. A ∪ B = {∅, {∅}, {∅, {∅}}} ⇒ |A ∪ B| = 3 2. A ∩ B = {∅, {∅}} ⇒ |A ∩ B| = 2 3. P(B) = {∅, {∅}, {{∅}}, {∅, {∅}}} ⇒ |P(B)| = 4 4. A \\ P(B) = ∅ ⇒ |A \\ P(B)| = 0 5. B × P(∅) = {(∅, ∅), ({∅}, ∅)} ⇒ |B × P(∅)| = 2 1.4 Proof. Assumption: P(A) = P(B) Conclusion (to show): A = B First, we will show that A ⊆ B. Let x ∈ A be any element of A. ⇒ {x} ∈ P(A) definition of power set ⇒ {x} ∈ P(B) assumption ⇒ x ∈ B definition of power set Now, we show that B ⊆ A. Let x ∈ B be any element of B. ⇒ {x} ∈ P(B) definition of power set ⇒ {x} ∈ P(A) assumption ⇒ x ∈ A definition of power set We have shown that A ⊆ B and B ⊆ A. Combining these two results, we get A = B. 1.5 Proof. First, we need to observe that the statement is false for A = ∅ and true otherwise. 1. Case A = ∅ In this case, we get a counterexample for B = {0} and C = {1} since ∅ × {0} = ∅ = ∅ × {1}, but {0} ̸= {1}. So the statement is false for A = ∅. 13 2. Case A ̸= ∅ We want to prove that the statement is true in this case. Assumption: A × B = A × C Conclusion (to show): B = C Let a ∈ A be an element of A which has to exist since A ̸= ∅. We continue by making a further case distinction on B (we could also take C): (a) Case B = ∅ B = ∅ ⇒ A × B = ∅ definition of cartesian product ⇒ A × C = ∅ assumption ⇒ C = ∅ definition of cartesian product, A ̸= ∅ ⇒ B = C B = ∅ = C (b) Case B ̸= ∅ Let x ∈ B be an arbitrary element of B. ⇒ (a, x) ∈ A × B definition of cartesian product ⇒ (a, x) ∈ A × C assumption ⇒ x ∈ C definition of cartesian product So we have proven that B ⊆ C. Analogously, we get that C ⊆ B. By definition of set equality we conclude that B = C. Since the statement is true in all cases, we get that A × B = A × C ⇒ B = C for all A ̸= ∅. All in all, we get that the statement is correct, if and only if, A ̸= ∅. 1.6 Proof. We have to show that B = C by individually showing that B ⊆ C and C ⊆ B. 1. B ⊆ C Let b ∈ B be arbitrary. We can distinct two cases 1. Case: b ∈ A We have b ∈ A ⇒ b ∈ (A ∪ B) ∧ b ∈ (A ∩ B) ⇒ b ̸∈ (A ∪ B) \\ (A ∩ B) As. ⇒ b ̸∈ (A ∪ C) \\ (A ∩ C) (1) ⇒ b ∈ (A ∩ C) 0 ⇒ b ∈ C whereas (1) is based on b ∈ (A ∪ C) via assumption. 2. Case: b ̸∈ A We have b ̸∈ A ⇒ b ∈ (A ∪ B) ∧ b ̸∈ (A ∩ B) ⇒ b ∈ (A ∪ B) \\ (A ∩ B) As ⇒ b ∈ (A ∪ C) \\ (A ∩ C) (1) ⇒ b ∈ (A ∪ C) (2) ⇒ b ∈ C 14 whereas (1) follows from the fact that we subtract from (A ∪ C) and (2) follows from b ̸∈ A We therefore have B ⊆ C 2. C ⊆ B Follows analogous as it is completely symmetric to the first case. 3. Conclusion We have shown that (A ∪ B) \\ (A ∩ B) = (A ∪ C) \\ (A ∩ C) ⇒ B ⊆ C ∧ C ⊆ B therefore (A ∪ B) \\ (A ∩ B) = (A ∪ C) \\ (A ∩ C) ⇒ B = C 1.7 Claim 1.1 ∅, P(∅), P(P(∅)), P(P(P(∅))), . . . is a sequence of infinitely many different sets, where S ⊆ P (S) is true for all S in the sequence. Proof. We will use a classic proof by induction. 1. Base Let S = ∅, then we have P(S) = P(∅) = {∅} and by definition of ∅ trivially ∅ ⊆ {∅} 2. Hypothesis Let S be an arbitrary set such that S ⊆ P(S) (i.e. such an S exists) 3. Step Let S be an arbitrary set such that S ⊆ P(S), then we claim P(S) ⊆ P(P(S)) P(S) ⊆ P(P(S)) ⇔ ∀x (x ∈ P(S) → x ∈ P(P(S))) Let x ∈ P(S) be arbitrary. We have x ⊆ S by definition of the power set. Via induction hypothesis we also have that S ⊆ P(S) and therefore x ⊆ S ⊆ P(S) ⇒ x ⊆ P(S) ⇒ x ∈ P(P(S)) using the transitivity of the subset relation. This concludes our proof. To be completey correct we should also show that none of these sets are equal to one another. This directly follows from the fact that for all sets S we have |S| < |P(S)| 15 1.6.2 Solutions for Hands-On 2 2.1 • reflexivity: ρ is not reflexive since for example (a, a) /∈ ρ. • irreflxivity: ρ is irreflexive because (a, a), (b, b), (c, c), (d, d) /∈ ρ. • symmetry: ρ is symmetric because (a, b), (b, a) ∈ ρ, (b, c), (c, b) ∈ ρ and (b, d), (d, b) ∈ ρ. • antisymmetry: ρ is not antisymmetric because, for example (a, b), (b, a) ∈ ρ, but a ̸= b. • transitivity: ρ is not transitive because, for example (a, b), (b, c) ∈ ρ, but (a, c) /∈ ρ. 2.2 • ̂ρ = ρ = {(b, a), (a, b), (c, b), (d, b), (b, c), (b, d)} • ρ2 = {(a, a), (a, c), (a, d), (b, b), (c, a), (c, c), (c, d), (d, a), (d, c), (d, d)} • ρ∗ = {(a, a), (a, b), (a, c), (a, d), (b, a), (b, b), (b, c), (b, d), (c, a), (c, b), (c, c), (c, d), (d, a) (d, b), (d, c), (d, d)} 2.3 1. This statement is false. A counter example is σ = {(1, 1), (1, 2)} on the set A = {1, 2} because σ is not reflexive as (2, 2) /∈ σ but also it is not irreflexive as (1, 1) ∈ σ. 2. This statement is true. Since A ̸= ∅, there exists some a ∈ A. Due to the irreflexivity, we have (a, a) /∈ σ. This immediately implies that σ is not reflexive. 3. This statement is false. A counter example is σ = {(1, 1)} on the set A = {1}. This is symmetric because (1, 1) ∈ σ, but also antisymmetric as there are no two distinct elements a, b ∈ A such that (a, b), (b, a) ∈ σ (since A only has one element). This statement furthermore holds for any equivalence relation in which the individual classes have a cardinality of 1 2.4 Proof. • reflexivity: to show that ∼ is reflexive, we have to show that (x, y) ∼ (x, y) for all (x, y) ∈ R2. Let (x, y) ∈ R2 be arbitrary. We have that x2 + y = x2 + y so we have according to the definition of ∼ also (x, y) ∼ (x, y). • symmetry: we have to show that if (x, y) ∼ (a, b) for any (x, y), (a, b) ∈ R2, then we also have (a, b) ∼ (x, y). Let (x, y), (a, b) ∈ R2 such that (x, y) ∼ (a, b). This means that x2 + b = a 2 + y which implies that a 2 + y = x2 + b, so we have (a, b) ∼ (x, y). • transitivity: to show that ∼ is transitive, we have to show that (a, b) ∼ (c, d) and (c, d) ∼ (e, f ) for any (a, b), (c, d), (e, f ) ∈ R2 implies (a, b) ∼ (e, f ). Let (a, b), (c, d), (e, f ) ∈ R2 such that (a, b) ∼ (c, d) and (c, d) ∼ (e, f ). This means that a 2 + d = c2 + b and c2 + f = e2 + d which imply a 2 + d + c 2 + f = c 2 + b + e2 + d ⇒ a 2 + f = e2 + b which means (a, b) ∼ (e, f ). We have proven that ∼ is reflexive, symmetric and transitive so it is an equivalence relation according to the definition. 2.5 Proof. We will prove that (A, ̂⪯) is a poset. • reflexivity: we have to show that a ̂⪯ a for all a ∈ A. Let a ∈ A be arbitrary. Since (A, ⪯) is a poset, we have a ⪯ a, which implies according to the definition of the inverse that a ̂⪯ a. 16 • antisymmetry: we have to show that if a ̂⪯ b and b ̂⪯ a for any a, b ∈ A, then we have a = b. Let a, b ∈ A such that a ̂⪯ b and b ̂⪯ a. According to the definition of the inverse, we get that b ⪯ a and a ⪯ b. Since ⪯ is antisymmetric, we get that a = b. • transitivity: we have to show that a ̂⪯ b and b ̂⪯ c for any a, b, c ∈ A we have that a ̂⪯ c. Let a, b, c ∈ A such that a ̂⪯ b and b ̂⪯ c. According to the definition of the inverse, we have that b ⪯ a and c ⪯ b. Since ⪯ is transitive, we get that c ⪯ a which implies that a ̂⪯ c We have shown that (A, ̂⪯) is reflexive, antisymmetric and transitive so it is a poset. 2.6 Proof. Lets consider the relation ρ = {(a, a) | a ∈ N} on N. ρ is obviously reflexive since (a, a) ∈ ρ for all a ∈ N. It is also symmetric because a ρ b implies b ρ a for all a, b ∈ N since there doesn’t exist any (a, b) ∈ ρ with a ̸= b. For basically the same reason it is antisymmetric as well and it is also transitive because if we have a ρ b and b ρ c for any a, b, c ∈ N, then we also have a ρ c because again, there are no two different elements that are in the relation. 2.7 {a, b, c} {a, b} {b, c}{a, c} {a} {c}{b} {} {d} The minimal element is {}, the maximal elements are {a, b, c} and {d}. The least element is {} while there is no greatest element. 2.8 The lower bounds of {{a, b}, {a, c}} are {a} and {}. Its greatest lower bound is {a}. The poset (P({a, b, c}) ∪ {{d}}, ⊆) is not a lattice because for example {a} and {d} don’t have a least upper bound. 2.9 Proof. We have ̂ρσ = {(c, a) | a ρσ c} = {(c, a) | ∃b ∈ B : (a ρ b ∧ b σ c)} = {(c, a) | ∃b ∈ B : (b σ c ∧ a ρ b)} = {(c, a) | ∃b ∈ B : (c ̂σ b ∧ b ̂ρ a)} = ̂σ ̂ρ 2.10 1. • Reflexivity : ∀x ∈ Z : x + x = 2 · x, hence ∼ is reflexive. • Symmetry : ∀x, y ∈ Z : x ∼ y ⇐⇒ x + y is even ⇐⇒ y + x is even ⇐⇒ y ∼ x, hence ∼ is symmetric. • Transitivity : ∀x, y, z ∈ Z : x ∼ y ∧ y ∼ z ⇒ ∃k, k′ ∈ Z : x + y = 2 · k ∧ y + z = 2 · k′ ⇒ x + z = x + 2 · k′ − y = x + 2 · k′ − (2 · k − x) = 2(x + k′ − k) ⇒ x ∼ z Hence ∼ is transitive. 17 2. • Reflexivity : 0 · 0 = 0 =⇒ 0 ̸∼ 0, hence ∼ is not reflexive. • Symmetry : ∀x, y ∈ Z : x ∼ y ⇐⇒ x · y is odd ⇐⇒ y · x is odd ⇐⇒ y ∼ x, hence ∼ is symmetric. • Transitivity : ∀x, y, z ∈ Z : x ∼ y ∧ y ∼ z =⇒ x · y is odd ∧ y · z is odd =⇒ x, y, z are odd =⇒ x · z is odd =⇒ x ∼ z, hence ∼ is transitive. 3. • Reflexivity : ∀x ∈ Z : x + x · x = x(1 + x) =⇒ x is even or 1 + x is even =⇒ x · (1 + x) is even =⇒ x ∼ x, hence ∼ is reflexive. • Symmetry : 0 ∼ 1 as 0 + 0 · 1 = 0 but 1 ̸∼ 0 as 1 + 1 · 0 = 1, hence ∼ is not symmetric. • Transitivity : let a, b ∈ Z : – a is even and b is even =⇒ a + a · b is even =⇒ a ∼ b. – a is even and b is odd =⇒ a + a · b is even =⇒ a ∼ b. – a is odd and b is even =⇒ a + a · b is odd =⇒ a ̸∼ b. – a is odd and b is odd =⇒ a + a · b is even =⇒ a ∼ b. It follows that a ∼ b ⇐⇒ R2(|a|) ≤ R2(|b|). Let x, y, z ∈ Z : x ∼ y ∧ y ∼ z =⇒ R2(|x|) ≤ R2(|y|) ∧ R2(|y|) ≤ R2(|z|) =⇒ R2(|x|) ≤ R2(|z|) =⇒ x ∼ z, hence ∼ is transitive. 2.11 • Reflexivity : ∀a ∈ N+ : gcd(a, a) = a, hence ρ is reflexive. • Antisymmetry : ∀a, b ∈ N+ : aρb ∧ bρa =⇒ gcd(a, b) = a ∧ gcd(b, a) = b =⇒ a = gcd(a, b) = gcd(b, a) = b =⇒ a = b, hence ρ is antisymmetric. • Transitivity : ∀a, b, c ∈ N+ : aρb ∧ bρc ⇒ gcd(a, b) = a ∧ gcd(b, c) = b ⇒ a|b ∧ b|c ⇒ a|c ⇒ gcd(a, c) = a The third implication holds as the division relation is transitive, the last one holds as no integer greater than a can divide a, it follows that a is the gcd. Hence ρ is transitive. 18 1.6.3 Solutions for Hands-On 3 3.1 Proof. We disprove this statement with a counterexample. Let A = {1, 2}, B = {1} and f a function from the set {1, 2} to {1} with f = {(1, 1), (2, 1)}. We have that f (A \\ B) = {f (x) | x ∈ {A \\ B}} = {f (x) | x ∈ {2}} = {f (2)} = {1} but f (A) \\ f (B) = {f (x) | x ∈ A} \\ {f (x) | x ∈ B} = {1} \\ {1} = {} so f (A \\ B) ̸= f (A) \\ f (B). Since we have found a counterexample, the statement is false. 3.2 Proof. ”⇒”: Let a, b ∈ X such that a ̸= b (if that’s not possible then the function is trivially injective as there’s only one element in the domain). Let A = {a} and B = {b}. We have {a} ∩ {b} = ∅ ⇒f ({a} ∩ {b}) = ∅ Ass ⇒ f ({a}) ∩ f ({b}) = ∅ ⇒f (a) ̸= f (b) using that f (∅) = ∅ (because nothing will get projected onto nothing) and that f (a) ∈ f ({a}) as f ({a}) is a set (even if it contains only one element). Now we’ve exactly shown that a ̸= b ⇒ f (a) ̸= f (b) i.e. injectivity. ”⇐”: 1) f (A ∩ B) ⊆ f (A) ∩ f (B) Let A ∩ B ̸= ∅, because otherwise it’s trivially true. Furthermore let z be an arbitrary element in f (A ∩ B) with a ∈ (A ∩ B), f (a) = z. We get a ∈ A ∧ a ∈ B ⇒f (a) ∈ f (A) ∧ f (a) ∈ f (B) ⇒z ∈ f (A) ∩ f (B) ⇒f (A ∩ B) ⊆ f (A) ∩ f (B) 2) f (A) ∩ f (B) ⊆ f (A ∩ B) Let z ∈ f (A) ∩ f (B) be arbitrary (∅ is again trivial). We have z ∈ f (A) ∧ z ∈ f (B) ⇒∃a ∈ A, b ∈ B : f (a) = z ∧ f (b) = z Inj ⇒ a = b ⇒∃a ∈ A ∩ B : f (a) = z ⇒z ∈ f (A ∩ B) ⇒f (A) ∩ f (B) ⊆ f (A ∩ B) As the function is injective, there can exist at most one element a in X such that f (a) = z. A and B have to contain the same element. We have shown that f (A ∩ B) = f (A) ∩ f (B), concluding the proof. 3.3 Proof. Because of the iff, we have to prove two directions. 19 • There exists an injective function f : A → B ⇒ there exists a surjective function g : B → A. Let f : A → B be an injective function. Let a ∈ A be arbitrary (which exists because A is non-empty) and g : B → A be a function with g : { x ↦→ f −1(x) , if the inverse f −1(x) exists x ↦→ a , else . Let a ∈ A be arbitrary and b ∈ B such that b = f (a). According to the definition of g and because f is injective (so f −1(b) is unique), we have that g(b) = f −1(b) = a. Hence, g is surjective. Therefore, we have found a surjective function g in this case. • There exists a surjective function g : B → A ⇒ there exists an injective function f : A → B. Let g : B → A be a surjective function. Let f : A → B be a functions such that for any a ∈ A, we have that f (a) := b for some b ∈ B with g(b) = a which exists because g is surjective. It follows a = g(b) = g(f (a)) = (g ◦ f )(a). Let a, a ′ ∈ A with f (a) = f (a ′). Because of the definition of f , we also get a = (g ◦ f )(a) = g(f (a)) = g(f (a ′)) = (g ◦ f )(a ′) = a ′ so f is injective. Therefore, we have found an injective funtion f in this case. 3.4 Claim We claim that F is not injective. Proof. First let us take a look at the map of the constant 0 function ∀x. g(x) = 0. Trivially we have F (g) = g ◦ g = 0 for all x. In order to show that F is not injective we will create another function g′ that will also be mapped to the 0 function. Let g′ : N → N g′(0) ↦→ 0 g′(1) ↦→ 0 g′(x) ↦→ 1 Trivially g ̸= g′. Now realize that Im(g′) = {0, 1} and therefore (as g′(0) = 0 and g′(1) = 0) ∀x. (g′ ◦ g′)(x) = g′(g′(x)) = 0. Therefore F (g) = F (g′) ∧ g! = g′ and by definition of injectivity F is not injective. Claim We claim that F is not surjective. Proof. We will prove this by finding a function f such that ̸ ∃g. F (g) = f . (When this was an exam question this function was actually provided to you). Define f as f : N → N f (0) ↦→ 1 f (1) ↦→ 0 f (x) ↦→ x Now we continue by assuming there exists a g s.t. g ◦ g = f . We proceed by case distinction over the possible mappings of g: 20 Constraints (g ◦ g)(0) = f (0) = 1 (1) (g ◦ g)(1) = f (1) = 0 (2) (g ◦ g)(x) = f (x)= x (3) 1. Case g(0) = 0 Then via (1) (g ◦ g)(0)= f (0) ⇒g(g(0)) = f (0) ⇒g(0) = f (0) ⇒g(0) = 1 A contradiction. 2. Case g(0) = 1 Then via (1) (g ◦ g)(0)= f (0) ⇒g(g(0)) = f (0) ⇒g(1) = 1 using this we get via (2) (g ◦ g)(1)= f (1) ⇒g(g(1)) = f (1) ⇒g(1) = 0 A contradiction. 3. Case g(0) = k with k > 1 Then via (1) (g ◦ g)(0)= f (0) ⇒g(g(0)) = 1 ⇒g(k) = 1 Now we get via (3) (g ◦ g)(k)= f (k) ⇒g(g(k)) = k ⇒g(1) = k But then via (2) (g ◦ g)(1)= f (1) ⇒g(g(1)) = 0 ⇒g(k) = 0 A contradiction. As the case distinction was complete this concludes the proof. 21 1.6.4 Solutions for Hands-On 4 4.1 Claim We claim that A = ⋃n i=1 Ai is countable. Proof. We will prove it via induction but a proof via primes is also possible. 1. Basis We’ve already shown that the union of two countable sets is countable. 2. Assumption Assume that the union of n countable sets is countable. 3. Step: n → n + 1 We have to look at n+1⋃ i=1 Ai = n⋃ i=1 Ai ∪ An+1 and via our inductive assumption realize that ⋃n i=1 Ai is countable. Furthermore An+1 is countable by definition of the Ai’s. We have already shown that the union of two countable sets is countable which concludes the proof. 4.2 Claim We claim that A = ⋃∞ i=1 Ai is countable. Proof. We can prove this geometrically. Since all sets Ai are countable, for each of them there exists an injection fi : Ai → N. Using these injections, we can construct a new injection F : ⋃∞ i=1 Ai → N × N in the following way. For all i ∈ N, for all ai ∈ Ai, we define F (ai) = (i, fi(a)) Clearly, F is injective. We can easily see this, since for a, a ′ ∈ ⋃∞ i=1 Ai and a ̸= a ′ we have F (a) ̸= F (a′), since either a and a ′ are from different sets Ai ̸= Aj or they are from the same set Ai, in which case fi(a) ̸= fi(a ′) due to the injectivity of fi. In both cases, F (a) ̸= F (a ′). Since N × N is countable and F is injective, ⋃∞ i=1 Ai is countable. Note: In this proof we neglect the fact that the same element may occur multiple times in different sets. To get a rigorous definition of the function F , we would therefore need to explicitly exclude double- definitions for reoccurring elements. We omit this in favor of readability, as the correctness of the proof doesn’t depend on it. If we would have chosen the primes approach in (1) then we could have generalized it for (2) as we could make use of infinite number of primes. Nevertheless this does not hold for the induction approach (which is a very common mistake!). Showing that something holds for an arbitrarily large but finite n is not equal to showing that it holds for infinity! 4.3 Claim We claim that A = {f : N → N | ∀n ∈ N f (n) ≥ f (n + 1)} is countable. We will provide two proofs as each one emphasizes a different yet equally important take on such a task Proof. We start of by splitting S∞ := A = {f : N → N | ∀n ∈ Nf (n) ≥ f (n + 1)} into subsets SN , with each SN = {f : N → N | ∀n ∈ Nf (n) ≥ f (n + 1) ∧ f (0) = N } i.e. all functions that ”start at N ”. Given that every function in A has exactly one value f (0) ∈ N we have S∞ = ∞⋃ N =0 SN 22 We already know that the union of countably many countable sets is countable, so we’re left with showing that ∀N ∈ NSN is countable. We will prove this via induction on N (though other methods also work here): 1. Basis For N = 0 there exists only one function, namely ∀n ∈ N : f (n) = 0. As the set containing this function is finite, it is countable. 2. Assumption For arbitrary N ∈ N let SN be countable. 3. Step N → N + 1 We have to show that SN +1 is countable. There are two types of functions in SN +1, exactly one that never falls, i.e. ∀n ∈ N : f (n) = N + 1 always and all other functions which have a smallest k ∈ N such that f (k) > f (k + 1). These functions will then fulfil (via monotonicity assumption) ∀n > k : f (n) ≤ N . In particular we’re able to describe them through a function in SN , that behaves the same for all k′ ≥ k + 1. In conclusion we can identify every function f ∈ SN +1 that drops at least once with the tuple (k, f ′ k,f (n)) ∈ N × SN where k marks the the first drop and f ′ k,f (n) is the function in SN that shares the same behaviour after the drop. We have already shown that the product of countable sets is countable and as both N and SN are countable via assumption so is N × SN and in turn all (minus 1) functions in SN +1. Finally we unify this result with the single constant function, which obviously doesn’t change the countability and our induction is finished. Proof. This approach might be even more elegant than the last one. As all functions are monotonically decreasing, we can ”encode” them in a string containing their consecutive values ϕ : NN → N∞ f → f (0)f (1)f (2)f (3)... = x0x1x2x3... As every function f has a last k for which f (k − 1) > f (k) the resulting string will repeat itself for all k′ > k. Therefore we can maintain a unique encoding by just ”cutting” ϕ(f ) after position k. Let us call this mapping ϕ′ : NN → N∗ f → f (0)f (1)f (2)f (3)...f (kf ) = x0x1x2x3...xkf Now every function corresponds to one finite length string. As ϕ′ is injective by construction we are left with showing that N∗ = ⋃∞ k=0 Nk is countable. Every Nk is countable for k ∈ N (we’ve already shown that) and furthermore we’ve also shown that the countable union of countable sets is itself countable. Therefore ⋃∞ k=0 Nk is countable. This concludes the proof as it implies that A is countable. 4.4 Claim We claim that A = {f : N → N | ∀n ∈ N (n even → f (n) > f (n + 1) ∧ n odd → f (n) < f (n + 1))} is uncountable. Proof. We will prove that this set is uncountable, to do that we show an injection g from the set of semi-infinite bit-strings to A (g : {0, 1} ∞ → A). Let α ∈ {0, 1} ∞ be an arbitrary semi-infinite bit-string. We define g(α) = fα, where fα ∈ A is defined as follows : ∀n ∈ N : fα(n) = { 2 if n = 2k for some k ∈ N αk if n = 2k + 1 for some k ∈ N Notice that αk is the k-th bit of bit-string α. We now show that for any α ∈ {0, 1} ∞ the constructed function g(α) = fα belongs to the set A. Let α ∈ {0, 1}∞ be an arbitrary semi-infinite bit-string. 23 Let n ∈ N be arbitrary, if n is even then there exists some k ∈ N with n = 2k, fα(n) = 2 and fα(n + 1) = αk ∈ {0, 1}, therefore fα(n + 1) > fα(n). Using a similar argument one can show that when n is odd then fα(n) < fα(n + 1), it follows that g(α) = fα ∈ A for all α ∈ {0, 1}∞. We now have to show that g is an injection. Let α ∈ {0, 1} ∞ and β ∈ {0, 1} ∞ be two arbitrary semi-infinite bit-strings with α ̸= β, we have to show that fα ̸= fβ. As the bit-strings α and β differ there must be a position where the two bit-strings differ, therefore there must exist some k ∈ N with αk ̸= βk. We now set n = 2k + 1, and see that fα(n) = αk ̸= βk = fβ(n), this implies that the functions g(α) = fα and g(β) = fβ are different. This implies that the function g we gave is injective. From the existence of an injection from an uncountable set ({0, 1}∞) to A one can easily show that A has to be uncountable too. 24 1.6.5 Solutions for Hands-On 5 5.1 Proof. This proof works by contradiction: Assume that Zf ∈ f (A). I.e. there exists at least one a ∈ A for which f (a) = Zf . Now we continue with a case distinction: 1. Case: a ∈ Zf Then we have a ∈ Zf ⇒ a ̸∈ f (a) ⇒ a ̸∈ Zf a contradiction. 2. Case: a ̸∈ Zf Now we have a ̸∈ Zf ⇒ a ∈ f (a) ⇒ a ∈ Zf also a contradiction. As the case distinction was complete (as either a ∈ Zf or a ̸∈ Zf ), we conclude that our assumption had to be wrong. There does not exist an a ∈ A such that f (a) = Zf and thus Zf ̸∈ f (A) This concludes the proof. 5.2 Proof. We will show that this relation is not transitive. Let f, g, h ∈ NN such that f : n ↦→ 1 and g : { n ↦→ 0 , if n = 0 n ↦→ 1 , else h : { n ↦→ 0 , if n = 0 ∨ n = 1 n ↦→ 1 , else It’s easy to see that these functions satisfy f ρ g, g ρ h, but not f ρ h. Therefore, we have found a counterexample for transitivity and thus ρ is not an equivalence relation. 5.3 Proof. We will show that ρ on A is reflexive, antisymmetric and transitiv seperately. • reflexivity: for any f ∈ N \\ {0} N\\{0} be arbitrary. We have that f (x)|f (x) for any x ∈ N \\ {0} because f (x) · 1 = f (x). Hence, it is reflexive. • antisymmetry: let f, h ∈ N \\ {0} N\\{0} such that f ρ h and h ρ f . According to the definition of ρ, we get that ∀x (f (x) | g(x) ∧ g(x) | f (x)). Therefore, for all x ∈ N \\ {0} there exist some c1, c2 ∈ N such that f (x)c1 = g(x) and g(x)c2 = f (x), which implies f (x)c1c2 = f (x), so we have c1 = c2 = 1. Therefore, we have f (x) = g(x) for any x ∈ N \\ {0}. Hence, antisymmetry is proven. • transitivity: let f, g, h ∈ N \\ {0} N\\{0} such that f ρ g and g ρ h. According to the definition of ρ we have that f ρ g :⇔ ∀x (f (x) | g(x)) and g ρ h :⇔ ∀x (g(x) | h(x)). For any x ∈ N we get that f (x)|g(x) and g(x)|h(x). According to the definition of |, there exists c1, c2 ∈ N such that f (x)c1 = g(x) and g(x)c2 = h(x). So we get f (x)(c1c2) = h(x) which means f (x)|h(x). Hence, ρ is transitive. Hence, we have proven that ρ on A is reflexive, antisymmetric and transitiv and therefore, (A, ρ) is a poset. The least element and the only minimal element is the function f that fulfills f (n) := 1 ∀n ∈ N \\ {0}. 5.4 Proof. Let us first revise the definitions of sufficient and necessary: 25 • We say that A is sufficient for B if A ⇒ B holds • We say that A is necessary for B if ¬A ⇒ ¬B holds Furthermore let f −1 : B → A denote the inverse function of f if it exists. We will show that injectivity is a sufficient and a necessary condition for the statement. Let f be injective. Let x ∈ X be arbitrary. Trivially we have that x ∈ f −1(f (X)) as f (x) ∈ f (X) and ∀a ∈ A f (a) = f (x) ⇒ a ∈ f −1(f (X)). This implies that X ⊆ f −1(f (X)). Now assume there exists a y ∈ f −1(f (X))∧y /∈ X. According to the definition of the inverse, there has to exist a z ∈ f (X) such that f (y) = z. Using the same definition we conclude that there also needs to exist a x′ ∈ X with f (x′) = z (as z ∈ f (X)). But as y /∈ X and x′ ∈ X they can’t be the same element and we get x′ ̸= y ∧ f (x′) = f (y) which is a contradiction given f ′s injectivity. Therefore f −1(f (X)) ⊆ X. Putting it together yields the result f −1(f (X)) = X Let the statement ”for all X ⊆ A we have f −1(f (X)) = X” be true. Let a1, a2 ∈ A be such that f (a1) = f (a2) and a1 ̸= a2. For X = {a1, a2} we get f −1(f ({a1, a2})) = f −1(f (X)) = X = {a1, a2}, which means that f (a1) ̸= f (a2) and hence, f is injective. 26 2 Number Theory 2.1 Divisibility For all variables where it’s not explicitly stated otherwise we’ll assume the domain to be Z. Nevertheless most constructs and concepts usually hold for a wider variety of arithmetic structures. Definition 2.1. We say a | b, if there exists an integer such that b=ac. If the divisor is unique we can write c = b a Moreover we define that every non-zero integer is a divisor of 0 and that 1 and -1 are divisors of every integer. More generally we can define for all integers a and b the division with remainder: Theorem 2 ∀a∀b∃c∃r b = ac + r ∧ 0 ≤ r < |a| In the case a | b, we will get r = 0 2.2 Greatest Common Divisor For every integers a and b (not both 0) we can define a greatest common divisor which we will denote as gcd(a,b): Definition 2.2. The gcd d for a and b is defined as: d | a ∧ d | b ∧ ∀c ((c | a ∧ c | b) → c | d) If gcd(a,b) = 1, then we call a and b relatively prime. One very useful Lemma is the following: Lemma 2.1 ∀q ∈ Z we have gcd(m, n) = gcd(n, m) and gcd(m, n) = gcd(m, n − qm) 2.3 Euclid’s Extended GCD Algorithm For a, b ∈ Z (not both 0), there exist u, v ∈ Z such that gcd(a,b) = ua + vb We can use Euclid’s Extended GCD Algorithm to efficiently compute this u and v. In this script we will show an alternative way to execute this algorithm which is more suitable for a computation by hand. In the official script of the course you can find the original description of the algorithm (which is suitable for implementation on a computer). The algorithm can be separated into two phases 1. Calculating the gcd of the numbers a and b 27 2. Recovering the corresponding numbers u and v To explain our approach we will work with the example of calculating the gcd (and corresponding u and v) for a=553 and b=26. Phase 1 In each step of this phase you have to perform a division with remainder, i.e. to find the respective c and r for the equation b = ac + r. In our example we will find 553 = 26 · 21 + 7. Now we take the smaller of the two original numbers and the remainder r and perform another division with remainder(in this case 26 and 7). We continue executing this schema until the remainder r=0. We write down all intermediate steps of what we have done in the following way: 553 − 26 · 21 = 7 (1) 26 − 7 · 3 = 5 (2) 7 − 5 · 1 = 2 (3) 5 − 2 · 2 = 1 (4) 2 − 1 · 2 = 0 (5) The gcd(a,b) is the last r ̸= 0 which we have calculated (i.e. the number behind the equal-sign in the second to last line). In our case we get gcd(553,26) = 1. Phase 2 In this phase we will make use of what we have written down in Phase 1 to recover the u and v such that we get 553 · u + 26 ∗ v = gcd(553, 26) = 1. To start we take the second to last equation of our calculations in Phase 1 (remember that this equation already has the gcd(a,b) as the right hand side of the equation). For our example this means that we start with (4) 5 − 2 · 2 = 1 In each step of our equations we know that one of the numbers was the remainder (the number on the right hand side) of the equation above it. In our example the equation above is (3) 7 − 1 · 5 = 2. We can now just insert (3) into (4) to get 5 − 2 · (7 − 1 · 5) = 1 This can be simplified to (−2) · 7 + 3 · 5 = 1 Now we can use (2) to substitute the 5 so that we get (−2) · 7 + 3 · (26 − 7 · 3) = 1 which simplifies to 3 · 26 − 11 · 7 = 1 We just always continue substituting the smaller number for one of the equations higher up. So for our last step we use (1) to substitute 7. 3 · 26 − 11 · (553 − 26 · 21) = 1 so that we finally get 553 · (−11) + 26 · 234 = 1 which means u = -11 and v=234. 28 2.4 GCD Tableau Quite often we just need the a quick way to do all these computations together. One way to do this is writing them in a compact table. For the given numbers a = 553 and b = 26 we get the following one: a b q u1 u2 v1 v2 553 26 21 1 0 0 1 26 7 3 0 1 1 -21 7 5 1 1 -3 -21 64 5 2 2 -3 4 64 -85 2 1 0 4 -11 -85 234 1 0 2 -11 4 234 -85 by simply applying the following set of rules (given that xi describes a variable (x ∈ {a, b, q, u1, u2}) in the i-th row): • The first row contains our a and b as well as the q = ⌊ a b ⌋. The other variables are always 1 0 0 1. • For any row i > 1 we have ai = bi−1, bi = ai−1 mod bi−1, qi = ⌊ ai bi ⌋, (u1)i = (u2)i−1 as well as (v1)i = (v2)i−1 • For any row i > 1 we have (u2)i = (u1)i−1 − qi−1 ∗ (u2)i−1 as well as (v2)i = (v1)i−1 − qi−1 ∗ (v2)i−1 • Repeat this until b = 0 • a contains the gcd. u1, v1 contain the respective factors to get it via multiplication. While these ”formulas” look like a lot in the beginning, more than half of it is just simply transferring values. After having done it several times it’s a really fast way to calculate the gcd. 2.5 Ideals Definition 2.3. We define the Ideal generated by two integers a and b in the following way: (a, b) = {ua + vb | u, v ∈ Z} A special case is the ideal defined by only one integer (a) = {ua | u ∈ Z} Moreover we know about the following two Lemmas (4.3 and 4.4) from the course script. Collection 2.1 • For a, b ∈ Z there exists d ∈ Z such that (a, b) = (d). • Let a, b ∈ Z (not both 0). If (a, b) = (d), then d is a greatest common divisor of a and b. As a matter of fact the gcd of a and b is the smallest positive element in their ideal: (a, b) = (gcd(a, b)). Generally it can be noted that ideals are the topic of relatively few exam problems. 2.6 Irrationality of Roots and Logarithms Usually when you have to prove the irrationality of a number, you do a proof by contradiction. You assume that the number is rational and show that this assumption leads to a contradiction to some fundamental knowledge about numbers that you already have (in the scope of this course the contradiction will usually be with the Unique Prime Factorization) 29 Definition 2.4. Every positive integer can be written uniquely as the product of primes. Example. Here we show as an example that log5 7 is irrational: 1. Assume that log5 7 ∈ Q i.e. log57 = a b 2. Show that this leads to a contradiction log57 = a b 7 = 5 a b 7b = 5 a This contradicts the unique prime factorization, because it would mean that two numbers with different prime factorizations are equal. Thus log5 7 has to be irrational. For roots the script already gives us the following theorem (4.9) which could be proved through a similar approach. Theorem 3 √n is irrational unless n is a square (n = c 2 for some c ∈ Z) 2.7 Least Common Multiples The least common multiple of two positive integers a and b is well-defined and unique. It can be denoted lcm(a,b). Definition 2.5. The lcm l for a and b is defined as a | l ∧ b | l ∧ ∀m(( a | m ∧ b | m) → l | m ) The least common multiple is closely related to the greatest common divisor (gcd). In fact it holds that: gcd(a, b) · lcm(a, b) = ab This can be explained by looking at the unique prime factorization of a and b a = ∏ i pei i b = ∏ i p fi i It follows that gcd(a, b) = ∏ i pmin(ei,fi) i and lcm(a, b) = ∏ i pmax(ei,fi) i Hands-On 1 1.1. Give the prime factorization of 45 and 42. Then use the factorizations to calculate: 1. gcd (45,42) 2. lcm (45,42) 3. 45 · 42 1.2. Let n be a positive integer and let Dn be the set of all positive divisors of n. Show |Dn| is not even ⇐⇒ ∃r ∈ N such that r2 = n 1.3. Prove that gcd(x-zy, y) = gcd(x,y) 1.4. Let a, b ∈ Z be any two different integers. Prove that there exists infinitely many k ∈ Z such that a + k and b + k are relatively prime. (You can use 1.3) 30 2.8 Modular Congruences and Arithmetic Definition 2.6. We define modular congruence in the following way a ≡m b :⇐⇒ m | (a − b) We say that a is congruent to b modulo m. Lemma 2.2 It follows that a = b =⇒ a ≡m b and a ̸≡m b =⇒ a ̸= b The second lemma can be very useful for proving inequalities with case distinction (e.g. for m=2 you can do a case distinction for even and odd numbers). Note these important rules which are given in Lemma 4.14 (script): If a ≡m b and c ≡m d, then a + c ≡m b + d and ac ≡m bd For any m ≥ 1, ≡m is an equivalence relation on Z with m equivalence classes [0], [1], ... , [m-1]. Each of those equivalence classes is represented by one of the possible remainder modulo m. We introduce the notation for a remainder modulo m Rm(a).This remainder is just a number Rm(a) ∈ Z, which represents the remainder in a division with remainder of a by m. The concept of a remainder modulo m and congruence modulo m are very similar. In fact the two concepts are closely connected through Lemma 4.16 (script) 1. a ≡m Rm(a) 2. a ≡m b ⇐⇒ Rm(a) = Rm(b) The following arithmetic rules concerning the remainder operator, which are given by Lemma 4.18, are important and will be useful in many exam problems for number theory and are also essential to understanding the Diffie-Hellman protocol: 1. Rm(a + b) = Rm(Rm(a) + Rm(b)) 2. Rm(ab) = Rm(Rm(a) ∗ Rm(b)) 31 2.9 Multiplicative Inverses Lemma 2.3 The congruence equation ax ≡m 1 has a solution x ∈ Zm if and only if gcd(a,m) = 1. The solution is unique. We call x the multiplicative inverse of a modulo m. And we use the following notations x ≡m a −1 or x ≡m 1 a . If gcd(a, m) ̸= 1 then no multiplicative inverse modulo m exists for a. To calculate the multiplicative inverse efficiently we can use Euclid’s Extended GCD Algorithm. If we set b = m, then the Algorithm will give us u and v such that au + mv = 1. It then follows that m | (au − 1) which is the definition of au ≡m 1. This means that the u we found through the algorithm is our multi- plicative inverse x = u. Hands-On 2 2.1. Show that for every odd a ∈ N holds either 3 | a, or 3 | (a + 2), or 3 | (a + 4) 2.2. Show that 11 | (22016 + 32016 − 1) 2.3. Let sa be the sum of the decimal digits of a ∈ N. Show that R9(a) = R9(sa) 2.4. Calculate R9(988777666655555444444333333322222222111111111). 2.10 Chinese Remainder Theorem Definition 2.7. For m1, m2, ..., mr pairwise relatively prime, the systems of congruence equations x ≡m1 a1 x ≡m2 a2 ... x ≡mr ar has a unique solution satisfying 0 ≤ x < M = ∏r i=1 mi The Chinese Remainder Theorem can be useful in two different ways: 1. It tells us that for a certain set of equations a unique solution exists in a certain range of numbers. This can be useful for more abstract exercises where we are not given explicit values. 2. It can show us how to construct the solution, i.e. how to find the x which satisfies all the equations. This will give us the explicit solution in exercises with explicit values in the problem. We will now show how to construct this explicit solution: First we define Mi = M/mi for each i. Now for each of these Mi we have to find the multiplicative inverse modulo mi, i.e. we have to find Ni such that MiNi ≡mi 1 As explained in the previous chapter you can find these Ni by using Euclid’s Algorithm, but often the numbers are small and you can save time by trying out the possibilities in your head. Now you can already calculate the solution to the equation system by using the sub-results you just calculated: x = RM ( r∑ i=1 aiMiNi ) 32 2.11 Diffie-Hellman protocol The Diffie-Hellmann protocol is used to solve the key distribution problem. The final goal of the protocol is that both communicating parties (usually called Alice and Bob) are in possession of a secret key, which no outside party can know. The protocol is based on the important assumption that exponentiation can be computed efficiently, but computing logarithms (the inverse operation) can not be done efficiently. The public parameters of the protocol are a (very large) prime number p and the generator g. These parameters are known by everybody including potential adversarial parties. Moreover Alice and Bob can only communicate through a public/insecure channel. The protocol works in the following way 1. Alice selects xA randomly from {0, ..., p-2}. This xA will stay secret. 2. Alice efficiently calculates yA := Rp(gxA). yA will be public. 3. Bob selects xB randomly from {0, ..., p-2}. This xB will stay secret. 4. Bob efficiently calculates yB := Rp(gxB ). yB will be public. 5. Now Alice and Bob exchange their respective public keys yA and yB through the public/insecure channel (due to the inefficiency of calculating logarithms it is not possible to calculate xA even if yA, p and g are known) 6. Alice efficiently calculates kAB := Rp(yxA B ) 7. Bob efficiently calculates kBA := Rp(yxB A ) 8. Through modular arithmetics we can see that kAB ≡p kBA : kAB ≡p yxA B ≡p (gxB ) xA ≡p gxAxB ≡p (gxA ) xB ≡p yxB A ≡p kBA Thus we have achieved our goal: Alice and Bob are both in possession of the same secret key. 33 Hands-On 3 3.1. Calculate all the solutions 0 ≤ x < 130 for the following system of equations x ≡2 1 x ≡5 2 x ≡13 3 3.2. Calculate all solutions in N for the following system of equations. Justify why you have all answers. x ≡10 6 x ≡15 11 3.3. Calculate all solutions in 0 ≤ x < 195 for the following system of equations. Justify why you have all answers. 2 · x2 + 8 ≡13 6 x ≡15 2 3.4. Alice and Bob use the Diffie-Hellman key agreement protocol to communicate. The public parameters are p = 19 g = 2. The secret parameters which they choose are xA = 6 and xB = 11. Calculate yA, yB and the keys kAB and kBA 3.5. We give you an exercise that goes a bit beyond the material already covered, but you should be able to follow the steps below without reading ahead: Find an isomorphism from Z3 × Z5 to Z15 using the CRT. Prove your answer. (a) Define a reasonable function ϕ : Z3 × Z5 → Z15 (b) Prove that ϕ is a function (i.e. well-defined and totally defined) (c) Prove that ϕ is a homomorphism, i.e. that for all a, c ∈ Z3 and all b, d ∈ Z5: ϕ((a, b) ⊕ (c, d)) = ϕ((a, b)) ⊕15 ϕ((c, d)) (⊕ is component-wise addition in Z3 and Z5 respectively) (d) Prove that ϕ is surjective (e) Prove that ϕ is bijective (this is easy!) And now you’ve found a bijective homomorphism - in other words an isomorphism. 34 2.12 Solutions 2.12.1 Solution for Hands-On 1 Exercise 1.1 The prime factorizations are 45 = 3 2 · 5 1 42 = 2 1 · 3 1 · 7 1 We get the gcd by taking the minimum occurring exponent of each prime factor. gcd(45, 42) = 2 0 · 31 · 50 · 7 0 = 3 We get the lcm by taking the maximum occurring exponent of each prime factor. lcm(45, 42) = 2 1 · 3 2 · 5 1 · 71 = 18 · 35 = 630 Now we can calculate 45 · 42 = gcd(45, 42) · lcm(45, 42) = 630 · 3 = 1890 Exercise 1.2 We give a proof of the form S ⇒ T and ¬S ⇒ ¬T , which in turn proves S ⇔ T . We do this to obtain two very similar logical derivations, so we can reuse the same idea for both directions. (S is the right-hand side.) (S ⇒ T ) Assume ∃r ∈ N s.t. r2 = n. This r is therefore in Dn and unique. For every other element a ∈ Dn exists a unique element ∈ Dn for which a ̸= b s.t. ab = n. Let m denote the number of pairs of such two elements a and b for which a < b. (The < means we only count each pair once.) Since elements in those pairs and r cover all elements in Dn we have |Dn| = 2m + 1. Cleary, |Dn| is not even. (¬S ⇒ ¬T ) Assume ¬∃r ∈ N s.t. r2 = n. Thus for every other element a ∈ Dn exists a unique element ∈ Dn for which a ̸= b s.t. ab = n. Let m denote the number of pairs of such two elements a and b for which a < b. Since the elements in those pairs cover all elements in Dn we have |Dn| = 2m. Cleary, |Dn| is even. Exercise 1.3 Let p = gcd(x-zy,y) ⇒ p | (x − zy) ∧ p | y ⇒ ∃n1 ∈ N pn1 = x − zy ∧ ∃n2 ∈ N pn2 = y ⇒ pn1 = x − zpn2 ⇒ pn1 + zpn2 = x ⇒ p(n1 + zn2) = x ⇒ p | x ∧ p | y Let q ∈ N with q | x ∧ q | y ⇒ ∃m1qm1 = x ∧ ∃m2 qm2 = y ⇒ x − zy = qm1 − zqm2 ⇒ x − zy = q(m1 − zm2) ⇒ q | (x − zy) ∧ q | y We know that q is a common divisor of x−zy and y, so by definition, it must also divide gcd(x−zy, y) = p. ⇒ q | p ⇒ p | x ∧ p | y ∧ (∀q(q | x ∧ q | y) → q | p) ⇒ p = gcd(x, y) ⇒ gcd(x − zy, y) = gcd(x, y) Exercise 1.4 W.l.o.g. assume that b > a. We have by (1.3) that gcd(a + k, b + k) = gcd(a + k, b + k − (a + k)) = gcd(a + k, b − a) Now we can see that we just have to choose k in a way such that gcd(a + k, b − a) = 1. There exists an infinite amount of such ki e.g. ki = pi − a where pi is a prime that is not part of the prime factorization of b − a (Trivially there exists an infinite number of such pi). Feel encouraged to come up with another general form for the ki. 35 2.12.2 Solution for Hands-On 2 Exercise 2.1 We use a case distinction which covers all possible cases for a remainder modulo 3: 1. Case R3(a) = 0 ⇒ 3 | a 2. Case R3(a) = 1 ⇒ ∃k a = 3.k + 1 ⇒ ∃k a + 2 = 3.k + 3 ⇒ 3 | (a + 2) 3. Case R3(a) = 2 ⇒ ∃k a = 3.k + 2 ⇒ 3 | (a + 1) ⇒ 3 | (a + 4) The statement hold for all possible cases, thus it holds for every a ∈ N. Thus it holds for every odd a. Exercise 2.2 Check R11(22016 + 32016) under consideration of R11(210) = 1 and R11(35) = 1. R11(22016 + 32016) = R11((2 10) 201 · 26 + (35) 403 · 3) = R11(R11(210)201 · R11(2 6) + R11(3 5) 403 · R11(3)) = R11(1 · R11(26) + 1 · R11(3)) = R11(9 + 3) = R11(1) = 1 ⇒ 11 | (2 2016 + 32016 − 1) Exercise 2.3 Let a have n digits and let ak be the k’th digit of a. a = ∑n−1 k=0 ak10k = ∑n−1 k=0 ak(10 k − 1) + ∑n−1 k=0 ak = ∑n−1 k=0 ak(10 k − 1) + sa Now we consider that R9(10 k − 1) = R9(R9(10) k − 1)) = R9(1 − 1) = 0 Thus ∑n−1 k=0 ak(10 k − 1) + sa ≡9 sa and it follows that R9(a) = R9(sa) Exercise 2.4 We use from Exercise 1.3 hat R9(a) = R9(sa). R9(988777666655555444444333333322222222111111111) = R9(9+2·8+3·7+4·6+5·5+6·4+7·3+8·2+9) = R9(5 · 5 + 2(2 · 8 + 3 · 7 + 4 · 6 + 9)) = R9(25 + 2 · 70) = R9(165) = R9(1 + 6 + 5) = R9(3) = 3 36 2.12.3 Solution for Hands-On 3 Exercise 3.1 Since 2, 5 and 13 are relatively prime, the Chinese Remainder Theorem tells us that there is only one unique solution for 0 ≤ x < 130 = 2 · 5 · 13 First we calculate Mi = M/m1 M1 = 130/2 = 65 M2 = 130/5 = 26 M3 = 130/13 = 10 Now we find Ni such that Mi · Ni ≡mi 1 65 · N1 ≡2 1 ⇒ N1 = 1 26 · N2 ≡5 1 ⇒ N2 = 1 10 · N3 ≡13 1 ⇒ N3 = 4 Now we can calculate our x: x = R130(1 · 65 · 1 + 2 · 26 · 1 + 3 · 10 · 4) = R130(237) = 107 Exercise 3.2 Note that 10 and 15 are not relative prime. Therefore we cannot use the CRT directly. To remedy this we will decompose the system into the individual prime factors: Via 10 = 2 · 5 and 15 = 3 · 5 our system x ≡10 6 x ≡15 11 becomes x ≡2 0 x ≡3 2 x ≡5 1 as 6 mod 2 = 0 and 11 mod 3 = 2. Note that both 11 and 6 are 1 mod 5. If that would not be the case the system would not have a solution. Now we will solve this system via CRT as 2, 3 and 5 are relative primes. The CRT solution (see procedure in task 1) is x = 26 which is unique up to 2 ∗ 3 ∗ 5 = 30. Not by coincidence 30 is also the lcm of 10 and 15. Therefore the general solution to the system is x = 26 + n · 30, n ∈ N It is easy to see that all numbers of this form are solutions to the system. To prove it is the only set of solutions assume there exists another solution xo. Then xo ± 30 would also be a solution (show why). We can therefore construct a solution xi ̸= 26 in the range 0 ≤ xi < 30. This is a contradiction to the CRT. Exercise 3.3 First let us bring the system into a more orderly form. 2 · x2 + 8 ≡13 6 x ≡15 2 is equivalent to 2 · x2 ≡13 11 x ≡15 2 is equivalent to (7 is the multiplicative inverse of 2 mod 13) x2 ≡13 12 x ≡15 2 37 Now we need to decompose the equation x2 ≡13 12. A simple quadratic form has at most 2 solutions. One trick to find these more easily is to add 13 to the right side until we hit a square number. In our case we get the sequence 12, 25 = 52. So we know that 5 and −5 ≡13 8 are the two solutions to our system (another trick is using that x2 = k implies (−x) 2 = k). Now we have the two systems: x ≡13 5 x ≡15 2 and x ≡13 8 x ≡15 2 Solving each one with the CRT gives us x0 = 122 and x1 = 47 respectively. These are unique in 0 ≤ xi < 13 ∗ 15 = 195. Therefore we’ve computed all solutions to our initial system. Exercise 3.4 yA = R19(2 6) = 7 yB = R19(211) = 15 Now we can calculate the keys kAB = R19(15 6) = 11 kBA = R19(711) = 11 Note that as expected we got kAB = kBA. This means Alice and Bob both calculated the same key. Exercise 3.5 (a) Define a reasonable function ϕ : Z3 × Z5 → Z15 We define ϕ as follows: ϕ((a, b)) = x such that x ≡3 a ∧ x ≡5 b (b) Prove that ϕ is a function (i.e. well-defined and totally defined) We can use the fact that 3 and 5 are coprime and what we know about the CRT to conclude that such an x exists for any choice of a and b (ϕ is totally defined) and also that such a solution is unique in Z15 (ϕ is well-defined). (c) Prove that ϕ is a homomorphism, i.e. that for all a, c ∈ Z3 and all b, d ∈ Z5: ϕ((a, b) ⊕ (c, d)) = ϕ((a, b)) ⊕15 ϕ((c, d)) (⊕ is component-wise addition in Z3 and Z5 respectively) We first calculate that (a, b) ⊕ (c, d) = (a ⊕3 b, c ⊕5 d). Thus, the left-hand side is equal to some x ∈ Z15 where: x ≡3 a ⊕3 b ≡3 a + b x ≡5 c ⊕5 d ≡3 c + d From the definition of ϕ, we also get that ϕ((a, b)) = y, ϕ((c, d)) = z for some y, z ∈ Z15 where: y ≡3 a z ≡3 c y ≡5 b z ≡5 d Now all that’s left is to combine these equations to get: y + z ≡3 a + b y + z ≡5 c + d 38 And because 15 is a multiple of both 3 and 5 also: y ⊕15 z ≡3 a + b y ⊕15 z ≡5 c + d From which we see that x ∈ Z15 and y ⊕15 z ∈ Z15 are solutions to the same system of modular congruences with coprime moduli, and because the solution to this system is unique in Z15, we know: ϕ((a, b) ⊕ (c, d)) = x = y ⊕15 z = ϕ((a, b)) ⊕15 ϕ((c, d)) (d) Prove that ϕ is surjective We need to show that any a ∈ Z15 is the solution to some system of modular congruences. Let a ∈ Z15 be arbitrary. We can trivially check that x is a solution to the system: x ≡3 R3(a) x ≡5 R5(a) Therefore, for all a ∈ Z15, we can find (b, c) ∈ Z3 × Z5 such that ϕ((b, c)) = a and thus, ϕ is surjective. (e) Prove that ϕ is bijective (this is easy!) Any surjection from a finite set to another finite set of equal cardinality is a bijection. As Z3 × Z5 and Z15 are both finite and have the same number of elements (15), we know that our surjection ϕ must also be a bijection. Now we have proved all we need in order to claim that ϕ is an isomorphism from Z3 × Z5 to Z15. 39 3 Algebra 3.1 Algebras This section is meant to provide a general introduction into algebras by simply recapitulating the useful definitions and introducing the concepts needed in the following subchapters. Definition 3.1 (Algebra). An algebra is the combination of a set and several operations on that set. Strictly speaking we have a set S of elements which the operations ”operate on” and a set Ω which contains all the operations that we defined. We call ⟨S; Ω⟩ an algebra on S. For every operation ω ∈ Ω we require ω to be of the form ω : Sn → S , n ∈ N or in words: it does not ”leave” the set S and it always takes the same amount of arguments. While it is good to know this general definition, we will now mainly focus on one special case of algebras, in explicit algebras with one binary operation (i.e. on operation that takes two arguments). Definition 3.2 (Associativity). An operation ω on a set G is called associative if and only if we have for any a, b, c in G a ω (b ω c) = (a ω b) ω c • It is quite popular to just not use any parentheses at all when only using ω. This practise is generally (i.e. in the context of this course) not recommendable, as we lose formalism (in a strict sense a ω b ω c is not properly defined) and therefore usually points in the exam. Example. An example of associative operations would be addition and multiplication on the set R, while exponentiation is not associative on R i.e. (2 3)2 = 8 2 = 64 ̸= 2(3 2) = 2 9 = 512 Definition 3.3 (Identity). An operation ω on a set G has an identity if and only if ∃e ∈ G ∀a ∈ G a ω e = e ω a = a We call e the ”neutral element” or simply identity. We can weaken this condition to ∃e′ ∈ G ∀a ∈ G a ω e ′ = a and then call e′ a ”right-neutral element”. The definition of ”left-neutral element” follows symmet- rically. If we have a right- and a left-neutral element then they have to be equal since el = el ω er = er • Given the small proof above it follows that there exists at most one neutral element per operation ω, as e is left- and right-neutral. • Addition and multiplication on R have the neutral element 0 respectively 1 40 Definition 3.4 (Inverse). An operation ω on a set G is invertible if and only if for all a ∈ G there exists an ̂a ∈ G such that a ω ̂a = ̂a ω a = e where e denotes the identity of ω. We call ̂a the inverse element of a. • Given this formalism we have division and subtraction as as special case of multiplication and addition with the respective inverse elements. It should be mentioned that the function that maps an element to its inverse is unary and not binary. This usually leads to some confusion when working with ”−” in rings as we’re used to interpret ”−” as a binary operator while in our context it simply denotes the inverse of the element. Example. For every element a ∈ R\\{0} we have an inverse element regarding addition and multiplica- tion, namely −a and 1 a . On Z we still have the inverse regarding addition but no longer regarding multiplication. On N for neither of the operations. We therefore conclude that it is very important to look at the set that we are operating on in order to define the properties of an operation ω. Definition 3.5 (Commutativity). An operation ω on a set G is called commutative if and only if for all a, b ∈ G a ω b = b ω a • If an algebra has a commutative operation we usually call this algebra ”abelian” Example. Addition and multiplication are commutative on R, while exponentiation is not, e.g. 2 3 ̸= 3 2 Given all these definitions we can now classify algebras with only one operation based on the fulfilment of the definitions. We get the following chart: • Associativity Identity Inverse Commutativity Monoid Needed Needed Not Needed Not Needed Group Needed Needed Needed Not Needed Abelian Group Needed Needed Needed Needed 3.2 Groups We already know from the previous chapter what conditions an algebra has to fulfil in order to be considered a group. As proofs involving groups are usually ”stricter” than other kinds of proofs we abbreviate and formulate each of the three conditions as a so-called ”group-axiom” in order to define groups more easily. Definition 3.6 (Group). A group is an algebra ⟨G; ∗⟩ fulfilling the following conditions/axioms: G1 ∗ is associative in G G2 There exists a (unique) neutral element e in G G3 Every element a ∈ G has an inverse element ̂a in G We write (unique) in parentheses as the fact that they there can be only one straight-up follows from the three axioms. It is often recommended to denote a group in a more verbose way, i.e. by explicitly 41 stating the neutral element (here e) and the operator that sends an element on its inverse: ̂: G → G a ↦→ ̂a which allows us to write ⟨G; ∗,̂, e⟩ We have already seen that every operator has a unique neutral element, we therefore only need to proof that the same holds for inverse elements. Furthermore this can be seen as an exemplary proof regarding statements about general groups as it uses only the three axioms. Claim Let ⟨G; ∗,̂, e⟩ be a group. Then for every a ∈ G we have exactly one inverse element ̂a. Proof. Let a be an arbitrary element in G. By axiom G3 we know that there exists at least one inverse element ̂a. Let ̂a ′ be another inverse element, then: ̂a G2 = ̂a ∗ e G3 = ̂a ∗ (a ∗ ̂a ′) G1 = (̂a ∗ a) ∗ ̂a ′ G3 = e ∗ ̂a ′ G2 = ̂a ′ There are several lemmas in the script that you can use in your exam without having to proof them (obviously as long as it isn’t your task to proof them), they are as follows Collection 3.1 Following lemmas hold for every group, i.e. let ⟨G; ∗,̂, e⟩ be a group and a, b any two elements in them. • ̂(̂a) = a • ̂a ∗ b = ̂b ∗ ̂a • a ∗ b = a ∗ c ⇒ b = c (left cancellation law) • b ∗ a = c ∗ a ⇒ b = c (right cancellation law) • a ∗ x = b has a unique solution x for every a, b While one can usually use these lemmas without proving them it is a quite useful exercise to prove them. (Therefore they are included in the Hands-On part) One type of group is used quite often and therefore its notation should be known: Example. • The group ⟨Zn; ⊕, ⊖, 0⟩ is the group of all natural numbers (including 0) up to n − 1 using the binary operation ⊕ which denotes the addition modulo n. It is important to remember that despite the fact that we are not used to it ⊖ is an operation that takes only one element and not two. We sometimes simply write Zn. • The group ⟨Zp − {0}; ⊙,−1 , 1⟩ is the group of all natural numbers (excluding 0) up to p − 1 using the binary operation ⊙ which denotes the multiplication modulo p. In order for it to be a group p has to be prime. We sometimes write this as Z ∗ p (we will define this notation later). 3.2.1 Subgroups 42 Definition 3.7 (Subgroup). A set of elements H is called a subgroup of a group ⟨G; ∗,̂, e⟩ if the following conditions hold • ⟨H; ∗,̂, e⟩ is a group • H ⊆ G • For all a, b ∈ H we have a ∗ b ∈ H (it is closed under its operation) • e ∈ H (it is closed regarding the identity) • Every a ∈ H has an inverse element ̂a ∈ H (it is closed under inversion) We conclude that there always exist two trivial subgroups of a group ⟨G; ∗,̂, e⟩, namely {e} and G. In general it is not trivial to find the number of subgroups of a group. For a given group ⟨G; ∗,̂, e⟩ a usual task is to prove that some subset H ⊆ G is in fact a subgroup of ⟨G; ∗,̂, e⟩ (or not). The main way to find out whether a set is a subgroup is to simply check all the the individual conditions one by one. While it might not always be the fastest, it is a safe and structured way to solve the task at hand. There are two other options, but as they are not explicitly stated in the script they require a proof when using them and are therefore part of the Hands-On. 3.2.2 Order of groups and elements - Cyclic groups and generators One might just use the group operation over and over again on the same element. Despite the simplicity of this procedure we actually get several useful properties out of it. Most of them are captured in the following definitions: Definition 3.8 (Order of a Group). For any given finite group G (G here represents the whole group, not only the set of elements) we denote the order of the group as |G| = number of elements in G in comparison to that we have for the order of an element Definition 3.9 (Order of an element). Given an element a of a finite group ⟨G; ∗,̂, e⟩ we denote the order of this element with ord(a) = r where r is the lowest number such that a r = a ∗ a ∗ · · · ∗ a︸ ︷︷ ︸ r times = e We can prove by usage of the Pigeonhole-Principle that for every finite group also the order of each of its elements has to be finite. Furthermore we have the corollary that Corollary 3.1 Given an element a of a finite group ⟨G; ∗,̂, e⟩ we have a m = a Rord(a)(m) 43 which directly follows from the division theorem i.e. because if m = k · ord(a) + r with r < ord(a) a m = a ∗ a ∗ · · · ∗ a︸ ︷︷ ︸ ord(a) times ∗ a ∗ a ∗ · · · ∗ a︸ ︷︷ ︸ ord(a) times ∗ · · · ∗ a ∗ a ∗ · · · ∗ a︸ ︷︷ ︸ ord(a) times ︸ ︷︷ ︸ k times ∗ a ∗ a ∗ · · · ∗ a︸ ︷︷ ︸ r times = ek ∗ a r = a r = a Rord(a)(m) These definitions prove themselves to be especially useful when looking at further concepts like cyclic groups and subgroups generated by elements. We will handle the latter first: Theorem 4: Subgroups generated by an element For any element a of a finite group ⟨G; ∗, −1 , e⟩ we have that ⟨a⟩ := {a n | n ∈ Z} is a subgroup of ⟨G; ∗,−1 , e⟩. There are two remarks that have to be made regarding this theorem. The first is that we should definitely prove that we actually always generate a group by applying this procedure. Claim Let ⟨G; ∗, −1 , e⟩ be a finite group. Then for any a ∈ G we have that ⟨a⟩ is a subgroup of ⟨G; ∗, −1 , e⟩. Proof. Let a ∈ G be arbitrary. We show that ⟨a⟩ is a subgroup of ⟨G; ∗, −1 , e⟩ by individually showing that each required condition holds. • For all x, y ∈ ⟨a⟩ we have that x ∗ y ∈ ⟨a⟩ Any element x in ⟨a⟩ can be written as a mx = x where mx < ord(a) is the smallest natural number fulfilling a k = x. We therefore have x ∗ y = a mx ∗ a my = a mx+my ∈ ⟨a⟩ We used already known laws of power and the definition of ⟨a⟩ • e ∈ ⟨a⟩ This directly follows from the definition of the order of an element. Otherwise we also have that a 0 = e ∈ ⟨a⟩. This last part follows from the fact that a 0 ∗ ak = a k ∗ a 0 = a k (one should mention that this is not a complete proof). • Every x ∈ ⟨a⟩ has an inverse element x−1 ∈ ⟨a⟩ As x ∈ ⟨a⟩ we can write it as x = a mx and as mx < ord(a) we can find the element x−1 = a ord(a)−mx ∈ ⟨a⟩. It follows that it is actually the inverse of x as x ∗ x−1 = a mx ∗ a ord(a)−mx = a ord(a) = e • The fact that ⟨a⟩ ⊆ G follows from the fact that ⟨G; ∗,−1 , e⟩ is a group and therefore closed under its operation. • In order to show that ⟨⟨a⟩; ∗, −1 , e⟩ is a group we additionally have to show that ∗ is associative. Luckily this follows from the fact that ∗ is also the operation in ⟨G; ∗, −1 , e⟩ and therefore has to be associative as ⟨G; ∗, −1 , e⟩ is a group. As every condition hold ⟨a⟩ is a subgroup of ⟨G; ∗, −1 , e⟩ The second remark is about the at first sight rather confusing use of Z as set of potential n. As a matter of fact it does not make a difference here whether we choose N or Z, but to avoid further confusion it is helpful to remind oneself of the fact that for any negative n we have a n = (a −n)−1 = (a |n|)−1 44 which is just the inverse element of a |n| and makes sense when we (informally!) think about a n ∗ a −n = a 0 = e Given the earlier definition it’s now very simple to define a cyclic group: Definition 3.10 (Cyclic group). A cyclic group is a group G = ⟨g⟩ generated by an element g ∈ G. We call G cyclic and g a generator of G. From this follows that an element g ∈ G is exactly then a generator when ord(g) = |G|, i.e. ord(g) = |G| ⇔ g is a generator of G Most groups are not cyclic. Being cyclic is a special property that implies several interesting and important facts: • All cyclic groups are commutative • All cyclic groups of the same order (i.e. with the same number of elements) are isomorphic • For ⟨Zn, ⊕, ⊖, 0⟩ every element g with gcd(g, n) = 1 is a generator of ⟨Zn, ⊕, ⊖, 0⟩ • From earlier we already know that for an element a of a group G ⟨a⟩ is a subgroup. By defini- tion/construction it is a cyclic group and its order is equal to the order of a i.e. ord(a) = |⟨a⟩| Finally we conclude this part by reminding ourselves of the one theorem that connects most of the given definitions and that is Theorem 5: Lagrange For any finite group G and any subgroup H of G we have that the order of H divides the order G i.e. |H| | |G| This theorem has several implications, most notably we have that Claim The order of any group element has to divide the order of the group. Proof. For any element a of a group G we know that ⟨a⟩ is a subgroup of G. By Lagrange it follows that |⟨a⟩| | |G|. Now we already know that |⟨a⟩| = ord(a). We conclude that ord(a) | |G| this obviously implies for every element a ∈ G of a group G that a |G| = a k∗ord(a) = (a ord(a))k = ek = e It follows as a direct corollary that Corollary 3.2: Prime orders Every group of prime order is cyclic and therefore commutative. A common task when it comes to cyclic groups is to find a generator. While this can obviously be done by brute-force it is usually smarter to consider (based on Lagrange) only the divisors of the group’s order. We present the following example: 45 Example. We want to determine all generators of the group ⟨Z ∗ 11, ⊙,−1 , 1⟩. While this document only introduces this type of group in later sections one should already be familiar with the concept by either having read the lecture script or having skipped several steps ahead to look it up. We now have that the order of our group is |⟨Z ∗ 11, ⊙,−1 , 1⟩| = 10 = 2 · 5. As we know the order of every element has to divide the order of the group itself, for any element x ∈ Z ∗ 11 = {1, 2, 3, 4, 5, 6, 7, 8, 9, 10} we therefore have ord(x) ∈ 1, 2, 5, 10. As only the neutral element, i.e. here 1, has order 1, we will only check for 2 and 5. If a number x ∈ Z ∗ 11 does not satisfy x2 = 1 and x5 = 1 then we know that ord(x) = 10 which means that x is a generator. Checking all elements like this gives us 1 : 1 1 = 1 2 : 2 2 = 4, 25 = 10 3 : 3 2 = 9, 35 = 1 4 : 4 2 = 5, 45 = 1 5 : 5 2 = 3, 55 = 1 6 : 6 2 = 3, 65 = 10 7 : 7 2 = 5, 75 = 10 8 : 8 2 = 7, 85 = 10 9 : 9 2 = 4, 95 = 1 10 : 10 2 = 1 We conclude that the set of generators of ⟨Z ∗ 11, ⊙, −1 , 1⟩ is {2, 6, 7, 8} Hands-On 1 This Hands-On contains three major subparts. The first is the normal Hands-On part with exercises regarding the specific topics. The second contains several proofs for claims that have been made earlier and the third contains more challenging tasks. As we’ve taken an approach that tends to giving more tasks, you’re absolutely not expected to finish all tasks within the provided time. 1.1. Working with group axioms Let ⟨G; ∗,̂, e⟩ be a group. (a) Show that the function fa defined as fa : G → G x ↦→ a ∗ x with a ∈ G arbitrary is bijective. Use only the group axioms, apply an axiom only once per step and only in one place. (b) Show that the following statement holds for any x, y ∈ G ((x ∗ y) ∗ ̂x) ∗ ̂y = e ⇔ x ∗ y = y ∗ x Use only the group axioms, apply an axiom only once per step and only in one place. (c) Show that every group of order < 6 has to be commutative. (d) Show that every group that only contains self-inverse elements has to be commutative. (e) Given any two subgroups H1, H2 of a group G: Is H1 ⋃ H2 ̸= ∅ a subgroup of G? Are H1 ⋂ H2 or H1\\H2 subgroups? 46 (f) Show that for any two elements a, b ∈ G we have ord(a ∗ b) = ord(b ∗ a) 1.2. Subgroups (a) Give the smallest non-trivial subgroup of ⟨Z30; ⊕⟩ (b) Let G have 77 elements and H be a subgroup of G with H ̸= G. Show that H is commutative. (c) Given any abelian group ⟨G; ⊙⟩ with the neutral element e. Is the set Hk, k ∈ N containing all elements which fulfill h k = e a subgroup of ⟨G; ⊙⟩? Assume we would now define H ′ k ⊆ G, k ∈ N as the set of all elements that have order k. Is the statement now true or false for H ′ k? 1.3. Cyclic groups and generators (a) Find all generators of ⟨Z ∗ 17; ⊙⟩. (b) Find all generators of ⟨Z20; ⊕⟩ (c) If we have two cyclic groups G1 = ⟨g1⟩ and G2 = ⟨g2⟩, is (g1, g2) a generator of G1 × G2? (d) Given the groups G1 and G2 as Zm and Zn with addition. Give a necessary and sufficient condition which has to hold such that for any generators of G1 and G2 we have that (g1, g2) is a generator of G1 × G2. (Fancy extra task: Try to generalize this statement for any kind of cyclic groups) (e) Let m > 1 be a natural number and G a cyclic group with |G| = m ∗ k, k ∈ N\\{0}. Show that there always exists an element a ∈ G such that a ̸= e and am = e 1.4. Proofs from the script For these tasks let a, b ∈ ⟨G; ∗,̂, e⟩. (a) Show that ̂(̂a) = a (b) Show that ̂a ∗ b = ̂b ∗ ̂a (c) Show that a ∗ b = a ∗ c ⇒ b = c (d) Prove the One-Step Subgroup Test Let ⟨G; ◦⟩ be a group and H ⊆ G. Then H is a subgroup of ⟨G; ◦⟩ if and only if • H ̸= ∅ • a, b ∈ H ⇒ a ◦ b−1 ∈ H 1.5. Challenge This challenge will guide you through a proof of Lagrange’s theorem. Assume in this context the finite group ⟨G; ∗,̂, e⟩ with a subgroup H (a) Show that the follwing relation is an equivalence relation on ⟨G; ∗,̂, e⟩: x ∽ y ⇔ x−1y ∈ H (b) Let us define for any x ∈ G the set xH := {y ∈ G | ∃h ∈ H y = x ∗ h} Show that this set corresponds to the equivalence classes of the relation ∽ (c) Show that for all xH we have that |xH| = |H| (Hint: Use subtask 1.4 or 1.1) (d) Put it all together and enjoy your glory! 3.3 Morphisms Having been briefly mentioned in the earlier parts we will now look at the concept of homomorphisms and their ”extension”, the isomorphisms. While they are both more general concepts we will look at them specifically regarding groups. 47 First we have Definition 3.11 (Group homomorphism and isomorphism). Let ⟨G; ∗, −1 , eG⟩ and ⟨H; ·,̂, eH ⟩ be two groups. Then we call a mapping ϕ : G → H a homomorphism if it fulfils: For any a, b in G ϕ(a ∗ b) = ϕ(a) · ϕ(b) As an extension we call ϕ an isomorphism when it is a homomorphism as well as bijective. Homomorphisms and isomorphisms have several useful properties which are often asked to be proved: For homomorphisms we have (again given the groups G and H) Collection 3.2: Homomorphisms • ϕ(eG) = eH • ϕ(a −1) = ̂ϕ(a) • ϕ(a n) = ϕ(a) n • Based on the statements above we have that a homomorphism always projects G onto a subgroup of H. For isomorphisms we additionally have Collection 3.3: Isomorphisms • The inverse of an isomorphism is again an isomorphism • An isomorphism always maps generators in one group onto generators in the other group. I.e. two isomorphic groups always have the same number of generators (and the complete isomorphism can be described by just giving the image of just one generator of G under ϕ) • As direct corollary we have that ”All cyclic groups with same cardinality are isomorphic” • Isomorphisms carry commutativity i.e. if a group is isomorphic to a commutative group it has to be commutative itself We will give an exemplary proof for the first statement of each while putting the others in the Hands-On part. Claim ϕ(eG) = eH Proof. ϕ(eG) = ϕ(eG) · eH = ϕ(eG) · (ϕ(eG) · ̂ϕ(eG)) = (ϕ(eG) · ϕ(eG)) · ̂ϕ(eG) = ϕ(eG ∗ eG) · ̂ϕ(eG) = ϕ(eG) · ̂ϕ(eG) = eH 48 Claim The inverse of an isomorphism ϕ is again an isomorphism Proof. It is given by the definition of ϕ as an bijective function, that its inverse ϕ−1 is also bijective, we therefore only have to show that it is also a homomorphism. Let a, b ∈ H be arbitrary with x, y ∈ G such that ϕ(x) = a and ϕ(y) = b then we have ϕ−1(a · b) = ϕ−1(ϕ(x) · ϕ(y)) = ϕ−1(ϕ(x ∗ y)) = x ∗ y = ϕ−1(a) ∗ ϕ−1(b) We realize that the bijectivity of ϕ and ϕ−1 is crucial in several steps of this proof. Hands-On 2 2.1. Homomorphisms Let ⟨G; ∗, −1 , eG⟩ and ⟨H; ·,̂, eH ⟩ be two groups. (a) Given the homomorphism ϕ : G → H and g0 ∈ ϕ−1({eH }). Show that {g ∈ G | g ∗ g0 ∗ g−1 ∈ ϕ−1({eH })} = G 2.2. Proofs - Homomorphisms Let ⟨G; ∗, −1 , eG⟩ and ⟨H; ·,̂, eH ⟩ be two groups and ϕ : G → H a homomorphism. (a) Show that for any a ∈ G ϕ(a−1) = ̂ϕ(a) (b) Show that for any a ∈ G ϕ(a n) = ϕ(a) n, n ∈ Z (c) Show that based on the statements above we have that a homomorphism always projects G onto a subgroup of H. 2.3. Isomorphisms Let ⟨G; ∗, −1 , eG⟩ and ⟨H; ·,̂, eH ⟩ be two groups. (a) Show that for any m, n with gcd(m, n) = 1 the following map is an isomorphism between ⟨Zmn; ⊕mn⟩ and ⟨Zm; ⊕m⟩ × ⟨Zn; ⊕n⟩: ϕ : Zmn → Zm × Zn x ↦→ (Rm(x), Rn(x)) (b) Show that the condition gcd(m, n) = 1 is necessary for ϕ to be an isomorphism. (c) Let m be the product of two distinct prime numbers p and q. How many non-isomorphic subgroups does ⟨Zm; ⊕⟩ have? (d) Prove or disprove: If ϕ : G → H is an injective homomorphism, it has to be an isomorphism. 2.4. Proofs - Isomorphisms Let ⟨G; ∗,−1 , eG⟩ and ⟨H; ·,̂, eH ⟩ be two cyclic groups and ϕ : G → H a homomorphism. (a) Show that an isomorphism always maps generators in one group onto generators in the other group. I.e. two isomorphic groups alway have the same number of generators (and the complete isomorphism can be described by giving the image of just one generator of G under ϕ) (b) Show that we then have that ”All cyclic groups with same finite cardinality are isomorphic” 49 (c) Show that isomorphisms carry commutativity i.e. if a group is isomorphic to a commutative group it itself has to be commutative. 2.5. Challenge (a) Given two cyclic groups of order n > 0. How many different isomorphisms do exist between them? (This ”challenge” is actually sometimes an exam question!) 3.4 Z ∗ m and Euler’s totient function Finally we will look at another special kind of group and the Euler’s function which is closely related to it. The group is ⟨Z ∗ m, ⊙, −1 , 1⟩ and is defined as follows Definition 3.12 (Z ∗ m). The set Z ∗ m is defined as the set of all natural numbers smaller than m that are pairwise prime to m (i.e. have no common divisor). Formally we write it as Z ∗ m := {a ∈ N | a < m ∧ gcd(a, m) = 1} using the multiplication modulo m, denoted as ⊙, we can define the group ⟨Z ∗ m, ⊙, −1 , 1⟩ The fact that ⟨Z ∗ m, ⊙, −1 , 1⟩ is actually a group follows mostly from the chapter about number theory and shall not be proved here. Z ∗ m has the interesting property that it is cyclic if and only if m = 2, 4, p e, 2 · pe where e ≥ 1 and p is an uneven prime. Closely related to it we have the concept of Euler’s totient function which simply states the cardinality of Z ∗ m Definition 3.13 (Euler’s totient function). Euler’s totient function φ gives for an input m ∈ N the number of numbers in N that are smaller and pairwise prime to m. Formally this is denoted as φ : N → N m ↦→ |Z ∗ m| i.e. Z ∗ m has exactly φ(m) elements (the order of Z ∗ m is φ(m)). Obviously it’s quite common to actually compute ϕ(m) given m ∈ N. This is most easily done by using the following formula(s): Lemma 3.1 50 For a given m ∈ N with the prime factorization m = p e1 1 · p e2 2 · · · · · p en n we have φ(m) = φ(pe1 1 · p e2 2 · ... · pen n ) = φ(pe1 1 ) · φ(pe2 2 ) · ... · φ(pen n ) = n∏ i=1 p ei−1 i · (pi − 1) = n∏ i=1 p ei i · (1 − 1 pi ) = m n∏ i=1 (1 − 1 pi ) We actually most commonly use the formula on line three i.e. φ(m) = ∏n i=1 pei−1 i · (pi − 1). The reader may also notice how we used the multiplicative property of φ when dealing with pairwise prime numbers. The proof will be part of the hands-on. Example. We want to calculate φ(56) = φ(2 ∗ 28) = φ(22 ∗ 14) = φ(23 ∗ 7). We put the values of the factorization into the formula and obtain φ(56) = 2 3−1 · (2 − 1) · 7 1−1 · (7 − 1) = 4 · 6 = 24 Finally we have one more corollary that is a very direct consequence of what we already know but should nevertheless be explicitly formulated to stress its significance Corollary 3.3: Euler, Fermat For all m ≥ 2 and all a with gcd(a, m) = 1 we have a φ(m) ≡m 1 in particular, for every prime p and every a not divisible by p we have a p−1 ≡p 1 This corollary is particularly interesting because you will usually encounter its usage in the chapter about number theory, hereby showing a connection between these two fields. One example application is to find the inverse of some a modulo m if φ(m) is known (for example in RSA): aaφ(m)−1 ≡m 1 ⇒ a φ(m)−1 = ̂a Hands-On 3 3.1. Z ∗ m and Euler (a) Find the inverse of 3 in Z ∗ 13 (b) How many elements does Z ∗ 30 have? (c) Find an m ∈ N such that 3036 m ≡7105 1 given that 3036 = 22 ∗ 3 ∗ 11 ∗ 23 and 7105 = 5 ∗ 72 ∗ 29 (d) Show that every real (̸= Z ∗ 6) subgroup of Z ∗ 6 is abelian. 3.2. Challenge In this challenge we will prove the multplicativity of Euler’s totient function 51 (a) Show that for every two natural numbers m, n with gcd(m, n) = 1 we have that φ(m · n) = φ(m) · φ(n) 3.5 Rings Definition 3.14. A ring ⟨R; +, −, 0, ·, 1⟩ is an algebra for which (i) ⟨R; +, −, 0⟩ is a commutative group. (ii) ⟨R; ·, 1⟩ is a monoid. (iii) a(b + c) = (ab) + (ac) and (b + c)a = (ba) + (ca) for all a, b, c ∈ R. (Distributivity) A ring is called commutative if multiplication is commutative (ab = ba). This algebraic structure is important because it has two operations which are connected over the dis- tributive laws. Many well known sets like Z, Q, R and C are rings. Since we have two operations, a ring has both an additive and a multiplicative neutral element which we denote as 0 and 1 respectively. We can prove some very important facts about rings which we summarize as Lemma 5.17: Collection 3.4 (i) 0a = a0 = 0 (ii) (−a)b = −ab (iii) (−a)(−b) = ab (iv) If R is non-trivial (|R| > 1), then 1 ̸= 0. All 4 points can be proven with only a few lines. (i) has been proven in the lecture notes, (ii) and (iii) have been proven in exercise 9 (FS 2017) and (iv) will be proven in the next hands-on of this script. In a ring every element has an additive inverse but not necessarily a multiplicative one. Therefore it becomes interesting to talk about the divisibility property in rings. Definition 3.15. For a, b ∈ R with a ̸= b we say that a divides b, denoted a | b, if there exists c ∈ R such that ac = b. In this case, a is called a divisor of b and b is called a multiple of a. We can also state from this definition that every element divides 0, since a0 = 0 for every a, and that 1 divides every element, since 1a = a. Since ⟨R; ·, 1⟩ is a monoid and not necessarily a group, it is interesting to ask whether some elements of the ring have a multiplicative inverse. These elements are called units or invertible elements and the set containing all units is the multiplicative group of R, denoted as R∗. As we have seen in the previous chapter, we can find the order of this group using Euler’s totient function. Example. Z6 with addition and multiplication is a ring. Which elements of the ring have a multiplicative inverse? It’s exactly those elements where gcd(a, 6) = 1. These are Z ∗ 6 = {1, 5}. Another algebraic structure we can now define is the integral domain: Definition 3.16. An integral domain is a non-trivial commutative ring without zerodivisors: ∀a∀b(ab = 0 → a = 0 ∨ b = 0). In an integral domain no two elements can be multiplied to obtain 0. Previously we were not allowed to use the quotient notation c = a b since b might have been 0. In an integral domain we are allowed to use 52 it and can also prove that this quotient is unique (Lemma 5.20). Example. Is Z6 an integral domain? Z6 contains zerodivisors since for example 2 · 3 = 0 and is thus not an integral domain. In fact Zm can only be an integral domain if m is prime. If m is not prime, then it is by definition divisible by some a ̸= 0 and b ̸= 0, making a and b zerodivisors. Hands-On 4 4.1. Rings (a) How many units are in the ring Z12? (b) Let R be a commutative Ring with at least two elements. Let r ∈ R. Show that r ∈ R∗ ⇐⇒ ∀s ∈ R ∃t ∈ R s = rt (c) How many elements can a ring ⟨R; +, −, 0, ·, 1⟩ where 0 = 1 contain? Justify your answer. (d) Let ⟨R; +, −, 0, ·, 1⟩ be a ring such that a · a = a. Show that ∀a ∈ R. a + a = 0 (e) Using you knowledge from the last task show that R is furthermore commutative. 4.2. Integral Domains (a) Show that the order of the element 1 in the additive group is a prime number in any finite integral domain. 3.6 Fields Definition 3.17. A field is a non-trivial commutative ring F in which every non-zero element is a unit, i.e., F ∗ = F \\{0} Remember that in a ring R, ⟨R; ·, 1⟩ is a monoid. In a field F , ⟨F \\{0}; ·, −1 , 1⟩ is an abelian group. Before we look at different specific fields, we might wonder about the connection between integral domains and fields. In fact, every field is an integral domain (Theorem 5.24 - script). To prove this, we have to show that a field has no zerodivisors: Proof. Let u, v ∈ F \\{0} and uv = 0. Then v = 1v = u−1uv = u −10 = 0, which is a contradiction. Now the other question we might pose is: Is every integral domain a field? Z is an integral domain but not a field because no elements except for 1 and −1 have a multiplicative inverse. We can however prove that finite integral domains are fields. The intuition here is that because the quotient between two numbers is unique and there is a finite number of elements, the quotient between one and any other element should exist and be unique. Here goes the proof: Proof. Let I be an finite integral domain. By definition I has at least two elements and no zerodivisors. Let u ∈ I\\{0}. We want to show that u has an inverse in I. Let X = {ua|a ∈ I\\{0}} be the set of all multiples of u. Since I has no zerodivisors, 0 /∈ X. Furthermore if a and a ′ are two distinct elements in I\\{0}, then according to Lemma 5.20, ua = c and ua ′ = c ′ are distinct as well. This implies that X = I\\{0} and by the definition of X, the inverse of u must be contained in I. Hence I is a field. Which fields do we know? Q, R and C are some examples of infinite fields. What about finite fields? Is it possible to construct finite fields of any cardinality? In the following we will discuss and prove that only fields of certain cardinalities exist. First of all we know that Zp is a field if an only p is prime. This follows from the fact that Zp\\{0} is a multiplicative group if p is prime. Because fields with the same cardinality are all isomorphic we call a field with p elements GF(p), where GF stands for Galois field. 53 Hands-On 5 5.1. Fields (a) Show without using the Theorem 5.23 that Z4 is not a field. (b) Let F be a finite field. Find all elements x ∈ F that fulfil the equation x2 = 1. (c) Let F be a finite field. Using the previous exercise, show that ∏ a∈F ∗ a = −1. 3.7 Polynomials over Rings and Fields From calculus we are used to deal with polynomials where the coefficients and the values a polynomial can have, lie in R. Since the concept of polynomials is the same for all rings we can simply change the underly- ing algebra. This means that we still use the same idea of polynomials only that the values they can have are elements of some other ring and addition and multiplication may be defined differently. More formally: Definition 3.18. A polynomial a(x) over a ring R in the indeterminate x is a formal expression of the form a(x) = adxd + ad−1xd−1 + · · · + a1x + a0 = d∑ i=1 aixi for some non-negative integer d, where ai ∈ R. The degree of a polynomial a(x), denoted as deg(a(x)) is the largest power appearing in the polynomial, for example deg(x4 + 3x2 + 5) = 4. We write the set of all polynomials over a ring as R[x]. The same notation is used for polynomials over integral domains or fields, D[x], and F[x]. Note that there are infinitely many polynomials in R[x] as long as we don’t restrict the degree they can have. When performing addition or multiplication of polynomials one must respect the properties of the un- derlying ring: Example. Let’s consider some examples of addition and multiplication in Z6[x]: • 3x2 + 3x2 + x + 4x + 2 = (3 + 3)x2 + (1 + 4)x + 2 = 5x + 2 • x(x2 + 5x + 1) + 5x = x3 + 5x2 Now we may ask ourselves whether the set of polynomials over a ring is a ring as well. For this we simply have to check whether the properties listed in the definition for rings hold for polynomials as well. It turns out that: Collection 3.5 (i) For any ring R, R[x] is a ring as well. (ii) For any integral domain D, D[x] is an integral domain as well. Moreover since R[x] is a ring, we can also define a set of polynomials over R[x]. Such a structure would be denoted as R[x][y]. We will see an example with finite fields in the later section. But what about polynomials over fields? According to the definition of fields, F [x] is a field only if every polynomial in F [x] except for 0 has an inverse. This is not the case since there are no two polynomials of degree at least one that multiplied with each other result in the neutral element 1. Also multiplying two polynomials never reduces the degree of a polynomial. We can also define the root of a polynomial which will be of greater importance later: 54 Definition 3.19. Let a(x) ∈ R[x]. An element α ∈ R for which a(α) = 0 is called a root of a(x). Example. For example 2x2 + x ∈GF(3)[x] has the root 1 because 2(1) 2 + 1 = 0. 3.7.1 Factorization of Polynomials Analogously to divisibility in fields we can define divisibility of polynomials. If a polynomial can be written as the product of two lower-degree polynomials it is called reducible. Definition 3.20. A polynomial a(x) ∈ F [x] with degree at least 1 is called irreducible if it is divisible only by constant polynomials and by constant multiples of a(x). Example. Let’s observe the polynomials in field GF(3)[x]: • x2 is reducible since xx = x2. • 2x2 + x is reducible since x(2x + 1) = 2x2 + x. • x2 + 1 is irreducible. We will now see how we can justify this. Finding out whether a polynomial is irreducible can take time because one has to try out many possible divisors. It is therefore very important to have a good strategy which allows us to limit the number of divisor candidates we have to test. We will now look at some examples of polynomials of different degrees: • Polynomials of degree 1 are always irreducible. This follows from the definition. It is not possible to write such a polynomial as a product of two polynomials of degree 1. • Polynomials of degree 2 and 3 are irreducible if they have no root (Corollary 5.30). This is because if they are reducible they must have a factor of degree 1 and according to Lemma 5.29 they have such a factor if and only if they have a root • Polynomials of degree 4 are irreducible if they don’t have a root and no irreducible factors of degree 2. This means that we first check the polynomial for roots, then find the irreducible polynomials of degree 2 and test whether the polynomial of degree 4 can be factorized into two of those polynomials of degree 2. • For higher degree polynomials of degree d we have to first check the roots of the polynomial and then test for all irreducible polynomials that have a degree ≤ d/2. Another technique that is very important in this topic is polynomial division. We can in fact use same procedure as we use in calculus with polynomials in R, but we have to take into account the underlying algebra as for example in Z5, 2 + 4 = 1. This means that we have to take every result we obtain from addition or subtraction, modulo the order of the additive group. This is not hard but can easily lead to errors, especially when performing subtraction. It is advised to have a clear and consistent way of writing down each step of the division. Example. We want to divide (x3 + x2 + 2) by (2x + 1) over GF(3): (x3 + x2 + 2) : (2x + 1) = 2x2 + x + 1 −(x3 + 2x2 ) 2x2 + 2 −(2x2 + x ) 2x + 2 −(2x + 1) Rest : 1 In this example the polynomials are not divisible and we get a rest of 1. 55 3.7.2 Polynomial Interpolation A polynomial in R[x] can also be seen as a function R → R that can be evaluated at different points. For polynomials over fields the following property is very important: Lemma 3.2 A polynomial a(x) ∈ F [x] of degree at most d is uniquely determined by any d + 1 values of a(x). For this we use the following formula by Lagrange: a(x) = d+1∑ i=1 βiui(x) , where ui = (x − α1) . . . (x − αi−1)(x − αi+1) . . . (x − αd+1) (αi − α1) . . . (αi − αi−1)(αi − αi+1) . . . (αi − αd+1) This means that if we have d + 1 distinct (αi, βi) pairs where a(αi) = βi, we can recover the polynomial a(x) exactly. A use case for this is sharing a secret among multiple parties. To share a secret among d parties one could pick a polynomial of degree d − 1. Each party would get the knowledge about one position of the polynomial. The polynomial would then be forgotten and could only be recovered if every party is willing to give its share of the information. The secret itself might be the value of some other position of the polynomial. Hands-On 6 6.1. Polynomials: (a) Find all roots of the polynomial a(x) = x2 + 3x + 2 in GF(5). (b) Find all roots of the polynomial a(x) = 2x2 + 3x + 1 in GF(7). (c) Find a common irreducible factor of the following two polynomials from GF(5)[x]: a(x) = 2x3 + 2x2 + 2x + 2 b(x) = 3x4 + 4x 3 + x2 + x + 4 (d) Either show or disprove that x4 + x 2 + 1 ∈ GF(2)[x] is irreducible. (e) Split x5 + x4 + 1 ∈GF(2)[x] into irreducible factors. (f) Let F be a finite field. Show that there exists a non-constant polynomial a(x) ∈ F [x] that has no roots in F . (g) Find all irreducible monic polynomials of degree 2, 3 and 4 in Z2[x]. 3.8 Finite Fields So the question remains: Is it possible to construct finite fields of other sizes than prime numbers? Similarly to constructing the set Zm where the maximal number is limited by m, we define the set of polynomials over fields where the maximal degree is limited: Definition 3.21. Let m(x) be a polynomial of degree d over a field F . Then F [x]m(x) = {a(x) ∈ F [x]|deg(a(x)) < d} Moreover the cardinality of F [x]m(x) equals |F | d. Analogously to the modulo operation we perform after every calculation in Zm, we have to perform a modulo operation with polynomials. That is, if after some calculation the degree of the resulting poly- 56 nomial is larger than deg(m(x)), we need to divide that polynomial by m(x) and take the rest. Example. In GF(3)[x]x2 we have 2x(x + 1) = 2x because we divide the resulting polynomial 2x2 + 2x by x2 where 2x is the rest. Such a structure is a ring but not necessarily a field because not every polynomial might have multiplica- tive inverse. This is in fact very similar to what we have in Zm and we can now draw a nice comparison between Zm and F [x]m(x): Zm is a field if and only if m is prime because otherwise we have some a for which gcd(a, m) ̸= 1 and thus a zerodivisor. The same holds in F [x]m(x) only that for polynomials the concept of primality is analogous to irreducibility. So F [x]m(x) is a field if and only if m(x) is irreducible (Theorem 5.37). So to construct a field with pd elements we need to: • Pick a field of prime order GF(p). • Find an irreducible polynomial m(x) of degree d in GF(p)[x]. Example. We want to construct a field with 9 elements. As 9 = 32, we choose GF(3) and the polynomial m(x) = x2 + 1, m(x) is irreducible over GF(3) as it has degree 2 and no root. Hence GF(3)[x]x2+1 is a finite field with 9 elements. Hands-On 7 7.1. Extension Fields: (a) Calculate the product of 2x + 1 and x + 2 in GF(3)[x]x2+x+2. (b) Find all zerodivisors in the ring GF(3)[x](x2+2x). (c) Choose which of the following statements is true and prove it. Let F be field, then . . . • . . . F [x] must be a field as well. • . . . F [x] could be, but is not necessarily a field. • . . . F [x] cannot be a field. (d) Find all roots of the polynomial p(y) = xy2 + y + (x + 1) ∈ GF(2)[x]x2+x+1[y]. Keep in mind that p(y) is polynomial over GF(2)[x]x2+x+1, so the coefficients of p(y) are polynomials themselves. (e) Factorize the polynomial a(y) = xy3 + xy2 + (x + 1)y + x ∈ GF(2)[x]x2+x+1[y] into irreducible polynomials. (f) In case you want more exercises for this specific type of ”nested” polynomial fields, here are more exercises: (i) Check if y4 + (x + 1)y2 + y + x ∈ GF(2)[x]x2+x+1[y] is reducible. (ii) Find all roots of x2y2 + (x + 1)y + (x2 + 1) ∈ GF(2)[x]x3+x+1[y]. (iii) Factorize xy3 + 2y + 2x ∈ GF(3)[x]x2+x+2[y] 57 3.9 Solutions 3.9.1 Solutions for Hands-On 1 1.1 (a) We have to show the following: Claim The function fa defined as fa : G → G x ↦→ a ∗ x with a ∈ G arbitrary is bijective. Proof. To save space, we will sometimes apply axioms in multiple places at once, but you should make sure not to do that in the exam in such an exercise. A function bijective if it is injective and surjective. First we will prove that the mapping is injective using an indirect proof: Let x, x ′ ∈ G be arbitrary and let’s assume that f (x) = f (x′). Then: f (x) = f (x′) ⇔ a ∗ x = a ∗ x′ def. f ⇔ ̂a ∗ (a ∗ x) = ̂a ∗ (a ∗ x′) ̂a from the left on both sides ⇔ (̂a ∗ a) ∗ x = (̂a ∗ a) ∗ x′ associativity (G1) ⇔ e ∗ x = e ∗ x′ inverses (G3) ⇔ x = x′ neutral element (G2) Hence the function is injective. To show that fa is surjective we show that for any b ∈ G there is some c ∈ G such that f (c) = b. For any b we can define c as c = ̂a∗b, c ∈ G. We can see that f (c) = a∗(̂a∗b) = (a∗̂a)∗b = e∗b = b. Hence the function is also surjective. The function is injective and surjective, it follows that it is bijective. (b) We have to show the following: Claim The following statement holds for any x, y ∈ G ((x ∗ y) ∗ ̂x) ∗ ̂y = e ⇔ x ∗ y = y ∗ x 58 Proof. We can prove this directly by transforming one into the other ((x ∗ y) ∗ ̂x) ∗ ̂y = e ⇔ (((x ∗ y) ∗ ̂x) ∗ ̂y) ∗ y = e ∗ y y from the right on both sides ⇔ ((x ∗ y) ∗ ̂x) ∗ (̂y ∗ y) = e ∗ y associativity (G1) ⇔ ((x ∗ y) ∗ ̂x) ∗ e = e ∗ y inverses (G3) ⇔ ((x ∗ y) ∗ ̂x) ∗ e = y neutral element (G2) ⇔ (x ∗ y) ∗ ̂x = y neutral element (G2) ⇔ ((x ∗ y) ∗ ̂x) ∗ x = y ∗ x x from the right on both sides ⇔ (x ∗ y) ∗ (̂x ∗ x) = y ∗ x assciativity (G1) ⇔ (x ∗ y) ∗ e = y ∗ x inverses (G3) ⇔ x ∗ y = y ∗ x neutral element (G2) Which proves both directions of the claim. (c) We have to show the following: Claim Every group of order < 6 has to be commutative. Proof. There are two major ways of proving this. One is to realize that you need at least five elements to express non-commutativity. We get this by assuming that the group is not commutative i.e. there exist x, y ∈ G such that x ∗ y ̸= y ∗ x We can derive the following • x ̸= e and y ̸= e • This implies x ∗ y ̸= x and y ∗ x ̸= x as both imply y = e • This implies x ∗ y ̸= y and y ∗ x ̸= y as both imply x = e • x ∗ y ̸= e and y ∗ x ̸= e as in both cases they would be the inverse of each other and therefore commute. Following from that we have that e, x, y, x ∗ y, y ∗ x have to be pairwise different, so any group with order smaller than 5 is commutative. Every group of order 5 has obviously prime order, is therefore cyclic and also commutative. The second way is to look at each order individually • order = 1 - trivially commutative • order = 2, 3, 5 - of prime order, therefore cyclic therefore commutative • order = 4 We have 2 options: either there exists an element of order 4 or not. If yes then we again have a cyclic group. If no then all elements (except e) have order 2 i.e. all elements are self-inverse and the group has to be commutative. (d) We have to show the following: Claim Every group that only contains self-inverse elements has to be commutative. Proof. Let a, b be any elements within the group, then we have a ∗ b = ̂a ∗ b = ̂b ∗ ̂a = b ∗ a by only using the assumption that for all a in our group a = ̂a. (e) We have to show the following: 59 Claim Given any two subgroups H1, H2 of a group G: • H1 ⋃ H2 is not a subgroup of G • H1 ⋂ H2 is a subgroup of G • H1\\H2 is not a subgroup of G Proof. • Take ⟨Z6; ⊕⟩ with the subgroups {0, 2, 4} and {0, 3}. Their union is obviously no subgroup as it is not closed under ⊕ as 2 + 3 ̸∈ {0, 2, 3, 4} • We will show that all conditions for a subgroup hold • e ∈ H1 ⋂ H2as it is in H1 and H2 • If a, b ∈ H1 ⋂ H2 then also a ∗ b ∈ H1 ⋂ H2 because if a, b ∈ H1 ⋂ H2 then a, b ∈ H1 ⇒ a ∗ b ∈ H1 and a, b ∈ H2 ⇒ a ∗ b ∈ H2 • If a ∈ H1 ⋂ H2 then also ̂a ∈ H1 ⋂ H2 with the same argument as above • The associativity follows from the associativity of G As all conditions hold it is a subgroup. • This would allow H1\\H2 = ∅ which is no subgroup. Furthermore even if we would require that H1\\H2 ̸= ∅, we could just take any of the real/proper subgroups of ⟨Z6; ⊕⟩ as H2 and realize that Z6\\H2 is no subgroup of ⟨Z6; ⊕⟩. (f) We have to show the following: Claim For any two elements a, b ∈ G we have ord(a ∗ b) = ord(b ∗ a) Proof. First realize that this is equivalent to showing ord(a ∗ b) = n ⇒ ord(b ∗ a) ≤ n as we could you the argument in reverse on ord(b ∗ a) = n ⇒ ord(a ∗ b) ≤ n and ord(a ∗ b) ≤ ord(b ∗ a) ∧ ord(a ∗ b) ≥ ord(b ∗ a) ⇒ ord(a ∗ b) = ord(b ∗ a) So let us assume that ord(a ∗ b) = n > 0 and we need to show that ord(b ∗ a) ≤ n which we will by showing that (b ∗ a) n = e (a ∗ b) n = e ⇔ (a ∗ b) ∗ (a ∗ b) ∗ ... ∗ (a ∗ b) ︸ ︷︷ ︸ n times = e ⇒ b ∗ (a ∗ b) ∗ (a ∗ b) ∗ ... ∗ (a ∗ b) ︸ ︷︷ ︸ n times ∗a = b ∗ a ⇒ b ∗ a ∗ (b ∗ a) ∗ (b ∗ a) ∗ ... ∗ (b ∗ a) ︸ ︷︷ ︸ n times = b ∗ a ⇒̂a ∗ ̂b ∗ b ∗ a ∗ (b ∗ a) ∗ (b ∗ a) ∗ ... ∗ (b ∗ a) ︸ ︷︷ ︸ n times = ̂a ∗ ̂b ∗ b ∗ a ⇒ (b ∗ a) ∗ (b ∗ a) ∗ ... ∗ (b ∗ a) ︸ ︷︷ ︸ n times = e ⇔ (b ∗ a) n = e This concludes our proof. We would like to note that we’ve dropped some formalism to make the proof more concise. It is advisable to mention this and also what you’ve dropped (i.e. paranthe- sization) and why it’s still correct (because we’re associative in groups) in the exam. 60 1.2 (a) We have to give: Claim The smallest non-trivial subgroup of ⟨Z30; ⊕⟩ Proof. As trivial subgroup we have the subgroup of order 1. The second smallest therefore must have at least order 2 (there can only be one subgroup of order 1). We find ⟨15⟩ = {0, 15} fulfils this. As we’ve already shown that ⟨a⟩ is always a subgroup, we don’t have to explicitly show it again. (b) We have to show the following: Claim Every real subgroup of G is commutative. Proof. The order of the subgroup has to divide 77. We get as potential candidates for the group order: {1, 7, 11}. The case with order 1 is trivially commutative while the other orders are prime and therefore cyclic and therefore commutative. (c) We have to show the following: Claim Hk is a subgroup of G and H ′ k not. Proof. We will show this by showing that all conditions for a subgroup hold: • e ∈ Hk as ek = e for all k ∈ N • If a, b ∈ Hk then a ∗ b ∈ Hk as (a ∗ b) k = (a ∗ b) ∗ (a ∗ b) ∗ · · · ∗ (a ∗ b) ∗ (a ∗ b) ︸ ︷︷ ︸ k times = ak ∗ bk = e ∗ e = e Where we had to use that the group is abelian. • If a ∈ Hk then ̂a ∈ Hk as ̂ak = ̂a k ∗ e = ̂a k ∗ a k = (̂a ∗ a) k = ek = e Where again used that the group is abelian. • The fact that it is associative follows from the fact that G is a group. As all conditions hold Hk is a subgroup of G. The same does not hold for all elements of the order k. We can e.g. choose k such that no element is in the set or realize that for every k ̸= 1 we don’t have e ∈ H ′ k. 1.3 (a) We have to find the following: Claim All generators of ⟨Z ∗ 17; ⊙⟩ 61 Proof. We know that the order of ⟨Z ∗ 17; ⊙⟩ is 16 as 17 is prime. For the order of every element we therefore get that it has to be in the set {1, 2, 4, 8, 16}. We realize that when the order of an element a is ≤ 8 then a 8 = 1. Therefore we only have to check for every element if a 8 ̸= 1 to find out that it is a generator. We get the following results 1 : 1 8 = 1 2 : 2 8 = 1 3 : 3 8 = 16 4 : 4 8 = 1 5 : 5 8 = 16 6 : 6 8 = 16 7 : 7 8 = 16 8 : 8 8 = 1 9 : 9 8 = 1 10 : 10 8 = 16 11 : 11 8 = 16 12 : 12 8 = 16 13 : 13 8 = 1 14 : 14 8 = 16 15 : 15 8 = 1 16 : 16 8 = 1 We therefore have the generatorset {3, 5, 6, 7, 10, 11, 12, 14} with |{3, 5, 6, 7, 10, 11, 12, 14}| = 8 = ϕ(16). This cardinality check is great to be relatively sure that you didn’t make any mistakes. (b) We have to find the following: Claim All generators of ⟨Z20; ⊕⟩ Proof. We just have to find all elements a smaller than 20 that fulfill gcd(a, 20) = 1. We get {1, 3, 7, 9, 11, 13, 17, 19} with |{1, 3, 7, 9, 11, 13, 17, 19}| = 8 = ϕ(20) (c) We have to show the following: Claim For two cyclic groups G1 = ⟨g1⟩ and G2 = ⟨g2⟩ (g1, g2) is generally not a generator of G1 × G2 Proof. Take the cyclic groups ⟨Z2; ⊕⟩ and ⟨Z4; ⊕⟩ with g1 = 1 = g2. For (1, 1) we generate now the group {(1, 1), (0, 2), (1, 3)(0, 0)} which is not equal to Z2 × Z4. (d) We will show the following: Claim m and n have to be relatively prime for this statement to hold. The same goes for the orders of general cyclic groups. We will therefore look at the general cyclic groups G1 and G2 with 62 the respective orders m and n. Proof. First we show that this condition is necessary, then we will show that it is sufficient. Assume that gcd(m, n) = d with d > 1 Which let’s us define lcm(m, n) = l = m·n d < m · n. We already know that gr 1 = gr+k·m 1 and gr 2 = gr+v·n 2 for m, n ∈ N. Further we know that we can write l as l = q · m or l = p · n with p, q ∈ N. We conclude that (g1, g2)1+l = (g1+q·m 1 , g1+p·n 2 ) = (g1 1, g1 2) = (g1, g2) Therefore we can at most (exactly) generate l different elements with (g1, g2) and as l < m · n = |G1 × G2| we know that it is no generator. To show that it is sufficient we only have to realize that we will always generate exactly l elements as l is by definition the lowest number for which we can find a factorization l as l = q · m or l = p · n with p, q ∈ N. Furthermore if gcd(m, n) = 1 so is l = m·n and by generating m·n different elements we will generate G1 × G2. (One might develop this proof further by showing that we can’t get into a ”loop” earlier but this should in theory be clear from the chapter about number theory and the fact that G1 × G2 is a group) (e) We will show the following: Claim For any cyclic group G of order m · k we have an element a ∈ G such that am = e Proof. This proof makes use of the isomorphic relationship between G and ⟨Zm·k; ⊕⟩. It is obvious (or after the chapter about isomorphisms it should be obvious) that if there exists such an element in ⟨Zm·k; ⊕⟩, it also has to exist in G. The element in ⟨Zm·k; ⊕⟩ is just aZ = k where k is the number in Zm·k. Obviously we have km = m · k = 0. As we already know G and ⟨Zm·k; ⊕⟩ are isomorph. Let ϕ such an isomorphism defined as ϕ : Zm·k → G x ↦→ ϕ(x) then we have a m = ϕ(aZ)m = ϕ(am Z ) = ϕ(0) = e using our knowledge about isomorphisms. 1.4 (a) We have to show the following: Claim ̂(̂a) = a Proof. We have ̂(̂a) G2 = ̂(̂a) ∗ e G3 = ̂(̂a) ∗ (̂a ∗ a) G1 = ( ̂(̂a) ∗ ̂a) ∗ a G3 = e ∗ a G2 = a otherwise we could argue the same using the uniqueness of the inverse element. (b) We have to show the following: 63 Claim ̂a ∗ b = ̂b ∗ ̂a Proof. We have ̂a ∗ b G2 = ̂a ∗ b∗e G3 = ̂a ∗ b∗(a∗̂a) G2 = ̂a ∗ b∗(a∗e∗̂a) G3 = ̂a ∗ b∗(a∗(b∗̂b)∗̂a) G1 = ( ̂a ∗ b∗(a∗b))∗̂b∗̂a G3 = e∗̂b∗̂a G2 = ̂b∗̂a again we could otherwise use that (a ∗ b) ∗ ̂b ∗ ̂a = e and rely on the uniqueness of the inverse element. (c) We have to show the following: Claim a ∗ b = a ∗ c ⇒ b = c Proof. We have b G2 = e ∗ b G3 = (̂a ∗ a) ∗ b G1 = ̂a ∗ (a ∗ b) As. = ̂a ∗ (a ∗ c) G1 = (̂a ∗ a) ∗ c G3 = e ∗ c G2 = c (d) We have to prove the following: Claim One-Step Subgroup Test Proof. We show that H ⊆ G is a subgroup of the group ⟨G; ◦⟩ if and only if it fulfils the ”One-Step Subgroup Test”. First and foremost we realize that all parts of the ”One-Step Subgroup Test” are direct corollaries of the characteristics of a subgroup. So it is trivial that H is subgroup of G ⇒ H fulfills the One-Step Subgroup Test We will therefore only focus on the part H fulfills the One-Step Subgroup Test ⇒ H is subgroup of G We will show each required condition individually • H ̸= ∅, therefore let a be an element in H. By the ”second rule” we now have a ◦ a −1 = e ∈ H • Knowing that we have e ∈ H we have for every a in H again by the ”second rule” that e ◦ a −1 = a −1 ∈ H • As we know that all inverse elements are also in H we use the ”second rule” one more time to show that for any a, b ∈ H we have a ◦ b = a ◦ (b −1)−1 ∈ H • That H ⊂ G is already given and the fact that ◦ is associative follows from the fact that it is associative in ⟨G; ◦⟩. We have shown the second part and therefore H fulfills the One-Step Subgroup Test ⇔ H is subgroup of G 1.5 (Challenge) 64 (a) We will show that it is an equivalence relation by showing that it fulfills each requirement: • reflexivity: ∀x ∈ G x−1x = e ∈ H • symmetry: ∀x, y ∈ G x−1y ∈ H ⇒ (x−1y)−1 ∈ H ⇒ y−1(x−1)−1 ∈ H ⇒ y−1x ∈ H • transitivity: ∀x, y, z ∈ G x−1y ∈ H ∧ y−1z ∈ H ⇒ x−1yy−1z ∈ H ⇒ x−1z ∈ H (b) We have for every element y in xH that ∃h ∈ H x = yh ⇔ ∃h ∈ H y = xh ⇔ ∃h ∈ H x −1y = h ⇔ x−1y ∈ H ⇔ x ∽ y Where the bold part comes from the fact that H is a subgroup and therefore contains the inverse element to the h of the first equation. (We could also argue with the symmetry of ∽) We can follow from this equation that in fact all elements xH form an equivalence class of ∽. (c) We take a look at the map ϕ : H → xH h ↦→ xh which we happen to know is injective. It is also surjective as we defined xH to be the image of ϕ. As the map is therefore bijective we can conclude that |xH| = |H| (d) We have an equivalence relation on G, which by definition forms a partition of G. Furthermore we know that equivalence classes have the same size and that H is one of these equivalence classes (for x = e). Therefore the cardinality of H has to divide the cardinality of G. 65 3.9.2 Solutions for Hands-On 2 2.1 (a) We have to show the following: Claim {g ∈ G | g ∗ g0 ∗ g−1 ∈ ϕ−1({eH }} = G Proof. First we realize that this is equivalent to stating that for all g ∈ G we have ϕ(g ∗ g0 ∗ g−1) = eH So let g ∈ G be arbitrary. We have ϕ(g ∗ g0 ∗ g−1) = ϕ(g) · ϕ(g0) · ϕ(g−1) = ϕ(g) · eH · ϕ(g−1) = ϕ(g) · ̂ϕ(g) = eH where we used that ϕ(g−1) = ̂ϕ(g) and ϕ(g0) = eH 2.2 (a) We have to show the following: Claim For any a ∈ G ϕ(a −1) = ̂ϕ(a) Proof. We have ϕ(a −1) = ϕ(a −1) · eH = ϕ(a −1) · (ϕ(a) · ̂ϕ(a)) = (ϕ(a −1) · ϕ(a)) · ̂ϕ(a) = ϕ(a −1 ∗ a) · ̂ϕ(a) = ϕ(eG) · ̂ϕ(a) = eH · ̂ϕ(a) = ̂ϕ(a) (b) We have to show the following: Claim For any a ∈ G ϕ(a n) = ϕ(a) n, n ∈ Z Proof. We will prove this by induction. Before that we remind ourselves of the fact that for n < 0 we have that a n = (a −1)|n| and we can prove it analogous. Thus it is enough to focus on the case where n ∈ N. Let a ∈ G be arbitrary 1.Induction basis For n = 0, 1 we have ϕ(a 0) = ϕ(eG) = eH = ϕ(a)0 and ϕ(a 1) = ϕ(a) = ϕ(a) 1 2.Assumption For arbitrary n we have ϕ(a n) = ϕ(a) n 3.Induction step We have ϕ(a n+1) = ϕ(a n ∗ a) = ϕ(a n) · ϕ(a) IA = ϕ(a) n · ϕ(a) = ϕ(a) n+1 The induction shows that the statement holds for all n ∈ N, which together with our earlier thoughts shows that it holds for all n ∈ Z. (c) We have to show the following: 66 Claim A homorphisms projects a group always onto a subgroup of the codomain. I.e. Im(ϕ) is a subgroup of H Proof. We will show that it fulfills the usual conditions • We have eH ∈ Im(ϕ) as ϕ(eG) = eH • For any a, b ∈ Im(ϕ) we have a · b ∈ Im(ϕ) as a · b = ϕ(x) · ϕ(y) = ϕ(x ∗ y) ∈ Im(ϕ) where x, y ∈ G such that ϕ(x) = a and ϕ(y) = b • For all a ∈ Im(ϕ) we have ̂a ∈ Im(ϕ). Let a ′ ∈ G with ϕ(a′) = a, then ϕ(a ′−1) = ̂ϕ(a′) = ̂a ∈ Im(ϕ) The rest follows as usual. 2.3 (a) We have to show the following: Claim For any m, n with gcd(m, n) = 1 the following map is an isomorphism: ϕ : Zmn → Zm × Zn x ↦→ (Rm(x), Rn(x)) Proof. First we will show that it is a homomorphism as we have for any x, y ∈ Zmn ϕ(x ⊕mn y) = (Rm(x ⊕mn y), Rn(x ⊕mn y)) = (Rm(x + y), Rn(x + y)) = (Rm(Rm(x) + Rm(y)), Rn(Rn(x) + Rn(y))) = (Rm(x), Rn(x)) ⊕m×n (Rm(y), Rn(y)) = ϕ(x) ⊕m×n ϕ(n) Now we will show its bijectivity: By the CRT we know that there is a one-to-one relationship between Zmn and Zm × Zn. We will show that ϕ is injective and therefore constitutes such a relationship. Assume there exist x, y ∈ Zmn with x ̸= y but with ϕ(x) = ϕ(y). This follows from the fact that both x and y are a solution to the following system of equations a ≡m ϕ(x) a ≡n ϕ(x) Given that x, y < m · n and ϕ(x) < m and ϕ(x) < n we have by the CRT that this solution has to be unique i.e. x = y which contradicts the assumption. It follows that ϕ has to be injective. Knowing that |Zmn| = |Zm × Zn| we conclude that it also has to be surjective. We have a bijective homomorphism, an isomorphism. (b) We have to show the following: 67 Claim The condition gcd(m, n) = 1 is necessary for ϕ to be an isomorphism. Proof. Assume that gcd(m, n) = d > 1, then lcm(m, n) = l = m·n d < m · n. With l ∈ Zmn we observe that ϕ(0) = (0, 0) = ϕ(l) leading to the fact the the mapping is neither injective nor surjective. (c) We have to show the following: Claim There exist exactly 4 non-isomorphic subgroups of ⟨Zm; ⊕⟩. Proof. By Lagrange we know that the order of every subgroup has to divide the order of the group. We get next to ⟨Zm; ⊕⟩ and {1} just subgroups with either order q or p. As both q and p are prime we know that groups of this order have to be cyclic and therefore isomorphic to ⟨Zq; ⊕⟩ and ⟨Zp; ⊕⟩ respectively. Therefore all subgroups of ⟨Zm; ⊕⟩ with order p or q are (respectively) isomorphic. In order to show that there exist subgroups of order p and q we can simply write them down: ⟨p⟩ has order q and ⟨q⟩ has order p. (d) We have to show the following: Claim If ϕ : G → H is an injective homomorphism, it has not to be an isomorphism. Proof. Take any group H with a ”real” subgroup G. Now take the identity-map as ϕ defined as ϕ : G → H x ↦→ x Obviously this map is injective but it is not surjective onto H i.e. no isomorphism. The same goes for any injective map onto a real subgroup of a group. 2.4 (a) We have to show the following: Claim An isomorphism between cyclic groups always maps a generator to a generator Proof. Let G and H be two isomorph finite cyclic groups with the isomorphism ϕ : G ↦→ H and let g be a generator of ⟨g⟩ = G. We want to show that H = ⟨ϕ(g)⟩ i.e. ϕ(g) generates H. We have: 1. H is closed under its operation, explicitly we have ⟨ϕ(g)⟩ ⊆ H 2. ϕ is bijective and therefore for all h ∈ H there exists exactly one b ∈ G ϕ(b) = h 3. ⟨g⟩ = G which implies that there exists a m ∈ N gm = b 68 4. Combining 2. and 3. we get that for all h ∈ H there exists an m ∈ N such that ϕ(gm) = [ϕ(g) m] = h ⇔ H ⊆ ⟨ϕ(g)⟩ 5. Based on 1. and 4. and our knowledge about sets we get H = ⟨ϕ(g)⟩ which proves our initial statement. (b) We have to show the following: Claim Based on the above we can follow that all finite cyclic groups of the same order are isomorphic. Proof. Let ⟨G; ∗,−1 , 1⟩ with the generator g and ⟨H; ·,̂, e⟩ with the generator h. We just give the isomorphism ϕ : G → H gr ↦→ h r Now we show that this map has all the wished properties. 1. Homomorphism: ϕ(a ∗ b) = ϕ(gr ∗ gj) = ϕ(gr+j) = h r+j = h r · h j = ϕ(gr) · ϕ(gj) = ϕ(a) · ϕ(b) 2. Surjectivity: Let h ′ ∈ H be arbitrary, via assumption there exists a (smallest) r such that h r = h ′. We then have (based on equal cardinality), that ϕ(gr) = ϕ(g)r = h r = h ′ which proves the surjektivity. 3. Injectivity: Suppose ϕ is not injective, i.e. there exist r, r′ < |H| with h r ∈ H and h r′ ∈ H with r ̸= r′ ∧ h r = h r′, which is a direct contradiction to the assumption that h is a generator of H. (c) We have to show the following: Claim Isomorphisms carry commutativity. Proof. We want to show that if one group is commutative then also all isomorphic groups. Let ⟨G; ∗,−1 , 1⟩ be an commutative group isomorph to ⟨H; ·,̂, e⟩ with an isomorphism ϕ. We will show the commutativity of H directly: Let h1, h2 ∈ H with their respective preimage g1, g2 ∈ G be arbitrary. We have h1 · h2 = ϕ(g1) · ϕ(g2) = ϕ(g1 ∗ g2) = ϕ(g2 ∗ g1) = ϕ(g2) · ϕ(g1) = h2 · h1 Which proves the statement. 2.5 (Challenge) (a) We (this is normally a hint that you get with the task) know that a cyclic group of order n has φ(n) generators. We’ve already seen in the previous task that every isomorphism maps a generator in one group to a generator in the other group. Furthermore it is actually enough to give just the map of one of the generators to fully identify the isomorphism, as we can easily infer all other mappings out of this (with ϕ(gn) = ϕ(g)n). 69 One might now think that there are φ(n) 2 possible isomorphisms between these two groups, based on all possible combinations. This thought is wrong as it doesn’t assume that when we map one generator onto another and by that create an isomorphism we at the same time also determine the mapping of all other generators onto generators of the other group. As a matter of fact we can by applying this method create exactly φ(n) different isomorphisms, i.e. by taking one generator and mapping it to each of the generators of the other group individually. If we want to repeat this procedure with the second generator of the first group we would obviously create the same φ(n) isomorphisms. As every isomorphism has to map every generator onto a generator we conclude that there exist exactly φ(n) different isomorphisms between two cyclic groups of order n. 70 3.9.3 Solutions for Hands-On 3 3.1 (a) We have to show the following: Claim The inverse of 3 in Z ∗ 13 is 9. Proof. Obviously we have 9 ∈ Z ∗ 13. Furthermore we have 9 · 3 = 27 ≡13 1 (b) We have to show the following: Claim |Z ∗ 30| = 8 Proof. We have 30 = 2 · 3 · 5 therefore φ(30) = 1 · 2 · 4, the elements are 1, 7, 11, 13, 17, 19, 23, 29. (c) We have to show the following: Claim The number 4 · 7 · 6 · 28 = 4704 solves this equation. Proof. First we realize that gcd(3036, 7105) = 1 i.e. they are relativ prime. Based on that we can use our corollary of Fermat/Euler. We calculate φ(7105) = 4 · 7 · 6 · 28 = 4704 and get that 3036 4704 ≡7105 1 (d) We have to show the following: Claim Every real subgroup of Z ∗ 6 is commutative. Proof. Every real subgroup of Z ∗ 6 must have an order < 6 and is therefore commutative by the task solved on the first Hands-On. 3.2 (Challenge) (a) Let n, m ∈ N be relatively prime, i.e. gcd(n, m) = 1. From this follows directly, that the sets of primes of each respective prime factorization are disjunctive. (Otherwise, gcd(n, m) ̸= 1) Let p a1 1 p a2 2 · ... · pai i = n and qb1 1 qb2 2 · ... · qbj j = m denote these factorizations with {p1, p2, ..., pi} ∩ {q1, q2, ..., qj} = ∅. Lemma 5.12 says φ(n) = i∏ k=1 (pk − 1)p ak−1 k φ(m) = j∏ k=1 (pk − 1)p bk−1 k Therefore, since mn = pa1 1 p a2 2 · ... · pai i · qb1 1 qb2 2 · ... · qbj j and ∀s, t: ps ̸= pt φ(nm) Lem.5.12 = ( i∏ k=1 (pk − 1)p ak−1 k ) · ( j∏ k=1 (pk − 1)pbk−1 k ) = φ(n) · φ(m) 71 3.9.4 Solutions for Hands-On 4 4.1 (a) This ring has 4 units. Units in Z12 are elements that have a multiplicative inverse. According to Definition 5.16 these are all elements a where gcd(a, 12) = 1. These are exactly 1, 5, 7 and 11. (b) We know that R is a ring with at least 2 elements and R∗ is its multiplicative group. We have to show the following equivalence: Claim r ∈ R∗ ⇐⇒ ∀s ∈ R ∃t ∈ R s = rt Proof. We show both sides of the equivalence separately: (=⇒) : Let r ∈ R∗ and s ∈ R be arbitrary. We have to somehow construct a t such that s = rt. If we don’t know how to approach this, we should think about what we are given: r is element of the multiplicative group which means, that r has an inverse element, r−1, such that rr−1 = 1. We can thus say that s = 1s = rr−1s. This shows that for every s ∈ R there exists t = r−1s such that s = rt. (⇐=) : We again do a direct proof of the implication. We know that r ∈ R and we have to prove that r has a multiplicative inverse, meaning that r−1 exists in R. We are given that for any s we can find t such that s = rt. Since 1 ∈ R, this implies that there is a t such that 1 = rt. This is exactly the definition of a multiplicative inverse. r−1 = t ∈ R and thus r ∈ R∗. (c) We show that: Claim A ring where 0 = 1 contains exactly one element. Proof. Let R be a ring where 0 = 1. Since the neutral element must be in the ring, R contains at least one element. Let’s assume that |R| > 1 is the case and some a ̸= 0 ∈ R. It follows that a = 1a = 0a = 0 using the definition of the neutral element and Lemma 5.17. Therefore ∀a ∈ R a = 0 and thus |R| = 1. (One could also use Lemma 5.17(iv) to prove this.) (d) We show that: Claim Let ⟨R; +, −, 0, ·, 1⟩ be a ring such that a · a = a. We claim that ∀a ∈ R. a + a = 0 Proof. Let a ∈ R be arbitrary. Then we have a + a As. = (a + a) · (a + a) Dis. = (a · a) + (a · a) + (a · a) + (a · a) As. = a + a + a + a 72 using our assumption twice and grouping multiple distributivity (and associativity) uses. But then a + a = a + a + a + a ⇒a + a + (−a) + (−a) = a + a + a + a + (−a) + (−a) ⇒0 = a + a Using the existence of the additive inverse. This concludes the proof. (It should be mentioned that most steps can be split into more individual sub-steps. We omitted them for conciseness) (e) We show that Claim Let ⟨R; +, −, 0, ·, 1⟩ be a ring such that a · a = a. We claim that R is commutative using the result above. Proof. First note that the result above can be rewritten as a + a = 0 ⇔ a = −a We need to show that ∀a, b ∈ R. a · b = b · a. Let us take a, b ∈ R arbitrary. Then (a + b) as. = (a + b) ∗ (a + b) dist. = aa + ab + ba + bb as. = a + ab + ba + b ⇒ 0 = ab + ba. (Due to right and left cancellation laws.) def.− ⇒ ba = −(ab) as. ⇒ ab = −(ab) = ba Therefore, R is commutative. 4.2 (a) Let D be an arbitrary integral domain where 1 has an additive order of c. We have to show that if D is finite, c is prime: Proof. Suppose c ∈ N is not a prime. Then there are m, n ∈ N such that m · n = c (both not 1). We want to show a contradiction to the definition of integral domains. The special thing about integral domains is that they don’t have zerodivisors. So to prove our claim, we have to show that if c is not prime, we have zerodivisors: Since c is the order of the additive group, we have: 0 = c∑ i=1 1 = mn∑ i=1 1 = (1) ( m∑ i=1 1) · ( n∑ i=1 1) Since m < c and n < c, both factors are not 0 and thus they are zerodivisors and D cannot be an integral domain. The last equivalence (1) is not trivial and should be proven separately. For this we can use induction over n: Basis step: For n = 0 it’s easy to see that both sides of the equation are 0. An empty sum evaluates to 0 and 0a = a for all a. Let n be arbitrary. Under the assumption that the equation holds for n, we prove that it also holds for n + 1: Induction step: (n+1)m∑ i=1 1 = nm∑ i=1 1 + m∑ i=1 1 = I.H. ( n∑ i=1 1) · ( m∑ i=1 1) + m∑ i=1 1 = ( n∑ i=1 1) · ( m∑ i=1 1) + 1 · m∑ i=1 1 = ( n∑ i=1 1 + 1) · ( m∑ i=1 1) = ( n+1∑ i=1 1) · ( m∑ i=1 1) In the last steps we applied the distributivity law. 73 3.9.5 Solutions for Hands-On 5 5.1 (a) We show that: Claim Z4 is not a field. Proof. (We give 2 possible proofs) (1) Let Z4 be a field. According to the definition of fields, Z ∗ 4 = Z4\\{0} is a multiplicative group. According to Definition 5.16 for all elements a in Z ∗ 4 gcd(a, 4) = 1. However gcd(2, 4) ̸= 1. This is a contradiction. (2) A field is an integral domain (Theorem 5.24). An integral domain has no zerodivisors. However 2 is a zerodivisor in Z4 since 2 · 2 = 0. Thus Z4 is not an integral domain and also not a field. (b) Let F be an arbitrary finite field. Note that in the following we are not dealing with numbers, but for the sake of simplicity we denote the elements we’re dealing with the same way. We can derive the following: x2 = 1 ⇐⇒ x2 − 1 = 0 ⇐⇒ (x + 1)(x − 1) = 0 This implies that x is either −1 or 1. This solution holds for all fields since every field contains the elements −1 and 1. For example in GF(3), −1 = 2 and 2 · 2 = 1 Such elements are inverses of themselves. (c) Let F be an arbitrary finite field. Claim ∏ a∈F ∗ a = −1 . Proof. We are allowed to use the results of the previous exercise, which is that in any field only 1 and −1 are self-inverse elements. Every other element a ∈ F ∗ has a unique multiplicative inverse a −1 ∈ F ∗ where a ̸= a −1. Let a1, ...ak be those elements, then (after reordering them, so they appear in the appropriate pairs): ∏ a∈F ∗ a = a1 · a −1 1 · .... · a k 2 · a −1 k 2 · 1 · (−1) = k 2∏ i=1 ·1 · (−1) = −1. For finite fields with characteristic 2 (i.e. 1 = −1) we would have to slightly adjust the argument (since x2 = 1 now only has one solution) and we’d get ∏ a∈F ∗ a = 1 but because 1 = −1 the claim is true here as well. 74 3.9.6 Solutions for Hands-On 6 6.1 (a) We simply evaluate the polynomial for all elements in GF(5): • a(0) = 0 + 0 + 2 = 2 • a(1) = 1 + 3 + 2 = 1 • a(2) = 4 + 1 + 2 = 2 • a(3) = 4 + 4 + 2 = 0 • a(4) = 1 + 2 + 2 = 0 The roots are 3 and 4. Also (x − 3)(x − 4) = x2 + 3x + 2. (b) We evaluate the polynomial for all elements in GF(7): • a(0) = 0 + 0 + 1 = 1 • a(1) = 2 + 3 + 1 = 6 • a(2) = 1 + 6 + 1 = 1 • a(3) = 4 + 2 + 1 = 0 • a(4) = 4 + 5 + 1 = 3 • a(5) = 1 + 1 + 1 = 3 • a(6) = 2 + 4 + 1 = 0 The roots are 3 and 6. Also 2 · (x − 3)(x − 6) = 2x2 + 3x + 1. (c) We need to find a common irreducible factor of both polynomials. Such a polynomial could be either of degree 1 or degree 2. Since it’s easier, let’s first check for factors of degree 1. Remember that a polynomial has a factor of degree 1 if and only if it has a root. So we can check for common roots: After first testing 0 and 1 we find out that 2 is a root of both a(x) and b(x) since a(2) = 1+3+4+2 = 0 and b(2) = 3 + 2 + 4 + 2 + 4 = 0. According to Lemma 5.29, (x − 2) is a common factor of both polynomials. (d) Let’s use our strategy to find out whether x4 + x2 + 1 is irreducible: • First we check for roots of the polynomial: Neither 0, nor 1 are roots. If the polynomial is reducible, the factors have to be of degree 2. • We look for irreducible polynomials of degree 2: For this we check x2, (x2 + 1), (x2 + x) and (x2 + x + 1) for roots. It turns out that (x2 + x + 1) is the only irreducible polynomial of degree 2. If our polynomial is reducible, then (x2 + x + 1) must be the factor. • We check that in fact (x2 + x + 1)(x2 + x + 1) = (x4 + x2 + 1). Therefore (x4 + x2 + 1) is not irreducible. (e) This exercise is tricky because it again involves higher degree polynomials. We can use the same strategy we use to test for irreducible polynomials: • First we check for roots: Neither 0, nor 1 are roots. Therefore one of the factors must be an irreducible polynomial of degree 2. • We find all irreducible polynomials of degree 2. We do this by checking x2, (x2 +1), (x2 +x) and (x2 + x + 1) for roots. The only polynomial with no roots, and thus irreducible, is (x2 + x + 1). • We divide (x5 + x4 + 1) by (x2 + x + 1) using polynomial division. The resulting polynomial is (x3 + x + 1). Since we know that there are no factors of degree 1, we can conclude that (x3 + x + 1) is irreducible as well and therefore we are done. 75 (f) This is a more difficult one. We need to show that over any finite field F there exists at least one polynomial with no roots. To prove the statement we need to construct such a polynomial given an arbitrary field F . According to Lemma 5.29 we know that a polynomial has a root if and only if it is divisible by some polynomial of degree 1. This means that irreducible polynomials of degree ≥ 2 have no roots. In other words we can prove the statement by constructing an irreducible polynomial of degree ≥ 2. (g) To find all irreducible monic polynomials of degree 2, 3 and 4 in Z2[x], we can employ some tricks: • Notice that for any polynomial in Z2[x], we have a(1) = 0 if and only if there is an even number of non-zero coefficients. That means, any irreducible polynomial must have an odd number of terms. • For any polynomial in Z2[x], we have a(0) = 0 if the constant term is 0. Thus, any irreducible polynomial must have a constant term of 1. Now we can see that out of all the monic polynomials of degree 2 (x2, x 2 + 1, x 2 + x, x 2 + x + 1), only x2 + x + 1 satisfies all these conditions and indeed, we can check that it has no roots and is therefore irreducible. Similarly we can eliminate some degree 3 polynomials: out of x3, x 3 + 1, x 3 + x, x 3 + x + 1, x 3 + x2, x 3 + x2 + 1, x 3 + x2 + x, x 3 + x2 + x + 1, we are left with only x3 + x + 1, x 3 + x2 + 1 which both have no roots and are thus irreducible. For degree 4 polynomials we repeat the process to get the candidates x4 + x3 + x2 + x + 1, x 4 + x3 + 1, x 4 + x2 + 1, x 4 + x + 1 which all have no roots. But we also need to check if any irreducible degree 2 polynomial divides one of these candidates, so we carry out long division with x2 + x + 1. After this, we’re left with the irreducible polynomials x4 + x3 + x2 + x + 1, x 4 + x3 + 1, x 4 + x + 1. Proof. Let F be an arbitrary finite field with m elements. Let a1(x), a2(x), ...am(x) be all irreducible polynomials of degree 1 in F [x]. Then p(x) = a1(x) · a2(x) · ... · am(x) + 1 is a polynomial which is not divisible by any polynomial of degree 1. p(x) is either irreducible or it is divisible by some irreducible polynomial of degree ≥ 2. In both cases this proves the existence of an irreducible polynomial of degreee ≥ 2 and such a polynomial does not have a root. 76 3.9.7 Solutions for Hands-On 7 7.1 (a) In GF(3)[x]x2+x+2 we need to perform a modulo operation every time the degree of a polynomial is larger than 1. We then have: (2x + 1)(x + 2) = 2x2 + x + x + 2 = 1 (b) Let’s first list all elements of GF(3)[x](x2+2x): They are {0, 1, 2, x, x + 1, x + 2, 2x, 2x + 1, 2x + 2}. As sanity check we count that the number of elements equals 32. Now the question is which of those elements are zerodivisers, that is, which of them divide x2 + 2x? We only have to check the polynomials of degree 1 since polynomials of degree 0 cannot divide x2 + 2x. By trying out combinations of the degree 1 polynomials we find out that: x(x + 2) = x2 + 2x ≡x2+2x 0 and 2x(2x + 1) = x2 + 2x ≡x2+2x 0. Therefore the four polynomials {x, x + 2, 2x, 2x + 1} are zerodivisors. (c) F [x] cannot be a field. This follows from the fact that multiplying two polynomials of degree at least 1 results in a poly- nomial with higher degree. To prove this more formally we can show that at least one element in F [x] has no inverse. We pick x, which is contained in F [x] because 1 ∈ F . Let a(x) = adxd + · · · + a1x + a0 ̸= 0 be an arbitrary polynomial in F [x], then: a(x) · x = adxd+1 + · · · + a1x2 + a0x. Since there are no zerodivisors in F and a(x) ̸= 0, this polynomial cannot be 1. (d) This exercise seems complicated because we have a polynomial where the coefficients are polyno- mials as well. However once we understand what’s going on, the exercise gets a lot easier. p(y) is polynomial with the coefficients x, 1 and (x + 1) (and 0). y itself can be any polynomial from the underlying ring GF(2)[x]x2+x+1. First we observe GF(2)[x]x2+x+1: Since the modulo polynomial has degree 2, this set has only 22 elements, namely {0, 1, x, x + 1}. Now we only have to test which of these elements are roots of p(y). For this we simply evaluate p(y) for each possible candidate: • p(0) = x · 0 2 + 0 + (x + 1) = x + 1 ̸= 0 • p(1) = x · 1 2 + 1 + (x + 1) = x + 1 + x + 1 = 0 • p(x) = x · x2 + x + (x + 1) = x3 + 1 = x2 + x + 1 = 0 • p(x + 1) = x · (x + 1)2 + (x + 1) + (x + 1) = x(x2 + 1) = x2 = x + 1 ̸= 0 Therefore 1 and x are the roots of p(y). (e) Like in the previous exercise the coefficients of this polynomial are polynomials as well and y ∈ {0, 1, x, x + 1}. We start by looking at the roots of a(y). This is done the same way as in the previous exercise: • a(0) = x · 03 + x · 0 2 + (x + 1) · 0 + x = x • a(1) = x · 13 + x · 1 2 + (x + 1) · 1 + x = 1 • a(x) = x · x3 + x · x2 + (x + 1) · x + x = x4 + x3 + x 2 = 0 77 • a(x + 1) = x · (x + 1)3 + x · (x + 1)2 + (x + 1) · (x + 1) + x = (x4 + x3 + x2 + x) + (x 3 + x) + (x2 + 1) + x = x + (x3 + x) + (x2 + 1) + x = x + x + 1 = 1 If you are confused about any of those derivations, remember that whenever a polynomial has de- gree ≥ 2, we have to perform a modulo operation with x2 + x + 1. For example we use the fact that x4 + x3 + x2 = x2 · (x2 + x + 1). Now since x is a root of a(y), we know that (y + x) divides a(y). (x = −x because we are in GF(2)). We divide a(y) by (y + x) using polynomial division and find another factor, xy2 + y + 1. The question remains whether xy2 + y + 1 is further reducible. To find out whether it is, we test it for roots: • a(0) = x · 02 + 0 + 1 = 1 • a(1) = x · 12 + 1 + 1 = x • a(x) = x · x2 + x + 1 = x3 + x + 1 = x • a(x + 1) = x · (x + 1)2 + (x + 1) + 1 = x3 + x + x + 1 + 1 = x3 = 1 This shows us that xy2 + y + 1 has no roots and is therefore an irreducible polynomial. We can conclude that (y + x) and (xy2 + y + 1) are the only factors of a(y). (f) These exercises are more annoying than they would appear in the exam, but nonetheless, we can practice working with fields: (i) Check if y4 + (x + 1)y2 + y + x ∈ GF(2)[x]x2+x+1[y] is reducible. Here it suffices to substitute every element of GF(2)[x]x2+x+1 for y and check for roots: y = 0 0 4 + (x + 1)02 + 0 + x = x y = 1 1 4 + (x + 1)12 + 1 + x = 1 + (x + 1) + 1 + x = 1 y = x x4 + (x + 1)x2 + x + x = x4 + x3 + x2 = 0 y = x + 1 (x + 1)4 + (x + 1)(x + 1)2 + (x + 1) + x = (x + 1) + 1 + (x + 1) + x = x + 1 So we have found a root y = x, and thus, the polynomial is reducible. (ii) Find all roots of x2y2 + (x + 1)y + (x2 + 1) ∈ GF(2)[x]x3+x+1[y]. Again, the process is similar to above: Substitute every polynomial in GF(2)[x]x3+x+1 for y and check if we find a root: y = 0 x20 2 + (x + 1)0 + (x2 + 1) = x2 + 1 y = 1 x21 2 + (x + 1)1 + (x2 + 1) = x y = x x2x2 + (x + 1)x + (x2 + 1) = x4 + x + 1 = x2 + 1 y = x + 1 x2(x + 1)2 + (x + 1)(x + 1) + (x2 + 1) = x y = x2 x2x4 + (x + 1)x2 + (x2 + 1) = x2 + x + 1 y = x2 + 1 x2(x2 + 1)2 + (x + 1)(x2 + 1) + (x2 + 1) = 0 y = x2 + x x2(x2 + x) 2 + (x + 1)(x2 + x) + (x2 + 1) = x2 + x + 1 y = x2 + x + 1 x2(x2 + x + 1)2 + (x + 1)(x2 + x + 1) + (x2 + 1) = 0 This gives us the two roots y = x2 + 1 and y = x2 + x + 1. 78 (iii) Factorize xy3 + 2y + 2x ∈ GF(3)[x]x2+x+2[y] into a product of irreducible monic polynomials and one constant factor. We first employ the same process again to find a root y = x + 2 i.e. a factor of y − (x + 2) = y + (2x + 1). From here, we carry out long division to get xy3 + 2y + 2x = (y + (2x + 1))(xy2 + (x + 1)y + (2x + 2)) The quotient is not monic but we can take out the x (multiply by x−1 = x + 1) to get: xy3 + 2y + 2x = x(y + (2x + 1))(y2 + (x + 2)y + (2x + 1)) You can verify that (2x + 1) 2 + (x + 2)(2x + 1) + (2x + 1) indeed has no roots, and thus is irreducible. We reiterate: this is much more work than an exam question would be. Even when creating the solution, I relied heavily on an online tool 1 to carry out the calculations in GF(3)[x]x2+x+2. 1https://wims.univ-cotedazur.fr/wims/en_tool~algebra~calcff.en.html 79 4 Logic 4.1 Proof Systems Definition 4.1. Let Σ be an alphabet (i.e., a set of allowed symbols). A proof system is a quadruple Π = (S, P, τ, ϕ), where: • S ⊆ Σ∗ is the set of (syntactic representations of ) mathematical statements. • P ⊆ Σ∗ is the set of (syntactic representations of ) proof strings. • τ : S → {0, 1} is a truth function. • ϕ : S × P → {0, 1} is a verification function. A given statement s ∈ S is true if τ (s) = 1. For a given statement s ∈ S and a proof p ∈ P, we say that p is a valid proof for s if ϕ(s, p) = 1. So far, these were only definitions. In general, we are free to define the truth- and verification functions as we like, which can lead to not very useful proof systems: Example. Let Σ = {0, 1} and S = {0, 1} ∗. We interpret every s ∈ S as a binary encoding of a natural number n(s) ∈ N. Next we define: • τ (s) = 1 :⇔ n(s) is an even prime. • ϕ(s, p) = 1 :⇔ n(s) = 2 · n(p). In this example, p = 10 is a valid proof proof for the statement s = 100 because n(s) = 4 = 2 · 2 = 2 · n(p). But does that mean that 4 is an even prime? Obviously, the answer is no. We all know that the only even prime is 2. The problem is, that we have just proved a false statement!2 Intuitively it should be clear, that proof systems which provide proofs to wrong statements, are not what we desire. If we can prove a statement, we want this statement to be true. This property of a proof system is called soundness. Another condition that is desirable is completeness. Completeness means, that every true statement can be proven by the proof system: Definition 4.2. Let Π = (S, P, τ, ϕ) be a proof system. (i) Π is sound ⇔ ∀s ∈ S [ (1) ︷ ︸︸ ︷ (∃p ∈ P ϕ(s, p) = 1) → (2) ︷ ︸︸ ︷ τ (s) = 1] (ii) Π is complete ⇔ ∀s ∈ S [τ (s) = 1 ︸ ︷︷ ︸ (2) → (∃p ∈ P ϕ(s, p) = 1) ︸ ︷︷ ︸ (1) ] Note, that the only difference is the order of (1) and (2) in the implication. Example. Consider statements of the form: ”F is satisfiable.” where F is a formula in propositional logic that uses only the variables A, B and C. Let S = P = {0, 1} ∗. We now propose functions τ and ϕ such that Π = (S, P, τ, ϕ) is a sound and complete proof system for statements described above. To do so, we first note that every such formula consists of 8 symbols: Σ3PL = {A, B, C, ∧, ∨, ¬, (, )} We can therefore encode every symbol in Σ3PL by a bitstring in {0, 1} 3 and therefore, every propositional formula containing only the variables A, B and C can be encoded by replacing every symbol with the corresponding bitstring of length 3. The truth function is defined as follows: τ (s) = 1 if and only if 2Note that τ (100) = 0 because 4 is not an even prime. 80 |s| = 3k (for some k ∈ N) 3 and s encodes a syntactically correct propositional formula that is satisfiable. Next, we define the verification function: ϕ(s, p) = 1 if and only if all of the following holds: • s encodes a syntactically correct propositional formula that only uses the variables A, B and C (as described in the definition of τ ). • |p| = 3. • The three bits of p describe a truth assignments A for the variables A, B and C respectively. The formula encoded by s is true under A. We now head on to prove soundness and completeness of our proof system: Claim Π is sound. Proof. Let s ∈ S and p ∈ P be arbitrary, such that ϕ(s, p) = 1. Therefore, by the definition of ϕ, we know that s encodes a syntactically correct formula F , and that p encodes a truth assignment A which is a model for F . Thus, F must be satisfiable. Claim Π is complete. Proof. We have to show that for an arbitrary s ∈ S with τ (s) = 1 there exists a p ∈ P with ϕ(s, p) = 1. Thus, let s ∈ S be arbitrary, such that τ (s) = 1. By Definition of τ , s encodes a satisfiable formula F of propositional logic that uses only A, B and C as variables. Hence, there exists a truth assignment A such that A ⊨ F . We set p as the bitstring of length 3 that encodes A. Then obviously ϕ(s, p) = 1. Another important condition that we want our proof systems to fulfil, is that the verification function ϕ should be ”efficiently computable”. Where it is not always clear what is meant by ”efficient”, one generally agrees that an algorithm that has an exponential runtime is considered inefficient. One can come up with an algorithm that parses a formula of propositional logic into a syntax tree, by linearly scanning the bitstring and dividing the string into substrings if an ∧ or an ∨ is encountered. Then one recursively applies the method on the substrings. Such an algorithm would have a the same time complexity as quicksort, which is O(n2) in the worst case, which is efficient enough for us. Such an algorithm - if cleverly designed - will also detect syntax errors while parsing, and therefore satisfy the first bullet point in the definition of τ . Having the formula in a syntax tree, it is easy to verify if the formula is satisfied under the truth function encoded in p, by simply traversing the tree recursively according to Definition 6.16. 4.2 Logical calculi A logical calculus gives us a way of deriving new formulas from an existing set of formulas. This is done by applying rules. Definition 4.3. Let M = {F1, . . . , Fk} be a set of formulas. A derivation rule is a rule for deriving a formula G from a set of formulas M. We write: M ⊢R G if G can be derived from the set M by rule R. M is called the precondition. A logical calculus is then easily defined in terms of derivation rules: 3|s| denotes the number of bits in the string s. 81 Definition 4.4. A (logical) calculus K is a finite set of derivation rules: K = {R1, . . . Rm}. If we have derived a new formula Fk+1 from an initial set of formulas M = {F1, . . . Fk} by applying a certain rule Ri ∈ K, we can include this newly derived formula in M : M1 = M ∪ {Fk+1} = {F1, . . . , Fk, Fk+1}. The next time we want to derive a new formula, we can start from M1 instead of M and thus we may be able to derive formulas that were not possible to derive by only using M . We can repeat this procedure an arbitrary but finite amount of times. If we eventually decide to stop, and we derived a formula G in the last step, then we say that we made a derivation of G from M in the calculus K. Formally: Definition 4.5. A derivation of a formula G from a set M of formulas in a calculus K is a finite sequence (of some length n) of applications of rules in K, leading to G. More precisely, we have M0 := M , Mi := Mi−1 ∪ {Gi} for 1 ≤ i ≤ n, where N ⊢Rj Gi for some N ⊆ Mi−1 and for some Rj ∈ K, and where Gn = G. We write M ⊢K G if there is a derivation of G from M in the calculus K. Example. Consider the calculus K = {R1, R2, R3} where: {F ∧ G} ⊢R1 F, {F ∧ G} ⊢R2 G, {F, G} ⊢R3 F ∧ G We want to derive the formula G = B ∧ A, from the set M = {A ∧ B}: {A ∧ B} ⊢R1 A, ⇒ M1 = {A ∧ B, A} {A ∧ B} ⊢R2 B, ⇒ M2 = {A ∧ B, A, B} {B, A} ⊢R3 B ∧ A, ⇒ M3 = {A ∧ B, A, B, B ∧ A} We have just showed that there is a derivation (of length 3) that started from the set M = {A ∧ B} and lead to G = B ∧ A and hence: {A ∧ B} ⊢K B ∧ A. Again, the above definitions introduced only a purely syntactic concept. We have never mentioned anything about whether the derivation rules actually make sense. Intuitively, we want to guarantee, that if we can derive a formula G from a set of formulas M , then we want G to be a logical consequence of M . This property is called soundness or correctness of a calculus. Another - in some sense less important - property of a calculus is completeness. In a complete calculus we always can derive a formula G from a set of formulas M if G is a logical consequence of M . Definition 4.6. Let K be a calculus, F be a formula and M be a set of formulas. We say: K is sound if M ⊢K F ⇒ M ⊨ F K is complete if M ⊨ F ⇒ M ⊢K F An easy way to show that a given calculus K is sound, is to show that every rule R ∈ K is correct: Definition 4.7. A derivation rule is correct if for every set M of formulas and every formula F : M ⊢R F ⇒ M ⊨ F. 82 Hands-On 1 1.1. Soundness And Completeness Of Proof Systems Let Σ = {0, 1}, S = P = {0, 1} 3. For the following definitions of τ and ϕ, decide whether the proof systems fulfil the soundness and completeness property. Justify your answers. (a) • τ (s) = 1 if s contains at most one 0. • ϕ(s, p) = 1 if s contains at most two 0 and s = p. (b) • τ (s) = 1 if s contains at least one 1. • ϕ(s, p) = 1 if the Hamming distance between s and p is exactly 3 and p contains exactly one 0. 1.2. Finding The Verification Function Let Π = (S, P, τ, ϕ) with S = P = N. Let τ : S ↦→ {0, 1} with τ (n) = 1 only if n has at least 4 (not necessarily different) prime factors. Define a function ϕ : S × P ↦→ {0, 1} such that Π is sound and complete. Justify your answers. Hint: You may assume that it can be tested efficiently, whether a given natural number is prime. Hint: If you are stuck, try solving the exercise with P = N4. 1.3. Finding The Truth Function Let S = P = {0, 1} 5 and ϕ : S × P ↦→ {0, 1} such that ϕ(s, p) = 1 only if p and s differ at exactly 4 positions and p contains at most one 0. Find a function τ : S ↦→ {0, 1}, such that Π = (S, P, τ, ϕ) is a proof system that is sound and complete. Proof both soundness and completeness. 1.4. Correct Rules (Challenge) Let K be a calculus. In this exercise we are going to show that K is sound if and only if every rule in K is correct. 1. Show that if K is sound then every rule R ∈ K is correct. 2. Show that if every rule in K is correct then K is sound. Hint: For this task you may use a slight abuse of notation, namely statements of the form: M ⊨ N where M and N are sets of formulas (normally only one formula is allowed on the right-hand side). We say M ⊨ N if for every structure A that is suitable for both M and N it holds that: A ⊨ M ⇒ A ⊨ N. Especially, you can use the two lemmas without a proof (but you are encouraged to try it as an exercise): Lemma 1 Let L, M and N be sets of formulas. Then: M ⊨ N ⇒ L ∪ M ⊨ N. Lemma 2 Let M and N be sets of formulas. Then: M ⊨ N ⇒ M ⊨ M ∪ N. 1.5. Soundness And Completeness Of Logical Calculi (a) Give a logical calculus that is sound but not complete. (b) Give a logical calculus that is complete but not sound. 83 4.3 Propositional Logic 4.3.1 Concepts: Syntax, Semantics, Interpretation, Model Definition 4.8. A logic is defined by the syntax and the semantics. The basic concept in any logic is that of a formula. Definition 4.9. The syntax of a logic defines an alphabet Λ (of allowed symbols) and specifies which strings in Λ ∗ are formulas (i.e, are syntactically correct). Definition 4.10. The syntax in propositional logic is defined as follows: An atomic formula is of the form Ai with i ∈ N. A formula is defined inductively: • An atomic formula is a formula • If F and G are formulas, then also ¬F , (F ∧ G), and (F ∨ G) are formulas. The alphabet of propositional logic could be made explicit. 4 Note that the symbols in F → G and F ↔ G are not part of the syntax, but are abbreviations for ¬F ∨ G and (F ∧ G) ∨ (¬F ∧ ¬G) respectively. Example. Which of these are syntactically correct formulas? a) A3A2∨ b) ((A13 ∨ A6) ∧ (A3 ∧ ¬A7)) c) A ∧ B ∧ C d) A2 ∨ A4 ∧ A3 Solution a) incorrect b) correct c) technically incorrect, but under standard convention it is correct. One may use A, B or C (and other letters) for atomic formulas and omit parentheses between atomic formulas with the same operator. d) incorrect, missing necessary parentheses Definition 4.11. The semantics of a logic defines two properties: • The semantics of a logic decides for each symbol in a formula F if it occurs free in F. • The semantics of a logic also defines a function assigning to each formula F and each suitable interpretation for F a truth value in {0, 1}. Note that a free symbol is not the same as a free variable (which will we see later in predicate logic). The exact formal definition of syntax and semantics of a logic are not the most important concepts to know, but the syntax and semantics of the specific logic, namely propositional and predicate logic, are of high importance. 4The alphabet of propositional logic is Λ = {Ai, ∧, ∨, ¬, (, )|i ∈ N} 84 Definition 4.12. An interpretation consists of a function that assigns to a certain set of symbols of Λ concrete values, where each symbol has a certain domain (of possible values). Typically, the domains are specified in terms of an universe U . For example, in predicate logic the domain for variables is U , for k-ary functions it is U k → U and for k-ary predicates it is U k → {0, 1} Definition 4.13. An interpretation is suitable for a formula F if it assigns a value to all symbols β ∈ Λ occurring free in F . The second property of the semantics of a logic induces a function σ assigning to each Formula F , and each interpretation A suitable for F , a truth value σ(F, A) in {0, 1}. In treatments of logic one often writes A(F ) instead of σ(F, A) and calls A(F ) the truth value of F under interpretation A. Definition 4.14. The semantics in propositional logic is defined as follows: In propositional logic, the free symbols of a formula are all symbols Ai, i.e, all the atomic formulas. For a set M of atomic formulas, an interpretation, called truth assignment, is a function α : M → {0, 1}. A truth assignment α is suitable for a formula F if M contains all atomic formulas appearing in F . For all atomic formulas Ai : A(Ai) = α(Ai) and A((F ∧ G)) = 1 if and only if A(F ) = 1 and A(G) = 1. A((F ∨ G)) = 1 if and only if A(F ) = 1 or A(G) = 1. A(¬F ) = 1 if and only if A(F ) = 0. Definition 4.15. A suitable interpretation A for which a formula F is true, (i.e., A(F ) = 1) is called a model for F , and one also writes A ⊨ F . More generally, for a set M of formulas, a suitable interpretation for which all formulas in M are true is called a model for M , denoted as A ⊨ M . If A is not a model for M one writes A ̸⊨ M . 85 4.3.2 Concepts: Satisfiability, Tautology, Consequence, Equivalence Definition 4.16. A formula F (or set M of formulas) is called satisfiable if there exists a model for F (or M ), and unsatisfiable otherwise. The symbol ⊥ is used for an unsatisfiable formula. Claim Let M be a set of formulas, then the following two statements are not equivalent: • M is satisfiable. • Every formula in M is satisfiable. In fact, the first statement implies the second. Proof. We determine a set M of formulas, for which every formula in M is satisfiable, but M itself is not satisfiable. M = {A, ¬A} A and ¬A are both satisfiable, but M is not satisfiable. Definition 4.17. A formula F is called a tautology or valid if it is true for every suitable inter- pretation. The symbol ⊤ is used for a tautology. Note that the symbols ⊥ and ⊤ are not formulas itself, i.e., are not part of the syntax of the logic, but they are used in expressions like F ≡ ⊥, which is to be understood as standing for an arbitrary unsatisfiable formula. F ≡ ⊥ just means, that F is unsatisfiable. Lemma 4.1 A formula F is a tautology if and only if ¬F is unsatisfiable. Definition 4.18. A formula G is a logical consequence of a formula F (or a set M of formulas), denoted F ⊨ G (or M ⊨ G ), if every interpretation suitable for both F (or M ) and G, which is a model for F (for M ), is also a model for G. Example. We show for any formulas F and G: F ∧ G ⊨ F . Let A be any model for F ∧ G, i.e, A(F ∧ G) = 1. Then we have, by the semantics of propositional logic, that A(F ) = 1 and A(G) = 1. That means, that any model for F ∧ G is also a model for F , thus F ∧ G ⊨ F . Definition 4.19. Two formulas F and G are equivalent, denoted F ≡ G, if every interpretation suitable for both F and G yields the same truth value for F and G, i.e., if each one is a logical consequence of the other: F ≡ G :⇐⇒ F ⊨ G and G ⊨ F . Definition 4.20. If F is a tautology, one also writes ⊨ F 86 4.3.3 Normal Forms Definition 4.21. A literal is an atomic formula or the negation of an atomic formula. Definition 4.22. A formula F is in conjunctive normal form (CNF) if it is a conjunction of disjunctions of literals, i.e., if it is of the form F = (L11 ∨ ... ∨ L1m1) ∧ ... ∧ (Ln1 ∨ ... ∨ Lnmn ) for some literals Lij. Definition 4.23. A formula F is in disjunctive normal form (DNF) if it is a disjunction of conjunctions of literals, i.e., if it is of the form F = (L11 ∧ ... ∧ L1m1) ∨ ... ∨ (Ln1 ∧ ... ∧ Lnmn ) for some literals Lij. Theorem 6 Every formula is equivalent to a formula in CNF and also to a formula in DNF. Example. Consider the formula F = B ∧ (¬B → A). We can construct an equivalent formula to F in DNF and CNF with its function table. A B B ∧ (¬B → A) 0 0 0 0 1 1 1 0 0 1 1 1 Each row in the function table represents one assignment or one clause. For DNF we ”or” all the rows, which evaluate to 1. For each row we ”and” all the literals as follows: If an atomic formula Ai = 0 in that row, then we take the literal ¬Ai, otherwise we take the literal Ai. DNF: (¬A ∧ B) ︸ ︷︷ ︸ row 2 ∨ (A ∧ B) ︸ ︷︷ ︸ row 4 For CNF we ”and” all the rows, which evaluate to 0. For each row we ”or” all the literals as follows: If an atomic formula Ai = 0 in that row, then we take the literal Ai, otherwise we take the literal ¬Ai. CNF: (A ∨ B) ︸ ︷︷ ︸ row 1 ∧ (¬A ∨ B) ︸ ︷︷ ︸ row 3 Note that in this example it is only by chance, that the CNF is equal to the DNF when exchanging all ∧ with an ∨ and vice versa. It is also not always true that CNF and DNF have the same amount of clauses. 4.4 Resolution Calculus The resolution calculus serves as a concrete example of a logical calculus. The calculus consists of only one derivation rule and a derivation serves the purpose of showing that a given formula F is unsatisfiable. An important detail about the resolution calculus is, that the precondition 5 consists of clauses: 5The set of formulas on the left side of the derivation rule. 87 Definition 4.24. A clause is a set of literals. We can easily associate any propositional formula with a set of claues, if we transform the formula into CNF: Definition 4.25. The set of clauses associated to a formula F = (L11 ∨ · · · ∨ L1m1) ∧ · · · ∧ (Ln1 ∨ · · · ∨ Lnmn ) in CNF, denoted as the set K(F ) is the set: K(F ) = {{L11, . . . , L1m1}, . . . , {Ln1, . . . , Lnmn }} The set of clauses associated with a set M = {F1, . . . Fk} of formulas is the union of their clause sets: K(M ) = k⋃ i=1 K(Fi) Now we can define how the derivation rule works: Definition 4.26. A clause K is a resolvent of clauses K1 and K2 if there is a literal L such that L ∈ K1, ¬L ∈ K2, and K = (K1 \\ {L}) ∪ (K2 \\ {¬L}). If two clauses K1, K2 have a resolvent K, then K is derived by the rule res: {K1, K2} ⊢res K. As mentioned earlier, the resolution calculus is defined as: Res = {res}. Example. Let F := (A ∨ C) ∧ (¬D ∨ ¬A) ∧ B ∧ (¬B ∨ ¬C) ∧ ¬C ∧ (C ∨ D) We want to show that F is unsatisfiable. Because F is already in CNF we can derive ∅ from K(F ) as follows: ∅ {D} {C, D}{¬C} {¬D} {¬C} {¬B, ¬C}{B} {C, ¬D} {¬D, ¬A}{A, C} Hands-On 2 2.1. Propositional Logic (a) We would like to extend propositional logic by the symbol ↓, denoting the NOR operation ((A ↓ B) is true if and only if both A and B are false). How would one extend the definitions of syntax and semantics of propositional logic, in order to incorporate NOR? (b) For each entry, determine if the interpretation is suitable for the formula and if so, also deter- mine if it is a model. 88 A ∧ (B ∨ ¬A) ∧ ¬C A ∨ ¬A ∨ B A ∨ ¬D A = {A = 0, B = 0, C = 1, D = 1} A = {A = 0, D = 0} A = {A = 1, B = 1, C = 0} (c) For each of the following sets of formulas, either find a model or show that it is unsatisfiable: (1) M = {A ∨ B, C ∨ D, B ∧ C, ¬D ∧ B} (2) N = {A → B, C ∧ D, C → A, B → ¬D} (d) Show that F ⊨ G if and only if ⊨ F → G. (e) Consider this statement: B ∨ C ⊨ (B ∧ C) ∨ (A ∧ ¬A) Does the following text correctly prove that the statement is false? If not, prove or disprove the statement correctly. ”This statement is false. One can find an interpretation A = {B = 1, C = 0}, which is a model for the left side, but not a model for the right side, since A(B ∧ C) = 0 and A ∧ ¬A is unsatisfiable. Therefore is A a counterexample and the statement is false.” (f) Determine which of those formulas are in DNF, CNF or not in a normal form: (1) (B ∨ ¬A) ∧ (C ∨ D) ∧ (A ∨ C) (2) (A ∨ B) ∧ C (3) A ∨ (B ∧ C) (4) A ∨ B ∨ C (5) (A ∧ B) ∨ (¬A ∧ C) ∧ (¬D ∧ B) (g) Let F ≡ (A → (B∧C))∧B. Using the method of function tables, construct a formula equivalent to F in conjunctive normal form and a formula equivalent to F in disjunctive normal form. 2.2. The Resolution Calculus (a) Prove or disprove the following statement about the resolution calculus: ”The resolution calculus is not complete.” (b) Show that for a set of formulas M and a formula F it holds that: M ⊨ F if and only if M ∪ {¬F } is unsatisfiable. (c) Show that A ⊨ A ∨ B using the resolution calculus. 89 4.5 Predicate logic 4.5.1 Syntax, Free Variables Definition 4.27. The syntax in predicate logic is defined as follows: • A variable symbol is of the form xi with i ∈ N • A function symbol is of the form f (k) i with i, k ∈ N, where k denotes the number of arguments of the function. Function symbols for k = 0 are called constants. • A predicate symbol is of the form P (k) i with i, k ∈ N, where k denotes the number of arguments of the predicate. Predicate symbols for k = 0 are either Verum or Falsum i.e. always true or always false. • A term is defined inductively: A variable is a term, and if t1, ..., tk are terms, then f (k) i (t1, ..., tk) is a term. For k = 0 one writes no parentheses. • A formula is defined inductively: - For any i and k, if t1, ..., tk are terms, then P (k) i (t1, ..., tk) is a formula, called an atomic formula. - If F and G are formulas, then ¬F , (F ∧ G), and (F ∨ G) are formulas. - If F is a formula, then, for any i, ∀xi F and ∃xi F are formulas. Definition 4.28. Every occurrence of a variable in a formula is either bound or free. If a variable x occurs in a (sub-)formula of the form ∀x G or ∃x G, then it is bound, otherwise it is free. The occurrence of a variable x immediately following a quantifier is also bound. Any variable x - if it is bound - can only be bound to exactly one quantifier. A formula is closed if it contains no free variables. Example. Consider the formula ∀x ↓ 1 ∃x ↓ 2 P (x ↓ 3 ) ∧ ∀x ↓ 4 (∃x ↓ 5 Q(x ↓ 6 ) ∧ R(x ↓ 7 )) ∧ S(x ↓ 8 ). In this formula there are in total 4 quantifiers and 8 occurrences of the variable x. • Occurrence 1: x directly follows the first ∀ quantifier and is hence bound to it. • Occurrence 2: x directly follows the first ∃ quantifier and is hence bound to it. • Occurrence 3: x is bound to the first ∃ quantifier (and it is not bound to the first ∀ quantifier). • Occurrence 4: x directly follows the second ∀ quantifier and is hence bound to it. • Occurrence 5: x directly follows the second ∃ quantifier and is hence bound to it. • Occurrence 6: x is bound to the second ∃ quantifier. • Occurrence 7: x is bound to the second ∀ quantifier. • Occurrence 8: x is free. 90 4.5.2 Semantics In predicate logic, the free symbols of a formula are all free variables, all function symbols, and all predicate symbols. An interpretation in the context of predicate logic is called a structure and must define a universe and the meaning of all these free symbols. Definition 4.29. An interpretation or structure is a tuple A = (U, ϕ, ψ, ξ) where • U is a non-empty set, the so-called universe, • ϕ is a function assigning to each function symbol (in a certain subset of all function symbols) a function, where for a k-ary function symbol f , ϕ(f ) is a function U k → U . • ψ is a function assigning to each predicate symbol (in a certain subset of all predicate symbols) a function, where for a k-ary predicate symbol P , ψ(P ) is a function U k → {0, 1}, and where • ξ is a function assigning to each variable symbol (in a certain subset of all variable symbols) a value in U . For notational convenience, for a structure A = (U, ϕ, ψ, ξ) and a function symbol f one usually writes f A instead of ϕ(f ). Similarly, one writes P A instead of ψ(P ) and xA instead of ξ(x). One also writes U A rather than U to make A explicit. Definition 4.30. A structure A is suitable for a formula F if it defines all function symbols, predicate symbols, and free variables of F . Example. Consider the formula F = ∀x∃y (P (y) ∧ Q(f (x), y)) ∧ P (z). A suitable structure for F must define the universe, predicates P, Q, function f and free variable z. An example of a suitable structure for F is A, where U A = N \\ {0} P A(x) = 1 if and only if x is odd Q A(x, y) = 1 if and only if x < y f A(x) = x zA = 5 This structure defines all free symbols and is hence suitable. A is also a model. It is important that the predicates and functions are defined correctly. One might think that if the structure is defined slightly different, in particular, if f is defined as f A(x) = y − x, it is also a model. But this structure is incorrect, because a function or predicate can only depend on its arguments and no other variables. Another incorrect structure would be, if f was defined as f A(x) = x − x, which is equal to f A(x) = 0. In this case, f is not a function U 2 → U , because 0 is not in U . Definition 4.31. For a structure A = (U, ϕ, ψ, ξ), we define the value (in U ) of terms and the truth value of formulas under that structure. • The value A(t) of a term t is defined recursively as follows: - If t is a variable, then A(t) = ξ(t). - If t is of the form f (t1, ..., tk) for terms t1, ..., tk and a k-ary function symbol f , then A(t) = ϕ(f )(A(t1), ..., A(tk)). • The truth value of a formula F is defined recursively as follows: - A((F ∧ G)) = 1 if and only if A(F ) = 1 and A(G) = 1; - A((F ∨ G)) = 1 if and only if A(F ) = 1 or A(G) = 1; - A(¬F ) = 1 if and only if A(F ) = 0 91 - If F is of the form F = P (t1, ..., tk) for terms t1, ..., tk and a k-ary predicate symbol P , then A(F ) = ψ(P )(A(t1), ..., A(tk)). - If F is of the form ∀x G or ∃x G, then let A[x→u] for u ∈ U be the same structure as A except that ξ(x) is overwritten by u (i.e., ξ(x) = u): A(∀x G) = { 1 if A[x→u](G) = 1 for all u ∈ U 0 else. A(∃x G) = { 1 if A[x→u](G) = 1 for some u ∈ U 0 else. 4.5.3 Summary of the General Concepts in Logic Concept Interpretation Propositional Logic Also called truth assignment [The function α] assigns to each atomic formula Ai (in a certain subset of all atomic formulas) a value in {0, 1}. Predicate Logic Also called structure 4-tuple (U, ϕ, ψ, ξ) Defines a non-empty universe U. [The function ϕ] assigns to each k-ary function symbol f (in a certain subset of all function symbols) a function, where ϕ(f ) is a function U k → U . [The function ψ] assigns to each k-ary predicate symbol P (in a certain subset of all predicate symbols) a function, where ψ(P ) is a function U k → {0, 1}. [The function ξ] assigns to each variable symbol (in a certain subset of all variable symbols) a value in U . Concept Suitable Interpretation Propositional Logic A truth assignment is suitable for a formula F if it defines all free symbols: all atomic formulas in F Predicate Logic A structure is suitable for a formula F if it defines all free symbols: all function symbols in F all predicate symbols in F all free variables in F 92 Concepts Model, Satisfiability, Tautology, Logical Consequence, Equivalence Propositional Logic and Predicate Logic Model: An interpretation A suitable for formula F is a model, if the formula is true under this interpretation, i.e, if A(F ) = 1. Satisfiability: A formula F is satisfiable if there exists a model for F . Otherwise, F is unsatisfiable (F ≡⊥). Tautology: A formula F is a tautology (F ≡ ⊤), if the formula is true under every suitable interpretation, i.e., if A(F ) = 1 for every suitable interpretation A. Logical Consequence: A formula G is a logical consequence of formula F (F ⊨ G), if every interpretation suitable for both F and G, which is a model for F , is also a model for G. Equivalence: Two formulas F and G are equivalent if and only if F ⊨ G and G ⊨ F . Concept Function Table Propositional Logic The function table of a formula F lists all ”non-redundant”6 suitable interpretations. and the truth value of F under those interpretations. If F has n different atomic formulas, then the function table will have 2 n rows. Each row represents exactly one ”non-redundant” suitable interpretation. One can easily check if a formula F is a tautology given its function table. Predicate Logic For a formula in predicate logic one cannot construct a function table, because every formula in predicate logic has infinitely many ”non-redundant” suitable interpretations. For example, the structure A is a model for the formula G ≡ ∀x P (x), where U A = N and P A(x) = 1 if and only if x ≥ 0. But note that this does not mean, that G is a tautology, because, just like in propositional logic, a single structure only represents a single row in the function table. 4.5.4 Some basic Equivalences Lemma 4.2 For any formulas F , G and H we have 1) F ∧ F ≡ F and F ∨ F ≡ F (idempotence); 2) F ∧ G ≡ G ∧ F and F ∨ G ≡ G ∨ F (commutativity); 3) (F ∧ G) ∧ H ≡ G ∧ (F ∧ H) and (F ∨ G) ∨ H ≡ G ∨ (F ∨ H) (associativity); 4) F ∧ (F ∨ G) ≡ F and F ∨ (F ∧ G) ≡ F (absorption); 5) F ∧ (G ∨ H) ≡ (F ∧ G) ∨ (F ∧ H) (distributive law); 6a ”non-redundant” suitable interpretation only assigns to free symbols (and no other symbols). 93 6) F ∨ (G ∧ H) ≡ (F ∨ G) ∧ (F ∨ H) (distributive law); 7) ¬¬F ≡ F (double negation); 8) ¬(F ∧ G) ≡ ¬F ∨ ¬G and ¬(F ∨ G) ≡ ¬F ∧ ¬G (de Morgan’s rules); 9) F ∨ ⊤ ≡ ⊤ and F ∧ ⊤ ≡ F (tautology rules); 10) F ∨ ⊥≡ F and F ∧ ⊥≡⊥ (unsatisfiability rules); 11) F ∨ ¬F ≡ ⊤ and F ∧ ¬F ≡⊥. Lemma 4.3 For any formulas F , G, and H, where x does not occur free in H, we have 1) ¬(∀x F ) ≡ ∃x ¬F ; 2) ¬(∃x F ) ≡ ∀x ¬F ; 3) (∀x F ) ∧ (∀x G) ≡ ∀x (F ∧ G); 4) (∃x F ) ∨ (∃x G) ≡ ∃x (F ∨ G); 5) ∀x∀y F ≡ ∀y∀x F ; 6) ∃x∃y F ≡ ∃y∃x F ; 7) (∀x F ) ∧ H ≡ ∀x (F ∧ H); 8) (∀x F ) ∨ H ≡ ∀x (F ∨ H); 9) (∃x F ) ∧ H ≡ ∃x (F ∧ H); 10) (∃x F ) ∨ H ≡ ∃x (F ∨ H). Lemma 4.4 If one replaces a subformula G of a formula F by an equivalent (to G) formula H, then the resulting formula is equivalent to F . 4.5.5 Substitution of (Bound) Variables, Normal Forms We have seen two ways of modifying formulas through substitution: • Substitution of a free variable through a term. • Substitution of a bound variable through another variable symbol. The former is only a syntactic modification: Definition 4.32. For a formula F , a variable x and a term t, F [x/t] denotes the formula obtained from F by substituting every free occurrence of x by t. We can substitute a simple variable by an arbitrary term: Example. Let F = ∀x P (x, y) ∨ (∃y Q(y) ∧ ¬P (y, y)) The first occurrence of y and the last two occurrences of y are free. Because f (a, b, c) is a term, we can substitute all free y by this term: F [y/f (a, b, c)] = ∀x P (x, f (a, b, c)) ∨ (∃y Q(y) ∧ ¬P (f (a, b, c), f (a, b, c))) 94 The other substitution that we learned is called bound substitution. Whereas the substitution of free variables is simply a syntactic rule that allows to construct new formulas (without any semantic meaning), bound substitution is a way to retrieve an equivalent formula from a given formula, and hence has a semantic meaning: Lemma 4.5 For a formula G in which y does not occur, we have • ∀x G ≡ ∀y G[x/y], • ∃x G ≡ ∃y G[x/y]. The reason why we need bound substitution is, that we sometimes want our formulas to be in a specific format. An example for such a format is called rectified form: Definition 4.33. A formula F is in rectified form if: • No variable in F occurs both as a bound and as a free variable • All bound variables in F that appear directly after a quantifier symbol are distinct. Another format is called prenex form: Definition 4.34. A formula of the form Q1x1 Q2x2 · · · Qnxn G, where the Qi are arbitrary quantifiers (∀ or ∃) and G is a formula free of quantifiers, is said to be in prenex form. Theorem 7 For every formula there is an equivalent formula in prenex form. For a formula F one can construct an equivalent formula in prenex form with the following steps: 1. Transform F into an equivalent formula F ′ that is in rectified form. 2. Apply a sequence of equivalences of Lemma 4.2 and Lemma 4.3 to move all quantifiers to the beginning of the formula. Example. We want to transform F = ∀y (∃z P (x, y, z) ∧ ∀x ¬Q(x)) into prenex form. F ≡ ∀y (∃z P (x, y, z) ∧ ∀u ¬Q(u)) (bound substitution) ≡ ∀y (∃z (P (x, y, z) ∧ ∀u ¬Q(u))) (9) ≡ ∀y ∃z (P (x, y, z) ∧ ∀u ¬Q(u)) (omit brackets) ≡ ∀y ∃z (∀u ¬Q(u) ∧ P (x, y, z)) (∧-commutativity) ≡ ∀y ∃z (∀u (¬Q(u) ∧ P (x, y, z))) (7) ≡ ∀y ∃z ∀u (¬Q(u) ∧ P (x, y, z)) ︸ ︷︷ ︸ =:G (omit brackets) In the first step we have made a bound substitution of x by u, because x occurs both free (first occurrence of x) and bound in F . After that, we have a rectified formula. The numbers (7) and (9) refer to Lemma 4.3. It is important to note that in general, a substitution of a free variable is not an equivalence transforma- tion: 95 Example. Let F := P (x) and F ′ := F [x/y] = P (y). Then F ̸≡ F ′. To see this, consider the structure A where: U A = {0, 1}, P A(x) = 1 :⇔ x = 1, x A = 1, yA = 0 A is suitable for both F and F ′ but obviously A ⊨ F while A ̸⊨ F ′. Hands-On 3 3.1. Structures For the formula F ≡ ∀x(∃y (P (x, y) ∧ P (y, x)) → P (f (x, y), f (y, x))) give a structure A, which is (a) not suitable for F (b) suitable and a model for F (c) suitable but not a model for F 3.2. Free Variables For each formula, determine which variables are free and which are bound. For each bound variable, determine the quantifier to which it is bound. (a) ∀x∃y Q(x) ∨ P (f (y)) ∨ ∀y R(x) (b) ∀x (∃y Q(x) ∨ P (f (y))) ∨ ∀y R(x) (c) ∀x∃y (Q(x) ∨ P (f (y)) ∨ ∀y R(x)) 3.3. Syntax For each formula, decide if it is a formula, a statement about formulas or syntactically incorrect. P, Q, R are a predicates, x, y, z are variables and f, g, h are functions. (a) P (f (x)) ∨ Q(g(y)) ⊨ R(h(z)) (b) P (x) ≡ ∀x (Q(x) ∨ ∃f P (f (x))) (c) ∀x P (x) → (Q(x) ∨ Q(f (z))) (d) P (f (x) ∧ f (y)) → P (f (x)) (e) (P (x) ⊨ Q(x)) ∨ (Q(x) ⊨ P (x)) (f) ∀x P (x) ↔ Q(x) ≡ ∃x P (x) → Q(x) 3.4. Logical Consequences For each logical consequence, decide if it true or false. If it is false, give a counterexample (that is, an example of the formulas F and G, aswell as an appropriate structure). (a) (∀x F ) ∨ G ⊨ ∀x (F ∨ G) (b) ∀x (F ∨ G) ⊨ (∀x F ) ∨ G (c) (∀x F ) ∧ G ⊨ ∀x (F ∧ G) (d) ∀x (F ∧ G) ⊨ (∀x F ) ∧ G 3.5. Prenex Form Consider the formula F ≡ ∀x (P (x) ∨ ∃x Q(f (x))) ∧ ∃y R(g(y, x)). (a) Bring F into a rectified form. (b) Bring F into a prenex form. You can use the result of the previous subtask. 3.6. Tautologies 96 (a) Which of the following formulas are tautologies? If they are, prove it by using the equivalences from Lemma 4.2, 4.3 and 4.5. You can also use the the fact, that F ∨ ∃y ¬F is a tautology. You can try to prove this yourself or find it in the solutions of exercise 13.3 a) (not in this script). If they aren’t, prove it by showing an counterexample (give an interpretation under which the formula is false). (1) ∃x (P (x) → ∀x P (x)) (2) ∃x P (x) ∨ ∀x P (x) (3) (∀x (P (x) → Q(x)) ∧ P (y)) → Q(y) (b) Prove that the following statements are true: (1) ”There exists a student, such that, if this student passes the exam, then everyone passes the exam.” (2) ”If every planet is flat and the earth is a planet, then the earth is also flat” 3.7. Challenge (a) For any formulas F ,G and H, where x does not occur free in H: Prove this equivalence: (∃x F ) ∨ H ≡ ∃x (F ∨ H). (b) (1) Extend the syntax and the semantics of predicate logic to include the equality symbol ”=”, where t1 = t2 is true if and only if the term t1 has the same value as the term t2. (2) Consider the Universe G and the function f (x, y) = x ∗ y. Using only the function f , describe the following sentences as formulas. (∀, ∃, ∧, ∨, ¬, =, parentheses and variables are all allowed aswell). (i) ⟨G; ∗⟩ is a monoid. (ii) ⟨G; ∗⟩ is a group. 97 4.6 Solutions 4.6.1 Solutions for Hands-On 1 1.1 Soundness And Completeness Of Proof Systems (a) The system is complete but not sound. Proof. To see that it is complete, let s ∈ S such that τ (s) = 1. Hence, s contains at most one 0. Set p = s and we get ϕ(s, p) = 1 because if s contains at most one 0, it also contains at most two 0. To show that the system is unsound, we give a counterexample: s = 001 is not true under τ because it contains more than one 0. Nevertheless p = s is a valid proof for s. (b) The system is sound but not complete. Proof. To show soundness, let s ∈ S and p ∈ P such that ϕ(s, p) = 1. Because p has exactly one 0 and d(s, p) = 3, s must contain exactly one 1, thus τ (s) = 1. To disprove completeness, we again give a counterexample: s = 111 is true, because it contains at least one 1. But there is no proof for s, because if such a p would exist, it would have the form p = 000 in order to satisfy the first condition of ϕ. But then the second condition is violated, which is a contradiction. 1.2 Finding The Verification Function If we can choose p ∈ N4, the answer is simple: ϕ(n, p) = 1 only if p = (p1, p2, p3, p4) and it holds that: • pi is prime (for 1 ≤ i ≤ 4). • p1 · p2 · p3 · p4 | n Claim Π is complete. Proof. Let n ∈ N such that τ (n) = 1. Therefore s has at least four prime factors. Then choose p = (p1, p2, p3, p4) where the pi are four prime factors of n (1 ≤ i ≤ 4). But then the product of those prime factors divides p and hence ϕ(n, p) = 1. Claim Π is sound. Proof. Let n ∈ N, p ∈ N4 such that ϕ(n, p) = 1. But then, n is dividable by the product of four primes pi stored in p. By the definition of the divides relation, there exists a k ∈ N such that: n = k · (p1 · p2 · p3 · p4). This means that n has four prime factors and hence τ (n) = 1. The problem if we only have P = N is, that we somehow need to encode 4 natural numbers in a single natural number. We remember from Theorem 3.16, that there exists a bijection f : N → N × N, given by the definition: f (n) = (k, m), where:    t > 0 is the smallest integer such that (t+1 2 ) > n k + m = t − 1 m = n − (t 2 ) (∗) The tuples f (0), f (1), f (2), . . . correspond to the enumeration of the diagonals in the N2-plane. Note, that the tuples f (n) can be efficiently computed by solving the equations in (∗). By using the bijection f , we can construct a function g : N4 → N as follows: g(x) = (z1, z2, z3, z4), where {(z1, z2) = f (y1) (z3, z4) = f (y2) } , where (y1, y2) = f (x). We first apply f (x) to get a tuple (y1, y2) and then apply f again on the two entries y1 and y2 to receive a 4-tuple (z1, z2, z3, z4). 98 Claim g is bijective. Proof. We need to show that g is injective and surjective. • Injectivity: Let x, y ∈ N and x ̸= y. Hence f (x) = (a1, a2) ̸= (b1, b2) = f (y) because f is injective. But then for some i ∈ 1, 2 we have ai ̸= bi which - again by injectivity of f - implies that f (ai) ̸= f (bi). Thus the 4-tuples g(x) and g(y) cannot be equal.✓ • Surjectivity: Let (z1, z2, z3, z4) ∈ N4 be arbitrary. Because f is surjective, there exists a y1 such that f (y1) = (z1, z2) and a y2 such that y2 = (z3, z4). By the same reason, there exists an x such that f (x) = (y1, y2).✓ We define our verification function as follows: ϕ(n, p) = 1 :⇔ All of the three hold:    g(p) = (p1, p2, p3, p4) pi is prime (for 1 ≤ i ≤ 4) p1 · p2 · p3 · p4 | n Since g is a bijection, we can always encode the four primes in a single number p. The proof that the proof system is still sound and complete under this verification function, stays the same as in the case where P = N4. 1.3 Finding the truth function We first have to introduce some notation: Let x = x1, x2, x3, x4, x5 ∈ {0, 1}5, that is we denote the i-th bit of x by xi ∈ {0, 1}. We call the set of indices I = {1, 2, 3, 4, 5}. For every x, we can partition I into two disjoint subsets x(0) and x(1) as follows: x(0) := {i ∈ I|xi = 0}, x (1) := {i ∈ I|xi = 1}, x (0) ∪ x (1) = I Next we define our truth function: τ (s) = 1 :⇔ |s (0)| ≥ 3. In words: s is true if it contains at least three 0. Claim Π is complete. Proof. Let s ∈ S be arbitrary such that τ (s) = 1. Therefore s may contain 3, 4 or 5 zeros. We make a case distinction: • s (0) = 5. Then s = 00000 and for example p = 11110 is a valid proof, because the hamming distance of p and s is 4, and p has only one 0.✓ • s (0) = 4. Then we choose p = 11111 and s and p differ at exactly 4 positions.✓ • s (0) = 3. We choose a p ∈ P such that p(1) = s (0) and for the remaining two positions I \\ s (0) in p we set one to 1 and one to 0. Therefore p consists of four 1 and one 0. p and s differ in the three positions s (0) (because s contains only 0 and p contains only 1 at those positions), and at the position where we set p to 0. This leads to a total difference of 4 positions. ✓ 99 Claim Π is sound. Proof. By contradiction, assume that ϕ(s, p) = 1 and τ (s) = 0 for some s ∈ S, p ∈ P. Because we assumed that τ (s) = 0, we conclude that |s (0)| ≤ 2. But then the hamming distance between p and s can be at most 3, by setting the only available 0 of p to a position where s is not 0 already. But this means that it is impossible to find a p that differs at 4 positions and thus ϕ(s, p) = 0 which is a contradiction. Hence, Π is sound. 1.4 Correct Rules (Challenge) Let K = {R1, . . . , Rm} be a calculus. 1. K is sound ⇒ Ri is correct (for 1 ≤ i ≤ m). Proof. Assume (by contradiction) that K is sound but there exists a rule R ∈ K that is not correct and is defined as: M ⊢R F Consider the derivation M ⊢K F that consists of a single step in which the rule R got applied. Because K is sound by assumption, we have M ⊨ F . But because we only used the rule R which is incorrect by assumption, we have M ̸⊨ F which is a contradiction. 2. Ri is correct (for 1 ≤ i ≤ m) ⇒ K is sound. Proof. Let M ⊢K G be an arbitrary derivation of length n (for some n) in K. During this derivation, we construct a sequence of sets M0, . . . , Mn that we get by deriving G from M . Consider an arbitrary derivation step where we derived N ⊢Rj Gi from some N ⊆ Mi−1. Because all rules are correct, we have N ⊨ Gi, or in our newly introduced notation: N ⊨ {Gi}. Because N ⊆ Mi−1 we have Mi−1 ⊨ {Gi} by Lemma 1. Next, we apply Lemma 2 and get Mi−1 ⊨ Mi−1 ∪ {Gi}. But Mi−1 ∪ {Gi} =: Mi (by Definition 4.5) and hence we have: Mi−1 ⊨ Mi (for 1 ≤ i ≤ n). Note that Mn = Mn−1 ∪ {G} and hence Mn ⊨ G. Because of transitivity of ⊨ we conclude: M = M0 ⊨ M1 ⊨ . . . ⊨ Mn ⊨ G ⇒ M ⊨ G 1.5 Soundness And Completeness Of Logical Calculi (a) Let K1 = {R1} where {F } ⊢R1 F . Claim K1 is sound. Proof. Let M = {F1, . . . Fn} be an arbitrary set of formulas. The only ways to apply a derivation rule, is to pick an arbitrary Fi ∈ M and applying the rule: {Fi} ⊢R1 Fi. Since such a step only generates a formula, that is already in M , every derivation 7 will end up deriving a formula Fi ∈ M : M ⊢K1 Fi for some i ∈ {1, . . . , n}. But obviously M ⊨ Fi, because if A ⊨ M , then A ⊨ Fi for all Fi ∈ M 7Here, we mean an arbitrary sequence of applications of derivation rule, according to Definition 6.19. 100 Claim K1 is not complete. Proof. Assume it is. But then we should be able to derive {A} ⊢K1 A ∨ B, because {A} ⊨ A ∨ B. But we have already seen above, that we cannot derive new formulas from any starting set M , hence we will never derive A ∨ B and therefore: {A} ̸⊢K1 A ∨ B. (b) Let K2 = {R2} where ⊢R2 F . Claim K2 is complete. Proof. Let M be set of formulas and F be an formula such that M ⊨ F . Obviously M ⊢K2 F , namely by the derivation consisting of a single step: ⊢R2 F . Note that in a derivation, we always choose only a subset of formulas in M to apply a derivation rule. Here we used the empty set, because our only rule R2 demands this format. Claim K2 is not sound. Proof. We can derive ⊢K2 A, but obviously ̸⊨ A, because A(A) = 0 if A = {A = 0}. 101 4.6.2 Solutions for Hands-On 2 2.1 Propositional Logic (a) To the definition of syntax we add the following statement: If F and G are formulas, then (F ↓ G) is also a formula. To the definition of semantics we add the following: A((F ↓ G)) = 1 if and only if A(F ) = 0 and A(G) = 0. (b) Notice that a given interpretation can be both suitable and not suitable depending on the formula. A ∧ (B ∨ ¬A) ∧ ¬C A ∨ ¬A ∨ B A ∨ ¬D A = {A = 0, B = 0, C = 1, D = 1} suitable, not a model suitable, model suitable, not a model A = {A = 0, D = 0} not suitable not suitable suitable, model A = {A = 1, B = 1, C = 0} suitable, model suitable, model not suitable (c) (1) A = {A = 1, B = 1, C = 1, D = 0} is a model for M . Another solution is A = {A = 0, B = 1, C = 1, D = 0} (2) N is unsatisfiable. We show this with an proof by contradiction. Suppose N was satisfiable and A is a model for N . Then we have A(A → B) = 1, A(C ∧ D) = 1, A(C → A) = 1 and A(B → ¬D) = 1. Then A(C) = 1 and A(D) = 1 follows from A(C ∧ D) = 1. Then A(A) = 1 follows from A(C) = 1 and A(C → A) = 1. Then A(B) = 1 follows from A(A) = 1 and A(A → B) = 1. Then A(¬D) = 1 follows from A(B) = 1 and A(B → ¬D) = 1. Then A(D) = 0 follows from A(¬D) = 1. But this is a contradiction, because we derived both A(D) = 1 and A(D) = 0. Thus, N must be unsatisfiable. (d) We have to show the following: Claim F ⊨ G ⇔ F → G is a tautology Proof. • ”⇒” Let A be any interpretation suitable for both F and G, thus also suitable for F → G. Case 1 A(F ) = 1. Then, by assumption F ⊨ G we have A(G) = 1. Therefore we have A(¬F ∨ G) = 1, because A(G) = 1. This is exactly A(F → G) = 1. Case 2 A(F ) = 0. Then we have A(¬F ) = 1. Therefore we have A(¬F ∨ G) = 1, because A(¬F ) = 1. This shows again A(F → G) = 1. We have shown for all cases that A(F → G) = 1 for any suitable interpretation A. Therefore, F → G is a tautology. 102 • ”⇐” If F → G is a tautology, then any interpretation A suitable for F and G is a model for F → G, i.e., A(F → G) = 1. Therefore, A(¬F ∨ G) = 1. Let B be an interpretation suitable for both F and G, which is a model for F , i.e, B(F ) = 1. By the assumption the following must also hold for B: B(¬F ∨ G) = 1. Since B(¬F ) = 0, we have that B(¬F ∨ G) is equal to 1 only if B(G) = 1. This shows, that every suitable interpretation for F and G, which is a model for F , is also an model for G, thus F ⊨ G. (e) The statement is indeed false, but the given proof is incorrect. A is not a correct counterexample, because A is not suitable for the right side. The definition of F ⊨ G requires an interpretation which is suitable for both F and G. This will be more important in predicate logic. A correct counterexample would be: A = {A = 1, B = 0, C = 1} (f) (1) CNF (2) CNF (3) DNF (4) Both in CNF and DNF (5) Not in a normal form (g) After determining the function table we can construct the formulas in CNF and DNF. A B C (A → (B ∧ C)) ∧ B 0 0 0 0 0 0 1 0 0 1 0 1 0 1 1 1 1 0 0 0 1 0 1 0 1 1 0 0 1 1 1 1 DNF: (¬A ∧ B ∧ ¬C) ∨ (¬A ∧ B ∧ C) ∨ (A ∧ B ∧ C) CNF: (A ∨ B ∨ C) ∧ (A ∨ B ∨ ¬C) ∧ (¬A ∨ B ∨ C) ∧ (¬A ∨ B ∨ ¬C) ∧ (¬A ∨ ¬B ∨ C) 2.2 The Resolution Calculus (a) We show the following: Claim The resolution calculus is not complete. Proof. To do so, it suffices to provide two formulas F and G such that F ⊨ G but K(F ) ̸⊢Res K(G). For example G := A ∨ B is a logical consequence of F := A which can be easily verified by looking at the truth tables. But there is no way to derive K(A ∨ B) from K(A) because we cannot apply ⊢res. (The only way to pick two clauses on the left side, is to pick K1 = K(A) and K2 = K(A). But those do not provide a resolvent as described in Definition 4.26.) (b) We have to show the following: 103 Claim M ⊨ F ⇔ M ∪ {¬F } is unsatisfiable. Proof. Let M = {G1, . . . , Gk}. M ⊨ F ⇔ (G1 ∧ · · · ∧ Gk) ⊨ F ⇔ (G1 ∧ · · · ∧ Gk) → F is a tautology ⇔ ¬(G1 ∧ · · · ∧ Gk) ∨ F is a tautology ⇔ ¬(¬(G1 ∧ · · · ∧ Gk) ∨ F ) is unsatisfiable ⇔ (G1 ∧ · · · ∧ Gk) ∧ ¬F is unsatisfiable ⇔ M ∪ {¬F } is unsatisfiable An alternative proof is as follows: Proof. • ”⇒” Let A be an arbitrary interpretation suitable for both M and F (and hence also for ¬F ). Case 1 A ⊨ M . Then, by assumption we have A ⊨ F and hence A(¬F ) = 0. But then, we conclude that A ̸⊨ M ∪ {¬F }, because A is not a model for one of these formulas. Case 2 A ̸⊨ M . Then A ̸⊨ M ∪ {¬F } (trivially). Because A was an arbitrary interpretation and A ̸⊨ M ∪{¬F }, the set M ∪{¬F } is unsatisfiable. ✓ • ”⇐” Assume M ∪ {¬F } is unsatisfiable. Let A be an arbitrary interpretation, such that A ⊨ M . Note, that then A(¬F ) = 0, because otherwise we would have A ⊨ M ∪ {¬F } which would contradict our assumption. Hence A(F ) = 1 and therefore M ⊨ F . ✓ (c) Proof. We use the results from the previous subtask: M = {A}, F = A ∨ B and M ∪ {¬F } = {A, ¬A ∧ ¬B}. Remember, that we can always associate a set of formulas with a single formula that is the conjunctions of all formulas in the set: G := A ∧ (¬A ∧ ¬B) According to (b), it suffices to show that G is unsatisfiable. G is already in CNF, therefore the clauses are: K(G) = {{A}, {¬A}, {¬B}} Obviously, the first two clauses provide a resolvent: {{A}, {¬A}} ⊢res ∅ which concludes the proof. Note, that it is not necessary to involve all clauses in a derivation. The clause {B} for example was not used at all. 104 4.6.3 Solutions for Hands-On 3 3.1 Structures (a) U A = N and none of the functions, predicates and variables are defined. (b) U A = Z P A(x, y) is always 0 f A(x, y) = 1337 yA = 42 The left side of the implication is always 0, because P A(x, y) is always 0. Therefore the whole implication is true for all x. (c) U A = Z P A(x, y) = 1 if and only if x ≥ y f A(x, y) = x − y yA = 42 We can see that the left side of the implication is always true, because there always exists a y, such that x ≥ y and y ≥ x. The right side is not true for all x, where x = 0 is such an example. Therefore the whole implication is not true for all x. 3.2 Free Variables In each formula there are 3 quantifier, 3 occurrences of x and 3 occurrences of y. (a) ∀x ↓ 1 ∃y ↓ 1 Q(x ↓ 2 ) ∨ P (f (y ↓ 2)) ∨ ∀y ↓ 3 R(x ↓ 3 ) • Occurrence 1 of directly follows the first ∀ quantifier and is hence bound to it. • Occurrence 2 of is bound to the first ∀ quantifier. • Occurrence 3 of is free. • Occurrence 1 of y directly follows the ∃ quantifier and is hence bound to it. • Occurrence 2 of y is free. • Occurrence 3 of y directly follows the second ∀ quantifier and is hence bound to it. (b) ∀x ↓ 1 (∃y ↓ 1 Q(x ↓ 2 ) ∨ P (f (y ↓ 2))) ∨ ∀y ↓ 3 R(x ↓ 3 ) • Occurrence 1 of x directly follows the first ∀ quantifier and is hence bound to it. • Occurrence 2 of x is bound to the first ∀ quantifier. • Occurrence 3 of x is free. • Occurrence 1 of y directly follows the ∃ quantifier and is hence bound to it. • Occurrence 2 of y is free. • Occurrence 3 of y directly follows the second ∀ quantifier and is hence bound to it. (c) ∀x ↓ 1 ∃y ↓ 1 (Q(x ↓ 2 ) ∨ P (f (y ↓ 2)) ∨ ∀y ↓ 3 R(x ↓ 3 )) • Occurrence 1 of x directly follows the first ∀ quantifier and is hence bound to it. • Occurrence 2 of x is bound to the first ∀ quantifier. • Occurrence 3 of x occurrence of x is bound to the first ∀ quantifier. • Occurrence 1 of y directly follows the ∃ quantifier and is hence bound to it. • Occurrence 2 of y is bound to the ∃ quantifier. • Occurrence 3 of y directly follows the second ∀ quantifier and is hence bound to it. 3.3 Syntax 105 (a) ⊨ is not allowed inside formulas. This is a statement about formulas. (b) Incorrect. (again, ≡ is not allowed inside formulas) (c) Formula. (d) Syntactically incorrect, as we’re not allowed to use logical operators between two terms. (e) Incorrect. This is not even a valid statement about formulas. (f) Statement about formulas. 3.4 Logical Consequences (a) This statement is false. We provide the following counterexample: F ≡ P (x) G ≡ Q(x) U A = N P A(x) is false for all x Q A(x) is true if and only if x > 3 xA = 5 Notice that the variable x in Q(x) occurs free on the left side, but bound on the right side. A is suitable for both sides and a model for the left side, but not a model for the right side. (b) This statement is false. We provide the following counterexample: F ≡ P (x) G ≡ Q(x) U A = N P A(x) is true if and only if x is even Q A(x) is true if and only if x is odd xA = 4 Notice that the variable x in Q(x) occurs free on the left side, but bound on the right side. Because of this, a suitable interpretation A must assign a value to x. A is suitable for both sides and a model for the left side, but not a model for the right side. (c) This statement is false. We provide the following counterexample: F ≡ P (x) G ≡ Q(x) U A = N P A(x) is true for all x Q A(x) is true if and only if x > 3 xA = 5 Notice that the variable x in Q(x) occurs free on the left side, but bound on the right side. A is suitable for both sides and a model for the left side, but not a model for the right side. (d) This statement is correct. 3.5 Prenex Form (a) F ≡ ∀u (P (u) ∨ ∃z Q(f (z))) ∧ ∃y R(g(y, x)) (b) ≡ ∀u(∃z (P (u) ∨ Q(f (z)))) ∧ ∃y R(g(y, x)) (Lm 4.2, Rule 2 and Lm 4.3, Rule 10) ≡ ∀u(∃z (P (u) ∨ Q(f (z))) ∧ ∃y R(g(y, x))) (Lm 4.3, Rule 7) ≡ ∀u(∃z ((P (u) ∨ Q(f (z))) ∧ ∃y R(g(y, x)))) (Lm 4.3, Rule 9) ≡ ∀u(∃z(∃y ((P (u) ∨ Q(f (z))) ∧ R(g(y, x))))) (Lm 4.2, Rule 2 and Lm 4.3, Rule 9) ≡ ∀u∃z∃y ((P (u) ∨ Q(f (z))) ∧ R(g(y, x))) (omit brackets) 3.6 Tautologies 106 (a) (1) This formula is a tautology: Let F := ∀x P (x) ∃x (P (x) → ∀x P (x)) ≡ ∃x (¬P (x) ∨ ∀x P (x)) (Def. of →) ≡ ∃x ¬P (x) ∨ ∀x P (x) (Lm 4.3, Rule 10) ≡ ¬(∀x P (x)) ∨ ∀x P (x) (Lm 4.3, Rule 1) ≡ ¬(F ) ∨ F (Substitution with F ) ≡ F ∨ ¬F (Lm 4.2, Rule 2) ≡ ⊤ (Lm 4.2, Rule 11) (2) This formula is not a tautology. This formula is false under the interpretation A, where U A = N and P A(x) is false for all x. (3) This formula is a tautology: Let G := ¬P (y) ∨ Q(y) (∀x (P (x) → Q(x)) ∧ P (y)) → Q(y) ≡ ¬(∀x (¬P (x) ∨ Q(x)) ∧ P (y)) ∨ Q(y) (Def. of →) ≡ (¬∀x (¬P (x) ∨ Q(x)) ∨ ¬P (y)) ∨ Q(y) (Lm 4.2, Rule 8) ≡ ¬∀x (¬P (x) ∨ Q(x)) ∨ (¬P (y) ∨ Q(y)) (Lm 4.2, Rule 3) ≡ ∃x ¬(¬P (x) ∨ Q(x)) ∨ (¬P (y) ∨ Q(y)) (Lm 4.3, Rule 1) ≡ ∃y ¬(¬P (y) ∨ Q(y)) ∨ (¬P (y) ∨ Q(y)) (Lm 4.5) ≡ ∃y ¬(G) ∨ (G) (Substitution with G) ≡ G ∨ ∃y ¬G (Lm 4.2, Rule 2) ≡ ⊤ (Exercise 13.3 a)) (b) (1) From the previous subtask we know, that ∃x (P (x) → ∀x P (x)) is a tautology. That means, that the formula is true for every suitable structure, in particular for the structure A: U A is the set of all the student who take the exam. P A(x) is true if and only if student x passes the exam. The valid formula under A exactly translates to the sentence: ”There exists a student, such that, if this student passes the exam, then everyone passes the exam.”. (2) From the previous subtask we know, that (∀x (P (x) → Q(x)) ∧ P (y)) → Q(y) is a tautology. That means, that the formula is true for every suitable structure, in particular for the structure A: U A is the set of all stars and planets in the physical universe P A(x) is true if and only if x is a planet Q A(x) is true if and only if x is flat yA is the earth The valid formula under A exactly translates to the sentence: ”If every planet is flat and the earth is a planet, then the earth is also flat” . 3.7 (Challenge) (a) We have to show the following: Claim For any formulas F ,G and H, where x does not occur free in H: (∃x F ) ∨ H ≡ ∃x (F ∨ H). Proof. We have to show that every structure A that is a model for (∃x F ) ∨ H is also a model for ∃x (F ∨ H), and vice versa. 107 Recall that the definition of the semantics of a formula ∃x G for a structure A is that, for some u ∈ U , A[x→u](G) = 1. We first prove the first direction, i.e., (∃x F ) ∨ H ⊨ ∃x (F ∨ H). Suppose that A((∃x F ) ∨ H) = 1 and hence that (i) A(∃x F ) = 1 or that (ii) A(H) = 1. Recall that (i) means that A[x→u](F ) = 1 for some u ∈ U , and (ii) means that A[x→u](H) = 1 for some u ∈ U . (This is because x does not occur free in H and hence for all u ∈ U : A[x→u](H) = A(H)). Therefore A[x→u](F ∨ H) = 1 for some u ∈ U , which means that A(∃x (F ∨ H)) = 1, which was to be proved. We now prove the other direction, i.e., ∃x (F ∨ H) ⊨ (∃x F ) ∨ H. Suppose that A(∃x (F ∨ H)) = 1, i.e., for some u ∈ U , A[x→u](F ∨ H) = 1 and hence that (i) A[x→u](F ) = 1 for some u ∈ U or that (ii) A[x→u](H) = 1 for some u ∈ U . By definition, (i) means that A(∃x F ) = 1. Moreover, because x does not occur free in H, by (ii) we have A[x→u](H) = A(H) = 1 for all u ∈ U . Therefore, A((∃x F ) ∨ H) = 1, which was to be proved. (b) (1) To the definition of syntax we add the following statement: If t1 and t2 are terms, then (t1 = t2) is a formula. To the definition of semantics we add the following: If F is of the form (t1 = t2) for terms t1 and t2, then A(F ) = 1 if and only if A(t1) = A(t2). (2) For the monoid we have to describe associativity and the neutral element. For the group we have to describe the inverse element aswell. (i) ∀a∀b∀c (f (f (a, b), c) = f (a, f (b, c))) ∧ ∃e∀a (f (a, e) = f (e, a) ∧ f (e, a) = a) (ii) ∀a∀b∀c (f (f (a, b), c) = f (a, f (b, c))) ∧ ∃e (∀a (f (a, e) = f (e, a) ∧ f (e, a) = a) ∧ ∀a∃b (f (a, b) = f (b, a) ∧ f (b, a) = e)) 108","libVersion":"0.3.2","langs":""}