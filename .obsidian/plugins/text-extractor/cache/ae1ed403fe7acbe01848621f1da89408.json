{"path":"sem4/W&S/UE/e/W&S-e-u10.pdf","text":"Probability and Statistics (D-INFK) Lecturer: Prof. Dr. Dylan Possamaï Coordinator: Daniel Kršek Probability and Statistics Exercise sheet 10 MC 10.1. Let n ∈ N and let X1, . . . , Xn be i.i.d. and standard normally distributed, i.e., Xi ∼ N (0, 1). Define Y := n∑ i=1 X 2 i . In particular, Y is a χ 2 n-distributed random variable. (Exactly one answer is correct in each question.) 1. What is the value of E[Y ]? (a) E[Y ] = 0. (b) E[Y ] = n2. (c) E[Y ] = n. (d) E[Y ] = √n. 2. What is the value of Var[Y ]? (a) Var[Y ] = n2. (b) Var[Y ] = 2n. (c) Var[Y ] = n. (d) Var[Y ] = 2n2. 3. Let now n = 12. What is the approximation of the probability P [ ∣ ∣ Y n − 1 ∣ ∣ ≤ 0.75] using the CLT? (a) P [ ∣ ∣ Y n − 1 ∣ ∣ ≤ 0.75] ≈ 2Φ ( 3 4 √6 ) − 1. (b) P [ ∣ ∣ Y n − 1 ∣ ∣ ≤ 0.75] ≈ 2Φ ( 7 4 √6 ). (c) P [ ∣ ∣ Y n − 1 ∣ ∣ ≤ 0.75] ≈ Φ (√ 7 4 ). (d) P [ ∣ ∣ Y n − 1 ∣ ∣ ≤ 0.75] ≈ 1 − 2Φ (√6). Exercise 10.2. Let U1, U2, U3 be i.i.d. random variables uniformly distributed on [0, 1]. We consider the random variables L := min{U1, U2, U3} and M := max{U1, U2, U3}. (a) Show that M and L have densities and find them. (b) Show that for ϕ, ψ : R → R piecewise continuous and bounded, the following holds: E[ϕ(M )ψ(L)] = ∫ ∞ −∞ ∫ ∞ −∞ ϕ(m) · ψ(ℓ) · 6(m − ℓ)1{0≤ℓ≤m≤1}dℓdm. (c) Use (b) to determine the joint distribution function and the joint density of (M, L). 1 Probability and Statistics (D-INFK) Lecturer: Prof. Dr. Dylan Possamaï Coordinator: Daniel Kršek Exercise 10.3. Let (Xi)i∈N, (Yi)i∈N, and (Zi)i∈N be sequences of i.i.d. random variables with P[X1 = 1] = P[X1 = −1] = 1/2, and similarly P[Y1 = 1] = P[Y1 = −1] = 1/2 as well as P[Z1 = 1] = P[Z1 = −1] = 1/2, which are also independent of each other. Put differently, the sequence (X1, Y1, Z1, X2, Y2, Z2, X3, Y3, Z3, . . .) is a sequence of i.i.d. random variables. We define the partial sums S(x) n := n∑ i=1 Xi, S(y) n := n∑ i=1 Yi, and S(z) n := n∑ i=1 Zi. The sequence ((S(x) n , S(y) n , S(z) n ) ) n∈N is called a random walk in Z 3. Let α > 1/2. Show that lim n→∞ P [∥(S(x) n , S(y) n , S(z) n )∥ ≤ nα] = 1, where ∥(x, y, z)∥ := √ x2 + y2 + z2 is the Euclidean norm. Hint: First, apply the CLT to show that for all α > 1/2, we have lim n→∞ P [|S(x) n | ≤ nα] = lim n→∞ P [|S(y) n | ≤ nα] = lim n→∞ P [|S(z) n | ≤ nα] = 1. Then, notice that for α′ ∈ (1/2, α) we have: ({|S(x) n | ≤ nα ′} ∩ {|S(y) n | ≤ nα ′} ∩ {|S(z) n | ≤ nα ′}) ⊆ { ∥(S(x) n , S(y) n , S(z) n )∥ ≤ √3nα ′} . Use this to conclude. Exercise 10.4. The median m of a distribution F is defined by m := F −1(1/2) = inf{x ∈ R : F (x) ≥ 1/2}. Let X1, X2, . . . be i.i.d. random variables with distribution function F and median m = 0. Let Zn denote the sample median of X1, . . . , Xn, that is, Zn is the middle observation. More formally Zn = X(k) where k = [ n 2 + 1] and X(1) ≤ · · · ≤ X(n) denote the order statistics of X1, . . . , Xn (i.e. X(1) = min{Xi | i ∈ {1, . . . , n}}, X(n) = max{Xi | i ∈ {1, . . . , n}}, etc.), and [x] denotes the integer part of x. (a) Let Y x i = 1{Xi≤x} and define Sx n := ∑n i=1 Y x i . Compute E[Sx n] and Var[Sx n]. (b) Express the event {Zn ≤ x} using the random variable Sx n. (c) Using the CLT, give an approximation for P[Zn ≤ x] as n → ∞. (d) (*) Find the limit lim n→∞ 1/2 − αn√ 1 n αn(1 − αn) , where αn := F ( x√n ) . 2 Probability and Statistics (D-INFK) Lecturer: Prof. Dr. Dylan Possamaï Coordinator: Daniel Kršek Exercise 10.5. Let X1, . . . , Xn be i.i.d. random variables with Xi ∼ U([θ − 1, θ]) under Pθ, where θ ∈ R is an unknown parameter. We consider the following estimators for θ: T (n) 1 = 1 n n∑ i=1 ( Xi + 1 2 ) and T (n) 2 = max{X1, . . . , Xn}. (a) Determine whether the estimators are unbiased. (b) Compute the variances Varθ[T (n) 1 ] and Varθ[T (n) 2 ]. (c) Compute the mean squared error MSEθ[T (n) i ] := Eθ[(T (n) i − θ) 2], i ∈ {1, 2}. Remark: Here, Eθ and Varθ denote the mean and the variance under probability measure Pθ. Exercise 10.6. We model the water level above the critical flood mark (140 cm above normal) in Lake Zurich. Let X denote the water height (in cm) above the critical mark. We use a generalized Pareto distribution: fX (x; θ) = { 1 θ (1 + x)−(1+ 1 θ ) for x > 0, 0 for x ≤ 0, where θ > 0 is an unknown parameter to be estimated based on observations x1, . . . , xn. These are modeled as realizations of i.i.d. random variables X1, . . . , Xn with density fX (x; θ). We define the estimator by T (n) = 1 n n∑ i=1 log(1 + Xi). (a) Compute the expectation and variance of T (n) under Pθ for each θ > 0. Hint: Define Yi := log(1 + Xi). Then Yi ∼ Exp(1/θ), i.e., the density of Yi is fYi(y) = 1 θ e−y/θ1{y≥0}. (b) Is T (n) an unbiased estimator for θ? (c) Compute the mean squared error MSEθ[T (n)]. (d) Find the maximum likelihood estimator for θ. 3","libVersion":"0.5.0","langs":""}