{"path":"sem3/LinAlg/VRL/extra/plans/LinAlg-plan-w01.pdf","text":"Lecture plan Linear Algebra (401-0131-00L, HS24), ETH Z ¨urich Numbering of Sections, Definitions, Figures, etc. as in the Lecture Notes Week 1 Dot-free notation: Sequence (of vectors): v1, v2, . . . , vn = (v1, v2, . . . , vn) = (vj) n j=1 n j=1: “all j such that 1 ≤ j ≤ n, in increasing order” n = 2 : (v1, v2) n = 1 : (v1) n = 0 : () (empty sequence) Linear combination: λ1v1 + λ2v2 + · · · + λnvn = n∑ j=1 λjvj n = 2 : λ1v1 + λ2v2 n = 1 : λ1v1 n = 0: 0 (without moving, we’re stuck at 0) Set (of vectors): {v1, v2, . . . , vn} = {vj : j ∈ [n]}, [n] = {1, 2, . . . , n} Vectors:      v1 v2 ... vm      = [vi]m i=1 [0] 6 i=1 = 0 ∈ R 6, [i 2] 5 i=1 =       1 4 9 16 25       , [vi]0 i=1 = () ∈ R 0 Scalar products, lengths and angles (Section 1.2) Scalar product: multiply two vectors! [1 2 ] · [ 3 4 ] = 1 · 3 + 2 · 4 = 11. 1 Definition 1.9: Let v =      v1 v2 ... vm      , w =      w1 w2 ... wm      ∈ R m. The scalar product of v and w is the number v · w := v1w1 + v2w2 + · · · + vmwm = m∑ i=1 viwi. Observation 1.10: Let u, v, w ∈ Rm be vectors and λ ∈ R a scalar. Then (i) v · w = w · v (ii) (λv) · w = λ(v · w) = v · (λw) (iii) u · (v + w) = u · v + u · w and (u + v) · w = u · w + v · w (iv) v · v ≥ 0, with equality exactly if v = 0 Euclidean norm: defines length of a vector Definition 1.11: Let v ∈ R m. The Euclidean norm of v is the number ∥v∥ := √ v · v. ∥ ∥ ∥ ∥ ∥ ∥ ∥ ∥ ∥      v1 v2 ... vm      ∥ ∥ ∥ ∥ ∥ ∥ ∥ ∥ ∥ = √ v2 1 + v2 2 + · · · + v2 m = √ √ √ √ m∑ i=1 v2 i ∥ ∥ ∥ ∥ [−4 2 ]∥ ∥ ∥ ∥ = √ (−4)2 + 22 = √20 In R 2: arrow length (Pythagoras!) v = [ v1 v2 ] 0|v2||v1|90 0 √ v2 1 + v2 2 Unit vector: ∥u∥ = 1. 0vu1 v ∥v∥ 2 For v ̸= 0, v ∥v∥ := 1 ∥v∥v is a unit vector. Standard unit vectors: R 3 : e1 =  1 0 0   , e2 =   0 1 0   , e3 =   0 0 1   Rm : ei =        0 ... 1 ... 0        ← coordinate i e1 =   1 0 0  e2 =   0 1 0   xy e3 =   0 0 1   z Cauchy-Schwarz inequality (Proof and application in lecture notes): Lemma 1.12: For any two vectors v, w ∈ R m, |v · w| ≤ ∥v∥∥w∥. Equality holds exactly if one vector is a scalar multiple of the other. Angle between two vectors: vwα Definition 1.14: Let v, w ∈ R m be two nonzero vectors. The angle between them is the unique α between 0 and π (180 degrees) such that cos(α) = v · w ∥v∥∥w∥ ︸ ︷︷ ︸ ↑ , or α = arccos ( v · w ∥v∥∥w∥ ) . between −1 and 1 by Cauchy-Schwarz In R 2: the usual angle Perpendicular vectors: Definition 1.15: Vectors v, w ∈ R n are perpendicular (or orthogonal) if v · w = 0 (same as cos(α) = 0, or 90 degrees). 3 xy [ 4 2 ][ −1 2 ] 900 [4 2 ] · [ −1 2 ] = −4 · 1 + 2 · 2 = 0 Triangle inequality (proof from Cauchy-Schwarz): Lemma 1.16: Let v, w ∈ R m. Then ∥v + w∥ ≤ ∥v∥ + ∥w∥. vwv + w∥w∥∥v∥∥v + w∥0∥w∥∥v∥ In R 2: From 0 directly to v + w is shorter than via v or w. Linear independence (Section 1.3) Linear (in)dependence: Definition 1.18: Vectors v1, v2, . . . , vn are linearly dependent if at least one of them is a linear combination of the others, i.e. there exists an index k ∈ [n] and scalars λj such that vk = n∑ j=1 j̸=k λjvj. Otherwise, v1, v2, . . . , vn are linearly independent. “j < k”: an additional condition on j (all j except k). [2 3 ] , [4 6 ] are linearly dependent: [4 6 ] = 2 [2 3 ] . [2 3 ] , [ 3 −1 ] are linearly independent. xy [ 2 3 ][ 4 6 ] 0xy [ 2 3 ][ 3 −1 ] 0 4 collinear linearly independent Three vectors in R 2 are linearly dependent: either two are collinear, or each is a linear combination of the other two (Challenge 1.6). linearly independent linearly dependent[2 3 ] , [ 3 −1 ] [2 3 ] , [4 6 ] v1, v2, v3 ∈ R 2 v ̸= 0 v = 0 . . . , 0, . . . . . . , v, . . . , v, . . . empty sequence Alternative definitions: Lemma 1.19: Let v1, v2 . . . , vn ∈ R m. The following statements are equivalent (all true, or all false). (i) At least one of the vectors is a linear combination of the other ones (linearly depen- dent by Definition 1.18). (ii) There are scalars λ1, λ2, . . . , λn besides 0, 0, . . . , 0 such that ∑n j=1 λjvj = 0. Math jar- gon: 0 is a nontrivial linear combination of the vectors. (iii) At least one of the vectors is a linear combination of the previous ones. Proof idea: (i) implies (ii): if (i) is true, then also (ii) is true. (i)⇒(ii) (ii) implies (iii). (ii)⇒(iii) (iii) implies (i). (iii)⇒(i) Each statement implies the other ones! (i)⇔(ii)⇔(iii) Math prose for (i)⇔(ii): (i) if and only if (ii) Proof. (i) ⇒(ii): Let vk = n∑ j=1 j̸=k λjvj. Define λk = −1. We get (ii): 0 = n∑ j=1 λjvj. 5 (ii)⇒(iii): Let k be the largest index such that λk ̸= 0. Then 0 = k∑ j=1 λjvj and we get (iii): vk = k−1∑ j=1 (− λj λk ) vj. (iii)⇒(i): a linear combination of the previous ones is also a linear combination of the other ones. For linear independence, simply take the opposite statements. Corollary 1.20: Let v1, v2 . . . , vn ∈ R m. The following statements are equivalent (all true, or all false). (i) None of the vectors is a linear combination of the other ones (linearly independent by Definition 1.18.) (ii) There are no scalars λ1, λ2, . . . , λn besides 0, 0, . . . , 0 such that ∑n j=1 λjvj = 0. Math jargon: 0 can only be written as a trivial linear combination of the vectors. (iii) None of the vectors is a linear combination of the previous ones. Uniqueness of linear combination: Lemma 1.21: Let v1, v2 . . . , vn ∈ R m be linearly independent, and let w = ∑n j=1 λjvj =∑n j=1 µjvj be two ways of writing w as a linear combination. Then λj = µj for all j ∈ [n]. Proof. Subtraction: 0 = n∑ j=1 (λj − µj)vj. Since 0 can only be written as a trivial linear combination, we get λj − µj = 0 for all j. Span of vectors: set of all linear combinations Definition 1.22: Let v1, v2, . . . , vn ∈ R m. Their span is Span(v1, v2, . . . , vn) := { n∑ j=1 λjvj : λj ∈ R for all j ∈ [n] } . Span of three vectors in R 3: 6 xyz   −2 2 2     −1 1 1     −3 3 3   xyz   −2 2 0     −1 1 3     −3 3 3   xyz   −2 2 0     −1 1 3     2 3 −1   a line a plane the whole space . . . or a point (if all vectors are 0) Always: 0 ∈ Span(. . .) Fact 1.5: Span ([ 2 3 ] , [ 3 −1 ]) = R 2. xy [ 2 3 ][ 4 6 ] 0xy [ 2 3 ][ 3 −1 ] 0 collinear linearly independent Lemma 1.23: Let v1, v2 . . . , vn ∈ R m, and let v ∈ R m be a linear combination of v1, v2 . . . , vn. Then Span(v1, v2, . . . , vn) ︸ ︷︷ ︸ S = Span(v1, v2, . . . , vn, v) ︸ ︷︷ ︸ T . Proof idea: Each element of S is contained in T (S is subset of T ). S ⊆ T T is subset of S. T ⊆ S The two sets are equal! S = T 7 Proof. S ⊆ T : Each w ∈ S is a linear combination of v1, v2, . . . , vn and therefore of v1, v2, . . . , vn, v (add scalar multiple 0v). So w ∈ T . T ⊆ S: each w ∈ T is a linear combination of v1, v2, . . . , vn, v, w = n∑ j=1 λjvj + λv. We know: v is a linear combination of v1, v2 . . . , vn, v = n∑ j=1 µjvj. Together: w = n∑ j=1 λjvj + λv = n∑ j=1 λjvj + λ ( n∑ j=1 µjvj ) = n∑ j=1 (λj + λµj)vj. So w is a linear combination of v1, v2, . . . , vn, w ∈ S. 8","libVersion":"0.3.2","langs":""}