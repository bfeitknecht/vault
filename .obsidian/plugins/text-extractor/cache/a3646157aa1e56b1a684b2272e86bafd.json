{"path":"sem2/DDCA/VRL/slides/DDCA-L24-chaches.pdf","text":"Digital Design & Computer Arch. Lecture 24: Memory Hierarchy and Caches Frank K. Gürkaynak Mohammad Sadrosadati Prof. Onur Mutlu ETH Zürich Spring 2024 24 May 2024 The Memory Hierarchy Memory Hierarchy in a Modern System (I) 3 CORE 1L2 CACHE 0SHARED L3 CACHEDRAM INTERFACE CORE 0 CORE 2 CORE 3L2 CACHE 1L2 CACHE 2L2 CACHE 3DRAM BANKS DRAM MEMORY CONTROLLER AMD Barcelona, circa 2006 Memory Hierarchy in a Modern System (II) 4Source: https://www.anandtech.com/show/16252/mac-mini-apple-m1-tested Apple M1, 2021 Memory Hierarchy in a Modern System (III) 5https://www.gsmarena.com/apple_announces_m1_ult ra_with_20core_cpu_and_64c ore_gpu-news -53481.php Apple M1 Ultra System (2022) DRAM DRAM A lot of SRAMStorage Storage Memory Hierarchy in an Older System 6 By Moshen - http://en.wikipedia.org/wiki/Image:P entiumpro_moshen.jpg, CC BY-SA 2.5, https://commons.wikimedia.org/w/index.php?curid=2262471 Processor chip Level 2 cache chip Multi-chip module package Intel Pentium Pro, 1995 Memory Hierarchy in an Older System 7 https://download.intel.com/newsroom/kits/40thanniversary/gallery/images/Pentium_4_6xx-die.jpg L2 Cache Intel Pentium 4, 2000 Memory Hierarchy in a Modern System (IV) 8https://wccftech.com/amd-ryzen-5000-zen-3-vermeer-undressed-high-res-die-shots-close-ups-pictured-detailed/ AMD Ryzen 5000, 2020 Core Count: 8 cores/16 threads L1 Caches: 32 KB per core L2 Caches: 512 KB per core L3 Cache: 32 MB shared Memory Hierarchy in a Modern System (V) 9https://www.it-techblog.de/ibm-power10-prozessor-mehr-speicher-mehr-tempo-mehr-sicherheit/09/2020/ IBM POWER10, 2020 Cores: 15-16 cores, 8 threads/core L2 Caches: 2 MB per core L3 Cache: 120 MB shared Memory Hierarchy in a Modern System (VI) 10https://www.tomshardware.com/news/infrared-photographer-photos-nvidia-ga102-ampere-silicon Nvidia Ampere, 2020 Cores: 128 Streaming Multiprocessors L1 Cache or Scratchpad: 192KB per SM Can be used as L1 Cache and/or Scratchpad L2 Cache: 40 MB shared Ideal Memory  Zero access time (latency)  Infinite capacity  Zero cost  Infinite bandwidth (to support multiple accesses in parallel)  Zero energy 11 The Problem  Ideal memory’s requirements oppose each other  Bigger is slower  Bigger  Takes longer to determine the location  Faster is more expensive  Memory technology: SRAM vs. DRAM vs. SSD vs. Disk vs. Tape  Higher bandwidth is more expensive  Need more banks, more ports, more channels, higher frequency or faster technology 12 The Problem  Bigger is slower  SRAM, < 1KByte, sub-nanosec  SRAM, KByte~MByte, ~nanosec  DRAM, Gigabyte, ~50 nanosec  PCM-DIMM (Intel Optane DC DIMM), Gigabyte, ~300 nanosec  PCM-SSD (Intel Optane SSD), Gigabyte ~Terabyte, ~6-10 µs  Flash memory, Gigabyte~Terabyte, ~50-100 µs  Hard Disk, Terabyte, ~10 millisec  Faster is more expensive (monetary cost and chip area)  SRAM, < 0.3$ per Megabyte  DRAM, < 0.006$ per Megabyte  PCM-DIMM (Intel Optane DC DIMM), < 0.004$ per Megabyte  PCM-SSD, < 0.002$ per Megabyte  Flash memory, < 0.00008$ per Megabyte  Hard Disk, < 0.00003$ per Megabyte  These sample values (circa ~2023) scale with time  Other technologies have their place as well  FeRAM, MRAM, RRAM, STT-MRAM, memristors, … (not mature yet) 13 The Problem (Table View) 14 Memory Device Capacity Latency Cost per Megabyte SRAM < 1 KByte sub-nanosec SRAM KByte~MByte ~nanosec < 0.3$ DRAM Gigabyte ~50 nanosec < 0.006$ PCM-DIMM (Intel Optane DC DIMM) Gigabyte ~300 nanosec < 0.004$ PCM-SSD (Intel Optane SSD) Gigabyte ~Terabyte ~6-10 µs < 0.002$ Flash memory Gigabyte ~Terabyte ~50-100 µs < 0.00008$ Hard Disk Terabyte ~10 millisec < 0.00003$ These sample values (circa ~2023) scale with time Bigger is slower Faster is more expensive ($$$ and chip area) The Problem (Table View): Energy Memory Device Capacity Latency Cost per Megabyte Energy per access Energy per byte access SRAM < 1 KByte sub-nanosec ~5 pJ ~1.25 pJ SRAM KByte~MB yte ~nanosec < 0.3$ DRAM Gigabyte ~50 nanosec < 0.006$ ~40-140 pJ ~10-35 pJ PCM-DIMM (Intel Optane DC DIMM) Gigabyte ~300 nanosec < 0.004$ ~80-540 pJ ~20-135 pJ PCM-SSD (Intel Optane SSD) Gigabyte ~Terabyte ~6-10 µs < 0.002$ ~120 µJ ~30 nJ Flash memory Gigabyte ~Terabyte ~50-100 µs < 0.00008$ ~250 µJ ~61 nJ Hard Disk Terabyte ~10 millisec < 0.00003$ ~60 mJ ~15 µJ These sample values (circa ~2023) scale with time Bigger is slower Faster is more expensive ($$$ and chip area) Faster is more energy-efficient Disclaimer: Take the energy values with a grain of salt as there are different assumptions Aside: The Problem (2011 Version)  Bigger is slower  SRAM, 512 Bytes, sub-nanosec  SRAM, KByte~MByte, ~nanosec  DRAM, Gigabyte, ~50 nanosec  Hard Disk, Terabyte, ~10 millisec  Faster is more expensive (monetary cost and chip area)  SRAM, < 10$ per Megabyte  DRAM, < 1$ per Megabyte  Hard Disk < 1$ per Gigabyte  These sample values (circa ~2011) scale with time  Other technologies have their place as well  Flash memory (mature), PC-RAM, MRAM, RRAM (not mature yet) 16 Why Memory Hierarchy?  We want both fast and large  But, we cannot achieve both with a single level of memory  Idea: Have multiple levels of storage (progressively bigger and slower as the levels are farther from the processor) and ensure most of the data the processor needs is kept in the fast(er) level(s) 17 The Memory Hierarchy 18 fast small large but slow move what you use here back up everything here With good locality of reference, memory appears as fast as and as large asfaster per bytecheaper per byte Memory Hierarchy  Fundamental tradeoff  Fast memory: small  Large memory: slow  Idea: Memory hierarchy  Latency, cost, size, bandwidth 19 CPU Main Memory (DRAM)RF Cache Hard Disk Memory Hierarchy Example 20 Kim & Mutlu, “Memory Systems,” Computing Handbook, 2014 https://people.inf.ethz.ch/omutlu/pub/memory-systems-introduction_computing-handbook14.pdf Locality  One’s recent past is a very good predictor of their near future  Temporal Locality: If you just did something, it is very likely that you will do the same thing again soon  since you are here today, there is a good chance you will be here again and again regularly  Spatial Locality: If you did something, it is very likely you will do something similar/related (in space)  every time I find you in this room, you are probably sitting close to the same people AND/OR in closeby seats 21 Memory Locality  A “typical” program has a lot of locality in memory references  typical programs are composed of “loops”  Temporal: A program tends to reference the same memory location many times and all within a small window of time  Spatial: A program tends to reference nearby memory locations within a window of time  most notable examples: 1. instruction memory references  mostly sequential/streaming 2. references to arrays/vectors  often streaming/strided 22 Caching Basics: Exploit Temporal Locality  Idea: Store recently accessed data in automatically-managed fast memory (called cache)  Anticipation: same mem. location will be accessed again soon  Temporal locality principle  Recently accessed data will be again accessed in the near future  This is what Maurice Wilkes had in mind:  Wilkes, “Slave Memories and Dynamic Storage Allocation,” IEEE Trans. On Electronic Computers, 1965.  “The use is discussed of a fast core memory of, say 32000 words as a slave to a slower core memory of, say, one million words in such a way that in practical cases the effective access time is nearer that of the fast memory than that of the slow memory.” 23 Caching Basics: Exploit Spatial Locality  Idea: Store data in addresses adjacent to the recently accessed one in automatically-managed fast memory  Logically divide memory into equal-size blocks  Fetch to cache the accessed block in its entirety  Anticipation: nearby memory locations will be accessed soon  Spatial locality principle  Nearby data in memory will be accessed in the near future  E.g., sequential instruction access, array traversal  This is what IBM 360/85 implemented  16 Kbyte cache with 64 byte blocks  Liptay, “Structural aspects of the System/360 Model 85 II: the cache,” IBM Systems Journal, 1968. 24 The Bookshelf Analogy  Book in your hand  Desk  Bookshelf  Boxes at home  Boxes in storage  Recently-used books tend to stay on desk  Comp Arch books, books for classes you are currently taking  Until the desk gets full  Adjacent books in the shelf needed around the same time  If I have organized/categorized my books well in the shelf 25 Caching in a Pipelined Design  The cache needs to be tightly integrated into the pipeline  Ideally, access in 1-cycle so that load-dependent operations do not stall  High frequency pipeline  Cannot make the cache large  But, we want a large cache AND a pipelined design  Idea: Cache hierarchy 26 CPU Main Memory (DRAM) RF Level1 Cache Level 2 Cache A Note on Manual vs. Automatic Management  Manual: Programmer manages data movement across levels -- too painful for programmers on substantial programs  “core” vs “drum” memory in the 1950s  done in embedded processors (on-chip scratchpad SRAM in lieu of a cache), GPUs (called “shared memory”), ML accelerators, …  Automatic: Hardware manages data movement across levels, transparently to the programmer ++ programmer’s life is easier  the average programmer doesn’t need to know about caches  You don’t need to know how big the cache is and how it works to write a “correct” program! (What if you want a “fast” program?) 27 Caches and Scratchpad in a Modern GPU 28https://www.tomshardware.com/news/infrared-photographer-photos-nvidia-ga102-ampere-silicon Nvidia Ampere, 2020 Cores: 128 Streaming Multiprocessors L1 Cache or Scratchpad: 192KB per SM Can be used as L1 Cache and/or Scratchpad L2 Cache: 40 MB shared Caches and Scratchpad in a Modern GPU 29https://wccftech.com/nvidia-hopper-gpus-featuring-mcm-technology-tape-out-soon-rumor/ Nvidia Hopper, 2022 L1 Cache or Scratchpad: 256KB per SM Can be used as L1 Cache and/or Scratchpad Cores: 144 Streaming Multiprocessors L2 Cache: 60 MB shared Caches and Scratchpad in a Modern GPU 30https://developer.nvidia.com/blog/nvidia-hopper-architecture-in-depth/ Nvidia Hopper, 2022 L1 Cache or Scratchpad: 256KB per SM Can be used as L1 Cache and/or Scratchpad Cores: 144 Streaming Multiprocessors L2 Cache: 60 MB shared Cerebras’s Wafer Scale Engine (2019) 31 Cerebras WSE 1.2 Trillion transistors 46,225 mm2 Largest GPU 21.1 Billion transistors 815 mm2  The largest ML accelerator chip  400,000 cores  18 GB of on-chip memory  9 PB/s memory bandwidth NVIDIA TITAN V https://www.anandtech.com/show/14758/hot-chips-31-live-blogs-cerebras-wafer-scale-deep-learning https://www.cerebras.net/cerebras-wafer-scale-engine-why-we-need-big-chips-for-deep-learning/ Scratchpad Memory in Cerebras WSE  Scratchpad Memory  Highly parallel and distributed scratchpad SRAM memory with 2D mesh interconnection fabric across tiles  16-byte read and 8-byte write single-cycle latency  48 KB scratchpad in each tile, totaling 18 GB on the full chip  No shared memory 32Rocki et al., “Fast stencil-code computation on a wafer-scale processor.” SC 2020. 84 dies4539 tiles Cerebras’s Wafer Scale Engine-2 (2021) 33 Cerebras WSE-2 2.6 Trillion transistors 46,225 mm2 Largest GPU 54.2 Billion transistors 826 mm2 NVIDIA Ampere GA100 https://cerebras.net/product/#overview  The largest ML accelerator chip  850,000 cores  40 GB of on-chip memory  20 PB/s memory bandwidth A Historical Perspective 34 Public Domain, https://commons.wikimedia.org/w/index.php?curid=239809 Magnetic Drum Memory Main Memory of 1950s-1960s Magnetic Core Memory Main Memory of 1960s-1970s By Orion 8 - Combined from Magnetic core memory card.jpg and Magnetic core.jpg., CC BY 2.5, https://commons.wikimedia.org/w/index.php?curid=11235412 A Historical Perspective 35 Public Domain, https://commons.wikimedia.org/w/index.php?curid=239809 Magnetic Drum Memory Main Memory of 1950s-1960s Magnetic Core Memory Main Memory of 1960s-1970s By Orion 8 - Combined from Magnetic core memory card.jpg and Magnetic core.jpg., CC BY 2.5, https://commons.wikimedia.org/w/index.php?curid=11235412 Automatic Management in Memory Hierarchy  Wilkes, “Slave Memories and Dynamic Storage Allocation,” IEEE Trans. On Electronic Computers, 1965.  “By a slave memory I mean one which automatically accumulates to itself words that come from a slower main memory, and keeps them available for subsequent use without it being necessary for the penalty of main memory access to be incurred again.” 36 Historical Aside: Other Cache Papers  Fotheringham, “Dynamic Storage Allocation in the Atlas Computer, Including an Automatic Use of a Backing Store,” CACM 1961.  http://dl.acm.org/citation.cfm?id=366800  Bloom, Cohen, Porter, “Considerations in the Design of a Computer with High Logic-to-Memory Speed Ratio,” AIEE Gigacycle Computing Systems Winter Meeting, Jan. 1962. 37 Cache in 1962 (Bloom, Cohen, Porter) 38 Tag Store Data Store A Modern Memory Hierarchy 39 Register File 32 words, sub-nsec L1 cache ~10s of KB, ~nsec L2 cache 100s of KB ~ few MB, many nsec L3 cache, many MBs, even more nsec Main memory (DRAM), Many GBs, ~100 nsec Swap Disk ~100 GB or few TB, ~10s of usec-msec manual/compiler register spilling automatic demand paging automatic HW cache management Memory Abstraction Hierarchical Latency Analysis  A given memory hierarchy level i has intrinsic access time of ti  It also has perceived access time Ti that is longer than ti  Except for the outer-most hierarchy level, when looking for a given address there is  a chance (hit-rate hi) you “hit” and access time is ti  a chance (miss-rate mi) you “miss” and access time ti +Ti+1  hi + mi = 1  Thus Ti = hi·ti + mi·(ti + Ti+1) Ti = ti + mi ·Ti+1 hi and mi are defined to be the hit-rate and miss-rate of only the references that missed at Li-1 40 Hierarchy Design Considerations  Recursive latency equation Ti = ti + mi ·Ti+1  The goal: achieve desired T1 within allowed cost  Ti  ti is desirable  Keep mi low  increasing capacity Ci lowers mi, but beware of increasing ti  lower mi by smarter cache management (replacement::anticipate what you don’t need, prefetching::anticipate what you will need)  Keep Ti+1 low  faster outer hierarchy levels can help, but beware of increasing cost  introduce intermediate hierarchy levels as a compromise 41 Intel Pentium 4 Example Boggs et al., “The Microarchitecture of the Pentium 4 Processor,” Intel Technology Journal, 2004. Intel Pentium 4 Example 43 https://download.intel.com/newsroom/kits/40thanniversary/gallery/images/Pentium_4_6xx-die.jpg L2 Cache  90nm P4, 3.6 GHz  L1 D-cache  C1 = 16 kB  t1 = 4 cyc int / 9 cycle fp  L2 D-cache  C2 = 1024 kB  t2 = 18 cyc int / 18 cyc fp  Main memory  t3 = ~ 50ns or 180 cyc  Notice  best case latency is not 1  worst case access latencies are into 500+ cycles if m1=0.1, m2=0.1 T1=7.6, T2=36 if m1=0.01, m2=0.01 T1=4.2, T2=19.8 if m1=0.05, m2=0.01 T1=5.00, T2=19.8 if m1=0.01, m2=0.50 T1=5.08, T2=108 Intel Pentium 4 Example Ti = ti + mi ·Ti+1 Cache Basics and Operation Cache  Any structure that “memoizes” used (or produced) data  to avoid repeating the long-latency operations required to reproduce/fetch the data from scratch  e.g., a web cache  Most commonly in the processor design context: an automatically-managed memory structure  e.g., memoize in fast SRAM the most frequently or recently accessed DRAM memory locations to avoid repeatedly paying for the DRAM access latency 46 Conceptual Picture of a Cache 47 Kim & Mutlu, “Memory Systems,” Computing Handbook, 2014 https://people.inf.ethz.ch/omutlu/pub/memory-systems-introduction_computing-handbook14.pdf Metadata Logical Organization of a Cache (I)  A key question: How to map chunks of the main memory address space to blocks in the cache?  Which location in cache can a given “main memory chunk” be placed in? 48 Logical Organization of a Cache (II)  A key question: How to map chunks of the main memory address space to blocks in the cache?  Which location in cache can a given “main memory chunk” be placed in? 49Kim & Mutlu, “Memory Systems,” Computing Handbook, 2014 Caching Basics  Block (line): Unit of storage in the cache  Memory is logically divided into blocks that map to potential locations in the cache  On a reference:  HIT: If in cache, use cached data instead of accessing memory  MISS: If not in cache, bring block into cache  May have to evict some other block  Some important cache design decisions  Placement: where and how to place/find a block in cache?  Replacement: what data to remove to make room in cache?  Granularity of management: large or small blocks? Subblocks?  Write policy: what do we do about writes?  Instructions/data: do we treat them separately? 50 Cache Abstraction and Metrics  Cache hit rate = (# hits) / (# hits + # misses) = (# hits) / (# accesses)  Average memory access time (AMAT) = ( hit-rate * hit-latency ) + ( miss-rate * miss-latency )  Important Aside: Is reducing AMAT always beneficial for performance? 51 Address Tag Store (is the address in the cache? + bookkeeping) Data Store (stores memory blocks) Hit/miss? Data A Basic Hardware Cache Design  We will start with a basic hardware cache design  Then, we will examine a multitude of ideas to make it better (i.e., higher performance) 52 Blocks and Addressing the Cache  Main memory logically divided into fixed-size chunks (blocks)  Cache can house only a limited number of blocks 53 Blocks and Addressing the Cache  Main memory logically divided into fixed-size chunks (blocks)  Cache can house only a limited number of blocks  Each block address maps to a potential location in the cache, determined by the index bits in the address  used to index into the tag and data stores  Cache access: 1) index into the tag and data stores with index bits in address 2) check valid bit in tag store 3) compare tag bits in address with the stored tag in tag store  If the stored tag is valid and matches the tag of the block, then the block is in the cache (cache hit) 54 8-bit address tag index byte in block 3 bits3 bits2b Let’s See A Toy Example  We will examine a direct-mapped cache first  Direct-mapped: A given main memory block can be placed in only one possible location in the cache  Toy example: 256-byte memory, 64-byte cache, 8-byte blocks 55Kim & Mutlu, “Memory Systems,” Computing Handbook, 2014 Direct-Mapped Cache: Placement and Access  Assume byte-addressable main memory: 256 bytes, 8-byte blocks  32 blocks in mem  Assume cache: 64 bytes, 8 blocks  Direct-mapped: A block can go to only one location  Blocks with same index contend for the same cache location  Cause conflict misses when accessed consecutively 56 Tag store Data store Address tag index byte in block 3 bits3 bits2b V tag =? MUX byte in block Hit? Data Block: 00000 Block: 00001 Block: 00010 Block: 00011 Block: 00100 Block: 00101 Block: 00110 Block: 00111 Block: 01000 Block: 01001 Block: 01010 Block: 01011 Block: 01100 Block: 01101 Block: 01110 Block: 01111 Block: 10000 Block: 10001 Block: 10010 Block: 10011 Block: 10100 Block: 10101 Block: 10110 Block: 10111 Block: 11000 Block: 11001 Block: 11010 Block: 11011 Block: 11100 Block: 11101 Block: 11110 Block: 11111 Main memory Direct-Mapped Caches  Direct-mapped cache: Two blocks in memory that map to the same index in the cache cannot be present in the cache at the same time  One index  one entry  Can lead to 0% hit rate if more than one block accessed in an interleaved manner map to the same index  Assume addresses A and B have the same index bits but different tag bits  A, B, A, B, A, B, A, B, …  conflict in the cache index  All accesses are conflict misses 57  Problem: Addresses N and N+8 always conflict in direct mapped cache  Idea: enable blocks with the same index to map to > 1 cache location  Example: Instead of having one column of 8, have 2 columns of 4 blocks Set Associativity Tag store Data store V tag =? V tag =? Address tag index byte in block 3 bits2 bits3 bits Logic MUX MUX byte in block Key idea: Associative memory within the set + Accommodates conflicts better (fewer conflict misses) -- More complex, slower access, larger tag store SET Hit? 2-way set associative cache: Blocks with the same index can map to 2 locations Higher Associativity  4-way + Likelihood of conflict misses even lower -- More tag comparators and wider data mux; larger tag store Tag store Data store =?=? =?=? MUX MUX byte in block Logic Hit? Address tag index byte in block 3 bits1 b4 bits 4-way set associative cache: Blocks with the same index can map to 4 locations Full Associativity  Fully associative cache  A block can be placed in any cache location Tag store Data store =? =? =? =? =? =? =? =? MUX MUX byte in block Logic Hit? Address tag byte in block 3 bits5 bits Fully associative cache: Any block can map to any location in the cache Associativity (and Tradeoffs)  Degree of associativity: How many blocks can map to the same index (or set)?  Higher associativity ++ Higher hit rate -- Slower cache access time (hit latency and data access latency) -- More expensive hardware (more comparators)  Diminishing returns from higher associativity 61 associativity hit rate Issues in Set-Associative Caches  Think of each block in a set having a “priority”  Indicating how important it is to keep the block in the cache  Key issue: How do you determine/adjust block priorities?  There are three key decisions in a set:  Insertion, promotion, eviction (replacement)  Insertion: What happens to priorities on a cache fill?  Where to insert the incoming block; whether or not to insert the block  Promotion: What happens to priorities on a cache hit?  Whether and how to change block priority  Eviction/replacement: What happens to priorities on a cache miss?  Which block to evict and how to adjust priorities 62 Eviction/Replacement Policy  Which block in the set to replace on a cache miss?  Any invalid block first  If all are valid, consult the replacement policy  Random  FIFO  Least recently used (how to implement?)  Not most recently used  Least frequently used?  Least costly to re-fetch?  Why would memory accesses have different cost?  Hybrid replacement policies  Optimal replacement policy? 63 Implementing LRU  Idea: Evict the least recently accessed block  Problem: Need to keep track of access order of blocks  Question: 2-way set associative cache:  What do you minimally need to implement LRU perfectly?  Question: 4-way set associative cache:  What do you minimally need to implement LRU perfectly?  How many different access orders are possible for the 4 blocks in the set?  How many bits needed to encode the LRU order of a block?  What is the logic needed to determine the LRU victim?  Repeat for N-way set associative cache 64 Approximations of LRU  Most modern processors do not implement “true LRU” (also called “perfect LRU”) in highly-associative caches  Why?  True LRU is complex  LRU is an approximation to predict locality anyway (i.e., not the best possible cache management policy)  Examples:  Not MRU (not most recently used)  Hierarchical LRU: divide the N-way set into M “groups”, track the MRU group and the MRU way in each group  Victim-NextVictim Replacement: Only keep track of the victim and the next victim 65 Cache Replacement Policy: LRU or Random  LRU vs. Random: Which one is better?  Example: 4-way cache, cyclic references to A, B, C, D, E  0% hit rate with LRU policy  Set thrashing: When the “program working set” in a set is larger than set associativity  Random replacement policy is better when thrashing occurs  In practice:  Performance of replacement policy depends on workload  Average hit rate of LRU and Random are similar  Best of both Worlds: Hybrid of LRU and Random  How to choose between the two? Set sampling  See Qureshi et al., ”A Case for MLP-Aware Cache Replacement,” ISCA 2006. 66 What Is the Optimal Replacement Policy?  Belady’s OPT  Replace the block that is going to be referenced furthest in the future by the program  Belady, “A study of replacement algorithms for a virtual-storage computer,” IBM Systems Journal, 1966.  How do we implement this? Simulate?  Is this optimal for minimizing miss rate?  Is this optimal for minimizing execution time?  No. Cache miss latency/cost varies from block to block!  Two reasons: Where miss is serviced from and miss overlapping  Qureshi et al. “A Case for MLP-Aware Cache Replacement,\" ISCA 2006. 67 Recommended Reading  Key observation: Some misses more costly than others as their latency is exposed as stall time. Reducing miss rate is not always good for performance. Cache replacement should take into account cost of misses.  Moinuddin K. Qureshi, Daniel N. Lynch, Onur Mutlu, and Yale N. Patt, \"A Case for MLP-Aware Cache Replacement\" Proceedings of the 33rd International Symposium on Computer Architecture (ISCA), pages 167-177, Boston, MA, June 2006. Slides (ppt) 68 What’s In A Tag Store Entry?  Valid bit  Tag  Replacement policy bits  Dirty bit?  Write back vs. write through caches 69 Handling Writes (I)  When do we write the modified data in a cache to the next level?  Write through: At the time the write happens  Write back: When the block is evicted  Write-back cache + Can combine multiple writes to the same block before eviction  Potentially saves bandwidth between cache levels + saves energy -- Need a bit in the tag store indicating the block is “dirty/modified”  Write-through cache + Simpler design + All levels are up to date & consistent  Simpler cache coherence: no need to check close-to-processor caches’ tag stores for presence -- More bandwidth intensive; no combining of writes 70 Handling Writes (II)  Do we allocate a cache block on a write miss?  Allocate on write miss: Yes  No-allocate on write miss: No  Allocate on write miss + Can combine writes instead of writing each individually to next level + Simpler because write misses can be treated the same way as read misses -- Requires transfer of the whole cache block  No-allocate + Conserves cache space if locality of written blocks is low (potentially better cache hit rate) 71 Handling Writes (III)  What if the processor writes to an entire block over a small amount of time?  Is there any need to bring the block into the cache from memory in the first place?  Why do we not simply write to only a portion of the block, i.e., subblock?  E.g., 4 bytes out of 64 bytes  Problem: Valid and dirty bits are associated with the entire 64 bytes, not with each individual 4 bytes 72 Subblocked (Sectored) Caches  Idea: Divide a block into subblocks (or sectors)  Have separate valid and dirty bits for each subblock (sector)  Allocate only a subblock (or a subset of subblocks) on a request ++ No need to transfer the entire cache block into the cache (A write simply validates and updates a subblock) ++ More freedom in transferring subblocks into the cache (a cache block does not need to be in the cache fully) (How many subblocks do you transfer on a read?) -- More complex design; more valid and dirty bits -- May not exploit spatial locality fully 73 tagsubblockvsubblockv subblockvd d d Instruction vs. Data Caches  Separate or Unified?  Pros and Cons of Unified: + Dynamic sharing of cache space  better overall cache utilization: no overprovisioning that might happen with static partitioning of cache space (i.e., separate I and D caches) -- Instructions and data can evict/thrash each other (i.e., no guaranteed space for either) -- I and D are accessed in different places in the pipeline. Where do we place the unified cache for fast access?  First level caches are almost always split  Mainly for the last reason above – pipeline constraints  Outer level caches are almost always unified 74 Multi-level Cache Design & Management  Cache level greatly affects cache design & management  First-level caches (instruction and data)  Decisions very much affected by cycle time & pipeline structure  Small, lower associativity; latency is critical  Tag store and data store are usually accessed in parallel  Second-level caches  Decisions need to balance hit rate and access latency  Usually large and highly associative; latency not as important  Tag store and data store can be accessed serially  Further-level (larger) caches  Access energy is a larger problem due to cache sizes  Tag store and data store are usually accessed serially 75 Serial vs. Parallel Access of Cache Levels  Parallel: Next level cache accessed in parallel with the previous level  a form of speculative access + Faster access to data if previous level misses -- Unnecessary accesses to next level if previous level hits  Serial: Next level cache accessed only if previous-level misses -- Slower access to data if previous level misses + No wasted accesses to next level if previous level hits  Next level does not see the same accesses as the previous  Previous level acts as a filter (filters some temporal & spatial locality)  Management policies are different across cache levels 76 Deeper and Larger Cache Hierarchies 77Source: https://www.anandtech.com/show/16252/mac-mini-apple-m1-tested Apple M1, 2021 Deeper and Larger Cache Hierarchies 78Source: https://twitter.com/Locuza_/status/1454152714930331652 Intel Alder Lake, 2021 Deeper and Larger Cache Hierarchies 79https://wccftech.com/amd-ryzen-5000-zen-3-vermeer-undressed-high-res-die-shots-close-ups-pictured-detailed/ AMD Ryzen 5000, 2020 Core Count: 8 cores/16 threads L1 Caches: 32 KB per core L2 Caches: 512 KB per core L3 Cache: 32 MB shared AMD’s 3D Last Level Cache (2021) 80https://youtu.be/gqAYMx34euU https://www.tech-critter.com/amd-keynote-computex-2021/ https://community.microcenter.com/discussion/5 134/comparing-zen-3-to-zen-2 Additional 64 MB L3 cache die stacked on top of the processor die - Connected using Through Silicon Vias (TSVs) - Total of 96 MB L3 cache AMD increases the L3 size of their 8-core Zen 3 processors from 32 MB to 96 MB Deeper and Larger Cache Hierarchies 81https://www.it-techblog.de/ibm-power10-prozessor-mehr-speicher-mehr-tempo-mehr-sicherheit/09/2020/ IBM POWER10, 2020 Cores: 15-16 cores, 8 threads/core L2 Caches: 2 MB per core L3 Cache: 120 MB shared Deeper and Larger Cache Hierarchies 82https://www.tomshardware.com/news/infrared-photographer-photos-nvidia-ga102-ampere-silicon Nvidia Ampere, 2020 Cores: 128 Streaming Multiprocessors L1 Cache or Scratchpad: 192KB per SM Can be used as L1 Cache and/or Scratchpad L2 Cache: 40 MB shared Deeper and Larger Cache Hierarchies 83https://wccftech.com/nvidia-hopper-gpus-featuring-mcm-technology-tape-out-soon-rumor/ Nvidia Hopper, 2022 L1 Cache or Scratchpad: 256KB per SM Can be used as L1 Cache and/or Scratchpad Cores: 144 Streaming Multiprocessors L2 Cache: 60 MB shared Deeper and Larger Cache Hierarchies 84https://developer.nvidia.com/blog/nvidia-hopper-architecture-in-depth/ Nvidia Hopper, 2022 L1 Cache or Scratchpad: 256KB per SM Can be used as L1 Cache and/or Scratchpad Cores: 144 Streaming Multiprocessors L2 Cache: 60 MB shared  Example of data movement between GPU global memory (DRAM) and GPU cores. NVIDIA V100 & A100 Memory Hierarchy A100 feature: Direct copy from L2 to scratchpad, bypassing L1 and register file. 85 https://images.nvidia.com/aem-dam/ en-zz/Solutions/data-center/nvidia-ampere-archit ecture-whitepaper. pdf NVIDIA A100 Tensor Core GPU Architecture In-Depth 40 NVIDIA A100 Tensor Core GPU Architecture A100 improves SM bandwidth efficiency with a new load-global-store-shared asynchronous copy instruction that bypasses L1 cache and register file (RF). Additionally, A100’s more efficient Tensor Cores reduce shared memory (SMEM) loads. Figure 15. A100 SM Data Movement Efficiency New asynchronous barriers work together with the asynchronous copy instruction to enable efficient data fetch pipelines, and A100 increases maximum SMEM allocation per SM 1.7x to 164 KB (vs 96 KB on V100). With these improvements A100 SMs continuously data stream data to keep the L2 cache constantly utilized. L2 Cache and DRAM Bandwidth improvements - The NVIDIA A100 GPU’s increased number of SMs and more powerful Tensor Cores in turn increase the required data fetch rates from DRAM and L2 cache. To feed the Tensor Cores, A100 implements a 5-site HBM2 memory subsystem with bandwidth of 1555 GB/sec, over 1.7x faster than V100. A100 further provides 2.3x the L2 cache read bandwidth of V100. Alongside the raw data bandwidth improvements, A100 improves data fetch efficiency and reduces DRAM bandwidth demand with a 40 MB L2 cache that is almost 7x larger than that of Tesla V100. To fully exploit the L2 capacity A100 includes improved cache management controls. Optimized for neural network training and inferencing as well as general compute workloads, the new controls ensure that data in the cache is used more efficiently by minimizing writebacks to memory and keeping reused data in L2 to reduce redundant DRAM traffic. Memory in the NVIDIA H100 GPU 86 … SM Core Control Core Core Core Core Core Core Core SM Core Control Core Core Core Core Core Core Core SM Core Control Core Core Core Core Core Core Core L2 Cache Global Memory Registers Shared Memory L1 Cache Constant Cache Registers Shared Memory L1 Cache Constant Cache Registers Shared Memory L1 Cache Constant Cache ≈1 cycle ≈5 cycles ≈5 cycles ≈500 cycles Slide credit: Izzat El Hajj 60 MB 80 GB Direct copy SM-to-SM 3 TB/s Multi-Level Cache Design Decisions  Which level(s) to place a block into (from memory)?  Which level(s) to evict a block to (from an inner level)?  Bypassing vs. non-bypassing levels  Inclusive, exclusive, non-inclusive hierarchies  Inclusive: a block in an inner level is always included also in an outer level  simplifies cache coherence  Exclusive: a block in an inner level does not exist in an outer level  better utilizes space in the entire hierarchy  Non-inclusive: a block in an inner level may or may not be included in an outer level  relaxes design decisions 87 Cache Performance Cache Parameters vs. Miss/Hit Rate  Cache size  Block size  Associativity  Replacement policy  Insertion/Placement policy  Promotion Policy 89 Cache Size  Cache size: total data capacity (not including tag store)  bigger cache can exploit temporal locality better  Too large a cache adversely affects hit and miss latency  bigger is slower  Too small a cache  does not exploit temporal locality well  useful data replaced often  Working set: entire set of data the executing application references  Within a time interval 90 hit rate cache size “working set” size Benefits of Larger Caches Widely Varies  Benefits of cache size widely varies across applications 91 Low Cache Utility High Cache Utility Saturating Cache Utility Number of ways from 16-way 1MB L2Misses per 1000 instructions Qureshi and Patt, “Utility-Based Cache Partitioning,” MICRO 2006. Block Size  Block size is the data that is associated with an address tag  not necessarily the unit of transfer between hierarchies  Sub-blocking: A block divided into multiple pieces (each w/ V/D bits)  Too small blocks  do not exploit spatial locality well  have larger tag overhead  Too large blocks  too few total blocks  do not exploit temporal locality well  waste cache space and bandwidth/energy if spatial locality is not high 92 hit rate block size Large Blocks: Critical-Word and Subblocking  Large cache blocks can take a long time to fill into the cache  Idea: Fill cache block critical-word first  Supply the critical data to the processor immediately  Large cache blocks can waste bus bandwidth  Idea: Divide a block into subblocks  Associate separate valid and dirty bits for each subblock  Recall: When is this useful? 93 tagsubblockvsubblockv subblockvd d d Associativity  How many blocks can be present in the same index (i.e., set)?  Larger associativity  lower miss rate (reduced conflicts)  higher hit latency and area cost  Smaller associativity  lower cost  lower hit latency  Especially important for L1 caches  Is power of 2 associativity required? 94 associativity hit rate Recall: Higher Associativity (4-way)  4-way 95 Tag store Data store =?=? =?=? MUX MUX byte in block Logic Hit? Address tag index byte in block 3 bits1 b4 bits Higher Associativity (3-way)  3-way 96 Tag store Data store =?=? =? MUX MUX byte in block Logic Hit? Address tag index byte in block 3 bits1 b4 bits Recall: 8-way Fully Associative Cache 97 Tag store Data store =? =? =? =? =? =? =? =? MUX MUX byte in block Logic Hit? Address tag byte in block 3 bits5 bits 7-way Fully Associative Cache 98 Tag store Data store =? =? =? =? =? =? =? MUX MUX byte in block Logic Hit? Address tag byte in block 3 bits5 bits Classification of Cache Misses  Compulsory miss  first reference to an address (block) always results in a miss  subsequent references to the block should hit in cache unless the block is displaced from cache for the reasons below  Capacity miss  cache is too small to hold all needed data  defined as the misses that would occur even in a fully- associative cache (with optimal replacement) of the same capacity  Conflict miss  defined as any miss that is neither a compulsory nor a capacity miss 99 How to Reduce Each Miss Type  Compulsory  Caching (only accessed data) cannot help; larger blocks can  Prefetching helps: Anticipate which blocks will be needed soon  Conflict  More associativity  Other ways to get more associativity without making the cache associative  Victim cache  Better, randomized indexing into the cache  Software hints for eviction/replacement/promotion  Capacity  Utilize cache space better: keep blocks that will be referenced  Software management: divide working set and computation such that each “computation phase” fits in cache 100 How to Improve Cache Performance  Three fundamental goals  Reducing miss rate  Caveat: reducing miss rate can reduce performance if more costly-to-refetch blocks are evicted  Reducing miss latency or miss cost  Reducing hit latency or hit cost  The above three together affect performance 101 Improving Basic Cache Performance  Reducing miss rate  More associativity  Alternatives/enhancements to associativity  Victim caches, hashing, pseudo-associativity, skewed associativity  Better replacement/insertion policies  Software approaches  Reducing miss latency/cost  Multi-level caches  Critical word first  Subblocking/sectoring  Better replacement/insertion policies  Non-blocking caches (multiple cache misses in parallel)  Multiple accesses per cycle  Software approaches 102 Software Approaches for Higher Hit Rate  Restructuring data access patterns  Restructuring data layout  Loop interchange  Data structure separation/merging  Blocking  … 103 Restructuring Data Access Patterns (I)  Idea: Restructure data layout or data access patterns  Example: If column-major  x[i+1,j] follows x[i,j] in memory  x[i,j+1] is far away from x[i,j]  This is called loop interchange  Other optimizations can also increase hit rate  Loop fusion, array merging, … 104 Poor code for i = 1, rows for j = 1, columns sum = sum + x[i,j] Better code for j = 1, columns for i = 1, rows sum = sum + x[i,j] Improving Basic Cache Performance  Reducing miss rate  More associativity  Alternatives/enhancements to associativity  Victim caches, hashing, pseudo-associativity, skewed associativity  Better replacement/insertion policies  Software approaches  Reducing miss latency/cost  Multi-level caches  Critical word first  Subblocking/sectoring  Better replacement/insertion policies  Non-blocking caches (multiple cache misses in parallel)  Multiple accesses per cycle  Software approaches 105 Research Opportunities Research Opportunities  If you are interested in doing research in Computer Architecture, Security, Systems & Bioinformatics:  Email me and Prof. Mutlu with your interest  Take the seminar course and the “Computer Architecture” course  Do readings and assignments on your own & talk with us  There are many exciting projects and research positions, e.g.:  Novel memory/storage/computation/communication systems  New execution paradigms (e.g., in-memory computing)  Hardware security, safety, reliability, predictability  GPUs, TPUs, FPGAs, PIM, heterogeneous systems, …  Security-architecture-reliability-energy-performance interactions  Architectures for genomics/proteomics/medical/health/AI/ML  A limited list is here: https://safari.ethz.ch/theses/ 107https://people.inf.ethz.ch/omutlu/projects.htm Bachelor’s Seminar in Computer Architecture  Fall 2024 (offered every Fall and Spring Semester)  2 credit units  Rigorous seminar on fundamental and cutting-edge topics in computer architecture  Critical paper presentation, review, and discussion of seminal and cutting-edge works in computer architecture  We will cover many ideas & issues, analyze their tradeoffs, perform critical thinking and brainstorming  Participation, presentation, synthesis report, lots of discussion  You can register for the course online  https://safari.ethz.ch/architecture_seminar 108 Bachelor’s Seminar in Computer Architecture 109 Bachelor’s Seminar in Computer Architecture 110 Research Opportunities  If you are interested in doing research in Computer Architecture, Security, Systems & Bioinformatics:  Email me and Prof. Mutlu with your interest  Take the seminar course and the “Computer Architecture” course  Do readings and assignments on your own & talk with us  There are many exciting projects and research positions, e.g.:  Novel memory/storage/computation/communication systems  New execution paradigms (e.g., in-memory computing)  Hardware security, safety, reliability, predictability  GPUs, TPUs, FPGAs, PIM, heterogeneous systems, …  Security-architecture-reliability-energy-performance interactions  Architectures for genomics/proteomics/medical/health/AI/ML  A limited list is here: https://safari.ethz.ch/theses/ 111https://people.inf.ethz.ch/omutlu/projects.htmhttps://www.youtube.com/watch?v=mV2OuB2djEs SAFARI Introduction & Research Computer architecture, HW/SW, systems, bioinformatics, security, memory Digital Design & Computer Arch. Lecture 24: Memory Hierarchy and Caches Frank K. Gürkaynak Mohammad Sadrosadati Prof. Onur Mutlu ETH Zürich Spring 2024 24 May 2024 Miss Latency/Cost  What is miss latency or miss cost affected by?  Where does the miss get serviced from?  What level of cache in the hierarchy?  Row hit versus row conflict in DRAM (bank/rank/channel conflict)  Queueing delays in the memory controller and the interconnect  Local vs. remote memory (chip, node, rack, remote server, …)  …  How much does the miss stall the processor?  Is it overlapped with other latencies?  Is the data immediately needed by the processor?  Is the incoming block going to evict a longer-to-refetch block?  … 114 Memory Level Parallelism (MLP)  Memory Level Parallelism (MLP) means generating and servicing multiple memory accesses in parallel [Glew’98]  Several techniques to improve MLP (e.g., out-of-order execution)  MLP varies. Some misses are isolated and some parallel How does this affect cache replacement? time A B C isolated miss parallel miss Traditional Cache Replacement Policies  Traditional cache replacement policies try to reduce miss count  Implicit assumption: Reducing miss count reduces memory- related stall time  Misses with varying cost/MLP breaks this assumption!  Eliminating an isolated miss helps performance more than eliminating a parallel miss  Eliminating a higher-latency miss could help performance more than eliminating a lower-latency miss 116 Misses to blocks P1, P2, P3, P4 can be parallel Misses to blocks S1, S2, and S3 are isolated Two replacement algorithms: 1. Minimizes miss count (Belady’s OPT) 2. Reduces isolated miss (MLP-Aware) For a fully associative cache containing 4 blocks S1P4 P3 P2 P1 P1 P2 P3 P4 S2 S3 An ExampleFewest Misses = Best Performance P3 P2 P1 P4 H H H H M H H H MHit/Miss Misses=4 Stalls=4 S1P4 P3 P2 P1 P1 P2 P3 P4 S2 S3 Time stall Belady’s OPT replacement M M MLP-Aware replacement Hit/Miss P3 P2 S1 P4 P3 P2 P1 P4 P3 P2 S2P4 P3 P2 S3P4 S1 S2 S3P1 P3 P2 S3P4 S1 S2 S3P4 H H H S1 S2 S3P4 H M M M H M M M Time stall Misses=6 Stalls=2 Saved cycles Cache Recommended: MLP-Aware Cache Replacement  How do we incorporate MLP/cost into replacement decisions?  How do we design a hybrid cache replacement policy?  Qureshi et al., “A Case for MLP-Aware Cache Replacement,” ISCA 2006. 119 Improving Basic Cache Performance  Reducing miss rate  More associativity  Alternatives/enhancements to associativity  Victim caches, hashing, pseudo-associativity, skewed associativity  Better replacement/insertion policies  Software approaches  …  Reducing miss latency/cost  Multi-level caches  Critical word first  Subblocking/sectoring  Better replacement/insertion policies  Non-blocking caches (multiple cache misses in parallel)  Multiple accesses per cycle  Software approaches  … 120 Lectures on Cache Optimizations (I) 121https://www.youtube.com/watch?v=OyomXCHNJDA&list=PL5Q2soXY2Zi9OhoVQBXYFIZywZXCPl4M_&index=3 Lectures on Cache Optimizations (II) 122https://www.youtube.com/watch?v=55oYBm9cifI&list=PL5Q2soXY2Zi9JXe3ywQMhylk_d5dI-TM7&index=6 Lectures on Cache Optimizations (III) 123https://www.youtube.com/watch?v=jDHx2K9HxlM&list=PL5PHm2jkkXmi5CxxI7b3JCL1TWybTDtKq&index=21 Lectures on Cache Optimizations  Computer Architecture, Fall 2017, Lecture 3  Cache Management & Memory Parallelism (ETH, Fall 2017)  https://www.youtube.com/watch?v=OyomXCHNJDA&list=PL5Q2soXY2Zi9OhoVQBX YFIZywZXCPl4M_&index=3  Computer Architecture, Fall 2018, Lecture 4a  Cache Design (ETH, Fall 2018)  https://www.youtube.com/watch?v=55oYBm9cifI&list=PL5Q2soXY2Zi9JXe3ywQMh ylk_d5dI-TM7&index=6  Computer Architecture, Spring 2015, Lecture 19  High Performance Caches (CMU, Spring 2015)  https://www.youtube.com/watch?v=jDHx2K9HxlM&list=PL5PHm2jkkXmi5CxxI7b3J CL1TWybTDtKq&index=21 124https://www.youtube.com/onurmutlulectures Multi-Core Issues in Caching Caches in a Multi-Core System 126 CORE 1L2 CACHE 0SHARED L3 CACHEDRAM INTERFACE CORE 0 CORE 2 CORE 3L2 CACHE 1L2 CACHE 2L2 CACHE 3DRAM BANKS DRAM MEMORY CONTROLLER Caches in a Multi-Core System 127Source: https://www.anandtech.com/show/16252/mac-mini-apple-m1-tested Apple M1, 2021 Caches in a Multi-Core System 128Source: https://twitter.com/Locuza_/status/1454152714930331652 Intel Alder Lake, 2021 Caches in a Multi-Core System 129https://wccftech.com/amd-ryzen-5000-zen-3-vermeer-undressed-high-res-die-shots-close-ups-pictured-detailed/ AMD Ryzen 5000, 2020 Core Count: 8 cores/16 threads L1 Caches: 32 KB per core L2 Caches: 512 KB per core L3 Cache: 32 MB shared Caches in a Multi-Core System 130https://youtu.be/gqAYMx34euU https://www.tech-critter.com/amd-keynote-computex-2021/ https://community.microcenter.com/discussion/5 134/comparing-zen-3-to-zen-2 Additional 64 MB L3 cache die stacked on top of the processor die - Connected using Through Silicon Vias (TSVs) - Total of 96 MB L3 cache AMD increases the L3 size of their 8-core Zen 3 processors from 32 MB to 96 MB 3D Stacking Technology: Example 131https://www.pcgameshardware.de/Ryzen-7-5800X3D-CPU-278064/Specials/3D-V -Cache-Release-1393125/ Caches in a Multi-Core System 132https://www.it-techblog.de/ibm-power10-prozessor-mehr-speicher-mehr-tempo-mehr-sicherheit/09/2020/ IBM POWER10, 2020 Cores: 15-16 cores, 8 threads/core L2 Caches: 2 MB per core L3 Cache: 120 MB shared Caches in a Multi-Core System 133https://www.tomshardware.com/news/infrared-photographer-photos-nvidia-ga102-ampere-silicon Nvidia Ampere, 2020 Cores: 128 Streaming Multiprocessors L1 Cache or Scratchpad: 192KB per SM Can be used as L1 Cache and/or Scratchpad L2 Cache: 40 MB shared Caches in a Multi-Core System 134https://developer.nvidia.com/blog/nvidia-hopper-architecture-in-depth/ Nvidia Hopper, 2022 L1 Cache or Scratchpad: 256KB per SM Can be used as L1 Cache and/or Scratchpad Cores: 144 Streaming Multiprocessors L2 Cache: 60 MB shared Caches in Multi-Core Systems  Cache efficiency becomes even more important in a multi- core/multi-threaded system  Memory bandwidth is at premium  Cache space is a limited resource across cores/threads  How do we design the caches in a multi-core system?  Many decisions and questions  Shared vs. private caches  How to maximize performance of the entire system?  How to provide QoS & predictable perf. to different threads in a shared cache?  Should cache management algorithms be aware of threads?  How should space be allocated to threads in a shared cache?  Should we store data in compressed format in some caches?  How do we do better reuse prediction & management in caches? 135 Private vs. Shared Caches  Private cache: Cache belongs to one core (a shared block can be in multiple caches)  Shared cache: Cache is shared by multiple cores 136 CORE 0 CORE 1 CORE 2 CORE 3 L2 CACHE L2 CACHE L2 CACHE DRAM MEMORY CONTROLLER L2 CACHE CORE 0 CORE 1 CORE 2 CORE 3 DRAM MEMORY CONTROLLER L2 CACHE Resource Sharing Concept and Advantages  Idea: Instead of dedicating a hardware resource to a hardware context, allow multiple contexts to use it  Example resources: functional units, pipeline, caches, buses, memory, interconnects, storage  Why? + Resource sharing improves utilization/efficiency  throughput  When a resource is left idle by one thread, another thread can use it; no need to replicate shared data + Reduces communication latency  For example, data shared between multiple threads can be kept in the same cache in multithreaded processors + Compatible with the shared memory programming model 137 Resource Sharing Disadvantages  Resource sharing results in contention for resources  When the resource is not idle, another thread cannot use it  If space is occupied by one thread, another thread needs to re- occupy it - Sometimes reduces each or some thread’s performance - Thread performance can be worse than when it is run alone - Eliminates performance isolation  inconsistent performance across runs - Thread performance depends on co-executing threads - Uncontrolled (free-for-all) sharing degrades quality of service - Causes unfairness, starvation Need to efficiently and fairly utilize shared resources 138 Private vs. Shared Caches  Private cache: Cache belongs to one core (a shared block can be in multiple caches)  Shared cache: Cache is shared by multiple cores 139 CORE 0 CORE 1 CORE 2 CORE 3 L2 CACHE L2 CACHE L2 CACHE DRAM MEMORY CONTROLLER L2 CACHE CORE 0 CORE 1 CORE 2 CORE 3 DRAM MEMORY CONTROLLER L2 CACHE Shared Caches Between Cores  Advantages:  High effective capacity  Dynamic partitioning of available cache space  No fragmentation due to static partitioning  If one core does not utilize some space, another core can  Easier to maintain coherence (a cache block is in a single location)  Disadvantages  Slower access (cache not tightly coupled with the core)  Cores incur conflict misses due to other cores’ accesses  Misses due to inter-core interference  Some cores can destroy the hit rate of other cores  Guaranteeing a minimum level of service (or fairness) to each core is harder (how much space, how much bandwidth?) 140 Example: Problem with Shared Caches 141 L2 $ L1 $ …… Processor Core 1 L1 $ Processor Core 2←t1 Kim et al., “Fair Cache Sharing and Partitioning in a Chip Multiprocessor Architecture,” PACT 2004. Example: Problem with Shared Caches 142 L1 $ Processor Core 1 L1 $ Processor Core 2 L2 $ …… t2→ Kim et al., “Fair Cache Sharing and Partitioning in a Chip Multiprocessor Architecture,” PACT 2004. Example: Problem with Shared Caches 143 L1 $ L2 $ …… Processor Core 1 Processor Core 2←t1 L1 $ t2→ t2’s throughput is significantly reduced due to unfair cache sharing Kim et al., “Fair Cache Sharing and Partitioning in a Chip Multiprocessor Architecture,” PACT 2004. Resource Sharing vs. Partitioning  Sharing improves throughput  Better utilization of space  Partitioning provides performance isolation (predictable performance)  Dedicated space  Can we get the benefits of both?  Idea: Design shared resources such that they are efficiently utilized, controllable and partitionable  No wasted resource + QoS mechanisms for threads 144 Lectures on Multi-Core Cache Management 145https://www.youtube.com/watch?v=7_Tqlw8gxOU&list=PL5Q2soXY2Zi9OhoVQBXYFIZywZXCPl4M_&index=17 Lectures on Multi-Core Cache Management 146https://www.youtube.com/watch?v=c9FhGRB3HoA&list=PL5Q2soXY2Zi9JXe3ywQMhylk_d5dI-TM7&index=29 Lectures on Multi-Core Cache Management 147https://www.youtube.com/watch?v=Siz86__PD4w&list=PL5Q2soXY2Zi9JXe3ywQMhylk_d5dI-TM7&index=30 Lectures on Multi-Core Cache Management  Computer Architecture, Fall 2018, Lecture 18b  Multi-Core Cache Management (ETH, Fall 2018)  https://www.youtube.com/watch?v=c9FhGRB3HoA&list=PL5Q2soXY2Zi9JXe3ywQM hylk_d5dI-TM7&index=29  Computer Architecture, Fall 2018, Lecture 19a  Multi-Core Cache Management II (ETH, Fall 2018)  https://www.youtube.com/watch?v=Siz86__PD4w&list=PL5Q2soXY2Zi9JXe3ywQM hylk_d5dI-TM7&index=30  Computer Architecture, Fall 2017, Lecture 15  Multi-Core Cache Management (ETH, Fall 2017)  https://www.youtube.com/watch?v=7_Tqlw8gxOU&list=PL5Q2soXY2Zi9OhoVQBXY FIZywZXCPl4M_&index=17 148https://www.youtube.com/onurmutlulectures Lectures on Memory Resource Management 149https://www.youtube.com/watch?v=0nnI807nCkc&list=PL5Q2soXY2Zi9xidyIgBxUz7xRPS-wisBN&index=21 Lectures on Memory Resource Management  Computer Architecture, Fall 2020, Lecture 11a  Memory Controllers (ETH, Fall 2020)  https://www.youtube.com/watch?v=TeG773OgiMQ&list=PL5Q2soXY2Zi9xidyIgBxUz 7xRPS-wisBN&index=20  Computer Architecture, Fall 2020, Lecture 11b  Memory Interference and QoS (ETH, Fall 2020)  https://www.youtube.com/watch?v=0nnI807nCkc&list=PL5Q2soXY2Zi9xidyIgBxUz7 xRPS-wisBN&index=21  Computer Architecture, Fall 2020, Lecture 13  Memory Interference and QoS II (ETH, Fall 2020)  https://www.youtube.com/watch?v=Axye9VqQT7w&list=PL5Q2soXY2Zi9xidyIgBxU z7xRPS-wisBN&index=26  Computer Architecture, Fall 2020, Lecture 2a  Memory Performance Attacks (ETH, Fall 2020)  https://www.youtube.com/watch?v=VJzZbwgBfy8&list=PL5Q2soXY2Zi9xidyIgBxUz7 xRPS-wisBN&index=2 150https://www.youtube.com/onurmutlulectures Cache Coherence Cache Coherence  Basic question: If multiple processors cache the same block, how do they ensure they all see a consistent state? 152 P1 P2 x Interconnection Network Main Memory 1000 The Cache Coherence Problem 153 P1 P2 x Interconnection Network Main Memory ld r2, x 1000 1000 The Cache Coherence Problem 154 P1 P2 x Interconnection Network Main Memory ld r2, x ld r2, x 1000 1000 1000 The Cache Coherence Problem 155 P1 P2 x Interconnection Network Main Memory ld r2, x add r1, r2, r4 st x, r1 ld r2, x 1000 10002000 The Cache Coherence Problem 156 P1 P2 x Interconnection Network Main Memory ld r2, x add r1, r2, r4 st x, r1 ld r2, x 1000 10002000 ld r5, x Should NOT load 1000 Hardware Cache Coherence  Basic idea:  A processor/cache broadcasts its write/update to a memory location to all other processors  Another processor/cache that has the location either updates or invalidates its local copy 157 A Very Simple Coherence Scheme (VI)  Idea: All caches “snoop” (observe) each other’s write/read operations. If a processor writes to a block, all others invalidate the block.  A simple protocol: 158  Write-through, no- write-allocate cache  Actions of the local processor on the cache block: PrRd, PrWr,  Actions that are broadcast on the bus for the block: BusRd, BusWr PrWr / BusWr Valid BusWr Invalid PrWr / BusWr PrRd / BusRd PrRd/-- Lecture on Cache Coherence 159https://www.youtube.com/watch?v=T9WlyezeaII&list=PL5Q2soXY2Zi9xidyIgBxUz7xRPS-wisBN&index=38 Lecture on Memory Ordering & Consistency 160https://www.youtube.com/watch?v=Suy09mzTbiQ&list=PL5Q2soXY2Zi9xidyIgBxUz7xRPS-wisBN&index=37 Lecture on Cache Coherence & Consistency  Computer Architecture, Fall 2020, Lecture 21  Cache Coherence (ETH, Fall 2020)  https://www.youtube.com/watch?v=T9WlyezeaII&list=PL5Q2soXY2Zi9xidyIgBxUz7 xRPS-wisBN&index=38  Computer Architecture, Fall 2020, Lecture 20  Memory Ordering & Consistency (ETH, Fall 2020)  https://www.youtube.com/watch?v=Suy09mzTbiQ&list=PL5Q2soXY2Zi9xidyIgBxUz 7xRPS-wisBN&index=37  Computer Architecture, Spring 2015, Lecture 28  Memory Consistency & Cache Coherence (CMU, Spring 2015)  https://www.youtube.com/watch?v=JfjT1a0vi4E&list=PL5PHm2jkkXmi5CxxI7b3JCL 1TWybTDtKq&index=32  Computer Architecture, Spring 2015, Lecture 29  Cache Coherence (CMU, Spring 2015)  https://www.youtube.com/watch?v=X6DZchnMYcw&list=PL5PHm2jkkXmi5CxxI7b3 JCL1TWybTDtKq&index=33 161https://www.youtube.com/onurmutlulectures Additional Slides: Cache Coherence 162 Two Cache Coherence Methods  How do we ensure that the proper caches are updated?  Snoopy Bus [Goodman ISCA 1983, Papamarcos+ ISCA 1984]  Bus-based, single point of serialization for all memory requests  Processors observe other processors’ actions  E.g.: P1 makes “read-exclusive” request for A on bus, P0 sees this and invalidates its own copy of A  Directory [Censier and Feautrier, IEEE ToC 1978]  Single point of serialization per block, distributed among nodes  Processors make explicit requests for blocks  Directory tracks which caches have each block  Directory coordinates invalidations and updates  E.g.: P1 asks directory for exclusive copy, directory asks P0 to invalidate, waits for ACK, then responds to P1 163 Directory Based Coherence  Idea: A logically-central directory keeps track of where the copies of each cache block reside. Caches consult this directory to ensure coherence.  An example mechanism:  For each cache block in memory, store P+1 bits in directory  One bit for each cache, indicating whether the block is in cache  Exclusive bit: indicates that a cache has the only copy of the block and can update it without notifying others  On a read: set the cache’s bit and arrange the supply of data  On a write: invalidate all caches that have the block and reset their bits  Have an “exclusive bit” associated with each block in each cache (so that the cache can update the exclusive block silently) 164 Directory Based Coherence Example (I) 165 Directory Based Coherence Example (I) 166 Maintaining Coherence  Need to guarantee that all processors see a consistent value (i.e., consistent updates) for the same memory location  Writes to location A by P0 should be seen by P1 (eventually), and all writes to A should appear in some order  Coherence needs to provide:  Write propagation: guarantee that updates will propagate  Write serialization: provide a consistent order seen by all processors for the same memory location  Need a global point of serialization for this write ordering 167 Coherence: Update vs. Invalidate  How can we safely update replicated data?  Option 1 (Update protocol): push an update to all copies  Option 2 (Invalidate protocol): ensure there is only one copy (local), update it  On a Read:  If local copy is Invalid, put out request  (If another node has a copy, it returns it, otherwise memory does) 168 Coherence: Update vs. Invalidate (II)  On a Write:  Read block into cache as before Update Protocol:  Write to block, and simultaneously broadcast written data and address to sharers  (Other nodes update the data in their caches if block is present) Invalidate Protocol:  Write to block, and simultaneously broadcast invalidation of address to sharers  (Other nodes invalidate block in their caches if block is present) 169 Update vs. Invalidate Tradeoffs  Which one is better? Update or invalidate?  Write frequency and sharing behavior are critical  Update + If sharer set is constant and updates are infrequent, avoids the cost of invalidate-reacquire (broadcast update pattern) - If data is rewritten without intervening reads by other cores, updates would be useless - Write-through cache policy  bus can become a bottleneck  Invalidate + After invalidation, core has exclusive access rights + Only cores that keep reading after each write retain a copy - If write contention is high, leads to ping-ponging (rapid invalidation-reacquire traffic from different processors) 170 Additional Slides: Memory Interference 171 Inter-Thread/Application Interference  Problem: Threads share the memory system, but memory system does not distinguish between threads’ requests  Existing memory systems  Free-for-all, shared based on demand  Control algorithms thread-unaware and thread-unfair  Aggressive threads can deny service to others  Do not try to reduce or control inter-thread interference 172 Unfair Slowdowns due to Interference (Core 0) (Core 1) Moscibroda and Mutlu, “Memory performance attacks: Denial of memory service in multi-core systems,” USENIX Security 2007. matlab (Core 1) gcc (Core 2) 173174 Uncontrolled Interference: An Example CORE 1 CORE 2 L2 CACHE L2 CACHE DRAM MEMORY CONTROLLER DRAM Bank 0 DRAM Bank 1 DRAM Bank 2 Shared DRAM Memory System Multi-Core Chip unfairness INTERCONNECT stream random DRAM Bank 3 // initialize large arrays A, B for (j=0; j<N; j++) { index = rand(); A[index] = B[index]; … } 175 A Memory Performance Hog STREAM - Sequential memory access - Very high row buffer locality (96% hit rate) - Memory intensive RANDOM - Random memory access - Very low row buffer locality (3% hit rate) - Similarly memory intensive // initialize large arrays A, B for (j=0; j<N; j++) { index = j*linesize; A[index] = B[index]; … } streaming random Moscibroda and Mutlu, “Memory Performance Attacks,” USENIX Security 2007. 176 What Does the Memory Hog Do? Row BufferRow decoder Column mux Data Row 0 T0: Row 0 Row 0 T1: Row 16 T0: Row 0T1: Row 111 T0: Row 0T0: Row 0T1: Row 5 T0: Row 0T0: Row 0T0: Row 0T0: Row 0T0: Row 0 Memory Request Buffer T0: STREAM T1: RANDOM Row size: 8KB, cache block size: 64B 128 (8KB/64B) requests of T0 serviced before T1 Moscibroda and Mutlu, “Memory Performance Attacks,” USENIX Security 2007. 177 DRAM Controllers  A row-conflict memory access takes significantly longer than a row-hit access  Current controllers take advantage of the row buffer  Commonly used scheduling policy (FR-FCFS) [Rixner 2000]* (1) Row-hit first: Service row-hit memory accesses first (2) Oldest-first: Then service older accesses first  This scheduling policy aims to maximize DRAM throughput  But, it is unfair when multiple threads share the DRAM system *Rixner et al., “Memory Access Scheduling,” ISCA 2000. *Zuravleff and Robinson, “Controller for a synchronous DRAM …,” US Patent 5,630,096, May 1997. Effect of the Memory Performance Hog 0 0.5 1 1.5 2 2.5 3 STREAM RANDOM 178 1.18X slowdown 2.82X slowdown Results on Intel Pentium D running Windows XP (Similar results for Intel Core Duo and AMD Turion, and on Fedora Linux) Slowdown 0 0.5 1 1.5 2 2.5 3 STREAM gcc 0 0.5 1 1.5 2 2.5 3 STREAM Virtual PC Moscibroda and Mutlu, “Memory Performance Attacks,” USENIX Security 2007. Greater Problem with More Cores  Vulnerable to denial of service (DoS)  Unable to enforce priorities or SLAs  Low system performance Uncontrollable, unpredictable system 179 Greater Problem with More Cores  Vulnerable to denial of service (DoS)  Unable to enforce priorities or SLAs  Low system performance Uncontrollable, unpredictable system 180 Distributed DoS in Networked Multi-Core Systems 181 Attackers (Cores 1-8) Stock option pricing application (Cores 9-64) Cores connected via packet-switched routers on chip ~5000X latency increase Grot, Hestness, Keckler, Mutlu, “Preemptive virtual clock: A Flexible, Efficient, and Cost-effective QOS Scheme for Networks-on-Chip,“ MICRO 2009. More on Memory Performance Attacks  Thomas Moscibroda and Onur Mutlu, \"Memory Performance Attacks: Denial of Memory Service in Multi-Core Systems\" Proceedings of the 16th USENIX Security Symposium (USENIX SECURITY), pages 257-274, Boston, MA, August 2007. Slides (ppt) 182 http://www.youtube.com/watch?v=VJzZbwgBfy8 More on Interconnect Based Starvation  Boris Grot, Stephen W. Keckler, and Onur Mutlu, \"Preemptive Virtual Clock: A Flexible, Efficient, and Cost- effective QOS Scheme for Networks-on-Chip\" Proceedings of the 42nd International Symposium on Microarchitecture (MICRO), pages 268-279, New York, NY, December 2009. Slides (pdf) 183 Energy Comparison of Memory Technologies The Problem: Energy  Faster is more energy-efficient  SRAM, ~5 pJ  DRAM, ~40-140 pJ  PCM-DIMM (Intel Optane DC DIMM), ~80-540 pJ  PCM-SSD, ~120 µJ  Flash memory, ~250 µJ  Hard Disk, ~60 mJ  Other technologies have their place as well  MRAM, RRAM, STT-MRAM, memristors, … (not mature yet) 185 The Problem (Table View): Energy 186 Memory Device Capacity Latency Cost per Megabyte Energy per access Energy per byte access SRAM < 1 KByte sub-nanosec ~5 pJ ~1.25 pJ SRAM KByte~MB yte ~nanosec < 0.3$ DRAM Gigabyte ~50 nanosec < 0.006$ ~40-140 pJ ~10-35 pJ PCM-DIMM (Intel Optane DC DIMM) Gigabyte ~300 nanosec < 0.004$ ~80-540 pJ ~20-135 pJ PCM-SSD (Intel Optane SSD) Gigabyte ~Terabyte ~6-10 µs < 0.002$ ~120 µJ ~30 nJ Flash memory Gigabyte ~Terabyte ~50-100 µs < 0.00008$ ~250 µJ ~61 nJ Hard Disk Terabyte ~10 millisec < 0.00003$ ~60 mJ ~15 µJ These sample values (circa ~2022) scale with time Bigger is slower Faster is more expensive ($$$ and chip area) Faster is more energy-efficient Basic Cache Examples: For You to Study Cache Terminology  Capacity (C):  the number of data bytes a cache stores  Block size (b):  bytes of data brought into cache at once  Number of blocks (B = C/b):  number of blocks in cache: B = C/b  Degree of associativity (N):  number of blocks in a set  Number of sets (S = B/N):  each memory address maps to exactly one cache set 188 How is data found?  Cache organized into S sets  Each memory address maps to exactly one set  Caches categorized by number of blocks in a set:  Direct mapped: 1 block per set  N-way set associative: N blocks per set  Fully associative: all cache blocks are in a single set  Examine each organization for a cache with:  Capacity (C = 8 words)  Block size (b = 1 word)  So, number of blocks (B = 8) 189 Direct Mapped Cache 7 (111) 00...00010000 2 30 Word Main Memory mem[0x00...00] mem[0x00...04] mem[0x00...08] mem[0x00...0C] mem[0x00...10] mem[0x00...14] mem[0x00...18] mem[0x00..1C] mem[0x00..20] mem[0x00...24] mem[0xFF...E0] mem[0xFF...E4] mem[0xFF...E8] mem[0xFF...EC] mem[0xFF...F0] mem[0xFF...F4] mem[0xFF...F8] mem[0xFF...FC] 2 3 Word Cache Set Number Address 00...00000000 00...00000100 00...00001000 00...00001100 00...00010100 00...00011000 00...00011100 00...00100000 00...00100100 11...11110000 11...11100000 11...11100100 11...11101000 11...11101100 11...11110100 11...11111000 11...11111100 6 (110) 5 (101) 4 (100) 3 (011) 2 (010) 1 (001) 0 (000) 190 Direct Mapped Cache Hardware DataTag 00 Tag Set Byte OffsetMemory Address DataHit V = 27 3 27 32 8-entry x (1+27+32)-bit SRAM 191 Direct Mapped Cache Performance # MIPS assembly code addi $t0, $0, 5 loop: beq $t0, $0, done lw $t1, 0x4($0) lw $t2, 0xC($0) lw $t3, 0x8($0) addi $t0, $t0, -1 j loop done: DataTagV 00...001 mem[0x00...04] 0 0 0 0 0 00 Tag Set Byte OffsetMemory Address V 3 00100...00 1 00...00 00...00 1 mem[0x00...0C] mem[0x00...08] Set 7 (111) Set 6 (110) Set 5 (101) Set 4 (100) Set 3 (011) Set 2 (010) Set 1 (001) Set 0 (000) Miss Rate = 192 Direct Mapped Cache Performance # MIPS assembly code addi $t0, $0, 5 loop: beq $t0, $0, done lw $t1, 0x4($0) lw $t2, 0xC($0) lw $t3, 0x8($0) addi $t0, $t0, -1 j loop done: DataTagV 00...001 mem[0x00...04] 0 0 0 0 0 00 Tag Set Byte OffsetMemory Address V 3 00100...00 1 00...00 00...00 1 mem[0x00...0C] mem[0x00...08] Set 7 (111) Set 6 (110) Set 5 (101) Set 4 (100) Set 3 (011) Set 2 (010) Set 1 (001) Set 0 (000) Miss Rate = 3/15 = 20% Temporal Locality Compulsory Misses 193 Direct Mapped Cache: Conflict # MIPS assembly code addi $t0, $0, 5 loop: beq $t0, $0, done lw $t1, 0x4($0) lw $t2, 0x24($0) addi $t0, $t0, -1 j loop done: DataTagV 00...001 mem[0x00...04] 0 0 0 0 0 00 Tag Set Byte OffsetMemory Address V 3 00100...01 0 0 Set 7 (111) Set 6 (110) Set 5 (101) Set 4 (100) Set 3 (011) Set 2 (010) Set 1 (001) Set 0 (000) mem[0x00...24] Miss Rate = 194 Direct Mapped Cache: Conflict # MIPS assembly code addi $t0, $0, 5 loop: beq $t0, $0, done lw $t1, 0x4($0) lw $t2, 0x24($0) addi $t0, $t0, -1 j loop done: DataTagV 00...001 mem[0x00...04] 0 0 0 0 0 00 Tag Set Byte OffsetMemory Address V 3 00100...01 0 0 Set 7 (111) Set 6 (110) Set 5 (101) Set 4 (100) Set 3 (011) Set 2 (010) Set 1 (001) Set 0 (000) mem[0x00...24] Miss Rate = 10/10 = 100% Conflict Misses 195 N-Way Set Associative Cache DataTag Tag Set Byte OffsetMemory Address Data Hit1 V =01 00 32 32 32 DataTagV = Hit1Hit0 Hit 28 2 28 28 Way 1 Way 0 196 N-way Set Associative Performance # MIPS assembly code addi $t0, $0, 5 loop: beq $t0, $0, done lw $t1, 0x4($0) lw $t2, 0x24($0) addi $t0, $t0, -1 j loop done: DataTagV DataTagV 00...001 mem[0x00...04]00...10 1mem[0x00...24] 0 0 0 0 0 0 Way 1 Way 0 Set 3 Set 2 Set 1 Set 0 Miss Rate = 197 N-way Set Associative Performance # MIPS assembly code addi $t0, $0, 5 loop: beq $t0, $0, done lw $t1, 0x4($0) lw $t2, 0x24($0) addi $t0, $t0, -1 j loop done: Miss Rate = 2/10 = 20% Associativity reduces conflict misses DataTagV DataTagV 00...001 mem[0x00...04]00...10 1mem[0x00...24] 0 0 0 0 0 0 Way 1 Way 0 Set 3 Set 2 Set 1 Set 0 198 Fully Associative Cache  No conflict misses  Expensive to build DataTagV DataTagV DataTagV DataTagV DataTagV DataTagV DataTagV DataTagV 199 Spatial Locality?  Increase block size:  Block size, b = 4 words  C = 8 words  Direct mapped (1 block per set)  Number of blocks, B = C/b = 8/4 = 2 DataTag 00 Tag Byte OffsetMemory Address Data V00011011 Block Offset 32 32 32 32 32 Hit = Set 27 27 2 Set 1 Set 0 200 Direct Mapped Cache Performance addi $t0, $0, 5 loop: beq $t0, $0, done lw $t1, 0x4($0) lw $t2, 0xC($0) lw $t3, 0x8($0) addi $t0, $t0, -1 j loop done: 00...00 0 11 DataTag 00 Tag Byte OffsetMemory Address Data V00011011 Block Offset 32 32 32 32 32 Hit = Set 27 27 2 Set 1 Set 000...001 mem[0x00...0C] 0 mem[0x00...08] mem[0x00...04] mem[0x00...00] Miss Rate = 201 Direct Mapped Cache Performance addi $t0, $0, 5 loop: beq $t0, $0, done lw $t1, 0x4($0) lw $t2, 0xC($0) lw $t3, 0x8($0) addi $t0, $t0, -1 j loop done: Miss Rate = 1/15 = 6.67% Larger blocks reduce compulsory misses through spatial locality 00...00 0 11 DataTag 00 Tag Byte OffsetMemory Address Data V00011011 Block Offset 32 32 32 32 32 Hit = Set 27 27 2 Set 1 Set 000...001 mem[0x00...0C] 0 mem[0x00...08] mem[0x00...04] mem[0x00...00] 202 Cache Organization Recap  Main Parameters  Capacity: C  Block size: b  Number of blocks in cache: B = C/b  Number of blocks in a set: N  Number of Sets: S = B/N Organization Number of Ways (N) Number of Sets (S = B/N) Direct Mapped 1 B N-Way Set Associative 1 < N < B B / N Fully Associative B 1 203 Capacity Misses  Cache is too small to hold all data of interest at one time  If the cache is full and program tries to access data X that is not in cache, cache must evict data Y to make room for X  Capacity miss occurs if program then tries to access Y again  X will be placed in a particular set based on its address  In a direct mapped cache, there is only one place to put X  In an associative cache, there are multiple ways where X could go in the set.  How to choose Y to minimize chance of needing it again?  Least recently used (LRU) replacement: the least recently used block in a set is evicted when the cache is full. 204 Types of Misses  Compulsory: first time data is accessed  Capacity: cache too small to hold all data of interest  Conflict: data of interest maps to same location in cache  Miss penalty: time it takes to retrieve a block from lower level of hierarchy 205 LRU Replacement # MIPS assembly lw $t0, 0x04($0) lw $t1, 0x24($0) lw $t2, 0x54($0) DataTagV DataTagVU DataTagV DataTagVU (a) (b) Set Number 3 (11) 2 (10) 1 (01) 0 (00) Set Number 3 (11) 2 (10) 1 (01) 0 (00) 206 LRU Replacement # MIPS assembly lw $t0, 0x04($0) lw $t1, 0x24($0) lw $t2, 0x54($0) DataTagV 0 DataTagV 0 0 0 0 0 U mem[0x00...04]1 00...000mem[0x00...24] 100...010 0 0 0 0 DataTagV 0 DataTagV 0 0 0 0 0 U mem[0x00...54]1 00...101mem[0x00...24] 100...010 0 0 0 1 (a) (b) Way 1 Way 0 Way 1 Way 0 Set 3 (11) Set 2 (10) Set 1 (01) Set 0 (00) Set 3 (11) Set 2 (10) Set 1 (01) Set 0 (00) 207 Slides for Future Lectures 208 Issues in Set-Associative Caches  Think of each block in a set having a “priority”  Indicating how important it is to keep the block in the cache  Key issue: How do you determine/adjust block priorities?  There are three key decisions in a set:  Insertion, promotion, eviction (replacement)  Insertion: What happens to priorities on a cache fill?  Where to insert the incoming block, whether or not to insert the block  Promotion: What happens to priorities on a cache hit?  Whether and how to change block priority  Eviction/replacement: What happens to priorities on a cache miss?  Which block to evict and how to adjust priorities 209 Eviction/Replacement Policy  Which block in the set to replace on a cache miss?  Any invalid block first  If all are valid, consult the replacement policy  Random  FIFO  Least recently used (how to implement?)  Not most recently used  Least frequently used?  Least costly to re-fetch?  Why would memory accesses have different cost?  Hybrid replacement policies  Optimal replacement policy? 210 Implementing LRU  Idea: Evict the least recently accessed block  Problem: Need to keep track of access ordering of blocks  Question: 2-way set associative cache:  What do you minimally need to implement LRU perfectly?  Question: 4-way set associative cache:  What do you minimally need to implement LRU perfectly?  How many different orderings possible for the 4 blocks in the set?  How many bits needed to encode the LRU order of a block?  What is the logic needed to determine the LRU victim?  Repeat for N-way set associative cache 211 Approximations of LRU  Most modern processors do not implement “true LRU” (also called “perfect LRU”) in highly-associative caches  Why?  True LRU is complex  LRU is an approximation to predict locality anyway (i.e., not the best possible cache management policy)  Examples:  Not MRU (not most recently used)  Hierarchical LRU: divide the N-way set into M “groups”, track the MRU group and the MRU way in each group  Victim-NextVictim Replacement: Only keep track of the victim and the next victim 212 Cache Replacement Policy: LRU or Random  LRU vs. Random: Which one is better?  Example: 4-way cache, cyclic references to A, B, C, D, E  0% hit rate with LRU policy  Set thrashing: When the “program working set” in a set is larger than set associativity  Random replacement policy is better when thrashing occurs  In practice:  Performance of replacement policy depends on workload  Average hit rate of LRU and Random are similar  Best of both Worlds: Hybrid of LRU and Random  How to choose between the two? Set sampling  See Qureshi et al., ”A Case for MLP-Aware Cache Replacement,” ISCA 2006. 213 What Is the Optimal Replacement Policy?  Belady’s OPT  Replace the block that is going to be referenced furthest in the future by the program  Belady, “A study of replacement algorithms for a virtual-storage computer,” IBM Systems Journal, 1966.  How do we implement this? Simulate?  Is this optimal for minimizing miss rate?  Is this optimal for minimizing execution time?  No. Cache miss latency/cost varies from block to block!  Two reasons: Where miss is serviced from and miss overlapping  Qureshi et al. “A Case for MLP-Aware Cache Replacement,\" ISCA 2006. 214 Recommended Reading  Key observation: Some misses more costly than others as their latency is exposed as stall time. Reducing miss rate is not always good for performance. Cache replacement should take into account cost of misses.  Moinuddin K. Qureshi, Daniel N. Lynch, Onur Mutlu, and Yale N. Patt, \"A Case for MLP-Aware Cache Replacement\" Proceedings of the 33rd International Symposium on Computer Architecture (ISCA), pages 167-177, Boston, MA, June 2006. Slides (ppt) 215 What’s In A Tag Store Entry?  Valid bit  Tag  Replacement policy bits  Dirty bit?  Write back vs. write through caches 216 Handling Writes (I)  When do we write the modified data in a cache to the next level?  Write through: At the time the write happens  Write back: When the block is evicted  Write-back + Can combine multiple writes to the same block before eviction  Potentially saves bandwidth between cache levels + saves energy -- Need a bit in the tag store indicating the block is “dirty/modified”  Write-through + Simpler design + All levels are up to date & consistent  Simpler cache coherence: no need to check close-to-processor caches’ tag stores for presence -- More bandwidth intensive; no combining of writes 217 Handling Writes (II)  Do we allocate a cache block on a write miss?  Allocate on write miss: Yes  No-allocate on write miss: No  Allocate on write miss + Can combine writes instead of writing each individually to next level + Simpler because write misses can be treated the same way as read misses -- Requires transfer of the whole cache block  No-allocate + Conserves cache space if locality of written blocks is low (potentially better cache hit rate) 218 Handling Writes (III)  What if the processor writes to an entire block over a small amount of time?  Is there any need to bring the block into the cache from memory in the first place?  Why do we not simply write to only a portion of the block, i.e., subblock  E.g., 4 bytes out of 64 bytes  Problem: Valid and dirty bits are associated with the entire 64 bytes, not with each individual 4 bytes 219 Subblocked (Sectored) Caches  Idea: Divide a block into subblocks (or sectors)  Have separate valid and dirty bits for each subblock (sector)  Allocate only a subblock (or a subset of subblocks) on a request ++ No need to transfer the entire cache block into the cache (A write simply validates and updates a subblock) ++ More freedom in transferring subblocks into the cache (a cache block does not need to be in the cache fully) (How many subblocks do you transfer on a read?) -- More complex design -- May not exploit spatial locality fully 220 tagsubblockvsubblockv subblockvd d d Instruction vs. Data Caches  Separate or Unified?  Pros and Cons of Unified: + Dynamic sharing of cache space: no overprovisioning that might happen with static partitioning (i.e., separate I and D caches) -- Instructions and data can evict/thrash each other (i.e., no guaranteed space for either) -- I and D are accessed in different places in the pipeline. Where do we place the unified cache for fast access?  First level caches are almost always split  Mainly for the last reason above – pipeline constraints  Outer level caches are almost always unified 221 Multi-level Caching in a Pipelined Design  First-level caches (instruction and data)  Decisions very much affected by cycle time & pipeline structure  Small, lower associativity; latency is critical  Tag store and data store usually accessed in parallel  Second- and third-level caches  Decisions need to balance hit rate and access latency  Usually large and highly associative; latency not as important  Tag store and data store can be accessed serially  Serial vs. Parallel access of levels  Serial: Second level cache accessed only if first-level misses  Second level does not see the same accesses as the first  First level acts as a filter (filters some temporal and spatial locality)  Management policies are therefore different 222 Deeper and Larger Cache Hierarchies 223Source: https://www.anandtech.com/show/16252/mac-mini-apple-m1-tested Apple M1, 2021 Deeper and Larger Cache Hierarchies 224Source: https://twitter.com/Locuza_/status/1454152714930331652 Intel Alder Lake, 2021 Deeper and Larger Cache Hierarchies 225https://wccftech.com/amd-ryzen-5000-zen-3-vermeer-undressed-high-res-die-shots-close-ups-pictured-detailed/ AMD Ryzen 5000, 2020 Core Count: 8 cores/16 threads L1 Caches: 32 KB per core L2 Caches: 512 KB per core L3 Cache: 32 MB shared AMD’s 3D Last Level Cache (2021) 226https://youtu.be/gqAYMx34euU https://www.tech-critter.com/amd-keynote-computex-2021/ https://community.microcenter.com/discussion/5 134/comparing-zen-3-to-zen-2 Additional 64 MB L3 cache die stacked on top of the processor die - Connected using Through Silicon Vias (TSVs) - Total of 96 MB L3 cache AMD increases the L3 size of their 8-core Zen 3 processors from 32 MB to 96 MB Deeper and Larger Cache Hierarchies 227https://www.it-techblog.de/ibm-power10-prozessor-mehr-speicher-mehr-tempo-mehr-sicherheit/09/2020/ IBM POWER10, 2020 Cores: 15-16 cores, 8 threads/core L2 Caches: 2 MB per core L3 Cache: 120 MB shared Deeper and Larger Cache Hierarchies 228https://www.tomshardware.com/news/infrared-photographer-photos-nvidia-ga102-ampere-silicon Nvidia Ampere, 2020 Cores: 128 Streaming Multiprocessors L1 Cache or Scratchpad: 192KB per SM Can be used as L1 Cache and/or Scratchpad L2 Cache: 40 MB shared Deeper and Larger Cache Hierarchies 229https://wccftech.com/nvidia-hopper-gpus-featuring-mcm-technology-tape-out-soon-rumor/ Nvidia Hopper, 2022 L1 Cache or Scratchpad: 256KB per SM Can be used as L1 Cache and/or Scratchpad Cores: 144 Streaming Multiprocessors L2 Cache: 60 MB shared Deeper and Larger Cache Hierarchies 230https://developer.nvidia.com/blog/nvidia-hopper-architecture-in-depth/ Nvidia Hopper, 2022 L1 Cache or Scratchpad: 256KB per SM Can be used as L1 Cache and/or Scratchpad Cores: 144 Streaming Multiprocessors L2 Cache: 60 MB shared  Example of data movement between GPU global memory (DRAM) and GPU cores. NVIDIA V100 & A100 Memory Hierarchy A100 feature: Direct copy from L2 to scratchpad, bypassing L1 and register file. 231https://images.nvidia.com/aem-dam/ en-zz/Solutions/data-center/nvidia-ampere-archit ecture-whitepaper. pdf NVIDIA A100 Tensor Core GPU Architecture In-Depth 40 NVIDIA A100 Tensor Core GPU Architecture A100 improves SM bandwidth efficiency with a new load-global-store-shared asynchronous copy instruction that bypasses L1 cache and register file (RF). Additionally, A100’s more efficient Tensor Cores reduce shared memory (SMEM) loads. Figure 15. A100 SM Data Movement Efficiency New asynchronous barriers work together with the asynchronous copy instruction to enable efficient data fetch pipelines, and A100 increases maximum SMEM allocation per SM 1.7x to 164 KB (vs 96 KB on V100). With these improvements A100 SMs continuously data stream data to keep the L2 cache constantly utilized. L2 Cache and DRAM Bandwidth improvements - The NVIDIA A100 GPU’s increased number of SMs and more powerful Tensor Cores in turn increase the required data fetch rates from DRAM and L2 cache. To feed the Tensor Cores, A100 implements a 5-site HBM2 memory subsystem with bandwidth of 1555 GB/sec, over 1.7x faster than V100. A100 further provides 2.3x the L2 cache read bandwidth of V100. Alongside the raw data bandwidth improvements, A100 improves data fetch efficiency and reduces DRAM bandwidth demand with a 40 MB L2 cache that is almost 7x larger than that of Tesla V100. To fully exploit the L2 capacity A100 includes improved cache management controls. Optimized for neural network training and inferencing as well as general compute workloads, the new controls ensure that data in the cache is used more efficiently by minimizing writebacks to memory and keeping reused data in L2 to reduce redundant DRAM traffic. Memory in the NVIDIA H100 GPU 232 … SM Core Control Core Core Core Core Core Core Core SM Core Control Core Core Core Core Core Core Core SM Core Control Core Core Core Core Core Core Core L2 Cache Global Memory Registers Shared Memory L1 Cache Constant Cache Registers Shared Memory L1 Cache Constant Cache Registers Shared Memory L1 Cache Constant Cache ≈1 cycle ≈5 cycles ≈5 cycles ≈500 cycles Slide credit: Izzat El Hajj 60 MB 80 GB Direct copy SM-to-SM 3 TB/s Multi-Level Cache Design Decisions  Which level(s) to place a block into (from memory)?  Which level(s) to evict a block to (from an inner level)?  Bypassing vs. non-bypassing levels  Inclusive, exclusive, non-inclusive hierarchies  Inclusive: a block in an inner level is always included also in an outer level  simplifies cache coherence  Exclusive: a block in an inner level does not exist in an outer level  better utilizes space in the entire hierarchy  Non-inclusive: a block in an inner level may or may not be included in an outer level  relaxes design decisions 233 Cache Performance Cache Parameters vs. Miss/Hit Rate  Cache size  Block size  Associativity  Replacement policy  Insertion/Placement policy  Promotion Policy 235 Cache Size  Cache size: total data (not including tag) capacity  bigger can exploit temporal locality better  Too large a cache adversely affects hit and miss latency  bigger is slower  Too small a cache  does not exploit temporal locality well  useful data replaced often  Working set: entire set of data the executing application references  Within a time interval 236 hit rate cache size “working set” size Benefit of Larger Caches Widely Varies  Benefits of cache size widely varies across applications 237 Low Utility Application High Utility Application Saturating Utility Application Num ways from 16-way 1MB L2Misses per 1000 instructions Qureshi and Patt, “Utility-Based Cache Partitioning,” MICRO 2006. Block Size  Block size is the data that is associated with an address tag  not necessarily the unit of transfer between hierarchies  Sub-blocking: A block divided into multiple pieces (each w/ V/D bits)  Too small blocks  do not exploit spatial locality well  have larger tag overhead  Too large blocks  too few total blocks  exploit temporal locality not well  waste cache space and bandwidth/energy if spatial locality is not high 238 hit rate block size Large Blocks: Critical-Word and Subblocking  Large cache blocks can take a long time to fill into the cache  Idea: Fill cache block critical-word first  Supply the critical data to the processor immediately  Large cache blocks can waste bus bandwidth  Idea: Divide a block into subblocks  Associate separate valid and dirty bits for each subblock  Recall: When is this useful? 239 tagsubblockvsubblockv subblockvd d d Associativity  How many blocks can be present in the same index (i.e., set)?  Larger associativity  lower miss rate (reduced conflicts)  higher hit latency and area cost  Smaller associativity  lower cost  lower hit latency  Especially important for L1 caches  Is power of 2 associativity required? 240 associativity hit rate Recall: Higher Associativity (4-way)  4-way 241 Tag store Data store =?=? =?=? MUX MUX byte in block Logic Hit? Address tag index byte in block 3 bits1 b4 bits Higher Associativity (3-way)  3-way 242 Tag store Data store =?=? =? MUX MUX byte in block Logic Hit? Address tag index byte in block 3 bits1 b4 bits Recall: 8-way Fully Associative Cache 243 Tag store Data store =? =? =? =? =? =? =? =? MUX MUX byte in block Logic Hit? Address tag byte in block 3 bits5 bits 7-way Fully Associative Cache 244 Tag store Data store =? =? =? =? =? =? =? MUX MUX byte in block Logic Hit? Address tag byte in block 3 bits5 bits Classification of Cache Misses  Compulsory miss  first reference to an address (block) always results in a miss  subsequent references should hit unless the cache block is displaced for the reasons below  Capacity miss  cache is too small to hold all needed data  defined as the misses that would occur even in a fully- associative cache (with optimal replacement) of the same capacity  Conflict miss  defined as any miss that is neither a compulsory nor a capacity miss 245 How to Reduce Each Miss Type  Compulsory  Caching (only accessed data) cannot help; larger blocks can  Prefetching helps: Anticipate which blocks will be needed soon  Conflict  More associativity  Other ways to get more associativity without making the cache associative  Victim cache  Better, randomized indexing into the cache  Software hints for eviction/replacement/promotion  Capacity  Utilize cache space better: keep blocks that will be referenced  Software management: divide working set and computation such that each “computation phase” fits in cache 246 How to Improve Cache Performance  Three fundamental goals  Reducing miss rate  Caveat: reducing miss rate can reduce performance if more costly-to-refetch blocks are evicted  Reducing miss latency or miss cost  Reducing hit latency or hit cost  The above three together affect performance 247 Improving Basic Cache Performance  Reducing miss rate  More associativity  Alternatives/enhancements to associativity  Victim caches, hashing, pseudo-associativity, skewed associativity  Better replacement/insertion policies  Software approaches  Reducing miss latency/cost  Multi-level caches  Critical word first  Subblocking/sectoring  Better replacement/insertion policies  Non-blocking caches (multiple cache misses in parallel)  Multiple accesses per cycle  Software approaches 248 Software Approaches for Higher Hit Rate  Restructuring data access patterns  Restructuring data layout  Loop interchange  Data structure separation/merging  Blocking  … 249 Restructuring Data Access Patterns (I)  Idea: Restructure data layout or data access patterns  Example: If column-major  x[i+1,j] follows x[i,j] in memory  x[i,j+1] is far away from x[i,j]  This is called loop interchange  Other optimizations can also increase hit rate  Loop fusion, array merging, … 250 Poor code for i = 1, rows for j = 1, columns sum = sum + x[i,j] Better code for j = 1, columns for i = 1, rows sum = sum + x[i,j] Restructuring Data Access Patterns (II)  Blocking  Divide loops operating on arrays into computation chunks so that each chunk can hold its data in the cache  Avoids cache conflicts between different chunks of computation  Essentially: Divide the working set so that each piece fits in the cache  Also called Tiling 251 Data Reuse: An Example from GPU Computing  Same memory locations accessed by neighboring threads for (int i = 0; i < 3; i++){ for (int j = 0; j < 3; j++){ sum += gauss[i][j] * Image[(i+row-1)*width + (j+col-1)]; } } 252 Gaussian filter applied on every pixel of an image Lecture 22: GPU Programming (Spring 2018) https://www.youtube.com/watch?v=y40-tY5WJ8A Data Reuse: Tiling in GPU Computing  To take advantage of data reuse, we divide the input into tiles that can be loaded into shared memory (scratchpad memory) __shared__ int l_data[(L_SIZE+2)*(L_SIZE+2)]; … Load tile into shared memory __syncthreads(); for (int i = 0; i < 3; i++){ for (int j = 0; j < 3; j++){ sum += gauss[i][j] * l_data[(i+l_row-1)*(L_SIZE+2)+j+l_col-1]; } } 253Lecture 22: GPU Programming (Spring 2018) https://www.youtube.com/watch?v=y40-tY5WJ8A Naïve Matrix Multiplication (I)  Matrix multiplication: C = A x B  Consider two input matrices A and B in row-major layout  A size is M x P  B size is P x N  C size is M x N 254 A B C P M P N i jk k Naïve Matrix Multiplication (II)  Naïve implementation of matrix multiplication has poor cache locality 255 #define A(i,j) matrix_A[i * P + j] #define B(i,j) matrix_B[i * N + j] #define C(i,j) matrix_C[i * N + j] for (i = 0; i < M; i++){ // i = row index for (j = 0; j < N; j++){ // j = column index C(i, j) = 0; // Set to zero for (k = 0; k < P; k++) // Row x Col C(i, j) += A(i, k) * B(k, j); } } A B C P M P N i jk k Consecutive accesses to B are far from each other, in different cache lines. Every access to B is likely to cause a cache miss Tiled Matrix Multiplication (I)  We can achieve better cache locality by computing on smaller tiles or blocks that fit in the cache  Or in the scratchpad memory and register file if we compute on a GPU 256 A B C P M P N k k tile_dimtile_dimi j Lam+, \"The cache performance and optimizations of blocked algorithms,\" ASPLOS 1991. https://doi.org/10.1145/106972.106981 Bansal+, \"Chapter 15 - Fast Matrix Computations on Heterogeneous Streams,\" in \"High Performance Parallelism Pearls\", 2015. https://doi.org/10.1016/B978-0-12-803819-2.00011-2 Kirk & Hwu, \"Chapter 5 - Performance considerations,\" in \"Programming Massively Parallel Processors (Third Edition)\", 2017. https://doi.org/10.1016/B978-0-12-811986-0.00005-4 Tiled Matrix Multiplication (II)  Tiled implementation operates on submatrices (tiles or blocks) that fit fast memories (cache, scratchpad, RF) 257 #define A(i,j) matrix_A[i * P + j] #define B(i,j) matrix_B[i * N + j] #define C(i,j) matrix_C[i * N + j] for (I = 0; I < M; I += tile_dim){ for (J = 0; J < N; J += tile_dim){ Set_to_zero(&C(I, J)); // Set to zero for (K = 0; K < P; K += tile_dim) Multiply_tiles(&C(I, J), &A(I, K), &B(K, J)); } } Multiply small submatrices (tiles or blocks) of size tile_dim x tile_dim A B C P M P N k k tile_dimtile_dimi j Lam+, \"The cache performance and optimizations of blocked algorithms,\" ASPLOS 1991. https://doi.org/10.1145/106972.106981 Bansal+, \"Chapter 15 - Fast Matrix Computations on Heterogeneous Streams,\" in \"High Performance Parallelism Pearls\", 2015. https://doi.org/10.1016/B978-0-12-803819-2.00011-2 Kirk & Hwu, \"Chapter 5 - Performance considerations,\" in \"Programming Massively Parallel Processors (Third Edition)\", 2017. https://doi.org/10.1016/B978-0-12-811986-0.00005-4 Tiled Matrix Multiplication on GPUs 258Computer Architecture - Lecture 9: GPUs and GPGPU Programming (Fall 2017) https://youtu.be/mgtlbEqn2dA?t=8157 Restructuring Data Layout (I)  Pointer based traversal (e.g., of a linked list)  Assume a huge linked list (1B nodes) and unique keys  Why does the code on the left have poor cache hit rate?  “Other fields” occupy most of the cache line even though they are rarely accessed! 259 struct Node { struct Node* next; int key; char [256] name; char [256] school; } while (node) { if (nodekey == input-key) { // access other fields of node } node = nodenext; } Rarely accessed Frequently accessed Frequently accessed Rarely accessed Restructuring Data Layout (II)  Idea: separate rarely- accessed fields of a data structure and pack them into a separate data structure  Who should do this?  Programmer  Compiler  Profiling vs. dynamic  Hardware?  Who can determine what is frequently accessed? 260 struct Node { struct Node* next; int key; struct Node-data* node-data; } struct Node-data { char [256] name; char [256] school; } while (node) { if (nodekey == input-key) { // access nodenode-data } node = nodenext; } Improving Basic Cache Performance  Reducing miss rate  More associativity  Alternatives/enhancements to associativity  Victim caches, hashing, pseudo-associativity, skewed associativity  Better replacement/insertion policies  Software approaches  Reducing miss latency/cost  Multi-level caches  Critical word first  Subblocking/sectoring  Better replacement/insertion policies  Non-blocking caches (multiple cache misses in parallel)  Multiple accesses per cycle  Software approaches 261 Miss Latency/Cost  What is miss latency or miss cost affected by?  Where does the miss get serviced from?  What level of cache in the hierarchy?  Row hit versus row conflict in DRAM (bank/rank/channel conflict)  Queueing delays in the memory controller and the interconnect  Local vs. remote memory (chip, node, rack, remote server, …)  …  How much does the miss stall the processor?  Is it overlapped with other latencies?  Is the data immediately needed by the processor?  Is the incoming block going to evict a longer-to-refetch block?  … 262 Memory Level Parallelism (MLP)  Memory Level Parallelism (MLP) means generating and servicing multiple memory accesses in parallel [Glew’98]  Several techniques to improve MLP (e.g., out-of-order execution)  MLP varies. Some misses are isolated and some parallel How does this affect cache replacement? time A B C isolated miss parallel miss Traditional Cache Replacement Policies  Traditional cache replacement policies try to reduce miss count  Implicit assumption: Reducing miss count reduces memory- related stall time  Misses with varying cost/MLP breaks this assumption!  Eliminating an isolated miss helps performance more than eliminating a parallel miss  Eliminating a higher-latency miss could help performance more than eliminating a lower-latency miss 264 Misses to blocks P1, P2, P3, P4 can be parallel Misses to blocks S1, S2, and S3 are isolated Two replacement algorithms: 1. Minimizes miss count (Belady’s OPT) 2. Reduces isolated miss (MLP-Aware) For a fully associative cache containing 4 blocks S1P4 P3 P2 P1 P1 P2 P3 P4 S2 S3 An ExampleFewest Misses = Best Performance P3 P2 P1 P4 H H H H M H H H MHit/Miss Misses=4 Stalls=4 S1P4 P3 P2 P1 P1 P2 P3 P4 S2 S3 Time stall Belady’s OPT replacement M M MLP-Aware replacement Hit/Miss P3 P2 S1 P4 P3 P2 P1 P4 P3 P2 S2P4 P3 P2 S3P4 S1 S2 S3P1 P3 P2 S3P4 S1 S2 S3P4 H H H S1 S2 S3P4 H M M M H M M M Time stall Misses=6 Stalls=2 Saved cycles Cache Recommended: MLP-Aware Cache Replacement  How do we incorporate MLP/cost into replacement decisions?  How do we design a hybrid cache replacement policy?  Qureshi et al., “A Case for MLP-Aware Cache Replacement,” ISCA 2006. 267 Improving Basic Cache Performance  Reducing miss rate  More associativity  Alternatives/enhancements to associativity  Victim caches, hashing, pseudo-associativity, skewed associativity  Better replacement/insertion policies  Software approaches  …  Reducing miss latency/cost  Multi-level caches  Critical word first  Subblocking/sectoring  Better replacement/insertion policies  Non-blocking caches (multiple cache misses in parallel)  Multiple accesses per cycle  Software approaches  … 268 Lectures on Cache Optimizations (I) 269https://www.youtube.com/watch?v=OyomXCHNJDA&list=PL5Q2soXY2Zi9OhoVQBXYFIZywZXCPl4M_&index=3 Lectures on Cache Optimizations (II) 270https://www.youtube.com/watch?v=55oYBm9cifI&list=PL5Q2soXY2Zi9JXe3ywQMhylk_d5dI-TM7&index=6 Lectures on Cache Optimizations (III) 271https://www.youtube.com/watch?v=jDHx2K9HxlM&list=PL5PHm2jkkXmi5CxxI7b3JCL1TWybTDtKq&index=21 Lectures on Cache Optimizations  Computer Architecture, Fall 2017, Lecture 3  Cache Management & Memory Parallelism (ETH, Fall 2017)  https://www.youtube.com/watch?v=OyomXCHNJDA&list=PL5Q2soXY2Zi9OhoVQBX YFIZywZXCPl4M_&index=3  Computer Architecture, Fall 2018, Lecture 4a  Cache Design (ETH, Fall 2018)  https://www.youtube.com/watch?v=55oYBm9cifI&list=PL5Q2soXY2Zi9JXe3ywQMh ylk_d5dI-TM7&index=6  Computer Architecture, Spring 2015, Lecture 19  High Performance Caches (CMU, Spring 2015)  https://www.youtube.com/watch?v=jDHx2K9HxlM&list=PL5PHm2jkkXmi5CxxI7b3J CL1TWybTDtKq&index=21 272https://www.youtube.com/onurmutlulectures Multi-Core Issues in Caching Caches in a Multi-Core System 274 CORE 1L2 CACHE 0SHARED L3 CACHEDRAM INTERFACE CORE 0 CORE 2 CORE 3L2 CACHE 1L2 CACHE 2L2 CACHE 3DRAM BANKS DRAM MEMORY CONTROLLER Caches in a Multi-Core System 275Source: https://www.anandtech.com/show/16252/mac-mini-apple-m1-tested Apple M1, 2021 Caches in a Multi-Core System 276Source: https://twitter.com/Locuza_/status/1454152714930331652 Intel Alder Lake, 2021 Caches in a Multi-Core System 277https://wccftech.com/amd-ryzen-5000-zen-3-vermeer-undressed-high-res-die-shots-close-ups-pictured-detailed/ AMD Ryzen 5000, 2020 Core Count: 8 cores/16 threads L1 Caches: 32 KB per core L2 Caches: 512 KB per core L3 Cache: 32 MB shared Caches in a Multi-Core System 278https://youtu.be/gqAYMx34euU https://www.tech-critter.com/amd-keynote-computex-2021/ https://community.microcenter.com/discussion/5 134/comparing-zen-3-to-zen-2 Additional 64 MB L3 cache die stacked on top of the processor die - Connected using Through Silicon Vias (TSVs) - Total of 96 MB L3 cache AMD increases the L3 size of their 8-core Zen 3 processors from 32 MB to 96 MB 3D Stacking Technology: Example 279https://www.pcgameshardware.de/Ryzen-7-5800X3D-CPU-278064/Specials/3D-V -Cache-Release-1393125/ Caches in a Multi-Core System 280https://www.it-techblog.de/ibm-power10-prozessor-mehr-speicher-mehr-tempo-mehr-sicherheit/09/2020/ IBM POWER10, 2020 Cores: 15-16 cores, 8 threads/core L2 Caches: 2 MB per core L3 Cache: 120 MB shared Caches in a Multi-Core System 281https://www.tomshardware.com/news/infrared-photographer-photos-nvidia-ga102-ampere-silicon Nvidia Ampere, 2020 Cores: 128 Streaming Multiprocessors L1 Cache or Scratchpad: 192KB per SM Can be used as L1 Cache and/or Scratchpad L2 Cache: 40 MB shared Caches in a Multi-Core System 282https://developer.nvidia.com/blog/nvidia-hopper-architecture-in-depth/ Nvidia Hopper, 2022 L1 Cache or Scratchpad: 256KB per SM Can be used as L1 Cache and/or Scratchpad Cores: 144 Streaming Multiprocessors L2 Cache: 60 MB shared Caches in Multi-Core Systems  Cache efficiency becomes even more important in a multi- core/multi-threaded system  Memory bandwidth is at premium  Cache space is a limited resource across cores/threads  How do we design the caches in a multi-core system?  Many decisions and questions  Shared vs. private caches  How to maximize performance of the entire system?  How to provide QoS & predictable perf. to different threads in a shared cache?  Should cache management algorithms be aware of threads?  How should space be allocated to threads in a shared cache?  Should we store data in compressed format in some caches?  How do we do better reuse prediction & management in caches? 283 Private vs. Shared Caches  Private cache: Cache belongs to one core (a shared block can be in multiple caches)  Shared cache: Cache is shared by multiple cores 284 CORE 0 CORE 1 CORE 2 CORE 3 L2 CACHE L2 CACHE L2 CACHE DRAM MEMORY CONTROLLER L2 CACHE CORE 0 CORE 1 CORE 2 CORE 3 DRAM MEMORY CONTROLLER L2 CACHE Resource Sharing Concept and Advantages  Idea: Instead of dedicating a hardware resource to a hardware context, allow multiple contexts to use it  Example resources: functional units, pipeline, caches, buses, memory  Why? + Resource sharing improves utilization/efficiency  throughput  When a resource is left idle by one thread, another thread can use it; no need to replicate shared data + Reduces communication latency  For example, data shared between multiple threads can be kept in the same cache in multithreaded processors + Compatible with the shared memory programming model 285 Resource Sharing Disadvantages  Resource sharing results in contention for resources  When the resource is not idle, another thread cannot use it  If space is occupied by one thread, another thread needs to re- occupy it - Sometimes reduces each or some thread’s performance - Thread performance can be worse than when it is run alone - Eliminates performance isolation  inconsistent performance across runs - Thread performance depends on co-executing threads - Uncontrolled (free-for-all) sharing degrades QoS - Causes unfairness, starvation Need to efficiently and fairly utilize shared resources 286 Private vs. Shared Caches  Private cache: Cache belongs to one core (a shared block can be in multiple caches)  Shared cache: Cache is shared by multiple cores 287 CORE 0 CORE 1 CORE 2 CORE 3 L2 CACHE L2 CACHE L2 CACHE DRAM MEMORY CONTROLLER L2 CACHE CORE 0 CORE 1 CORE 2 CORE 3 DRAM MEMORY CONTROLLER L2 CACHE Shared Caches Between Cores  Advantages:  High effective capacity  Dynamic partitioning of available cache space  No fragmentation due to static partitioning  If one core does not utilize some space, another core can  Easier to maintain coherence (a cache block is in a single location)  Disadvantages  Slower access (cache not tightly coupled with the core)  Cores incur conflict misses due to other cores’ accesses  Misses due to inter-core interference  Some cores can destroy the hit rate of other cores  Guaranteeing a minimum level of service (or fairness) to each core is harder (how much space, how much bandwidth?) 288 Lectures on Multi-Core Cache Management 289https://www.youtube.com/watch?v=7_Tqlw8gxOU&list=PL5Q2soXY2Zi9OhoVQBXYFIZywZXCPl4M_&index=17 Lectures on Multi-Core Cache Management 290https://www.youtube.com/watch?v=c9FhGRB3HoA&list=PL5Q2soXY2Zi9JXe3ywQMhylk_d5dI-TM7&index=29 Lectures on Multi-Core Cache Management 291https://www.youtube.com/watch?v=Siz86__PD4w&list=PL5Q2soXY2Zi9JXe3ywQMhylk_d5dI-TM7&index=30 Lectures on Multi-Core Cache Management  Computer Architecture, Fall 2018, Lecture 18b  Multi-Core Cache Management (ETH, Fall 2018)  https://www.youtube.com/watch?v=c9FhGRB3HoA&list=PL5Q2soXY2Zi9JXe3ywQM hylk_d5dI-TM7&index=29  Computer Architecture, Fall 2018, Lecture 19a  Multi-Core Cache Management II (ETH, Fall 2018)  https://www.youtube.com/watch?v=Siz86__PD4w&list=PL5Q2soXY2Zi9JXe3ywQM hylk_d5dI-TM7&index=30  Computer Architecture, Fall 2017, Lecture 15  Multi-Core Cache Management (ETH, Fall 2017)  https://www.youtube.com/watch?v=7_Tqlw8gxOU&list=PL5Q2soXY2Zi9OhoVQBXY FIZywZXCPl4M_&index=17 292https://www.youtube.com/onurmutlulectures Lectures on Memory Resource Management 293https://www.youtube.com/watch?v=0nnI807nCkc&list=PL5Q2soXY2Zi9xidyIgBxUz7xRPS-wisBN&index=21 Lectures on Memory Resource Management  Computer Architecture, Fall 2020, Lecture 11a  Memory Controllers (ETH, Fall 2020)  https://www.youtube.com/watch?v=TeG773OgiMQ&list=PL5Q2soXY2Zi9xidyIgBxUz 7xRPS-wisBN&index=20  Computer Architecture, Fall 2020, Lecture 11b  Memory Interference and QoS (ETH, Fall 2020)  https://www.youtube.com/watch?v=0nnI807nCkc&list=PL5Q2soXY2Zi9xidyIgBxUz7 xRPS-wisBN&index=21  Computer Architecture, Fall 2020, Lecture 13  Memory Interference and QoS II (ETH, Fall 2020)  https://www.youtube.com/watch?v=Axye9VqQT7w&list=PL5Q2soXY2Zi9xidyIgBxU z7xRPS-wisBN&index=26  Computer Architecture, Fall 2020, Lecture 2a  Memory Performance Attacks (ETH, Fall 2020)  https://www.youtube.com/watch?v=VJzZbwgBfy8&list=PL5Q2soXY2Zi9xidyIgBxUz7 xRPS-wisBN&index=2 294https://www.youtube.com/onurmutlulectures Cache Coherence Cache Coherence  Basic question: If multiple processors cache the same block, how do they ensure they all see a consistent state? P1 P2 x Interconnection Network Main Memory 1000 The Cache Coherence Problem P1 P2 x Interconnection Network Main Memory ld r2, x 1000 1000 The Cache Coherence Problem P1 P2 x Interconnection Network Main Memory ld r2, x ld r2, x 1000 1000 1000 The Cache Coherence Problem P1 P2 x Interconnection Network Main Memory ld r2, x add r1, r2, r4 st x, r1 ld r2, x 1000 10002000 The Cache Coherence Problem P1 P2 x Interconnection Network Main Memory ld r2, x add r1, r2, r4 st x, r1 ld r2, x 1000 10002000 ld r5, x Should NOT load 1000 A Very Simple Coherence Scheme (VI)  Idea: All caches “snoop” (observe) each other’s write/read operations. If a processor writes to a block, all others invalidate the block.  A simple protocol: 301  Write-through, no- write-allocate cache  Actions of the local processor on the cache block: PrRd, PrWr,  Actions that are broadcast on the bus for the block: BusRd, BusWr PrWr / BusWr Valid BusWr Invalid PrWr / BusWr PrRd / BusRd PrRd/-- Lecture on Cache Coherence 302https://www.youtube.com/watch?v=T9WlyezeaII&list=PL5Q2soXY2Zi9xidyIgBxUz7xRPS-wisBN&index=38 Lecture on Memory Ordering & Consistency 303https://www.youtube.com/watch?v=Suy09mzTbiQ&list=PL5Q2soXY2Zi9xidyIgBxUz7xRPS-wisBN&index=37 Lecture on Cache Coherence & Consistency  Computer Architecture, Fall 2020, Lecture 21  Cache Coherence (ETH, Fall 2020)  https://www.youtube.com/watch?v=T9WlyezeaII&list=PL5Q2soXY2Zi9xidyIgBxUz7 xRPS-wisBN&index=38  Computer Architecture, Fall 2020, Lecture 20  Memory Ordering & Consistency (ETH, Fall 2020)  https://www.youtube.com/watch?v=Suy09mzTbiQ&list=PL5Q2soXY2Zi9xidyIgBxUz 7xRPS-wisBN&index=37  Computer Architecture, Spring 2015, Lecture 28  Memory Consistency & Cache Coherence (CMU, Spring 2015)  https://www.youtube.com/watch?v=JfjT1a0vi4E&list=PL5PHm2jkkXmi5CxxI7b3JCL 1TWybTDtKq&index=32  Computer Architecture, Spring 2015, Lecture 29  Cache Coherence (CMU, Spring 2015)  https://www.youtube.com/watch?v=X6DZchnMYcw&list=PL5PHm2jkkXmi5CxxI7b3 JCL1TWybTDtKq&index=33 304https://www.youtube.com/onurmutlulectures","libVersion":"0.3.2","langs":""}