{"path":"sem3/LinAlg/VRL/extra/plans/LinAlg-plan-w05.pdf","text":"Lecture plan Linear Algebra (401-0131-00L, HS24), ETH Z ¨urich Numbering of Sections, Definitions, Figures, etc. as in the Lecture Notes Week 5 LU and LUP decomposition (Section 3.4) LU decomposition: Gauss elimination, 3 × 3, no row exchanges:   1 0 0 0 1 0 0 −c32 1   ︸ ︷︷ ︸ subtract c32·(row 2) from (row 3)   1 0 0 0 1 0 −c31 0 1   ︸ ︷︷ ︸ subtract c31·(row 1) from (row 3)   1 0 0 −c21 1 0 0 0 1   ︸ ︷︷ ︸ subtract c21·(row 1) from (row 2) A = U. Multiplying it out: L−1, complicated ︷ ︸︸ ︷  1 0 0 −c21 1 0 c32c21 − c31 −c32 1   A = U ↓ (take inverse)   1 0 0 c21 1 0 c31 c32 1   ︸ ︷︷ ︸ L, simple ⇒ A = LU Always works: focus on A → U (Table 3.6 without lines 12 and 22 for b) Theorem 3.13: Let A be an m × m matrix on which Gauss elimination succeeds without row exchanges, resulting in an upper triangular matrix U . Let cij be the multiple of row j that we subtract from row i > j when we eliminate in column j. Then A = LU where L =      1 c21 1 ... . . . cm1 · · · cm,m−1 1      . L is lower triangular with 1’s on the diagonal. L is computed “on the side”: time still O(m 3). 1 Proof. Look at a fixed row i. Whenever we change row i, we subtract cij · (row j) from it, for some previous row j. At this point, row j is “finalized”. u11 · · · ← finalized (in U ) 0 u22 · · · ← finalized (in U ) 0 0 . . . ... row j 0 0 · · · ujj · · · ujm ← finalized (in U ) ... row i 0 0 · · · ⋆ij · · · ⋆im ← now subtract cij · (row j) What happens to row i? (row i) in A initially − ci1 · (row 1) in U step 1 − ci2 · (row 2) in U step 2 ... − ci,i−1 · (row i − 1) in U step i − 1 = (row i) in U finalized. Move all ” − · · · ” to the other side: (row i) in A is a linear combination of the first i rows of U . Matrix notation: (row i) of A = [ ci1 ci2 · · · ci,i−1 1 0 · · · 0 ] ︸ ︷︷ ︸ row vector U. For all rows of A: A =      1 c21 1 ... . . . cm1 · · · cm,m−1 1      ︸ ︷︷ ︸ L U. Solving Ax = b from A = LU : Ax = b : L U x︸︷︷︸ y = b. Solve Ly = b for y (forward substitution): O(m2) Solve U x = y for x (back substitution): O(m2) What if row exchanges are needed? LU-decomposition may not exist: [0 1 1 0 ] ︸ ︷︷ ︸ A = [ℓ11 0 ℓ21 ℓ22 ] ︸ ︷︷ ︸ L [u11 u12 0 u22 ] ︸ ︷︷ ︸ U has no solution L, U . 2 LUP decomposition: Official correctness proof of Gauss elimination (Section 3.4.3). Theorem 3.18: Let A be an m × m matrix with linearly independent columns, m ≥ 1. There exist three m × m matrices P, L, U such that P A = LU, where P is a permutation matrix, L a lower triangular matrix with 1’s on the diagonal, and U an upper triangular matrix with nonzero diagonal entries. Permutation matrix: matrix of linear transformation that reorders the entries of v  0 1 0 0 0 1 1 0 0    v1 v2 v3   =   v2 v3 v1   . L, P are computed “on the side”: time still O(m3). Ax = b solved in time O(m2) for every b. Gauss-Jordan elimination (Section 3.5) Ax = b → R0x = c with R0 in row echelon form; works for every system! 1111000000j1j2j3j4123456 REF(2, 3, 6, 8), r = 4 Definition 3.19: Let R = [rij]m n i=1,j=1 be an m × n matrix. R is in row echelon form (REF) if the following holds: There exist r ≤ m column indices 1 ≤ j1 < j2 < · · · < jr ≤ n such that: (i) For i = 1, 2, . . . , r, we have riji = 1 (1’s in gray). (ii) For all i, j, we have rij = 0 whenever i > r (completely white rows) or j < ji (partially white rows) or j = jk (0’s in gray) for some k > i. If r = m, R is in reduced row echelon form (RREF) (no completely white rows). Precise description: REF(j1, j2, . . . , jr) or RREF(j1, j2, . . . , jm). Columns j1, j2, . . . , jr: the first r standard unit vectors I (m × m): in RREF(1, 2, . . . , m) 0 (m × m): in RREF() (r = 0) Observation 3.20: A matrix R in REF(j1, j2, . . . , jr) has rank r. Proof. Columns j1, j2, . . . , jr are the independent ones. 3 Direct solution: if A in REF(j1, j2, . . . , jr) (rows i > r are zero) If bi ̸= 0 for some i > m: no solution! Otherwise: xj = { bi, if j = ji 0, otherwise. (canonical solution) 1111000000j1j2j3j4123456b1b2b3b400000000b1b2b3b4=Axb←←if ̸= 0 here, no solution Elimination: if A is not in REF • Ax = b → R0x = c (same solutions, R0 in REF) focus on A → R0 • For R0x = c, apply direct solution Like Gauss, except. . . . . . turn pivots into 1: r counts “downward steps” so far ↑↑↑↑ A =   2 4 2 2 −2 6 12 6 7 1 4 8 2 2 6   (r = 0) divide (row 1) by 2: ↓  1 2 1 1 −1 6 12 6 7 1 4 8 2 2 6   subtract 6·(row 1) from (row 2): ↓  1 2 1 1 −1 0 0 0 1 7 4 8 2 2 6   subtract 4·(row 1) from (row 3): ↓  1 2 1 1 −1 0 0 0 1 7 0 0 −2 −2 10   downward step made, next column! ↓  1 2 1 1 −1 0 0 0 1 7 0 0 −2 −2 10   (r = 1) 4 . . . embrace ugly case: no downward step, next column!   1 2 1 1 −1 0 0 0 1 7 0 0 −2 −2 10   (r = 1) exchange (row 2) and (row 3): ↓  1 2 1 1 −1 0 0 −2 −2 10 0 0 0 1 7   divide (row 2) by −2: ↓  1 2 1 1 −1 0 0 1 1 −5 0 0 0 1 7   . . . also eliminate above the pivot:   1 2 1 1 −1 0 0 1 1 −5 0 0 0 1 7   subtract 1·(row 2) from (row 1): ↓  1 2 0 0 4 0 0 1 1 −5 0 0 0 1 7   downward step made, next column! ↓  1 2 0 0 4 0 0 1 1 −5 0 0 0 1 7   (r = 2) subtract 1·(row 3) from (row 2): ↓  1 2 0 0 4 0 0 1 0 −12 0 0 0 1 7   m downward steps made, done! ↓ R0 =   1 2 0 0 4 0 0 1 0 −12 0 0 0 1 7   (r = 3) Theorem 3.21: Let A be an m × n matrix. There exists an invertible m × m matrix M such that R0 = M A is in REF. M : product of (invertible) row operation matrices: • row exchanges • row divisions • row subtractions (below and above the pivot) 5 Solving Ax = b: • A → R0 = M A, b → c = M b (like in Gauss, apply row operations also to b) • Ax = b and R0x = c have the same solutions (M is “undoable”, proof of Lemma 3.3 applies). • Use direct solution on R0x = c. Lemma 3.22: Let A be an m × n matrix, M an invertible m × m matrix, and R0 = M A in REF(j1, j2, . . . , jr). Then A has independent columns j1, j2, . . . , jr. Proof. Column j of A R0 is dependent ⇔ there is x in R n: Ax = 0 R0x = 0 , xj = −1, xk = 0 for k > j ︸ ︷︷ ︸ column j is linear combination of previous ones x works for A ⇔ x works for R0, since Ax = 0 and M Ax = 0 have the same solutions (proof of Lemma 3.3 with b = 0). A and R0 have the same (in)dependent columns. R0 has independent columns j1, j2, . . . , jr (Observation 3.20). Therefore, A has the same. If A is m × m, invertible: all columns are independent ⇒ R0 = M A in RREF(1, 2, . . . , m) ⇒ R0 = I ⇒ M = A−1. Computing the CR decomposition: Recall Theorem 2.23: A = C︸︷︷︸ m×r R︸︷︷︸ r×n . C submatrix of independent columns; R how to combine them to get all columns. Theorem 3.24: Let A be an m × n matrix, A = CR (according to Theorem 2.23), A → R0 = M A in REF(j1, j2, . . . , jr). Then • R = the first r rows of R0 (the nonzero rows of R0). • C = columns j1, j2, . . . , jr of A (the independent columns of A) Proof. R0 = M CR︸︷︷︸ A . • C has columns j1, j2, . . . , jr of A (the independent ones by Lemma 3.22). • M C has columns j1, j2, . . . , jr of R0 = M A: the unit vectors e1, e2, . . . , er. 6 R0 = M CR =     I︸︷︷︸ r×r 0︸︷︷︸ (m−r)×r     ︸ ︷︷ ︸ M C R =     R︸︷︷︸ r×n 0︸︷︷︸ (m−r)×n     ︸ ︷︷ ︸ R0 . Verify this on   1 2 0 3 2 4 1 4 3 6 2 5   ︸ ︷︷ ︸ A =   1 0 2 1 3 2   ︸ ︷︷ ︸ C [1 2 0 3 0 0 1 −2 ] ︸ ︷︷ ︸ R from Section 2.2.3 by doing Gauss-Jordan on A! 7","libVersion":"0.5.0","langs":""}