{"path":"sem3/LinAlg/UE/s/LinAlg-u13-s.pdf","text":"D-INFK Linear Algebra HS 2023 Afonso Bandeira Bernd G¨artner Solution for Assignment 13 1. a) Let x ∈ Rn \\ {0} be an eigenvector of A + B corresponding to eigenvalue λ(A+B) min . By using our knowledge about Rayleigh quotients (Proposition 6.3.10), we get λ(A+B) min = x⊤(A + B)x x⊤x = x⊤Ax x⊤x + x⊤Bx x⊤x 6.3.10 ≥ λ(A) min + λ(B) min. b) Since both A and B are positive semidefinite, we have λ(A) min ≥ 0 and λ(B) min ≥ 0. Using our result from the previous subtask, we conclude that λ (A+B) min ≥ 0. Hence, A + B is positive semidefinite. c) This is analogous to the proof in the previous subtask: since both A and B are positive definite, we have λ(A) min > 0 and λ (B) min > 0. Using our result from the subtask a), we conclude that λ(A+B) min > 0. Hence, A + B is positive definite. Remark: Note that we actually only need one of A and B to be positive definite, as long as the other one is still positive semidefinite. 2. Consider first the r × n matrix B = ΣrV ⊤ r with rank r. In particular, B has full row rank and hence B† = B⊤(BB⊤) −1 = VrΣr(ΣrV ⊤ r VrΣr) −1 = VrΣr(Σ2 r )−1 = VrΣ−1 r where we have used Definition 4.5.3, the fact that Σr is a diagonal matrix, and the fact that V ⊤ r Vr = I. Similarly, the m × r matrix Ur has full column rank r and hence we get U † r = (U ⊤ r Ur)−1U ⊤ r = IU ⊤ r = U ⊤ r by Definition 4.5.1 and the fact that U ⊤ r Ur = I. Finally, we conclude that A† = B†U † r = VrΣ−1 r U ⊤ r by Proposition 4.5.9. 3. a) The main idea is to plug in the SVD of A. A crucial observation that we will need is that by orthogonality of U , we have ∥ ∥ U ⊤v ∥ ∥2 2 = (U ⊤v)⊤(U ⊤v) = v⊤U U ⊤v = v⊤v = ∥ v ∥ 2 2 for all v ∈ Rm. Equipped with this observation, we calculate min x∈Rn ∥Ax − b∥ 2 2 = min x∈Rn ∥U ΣV ⊤x − b∥2 2 = min x∈Rn ∥U ⊤U ΣV ⊤x − U ⊤b∥2 2 = min x∈Rn ∥ΣV ⊤x − U ⊤b∥ 2 2 = min y∈Rn ∥Σy − c∥ 2 2 where we have substituted y = V ⊤x in the end (which works because V ⊤ is invertible). b) Consider the expression ∥Σy − c∥2 2 and observe that we can write it as ∥Σy − c∥ 2 2 = n∑ i=1(Σiiyi − ci)2 = r∑ i=1(σiyi − ci)2 + n∑ i=r+1 c 2 i . We are looking to choose y such that this expression is minimized. Clearly, there is nothing that we can do about the term ∑n i=r+1 c2 i . But by choosing yi = ci/σi for all i ∈ [r], we get ∑r i=1(σiyi − ci)2 = 0. Hence, this choice of y must be optimal. Concretely, we conclude that the optimal solution is y∗ =           c1/σ1 ... cr/σr 0 ... 0           = arg min y∈Rn ∥Σy − c∥ 2 2. c) In subtask a), we substituted y = V ⊤x. Hence, it would make sense to guess that x∗ = V y∗. Indeed, we can verify that with this choice of x∗ we get ∥Σy∗−c∥2 2 = ∥ΣV ⊤x∗−c∥2 2 = ∥U ΣV ⊤x∗−U U ⊤b∥2 2 = ∥U ΣV ⊤x∗−U U ⊤b∥2 2 = ∥Ax∗−b∥ 2 2 and by min x∈Rn ∥Ax − b∥2 2 = min y∈Rn ∥Σy − c∥2 2 and optimality of y∗ we conclude that x∗ is optimal, i.e. x∗ = arg min x∈Rn ∥Ax∗ − b∥2 2. 4. a) We prove this by direct calculation ∥ x ∥2 2 = n∑ i=1 x2 i ≤ n∑ i=1 n∑ j=1 |xi||xj| = ( n∑ i=1 |xi|) 2 = ∥ x ∥2 1 . Observe that the inequality ∑n i=1 x2 i ≤ ∑n i=1 ∑n j=1 |xi||xj| holds because all terms appearing on the left actually appear on the right as well (but on the right we have some additional non-negative terms). b) Without loss of generality, assume that all entries in x are non-negative (if there was a negative entry, simply switch its sign and observe that both norms remain the same). Next, observe that ∥ x ∥1 = ∑n i=1 |xi| = ∑n i=1 xi = 1⊤x where 1 ∈ Rn is the all-ones vector. By Cauchy-Schwarz, we obtain 1⊤x ≤ ∥ 1 ∥2 ∥ x ∥2. It remains to calculate ∥ 1 ∥2 = ( ∑n i=1 1) 1 2 = √n to conclude that ∥ x ∥1 = 1 ⊤x ≤ ∥ 1 ∥2 ∥ x ∥2 = √n ∥ x ∥2 . 5. a) Recall that the trace of a matrix is the sum of its diagonal entries. Consider the matrix A⊤A. The j- th diagonal entry of A⊤A is exactly the norm of the j-th column of A which is given by ∑m i=1 A2 ij. Hence, the trace of A⊤A is given by Tr(A⊤A) = n∑ j=1 m∑ i=1 A 2 ij = m∑ i=1 n∑ j=1 A 2 ij = ∥ A ∥ 2 F . b) By Remark 7.1.13 we know that the squared singular values of A are the eigenvalues of the matrix A⊤A. Moreover, by Proposition 6.1.11 we know that the trace of A⊤A is equal to the sum of its eigenvalues. Hence, we conclude Tr(A⊤A) = min{m,n}∑ i=1 σ2 i and the result follows by combining this with the previous subtask. c) By definition, we have ∥ A ∥op = max x∈Rn ∥ x ∥2=1 ∥ Ax ∥2 . Now observe that we can rewrite the squared version of this as max x∈Rn ∥ x ∥ 2 2=1 ∥ Ax ∥ 2 2 = max x∈Rn\\{0} ∥ Ax ∥ 2 2 ∥ x ∥ 2 2 = max x∈Rn\\{0} x⊤A⊤Ax x⊤x . The matrix A⊤A is symmetric and its largest eigenvalue is σ2 1, hence we get maxx∈Rn\\{0} x⊤A⊤Ax x⊤x = σ2 1 by Proposition 6.3.10. It remains to observe that arg max x∈Rn ∥ x ∥2=1 ∥ Ax ∥2 = arg max x∈Rn ∥ x ∥ 2 2=1 ∥ Ax ∥2 2 and hence ∥ A ∥op = max x∈Rn ∥ x ∥2=1 ∥ Ax ∥2 = √ max x∈Rn ∥ x ∥ 2 2=1 ∥ Ax ∥ 2 2 = √ σ2 1 = σ1. d) This follows from b) and c) as ∥ A ∥op = σ1 = √ σ2 1 ≤ √ √ √ √ min{m,n}∑ i=1 σ2 i = ∥ A ∥F . e) Using previous subtasks, we obtain ∥ A ∥2 F = min{m,n}∑ i=1 σ2 i ≤ min{m, n}σ2 1 and hence ∥ A ∥F ≤ √ min{m, n}σ1 = √ min{m, n} ∥ A ∥op .","libVersion":"0.3.2","langs":""}