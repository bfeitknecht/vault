{"path":"sem4/W&S/VRL/extra/W&S-script-2-en.pdf","text":"Chapter 1: Estimators (Pages 4–12) ETH Z¨urich Lecture Notes (Translated) V. Tassion (based on a previous version from M. Schweizer) D-INFK Spring 2022 (Updated: May 24, 2022) Introduction and Basic Ideas Basic Ideas: One has observed data and wishes to draw inferences about the underlying mechanism generating these data. A common first step is to present the data graphically. This is often useful to form an initial impression and to generate ideas. (Such methods belong to descriptive statistics, which are not treated here.) We focus on inductive statistics in the following. The basic idea is simple: The data x1, x2, . . . , xn are viewed as realisations of random variables X1, X2, . . . , Xn, and (under suitable assumptions) one seeks statements about the distribution of X1, . . . , Xn. Important: Always clearly distinguish between the data x1, . . . , xn (lowercase, usually numbers) and the generating mechanism X1, . . . , Xn (uppercase, i.e., random variables on Ω). As William James (1842–1910) remarked, “We must be careful not to confuse data with the abstractions we use to analyze them.” The collection of observations (or random variables) is called a sample, and the number n is the sample size. A typical analysis proceeds by finding a suitable model for the data. The model is described by a (possibly high-dimensional) parameter θ ∈ Θ, and to use the concepts and 1 notation precisely, one must specify exactly how probability-theoretic statements depend on θ. Usually, one considers a family of probability spaces; that is, one has a fixed base space (Ω, F) and for each θ ∈ Θ a probability measure Pθ. One may imagine that ‘nature’ chooses a parameter θ ∈ Θ along with a stochastic mechanism Pθ. As statisticians we do not know which θ has been chosen; we thus treat the data x1, . . . , xn as outcomes of the random variables X1, . . . , Xn under the unknown mechanism Pθ, and we attempt to draw conclusions about θ. A typical parametric statistical analysis comprises the following steps: 1. Descriptive Statistics: Graphical methods are used to form an initial idea for choosing an appropriate model. (We do not discuss this step further.) 2. Choice of a (Parametric) Model: Specify the parameter space Θ and the family (Pθ)θ∈Θ of models. 3. Parameter Estimation: Based on the data, choose the best-fitting model. For this, an estimator is used—a function mapping the data x1, . . . , xn to a parameter value θ ∈ Θ. 4. Goodness-of-Fit Testing: Test whether the chosen parameter θ or model Pθ fits the data well using an appropriate statistical test. 5. Assessing Reliability of the Estimates: Instead of a single parameter value, one may specify a region in Θ (a confidence interval) such that, with a certain probability, all the models within that region are in agreement with the data. 1 Basic Concepts of Estimation We wish to estimate an unknown parameter θ based on a sample X1, X2, . . . , Xn. 1.1 Definition of an Estimator Definition 1.1 (Estimator). An estimator is a random variable T : Ω −→ R of the form T = t(X1, X2, . . . , Xn), where t : R n −→ R is a measurable function. Inserting the observed data x1, x2, . . . , xn (with xi = Xi(ω)) 2 yields the estimate t(x1, . . . , xn) for θ. Remark 1.1. It is essential to distinguish between the estimator (a random variable) and the estimate (the realised value when data are plugged in). Example 1.1 (Tea Tasting Lady). An English lady claims that when drinking tea with milk she can tell by taste whether milk or tea was poured first into the cup. To test her claim, we ask her over n days to classify two cups (one of type 1 and one of type 2) by indicating in which cup the milk was poured first. We record the outcomes as x1, x2, . . . , xn ∈ {0, 1} (with 1 indicating a correct and 0 an incorrect classification). These data are treated as realisations of the random variables X1, X2, . . . , Xn. Let Sn = n∑ i=1 Xi, be the random number of correct classifications, and denote the observed sum by sn. As- suming the Xi are i.i.d. ∼ Ber(θ) with unknown success probability θ ∈ [0, 1], we have Sn ∼ Bin(n, θ). Natural choices for estimators are: 1. Last Observation Estimator: T (1) = Xn. 2. Sample Mean Estimator: T (2) = 1 n ∑n i=1 Xi. For the observed data, these yield the estimates: t (1)(x1, . . . , xn) = xn and t (2)(x1, . . . , xn) = 1 n n∑ i=1 xi. 1.2 Bias and Mean Squared Error The estimator T is a random variable whose distribution (under Pθ) depends on the unknown parameter θ. Definition 1.2 (Unbiased Estimator). An estimator T is called unbiased for θ if, for all θ ∈ Θ, Eθ[T ] = θ. 3 Definition 1.3 (Bias and MSE). For θ ∈ Θ, the bias of an estimator T is defined as Biasθ(T ) = Eθ[T ] − θ. The mean squared error (MSE) of T is defined as MSEθ(T ) = Eθ[ (T − θ) 2] . In fact, MSEθ(T ) = Varθ(T ) + ( Biasθ(T ) )2. For an unbiased estimator, the MSE equals the variance. Example 1.2 (Tea Tasting Lady (Continued)). Both estimators T (1) = Xn and T (2) = 1 n ∑n i=1 Xi are unbiased: Eθ[ T (1)] = Eθ[Xn] = θ, and Eθ[T (2)] = 1 n n∑ i=1 Eθ[Xi] = θ. However, their variances are: Varθ[ T (1)] = θ(1 − θ), Varθ[ T (2)] = 1 nθ(1 − θ). Thus, the sample mean T (2) has a smaller variance since it uses the full sample informa- tion. ♢ 1.3 Maximum Likelihood Estimation (MLE) In this section, we introduce a systematic method for determining estimators. This method yields results that are both plausible and have good properties. Assume we know the joint distribution of X1, . . . , Xn under Pθ. For i.i.d. data, the joint probability (or density) is given by the product of the individual probabilities (or densities). For a given sample (x1, . . . , xn), the function L(x1, . . . , xn; θ) is called the likelihood function for θ. Definition 1.4 (Likelihood Function). For the observed sample (x1, . . . , xn), the likelihood 4 function is defined by L(x1, . . . , xn; θ) =    n∏ i=1 pXi(xi; θ), if Xi are discrete, n∏ i=1 fXi(xi; θ), if Xi are continuous. Definition 1.5 (Maximum Likelihood Estimator (MLE)). The maximum likelihood es- timator of θ is defined as ˆθML(x1, . . . , xn) ∈ arg max θ∈Θ L(x1, . . . , xn; θ). In practice, one maximises the log-likelihood function, ℓ(θ; x1, . . . , xn) = log L(x1, . . . , xn; θ), and then obtains the estimator by replacing the data with the random variables: TML = tML(X1, . . . , Xn). Example 1.3 (Bernoulli Distribution). Let X1, . . . , Xn be i.i.d. ∼ Ber(p) with unknown p ∈ (0, 1). The probability mass function is pX(x; p) = px(1 − p)1−x, x ∈ {0, 1}. Hence, the likelihood function is L(x1, . . . , xn; p) = n∏ i=1 pxi(1 − p)1−xi = p ∑n i=1 xi (1 − p)n−∑n i=1 xi. Taking the logarithm, ℓ(p; x1, . . . , xn) = ( n∑ i=1 xi ) log p + ( n − n∑ i=1 xi ) log(1 − p). Differentiating with respect to p: d dp ℓ(p; x1, . . . , xn) = ∑n i=1 xi p − n − ∑n i=1 xi 1 − p = 0. Solving, ∑ xi p = n − ∑ xi 1 − p =⇒ n∑ i=1 xi(1 − p) = ( n − n∑ i=1 xi ) p. 5 Thus, n∑ i=1 xi = np =⇒ ˆp = 1 n n∑ i=1 xi. Therefore, the MLE for p is given by TML = 1 n n∑ i=1 Xi. ♢ 1.4 Models with Multiple Parameters Thus far, our discussion has focused on models with a single parameter θ ∈ R. However, many situations require models with multiple parameters θ1, θ2, . . . , θm, where m ≥ 2. We now develop a general theory for such cases. Consider the parameter space Θ ⊂ Rm, where m is the number of parameters. The stochastic model is given by a family of probability measures (Pθ)θ∈Θ, and our goal is to estimate the vector θ = (θ1, θ2, . . . , θm). All previous definitions (of estimator, bias, MSE, MLE, etc.) extend naturally to this setting. Example 1.4 (Normal Distribution). Let X1, . . . , Xn be i.i.d. ∼ N (µ, σ2). Here, the unknown parameter is θ = (µ, σ2), so that m = 2. We wish to estimate both µ and σ2. The density function for Xi is fXi(x; µ, σ2) = 1 √2πσ2 exp (−(x − µ)2 2σ2 ). Thus, the likelihood function is: L(x1, . . . , xn; µ, σ2) = n∏ i=1 1 √ 2πσ2 exp (−(xi − µ)2 2σ2 ) . Taking the logarithm, log L(x1, . . . , xn; µ, σ2) = −n 2 log(2π) − n 2 log(σ2) − 1 2σ2 n∑ i=1 (xi − µ)2. 6 Differentiation with Respect to µ: Differentiate with respect to µ: ∂ ∂µ log L(x1, . . . , xn; µ, σ2) = 1 σ2 n∑ i=1 (xi − µ) = 0. Hence, ˆµ = ¯xn = 1 n n∑ i=1 xi. Differentiation with Respect to σ2: Differentiate with respect to σ2: ∂ ∂σ2 log L(x1, . . . , xn; µ, σ2) = − n 2σ2 + 1 2σ4 n∑ i=1 (xi − ¯xn)2 = 0. Solving for σ2 gives: ˆσ2 = 1 n n∑ i=1 (xi − ¯xn) 2. By expanding the square, we may also write: ˆσ2 = 1 n n∑ i=1 x2 i − (¯xn) 2. Remark 1.2. The maximum likelihood estimator ˆσ2 is not unbiased since E[ˆσ2] = n − 1 n σ2 < σ2. A commonly used unbiased estimator for σ2 is S2 = 1 n − 1 n∑ i=1 (Xi − ¯Xn) 2. Furthermore, the estimator T = (T1, T2) with T1 = ˆµ = ¯Xn and T2 = ˆσ2 = 1 n n∑ i=1 (Xi − ¯Xn) 2 = 1 n n∑ i=1 X 2 i − ( ¯Xn) 2, is, in general, also the so-called moment estimator for (E[X], Var[X]) in any model Pθ where X1, . . . , Xn are i.i.d. However, this estimator has the general drawback that it is 7 not unbiased for (E[X], Var[X]). In fact, while Eθ[T1] = Eθ[Xn] = 1 n n∑ i=1 Eθ[Xi] = Eθ[X], we obtain Eθ[(Xn)2] = 1 n2 n∑ i,k=1 Eθ[XiXk] = 1 n2 ( n n∑ i=1 Eθ[X 2 i ] + ∑ i̸=k Eθ[Xi]Eθ[Xk] ) , and, due to independence (for i ̸= k), Eθ[XiXk] = Eθ[Xi]Eθ[Xk] = ( Eθ[X] )2. Thus, one can show that Eθ[T2] = 1 n n∑ i=1 Eθ[X 2 i ] − Eθ[(Xn) 2] = n − 1 n Varθ[X]. To obtain an unbiased estimator for (E[X], Var[X]), one typically uses T ′ 1 = T1 = ¯Xn, T ′ 2 = n n − 1 T2 = 1 n − 1 n∑ i=1 (Xi − ¯Xn)2. The estimator T ′ 2 is often denoted by S2 and is called the empirical sample variance. ♢ 2 Confidence Intervals 2.1 Definition In the preceding chapter we introduced methods for estimating unknown parameters using formulas. A natural question is: How reliable are these estimators? For example, suppose we toss a coin n times without knowing the probability p of heads. If we observe, say, 70 heads, then the maximum likelihood estimator is TML = 0.7. But how far can TML deviate from the true value of p? To answer such questions, we introduce the concept of a confidence interval. Definition 2.1 (Confidence Interval). Let α ∈ [0, 1]. A confidence interval for θ with confidence level 1 − α is a random interval I = [A, B], 8 with endpoints A = a(X1, . . . , Xn), B = b(X1, . . . , Xn), where a, b : R n → R, such that for all θ ∈ Θ Pθ[A ≤ θ ≤ B] ≥ 1 − α. Remark 2.1. In the definition above, the parameter θ is nonrandom (fixed but unknown), while the endpoints A and B are random variables (functions of the data). Example 2.1 (Confidence Interval for a Normal Model with Known Variance). Assume we have i.i.d. random variables X1, . . . , Xn ∼ N (m, 1), i.e. a normal model with known variance σ2 = 1 but with unknown mean m. One may show that the maximum likelihood estimator is the sample mean T = ¯Xn = 1 n n∑ i=1 Xi. We now seek a confidence interval for m of the form I = [ T − c √n, T + c √n ] , where c > 0 is a constant independent of n. Note that Pθ[T − c √n ≤ m ≤ T + c √ n ] = Pθ[−c ≤ √n(T − m) ≤ c ]. Since Z = √n(T − m) ∼ N (0, 1), it follows that Pθ[−c ≤ Z ≤ c] = 2Φ(c) − 1. Consulting the standard normal table shows that 2Φ(1.96) − 1 ≥ 0.95, so by choosing c = 1.96 we obtain a 95%-confidence interval: I = [T − 1.96 √n , T + 1.96 √n ]. What does this mean exactly? Imagine that we perform n measurements of a physical quantity. For example, suppose we wish to determine, at room temperature, the temperature at which water begins to boil. The characteristics of the thermometer suggest that each measurement can be modeled 9 by a normally distributed random variable N (m, 1), where m is the (unknown) boiling temperature. We perform a series of measurements—say, x1 = 99.2, x2 = 98.7, . . . . After n = 100 successive trials, we calculate the empirical average ˆm(x) = x1 + · · · + xn n = 99.106. The confidence interval obtained above thus indicates that, assuming the stochastic model is correct, the true value m lies (with 95% probability) in the interval [ 99.106 − 0.196, 99.106 + 0.196 ] = [98.910, 99.302]. What are the key points? In the above example the most important observation is that the random variable Z = √n(T − m) is normally distributed for every value of the unknown parameter θ. In general, one may attempt to obtain a confidence interval for a parameter θ by first determining an estimator T for θ. Next, one seeks to find a random variable of the form Z = f (T, θ) whose distribution can be explicitly determined and that does not depend on θ. This is generally easier when the random variables X1, . . . , Xn are normally distributed since operations on normally distributed random variables are well understood—for instance, we have used above that the sum of independent normally distributed random variables is itself normally distributed. In the next section we will introduce new distributions that arise from operations on normally distributed random variables. 2.2 Distribution Statements In many situations it is useful or necessary to know the distribution of an estimator (or a function thereof) under Pθ, for every θ ∈ Θ or for certain values of θ. There are only a few exact general results; for the normal distribution, precise results are available. Definition 2.2 (Chi-Squared Distribution). A continuous random variable X is said to be chi-squared distributed with m degrees of freedom if its density is given by fX(y) = 1 2m/2Γ(m/2) y m 2 −1e −y/2, y ≥ 0. We write X ∼ χ2 m. 10 Here the Gamma function is defined as Γ(v) = ∫ ∞ 0 t v−1e−t dt, and for n ∈ N, Γ(n) = (n − 1)!. Remark 2.2. The chi-squared distribution with m degrees of freedom is a special case of the Gamma(α, λ)-distribution with α = m 2 and λ = 1 2. For m = 2, it corresponds to an exponential distribution with parameter 1 2. Theorem 2.1 (Sum of Squares Theorem). If X1, X2, . . . , Xm are i.i.d. ∼ N (0, 1), then Y = m∑ i=1 X 2 i ∼ χ 2 m. Definition 2.3 (t-Distribution). A continuous random variable X is said to be t-distributed with m degrees of freedom if its density is given by fX(x) = Γ ( m+1 2 ) √mπ Γ ( m 2 ) (1 + x2 m )− m+1 2 , x ∈ R. We write X ∼ tm. Remark 2.3. For m = 1 the t-distribution is equivalent to a Cauchy distribution, and as m → ∞ it converges asymptotically to the standard normal distribution N (0, 1). Like the N (0, 1) distribution, the t-distribution is symmetric about 0; however, it is heavy-tailed (i.e. its density decays more slowly to 0 as |x| → ∞), and this effect is more pronounced the smaller m is. Theorem 2.2 (Satz 2.6). Let X and Y be independent random variables with X ∼ N (0, 1) and Y ∼ χ2 m. Then the quotient Z := X √ Y /m is t-distributed with m degrees of freedom. 2.3 Normal Model with Unknown Variance and Mean For normally distributed samples, we have exact distributional statements that we will also use in hypothesis testing later. Recall the definitions of the sample mean and the 11 sample variance: ¯Xn = 1 n n∑ i=1 Xi, S2 = 1 n − 1 n∑ i=1 (Xi − ¯Xn)2. Theorem 2.3. If X1, . . . , Xn are i.i.d. ∼ N (m, σ2), then ¯Xn and S2 are independent. Remark 2.4. (Proof: See Rice, Section 6.3.) Example 2.2 (Ostrich Eggs). Mr. Smith and Dr. Thurston, two Australian researchers, are debating the average weight of ostrich eggs. They agree that the weights can be ap- proximately modeled as normally distributed. Mr. Smith claims that the mean weight is 1100 g, while Dr. Thurston contends that it is 1200 g. To settle their dispute, they collect n = 8 eggs with the following weights (in grams): 1090, 1150, 1170, 1080, 1210, 1230, 1180, 1140. These data are viewed as realizations of i.i.d. random variables X1, . . . , X8 ∼ N (m, σ2). The natural estimators for m and σ2 are the sample mean ¯X8 = 1 8 8∑ i=1 Xi, and the sample variance S2 = 1 7 8∑ i=1 (Xi − ¯X8)2. A confidence interval for m is constructed in the form C(X1, . . . , X8) = [ ¯X8 − t7,1−α/2 S √ 8 , ¯X8 + t7,1−α/2 S √8 ], where t7,1−α/2 is the (1 − α/2)-quantile of the t-distribution with 7 degrees of freedom. For example, if 1 − α = 99%, and the observed values are ¯x8 = 1156.25, s = 52.90, t7,0.995 = 3.499, the resulting confidence interval for m is approximately [1090.81, 1221.69]. Thus, both the claims of 1100 g and 1200 g are plausible with these data. To construct a confidence interval for σ2, we use the fact that (8 − 1)S2 σ2 ∼ χ 2 7. If χ2 7,γ denotes the γ-quantile of a χ 2 7 distribution, then a 1 − α confidence interval for σ2 12 is given by C(X1, . . . , X8) = [ 7S2 χ2 7,1−α/2 , 7S2 χ 2 7,α/2 ] . For instance, with 1 − α = 95%, if s2 = 2798.21, χ2 7,0.025 = 1.69, χ2 7,0.975 = 16.01, one obtains the confidence interval for σ2, and by taking square roots, the interval for σ is approximately [34.98, 107.66]. Remark 2.5. In this example, the confidence intervals are exact because precise distri- butional results are available. In many other situations, one can only obtain approximate confidence intervals using the central limit theorem. 2.4 Approximate Confidence Intervals A general approximate approach is provided by the central limit theorem (CLT). Often, an estimator T is a function of a sum, say, T = 1 n n∑ i=1 Yi. By the CLT, for large n, n∑ i=1 Yi ≈ N (n E[Yi], n Var[Yi] ), which can be used to approximate the distribution of T and, hence, to construct approx- imate confidence intervals. Example 2.3 (Tea Tasting Lady (Approximate Confidence Intervals)). Suppose that in n = 10 trials the tea tasting lady correctly classifies 6 pairs (i.e., s = 6). In any model Pθ, the number of successes S10 is Bin(10, θ). We wish to obtain a confidence interval for the unknown parameter θ. By the central limit theorem, for large n, S10 ≈ N (10 θ, 10 θ(1 − θ) ). Define the standardized statistic S∗ 10 = S10 − 10θ √10 θ(1 − θ) ≈ N (0, 1). 13 For θ = θ0 = 1 2, this becomes T = S10 − 5 √10 · 1 4 = 2S10 − 10 √10 ≈ N (0, 1) under Pθ0. To perform a two-sided test at a given level α, choose the critical region K = (−∞, −c) ∪ (c, ∞), so that Pθ0(|T | > c) ≈ α. Since the distribution of T under Pθ0 is symmetric, we have α ≈ 2 (1 − Φ(c)), and hence, c ≈ Φ−1(1 − α 2 ). For example, if α = 0.01, then c ≈ Φ −1(0.995) = 2.576. One can then derive an approximate confidence interval for θ by solving |S10 − 10θ| ≤ c √ 10 θ(1 − θ). There are several methods to solve this inequality: Method 1: Assume θ(1 − θ) ≈ 1 4 (its maximum value). Then the inequality simplifies to |S10 − 10θ| ≤ c 2 √ 10, yielding the approximate confidence interval [S10 − c 2 √ 10, S10 + c 2 √10 ] . Method 2: Use the exact approximate distribution S10 ∼ N (10θ, 10 θ(1 − θ) ), 14 and write the approximate confidence interval for θ as θ ≈ S10 10 ± c √10 √ θ(1 − θ) 10 . Then substitute θ by its estimate S10 10 to obtain a “double approximate” interval. For instance, with 1 − α = 95% (so that c = 1.96) and an observed s = 6, the estimated θ is 0.6. Using Method 1, one obtains an interval approximately [0.290, 0.910], and Method 2 gives an interval about [0.296, 0.904]. (Solving the quadratic equation exactly might yield an interval such as [0.3127, 0.8318].) For larger sample sizes (say, n = 100 with s = 60), the approximate intervals become much narrower and tend to agree closely across methods. 3 Tests 3.1 Null and Alternative Hypotheses The starting point is, as in the previous section, a sample X1, . . . , Xn. We again consider a family of probability measures Pθ with θ ∈ Θ that describes our possible models. (Note that θ can be unidimensional or multidimensional.) We often have a prior belief about where in Θ the correct (but unknown) parameter θ might lie, and we wish to test this belief using the data. The basic problem is to decide between two competing classes of models – namely, the null hypothesis and the alternative hypothesis. More precisely, one sets Null hypothesis H0 : θ ∈ Θ0, Alternative hypothesis HA : θ ∈ ΘA, with Θ0 ∩ ΘA = ∅. (If no explicit alternative is specified, one takes ΘA = Θ \\ Θ0.) When Θ0 or ΘA consists of a single value θ0 or θA, they are called simple; otherwise, they are called composite. In explicit terms, the null hypothesis states: H0 : “the true (but unknown) parameter θ lies in the set Θ0.” and the alternative hypothesis HA : “the true (but unknown) parameter θ lies in the set ΘA.” Example 3.1 (Tea Tasting Lady). An English lady claims that when drinking tea with milk she can, by taste alone, distinguish whether the milk or the tea was poured into the cup first. How can one verify whether this claim is true? 15 As in the previous section, we ask the lady over n days to classify two cups (one of type 1 and one of type 2); that is, she is required to state in which cup the milk was poured first. We record the outcomes x1, x2, . . . , xn ∈ {0, 1} (where 0 indicates an incorrect classification and 1 a correct classification) and, as usual, treat these data as realizations of the random variables X1, X2, . . . , Xn. Let Sn = n∑ i=1 Xi denote the (random) number of correctly classified pairs. As our model, we again assume that the Xi are independent and identically distributed according to a Bernoulli distribution, i.e., Xi i.i.d. ∼ Ber(θ) with parameter θ ∈ Θ = [0, 1]. Then, naturally, Sn ∼ Bin(n, θ) under Pθ; in other words, in the model Pθ (which corresponds to a given θ), the number Sn of successes is binomially distributed with parameters n and θ. As skeptics, we doubt the lady’s claimed ability. Therefore, we choose as our (simple) null hypothesis H0 : θ = 1 2, i.e. Θ0 = { 1 2} (“random guessing—anyone can do that”). The (composite) alternative hypothesis, asserting that the lady possesses special abilities, is then HA : θ > 1 2, i.e. ΘA = ( 1 2, 1 ] . To proceed further, we now need to formalize the decision-making process based on the data. (We will return to the details of this example later.) ♢ 3.2 Tests and Decisions Definition 3.1 (Test). A test is a pair (T, K), where • T is a statistic of the form T = t(X1, . . . , Xn) (the test statistic), and • K ⊂ R is a (deterministic) set, called the critical region (or rejection region). Given the observed data x1 = X1(ω), . . . , xn = Xn(ω), a statistical test enables us to systematically accept or reject the null hypothesis H0. We first compute the test statistic 16 T (ω) = t(X1(ω), . . . , Xn(ω)) and then follow the decision rule: Reject H0 if T (ω) ∈ K, Do not reject H0 if T (ω) /∈ K. Because T is a random variable, the event {T ∈ K} has a probability which we can consider under each model Pθ. There are two types of errors: 1. A Type I error occurs when the null hypothesis is wrongly rejected even though it is true. For θ ∈ Θ0, this error has probability Pθ[T ∈ K]. 2. A Type II error occurs when the null hypothesis is not rejected (or is accepted) even though it is false. For θ ∈ ΘA, this error has probability Pθ[T /∈ K] = 1−Pθ[T ∈ K]. Example 3.2 (Tea Tasting Lady). In the tea tasting lady example, a Type I error occurs if we reject the null hypothesis of random guessing even though it is true (i.e. the lady has no special ability). Conversely, a Type II error occurs if we fail to reject the null hypothesis, thereby missing the lady’s special ability when it is indeed present. 3.3 Significance Level and Power When choosing an appropriate test, minimizing the probability of a Type I error is crucial. A Type I error occurs if we reject H0 (i.e. if T ∈ K) even though H0 is true. We would like our test to have a low probability of a Type I error. To this end, we define the significance level of a test. Definition 3.2 (Significance Level). Let α ∈ (0, 1). A test (T, K) is said to have signifi- cance level α if for all θ ∈ Θ0 Pθ[T ∈ K] ≤ α. Our second objective is to avoid a Type II error. This leads directly to the definition of the power of a test. Definition 3.3 (Power). The power of a test (T, K) is defined as the function β : ΘA → [0, 1], θ ↦→ β(θ) := Pθ[T ∈ K]. The primary goal is to minimize the probability of a Type I error (i.e., keep the test’s significance level low). Having fixed a level α, we design a test with significance level α. Our secondary goal is to maximize the power (i.e., minimize the probability of a Type II error, which is 1 − β(θ) for θ ∈ ΘA). Notice that this asymmetry means it is inherently more difficult to reject H0 than to fail to reject it. Therefore, a serious test often adopts as the null hypothesis the negation of the statement one actually wishes to prove. If one 17 can still reject H0 under these stringent conditions, one can be more confident that an effect is truly present. It follows that the decision of a test depends on how one defines the null and alternative hypotheses. In fact, the same question might lead to different decisions if the roles of H0 and HA are interchanged. (We will illustrate this with an example later.) Important: The decision in a test is never a proof; it is only an interpretation of how well the data agree with the presumed model. If T (ω) ∈ K, we reject H0 and thus disbelieve that θ ∈ Θ0, which may (but need not) lead us to believe that θ is in ΘA. If T (ω) /∈ K, we do not reject H0 and are reinforced in our belief that θ ∈ Θ0. However, we know no more about the true value of θ than before — a test does not provide a proof. Example 3.3. Tea Testing Lady (Cont.) Under Pθ the random variables X1, . . . , Xn are again assumed to be i.i.d. distributed as Ber(θ), and hence the total number of successes Sn = n∑ i=1 Xi is distributed as Bin(n, θ). In this example the null and alternative hypotheses are set as H0 : θ = 1 2 and HA : θ > 1 2. Because one would expect more ones (correct classifications) for θ > 1 2 than for θ = 1 2, a large value of Sn supports HA. A plausible test is to use the test statistic T := Sn and to choose a critical region of the form K = (c, ∞). In other words, we reject the hypothesis of random guessing (i.e. H0) if the lady achieves many successes. Since our null hypothesis is θ = 1 2 (i.e. “no special ability”), we deliberately set the test up so that even if the lady has some ability, a positive result is required to reject the skeptical null hypothesis. In order to determine the critical value c corresponding to a significance level α, we need the probabilities P 1 2 [Sn > c] for θ = 1 2, and for the power function we also need β(θ) = Pθ[Sn > c] 18 for θ > 1 2. In general, one needs to know the distribution of the test statistic T under every Pθ (or at least under the null hypothesis). In practice, this is usually not possible exactly; if the distribution under H0 cannot be obtained exactly, an approximation must suffice. Suppose we perform the test over n = 10 days. The following table shows the binomial probabilities P 1 2 [S10 > k] for various values of k: θ k = 7 k = 8 k = 9 k = 10 0.7 0.3828 0.1493 0.0282 0 0.6 0.1673 0.0464 0.0060 0 0.5 0.0547 0.0107 0.0010 0 To obtain a significance level α of approximately 5%, we require that P 1 2 [S10 > c] ≤ 0.05. Choosing c = 7 yields P 1 2 [S10 > 7] = 0.0547, which is roughly 5%. At this level, we are willing to reject the null hypothesis (of random guessing) when 8 or more successes are observed. The power of the test can also be derived from the table. For example, for the chosen c = 7 we have β(0.6) = P0.6[S10 > 7] = 0.1673, and β(0.7) = 0.3828. Thus, we see that 1 − β(θ) = Pθ[S10 ≤ 7] becomes rather large for θ in the alternative, indicating that the test has a significant probability of a Type II error (i.e. failing to detect a real ability) when the deviation from 0.5 is weak. ♢ Remark 3.1 (Remark 3.4). Because the test statistic T in the above example is discrete, it is generally impossible to achieve exactly the preassigned significance level α. That is, one cannot usually find a critical region K such that Pθ0[T ∈ K] = α. (Indeed, even for a simple null hypothesis Θ0 = {θ0} this is problematic in the discrete case.) A common workaround is to use a randomized test: One selects a number γ ∈ [0, 1] such that γ Pθ0[T > c] + (1 − γ) Pθ0[T > c + 1] = α, and then decides as follows: If T > c, the null hypothesis is rejected with probability γ; that 19 is, H0 is rejected if (i) T > c holds, and (ii) an independent U (0, 1)-distributed random variable takes a value less than or equal to γ. In the above example, using such a randomized test one can achieve the exact level α = 5% by choosing c = 7 and setting γ = α − Pθ0[T > c + 1] Pθ0[T > c] − Pθ0[T > c + 1] ≈ 0.893. The above situation, with simple null and alternative hypotheses, is so specific that it rarely occurs in practice. However, the basic idea can be generalized and often leads to good or even optimal tests under less restrictive assumptions. In later sections, examples will illustrate that the resulting tests are often intuitively very plausible. 3.4 Construction of Tests In this section, we explain a systematic approach to test construction which, in many situations, leads to an optimal test. The idea dates back to Neyman and Pearson. Assume that θ0 ̸= θA are two fixed numbers. In this section, we assume that both the null hypothesis and the alternative hypothesis are simple, i.e., H0 : θ = θ0, HA : θ = θA. Furthermore, we assume that the random variables X1, . . . , Xn are either jointly discrete or jointly continuous under both Pθ0 and PθA. In particular, the likelihood function L(x1, . . . , xn; θ) is well-defined for θ = θ0 and θ = θA (see Definition 1.4). Definition 3.4 (Likelihood Ratio). For every x1, . . . , xn, define the likelihood ratio R(x1, . . . , xn) := L(x1, . . . , xn; θA) L(x1, . . . , xn; θ0) . By convention, if L(x1, . . . , xn; θ0) = 0, we set R(x1, . . . , xn) = +∞. Intuitively, a large ratio indicates that the observations x1, . . . , xn are far more likely under the alternative PθA than under the null Pθ0. Hence, it makes sense to define the test statistic as T := R(X1, . . . , Xn), and to choose the critical region as K := (c, ∞) for some constant c. Definition 3.5 (Likelihood Ratio Test). Let c ≥ 0. The likelihood ratio test with param- 20 eter c is the test (T, K) where T = R(X1, . . . , Xn) and K = (c, ∞). The likelihood ratio test is optimal in the following sense: Any other test with significance level no greater than the level of the likelihood ratio test will have lower power (i.e., a higher probability of a Type II error). Theorem 3.1 (Neyman–Pearson Lemma (Theorem 3.7)). Let c ≥ 0 and let (T, K) be the likelihood ratio test with parameter c and significance level α∗ := Pθ0[T > c]. If (T ′, K ′) is any other test with significance level α ≤ α∗, then PθA[T ′ ∈ K ′] ≤ PθA[T ∈ K]. Remark 3.2. (Proof: See Krengel, Theorem 6.2.) The situation above with simple hypotheses is very special; in practice such cases occur rarely. However, the basic idea can be generalized to yield good or even optimal tests under less restrictive conditions. Often, for composite hypotheses the generalized likelihood ratio R(x1, . . . , xn) := supθ∈ΘA L(x1, . . . , xn; θ) supθ∈Θ0 L(x1, . . . , xn; θ) (or an alternative version using the union ΘA ∪ Θ0) is used, and one chooses the test statistic as T := R(X1, . . . , Xn) with critical region K = (c0, ∞), where c0 is chosen such that the test has the preassigned significance level. Example 3.4 (Tea Testing Lady via the Likelihood Ratio Method). Assume that in the coin-toss model underlying the tea testing lady experiment, the random variables X1, . . . , Xn are independent and identically distributed with Xi ∼ Ber(θ), so that the probability mass function is pX(xi; θ) = θxi (1 − θ) 1−xi, for xi ∈ {0, 1}. Thus, the joint likelihood function for an observed sample (x1, . . . , xn) is given by L(x1, . . . , xn; θ) = n∏ i=1 pX(xi; θ) = θ∑n i=1 xi (1 − θ) n−∑n i=1 xi . 21 We now consider testing the hypothesis that the lady is merely guessing. Choose as the null hypothesis H0 : θ = 1 2, and as the alternative hypothesis HA : θ > 1 2 . For fixed data x1, . . . , xn, define the likelihood ratio as R(x1, . . . , xn; θ0, θA) := L(x1, . . . , xn; θA) L(x1, . . . , xn; θ0) . Since under H0 we have θ0 = 1 2, it follows that L(x1, . . . , xn; 1 2) = (1 2 )n . Therefore, the likelihood ratio becomes R(x1, . . . , xn; 1 2, θA) = L(x1, . . . , xn; θA) ( 1 2)n = ( θA 1 2 )∑n i=1 xi ( 1 − θA 1 2 ) n−∑n i=1 xi . Because by assumption θA > 1 2, it follows that the ratio θA(1 − θ0) θ0(1 − θA) = θA (1 − 1 2) ( 1 2)(1 − θA) = θA 1 − θA > 1. Thus, R(x1, . . . , xn; 1 2, θA) is large exactly when the exponent ∑n i=1 xi is large. Instead of working with the full likelihood ratio, we note that it is equivalent (in terms of ordering the data) to use the total number of successes as the test statistic. Hence, we define T := n∑ i=1 Xi = Sn, and choose the critical region K := (c, ∞), with the constant c chosen to ensure that the test meets the specified significance level under H0. Thus, the Neyman–Pearson approach leads us to reject H0 (i.e. the hypothesis of random guessing) if the observed sum Sn is large. This procedure is exactly equivalent to the test procedure we earlier motivated by plausibility arguments. ♢ Example 3.5 (Example: Testing the Mean in a Normal Model with Known Variance). Let X1, . . . , Xn be independent and identically distributed under Pθ with Xi ∼ N (µ, σ2) 22 and with known variance σ2; hence, the unknown parameter here is θ = µ ∈ R. The probability density function of Xi under Pθ is fX(xi; θ) = 1 σ√ 2π exp (−(xi − µ)2 2σ2 ) = 1 √ 2πv exp (−(xi − µ)2 2v ), where we have set v = σ2. Since the Xi are i.i.d., the joint likelihood function is L(x1, . . . , xn; θ) = n∏ i=1 fX(xi; θ) = (2πσ2)−n/2 exp (− 1 2σ2 n∑ i=1 (xi − µ)2). The likelihood ratio is then defined as R(x1, . . . , xn; θ0, θA) = L(x1, . . . , xn; θA) L(x1, . . . , xn; θ0) . In our application we wish to test the hypothesis H0 : θ = θ0 versus HA : θ = θA. Because the likelihood under H0 is L(x1, . . . , xn; θ0) = (2πσ2) −n/2 exp (− 1 2σ2 n∑ i=1 (xi − θ0)2), the ratio becomes R(x1, . . . , xn; θ0, θA) = exp ( − 1 2σ2 [ n∑ i=1 (xi − θA) 2 − n∑ i=1 (xi − θ0) 2] ) . This can be rewritten (up to a multiplicative constant depending on σ, θ0, θA) as R(x1, . . . , xn; θ0, θA) = const.(σ, θ0, θA) · exp (θA − θ0 σ2 n∑ i=1 xi). Thus, the likelihood ratio tends to be large when the exponent (θA − θ0) n∑ i=1 xi is large. Note that the interpretation of “large” here depends on the sign of θA − θ0. In any case, we choose as the test statistic T ′ := n∑ i=1 Xi. 23 If θA > θ0 (so that θA − θ0 > 0), the exponent is large when T ′ is large; then we choose the critical region of the form K ′ (>) := (c ′ (>), ∞), i.e. we reject H0 when T ′ is large. Conversely, if θA < θ0, then the exponent is large when T ′ is small (i.e. negative), and the critical region is of the form K ′ (<) := (−∞, c′ (<)). In both cases, the critical region must be chosen (i.e. the constants c′ (>) or c′ (<) determined) so that the test attains a preassigned significance level α. That is, we wish to have Pθ0[T ′ ∈ K ′] ≤ α, and for that we need the distribution of T ′ under Pθ0, i.e. under H0. In the present case this is straightforward. Under any Pθ the Xi are i.i.d. ∼ N (θ, σ2), hence the sum T ′ = n∑ i=1 Xi ∼ N (nθ, nσ2) under Pθ. Equivalently, we may define T = Xn − θ σ/ √n ∼ N (0, 1) under Pθ, so that we can use T in place of T ′. One should note that T is actually computable in the model Pθ for θ ∈ Θ0, i.e. with θ = θ0 (under H0), because by assumption the variance σ2 is known and the mean θ0 to be tested is also known. (The same applies to T ′; however, the distribution of T under H0 is simpler than that of T ′.) ♢ 3.5 Examples In this section we illustrate the considerations above by a few examples. (We largely refrain from detailed derivations and present only the recipes.) Example 3.6 (Normal Distribution, Test for the Mean with Known Variance). This test is known as the z-test. Here the random variables X1, . . . , Xn are i.i.d. ∼ N (θ, σ2) under Pθ with known variance σ2, and we wish to test the hypothesis H0 : θ = θ0. Possible alternatives HA are either θ > θ0 or θ < θ0 (one-sided), or θ ̸= θ0 (two-sided); 24 which alternative is most appropriate depends on the concrete question. In every case the test statistic is (see the last example in Section 3.4) T := Xn − θ0 σ/ √n ∼ N (0, 1) under Pθ0. The critical region K takes the form: • K = (c>, ∞) for a one-sided test against HA : θ > θ0, • K = (−∞, c<) for a one-sided test against HA : θ < θ0, • K = (−∞, −c=) ∪ (c=, ∞) for a two-sided test against HA : θ ̸= θ0. For example, the condition α = Pθ0[T ∈ (c>, ∞)] = Pθ0[T > c>] = 1 − Pθ0[T ≤ c>] = 1 − Φ(c>) implies that c> = Φ −1(1 − α) ≡ z1−α. Thus, for θ > θ0 we reject H0 if Xn > θ0 + z1−α σ √n. Analogous reasoning yields c< = zα = −z1−α and for the two-sided test c= = z1−α/2. ♢ Example 3.7 (Ostrich Eggs (Known Variance)). The Australians Mr. Smith and Dr. Thurston are still disputing the average weight of ostrich eggs. Both agree that the weights may be modeled as normally distributed; however, Mr. Smith claims that the mean is 1100 g while Dr. Thurston insists that the eggs are heavier (approximately 1200 g on average). To settle their dispute, they travel to Africa to search for ostrich eggs. Because these are usually well hidden, they find only eight eggs with the following weights (in grams): 1090, 1150, 1170, 1080, 1210, 1230, 1180, 1140. Dr. Thurston proposes to test Mr. Smith’s claim by taking the hypothesis H0 : µ = µ0 = 1100 against the alternative HA : µ > 1100 (or alternatively, against µ = 1200) at the 5% level. The variance is known; in fact, σ = 55 g. Dr. Thurston computes the sample mean as ¯x = 1 8 8∑ i=1 xi = 1156.25, 25 and from a standard normal table he finds z0.95 = 1.645. Hence, the test statistic is TTh = ¯x − µ0 σ/ √8 ≈ 1156.25 − 1100 55/ √8 ≈ 2.89. Since TTh > 1.645, the hypothesis µ = 1100 is rejected at the 5% level. Mr. Smith, however, feels that this procedure disadvantages him and suggests instead test- ing Dr. Thurston’s claim with the hypothesis H0 : µ = µ1 = 1200 against the alternative HA : µ < 1200 (or alternatively, µ = 1100). He computes the corresponding test statistic as TSm = ¯x − µ1 σ/ √8 ≈ 1156.25 − 1200 55/ √ 8 ≈ −2.25. Since −2.25 < −1.645 (with z0.05 = −1.645), the hypothesis µ = 1200 is rejected at the 5% level. ♢ Example 3.8 (Normal Distribution, Test for the Mean with Unknown Variance (t-test)). This test is known as the t-test. Here, the observations X1, . . . , Xn are i.i.d. ∼ N (µ, σ2) under Pθ, but now the variance σ2 is unknown. In this case the parameter is θ = (µ, σ2) (with σ2 unknown). We wish to test H0 : µ = µ0. Strictly speaking, this is a composite hypothesis since σ2 is unspecified. Explicitly, the null set is Θ0 = {µ0} × (0, ∞). The test statistic is defined as T := ¯X − µ0 S/ √n ∼ tn−1 under Pθ0, where S2 = 1 n − 1 n∑ i=1 (Xi − ¯X) 2. Depending on whether the alternative is one-sided or two-sided, the critical region is cho- sen according to the appropriate tn−1 quantiles. ♢ Example 3.9 (Ostrich Eggs (t-test Version)). Now, Mr. Smith and Dr. Thurston won- 26 der whether, in their first experiment, they might have used an incorrect estimate of the variance of ostrich eggs. Therefore, they decide to perform the tests again without the assumption of known variance—that is, by using a t-test. Dr. Thurston still insists on testing H0 : µ = 1100 against HA : µ > 1100, at the 5% level. Since the variance is unknown, he calculates the sample variance as s2 = 1 n − 1 ( n∑ i=1 x2 i − n¯x 2) = 2798.21, so that s = 52.90. From a t-distribution with 7 degrees of freedom one finds t7,0.95 = 1.895. Then the test statistic is ˜TTh = ¯x − 1100 s/ √ 8 ≈ 1156.25 − 1100 52.90/ √8 ≈ 3.008. Since 3.008 > 1.895, the hypothesis H0 : µ = 1100 is rejected at the 5% level. Not surprisingly, Mr. Smith remains unconvinced and suggests instead testing Dr. Thurston’s claim with the alternative hypothesis reversed: H0 : µ = µ1 = 1200 against HA : µ < 1200 (or also µ = 1100). He computes the test statistic ˜TSm = ¯x − 1200 s/ √ 8 ≈ 1156.25 − 1200 52.90/ √8 ≈ −2.339. Since for the 5% level we have t7,0.05 = −t7,0.95 = −1.895 and ˜TSm < −1.895, the hypothesis H0 : µ = 1200 is rejected at the 5% level. ♢ Example 3.10 (Paired Two-Sample Test in a Normal Model). Suppose that a group of subjects is measured under two different conditions, yielding paired observations (X1, Y1), . . . , (Xn, Yn). Assume that under Pθ the pairs are independent and that Xi ∼ N (µX, σ2) and Yi ∼ N (µY , σ2), with identical variance σ2. Such a situation occurs, for example, when the same subjects try two different treatments, yielding a natural pairing. In this case one can reduce the two-sample problem to a one-sample problem by considering the differences Zi := Xi − Yi. 27 Then the Zi are i.i.d. with Zi ∼ N (µX − µY , 2σ2). Thus, tests for the difference of the means can be performed by applying the one-sample test (using a z-test when σ2 is known or a t-test when it is unknown). These tests are known as the paired two-sample z-test (if σ2 is known) or the paired two-sample t-test (if σ2 is unknown). ♢ Example 3.11 (Unpaired Two-Sample Test in a Normal Model). Consider now two independent samples: X1, . . . , Xn ∼ N (µX, σ2) and Y1, . . . , Ym ∼ N (µY , σ2), where the variance is assumed to be the same in both groups, but the sample sizes n and m may differ. In this unpaired situation, pairwise differences cannot be formed and one must use an unpaired test. (a) If σ2 is known, the test statistic is given by T := (Xn − Ym) − (µX − µY ) σ√ 1 n + 1 m ∼ N (0, 1) under every Pθ. Here σ is known and µX − µY is presumed known under H0; this is the unpaired two-sample z-test. (b) If σ2 is unknown, one first computes the sample variances S2 X := 1 n − 1 n∑ i=1 (Xi − ¯X) 2, S2 Y := 1 m − 1 m∑ j=1 (Yj − ¯Y ) 2. Then the pooled variance is defined as S2 := (n − 1)S2 X + (m − 1)S2 Y n + m − 2 . The test statistic becomes T := (Xn − Ym) − (µX − µY ) S√ 1 n + 1 m ∼ tn+m−2 under Pθ. This test is known as the unpaired two-sample t-test. ♢ Most of the tests presented above are based on the assumption that the samples are normally distributed; this situation is very convenient because then the distribution of the test statistic is available in explicit form. These tests work very well when the data are indeed normally distributed; however, if this assumption does not hold, they quickly lose a significant portion of their power. Therefore, it is advisable to also be familiar with 28 alternative tests that rely on less specific assumptions. 3.6 The p-value Let X1, . . . , Xn be a sample of size n. We wish to test a hypothesis H0 : θ = θ0 against an alternative HA : θ ∈ ΘA. Definition 3.6 (Ordered Family of Tests). A family of tests (T, (Kt)t≥0) is said to be ordered with respect to the test statistic T if for all s, t ≥ 0 s ≤ t =⇒ Ks ⊃ Kt. Typical examples are: Kt = (t, ∞) (right-tailed test), Kt = (−∞, −t) (left-tailed test), or Kt = (−∞, −t) ∪ (t, ∞) (two-sided test). Definition 3.7. Let H0 : θ = θ0 be a simple null hypothesis and let (T, (Kt)t≥0) be an ordered family of tests. The p-value is defined as the random variable p-value = G(T ), where the function G : R + → [0, 1] is given by G(t) = Pθ0[ T ∈ Kt] . Remark 3.3. • The p-value, as a function of the test statistic T , is itself a random variable. • The p-value depends directly on the initial observations X1, . . . , Xn; repeating the test with new data yields a new (random) p-value. • The p-value always lies in the interval [0, 1]. In the case where T is continuous and Kt = (t, ∞), it can be shown that under Pθ0 the p-value is uniformly distributed on [0, 1]. The p-value informs us which tests in our family {(T, Kt) : t ≥ 0} would lead to rejection of H0. In fact, if the observed p-value is p, then every test with significance level α > p would reject H0 and those with α ≤ p would not. Notice that the p-value depends solely on the null hypothesis; the alternative hypothesis does not enter its definition. 29 Example 3.12 (Coin Toss Example). Suppose we toss a coin 100 times and observe 60 heads. Our model assumes that X1, . . . , X100 are i.i.d. ∼ Be(θ) with θ ∈ [0, 1]. Under the null hypothesis we have H0 : θ = 1 2, so that Θ0 = { 1 2} and the alternative is HA : θ ̸= 1 2, i.e., ΘA = [0, 1] \\ { 1 2}. The number of successes is S100 = 100∑ i=1 Xi, and under Pθ we have S100 ∼ Bin(100, θ). To simplify calculations, we approximate the binomial distribution by a normal distribution (by the central limit theorem). For every θ we have S100 ≈ N (100θ, 100θ(1 − θ)). Thus, we may define T ′ := S100 − 100θ √ 100θ(1 − θ) ≈ N (0, 1) under Pθ. For θ = 1 2, this becomes T = S100 − 50 √ 25 = S100 − 50 5 . An equivalent form is T = 2S100 − 100 10 ≈ N (0, 1) under Pθ0. For a two-sided test we choose the symmetric critical region K := (−∞, −c) ∪ (c, ∞), so that H0 is rejected if |T | > c. To have a test of approximate level α, we require α = Pθ0[|T | > c] ≈ 2 ( 1 − Φ(c)) , 30 which implies c ≈ Φ−1(1 − α 2 ). For example, if α = 0.01, then c ≈ Φ−1(0.995) = 2.576. In our coin toss example, this translates (after back-transformation) to rejection of H0 if S100 > 62.88 or S100 < 37.12. Finally, the realized p-value is computed as p-value(ω) = Pθ0[ |T | > t0]∣ ∣ ∣t0=T (ω)≈ 2 (1 − Φ(T (ω))). For instance, if T (ω) = 2 then p-value(ω) ≈ 2( 1 − Φ(2)) = 2 ( 1 − 0.97725) = 0.0455. Thus, with 60 successes we would reject the hypothesis of a fair coin at the 5% level, but not at the 1% level. ♢ Summary of Tests To conclude this section, the general procedure for hypothesis testing is summarized as follows: 1. Choice of the Model. Specify the underlying probability model. 2. Formulation of Hypotheses. Clearly state the null hypothesis H0 and the alter- native hypothesis HA. 3. Test Statistic and Critical Region. Determine the test statistic T and the form of the critical region K (this can be derived via a generalized likelihood ratio test or taken from standard statistical literature). 4. Setting the Significance Level. Choose the significance level α so that the critical region K satisfies, approximately, sup θ∈Θ0 Pθ[T ∈ K] ≤ α. 5. Decision Rule. Compute the observed value T (ω) from the data. If T (ω) ∈ K reject H0, otherwise do not reject H0. 6. (Optional) p-value. Alternatively, compute the realized p-value. If it is less than or equal to α, reject H0. References: 31 • Bronstein et al., Taschenbuch der Mathematik, 4th ed., Harri Deutsch (1999) • Krengel, Einf¨uhrung in die Wahrscheinlichkeitstheorie und Statistik, 8th ed., Vieweg (2005) • Lengler, Steger, and Welzl, Algorithmen und Wahrscheinlichkeit (2021) • Lehn & Wegmann, Einf¨uhrung in die Statistik, 4th ed., Teubner (2004) • Rice, Mathematical Statistics and Data Analysis, 2nd ed., Duxbury Press (1995) • Schweizer, Wahrscheinlichkeit und Statistik (2010) • Stahel, Statistische Datenanalyse. Eine Einf¨uhrung f¨ur Naturwissenschaftler, 2nd ed., Vieweg (1999) • Williams, Weighing the Odds. A Course in Probability and Statistics, Cambridge University Press (2001) 32","libVersion":"0.5.0","langs":""}