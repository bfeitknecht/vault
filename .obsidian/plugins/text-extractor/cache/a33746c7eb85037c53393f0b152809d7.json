{"path":"sem3/LinAlg/UE/s/LinAlg-u05-s.pdf","text":"D-INFK Linear Algebra HS 2023 Afonso Bandeira Bernd G¨artner Solution for Assignment 5 1. a) The key insight here is to use v⊤v = v · v = ∥ v ∥2 = 1. We then get A2 = (vv⊤)(vv⊤) = v(v⊤v)v⊤ = v1v⊤ = vv⊤ = A for A2 and therefore P 2 = (I − A) 2 = I 2 − 2A + A2 = I − 2A + A = I − A = P for P 2. b) Knowing that w·v = 0, we compute Aw = (vv⊤)w = v(v⊤w) = v(v·w) = (v·w)v = 0v = 0. c) We are given Aw = 0 and hence get 0 = Aw = (vv⊤)w = v(v⊤w) = v(v · w) = (v · w)v. Note that v · w is a scalar and that v ̸= 0. From (v · w)v = 0 we hence get v · w = 0, as desired. d) Combining subtasks b) and c), we get that for all w ∈ R3, we have Aw = 0 if and only if v·w = 0. Hence, the nullspace of A is given by N(A) = {w ∈ R3 : w · v = 0}. In words, this is the set of all vectors orthogonal to v. e) Observe that A has the form A =   | | | v1v v2v v3v | | |   where v = [v1 v2 v3]⊤. Hence, all of its columns are linearly dependent on the first one. By v ̸= 0, we conclude that A has rank 1. Therefore, it is not invertible. Note that we have studied matrices of rank 1 before in Exercise 3 of Assignment 2. f) We first prove C(A) ⊆ {αv : α ∈ R}. Let w ∈ C(A) be arbitrary. By definition of C(A), there exists x with Ax = w. We conclude w = Ax = vv⊤x = (v · x)v = αv for α = v · x. Thus, w ∈ {αv : α ∈ R}. It remains to prove {αv : α ∈ R} ⊆ C(A). Let w ∈ {αv : α ∈ R} be arbitrary, i.e. w = αv for some α ∈ R. Choosing x = w, we get Ax = vv⊤x = vv⊤(αv) = α(v · v)v = αv = w and therefore w ∈ C(A). g) By subtask f), it suffices to prove {αv : α ∈ R} = {w ∈ R3 : Aw = w}. To see that {αv : α ∈ R} ⊆ {w ∈ R3 : Aw = w}, observe that A(αv) = αv (we have calculated this in more detail already above) and hence αv ∈ {w ∈ R3 : Aw = w} for all α. It remains to prove {w ∈ R3 : Aw = w} ⊆ {αv : α ∈ R}. By definition, any w ∈ {w ∈ R3 : Aw = w} is also in C(A). Hence, {w ∈ R3 : Aw = w} ⊆ C(A) and with subtask f) we conclude the proof. h) Let w ∈ R3 be arbitrary. We have P w = 0 ⇐⇒ (I − A)w = 0 ⇐⇒ w − Aw = 0 ⇐⇒ Aw = w. This implies N(P ) = {w ∈ R3 : Aw = w} and by subtask g) we conclude N(P ) = C(A). i) We start by proving C(P ) ⊆ N(A). Let w ∈ C(P ) be arbitrary. By definition, there is x ∈ R3 with P x = w. We use this to compute Aw as Aw = A(P x) = A(I − A)x = (A − A2)x = (A − A)x = 0 and conclude w ∈ N(A). It remains to prove N(A) ⊆ C(P ). Let w ∈ N(A) be arbitrary. By definition, we have Aw = 0. Choosing x = w, we get P x = (I − A)x = x − Ax = w − Aw = w. We conclude w ∈ C(P ). 2. a) The vector b is an element of U if it can be written as a linear combination of the vectors u1, u2, u3. This is equivalent to the existence of a solution of the linear system   | | | u1 u2 u3 | | |   x = b. To solve the linear system, we need to do elimination on the matrix and also apply the same row operations to b. It is sometimes convenient to do both at once by first concatenating b to the matrix (as a new column) and then applying elimination on this new matrix. By concatenating b to the matrix we get   | | | | u1 u2 u3 b | | | |   . To remember that the last column comes from b, we can insert a vertical separator (this is just notation) as follows:   | | | | u1 u2 u3 b | | | |   =     2 −1 2 1 −4 5 −5 −2 8 5 5 6 2 2 1 2     . We get     2 −1 2 1 0 3 −1 0 0 9 −3 2 0 3 −1 1     after performing elimination in the first column and finally     2 −1 2 1 0 3 −1 0 0 0 0 2 0 0 0 1     after performing elimination also in the second column. In particular, this means that the system   | | | u1 u2 u3 | | |   x = b. has the same solutions as the system     2 −1 2 0 3 −1 0 0 0 0 0 0     x =     1 0 2 1     . But this last system does not have a solution and hence b /∈ U . b) Interpreting the result of the elimination from the previous part of the task, we can determine that the rank of the system matrix is 2 (there are two pivots). Hence, the vectors u1, u2, u3 are linearly dependent and therefore do not form a basis. 3. Each of the four points yields one linear equation with variables a, b, c, d. For example, for x = 4, y = 5 we get the equation a4 3 + b4 2 + c4 + d = 5. In total, we get the linear system a03 + b02 + c0 + d = 1 a23 + b22 + c2 + d = 2 a43 + b4 2 + c4 + d = 5 a63 + b6 2 + c6 + d = 6 with four equations and four variables that we can write in matrix form as     1 0 0 0 1 2 4 8 1 4 16 64 1 6 36 216         d c b a     =     1 2 5 6     . We now want to solve this system by using the elimination technique. For this, it is convenient to apply the row operations to the system matrix and the right-hand side simultaneously by appending the right- hand side to the matrix as follows:     1 0 0 0 1 1 2 4 8 2 1 4 16 64 5 1 6 36 216 6     . After performing elimination in the first column we get     1 0 0 0 1 0 2 4 8 1 0 4 16 64 4 0 6 36 216 5     . Next, we perform elimination in the second columns to get     1 0 0 0 1 0 2 4 8 1 0 0 8 48 2 0 0 24 192 2     . Finally, we obtain     1 0 0 0 1 0 2 4 8 1 0 0 8 48 2 0 0 0 48 −4     . It remains to perform backward substitution. From the last row, we get a = − 4 48 = − 1 12 . Next, we get b = 2−48a 8 = 6 8 = 3 4 . From the second row we obtain c = 1−8a−4b 2 = 1+ 2 3 −3 2 = − 2 3 . Finally, we get d = 1 from the first row. Hence, the function f (x) = − 1 12 x3 + 3 4 x2 − 2 3 x + 1 interpolates all of our datapoints. 4. a) Note that the function 0 : x ∈ R ↦→ 0 is both in O and E. Hence, both sets are non-empty. Thus, it remains to prove that both O and E are closed under vector addition and scalar multiplication. We start with O. Let f , g ∈ O and c ∈ R be arbitrary. We have (f + g)(−x) = f (−x) + g(−x) = −f (x) − g(x) = −(g + f )(x) for all x ∈ R and hence f + g ∈ O. Similarly, we have (cf )(−x) = cf (−x) = −cf (x) = −(cf )(x) for all x ∈ R which proves cf ∈ O. We conclude that O is a subspace of V . We proceed analogously for E. Let f , g ∈ E and c ∈ R be arbitrary. We have (f + g)(−x) = f (−x) + g(−x) = f (x) + g(x) = (g + f )(x) for all x ∈ R and hence f + g ∈ E. And also (cf )(−x) = cf (−x) = cf (x) = (cf )(x) for all x ∈ R which proves cf ∈ E. We conclude that E is a subspace of V . b) We already know that 0 is in both O and E and therefore 0 ∈ O ∩ E. Now consider an arbitrary function f ∈ O ∩ E and fix x ∈ R. By definition of O, we have f (−x) = −f (x). By definition of E, we also get f (−x) = f (x). We conclude that we must have −f (x) = f (x). But this implies f (x) = 0. Since this works for any x ∈ R, we conclude that f must be the zero function 0. Hence, 0 is the only function in O ∩ E. c) Let f ∈ V be arbitrary and define g(x) := 1 2 (f (x) + f (−x)) h(x) := 1 2 (f (x) − f (−x)) for all x ∈ R. Observe that we have f = g + h. It remains to prove g ∈ E and h ∈ O: For all x ∈ R we have g(−x) = 1 2 (f (−x) + f (−(−x))) = 1 2 (f (x) + f (−x)) = g(x) and hence g ∈ E. Similarly, we have h(−x) = 1 2 (f (−x) − f (−(−x))) = 1 2 (f (−x) − f (x)) = − 1 2 (f (x) − f (−x)) = −h(x) for all x ∈ R. Hence, h ∈ O. 5. 1. Consider the vectors v1 =     1 2 3 4     and v2 =     7 6 5 4     . Which of the following sets of vectors is a basis of R4? (a)    v1, v2,     1 0 −2 0     ,     0 1 2 0     ,     0 0 0 1        A set of 5 vectors from R 4 can never be linearly independent. Hence, this is not a basis. (b)    v1, v2,     0 1 0 0     ,     0 0 0 0        The zero vector is linearly dependent on all other vectors. Hence, this is not a basis. √ (c)    v1, v2,     1 0 0 0     ,     0 1 0 0        These 4 vectors are linearly independent. If we put them as columns into a matrix A, then A will have full rank. By the inverse theorem, the system Ax = b will have a unique solution for all b ∈ R 4. Thus, C(A) = R 4 or in other words, the four vectors span all of R4. Thus, they are a basis of R4. 2. Which of the following matrices are in reduced row echelon form? (a)   1 0 2 4 0 0 1 5 0 0 0 0   Not in reduced row echolon form because of the 2 in the first row. √ (b)   1 0 2 4 0 1 1 5 0 0 0 0   This is in reduced row echolon form. There are two pivots, one in the first column and one in the second column. √ (c)   1 0 0 4 0 0 1 5 0 0 0 0   This is in reduced row echolon form. The pivots are in the first and third column. (d)   1 0 2 4 0 1 1 5 0 0 1 0   Not in reduced row echolon form because the 1 and 2 in the first and second row of the third column have not been eliminated.","libVersion":"0.3.2","langs":""}