{"path":"sem3/LinAlg/UE/s/LinAlg-s-u06.pdf","text":"D-INFK Linear Algebra HS 2024 Bernd G¨artner Robert Weismantel Solution for Assignment 6 1. a) Since H is a hyperplane, there exists a non-zero vector d ∈ Rm such that H = {v ∈ Rm : v · d = 0}. In order to prove that H is a subspace of Rm, we have to prove that H is non- empty and closed under vector addition and scalar multiplication. By definition, 0 ∈ H and hence H is non-empty. It remains to prove that, given arbitrary v, w ∈ H and c ∈ R, we also have (v + w) ∈ H and cv ∈ H. Indeed, we observe (v + w) · d = v · d + w · d = 0 + 0 = 0 and hence (v + w) is in H. Similarly, we have (cv) · d = c(v · d) = c0 = 0 and therefore (cv) ∈ H. We conclude that H is a subspace of Rm. b) Let H = {v ∈ Rm : v · d = 0} for some non-zero d ∈ Rm. Consider the standard unit vectors e1, . . . , em ∈ Rm and let di := ei · d for all i ∈ [m]. Observe that we must have dj ̸= 0 for some j ∈ [m], because d is non-zero. For every i ∈ [m], define the vector vi := ei − di dj ej. We claim that the set of vectors {vi : i ̸= j} is a basis of H. To prove this, observe first that the set of vectors {vi : i ̸= j} is linearly independent: Indeed, each of the vectors controls its own coordinate (namely i) and hence there is no way of obtaining the vectors 0 by a non-trivial linear combination of vectors in {vi : i ̸= j}. Next, observe that, by definition, we have vi · d = ei · d − di dj ej · d = di − di = 0 for all i ∈ [m] \\ {j}, and thus {vi : i ̸= j} ⊆ H. It remains to prove that the vectors span all of H. Let u = [ u1 u2 . . . um]⊤ ∈ Rm be an arbitrary vector with u·d = ∑m i=1 uidi = 0. Observe that this implies ∑ i∈[m]\\{j} uidi = −ujdj. Using this, we get ∑ i∈[m]\\{j} uivi = ∑ i∈[m]\\{j} ui(ei − di dj ej) = ∑ i∈[m]\\{j} uiei − ( ∑ i∈[m]\\{j} uidi) 1 dj ej = ∑ i∈[m]\\{j} uiei + ujej = u, which means that u ∈ Span{vi : i ̸= j}. This proves that {vi : i ̸= j} is a basis of H, and we concldue that the dimension of H is m − 1. c) Note that precise notation is very important here: f ∈ V is a function and we write f (x) ∈ R for the value of f evaluated at point x ∈ [0, 1]. In particular, f and f (x) are very different types of objects. Note that the symbol + is overloaded in the following sense: for two functions f , g ∈ V and x ∈ [0, 1], the + in the expression f (x) + g(x) denotes the normal addition of real numbers while the + in the expression f + g is the addition of functions defined in this exercise. 1 First, note that U is non-empty since every constant function is in U . Thus, consider arbitrary functions f , g ∈ U and scalar c ∈ R. For any x ∈ [0, 1], we have (f + g)(x) def = f (x) + g(x) f ∈U = f (1 − x) + g(x) g∈U = f (1 − x) + g(1 − x) def = (f + g)(1 − x) and therefore the function f + g is in U . Similarly, we have (cf )(x) def = cf (x) f ∈U = cf (1 − x) def = (cf )(1 − x) and hence cf ∈ U . We conclude that U is indeed a subspace of V . 2. Let H be the hyperplane H = {u ∈ Rm : u · v = 0}. By exercise 1b), we know that H has dimension m − 1. Let a1, . . . , am−1 ∈ H be a basis of H. We define the matrices Ai := [. . . a⊤ i . . . . . . 0⊤ . . . ] ∈ R 2×m, Bi := [ . . . 0⊤ . . . . . . a⊤ i . . . ] ∈ R2×m for every i ∈ [m − 1]. Observe that we have Aiv = 0 and Biv = 0 and thus Ai, Bi ∈ Sv for all i ∈ [m − 1]. We claim that the matrices A1, . . . , Am−1, B1, . . . , Bm−1 form a basis of Sv. In order to prove this, we first argue that they are linearly independent: For this, consider an arbitrary linear combination m−1∑ i=1 λiAi + m−1∑ i=1 µiBi = 0 with λi, µi ∈ R for all i ∈ [m−1]. Observe that, by definition of A1, . . . , Am−1 and B1, . . . , Bm−1, this implies ∑m−1 i=1 λiai = 0 as well as ∑m−1 i=1 µiai = 0. Since a1, . . . , am−1 are linearly inde- pendent, we conclude that λ1 = · · · = λm−1 = 0 and µ1 = · · · = µm−1 = 0. Hence, our set of matrices must be linearly independent. It remains to prove that our set of matrices spans Sv. For this, let C ∈ Sv be arbitrary with C = [. . . c⊤ 1 . . . . . . c⊤ 2 . . . ] . The condition Cv = 0 implies c1 · v = 0 and c2 · v = 0 and therefore c1, c2 ∈ H. Hence, there exist scalars λ1, . . . , λm−1 ∈ R and µ1, . . . , µm−1 ∈ R such that ∑m−1 i=1 λiai = c1 and∑m−1 i=1 µiai = c2. We conclude that C = m−1∑ i=1 (λiAi + µiBi) and thus C ∈ Span(A1, . . . , Am−1, B1, . . . , Bm−1), as desired. This proves that our set of matri- ces is a basis and we conclude that the dimension of Sv is 2(m − 1). 3. We want to prove that U ∪ W is a subspace of V if and only if U ⊆ W or W ⊆ U . “ ⇐= ” If U ⊆ W , then U ∪ W = W is a subspace of V by assumption. The same reasoning applies in the case W ⊆ U . “ =⇒ ” Assume now that U ∪ W is a subspace of V , and assume that W ̸⊆ U (otherwise, we are done). Then there exists w ∈ W \\ U . Let u ∈ U be arbitrary. Observe that, since U ∪ W is a subspace, we must have u+w ∈ U ∪W . But u+w ∈ U would imply that w = (u+w)−u is in U as well. Hence, we conclude u + w ∈ W . By w ∈ W and u + w ∈ W we finally obtain u = (u + w) − w ∈ W . We have proven that every vector in U is also in W , and thus conclude U ⊆ W . 2 4. Recall that the dimension of a subspace is defined as the size of a basis of that subspace. So to solve this exercise, it suffices to come up with a basis of Sm. It might be instructive to first consider the case m = 2. In the case of S2, the three matrices [ 1 0 0 0 ] , [0 0 0 1 ] , [0 1 1 0 ] form a basis: None of the matrices can be obtained from the others because each of the three matrices has a non-zero entry at a place where none of the other matrices has a non-zero entry (i.e. the three matrices are linearly independent). Moreover, every symmetric 2 × 2 matrix can be obtained as linear combination of those three matrices because it must have the form [a b b d ] = a [1 0 0 0 ] + b [ 0 1 1 0 ] + d [0 0 0 1 ] for some a, b, d ∈ R (i.e. the three matrices span all of S2). This idea generalizes to Sm. In particular, for i, j ∈ [m] with i ≤ j, define the m × m matrix B(ij) by B(ij) ℓk =    1 if ℓ = i and k = j 1 if ℓ = j and k = i 0 otherwise for all ℓ, k ∈ [m]. Observe that for i = j, B(ij) contains a single 1 on its diagonal and is zero everywhere else. For i < j, we find exactly two 1s in B(ij) and zeroes everywhere else. We claim that the set of matrices B = {B(ij) : i, j ∈ [m], i ≤ j} is a basis of Sm. We first check linear independence. Let i, j ∈ [m] with i ≤ j be arbitrary. Then B(ij) ij = 1 but none of the other matrices in the set has a non-zero entry at position (i, j). So B(ij) cannot be obtained as a linear combination of the other matrices. We conclude that set of matrices B is independent. Let now S ∈ Sm be an arbitrary symmetric m × m matrix. For all i, j ∈ [m], we must have Sij = Sji by symmetry. Thus, we can write S = ∑ i,j∈[m]:i≤j SijB(ij) and therefore we conclude that B spans all of Sm. Finally, observe that |B| = 1+2+3+· · ·+m = m(m+1) 2 . Hence, the dimension of Sm is m(m+1) 2 . 5. a) Note that the function 0 : x ∈ R ↦→ 0 is both in O and E. Hence, both sets are non-empty. Thus, it remains to prove that both O and E are closed under vector addition and scalar multiplication. We start with O. Let f , g ∈ O and c ∈ R be arbitrary. We have (f + g)(−x) = f (−x) + g(−x) = −f (x) − g(x) = −(g + f )(x) for all x ∈ R and hence f + g ∈ O. Similarly, we have (cf )(−x) = cf (−x) = −cf (x) = −(cf )(x) for all x ∈ R which proves cf ∈ O. We conclude that O is a subspace of V . 3 We proceed analogously for E. Let f , g ∈ E and c ∈ R be arbitrary. We have (f + g)(−x) = f (−x) + g(−x) = f (x) + g(x) = (g + f )(x) for all x ∈ R and hence f + g ∈ E. And also (cf )(−x) = cf (−x) = cf (x) = (cf )(x) for all x ∈ R which proves cf ∈ E. We conclude that E is a subspace of V . b) We already know that 0 is in both O and E and therefore 0 ∈ O ∩ E. Now consider an arbitrary function f ∈ O ∩ E and fix x ∈ R. By definition of O, we have f (−x) = −f (x). By definition of E, we also get f (−x) = f (x). We conclude that we must have −f (x) = f (x). But this implies f (x) = 0. Since this works for any x ∈ R, we conclude that f must be the zero function 0. Hence, 0 is the only function in O ∩ E. c) Let f ∈ V be arbitrary and define g(x) := 1 2 (f (x) + f (−x)) h(x) := 1 2 (f (x) − f (−x)) for all x ∈ R. Observe that we have f = g + h. It remains to prove g ∈ E and h ∈ O: For all x ∈ R we have g(−x) = 1 2 (f (−x) + f (−(−x))) = 1 2 (f (x) + f (−x)) = g(x) and hence g ∈ E. Similarly, we have h(−x) = 1 2 (f (−x) − f (−(−x))) = 1 2 (f (−x) − f (x)) = − 1 2 (f (x) − f (−x)) = −h(x) for all x ∈ R. Hence, h ∈ O. 6. The subspace Span(p, q, r) has dimension 3. We will prove this by showing that p, q, r are linearly independent and hence a basis of Span(p, q, r). For this, let λ, µ, γ ∈ R such that λp + µq + γr = 0. If we can prove that this implies λ = µ = γ = 0, we can conclude that the three polynomials p, q, r are linearly independent. Since p is the only one of the tree polynomials involving a non-zero coefficient for x3, we must have λ = 0. This further implies that we have µq + γr = 0. Since r has a non-zero coefficient for x1, but p does not, we next observe that we must have γ = 0. This means that we are left with µq = 0 and hence µ = 0. We conclude that there is no non-trivial linear combination of p, q, r ∈ R[x] that yields 0 ∈ R[x]. Therefore, the three polynomials are linearly independent. By definition, p, q, r span Span(p, q, r) and we conclude that they are a basis of size 3. This proves that the dimension of this subspace is 3. 4 7. 1. Let U1, U2 be arbitrary subspaces of Rm. Which of the following subsets of Rm must be subspaces of Rm as well? √ (a) U1 ∩ U2 Explanation: For vectors u, v ∈ U1∩U2 and a scalar c ∈ R we need to prove that u+v ∈ U1∩U2 and cv ∈ U1 ∩ U2. By u, v ∈ U1 ∩ U2, we also get u, v ∈ U1 and u, v ∈ U2. Since U1 and U2 are subspaces, this implies u + v ∈ U1, cv ∈ U1, u + v ∈ U2, and cv ∈ U2. Hence, we also have u + v ∈ U1 ∩ U2, and cv ∈ U1 ∩ U2. (b) U1 ∪ U2 Explanation: The set U1 ∪ U2 is in general not a subspace of Rm. For example, U1 and U2 could be distinct hyperplanes of Rm. Then, by exercise 1, both U1 and U2 are subspaces of Rm but adding a vector u1 ∈ U1 with a vector u2 ∈ U2 can take us outside of U1 ∪ U2. (c) U1 \\ U2 := {u ∈ U1 : u /∈ U2} Explanation: The set U1 \\ U2 can never be a subspace because the 0 is missing. (d) ∅ Explanation: By definition, a subspace has to be nonempty. √ (e) {0} Explanation: Adding any two vectors from {0} gives us 0 again. Similarly, multiplying with a scalar always gives us 0 as well. √ (f) U1 + U2 := {u1 + u2 : u1 ∈ U1, u2 ∈ U2} Explanation: The set U1 + U2 is a subspace by design. Consider arbitrary vectors u, v ∈ U1 + U2 and scalar c ∈ R. By definition of U1 + U2, we can write u = u1 + u2 with u1 ∈ U1 and u2 ∈ U2. Similarly, we can write v = v1 + v2 with v1 ∈ U1 and v2 ∈ U2. Then we have u + v = (u1 + u2) + (v1 + v2) = (u1 + v1) + (u2 + v2) ∈ U1 + U2 and cv = c(v1 + v2) = (cv1) + (cv2) ∈ U1 + U2. 5 2. Consider the vectors v1 =     1 2 3 4     and v2 =     7 6 5 4     . Which of the following sets of vectors is a basis of R4? (a)    v1, v2,     1 0 −2 0     ,     0 1 2 0     ,     0 0 0 1        A set of 5 vectors from R 4 can never be linearly independent. Hence, this is not a basis. (b)    v1, v2,     0 1 0 0     ,     0 0 0 0        The zero vector is linearly dependent on all other vectors. Hence, this is not a basis. √ (c)    v1, v2,     1 0 0 0     ,     0 1 0 0        These 4 vectors are linearly independent. If we put them as columns into a matrix A, then A will have full rank. By the inverse theorem, the system Ax = b will have a unique solution for all b ∈ R4. Thus, C(A) = R 4 or in other words, the four vectors span all of R4. Thus, they are a basis of R4. 6 3. Which of the following matrices are in row echelon form? (a)   1 0 2 4 0 0 1 5 0 0 0 0   Not in row echolon form because of the 2 in the first row. √ (b)   1 0 2 4 0 1 1 5 0 0 0 0   This is in row echolon form. There are two pivots, one in the first column and one in the second column. √ (c)   1 0 0 4 0 0 1 5 0 0 0 0   This is in row echolon form. The pivots are in the first and third column. (d)   1 0 2 4 0 1 1 5 0 0 1 0   Not in row echolon form because the 1 and 2 in the first and second row of the third column have not been eliminated. 7","libVersion":"0.5.0","langs":""}