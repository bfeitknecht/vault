{"path":"sem3/LinAlg/VRL/extra/slides/LinAlg-slides-w09.pdf","text":"Robert Weismantel Week 9: An explicit formula for projections, application of projections to data fitting and orthonormal bases Recall from the previous lecture: Let S be a subspace in Rm generated by a1, . . . , an ∈ S, i.e., S = span(a1, . . . , an) = C(A) = {Aλ | λ ∈ Rn} where A =   | | | a1 a2 · · · an | | |   . Lemma The projection of a vector b ∈ Rm to the subspace S = C(A) can be written as projS(b) = Aˆx, where ˆx satisfies the normal equations A T Aˆx = A T b. Robert Weismantel November 8, 2024 2 / 16 Can we derive an explicit formula for projections? Lemma For a matrix A ∈ Rm×n we have that AT A is invertible if and only if A has linearly independent columns. Proof. AT A is invertible if and only if N(AT A) = {0}. We know that N(AT A) = N(A) and hence, the result follows. Theorem Let S be a subspace in Rm and A a matrix whose columns are a basis of S. The projection of b ∈ Rm to S is given by projS(b) = Pb, where P = A ( A⊤A )−1 A⊤ is the projection matrix. Robert Weismantel November 8, 2024 3 / 16 A few coments about projections I Consider the projection matrix P = A ( A⊤A )−1 A⊤. The matrix A (and A⊤) are not necessarily square, and so they don’t have inverses. Hence, the expression A ( A⊤A )−1 A⊤ cannot be simplified by expanding ( A⊤A )−1. This would yield I = P and would only make sense if S was all of Rm. This case is less interesting as it means that A is invertible. P can be viewed as a mapping: for a given vector b its projection is given by projS(b) = Pb. Robert Weismantel November 8, 2024 4 / 16 A few coments about projections II Consider the projection matrix P = A ( A⊤A )−1 A⊤. If b ∈ Rm, then projS(projs(b)) = projS(b) by definition. This requires us to have that PPb = Pb, i.e., we should have P2 = P. Indeed P2 = ( A ( A⊤A)−1 A ⊤)2 = A ( A ⊤A )−1 A ⊤A ( A ⊤A )−1 A⊤ = P. Let S⊥ be the orthogonal complement of S and P the projection matrix onto the subspace S, i.e., projS(b) = Pb. Then I − P is the projection matrix that maps b ∈ Rm to projS⊥(b). This follows since b = e + projS(b) = e + Pb where e ∈ S⊥. Hence, (I − P)b = b − Pb = e = projS⊥(b). Note that – as it should be – we have that (I − P)2 = I − 2P + P2 = I − P. Robert Weismantel November 8, 2024 5 / 16 Another name for projections: least squares Definition For A ∈ Rm×n and b ∈ Rm a least squares solution solves min x∈Rn ∥Ax − b∥ 2. The link to projections Consider the subspace C(A) = {Ax | x ∈ Rn}. Then, min x∈Rn ∥Ax − b∥2 = min p∈C(A) ∥b − p∥2 = ∥b − projC(A)(b)∥2. Remark A least squares solution is given by projC(A)(b) = Aˆx, where AT Aˆx = AT b. If A has linearly independent columns, then AT A is invertible. Hence, for the least squares solution we have the explicit formula ˆx = (A T A) −1AT b. Robert Weismantel November 8, 2024 6 / 16 An application of least squares Figure: Fitting a line to points Robert Weismantel November 8, 2024 7 / 16 An application of least squares Linear regression is the task to fit a line through data points. Consider data points (t1, b1), (t2, b2), . . . , (tm, bm), representing some attribute b over time t. If the relation between t and b is explained by a linear relationship then it makes sense to search for constants α0 ∈ R and α1 ∈ R such that bk ≈ α0 + α1tk . Find α0 and α1 minimizing the sum of squares of the errors min α0,α1 m ∑ k=1 (bk − [α0 + α1tk ]) 2 . Robert Weismantel November 8, 2024 8 / 16 An explicit formula under one assumption. In matrix-vector notation min α0,α1 ∥ ∥ ∥ ∥b − A [ α0 α1 ]∥ ∥ ∥ ∥ 2 , (1) where b =        b1 b2 ... bm−1 bm        and A =        1 t1 1 t2 ... ... 1 tm−1 1 tm        . If A has independent columns, the solution is [ α0 α1 ] = (A ⊤A) −1A ⊤b = [ m ∑ m k=1 tk ∑ m k=1 tk ∑ m k=1 t 2 k ]−1 [ ∑ m k=1 bk ∑ m k =1 tk bk ] Robert Weismantel November 8, 2024 9 / 16 We can assume that A has independent columns Lemma The columns of the m × 2 matrix A defined before are linearly dependent if and only if ti = tj for all i ̸= j. Proof. Suppose that there are two indices i ̸= j such that ti ̸= tj . Let 1 be the all ones-vector in Rm and t the vector with components t1, . . . , tm. Consider the system in variables λ , µ λ 1 + µt = 0. Since ti ̸= tj we can subtract row j from row i to obtain λ 0 + µ(ti − tj ) = 0 ⇐⇒ µ = 0 since ti − tj ̸= 0. This implies that λ = 0 and hence A has full column rank. If ti = tj for all i ̸= j, then t = t11, i.e., the second column is a multiple of the first. Robert Weismantel November 8, 2024 10 / 16 Fitting a parabola: work it out! If we believe the relationship between tk and bk is quadratic we could attempt to fit a parabola: bk ≈ α0 + α1tk + α2t 2 k . This is a linear function in α0, α1, and α2. Similarly as with linear regression, it is natural to attempt to minimze min α0,α1,α2 ∥ ∥ ∥ ∥ ∥ ∥ b − A   α0 α1 α2   ∥ ∥ ∥ ∥ ∥ ∥ 2 . (2) b =        b1 b2 ... bm−1 bm        and A =        1 t1 t 2 1 1 t2 t 2 2 ... ... 1 tm−1 t 2 m−1 1 tm t 2 m        . Robert Weismantel November 8, 2024 11 / 16 Orthonormal vectors Definition (Orthonormal vectors) q1, . . . , qn ∈ Rm are orthonormal if they are orthogonal and have norm 1, i.e., qT i qj = δij = { 0 if i ̸= j 1 if i = j. , where δij is the Kronecker delta. Remark and example If Q is the matrix whose columns are the vectors qi ’s, then the condition that the vectors are orthonormal can be rewritten as Q⊤Q = I. Q may not be a square matrix, and so it is not necessarily the case that QQ⊤ = I. A classical example of an orthonormal set of vectors is the canonical basis, e1, . . . , en ∈ Rn where ei is the vector with a 1 in the i-th entry and 0 in all other entries, i.e., (ei )j = δij .Robert Weismantel November 8, 2024 12 / 16 Orthogonal matrices Definition (Orthogonal matrix) A square matrix Q ∈ Rn×n is an orthogonal matrix when Q⊤Q = I. In this case, QQ⊤ = I, Q−1 = Q⊤, and the columns of Q form an orthonormal basis for Rn. Example The 2 × 2 matrix Q that corresponds to rotating, counterclockwise, the plane by θ , Rθ = [ cos θ − sin θ sin θ cos θ ] is an orthogonal matrix. Indeed, RT θ Rθ = [ cos θ sin θ − sin θ cos θ ] [ cos θ − sin θ sin θ cos θ ] = I. Robert Weismantel November 8, 2024 13 / 16 Permutation matrices are orthogonal matrices Definition A permutation is a bijective map π : {1, . . . , n} ↦→ {1, . . . , n}, i.e., π(i) ̸= π(j) for i ̸= j. The permutation matrix A ∈ Rn×n associated with π has entries Aij = 1 if π(i) = j and Aij = 0, otherwise. Example Permutation matrices are another example of orthogonal matrices. Indeed, AT is the permutation matrix associated with the permutation σ defined as σ (j) = i whenever π(i) = j. Hence, AT A = I, i.e., A is an orthogonal matrix. Challenge For every permutation matrix A there exists a positive integer k such that Ak = I.Robert Weismantel November 8, 2024 14 / 16 A first observation about orthogonal matrices Proposition Orthogonal matrices preserve norm and inner product of vectors. In other words, if Q ∈ Rn×n is orthogonal then, for all x, y ∈ Rn ∥Qx∥ = ∥x∥ and (Qx) ⊤(Qy) = x ⊤y Proof of the second inequality. For x, y ∈ Rn, (Qx)⊤(Qy ) = x ⊤Q⊤Qy = x ⊤Iy = x ⊤y. Proof of the first equality. For x ∈ Rn we have that ∥Qx∥ ≥ 0 and ∥x∥ ≥ 0. Then ∥Qx∥2 = (Qx)⊤(Qx) = x ⊤x = ∥x∥2 ⇒ ∥Qx∥ = ∥x∥. Robert Weismantel November 8, 2024 15 / 16 Projections with Orthonormal Basis The message here An access to an orthonormal basis simplifies calculations for projections. Observation Let S be a subspace of Rm and q1, . . . , qn an orthonormal basis for S. Let Q = [ q1 , · · · , qn ] ∈ Rm×n. The projection matrix that projects to S is given by QQ⊤ and the least squares solution attaining minx∈Rn ∥Qx − b∥2 is given by ˆx = Q⊤b. Remark When Q is square then QQ⊤ is simply the identity corresponding to projecting to Rn. For x ∈ Rn it writes it a linear combination of the orthonormal basis. x = q1 ( q⊤ 1 x) + q2 ( q⊤ 2 x) + · · · + qn ( q⊤ n x) . Robert Weismantel November 8, 2024 16 / 16","libVersion":"0.3.2","langs":""}