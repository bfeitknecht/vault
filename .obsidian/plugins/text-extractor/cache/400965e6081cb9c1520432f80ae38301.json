{"path":"sem1/LinAlg/UE/s/LinAlg-u10-s.pdf","text":"D-INFK Linear Algebra HS 2023 Afonso Bandeira Bernd G¨artner Solution for Assignment 10 1. a) Let Cij be the co-factors of A where i, j ∈ [5]. Note that by combining Propositions 5.1.13 and 5.1.9, we get detA 5.1.9 = detA⊤ 5.1.13 = 5∑ j=1(A⊤)3,j(C⊤)3,j = 5∑ i=1 Ai,3Ci,3 = 0C1,3 + 0C2,3 + bC3,3 + 0C4,3 + 0C5,3 = b · (−1) (3+3) · ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ 0 1 4 c a 5 4 −1 0 −2 1 0 0 −4 3 1 ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ . This is also sometimes called expansion of the determinant along the third column. In particular, we chose the third column because it contains many zeroes and hence many terms disappeared. In order to compute ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ 0 1 4 c a 5 4 −1 0 −2 1 0 0 −4 3 1 ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ we use the same trick again for the first column. In this way we obtain ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ 0 1 4 c a 5 4 −1 0 −2 1 0 0 −4 3 1 ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ = a · (−1) (2+1) ∣ ∣ ∣ ∣ ∣ ∣ 1 4 c −2 1 0 −4 3 1 ∣ ∣ ∣ ∣ ∣ ∣ . We repeat this one more time for the third column of ∣ ∣ ∣ ∣ ∣ ∣ 1 4 c −2 1 0 −4 3 1 ∣ ∣ ∣ ∣ ∣ ∣ to get ∣ ∣ ∣ ∣ ∣ ∣ 1 4 c −2 1 0 −4 3 1 ∣ ∣ ∣ ∣ ∣ ∣ = c · (−1) (1+3) · ∣ ∣ ∣ ∣−2 1 −4 3 ∣ ∣ ∣ ∣ + 1 · (−1) (3+3) · ∣ ∣ ∣ ∣ 1 4 −2 1 ∣ ∣ ∣ ∣ . We can compute these 2 × 2 determinants using the formula from Proposition 5.1.3 to get ∣ ∣ ∣ ∣ −2 1 −4 3 ∣ ∣ ∣ ∣ = −2 and ∣ ∣ ∣ ∣ 1 4 −2 1 ∣ ∣ ∣ ∣ = 9. Putting everything together, we obtain detA = b · (−1) (3+3) (a · (−1) (2+1) (c · (−1) (1+3) · (−2) + 1 · (−1) (3+3) · 9)) = ab(2c − 9). We conclude that detA = 0 if and only if a = 0, or b = 0, or c = 9 2 . b) As it turns out, we only need to perform one step of Gauss elimination on B to obtain U : B =   1 2 −3 2 6 0 −1 −2 2   →   1 2 −3 0 2 6 0 0 −1   =: U. Using Proposition 5.1.8, we see that det(U ) = −2. By Proposition 5.1.19 (or the discussion in Section 5.1.6), we know that the determinant of U is the same as the determinant of B (we did not swap any rows). Hence, we conclude det(B) = −2. 2. a) As the hint suggests, we start by using Definition 5.1.6 for the determinant of M , i.e. we have detM = ∑ σ∈Πn sign(σ) n∏ i=1 Mi,σ(i) where Πn is the set of all permutations on n elements. The key observation for this exercise is that only those permutations σ ∈ Πn that satisfy σ(1), . . . , σ(m) ∈ {1, . . . , m} will contribute to this sum. To see this, let σ ∈ Πn be a permutation with σ(i) > m for some i ∈ [m]. By the pigeonhole principle, there must exist j ∈ [n] \\ [m] with σ(j) ∈ [m]. But by the shape of M , we must have Mj,σ(i) = 0 and hence the contribution of σ to the sum is 0. In particular, the relevant (those that contribute non-zero terms to the sum) permutations σ ∈ Πn satisfy σ(i) ∈ [m] for all i ∈ [m] and σ(j) ∈ [n] \\ [m] for all j ∈ [n] \\ [m]. In other words, restricting such a permutation σ to [m] yields a permutation on m elements, and restricting σ to [n] \\ [m] yields a permutation on n − m elements. Conversely, any two permutations σ1 ∈ Πm and σ2 ∈ Πn−m yield a permutation σ ∈ Πn that contributes to the sum (define σ(i) = σ1(i) for i ∈ [m] and σ(j) = m + σ2(j − m) for j ∈ [n] \\ [m]). Observe that the number of inversions in σ is exactly the number of inversions in σ1 plus the number of inversions in σ2. Hence, we always have sign(σ) = sign(σ1)sign(σ2) in this correspondence. We conclude that we can rewrite the sum as detM = ∑ σ∈Πn sign(σ) n∏ i=1 Mi,σ(i) = ∑ σ1∈Πm ∑ σ2∈Πn−m sign(σ1)sign(σ2) m∏ i=1 Mi,σ1(i) n∏ j=m+1 Mj, j+σ2(j−m). Next, observe that the terms Mi,σ1(i) are always in the A-part of M , i.e. we have Mi,σ1(i) = Ai,σ1(i). Similarly, the terms Mj, j+σ2(j−m) are always in the C-part of M , i.e. we have Mj, j+σ2(j−m) = Cj−m,σ2(j−m). Hence, we can further rewrite the sum as detM = ∑ σ1∈Πm ∑ σ2∈Πn−m sign(σ1)sign(σ2) m∏ i=1 Mi,σ1(i) n∏ j=m+1 Mj, j+σ2(j−m) = ∑ σ1∈Πm ∑ σ2∈Πn−m sign(σ1)sign(σ2) m∏ i=1 Ai,σ1(i) n∏ j=m+1 Cj−m, σ2(j−m) = ∑ σ1∈Πm sign(σ1) m∏ i=1 Ai,σ1(i)   ∑ σ2∈Πn−m sign(σ2) n∏ j=m+1 Cj−m, σ2(j−m)   =   ∑ σ1∈Πm sign(σ1) m∏ i=1 Ai,σ1(i)     ∑ σ2∈Πn−m sign(σ2) n∏ j=m+1 Cj−m, σ2(j−m)   =   ∑ σ1∈Πm sign(σ1) m∏ i=1 Ai,σ1(i)     ∑ σ2∈Πn−m sign(σ2) n−m∏ j=1 Cj, σ2(j)   = det(A)det(C) which concludes the proof. b) In order to calculate the determinant of M using the previous result, we must first bring it into the right form. Clearly, M already contains a lot of zero entries. In the end, we want to have a block of zeroes in the bottom left corner. We can use that transposing the matrix does not change its deter- minant. Moreover, by Proposition 5.1.18, swapping two rows of a matrix negates its determinant. Hence we proceed as follows: we first transpose M and then swap the second row and fourth row, as well as the third and sixth row of the resulting matrix. In this way, we obtain the matrix M ′ =         2 9 1 3 2 8 4 0 0 5 5 3 7 4 0 7 2 1 0 0 0 2 3 8 0 0 0 0 0 2 0 0 0 0 1 7         . Using the result from the previous subtask and some more row swaps as well as the formula for the determinant of triangular matrices, we get detM = (−1) 2detM ′ = detM ′ = ∣ ∣ ∣ ∣ ∣ ∣ 2 9 1 4 0 0 7 4 0 ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ 2 3 8 0 0 2 0 1 7 ∣ ∣ ∣ ∣ ∣ ∣ = (−1) 2 ∣ ∣ ∣ ∣ ∣ ∣ 4 0 0 7 4 0 2 9 1 ∣ ∣ ∣ ∣ ∣ ∣ (−1) ∣ ∣ ∣ ∣ ∣ ∣ 2 3 8 0 1 7 0 0 2 ∣ ∣ ∣ ∣ ∣ ∣ = −16 · 4 = −64. 3. a) We prove this by induction over k. • Property: T (α1u1 + · · · + αkuk) = α1T (u1) + · · · + αkT (uk) for all u1, . . . , uk ∈ U and all α1, . . . , αk ∈ R. • Base case: For k = 1, the property is true by linearity of T . • Induction step: Fix a natural number 1 ≤ k and assume that the property is true for k (in- duction hypothesis). We prove that the property is true for k + 1. Let u1, . . . , uk+1 ∈ U and α1, . . . , αk+1 ∈ R be arbitrary. By linearity of T , we have T (α1u1 + · · · + αk+1uk+1) = T (α1u1 + · · · + αkuk) + αk+1T (uk+1). Moreover, we have T (α1u1 + · · · + αkuk) = α1T (u1) + · · · + αkT (uk) by the induction hypothesis. Plugging both together yields the desired result T (α1u1 + · · · + αk+1uk+1) = α1T (u1) + · · · + αkT (uk) + αk+1T (uk+1). b) We explicitly construct such a function T . First, we define T (ui) = vi for all i ∈ [n]. Now let x ∈ U be arbitrary. Since u1, . . . , un is a basis of U , there exist unique scalars α1, . . . , αn ∈ R with x = α1u1 + · · · + αnun. Hence, we can define T (x) = α1T (u1) + · · · + αnT (un). Since this works for any x ∈ U and is consistent with T (ui) = vi for all i ∈ [n], it follows that T is a well-defined function on U . It remains to argue that T is linear. For this, let x, y ∈ U and c ∈ R be arbitrary. Let α1, . . . , αn ∈ R and β1, . . . , βn ∈ R be the unique coefficients such that x = α1u1 + · · · + αnun and y = β1u1 + · · · + βnun. We have T (x + y) = T ((α1 + β1)u1 + · · · + (αn + βn)un) = (α1 + β1)T (u1) + · · · + (αn + βn)T (un) = (α1T (u1) + · · · + αnT (un)) + (β1T (u1) + · · · + βnT (un)) = T (x) + T (y) and also T (cx) = T (cα1u1 + · · · + cαnun) = cα1T (u1) + · · · + cαnT (un) = c (α1T (u1) + · · · + αnT (un)) = cT (x) by definition of T . We conclude that T is indeed linear. c) Consider an arbitrary non-zero vector x ∈ Rn. If T were linear, we would have T (−x) = −T (x). But this is clearly not the case, as we have 0 ̸= T (x) = ∥ x ∥ = ∥ −x ∥ = T (−x). We conclude that T cannot be linear. d) Let x ∈ Rn be arbitrary. We directly observe that L ◦ T (x) = L(T (x)) = L(Ax) = B(Ax) = BAx. 4. a) By definition, we have C11 = (−1)2det( [ d ]) = d. Similarly, we obtain C12 = (−1) 3det [c ] = −c C21 = (−1) 3det [b ] = −b C22 = (−1) 4det [a ] = a. b) We directly compute this expression using our results from the last subtask. We get 1 det(A) C⊤ = 1 ad − bc [ d −b −c a ] . Observe that this is exactly the formula for inverses of 2 × 2 matrices. 5. a) Using the determinant formula for 2 × 2 matrices, we directly obtain det(A) = det[ 0 a −a 0 ] = a2. b) There are of course various ways to calculate determinants. One way is to use Proposition 5.1.13 (expansion along the first row) to get detB = b · (−1) 3 · det( [−b 2 1 0 ] ) + (−1) · (−1) 4 · det( [−b 0 1 −2 ]) = 2b − 2b = 0. c) Let A be an arbitrary n × n matrix. Proposition 5.1.19 states that the determinant is linear in each row. To illustrate how we can use this here, let a1, . . . , an ∈ Rn denote the rows of A, i.e. we have A =   |a⊤ 1|... ... ...|a⊤ n|    . Using Proposition 5.1.19 n times (once for each row), we hence calculate det(−A) = det   |−a⊤ 1|... ... ...|−a⊤ n|    = (−1) ndet   |a⊤ 1|... ... ...|a⊤ n|    = (−1) ndet(A). d) As it turns out, the determinant of C must be 0. To see this, observe that since C is skew-symmetric, we must have det(C) = det(−C⊤) = det(−C). But by the previous subtask, we also know det(−C) = (−1)ndet(C). Putting both things together, we obtain det(C) = (−1)ndet(C). For odd n, this implies that det(C) must be zero. 6. a) Let x denote the vector of x-coordinates x = [px,1 . . . px,n]⊤ and let y denote the vector of y-coordinates y = [ py,1 . . . py,n]⊤. The smoothness property can be rewritten as pj − 1 2 (pj−1 + pj+1) = 0 ∀ j ∈ {2, . . . , n − 1} p1 − 1 2 (pn + p2) = 0 pn − 1 2 (pn−1 + p1) = 0 which translates to Ax = 0 and Ay = 0 with A =           1 − 1 2 0 · · · 0 − 1 2 − 1 2 1 − 1 2 · · · 0 0 0 . . . . . . . . . 0 0 ... . . . . . . . . . . . . ... 0 · · · 0 − 1 2 1 − 1 2 − 1 2 0 · · · 0 − 1 2 1           . The matrix A can also be written as A = I − 1 2 (T + E1,n) − 1 2 (T + E1,n) T where T is the matrix with ones on the first strict upper diagonal (i.e. the entries where the row coefficient i and column coefficient j satisfy j = i+1 for i ∈ {1, ..., n−1}) and zeroes everywhere else, and E1,n has a single non-zero entry in row 1 and column n that is equal to 1. We also want to satisfy the constraints pjs = cs for all s ∈ [k]. Let xc denote the vector of x-coordinates of the locations, i.e. xc = [cx,1 . . . cx,k]⊤ and let yc denote the vector of y- coordinates yc = [cy,1 . . . cy,n]⊤. Then, the location constraints can be written as Bx = xc and By = yc where the matrix B ∈ Rk×n is given by Bs,r = δr,js for all s ∈ [k] and r ∈ [n] (recall that the Kronecker-Delta δr,js is one if r = js and zero otherwise). In other words, an entry Bs,r is one whenever the vertex pr should match location cs according to the prescribed correspondence C, and Bs,r is zero otherwise. The final systems of linear equations hence are [A B ] x = [0n xc ] and [A B ] y = [0n yc ] where 0n denotes the n dimensional all-zero vector. b) Let S = [A B ] denote the system matrix. Indeed, the system matrix is the same for both linear systems. Since A ∈ Rn×n and B ∈ Rk×n, the system matrix S is in R(n+k)×n. This implies that S has rank at most n. c) We are solving for the curve vertex positions in the least squares sense for the values n = 6, k = 3, C = {j1 = 1, j2 = 3, j3 = 5} and c1 = [cx,1 cy,1 ] = [2 2 ] c2 = [cx,2 cy,2 ] = [6 2 ] c3 = [cx,3 cy,3 ] = [4 0 ] . Our strategy is to first combine the two linear systems in one larger system and then solve this using the least squares method. Observe that the two systems [A B ] x = [0n xc ] and [A B ] y = [0n yc ] can be rewritten as M [x y ] =     A 0n,n B 0k,n 0n,n A 0k,n B     [x y ] =     0n xc 0n yc     where M is a 2(n + k) × 2n matrix block matrix (meaning that we put it together from smaller matrices) and 0n,n and 0k,n are zero-matrices of corresponding dimensions. The normal equations hence yield M ⊤M [x y ] = M ⊤     0n xc 0n yc     . Plugging in the values of this specific example for A, B, xc, and yc, we get S = [A B ] =                1 −1/2 0 0 0 −1/2 −1/2 1 −1/2 0 0 0 0 −1/2 1 −1/2 0 0 0 0 −1/2 1 −1/2 0 0 0 0 −1/2 1 −1/2 −1/2 0 0 0 −1/2 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0                , [ 0 xc ] =                0 0 0 0 0 0 2 6 4                , [ 0 yc ] =                0 0 0 0 0 0 2 2 0                . The exact final solution is (obtained by solving the normal equations with a computer) x = [76/29 4 156/29 148/29 4 84/29 ]⊤ and y = [ 52/29 60/29 52/29 28/29 12/29 28/29 ]⊤ . A drawing of this solution is provided in Figure 1 below. Figure 1: A drawing of the solution.","libVersion":"0.5.0","langs":""}