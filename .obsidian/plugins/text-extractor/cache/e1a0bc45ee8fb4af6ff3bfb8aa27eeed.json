{"path":"HS23/LinAlg/VRL/extra/LinAlg-script-HS23.pdf","text":"Lineare Algebra Herbstsemester 2011 Studiengang Informatik Martin H. Gutknecht ETH Z¨urich c⃝ Martin H. Gutknecht, 2000-2011, with minor modiﬁcations by Daniel Kressner . 2 Lineare Algebra (2011) Inhalt Inhaltsverzeichnis 0 Vorkenntnisse — Was man h¨atte lernen sollen 0-1 0.1 Lineare Gleichungssysteme . . . . . . . . . . . . . . 0-1 0.2 Vektorrechnung in der Ebene . . . . . . . . . . . . 0-4 0.3 Komplexe Zahlen . . . . . . . . . . . . . . . . . . . 0-5 0.4 Polynome . . . . . . . . . . . . . . . . . . . . . . . 0-7 0.5 Das griechische Alphabet . . . . . . . . . . . . . . . 0-8 0.6 Notation . . . . . . . . . . . . . . . . . . . . . . . . 0-8 1 Lineare Gleichungssysteme — Gauss-Elimination 1-1 1.1 Gauss-Elimination: der regul¨are Fall . . . . . . . . . 1-1 1.2 Gauss-Elimination: der allgemeine Fall . . . . . . . 1-9 1.3 Die L¨osungsmenge eines linearen Gleichungssystems . . . . . . . . . . . . . . . . . . 1-17 2 Matrizen und Vektoren im R n und C n 2-1 2.1 Matrizen, Zeilen- und Kolonnenvektoren . . . . . . 2-1 2.2 Das Rechnen mit Matrizen und Vektoren . . . . . . 2-4 2.3 Die Transponierte einer Matrix; symmetrische und Hermitesche Matrizen . . . . . . 2-12 2.4 Das Skalarprodukt und die Norm von Vektoren; L¨angen und Winkel . . . . . . . . . . . . 2-16 2.5 Das ¨aussere Produkt und die orthogonale Projektion auf eine Gerade . . . . . . . . . . . . . . 2-23 2.6 Matrizen als lineare Abbildungen . . . . . . . . . . 2-25 2.7 Die Inverse einer Matrix . . . . . . . . . . . . . . . 2-26 2.8 Orthogonale und unit¨are Matrizen . . . . . . . . . . 2-29 2.9 Strukturierte Matrizen . . . . . . . . . . . . . . . . 2-31 3 Die LR–Zerlegung 3-1 LA-Skript Inhalt–1 11. April 2016 c⃝M.H. Gutknecht Inhalt Lineare Algebra (2011) 3.1 Die Gauss-Elimination als LR–Zerlegung . . . . . . 3-1 3.2 Die Gauss-Elimination als LR–Zerlegung: der allgemeine Fall . . . . . . . . . . . . . . . . . . 3-9 3.3 Block–LR–Zerlegung und LR–Updating . . . . . . . 3-12 3.4 Die Cholesky-Zerlegung . . . . . . . . . . . . . . . . 3-16 4 Vektorr¨aume 4-1 4.1 Deﬁnition und Beispiele . . . . . . . . . . . . . . . 4-1 4.2 Unterr¨aume, Erzeugendensysteme . . . . . . . . . . 4-6 4.3 Lineare Abh¨angigkeit, Basen, Dimension . . . . . . 4-8 4.4 Basiswechsel, Koordinatentransformation . . . . . . 4-18 5 Lineare Abbildungen 5-1 5.1 Deﬁnition, Beispiele, Matrixdarstellung . . . . . . . 5-1 5.2 Kern, Bild und Rang . . . . . . . . . . . . . . . . . 5-7 5.3 Matrizen als lineare Abbildungen . . . . . . . . . . 5-11 5.4 Aﬃne R¨aume und die allgemeine L¨osung eines inhomogenen Gleichungssystems . . . . . . . . 5-17 5.5 Die Abbildungsmatrix bei Koordinaten- transformation . . . . . . . . . . . . . . . . . . . . 5-18 6 Vektorr¨aume mit Skalarprodukt 6-1 6.1 Normierte Vektorr¨aume . . . . . . . . . . . . . . . . 6-1 6.2 Vektorr¨aume mit Skalarprodukt . . . . . . . . . . . 6-2 6.3 Orthonormalbasen . . . . . . . . . . . . . . . . . . 6-5 6.4 Orthogonale Komplemente . . . . . . . . . . . . . . 6-13 6.5 Orthogonale und unit¨are Basiswechsel und Koordinatentransformationen . . . . . . . . . . 6-15 6.6 Orthogonale und unit¨are Abbildungen . . . . . . . 6-18 6.7 Normen von linearen Abbildungen (Operatoren) und Matrizen . . . . . . . . . . . . . 6-19 7 Die Methode der kleinsten Quadrate und die QR–Zerlegung einer Matrix 7-1 7.1 Orthogonalprojektionen . . . . . . . . . . . . . . . 7-1 7.2 Die Methode der kleinsten Quadrate . . . . . . . . 7-5 7.3 Die QR–Zerlegung einer Matrix . . . . . . . . . . . 7-8 7.4 Die QR–Zerlegung mit Pivotieren . . . . . . . . . . 7-12 8 Determinanten 8-1 c⃝M.H. Gutknecht 11. April 2016 LA-Skript Inhalt–2 Lineare Algebra (2011) Inhalt 8.1 Permutationen . . . . . . . . . . . . . . . . . . . . 8-1 8.2 Determinante: Deﬁnition, Eigenschaften . . . . . . 8-3 8.3 Entwicklung nach Zeilen und Kolonnen . . . . . . . 8-9 8.4 Determinanten von Blockdreiecksmatrizen . . . . . 8-12 9 Eigenwerte und Eigenvektoren 9-1 9.1 Eigenwerte und Eigenvektoren von Matrizen und linearen Abbildungen . . . . . . . . . 9-1 9.2 ¨Ahnlichkeitstransformationen; die Eigenwertzerlegung . . . . . . . . . . . . . . . . 9-9 9.3 Eigenwerte und Eigenvektoren symmetri- scher und Hermitescher Matrizen . . . . . . . . . . 9-16 9.4 Die Jordansche Normalform . . . . . . . . . . . . . 9-19 10 Anwendungen der Eigenwertzerlegung 10-1 10.1 Homogene lineare Diﬀerentiagleichungen mit konstanten Koeﬃzienten . . . . . . . . . . . . . 10-1 10.2 Funktionen von Matrizen . . . . . . . . . . . . . . . 10-7 10.3 Reelle lineare Diﬀerentialgleichungen mit komplexen Eigenwerten . . . . . . . . . . . . . 10-8 10.4 Quadratische Formen und ihre Hauptachsentransformation . . . . . . . . . . . . . 10-10 10.5 Quadratische Funktionen . . . . . . . . . . . . . . . 10-15 10.6 Die Spektralnorm . . . . . . . . . . . . . . . . . . . 10-16 11 Die Singul¨arwertzerlegung 11-1 11.1 Die Singul¨arwertzerlegung: Herleitung . . . . . . . . 11-1 11.2 Die Singul¨arwertzerlegung: Folgerungen . . . . . . . 11-5 LA-Skript Inhalt–3 11. April 2016 c⃝M.H. Gutknecht Inhalt Lineare Algebra (2011) c⃝M.H. Gutknecht 11. April 2016 LA-Skript Inhalt–4 Lineare Algebra (2009) Kapitel 0 — Vorkenntnisse Kapitel 0 Vorkenntnisse — Was man h¨atte lernen sollen Wir wollen hier kurz einige Themen aufgreifen, die die meisten Stu- dierenden in der Mittelschule durchgenommen haben sollten. Keine Angst: alles wird in dieser oder einen anderen Vorlesung ex- plizit oder implizit (in verallgemeinerter Form) noch einmal behan- delt. L¨ucken sind also keine Katastrophe, aber ein gewisser, zeitlich begrenzter Nachteil. Bemerkung: Die Figuren sind zu erg¨anzen. 0.1 Lineare Gleichungssysteme Eine einzige lineare Gleichung in einer Unbekannten x, ax = b , hat genau eine L¨osung, n¨amlich x = b a , ausser wenn a = 0. In dieser Ausnahmesituation gibt es zwei F¨alle: b = 0 : jedes x ist L¨osung ⇒ es gibt ∞ L¨osungen, b ̸= 0 : kein x ist L¨osung ⇒ es gibt keine L¨osungen. Betrachten wir als n¨achstes zwei Gleichungen in zwei Unbekannten. Beispiel 0.1: 2x1 − 4x2 = 8 5x1 + 3x2 = 7 . (1) Frage: Gibt es eine L¨osung und wenn ja, gibt es nur eine? Diese Frage nach Existenz und Eindeutigkeit tritt in der Mathematik bekanntlich immer wieder auf. Wir k¨onnen sie hier auf konstruktive Art beantworten: wir k¨onnen alle L¨osungen ausrechnen. Wir l¨osen die eine Gleichung (z.B. die erste) nach der einen Unbekannten (z.B. der ersten) auf: x1 = 4 + 2x2 . (2) Dann setzen wir dies in die andere Gleichung ein: 5(4 + 2x2) + 3x2 = 7 oder 13x2 = −13 , d.h. x2 = −1. Einsetzen in (2) liefert dann sofort x1 = 2, also: c⃝M.H. Gutknecht 11. April 2016 LA-Skript 0-1 Kapitel 0 — Vorkenntnisse Lineare Algebra (2009) x1 = 2 , x2 = −1 . Durch Einsetzen ins urspr¨ungliche System (1) veriﬁziert man, dass die- ses Zahlenpaar eﬀektiv eine L¨osung ist. Wir werden sehen, dass diese Kontrolle gar nicht n¨otig ist. Es ist auch die einzige L¨osung, denn wir haben sie ohne Wahlm¨oglichkeit abgeleitet. ♦ Gibt es zu zwei Gleichungen in zwei Unbekannten immer genau eine L¨osung? Beispiel 0.2: 2x1 − 4x2 = 8 x1 − 2x2 = 3 . Dieses Gleichungssystem hat oﬀensichtlich keine L¨osung. Multipliziert man n¨amlich die zweite Gleichung mit 2, erh¨alt man 2x1 − 4x2 = 6. Dies widerspricht der ersten Gleichung. ♦ Beispiel 0.3: 2x1 − 4x2 = 8 x1 − 2x2 = 4 . Nun ist die erste Gleichung einfach das Doppelte der zweiten, und of- fensichtlich besteht nun kein Widerspruch mehr. Man kann sogar die eine Variable, z.B. x2, frei w¨ahlen und erh¨alt f¨ur jede Wahl eine ande- re L¨osung. Das heisst, es gibt eine ganze (einparametrige) Schar von L¨osungen. ♦ Die Beispiele 2 und 3 zeichnen sich oﬀensichtlich dadurch aus, dass auf der linken Seite die eine Gleichung ein Mehrfaches der anderen ist. Bei Beispiel 3 besteht diese Abh¨angigkeit auch auf der rechten Seite. Beides sind oﬀensichtlich Ausnahmef¨alle. Also: In der Regel gibt es zu einer linearen Gleichung in einer Un- bekannten und zu zwei linearen Gleichungen in zwei Unbekannten genau eine L¨osung; aber in Ausnahmef¨allen kann es keine oder un- endlich viele geben. Wir werden sehen, dass das Analoge auch bei n Gleichungen in n Unbekannten richtig ist. Im Falle von n linearen Gleichungen in n Unbekannten gilt: • In der Regel gibt es genau eine L¨osung. • Es gibt eine Schar von L¨osungen, wenn eine Gleichung als eine Summe von Mehrfachen (“eine Linearkombination”) der anderen dargestellt werden kann. • Es gibt keine L¨osung, wenn eine solche Abh¨angigkeit zwar auf der linken Seite besteht, nicht aber gleichzeitig f¨ur die rechte Seite gilt. Was gilt bei m Gleichungen in n Unbekannten, wenn m ̸= n ? LA-Skript 0-2 11. April 2016 c⃝M.H. Gutknecht Lineare Algebra (2009) Kapitel 0 — Vorkenntnisse Beispiel 0.4: 2x1 − 4x2 = 8 5x1 + 3x2 = 7 x1 + x2 = 3 . Dieses System mit m = 3, n = 2 hat keine L¨osung. Denn aus Bei- spiel 1 wissen wir, dass die ersten zwei Gleichungen nur die L¨osung x1 = 2 , x2 = −1 zulassen, und die steht im Widerspruch zur dritten Gleichung. ♦ Ersetzt man die 3 auf der rechten Seite der letzten Gleichung durch eine 1, so gibt es oﬀenbar genau eine L¨osung. Erg¨anzt man Bei- spiel 0.3 durch die Gleichung −3x1 + 6x2 = −12, so ﬁndet man sogar ein (ziemlich triviales) Beispiel von drei Gleichungen in zwei Unbekannten mit unendlich vielen L¨osungen. Wenn m > n (“mehr Gleichungen als Unbekannte”), so gibt es in der Regel keine L¨osung. Es kann aber eine oder sogar unendlich viele geben, wenn gewisse Abh¨angigkeiten zwischen den Gleichun- gen bestehen. Beispiel 0.5: x1 − x2 + 2x3 = 1 2x1 + x2 + x3 = 5 . (3) Hier ist m = 2, n = 3. Wenn man von der zweiten Gleichung das Doppelte der ersten subtrahiert, folgt 3x2 − 3x3 = 3 oder x2 = x3 + 1. Einsetzen in die erste oder die zweite Gleichung ergibt dann noch x1 = 2 − x3. Man kann also x3 frei w¨ahlen. Nennen wir den freien Parameter t, so l¨asst sich jede L¨osung als Zahlentrippel der Form x1 = 2 − t , x2 = 1 + t , x3 = t darstellen. Mit anderen Worten, die zwei Gleichungen (3) haben die L¨osungsmenge {(2 − t, 1 + t, t); t ∈ R} , wenn wir uns auf reelle L¨osungen beschr¨anken. ♦ Man k¨onnte oﬀensichtlich auch ein Beispiel mit m = 2, n = 3 konstruieren, das keine L¨osung hat. Wir werden sehen, dass gilt: Wenn m < n (“weniger Gleichungen als Unbekannte”), so gibt es in der Regel eine Schar von L¨osungen. Es kann aber in Ausnah- mef¨allen auch keine geben, aber nie nur endlich viele. Wir werden im folgenden Kapitel 1 ganz allgemein ein lineares Glei- chungssystem von m Gleichungen in n Unbekannten l¨osen und seine L¨osungsmenge charakterisieren. c⃝M.H. Gutknecht 11. April 2016 LA-Skript 0-3 Kapitel 0 — Vorkenntnisse Lineare Algebra (2009) 0.2 Vektorrechnung in der Ebene Die Punkte in der Ebene kann man einerseits durch Ortsvektoren, anderseits durch kartesische Koordinaten charakterisieren: −−→ OW = w = ( u v ) , −→ OZ = z = ( x y ) , . . . . Es gibt noch viele andere Bezeichnungsweisen, aber wir verwenden hier diese, weil sie mit der sp¨ater verwendeten konsistent ist. Solche Vektoren kann man addieren und mit einer Zahl (“Skalar”) multiplizieren: w + z = ( u v ) + ( x y ) :≡ ( u + x v + y ) , αw = α ( u v ) :≡ ( αu αv ) . Diese Operationen haben einfache geometrische Interpretationen. Insbesondere kann der Vektor von W nach Z als Diﬀerenz darge- stellt werden: −−→ W Z = z − w . Ergibt sich −−→ OW = w durch Drehung von −→ OZ = z um den Winkel ϕ, so kann man dies ausdr¨ucken durch u = x cos ϕ − y sin ϕ , v = x sin ϕ + y cos ϕ . (4) Der Spezialfall (x, y) = (1, 0) illustriert die Deﬁnition von Sinus und Cosinus. Die Beziehungen (4) kann man zusammenfassen in ( u v ) = ( cos ϕ − sin ϕ sin ϕ cos ϕ ) ( x y ) , (5) oder k¨urzer als w = Q z (6) mit der 2×2 Matrix Q = ( cos ϕ − sin ϕ sin ϕ cos ϕ ) . (7) Q z ist ein Matrix-Vektor-Produkt. Man k¨onnte hier noch vieles anf¨ugen, was vielleicht schon bekannt ist: die L¨ange eines Vektors, das Skalarprodukt von zwei Vektoren, die Determinante und die Eigenwerte einer quadratischen Matrix. Die Verallgemeinerung des Vektor- und des Matrixbegriﬀes und die Behandlung der obgenannten Operationen und Begriﬀe (nicht aber des sogenannten Vektorproduktes!) ist das Thema dieser Vorlesung: lineare Algebra. LA-Skript 0-4 11. April 2016 c⃝M.H. Gutknecht Lineare Algebra (2009) Kapitel 0 — Vorkenntnisse 0.3 Komplexe Zahlen Bemerkung: Komplexe Zahlen werden ausf¨uhrlich in der Analysis behandelt. Wir beschr¨anken uns auf wenige Grundeigenschaften, die wir gelegentlich brauchen werden und k¨ummern uns nicht um vollst¨andige Herleitungen. Man k¨onnte die Punkte in der Ebene auch in der Form w = u + i v , z = x + i y , . . . darstellen, wobei man zun¨achst i als eine die zweiten Komponenten charakterisierende Flagge auﬀassen k¨onnte. Dann ist klar, dass das Folgende vern¨unﬁge Deﬁnitionen f¨ur die Addition zweier solcher “Punkte” und f¨ur die Multiplikation mit einer rellen Zahl α ist: w + z = (u + i v) + (x + i y) = (u + x) + i (v + y) , (8) αw = α(u + i v) = (αu + i αv) . (9) Von entscheidender Bedeutung ist jetzt aber, dass man f¨ur diese Zahlen eine die ¨ublichen Regeln erf¨ullende Multiplikation deﬁnie- ren kann, wenn man annimmt, dass formal die binomischen Formeln und die Relation i2 = −1 gelten: w z = (u + i v)(x + i y) = u x + i (v x + u y) + i2 (v y) (10) = (u x − v y) + i (v x + u y) . (11) Mit dieser Interpretation von i nennt man z = x + i y eine kom- plexe Zahl [complex number]; x ist der Realteil [real part], y der Imagin¨arteil [imaginary part]: z = x + i y ⇐⇒ x = Re z , y = Im z . Die Zahl ¯z = x − i y heisst konjugiert-komplex [conjugate com- plex] zu z. Die Menge der komplexen Zahlen bezeichnet man mit C. In An- betracht der geometrischen Interpretation als Punkte in der Ebene nennt man C auch die komplexe Ebene [complex plane]. Die Menge der rellen Zahlen R ist diejenige Teilmenge von C, deren Elemente Imagin¨arteil 0 haben: R = {z ∈ C; Im z = 0}. In der komplexen Ebene ist das die reelle Achse [real axis], w¨ahrend die Zahlen mit Realteil null die imagin¨are Achse [imaginary axis] bilden. Man beachte, dass z ¯z = x2 + y2 ≥ 0 , (12) wobei das Gleichheitszeichen nur gilt, wenn z = 0 ist. c⃝M.H. Gutknecht 11. April 2016 LA-Skript 0-5 Kapitel 0 — Vorkenntnisse Lineare Algebra (2009) Somit kann man stets die normale Quadratwurzel von z ¯z bilden: die nicht-negative reelle Zahl |z| :≡ √z ¯z (13) heisst Betrag [modulus] oder Absolutbetrag [absolute value] der komplexen Zahl z. Nach Pythagoras ist |z| gerade die L¨ange des Ortsvektors zum Punkt z = x + i y der komplexen Ebene. Die Beziehung (12) zeigt, wie man die Division deﬁnieren muss: z w = z ¯w w ¯w = τ z ¯w , wo τ :≡ 1 |w|2 . (14) In Real- und Imagin¨arteil von Z¨ahler und Nenner ausgedr¨uckt er- gibt sich z w = xu + yv u2 + v2 + i uy − xv u2 + v2 . (15) Mit diesen Deﬁnitionen gelten f¨ur die komplexen Zahlen die gleichen Rechenregeln wie f¨ur die reellen Zahlen. F¨ur den Absolutbetrag ist insbesondere folgendes richtig: |z + w| ≤ |z| + |w| , (16) |z w| = |z| |w| , (17) ∣ ∣ ∣ z w ∣ ∣ ∣ = |z| |w| . (18) Die komplexen Zahlen als Punkte der komplexen Ebene lassen sich auch in Polarkoordinaten darstellen: z = x + i y ⇐⇒ { x = r cos ϕ y = r sin ϕ falls    r = |z| , ϕ = arccos(x/r) = arcsin(y/r) . Es gilt also z = r (cos ϕ + i sin ϕ) . (19) Mit der ber¨uhmten Eulerschen Formel [Euler formula] eiϕ = cos ϕ + i sin ϕ (20) erhalten wir schliesslich z = r eiϕ . (21) Diese Darstellungen heisst Polarform [polar representation] von z. F¨ur die Exponentialfunktion mit komplexem Argument gelten die gleichen Regeln wie bei reellem Argument. Zum Beispiel ist ei(ϕ+ψ) = eiϕ eiψ , ei(ϕ−ψ) = eiϕ eiψ . (22) Wenn z = r eiϕ und w = s eiψ, so hat man also zw = rs ei(ϕ+ψ) , z w = r s ei(ϕ−ψ) . (23) LA-Skript 0-6 11. April 2016 c⃝M.H. Gutknecht Lineare Algebra (2009) Kapitel 0 — Vorkenntnisse 0.4 Polynome Sind a0, a1, . . . , an reelle oder komplexe Zahlen, so kann man das Polynom [polynomial] p(z) = anzn + an−1zn−1 + · · · + a1z + a0 vom H¨ochstgrade [maximum degree] n deﬁnieren. Ist an ̸= 0, so sagen wir, n sei der exakte Grad. Die Zahlen a0, a1, . . . , an bezeichnet man als die Koeﬃzienten [coeﬃcients] des Polynoms. F¨ur uns ist vor allem der Fall interessant, wo auch z reell oder komplex ist. Dann kann man p(z) einen (reellen oder komplexen) Wert zuweisen: p(z) ∈ R oder p(z) ∈ C . Die (reelle oder komplexe) Zahl z1 heisst Nullstelle [zero] von p, wenn p(z1) = 0 ist. Ist dies der Fall, so ist q(z) :≡ p(z) z − z1 (24) nach dem K¨urzen des Linearfaktors [linear factor] z − z1 ein Po- lynom vom Grade n − 1. Der ¨Ubergang von p(z) zu q(z) heisst Deﬂation [deﬂation] der Nullstelle z1. Sind die Koeﬃzienten a0, a1, . . . , an reell und beschr¨ankt man sich auf reelle z, so braucht ein Polynom bekanntlich keine Nullstelle zu haben (Bsp.: z2 + 1). Anders ist es, wenn wir komplexe Nullstellen zulassen; dann sind sogar komplexe Koeﬃzienten erlaubt: Satz 0.1 Ein (reelles oder komplexes) Polynom p vom exakten Grade n ≥ 1 hat in C (mindestens) eine Nullstelle: ∃z1 ∈ C : p(z1) = 0 . Mit Induktion folgert man daraus mittels Deﬂation: Korollar 0.2 Ein (reelles oder komplexes) Polynom p vom exak- ten Grade n ≥ 1 hat in C genau n Nullstellen, die aber nicht alle verschieden sein m¨ussen; genauer, p l¨asst sich als Produkt von n Linearfaktoren darstellen: p(z) = an(z − z1)(z − z2) · · · (z − zn) ≡ an n∏ k=1 (z − zk) . c⃝M.H. Gutknecht 11. April 2016 LA-Skript 0-7 Kapitel 0 — Vorkenntnisse Lineare Algebra (2009) 0.5 Das griechische Alphabet Das griechische Alphabet hat 24 Buchstaben, aber zu einzelnen gibt es zwei Schreibweisen: α alpha ι iota σ sigma β beta κ kappa ς sigma γ gamma λ lambda τ tau δ delta µ mu υ ypsilon ǫ epsilon ν nu φ phi ε epsilon ξ xi ϕ phi ζ zeta o omikron χ chi η eta π pi ψ psi θ theta ρ rho ω omega ϑ theta ̺ rho Γ Gamma Ξ Xi Φ Phi ∆ Delta Π Pi Ψ Psi Θ Theta Σ Sigma Ω Omega Λ Lambda Υ Ypsilon Die restlichen Grossbuchstaben sind gleich wie die entsprechenden des lateinischen Alphabets und sind deshalb f¨ur uns nutzlos. 0.6 Notation Im Prinzip ist jedermann frei, die Mathematik in eigenen Bezeich- nungen aufzuschreiben. Aber eine gute und systematische Notation erleichtert das Verst¨andnis. Wir werden f¨ur Vektoren und Matrizen kleine bzw. grosse fette Buchstaben verwenden, damit sie in jeder Formel leicht erkennbar sind: x, y, , ..., A, B, .... Der Nachteil ist, dass man das so nicht gut von Hand schreiben kann. Aber man kann in Handschrift auf den Unterschied zwischen gew¨ohnlichen und fetten Buchstaben verzichten, oder die Vektoren und Matrizen durch Unterstreichen markieren, d.h. zum Beispiel A schreiben statt A. Das Gleichheitszeichen hat in der Mathematik und Informatik min- destens drei verschiedene Bedeutungen, und wir werden versuchen, diese auseinander zu halten (was allerdings nicht immer eindeutig m¨oglich ist). Wir schreiben = f¨ur eine Gleichheit von mathematischen Objekten, := f¨ur eine Zuweisung, wie sie in Algorithmen und Programmen vorkommt, :≡ f¨ur eine Deﬁnition. LA-Skript 0-8 11. April 2016 c⃝M.H. Gutknecht Lineare Algebra (2009) Kapitel 1 — Lineare Gleichungssysteme Kapitel 1 Lineare Gleichungssysteme — Gauss-Elimination Die Gauss-Elimination 1 [Gauss elimination], oft einfach Gauss- Algorithmus [Gauss algorithm] genannt, ist die grundlegendste und weitverbreitetste Methode um lineare Gleichungssysteme zu l¨osen. Es ist im Prinzip jene Methode, die man schon in der Grund- schule lernt. Wir betrachten sie hier in systematischerer Weise. Zun¨achst einige Fakten: • Die Gauss-Elimination ist von grundlegender Bedeutung f¨ur das wissenschaftliche Rechnen [scientiﬁc computing] und allgemeiner, f¨ur die rechnergest¨utzten Wissenschaften [computational science and engineering]. Fast bei jeder Com- putersimulation sind irgendwo lineare Gleichungssysteme zu l¨osen, und oft sind diese sehr gross. • Im wissenschaftlichen Rechnen wird die Methode aber fast nur im Falle m = n eingesetzt. • Es kommen dabei allerdings z.T. komplizierte Varianten zum Einsatz, die in bezug auf Stabilit¨at, Rechengeschwindigkeit (auch Speicherzugriﬀ), Speicherplatz und Parallelisierbarkeit optimiert sind. • Wir wollen den allgemeinen Fall von m linearen Gleichungen in n Unbekannten analysieren und die entsprechende L¨osungs- menge beschreiben. • Die daraus ableitbare Theorie ist fundamental f¨ur die lineare Algebra. Wir werden uns sp¨ater immer wieder auf sie bezie- hen. 1.1 Gauss-Elimination: der regul¨are Fall Wir f¨uhren in diesem Abschnitt zun¨achst die Notation f¨ur allge- meine Systeme von m linearen Gleichungen in n Unbekannten ein und diskutieren dann die Gauss-Elimination im Normalfall m = n, beginnend mit Beispielen mit m = n = 3. Im n¨achsten Abschnitt werden wir dann den allgemeinen Fall behandeln. Ein Gleichungssystem [system of equations] von m linearen Glei- chungen in n Unbekannten hat die Form 1Carl Friedrich Gauss (30.4.1777 – 23.2.1855), ab 1807 Professor f¨ur Astronomie in G¨ottingen; einer der bedeutendsten Mathematiker aller Zeiten. c⃝M.H. Gutknecht 11. April 2016 LA-Skript 1-1 Kapitel 1 — Lineare Gleichungssysteme Lineare Algebra (2009) a11x1 + a12x2 + · · · + a1nxn = b1 a21x1 + a22x2 + · · · + a2nxn = b2 ... ... ... ... am1x1 + am2x2 + · · · + amnxn = bm . (1.1) Die gegebenen Zahlen aij (i = 1, . . . , m, j = 1, . . . , n) sind die Koeﬃzienten [coeﬃcients] des Systems, die ebenfalls gegebenen Zahlen bi (i = 1, . . . , m) bilden die rechte Seite [right-hand side] und die Gr¨ossen x1, x2, . . . , xn sind die Unbekannten [unknowns]. Falls m = n, nennen wir das System quadratisch [square]2. Ab Kapitel 2 werden wir das System (1.1) in der folgenden Form schreiben:      a11 a12 . . . a1n a21 a22 . . . a2n ... ... ... am1 am2 . . . amn      ︸ ︷︷ ︸ A      x1 x2 ... xn      ︸ ︷︷ ︸ x =      b1 b2 ... bm      ︸ ︷︷ ︸ b (1.2a) oder kurz, Ax = b (1.2b) mit der (Koeﬃzienten-)Matrix [matrix; pl. matrices] A und den Vektoren [vectors] x und b. Wir fassen in diesem Kapitel (1.2a) einfach als eine neue Schreib- weise f¨ur (1.1) auf und betrachten (1.2b) als eine Abk¨urzung f¨ur (1.2a). Es gibt deshalb im Moment nichts zu verstehen. In Kapi- tel 2 werden wir dann sehen, dass Ax als Produkt der Matrix A und des Vektors x aufgefasst werden kann — man muss dazu nur dieses Produkt richtig deﬁnieren. F¨ur die Praxis von grosser Bedeutung ist nur der Fall, wo m = n ist und es genau eine L¨osung gibt. Der allgemeine Fall, wo die Zahl m der Gleichungen und die Zahl n der Unbekannten nicht unbedingt gleich sind, ist aber f¨ur die Theorie der linearen Algebra wichtig. F¨ur diese liefert er fundamentale Aussagen. Wir behandeln prim¨ar reelle Systeme, aber alles gilt genau gleich f¨ur komplexe, d.h. solche mit komplexen Koeﬃzienten, komplexen Konstanten auf der rechten Seite und komplexen Unbekannten. In diesem Kapitel tragen wir die in (1.1) auftretenden Gr¨ossen in ein Eliminationsschema ein: x1 x2 . . . xn 1 a11 a12 . . . a1n b1 a21 a22 . . . a2n b2 ... ... ... ... am1 am2 . . . amn bm (1.3) 2Man beachte, dass es im Englischen zwei Bezeichnungen f¨ur “quadratisch” gibt: ein quadratisches Bild nennt man “square”, eine quadratische Gleichung “quadratic”. LA-Skript 1-2 11. April 2016 c⃝M.H. Gutknecht Lineare Algebra (2009) Kapitel 1 — Lineare Gleichungssysteme Wir k¨onnen uns damit nicht nur Schreibarbeit sparen, wichtiger ist, dass wir die in diesem Schema auszuf¨uhrenden Operationen besser veranschaulichen und damit auch leichter auf den Computer ¨ubertragen — sprich programmieren — k¨onnen als bei Verwendung des ausgeschriebenen Systems (1.1). Mit anderen Worten, das Sche- ma wird einerseits zum Rechnen von Hand angewandt (was wir nur zur Illustration ausf¨uhren) und erlaubt uns anderseits besser zu verstehen, was in der Rechnung abl¨auft. Das Schema (1.3) hat oﬀenbar einerseits Zeilen [rows] und ander- seits Spalten oder Kolonnen [columns]. Wir wollen nun zuerst ein Zahlenbeispiel mit m = n = 3 auf kon- ventionelle Weise l¨osen und dann die Rechnung auf das Schema ¨ubertragen. Beispiel 1.1: 2x1 − 2x2 + 4x3 = 6 −5x1 + 6x2 − 7x3 = −7 3x1 + 2x2 + x3 = 9 . (1.4) Wir k¨onnten wie ¨ublich x1 eliminieren, indem wir die erste Gleichung nach x1 auﬂ¨osen und den so erhaltenen Ausdruck f¨ur x1 in die ande- ren beiden Gleichungen einsetzen. Wir k¨onnen das Gleiche erreichen, indem wir geeignete Vielfache der ersten Zeile von den beiden anderen subtrahieren. Hier subtrahieren wir das −5/2-fache der ersten Gleichung von der zwei- ten und das 3/2-fache der ersten Gleichung von der dritten. 2x1 − 2x2 + 4x3 = 6 x2 + 3x3 = 8 5x2 − 5x3 = 0 . Um auch noch x2 zu eliminieren, k¨onnen wir das 5-fache der zweiten Gleichung von der dritten subtrahieren: 2x1 − 2x2 + 4x3 = 6 x2 + 3x3 = 8 − 20x3 = −40 . (1.5) Dieses System hat obere Dreieicksgestalt [upper triangular form] und l¨asst sich auﬂ¨osen, indem man aus der letzten Gleichung die letzte Va- riable bestimmt, dann aus der zweitletzten Gleichung die zweitletze Va- riable, und so fort. − 20x3 = −40 =⇒ x3 = (−40)/(−20) = 2 x2 + 3x3 = 8 =⇒ x2 = 8 − 3 · 2 = 2 2x1 − 2x2 + 4x3 = 6 =⇒ x1 = (6 + 2 · 2 − 4 · 2)/2 = 1 . Man nennt dieses rekursive L¨osen R¨uckw¨artseinsetzen [back substi- tution], weil man bei xn mit Auﬂ¨osen anf¨angt. ♦ Wir k¨onnen die Reduktion des gegebenen Systems (1.4) auf das Dreieckssystem (1.5) kompakter darstellen, indem wir die relevan- ten Zahlen in ein Eliminationsschema eintragen. Wir zeigen links c⃝M.H. Gutknecht 11. April 2016 LA-Skript 1-3 Kapitel 1 — Lineare Gleichungssysteme Lineare Algebra (2009) das Ausgangsschema f¨ur ein allgemeines 3×3-System, rechts jenes f¨ur unser konkretes Beispiel: l21 l31 x1 x2 x3 1 a11 a12 a13 b1 a21 a22 a23 b2 a31 a32 a33 b3 − 5 2 3 2 x1 x2 x3 1 2❧−2 4 6 −5 6 −7 −7 3 2 1 9 1 −1 2 3 (1.6) Wir haben links vom Eliminationsschema die Faktoren l21 = − 5 2 und l31 = 3 2 notiert, mit denen wir die erste Zeile (oder Gleichung) multiplizieren, bevor wir sie von der entsprechenden Zeile subtrahie- ren. Als Rechenhilfe haben wir schliesslich im Zahlenbeispiel unter dem Schema die durch zwei dividierte erste Zeile angegeben. Sie muss nur noch mit dem Z¨ahler (und dem Vorzeichen) der Faktoren multipliziert werden, falls diese als ungek¨urzter Bruch geschrieben sind; so kann man die im Kopf auszuf¨uhrenden Operationen verein- fachen. Auf dem Computer wird man diese Zeile nicht berechnen; f¨ur ihn spielt es (meist) keine Rolle, ob mit einer ganzen Zahl oder einem Bruch multipliziert wird. Nach einem Eliminationsschritt hat man l32 x1 x2 x3 1 a11 a12 a13 b1 0 a(1) 22 a(1) 23 b(1) 2 0 a(1) 32 a(1) 33 b(1) 3 5 x1 x2 x3 1 2 −2 4 6 0 1❧ 3 8 0 5 −5 0 0 1 3 8 (1.7) und nach einem weiteren Eliminationsschritt kommt man zum fol- genden Endschema: x1 x2 x3 1 a11 a12 a13 b1 0 a(1) 22 a(1) 23 b(1) 2 0 0 a(2) 33 b(2) 3 x1 x2 x3 1 2 −2 4 6 0 1 3 8 0 0 −20 ✖✕ ✗✔ −40 1 2 (1.8) Dabei haben wir das Element, das jeweils zum Auﬂ¨osen nach ei- ner Variablen dient, eingekreist. Es heisst Pivotelement [pivot element] oder kurz Pivot. Die Zeile bzw. Kolonne, in der das Pi- votelement liegt, heisst Pivotzeile [pivot row] bzw. Pivotkolonne [pivot column]. Die (fakultative) Hilfszeile unter dem Eliminations- schema, auch Kellerzeile genannt, ist also die durch das Pivotele- ment dividierte Pivotzeile. Da eine Zeile einer Gleichung entspricht, kann man die Pivotzeile auch als Pivotgleichung [pivot equati- on] bezeichnen. Das nach jedem Schritt weiter verkleinerte System heisst Restgleichungssystem [reduced system]. Es ist in obigem Beipiel durch eine zus¨atzliche Box hervorgehoben. LA-Skript 1-4 11. April 2016 c⃝M.H. Gutknecht Lineare Algebra (2009) Kapitel 1 — Lineare Gleichungssysteme Konkret ergeben sich im Beispiel als erstes und zweites Restglei- chungssstem (mit eingekreisten Pivotelementen) x2 x3 1 1❧ 3 8 5 −5 0 x3 1 −20 ✖✕ ✗✔ −40 Ausgehend von den Daten im Endschema (1.8) m¨usste man jetzt noch r¨uckw¨arts einsetzen, um die L¨osung zu berechnen. Dies macht man genau gleich wie oben in Beispiel 1.1. Im obigen Beispiel ist f¨ur k = 1, 2, 3 im k-ten Eliminationsschritt das Element a(k−1) kk als Pivotelement gew¨ahlt worden, und entspre- chend ist die k-te Zeile Pivotzeile und die k-te Kolonne Pivotko- lonne. Wir werden aber gleich sehen, dass das nicht immer so geht, ohne dass man Zeilen vertauscht, was problemlos ist, da Zeilen ja Gleichungen sind. Zun¨achst sei aber noch darauf hingewiesen, dass beim ersten Schritt ja die erste Gleichung unver¨andert bleibt, beim zweiten Schritt so- gar die zwei ersten Gleichungen. Im ersten Schritt wird eben das 3 × 3-System auf ein 2 × 2-System reduziert oder allgemeiner, ein m×n-System auf ein (m − 1)×(n − 1)-System. Da nach dem k-ten Schritt die ersten k Gleichungen unver¨andert bleiben und die ersten k Koeﬃzienten der restlichen Zeilen null sind, kann man sich Schreibarbeit sparen, indem man jeweils nur das neue Restgleichungssystem aufschreibt. F¨ur die Programmierung bedeutet dies, dass der Zugriﬀ auf jenen Teil der Daten beschr¨ankt bleibt, die zum Restgleichungssystem geh¨oren. Wir illustrieren diese neue, kompaktere Darstellung, die besser zeigt, was wirklich zu berechnen ist, an einem zweiten Beispiel. Beispiel 1.2: Die Reduktion des Systems x2 + 4x3 = 1 2x1 + 4x2 − 4x3 = 1 4x1 + 8x2 − 3x3 = 7 (1.9) auf obere Dreiecksgestalt wird nachfolgend links in der bisherigen Dar- stellung gezeigt, rechts in der kompakteren.Dabei tritt noch die Schwie- rigkeit auf, dass a11 = 0 ist, dieses Element also nicht als Pivotelement in Frage kommt.Wir w¨ahlen stattdessen a21 als erstes Pivotelement. In unserer linken Darstellung vertauschen wir in diesem Falle die erste und die zweite Zeile. Auf der rechten Seite entf¨allt dieser triviale Zwischen- schritt. Es gen¨ugt, das Pivotelement zu markieren. Die Kellerzeile lassen wir diesmal weg. c⃝M.H. Gutknecht 11. April 2016 LA-Skript 1-5 Kapitel 1 — Lineare Gleichungssysteme Lineare Algebra (2009) x1 x2 x3 1 0 1 4 1 2♠ 4 −4 1 4 8 −3 7 0 2 x1 x2 x3 1 0 1 4 1 2♠ 4 −4 1 4 8 −3 7 0 2 x1 x2 x3 1 2♠ 4 −4 1 0 1 4 1 4 8 −3 7 0 x1 x2 x3 1 2 4 −4 1 0 1♠ 4 1 0 0 5 5 0 x2 x3 1 1♠ 4 1 0 5 5 x1 x2 x3 1 2♠ 4 −4 1 0 1♠ 4 1 0 0 5♠ 5 x3 1 5♠ 5 Hier passiert im zweiten Eliminationsschritt (mit Pivot 1) nichts mehr, denn nach dem Zeilenvertauschen und dem ersten Schritt hat das Sche- ma bereits obere Dreiecksgestalt. Mit anderen Worten, hier hat das erste Restgleichungssystem (der G¨osse 2×2) per Zufall Dreiecksgestalt. Im letzten Schema links haben wir nun alle Pivotelemente markiert; das wird sp¨ater im allgemeinen Fall wichtig sein. Es bleibt noch das R¨uckw¨artseinsetzen: 5x3 = 5 =⇒ x3 = 5/5 = 1 x2 + 4x3 = 1 =⇒ x2 = 1 − 4 · 1 = −3 2x1 + 4x2 − 4x3 = 1 =⇒ x1 = [1 − 4 · (−3) + 4 · 1]/2 = 17/2 . ♦ Dieses Eliminationsverfahren kann auch so interpretiert werden, dass man schrittweise das gegebene System durch elementare Ope- rationen in andere Systeme umformt, die die gleiche L¨osungsmenge haben. Z.B. hat das gegebene System (1.4) die gleiche L¨osung wie das System (1.4), das Dreiecksgestalt hat. Da dieses oﬀensichtlich nur eine L¨osung hat (denn beim R¨uckw¨artseinsetzen haben wir hier keine Wahlm¨oglichkeit), ist auch das gegebene System eindeutig l¨osbar. In diesem Beispiel gibt es nur eine L¨osung, aber unser Vorgehen w¨urde die L¨osungsmenge auch nicht ver¨andern, wenn es mehrere L¨osungen g¨abe. Dies folgt daraus, dass alle Eliminationsschritte umkehrbar sind. Als elementare Zeilen-Operation [elementary row operation] z¨ahlen: i) das Vertauschen von Gleichungen (Zeilen), ii) die Addition/Subtraktion eines Vielfachen der Pivotgleichung (Pivotzeile) zu/von einer anderen Gleichung (Zeile). LA-Skript 1-6 11. April 2016 c⃝M.H. Gutknecht Lineare Algebra (2009) Kapitel 1 — Lineare Gleichungssysteme Man kann als weitere elementare Zeilen-Operation die Multiplikati- on einer Gleichung mit einer von null verschiedenen Zahl zulassen. Es zeigt sich aber, dass man ohne diese Operation auskommt. Beim Rechnen von Hand wird sie aber manchmal mit Vorteil angewandt. Zwei Gleichungssysteme, welche die gleiche L¨osungsmenge besitzen, nennen wir ¨aquivalent [equivalent]. Wir k¨onnen das Vorgehen, das wir im Prinzip auch im allgemeinen Falle m ̸= n beibehalten werden, damit wie folgt zusammenfassen: Grundidee der Gauss-Elimination: Ein lineares System aus m Gleichungen in n Unbekann- ten wird durch die elementaren Zeilen-Operationen i) und ii) schrittweise in ¨aquivalente, gleich grosse Sy- steme verwandelt, wobei es nach j Schritten ein Teil- system von m − j Gleichungen gibt, das nur noch h¨ochstens n − j Unbekannte enth¨alt. Normalerweise wird man zun¨achst x1 eliminieren, dann x2, und so weiter bis und mit xn−1. Dann kann man xn berechnen, daraus xn−1, und so weiter, bis man schliesslich x1 erh¨alt. Ist m = n und ist xn eindeutig bestimt, hat man auf diese Weise die eindeutige L¨osung des Systems bestimmt. Im allgemeinen geht das nicht immer so, aber im Normalfall funktioniert das mindestens theoretisch, d.h. wenn man von allf¨alligen Problemen mit Rundungsfehlern absieht. In der Box 1.1 auf Seite 1-8 ist dieser Algorithmus zun¨achst noch- mals formuliert f¨ur diesen Normalfall. Kann man in der angedeu- teten Weise eine eindeutige L¨osung eines quadratischen linearen Gleichungssystems berechnen, so nennen wir dieses regul¨ar [non- singular], andernfalls heisst es singul¨ar [singular]. Bemerkungen: 1) F¨ur ein regul¨ares n × n-System braucht es n − 1 Eliminations- schritte; dann hat das resultierende ¨aquivalente System obere Drei- ecksgestalt. Das letzte, nte Pivotelement a(n−1) nn braucht man nur beim R¨uckw¨artseinsetzen. 2) Wir nummerieren die Zeilen (Gleichungen) und Kolonnen (Unbe- kannte) im Restgleichungssystem immer entsprechend der Position im ganzen System. Die Pivotzeile im j-ten Eliminationsschritt hat anf¨anglich den Index p, nach dem Zeilenvertauschen den Index j. 3) Im Prinzip kann jedes von 0 verschiedene Element in der vorder- sten Kolonne eines Restgleichungssystems als Pivot gew¨ahlt wer- den. F¨ur die Handrechnung ist nat¨urlich eine Eins besonders prak- tisch. Beim Rechnen mit rellen Zahlen beschr¨ankter Genauigkeit (insbesondere in Gleitkomma-Arithmetik) ist es aber wichtig, Ele- mente zu w¨ahlen, die einen grossen Absolutbetrag haben relativ c⃝M.H. Gutknecht 11. April 2016 LA-Skript 1-7 Kapitel 1 — Lineare Gleichungssysteme Lineare Algebra (2009) Box 1.1 Gauss-Elimination im Falle eines regul¨aren Systems Algorithmus 1.1 (Gauss-Algorithmus, regul¨arer Fall) Zum L¨osen des n × n-Gleichungssystems Ax = b f¨uhre wie folgt f¨ur j = 1, . . . , n − 1 je einen Eliminationsschritt durch. Das ge- gebene System bezeichnen wir hier als 0-tes Restgleichungssystem mit Koeﬃzientenmatrix A(0) := A und rechter Seite b (0) := b. (a) W¨ahle in der vordersten Kolonne des Restgleichungssystems das j-te Pivotelement a(j−1) pj ̸= 0 aus. F¨ur den Zeilenindex p der Pivotzeile gilt dabei p ∈ {j, . . . , n}. Falls p ̸= j, vertau- sche die Zeile p mit der Zeile j und nummeriere die Koeﬃzi- enten und rechten Seiten entsprechend um; das Pivotelement heisst dann a(j−1) jj . L¨asst sich kein j-tes Pivotelement ﬁnden, weil die vorderste Kolonne des Restgleichungssystems lauter Nullen enth¨alt, so bricht der Algorithmus ab: das System ist singul¨ar. (b) Berechne f¨ur k = j + 1, . . . , n die Faktoren lkj := a(j−1) kj /a(j−1) jj (1.10) und subtrahiere das lkj-fache der Zeile mit Index j von der Zeile mit Index k: a(j) ki := a(j−1) ki − lkj a(j−1) ji , i = j + 1, . . . , n , (1.11a) b(j) k := b(j−1) k − lkj b(j−1) j . (1.11b) Ist nach n − 1 Eliminationsschritten a(n−1) nn ̸= 0, so gilt dieses Element als ntes Pivotelement; andernfalls bricht der Algorithmus ab: das System ist singul¨ar. Berechne durch R¨uckw¨artseinsetzen die eindeutig bestimmte L¨osung des Systems: berechne dazu f¨ur k = n, n − 1, . . . , 1 xk := ( b(k−1) k − n∑ i=k+1 a(k−1) ki xi ) 1 a(k−1) kk . (1.12) LA-Skript 1-8 11. April 2016 c⃝M.H. Gutknecht Lineare Algebra (2009) Kapitel 1 — Lineare Gleichungssysteme zu den anderen Elementen derselben Zeile. Die Umsetzung dieser Tatsache f¨uhrt auf Regeln zur Wahl der Pivotelemente (Pivotstra- tegien). Wir kommen auf Seite 3-8 in Abschnitt 3.1 darauf zur¨uck. 4) Die Faktoren lkj ergeben sich als Quotienten aus den Koeﬃzien- ten der vordersten Kolonne des j–ten Restgleichungssystems und dem Pivot. 5) Die L¨osung ist eindeutig bestimmt (wenn der Algorithmus nicht abbricht), weil das R¨uckw¨artseinsetzen zu einer eindeutigen L¨osung f¨uhrt und das nach den n − 1 Eliminationsschritten erhaltene Sy- stem von Dreiecksgestalt ¨aquivalent ist mit dem gegebenen System. ▼ 1.2 Gauss-Elimination: der allgemeine Fall Im Gauss-Algorithmus 1.1 tritt eine Ausnahmesituation ein, wenn die erste Kolonne des gegebenen Systems oder die erste Kolon- ne eines Restgleichungsystems lauter Nullen enth¨alt. Dann kann man nicht nach der entsprechenden Variablen auﬂ¨osen. Aber man braucht diese Variable auch nicht mehr zu eliminieren; sie kommt ja dann scheinbar gar nicht mehr vor. Genauer ausgedr¨uckt, hat der Wert einer solchen Variablen keinen Einﬂuss, das heisst man kann diese Variable frei w¨ahlen. Man nennt sie deshalb freie Variable [free variable] oder freier Parameter [free parameter]. Wir wollen diese Ausnahmesituation eines singul¨aren quadratischen linearen Systems (m = n) gerade im Rahmen des allgemeinen Falles eines “rechteckigen” Systems, wo m ̸= n erlaubt ist, behandeln. Zuerst betrachten wir aber ein kleines 3×3–Beispiel, in dem freie Variablen auftreten. Beispiel 1.3: Bei der Reduktion des Systems 2x1 − x2 + 2x3 = 2 4x1 − 2x2 + 2x3 = 6 8x1 − 4x2 + 6x3 = 10 (1.13) auf obere Dreiecksgestalt werden im ersten Eliminationsschritt nicht nur in der ersten Kolonne, sondern auch in der zweiten zwei Nullen unterhalb der Pivotzeile erzeugt. Wir zeigen dies hier wieder links mit den vollen Eliminationsschemata und rechts in der kompakten Darstellung. 2 4 x1 x2 x3 1 2♠ −1 2 2 4 −2 2 6 8 −4 6 10 2 4 x1 x2 x3 1 2♠ −1 2 2 4 −2 2 6 8 −4 6 10 1 x1 x2 x3 1 2 −1 2 2 0 0 −2 ✒✑ ✓✏ 2 0 0 −2 2 1 x2 x3 1 0 −2 ✒✑ ✓✏ 2 0 −2 2 c⃝M.H. Gutknecht 11. April 2016 LA-Skript 1-9 Kapitel 1 — Lineare Gleichungssysteme Lineare Algebra (2009) Die vorderste Kolonne des 2×2–Restgleichungssystems besteht aus lau- ter Nullen, was oﬀensichtlich daher kommt, dass im gegebenen System die zweite Kolonne gerade ein Vielfaches der ersten ist. Die Wahl von x2 hat keinen Einﬂuss auf den Wert der linken Seite des Restgleichungssy- stems: x2 darf also irgend einen Wert annehmen, das heisst, es ist frei w¨ahlbar. Wir k¨onnen nun in der vordersten Kolonne des Restgleichungs- systems aber kein Pivotelement festlegen und gehen stattdessen direkt zur n¨achsten Kolonnen ¨uber. Das eingekreiste Pivotelement −2 in der oberen Zeile liefert im n¨achsten Schritt die folgenden Endschemata: x1 x2 x3 1 2♠ −1 2 2 0 0 −2 ✒✑ ✓✏ 2 0 0 0 0 1 0 (1.14) Hier ist die letzte Gleichung (0x1 + 0x2 + 0x3 = 0 bzw. 0 = 0 — auf der linken Seite kommen keine Variablen mehr vor) f¨ur beliebige (x1, x2, x3) erf¨ullt. R¨uckw¨artseinsetzen in der zweiten und der ersten Gleichung lie- fert x3 = −1 und x1 = (2 − 2x3 + x2)/2 = (4 + x2)/2 , also die allgemeine L¨osung (x1, x2, x3) = ( 2 + 1 2 x2, x2, −1 ) (1.15) mit frei w¨ahlbarem x2. Wir hatten grosses Gl¨uck: W¨are die Konstante in der letzten gegebenen Gleichung in (1.13) nicht gerade 10 sondern zum Beispiel 13, so h¨atten wir stattdessen die folgenden Endschemata bekommen: x1 x2 x3 1 2♠ −1 2 2 0 0 −2 ✒✑ ✓✏ 2 0 0 0 3 1 3 (1.16) Nun hat die letzte Gleichung und damit das ganze System oﬀensichtlich keine L¨osung. ♦ Wir schliessen aus diesem Beispiel, dass es bei einem singul¨aren Sy- stem (d.h. einem n×n System, bei dessen Reduktion freie Variablen auftreten) rechte Seiten gibt, f¨ur die das System keine L¨osung hat. Oﬀenbar ist dies sogar der Normalfall bei einem singul¨aren System. Denn es kann nur L¨osungen geben, wenn im letzten Restgleichungs- system die rechte Seite null ist. Zu betonen ist nochmals, dass das Auftreten freier Variablen bei quadratischen linearen Systemen eine Ausnahme darstellt. Wir wer- den aber sp¨ater, beim L¨osen von Eigenwertproblemen in Kapitel 9 bewusst solche Ausnahmef¨alle herbeif¨uhren. Nun betrachten wir ein gr¨osseres, “rechteckiges” Gleichungssystem. LA-Skript 1-10 11. April 2016 c⃝M.H. Gutknecht Lineare Algebra (2009) Kapitel 1 — Lineare Gleichungssysteme Beispiel 1.4: 0 1 1 0 1 0 x1 x2 x3 x4 x5 x6 x7 x8 x9 1 1♠ 0 5 0 4 0 0 1 0 1 0 0 0 5 0 24 16 8 6 12 1 5 9 3 6 1 0 1 0 3 1 10 13 11 8 6 3 1 2 8 0 5 4 18 2 18 12 2 7 13 1 10 13 21 8 24 16 5 8 19 0 5 4 13 2 24 17 6 7 16 (1.17) Das ist ein System von sieben Gleichungen in neun Unbekannten. Da- mit es ¨uberhaupt auf die Seite passt, haben wir es direkt als Eliminati- onsschema hingeschrieben. Da es darin mehr Unbekannte als Gleichun- gen gibt, d¨urfen wir erwarten, dass das System eine ganze Schar von L¨osungen hat. Es w¨are allerdings auch in dieser Situation m¨oglich, dass es keine L¨osung gibt, was allerdings ein ganz besonderer Ausnahmefall w¨are, wie wir in K¨urze sehen werden. Wir haben durch den Kreis bereits angedeuetet, dass wir a11 = 1 als erstes Pivot w¨ahlen wollen, und wir haben die entsprechenden Faktoren f¨ur den ersten Eliminationsschritt links angef¨ugt. Er liefert das folgende zweite Schema: x1 x2 x3 x4 x5 x6 x7 x8 x9 1 1 0 5 0 4 0 0 1 0 1 0 0 0 5 0 24 16 8 6 12 0 5 4 3 2 1 0 0 0 2 0 10 8 11 4 6 3 0 2 7 0 5 4 18 2 18 12 2 7 13 0 10 8 21 4 24 16 4 8 18 0 5 4 13 2 24 17 6 7 16 Nun ist im Restgleichungssystem allerdings das erste Element der vor- dersten Kolonne null, also nicht als Pivot brauchbar. Die Vertauschung der zweiten und dritten Zeile (d.h. der ersten und zweiten Gleichung des Restgleichungssystems) liefert ein modiﬁzertes zweites Schema mit Pivotelement 5 in der linken oberen Ecke. Die f¨ur den zweiten Elimina- tionsschritt n¨otigen Faktoren schreiben wir wieder links hin: 0 2 1 2 1 x1 x2 x3 x4 x5 x6 x7 x8 x9 1 1 0 5 0 4 0 0 1 0 1 0 5♠ 4 3 2 1 0 0 0 2 0 0 0 5 0 24 16 8 6 12 0 10 8 11 4 6 3 0 2 7 0 5 4 18 2 18 12 2 7 13 0 10 8 21 4 24 16 4 8 18 0 5 4 13 2 24 17 6 7 16 c⃝M.H. Gutknecht 11. April 2016 LA-Skript 1-11 Kapitel 1 — Lineare Gleichungssysteme Lineare Algebra (2009) Das resultierende dritte Schema sieht so aus: x1 x2 x3 x4 x5 x6 x7 x8 x9 1 1 0 5 0 4 0 0 1 0 1 0 5 4 3 2 1 0 0 0 2 0 0 0 5 0 24 16 8 6 12 0 0 0 5 0 4 3 0 2 3 0 0 0 15 0 17 12 2 7 11 0 0 0 15 0 22 16 4 8 14 0 0 0 10 0 23 17 6 7 14 Nun enth¨alt die zu x3 geh¨orende vorderste Kolonne des Restgleichungs- systems lauter Nullen. Dies gilt auch f¨ur die Kolonne zu x5, was aber im Moment noch nicht wichtig ist. Die Variable x3 braucht also nicht eliminiert zu werden. Wir k¨onnen im n¨achsten Schritt direkt x4 eliminie- ren, und wir k¨onnten dazu das Pivot 5 in der obersten Zeile w¨ahlen. Es zeigt sich allerdings, dass man so auf grosse Zahlen st¨osst. Wir wollen deshalb, obwohl nicht unbedingt n¨otig, die dritte und die vierte Zeile vertauschen: 1 3 3 2 x1 x2 x3 x4 x5 x6 x7 x8 x9 1 1 0 5 0 4 0 0 1 0 1 0 5 4 3 2 1 0 0 0 2 0 0 0 5♠ 0 4 3 0 2 3 0 0 0 5 0 24 16 8 6 12 0 0 0 15 0 17 12 2 7 11 0 0 0 15 0 22 16 4 8 14 0 0 0 10 0 23 17 6 7 14 Nach einem weiteren Schritt ist die x5-Kolonne, die wieder nur Nullen enth¨alt, im Restgleichungssystems zuvorderst, so dass man direkt zur Elimination von x6 ¨ubergehen kann. Wir wollen dabei aber nochmals, um die Zahlen klein zu halten, die vierte und die f¨unfte Gleichung frei- willig vertauschen. Dann ergibt sich der Reihe nach: x1 x2 x3 x4 x5 x6 x7 x8 x9 1 1 0 5 0 4 0 0 1 0 1 0 5 4 3 2 1 0 0 0 2 0 0 0 5 0 4 3 0 2 3 0 0 0 0 0 20 13 8 4 9 0 0 0 0 0 5 3 2 1 2 0 0 0 0 0 10 7 4 2 5 0 0 0 0 0 15 11 6 3 8 4 2 3 x1 x2 x3 x4 x5 x6 x7 x8 x9 1 1 0 5 0 4 0 0 1 0 1 0 5 4 3 2 1 0 0 0 2 0 0 0 5 0 4 3 0 2 3 0 0 0 0 0 5♠ 3 2 1 2 0 0 0 0 0 20 13 8 4 9 0 0 0 0 0 10 7 4 2 5 0 0 0 0 0 15 11 6 3 8 LA-Skript 1-12 11. April 2016 c⃝M.H. Gutknecht Lineare Algebra (2009) Kapitel 1 — Lineare Gleichungssysteme 1 2 x1 x2 x3 x4 x5 x6 x7 x8 x9 1 1 0 5 0 4 0 0 1 0 1 0 5 4 3 2 1 0 0 0 2 0 0 0 5 0 4 3 0 2 3 0 0 0 0 0 5 3 2 1 2 0 0 0 0 0 0 1♠ 0 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 2 0 0 2 Jetzt kommt im Restgleichungssystem nur noch x7 vor. Wir wollen die- ses noch aus den zwei letzten Gleichungen eliminieren. Dann erhalten wir das folgende Endschema, in dem wir wieder alle f¨unf Pivotelemente markiert haben: x1 x2 x3 x4 x5 x6 x7 x8 x9 1 1♠ 0 5 0 4 0 0 1 0 1 0 5♠ 4 3 2 1 0 0 0 2 0 0 0 5♠ 0 4 3 0 2 3 0 0 0 0 0 5♠ 3 2 1 2 0 0 0 0 0 0 1♠ 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 (1.18) Das Endschema hat nun Zeilenstufenform [row echelon form] statt der spezielleren Dreiecksgestalt. Aus der Zeilenstufenform bekommt man wiederum durch R¨uckw¨artsein- setzen die L¨osungen des letzten und damit auch des gegebenen Systems (denn diese sind ja ¨aquivalent). In unserem Beispiel sind die letzten zwei Gleichungen f¨ur beliebige Werte der xk erf¨ullt. Die drittletzte Zeile kann man nach x7 auﬂ¨osen, die viertletzte nach x6, die f¨unftletzte (d.h. die dritte) nach x4, die zweite nach x2 und die erste nach x1. Dabei darf man die ¨ubrigen Variablen x9, x8, x5 und x3 frei w¨ahlen: es sind freie Variable. Dagegen sind die Werte jener Variablen, nach denen man auﬂ¨ost, eindeutig bestimmt, sobald man die Werte der freien Variablen vorgegeben hat. Jene Variablen stehen alle am Kopf einer Pivotzeile und k¨onnen deshalb als Pivotvariable [pivot variable] bezeichnet werden. Eine “einfache” L¨osung bekommt man, wenn man alle freien Variablen null w¨ahlt. In unserem Beispiel ergibt sich dann x7 = 1 , x6 = [2 − 3 · 1]/5 = −1/5 , x4 = [3 − 3 · 1 − 4 · (−1/5)]/5 = 4/25 , x2 = [2 − 0 · 1 − 1 · (−1/5) − 3 · (4/25)]/5 = 43/125 , x1 = 1 − 0 · 1 − 0 · (−1/5) − 0 · (4/25) − 0 · (43/125) = 1 . Also ist das 9-Tuppel (x1, x2, x3, x4, x5, x6, x7, x8, x9) = ( 1, 43 125 , 0, 4 25 , 0, − 1 5 , 1, 0, 0 ) (1.19) eine L¨osung unseres Systems (1.17). Man sieht hier ¨ubrigens, dass “ein- fach” in diesem Zusammenhang nicht unbedingt bedeutet, dass die L¨osung eines ganzzahligen Systems nur ganze Zahlen und Br¨uche mit m¨oglichst kleinen Nennern hat. c⃝M.H. Gutknecht 11. April 2016 LA-Skript 1-13 Kapitel 1 — Lineare Gleichungssysteme Lineare Algebra (2009) Die allgemeine L¨osung [general solution] des Systems bekommt man, indem man die freien Variablen als Parameter auﬀasst, was man dadurch betonen kann, dass man sie entsprechend umbenennt: x3 = α , x5 = β , x8 = γ , x9 = δ . (1.20) Dann ergibt das R¨uckw¨artseinsetzen: x7 = 1 , x6 = −[1 + δ + 2γ]/5 , x4 = [4 − 6δ + 8γ]/25 , x2 = [43 + 23δ − 14γ − 50β − 100α]/125 , x1 = 1 − γ − 4β − 5α . (1.21) Die allgemeine L¨osung besteht aus (1.20) und (1.21). Es ist eine vierpa- rametrige Schar von L¨osungen. Man kann sich fragen, ob es nicht auch eine ganzzahlige L¨osung gibt. Das ist alles andere als klar und im allgemeinen nur mit grossem Auf- wand entscheidbar. Man k¨onnte nat¨urlich alle 9-Tuppel mit z.B. xi ∈ {−20, −19, . . . , −1, 0, 1, . . . , 19, 20} durchtesten, das sind 419 ≈ 3.274 · 1014 Tuppel. (Es gibt auch eﬃzientere Vorgehensweisen!) So w¨urde man unter anderem die folgenden drei ganzahligen L¨osungen ﬁnden: (1, 1, −1, 0, 1, −1, 1, 1, 2) , (1.22a) (−15, −1, 1, 4, −1, −7, 1, 15, 4) , (1.22b) (−17, −2, 1, 2, 2, −2, 1, 5, −1) . (1.22c) ♦ Wie erkennt man wohl allgemein die F¨alle, wo es keine L¨osung gibt? Das einfachste Beispiel ist die Gleichung 0x1 = 1. Im Schlusssche- ma (1.16) des modiﬁzierten Beispiels 1.3 hatten wir analog die unl¨osbare Gleichung 0x1 + 0x2 + 0x3 = 3. Beispiel 1.5: Im Schlussschema (1.18) w¨aren wir hier ebenso auf ein unl¨osbares Teilsystem gestossen, wenn in den zwei letzten Gleichun- gen die rechte Seite nicht null gewesen w¨are. Man veriﬁziert leicht, dass dies eintritt, wenn man im Ausgangssystem (1.17) die zwei letzten Kon- stanten 19 und 16 durch ein anderes Zahlenpaar ersetzt. Genauer: das Gleichungssystem x1 x2 x3 x4 x5 x6 x7 x8 x9 1 1 0 5 0 4 0 0 1 0 1 0 0 0 5 0 24 16 8 6 12 1 5 9 3 6 1 0 1 0 3 1 10 13 11 8 6 3 1 2 8 0 5 4 18 2 18 12 2 7 13 1 10 13 21 8 24 16 5 8 c6 + 19 0 5 4 13 2 24 17 6 7 c7 + 16 (1.23) mit noch w¨ahlbaren Parametern c6 und c7 hat die Zeilenstufenform LA-Skript 1-14 11. April 2016 c⃝M.H. Gutknecht Lineare Algebra (2009) Kapitel 1 — Lineare Gleichungssysteme x1 x2 x3 x4 x5 x6 x7 x8 x9 1 1♠ 0 5 0 4 0 0 1 0 1 0 5♠ 4 3 2 1 0 0 0 2 0 0 0 5♠ 0 4 3 0 2 3 0 0 0 0 0 5♠ 3 2 1 2 0 0 0 0 0 0 1♠ 0 0 1 0 0 0 0 0 0 0 0 0 c6 0 0 0 0 0 0 0 0 0 c7 (1.24) Dieses Endschema hat oﬀensichtlich nur dann eine L¨osung, wenn c6 = c7 = 0 gilt. Man beachte, dass der Zusammenhang der Konstanten auf den rechten Seiten von (1.23) und (1.24) nur in den letzten zwei Zeilen so einfach ist. ♦ Der allgemeine Fall der Gauss-Elimination ist als Algorithmus 1.2 in der Box 1.2 auf Seite 1-16 formuliert. Wir lassen dabei sogar zu, dass das gegebene m × n–System Kolonnen hat, die aus lauter Nullen bestehen. In den Restgleichungssystemen k¨onnen weitere Kolonnen mit lauter Nullen entstehen. Der Eliminationsprozess ist zu Ende, falls es keine potentielle neue Pivotvariable mehr gibt (wenn nj+ℓ > n in (a) oder nj = n in (b)) oder keine Gleichungen ¨ubrig bleiben (wenn j = m in (b)). Den ersten Fall kann man noch aufteilen in zwei: entweder sind alle Koeﬃzienten im Restgleichungssystem null oder es bleiben gar keine zu eliminierende Unbekannte mehr ¨ubrig. Nach dem Eliminationsprozess hat das Gleichungssystem die fol- gende Zeilenstufenform bestehend aus r Pivotzeilen und, falls m > r, weiteren m − r Zeilen mit lauter Nullen auf der linken Seite: xn1 · · · xn2 · · · · · · xnr xnr+1 · · · xn 1 a1,n1 ✚✙ ✛✘ · · · ∗ · · · · · · ∗ ∗ · · · ∗ c1 0 · · · a(1) 2,n2 ✚✙ ✛✘ · · · · · · ∗ ∗ · · · ∗ c2 ... ... . . . ... ... ... ... 0 · · · 0 · · · a(r−1) r,nr ✚✙ ✛✘ ∗ · · · ∗ cr 0 · · · 0 · · · · · · 0 · · · · · · 0 cr+1 ... ... ... ... ... 0 · · · 0 · · · · · · 0 · · · · · · 0 cm (1.28) In diesem Schema haben wir allerdings angenommen, dass die erste Kolonne von A nicht null ist (d.h. n1 = 1) und dass m > r ist. Zudem haben wir die rechten Seiten umbenannt: ck :≡ b(k−1) k (k = 1, . . . , r), cl :≡ b(r) l (l = r + 1, . . . , m). (1.29) Definition: In der Zeilenstufenform (1.28) des Systems (1.1)– (1.2) heisst die Anzahl r der Pivotelemente in Algorithmus 1.2 der Rang [rank] der Matrix A und des Gleichungssystems. Die im Falle m > r f¨ur die Existenz einer L¨osung notwendigen Bedingungen cr+1 = cr+2 = · · · = cm = 0 (1.30) sind die Vertr¨aglichkeitsbedingung [consistency conditions] des Systems. ▲ c⃝M.H. Gutknecht 11. April 2016 LA-Skript 1-15 Kapitel 1 — Lineare Gleichungssysteme Lineare Algebra (2009) Box 1.2 Reduktion eines m × n Systems auf Zeilenstufenform mittels Gauss-Elimination. Algorithmus 1.2 (Gauss-Algorithmus, allgemeiner Fall) Zum L¨osen des m × n-Gleichungssystems Ax = b setze j := 1, n1 := 1, A(0) := A, b (0) := b, und betrachte das gegebene System als 0-tes Restgleichungssystem. jter Eliminationsschritt: (a) Sind alle Koeﬃzienten in den ℓ vordersten Kolonnen des (j− 1)sten Restgleichungssystems null, so sind xnj , . . . xnj +ℓ−1 freie Variablen; streiche diese ℓ Kolonnen und setze nj := nj + ℓ; falls dann nj > n, setze r := j − 1 und gehe zum Vertr¨aglichkeitstest. W¨ahle in der vordersten Kolonne des verbleibenden Restglei- chungssystems das j-te Pivotelement a(j−1) p,nj ̸= 0 aus. F¨ur den Zeilenindex p der Pivotzeile gilt dabei p ∈ {j, . . . , m}. Falls p ̸= j, vertausche die Zeile p mit der Zeile j und numme- riere die Koeﬃzienten und rechten Seiten entsprechend um; das Pivotelement heisst dann a(j−1) j,nj . (b) Falls j = m, setze r = m und gehe zum R¨uckw¨artseinsetzen. Andernfalls berechne f¨ur k = j + 1, . . . , m die Faktoren lkj := a(j−1) k,nj /a(j−1) j,nj (1.25) und subtrahiere das lkj-fache der Pivotzeile (mit Index j) von der Zeile mit Index k: a(j) ki := a(j−1) ki − lkj a(j−1) ji , i = nj + 1, . . . , n , (1.26a) b(j) k := b(j−1) k − lkj b(j−1) j . (1.26b) Falls nj = n, so dass (1.26a) leer ist, setze r = j und gehe zum Vertr¨aglichkeitstest. Andernfalls setze nj+1 := nj + 1 und j := j +1 und beginne den n¨achsten Eliminationsschritt. Vertr¨aglichkeitstest: Falls m > r und b(r) k ̸= 0 f¨ur ein k > r, so hat das System keine L¨osung; Abbruch. R¨uckw¨artseinsetzen: Bestimme eine L¨osung des resultierenden Systems in Zeilenstufenform: berechne dazu f¨ur k = r, r − 1, . . . , 1 xnk := ( b(k−1) k − n∑ i=nk+1 a(k−1) ki xi ) 1 a(k−1) k,nk , (1.27) wobei die freien Variablen (d.h. jene xi mit i ̸∈ {n1, . . . , nr}) frei w¨ahlbar sind. LA-Skript 1-16 11. April 2016 c⃝M.H. Gutknecht Lineare Algebra (2009) Kapitel 1 — Lineare Gleichungssysteme 1.3 Die L¨osungsmenge eines linearen Gleichungssystems In diesem Abschnitt wollen wir aus der Zeilenstufenform (1.28) ei- nes allgemeinen linearen Gleichungssystems (1.1) aus m Gleichun- gen in n Unbekannten eine Reihe von einfachen theoretischen Aus- sagen ableiten. Der Rang r des Systems spielt dabei eine wichtige Rolle. Er ist nach Deﬁnition gleich der Anzahl der Pivotelemente, die bei der Reduktion auf Zeilenstufenform mittels Gauss-Elimination (Algo- rithmus 1.2) auftreten oder auch die Zahl der Zeilen in der Zeilen- stufenform der Koeﬃzientenmatrix, die nicht lauter Nullen enthal- ten. Erinnern wir uns daran, dass aufgrund unserer Konstruktion das gegebene System (1.1) und das daraus abgeleitete System (1.28) in Zeilenstufenform ¨aquivalent sind, d.h. die gleichen L¨osungen haben. Beim letzteren k¨onnen wir aber sofort sehen, ob es L¨osungen hat, und wenn ja, k¨onnen wir die allgemeine L¨osung leicht berechnen. Die Anzahl der freien Variablen ist n − r, und die Anzahl der Ver- tr¨aglichkeitsbedingungen ist m − r. Aufgrund der Deﬁnition von r ist klar, dass gilt: r ≤ min{m, n} .Daraus ergibt sich bereits der folgende wichtige Satz ¨uber Existenz und Eindeutigkeit der L¨osung. Satz 1.1 Das System (1.28) und damit das System (1.1) hat ge- nau dann (mindestens) eine L¨osung, wenn • entweder r = m ist (d.h. keine Vertr¨aglichkeitsbedingungen auftreten), • oder r < m und cr+1 = cr+2 = · · · = cm = 0 ist (d.h. die Vertr¨aglichkeitsbedingungen erf¨ullt sind ). Gibt es L¨osungen, so gilt • falls r = n: die L¨osung ist eindeutig, • falls r < n: es gibt eine (n−r)–parametrige L¨osungsschar (denn die n − r freien Variablen sind frei w¨ahlbar). Wir k¨onnen daraus leicht weitere Schl¨usse ziehen. Zur Formulierung verwenden wir wieder die Bezeichnung Ax = b aus (1.2b). Korollar 1.2 Der Rang r h¨angt nur von der Koeﬃzientenmatrix A ab, aber nicht von den gew¨ahlten Pivotelementen oder von der rechten Seite b. Beweis: Der Rang bestimmt die Zahl n − r freier Parameter in der L¨osungsmenge. Letztere bleibt aber w¨ahrend der Gauss-Elimination un- ver¨andert. Zudem ist r unabh¨angig von der rechten Seite des Systems. Also muss r eindeutig bestimmt sein durch die Koeﬃzientenmatrix A. (Die Stichhaltigkeit dieses Beweises wird sp¨ater untermauert werden.) Aufgrund von Korollar 1.2 schreiben wir r = Rang A [r = rank A]. c⃝M.H. Gutknecht 11. April 2016 LA-Skript 1-17 Kapitel 1 — Lineare Gleichungssysteme Lineare Algebra (2009) Korollar 1.3 Es gelten die folgenden ¨Aquivalenzen: (i) Ax = b hat genau eine L¨osung    ⇐⇒    r = n = m oder{ r = n < m und cr+1 = · · · = cm = 0 } , (ii) Ax = b hat f¨ur jedes b (mindestens) eine L¨osung    ⇐⇒ r = m , (iii) Ax = b hat f¨ur jedes b genau eine L¨osung    ⇐⇒ r = m = n . Beweis: (i) Dies folgt sofort aus Satz 1.1. (ii) Man kann sich ¨uberlegen — wir werden das sp¨ater leicht einsehen — dass das gegebene System (1.1) genau dann f¨ur alle b1, . . . bm l¨osbar ist, wenn das Zeilenstufensystem (1.28) f¨ur alle c1, . . . , cm l¨osbar ist, was nach Satz 1.1 genau dann der Fall ist, wenn r = m, d.h. wenn keine Vertr¨aglichkeitsbedingungen existieren. (iii) Zus¨atzlich zur Bedingung aus (ii) verlangen wir hier noch die Ein- deutigkeit, die gem¨ass Satz 1.1 r = n erfordert. Man beachte, dass der Fall (iii) im Korollar 1.3 gerade bedeutet, dass das System und A regul¨ar sind. Dies setzt immer voraus, dass das System quadratisch ist, d.h. m = n gilt. In diesem Falle ergibt sich aus (ii) und (iii) noch die folgende interessante Aussage. Korollar 1.4 Die L¨osung eines quadratischen linearen Glei- chungssystems (mit m = n) ist genau dann eindeutig, wenn das System f¨ur beliebige rechte Seiten l¨osbar ist. Regul¨are Systeme sind in den Anwendungen mit Abstand am wich- tigsten. Fast alle Computerprogramme f¨ur die L¨osung linearer Glei- chungssysteme setzen ein regul¨ares System voraus. In der Theorie von spezieller Bedeutung sind Systeme mit einer rechten Seite aus lauter Nullen. Wir schreiben dann b = o, das heisst Ax = o. Ein solches System hat immer x1 = x2 = · · · = xn = 0 als L¨osung. Definition: Ein lineares Gleichungssystem heisst homogen [ho- mogeneous], falls die rechte Seite aus Nullen besteht. Andernfalls heisst es inhomogen [inhomogeneous]. Die L¨osung x1 = x2 = · · · = xn = 0 eines homogenen Systems heisst triviale L¨osung [trivial solution]. Jede andere L¨osung nennt man nichttrivial [non- trivial]. ▲ F¨ur homogene Systeme kann man die vorangehenden Aussagen ¨uber die L¨osungsmenge pr¨azisieren. Aus b1 = · · · = bm = 0 im gegebenen System (1.1), folgt in der Tat, dass c1 = · · · = cm = 0 im Zeilenstufensystem (1.28). Damit sind allf¨allige Vertr¨aglichkeits- LA-Skript 1-18 11. April 2016 c⃝M.H. Gutknecht Lineare Algebra (2009) Kapitel 1 — Lineare Gleichungssysteme bedingungen immer erf¨ullt. Der erste Teil von Satz 1.1 ist im Falle b = o trivial, und der zweite Teil kann so umformuliert werden: Korollar 1.5 Ein homogenes Gleichungssystem hat genau dann nichttriviale L¨osungen, wenn r < n ist. Falls letzteres zutriﬀt, gibt es eine (n − r)–parametrige L¨osungsschar. Insbesondere hat ein homogenes System mit m < n, d.h. mit mehr Unbekannten als Gleichungen, stets eine mindestens (n−m)–para- metrige Schar nichttrivialer L¨osungen. Aus den Teilen (ii) und (iii) von Korollar 1.3 k¨onnen wir weiter fol- gern, dass ein Zusammenhang besteht zwischen der L¨osungsmenge eines inhomogenen linearen n × n Gleichungsystems und jener des zugeh¨origen homogenen Systems, das dieselbe Koeﬃzientenmatrix besitzt: Korollar 1.6 Ein quadratisches lineares Gleichungssystem (mit m = n) ist genau dann f¨ur beliebige rechte Seiten l¨osbar, wenn das zugeh¨orige homogene System nur die triviale L¨osung besitzt. Beweis: Beide Bedingungen sind ¨aquivalent zu r = n = m. Fassen wir zum Schluss noch einige Fakten ¨uber quadratische linea- re Systeme zusammen: Korollar 1.7 F¨ur ein quadratisches lineares Gleichungssystem Ax = b von n Gleichungen in n Unbekannten gelten entweder die vier ¨aquivalenten Aussagen (i) r :≡ Rang A = n (d.h. A ist regul¨ar), (ii) f¨ur jedes b gibt es (mindestens) eine L¨osung, (iii) f¨ur jedes b gibt es genau eine L¨osung, (iv) das entsprechende homogene System hat nur die triviale L¨osung, oder es gelten die f¨unf ebenfalls ¨aquivalenten Aussagen (v) r :≡ Rang A < n (d.h. A ist singul¨ar), (vi) f¨ur gewisse b gibt es keine L¨osung, (vii) f¨ur kein b gibt es eine eindeutige L¨osung, (viii) f¨ur gewisse b gibt es unendlich viele L¨osungen, (ix) das entsprechende homogene System hat nichttriviale L¨osungen. Wir werden in Kapitel 3 auf die Gauss-Elimination zur¨uckkommen und sie neu interpretieren als Matrixzerlegung. c⃝M.H. Gutknecht 11. April 2016 LA-Skript 1-19 Kapitel 1 — Lineare Gleichungssysteme Lineare Algebra (2009) LA-Skript 1-20 11. April 2016 c⃝M.H. Gutknecht Lineare Algebra (2009) Kapitel 2 — Matrizen und Vektoren im R n und C n Kapitel 2 Matrizen und Vektoren im R n und C n Matrizen, Zeilen- und Kolonnenvektoren sind die Basiselemente der Anwendungs-orientierten linearen Algebra, also insbesondere der auf Computern implementierten numerischen linearen Algebra. Es lassen sich aber auch typische Fragen der analytischen Geometrie des Raumes als Matrizenaufgaben formulieren. Wir werden sp¨ater sogar sehen, dass sich viele Aufgabenstellungen aus abstrakteren R¨aumen, jedenfalls solange diese endlichdimensional sind, auf Ma- trizenaufgaben zur¨uckf¨uhren lassen, n¨amlich indem man im Raum ein Koordinatensystem festlegt. Matrizen und die entsprechenden Vektoren sind also f¨ur das Weitere von zentraler Bedeutung. 2.1 Matrizen, Zeilen- und Kolonnenvektoren Definitionen: Eine m × n–Matrix [m × n matrix; m-by-n ma- trix] (pl. Matrizen [matrices]) ist ein rechteckiges Schema von mn (reellen oder komplexen) Zahlen, genannt Elemente [elements] und angeordnet in m Zeilen [rows] und n Spalten oder Kolonnen [columns]. Das Paar (m, n) deﬁniert die Gr¨osse [size] der Matrix. Das (i, j)–Element der Matrix1 A, welches in der i–ten Zeile und in der j–ten Kolonne steht, bezeichnen wir mit aij oder (A)ij. Also: A =      a11 a12 · · · a1n a21 a22 · · · a2n ... ... ... am1 am2 · · · amn      . (2.1) Manchmal schreibt man auch etwas ungenau A = ( aij ). Die Elemente ajj (j = 1, 2, . . . , min{m, n}) heissen Diagonalele- mente [diagonal elements]. Die Gesamtheit der Diagonalelemente bildet die (Haupt-)Diagonale [(main) diagonal] der Matrix A. Eine m × 1–Matrix heisst Spaltenvektor oder Kolonnenvektor [column vector] oder auch m–Vektor [m–vector]. Eine 1 × n–Matrix nennen wir Zeilenvektor [row vector] oder n– Tupel [n–tuple]. 1Wir w¨ahlen f¨ur Matrizen Grossbuchstaben. Zur besseren Kennzeichnung sind sie in diesem Skript halbfett gesetzt, was in Handschrift durch Unter- streichen oder Unterwellen markiert wird, aber auch weggelassen werden kann, wenn keine Verwechslungsgefahr besteht. c⃝M.H. Gutknecht 11. April 2016 LA-Skript 2-1 Kapitel 2 — Matrizen und Vektoren im R n und C n Lineare Algebra (2009) Die k–ten Elemente des Kolonnenvektors x und des Zeilenvektors2 w nennen wir k–te Komponente [component] des Vektors und bezeichnen sie mit xk bzw. wk: x =      x1 x2 ... xm      , w = ( w1 w2 . . . wn ) . ▲ Beispiele 2.1: A = ( 5 3 1 4 −1 4 ) ist eine reelle, ganzzahlige 2 × 3–Matrix. Das zweite Element in ihrer ersten Zeile ist (A)12 = a12 = 3. Dagegen ist B =   1.234567 + 4.567890 i 9.876543 + 6.543210 i 2.345678 + 5.432109 i 8.765432 + 5.432109 i 3.456789 + 3.210987 i 7.654321 + 4.321098 i   eine komplexe 3 × 2–Matrix, wobei z.B. (B)31 = b31 = 3.456789 + 3.210987 i. Das Folgende sind zwei Kolonnenvektoren und ein Zeilenvektor: x =     1.05 2.16 3.27 4.38     , y = ( γ 2γ ) , w = ( 0 1 π 2π ) . ♦ Bemerkungen: 1) Wir arbeiten vorzugsweise mit Kolonnenvektoren und nur selten mit Zeilenvektoren. 2) Einige Autoren verwenden f¨ur Matrizen und Vektoren statt run- de eckige Klammern. 3) Oft trennt man die Komponenten eines Zeilenvektors durch Kom- mas, schreibt also zum Beispiel ( w1, w2, . . . , w5 ) = ( 4, −7, 12, 1, −9 ) . ▼ Einige spezielle Matrizentypen: Eine n × n–Matrix (d.h. eine Matrix mit n Kolonnen und gleich vielen Zeilen) ist eine quadratische Matrix [square matrix] der Ordnung [order] n. 2Wir bezeichnen Kolonnen- und Zeilenvektoren mit kleinen lateinischen Buchstaben, die ebenfalls halbfett gesetzt sind. Wir kennzeichnen Zeilenvek- toren vorderhand durch unterstreichen, obwohl das nicht ¨ublich ist. Sp¨ater werden wir statt w meist wT schreiben, wobei dann w der entsprechende Ko- lonnenvektor ist. LA-Skript 2-2 11. April 2016 c⃝M.H. Gutknecht Lineare Algebra (2009) Kapitel 2 — Matrizen und Vektoren im R n und C n Eine m×n–Matrix deren Elemente alle null sind heisst Nullmatrix [zero matrix]. Sie wird mit O bezeichnet3. Analog ist der Nullvek- tor [zero vector] ein Kolonnenvektor mit lauter Nullen als Kompo- nenten; er wird hier mit o bezeichnet4. Beispiel 2.2: Die 2 × 3–Nullmatrix und der Nullvektor mit 3 Kom- ponenten sind gegeben durch O = ( 0 0 0 0 0 0 ) , o =   0 0 0   . ♦ Eine n × n–Matrix D heisst diagonal [diagonal], d.h. ist eine Dia- gonalmatrix [diagonal matrix], falls (D)ij = 0 f¨ur i ̸= j. F¨ur die Diagonalmatrix mit gegebenen Diagonalelementen d11, d22, . . . , dnn schreiben wir D = diag (d11, d22, . . . , dnn) . Beispiel 2.3: D =     1 0 0 0 0 5 0 0 0 0 3 0 0 0 0 −1     = diag (1, 5, 3, −1) . ♦ Die n × n–Matrix In = diag (1, 1, . . . , 1) ist die Einheitsmatrix [unit matrix] oder Identit¨at [identity] der Ordnung n. Oft schrei- ben wir einfach I und entnehmen die Gr¨osse dem Kontext. Beispiel 2.4: I = I3 =   1 0 0 0 1 0 0 0 1   . ♦ Eine Matrix R ist eine obere Dreiecksmatrix [upper triangular matrix] oder Rechtsdreiecksmatrix, falls (R)ij = 0 f¨ur i > j. Beispiel 2.5: R =     1 3 5 7 0 2 4 6 0 0 3 6 0 0 0 4     . ♦ Eine Matrix L ist eine untere Dreiecksmatrix [lower triangular matrix] oder Linksdreiecksmatrix, falls (L)ij = 0 f¨ur i < j. Beispiel 2.6: L =   9.8765 0 0 7.6543 8.7654 0 4.3210 5.4321 6.5432   . ♦ 3Viele Autoren schreiben allerdings 0 (Null) statt O (gross “Oh”). 4 ¨Ublich ist auch hier die nicht gerade konsequente Bezeichnung durch eine gew¨ohnliche Null: 0. c⃝M.H. Gutknecht 11. April 2016 LA-Skript 2-3 Kapitel 2 — Matrizen und Vektoren im R n und C n Lineare Algebra (2009) Die Mengen der reellen bzw. komplexen m×n–Matrizen bezeichnen wir mit R m×n und C m×n (2.2) und jener der reellen bzw. komplexen n–Vektoren mit R n und C n . (2.3) Es wird Aussagen geben die sowohl im Reellen als auch im Komple- xen gelten, wo aber im ersten Falle alle Gr¨ossen reell sein m¨ussen. Diese k¨onnen wir f¨ur beide F¨alle formulieren, indem wir die Mengen der m × n–Matrizen und n–Vektoren bezeichnen mit Em×n und En , wobei E :≡ R oder C . (2.4) 2.2 Das Rechnen mit Matrizen und Vektoren In diesem Abschnitt deﬁnieren wir die Addition und Multiplikati- on zweier Matrizen sowie deren Multiplikation mit einem Skalar. Zeilen- und Kolonnenvektoren sind abgedeckt als Spezialf¨alle von Matrizen mit nur einer Zeile oder Kolonne. Es ist naheliegend, was man unter dem Vielfachen einer Matrix versteht, d.h. wie man sie mit einer Zahl (oder, wie man oft sagt, mit einem Skalar) multipliziert. F¨ur Matrizen sind aber auch Ad- dition und Multiplikation deﬁniert, vorausgesetzt, dass die Ope- randen passende Gr¨osse haben: w¨ahrend Summanden die gleiche Gr¨osse haben m¨ussen, kommt es beim Produkt darauf an, dass die “Breite” des linken Faktors mit der “H¨ohe” des rechten Faktors ¨ubereinstimmt. Die Deﬁnitionen sind unter diesen Voraussetzungen auch auf Zeilen- und Kolonnenvektoren anwendbar. Definition der Multiplikation einer Matrix mit einer Zahl : Eine m × n–Matrix A wird mit einer Zahl (einem Skalar) α multi- pliziert [multiplied by a scalar], indem man jedes Element von A mit α multipliziert. Die resultierende m × n–Matrix αA mit (αA)ij :≡ α(A)ij (i = 1, . . . , m ; j = 1, . . . , n) heisst Vielfaches [multiple] (genauer: α–faches) der Matrix A. ▲ Beispiele 2.7: 5 ( 1 −3 5 −2 4 −6 ) = ( 5 −15 25 −10 20 −30 ) , 1 4 ( 4 8 ) = ( 1 2 ) . ♦ Definition der Addition zweier Matrizen: Zwei m × n–Matrizen A und B werden addiert [added], indem man entsprechende Ele- mente addiert. Die resultierende m × n–Matrix A + B mit (A + B)ij :≡ (A)ij + (B)ij (i = 1, . . . , m ; j = 1, . . . , n) heisst Summe [sum] der Matrizen A und B. ▲ LA-Skript 2-4 11. April 2016 c⃝M.H. Gutknecht Lineare Algebra (2009) Kapitel 2 — Matrizen und Vektoren im R n und C n Beispiele 2.8: ( 5 2 −1 −5 ) + ( −1 1 6 5 ) = ( 4 3 5 0 ) , ( 1 2 3 4 5 6 ) + ( 0.9 0.8 0.7 0.6 0.5 0.4 ) = ( 1.9 2.8 3.7 4.6 5.5 6.4 ) ,   1 −1 −3   +   2 5 9   =   3 4 6   . ♦ Im Gegensatz zur Summe ist das Produkt zweier Matrizen nicht etwa durch elementweise Multiplikation deﬁniert; eine solche Ope- ration w¨urde in Anwendungen nur ¨ausserst selten gebraucht. Definition der Multiplikation zweier Matrizen: Eine m × n– Matrix A kann mit einer n×p–Matrix B multipliziert [multiplied] werden, indem man die Elemente des resultierenden Produktes [product] AB wie folgt deﬁniert: (AB)ij :≡ n∑ k=1(A)ik(B)kj = (A)i1(B)1j + (A)i2(B)2j + · · · + (A)in(B)nj (i = 1, . . . , m ; j = 1, . . . , p) . Das Produkt C :≡ AB ist also eine m × p–Matrix, die — in ein- facherer Notation — berechnet wird gem¨ass cij :≡ n∑ k=1 aikbkj = ai1b1j + ai2b2j + · · · + ainbnj (i = 1, . . . , m ; j = 1, . . . , p) . (2.5) ▲ Das Produkt AB kann also nur gebildet werden, wenn die Anzahl Kolonnen von A mit der Anzahl Zeilen von B ¨ubereinstimmt. Es l¨asst sich wie folgt veranschaulichen: i–te Zeile → n m A × × × × × × × p n ↑ j–te Kolonne B × × × × × × = p m (AB)ij ↑ j–te Kolonne ← i–te Zeile AB × Das Element (AB)ij in der i–ten Zeile und der j–ten Spalte von AB bekommt man also, indem man die i–te Zeile der Matrix A c⃝M.H. Gutknecht 11. April 2016 LA-Skript 2-5 Kapitel 2 — Matrizen und Vektoren im R n und C n Lineare Algebra (2009) mit der j–ten Kolonne der Matrix B multipliziert, wobei man die Zeile als Zeilenvektor (d.h. 1 × n–Matrix) auﬀasst und die Kolonne als Kolonnenvektor (d.h. m × 1–Matrix). Beispiele 2.9: Der Zeilenvektor ( 1 2 3 ) kann mit einem Kolon- nenvektor mit drei Komponenten multipliziert werden; der erste ist eine 1 × 3–Matrix, der zweite eine 3 × 1–Matrix. Das Produkt ist also eine 1 × 1–Matrix: ( 1 2 3 )   7 −8 9   = ( 18 ) , wobei das Resultat aus 1 · 7 + 2 · (−8) + 3 · 9 = 7 − 16 + 27 = 18 folgt. Wir kommen sp¨ater auf solche Produkte eines Zeilenvektors mit einem Spaltenvektor zur¨uck. Wir werden in Zukunft eine 1 × 1–Matrix als Zahl (Skalar) auﬀassen und das obige Resultat einfach als 18 schreiben statt( 18 ). Die 2 × 3–Matrix A und die 3 × 4–Matrix B seien gegeben als A = ( 1 2 3 4 5 6 ) , B =   7 −1 1 0 −8 −2 0 −1 9 −3 0 0   . Ihr Produkt ist dann die 2 × 4–Matrix AB = ( 18 −14 1 −2 42 −32 4 −5 ) . Dabei ergibt sich zum Beispiel das (2, 1)–Element 42 gem¨ass a21b11 + a22b21 + a23b31 = 4 · 7 + 5 · (−8) + 6 · 9 = 28 − 40 + 54 = 42 . ♦ Bemerkung: Der tiefere Grund f¨ur die etwas komplizierte Art das Matrixprodukt zu deﬁnieren, wird sp¨ater ersichtlich werden, wenn wir lineare Abbildungen mit Hilfe von Matrizen beschreiben. Die Matrizen-Multiplikation entspricht dann dem Zusammensetzen von linearen Abbildungen, und das Matrix-Vektor-Produkt erlaubt, Bildpunkte auszurechnen. ▼ Aus den Deﬁnitionen der drei eingef¨uhrten Matrizenoperationen lassen sich leicht eine ganze Reihe von Eigenschaften dieser Opera- tionen herleiten, von denen die wichtigsten nachfolgend aufgef¨uhrt sind. Wir k¨onnen sie auch als “Regeln” bezeichnen, betonen aber, dass wir diese herleiten k¨onnen und nicht voraussetzen m¨ussen. LA-Skript 2-6 11. April 2016 c⃝M.H. Gutknecht Lineare Algebra (2009) Kapitel 2 — Matrizen und Vektoren im R n und C n Satz 2.1 Die Addition, die Multiplikation und die skalare Multi- plikation von Matrizen (und Vektoren) haben die folgenden Eigen- schaften, vorausgesetzt dass die Operationen deﬁniert sind: (αβ)A = α(βA), (2.6) (αA)B = α(AB) = A(αB), (2.7) (α + β)A = (αA) + (βA), (2.8) α(A + B) = (αA) + (αB), (2.9) A + B = B + A (Add. kommutativ), (2.10) (A + B) + C = A + (B + C) (Add. assoziativ), (2.11) (AB)C = A(BC) (Mult. assoziativ), (2.12) (A + B)C = (AC) + (BC) (Add./Mult. distributiv), (2.13) A(B + C) = (AB) + (AC) (Add./Mult. distributiv), (2.14) Beweis: Die Eigenschaften (2.6) und (2.8)–(2.11) ergeben sich sofort aus entsprechenden Regeln f¨ur reelle und komplexe Zahlen und der Tat- sache, dass die skalare Multiplikation und die Addition von Matrizen elementweise deﬁniert sind, wobei nat¨urlich vorausgesetzt werden muss, dass die in einer Regel auftretenden Matrizen die gleiche Gr¨osse haben. Etwas zu zeigen bleibt also nur, wenn Matrizen-Multiplikationen auftre- ten wie in (2.7) und (2.12)–(2.14). Es ist zun¨achst zu veriﬁzieren, dass jeweils die linke und die rechte Seite unter den gleichen Bedingungen an die Gr¨osse der Matrizen deﬁniert sind. Dann zeigt man, dass das (i, j)– Element links und rechts gleich ist. F¨ur (2.7) ist dies einfach. F¨ur die Assoziativit¨at der Multiplikation (2.12) m¨ussen wir annehmen, dass A ∈ Em×n , B ∈ En×p , C ∈ Ep×q . Dann ist ((AB)C)ik = p∑ l=1 (AB)ilclk = p∑ l=1 n∑ j=1 aijbjlclk , (A(BC))ik = n∑ j=1 aij(BC)jk = n∑ j=1 p∑ l=1 aijbjlclk . Die zwei Summenzeichen darf man vertauschen, also sind die beiden Ausdr¨ucke auf den rechten Seiten identisch. F¨ur das erste Distributivgesetz (2.13) brauchen wir stattdessen A ∈ Em×n , B ∈ Em×n , C ∈ En×p , und bekommen dann ((A + B)C)ij = n∑ k=1(A + B)ikckj = n∑ k=1 (aik + bik) ckj = n∑ k=1 (aikckj + bikckj) = n∑ k=1 aikckj + n∑ k=1 bikckj = (AC)ij + (BC)ij = (AC + BC)ij. c⃝M.H. Gutknecht 11. April 2016 LA-Skript 2-7 Kapitel 2 — Matrizen und Vektoren im R n und C n Lineare Algebra (2009) Der Beweis f¨ur das zweite Distributivgesetz (2.14) verl¨auft nat¨urlich genau analog. Bemerkungen: 1) Mehrere der Aussagen von Satz 2.1 bedeuten, dass man im ent- sprechenden Ausdruck auf die Klammern verzichten kann, z.B. in A(BC) = ABC. Weitere fallen weg, weil wir vereinbaren, dass (wie in R und C) die skalare und die Matrizen-Multiplikation st¨arker bin- den als die Addition. Es ist also etwa (AB) + (CD) = AB + CD. 2) Die Matrizen-Multiplikation ist (selbst f¨ur quadratische Matrizen gleicher Ordnung) nicht kommutativ, d.h. im allgemeinen gilt AB ̸= BA . (2.15) Gilt f¨ur zwei Matrizen A und B, dass AB = BA, so sagt man, dass diese Matrizen kommutieren [commute]. Beispiel 2.10: F¨ur die drei Matrizen A = ( 2 6 1 7 ) , B = ( −3 −1 2 1 ) , C = ( 15 6 1 20 ) gilt AB = ( 6 4 11 6 ) , BA = ( −7 −25 5 19 ) , AC = ( 36 132 22 146 ) , CA = ( 36 132 22 146 ) . Es ist also AB ̸= BA , AC = CA . ♦ 3) F¨ur jede m × n–Matrix A gilt dagegen ImA = A = AIn. Die Einheitsmatrix kommutiert also mit jeder quadratischen Matrix der gleichen Ordnung. ▼ Drei weitere wichtige, einfache Eigenschaften der Matrixaddition sind die folgenden. Sie ergeben sich wiederum direkt aus den ent- sprechenden Eigenschaften der reellen und komplexen Zahlen. Satz 2.2 (i) Es gibt eine m × n–Nullmatrix O deﬁniert durch (O)ij :≡ 0 (∀i, j), so dass f¨ur jede m × n–Matrix A gilt A + O = O + A = A . (2.16) (ii) Zu jeder m × n–Matrix A gibt es eine m × n–Matrix −A deﬁniert durch (−A)ij :≡ −(A)ij (∀i, j), so dass A + (−A) = (−A) + A = O . (2.17) (iii) Zu zwei m × n–Matrizen A und B gibt es eine m × n–Matrix X deﬁniert durch (X)ij :≡ (B)ij − (A)ij (∀i, j), so dass A + X = B . (2.18) LA-Skript 2-8 11. April 2016 c⃝M.H. Gutknecht Lineare Algebra (2009) Kapitel 2 — Matrizen und Vektoren im R n und C n Die Nullmatrix in Teil (i) des Satzes ist nat¨urlich jene, die wir bereits in Abschnitt 2.1 erw¨ahnt haben. Teil (iii) des Satzes k¨onnte man im Prinzip auch aus den Teilen (i) und (ii) und der Assoziativit¨at der Addition (2.11) herleiten. Die Bedeutung von Teil (iii) liegt darin, dass er auf die genaue Deﬁnition der Matrixsubtraktion f¨uhrt: B − A :≡ X , wo X L¨osung von (2.18). (2.19) Bemerkungen: 1) Beschr¨anken wir uns auf die Matrixaddition, so gelten die Regeln (2.11), (2.16) und (2.17) f¨ur beliebige Elemente der Menge der m × n–Matrizen. Das bedeutet dass diese Menge bez¨uglich der Addition eine Gruppe [group] bildet, die wegen der Kommutativit¨at (2.10) der Addition sogar kommutativ [commutative] oder sogenannt Abelsch 5 [Abelian] ist. 2) Beschr¨anken wir uns auf die Menge der quadratischen Matrizen der Ordnung n, so gelten die Eigenschaften aus den S¨atzen 2.1 und 2.2 f¨ur beliebige Elemente dieser Menge. Die Eigenschaften (2.10)– (2.14) und (2.16)–(2.17) bedeuten dabei gerade, dass diese Menge (bez¨uglich Addition und Multiplikation) einen sogenannten Ring [ring] Matrizen bildet, der wegen (2.15) nicht-kommutativ [non- commutative] ist, falls n > 1. Weil InA = AIn = A f¨ur jede n × n– Matrix A, sagt man genauer auch, dass die Menge einen Ring mit Eins [ring with identity] bildet. 3) F¨ur reelle und komplexe Zahlen folgt aus αβ = 0, dass α = 0 oder β = 0 sein muss. Beim Matrizenprodukt (2.5) ist das nicht der Fall, wenn n > 0 ist. Auch dann nicht, wenn man sich auf quadratische Matrizen beschr¨ankt. Zwei n × n Matrizen A und B mit AB = O heissen Nullteiler [divisor of zero]. Beispiel 2.11: Wegen ( 1 −1 ) ( 1 1 ) = 0 ist klar, dass ( 1 −1 3 −3 ) ︸ ︷︷ ︸ A ( 1 2 1 2 ) ︸ ︷︷ ︸ B = ( 0 0 0 0 ) ︸ ︷︷ ︸ O . ♦ 4) Nach den Bemerkungen 2) und 3) bilden die rellen n × n Ma- trizen (n > 1) also einen nicht-kommutativen Ring mit Eins und Nullteilern. ▼ 5Niels Henrik Abel (5.8.1802 – 6.4.1829), Norwegischer Mathematiker, bewies z.B. 1824, dass im allgemeinen eine Gleichung 5. Grades nicht durch Wurzelziehen l¨osbar ist. c⃝M.H. Gutknecht 11. April 2016 LA-Skript 2-9 Kapitel 2 — Matrizen und Vektoren im R n und C n Lineare Algebra (2009) Von besonderer Bedeutung ist das Produkt einer Matrix mit einem Kolonnenvektor: Definition des Matrix-Vektor-Produktes: Das Matrix-Vektor- Produkt [matrix vector product] (oder kurz: Produkt) einer m × n–Matrix A mit einem n–Vektor x ist deﬁniert als Spezialfall des Produktes zweier Matrizen: b :≡ Ax ist gegeben durch bi :≡ n∑ k=1 aik xk = ai1 x1 + ai2 x2 + · · · + ain xn (i = 1, . . . , m) . (2.20) Als Figur: i–te Zeile → n m A × × × × × × × n x × × × × × × = m ← (Ax)ij × Ax Ein spezielles, aber wichtiges Matrixprodukt ist das Produkt eines Kolonnenvektors x mit einer 1 × 1–Matrix (α), die man als Zahl (Skalar) auﬀassen kann: es gilt    x1 ... xn    (α) =    x1α ... xnα    =    αx1 ... αxn    = α    x1 ... xn    . Man beachte, dass hier links ein spezielles Matrixprodukt, rechts aber ein ¨aquivalentes Produkt eines Skalars mit einer Matrix steht. Wir deﬁnieren deshalb x α :≡ x (α) = α x . (2.21) Wir werden sehen, dass es oft Sinn macht, einen Skalar auf der rechten Seite eines Kolonnenvektors zu schreiben statt wie ¨ublich auf der linken. Die Summe solcher Produkte des Typs Skalar mal Vektor, das heisst die Summe von Vielfachen von Vektoren, f¨uhrt zu einem weiteren grundlegenden Begriﬀ der linearen Algebra: Definition: Eine Linearkombination [linear combination] der Vektoren a1, a2, . . . , an ist ein Ausdruck der Form α1a1 + α2a2 + · · · + αnan , (2.22) worin α1, α2, . . . , αn Zahlen (Skalare) sind. ▲ LA-Skript 2-10 11. April 2016 c⃝M.H. Gutknecht Lineare Algebra (2009) Kapitel 2 — Matrizen und Vektoren im R n und C n Man beachte, dass man dank der Priorit¨at der Multiplikation vor der Addition und dank der Assoziativit¨at der Matrixaddition (siehe Satz 2.1) keine Klammern setzen muss. Derartige Vereinfachungen werden laufend benutzt, ohne dass wir darauf hinweisen. Oft ist es von Vorteil, die Vektoren in einer Linearkombination (2.22) als Kolonnen einer Matrix aufzufassen: A :≡ ( a1 a2 · · · an ) . (2.23) Um die Struktur hervorzuheben, kann man das auch so schreiben: A :≡   a1 a2 · · · an   . (2.24) Mit dieser Notation und der Regel (2.21) kommen wir rasch zu einer Neuinterpretation des Matrix-Vektor-Produktes: Satz 2.3 Sind a1, a2, . . . , an gem¨ass (2.23) die Kolonnenvektoren der m × n–Matrix A, und ist x ein n–Vektor, so gilt: Ax = a1x1 + a2x2 + · · · + anxn = x1a1 + x2a2 + · · · + xnan . (2.25) Ist insbesondere ej der j–te Kolonnenvektor der Einheitsmatrix In, so gilt: Aej = aj . (2.26) Beweis: Um (2.25) zu beweisen, betrachten wir die i-te Komponente. Wir bezeichnen jene von ak mit (ak)i, so dass also (ak)i = aik. Dann ist (Ax)i = n∑ k=1 aikxk = n∑ k=1(ak)ixk = n∑ k=1(akxk)i = n∑ k=1(xkak)i , wobei wir zuletzt noch (2.21) benutzt haben. Weiter ist Formel (2.26) bloss ein Spezialfall von (2.25). Auf ¨ahnliche Weise kann man das Produkt zweier Matrizen neu interpretieren: Satz 2.4 Ist A eine m × n–Matrix und B = ( b1 b2 · · · bp ) eine n × p–Matrix, so gilt: AB = ( Ab1 Ab2 · · · Abp ) , (2.27) oder, in anschaulicherer Schreibweise AB =   Ab1 Ab2 · · · Abp   . (2.28) Beweis: Nach Satz 2.3 ist bj = Bej, j = 1, . . . , p. Die j-te Kolonne der Produktmatrix AB l¨asst sich deshalb schreiben als (AB)ej. Dank der Assoziativit¨at der Matrizen-Multiplikation folgt damit aus Satz 2.3 f¨ur j = 1, . . . , p : (AB)ej = A(Bej) = Abj . c⃝M.H. Gutknecht 11. April 2016 LA-Skript 2-11 Kapitel 2 — Matrizen und Vektoren im R n und C n Lineare Algebra (2009) Beispiel 2.12: Matrixnotation f¨ur lineare Gleichungssysteme. Mit Hilfe des Matrix-Vektor-Produktes l¨asst sich oﬀensichtlich ein linea- res Gleichungssystem aus m Gleichungen in n Unbekannten, a11x1 + a12x2 + · · · + a1nxn = b1 ... ... am1x1 + am2x2 + · · · + amnxn = bm, (2.29) wie angek¨undigt in kompakter Form schreiben: Ax = b . (2.30) Die Matrix A aus (2.1) ist die Koeﬃzientenmatrix [coeﬃcient matrix, system matrix], x ist der L¨osungsvektor [solution vector] und b ist die rechte Seite [right-hand side]. Hat man ℓ verschiedene rechte Seiten b1, b2, . . . , bℓ und entsprechend ℓ L¨osungsvektoren x1, x2, . . . , xℓ, so kann man diese als Kolonnen zweier Matrizen B und X w¨ahlen: B :≡ ( b1 b2 · · · bℓ ) , X :≡ ( x1 x2 · · · xℓ ) . Auf diese Art lassen sich ℓ Gleichungssysteme zusammenfassen in AX = B . (2.31) ♦ Ferner sehen wir aus dem Vorangehenden unmittelbar, dass die folgende Aussage gilt: Satz 2.5 Das Gleichungssystem Ax = b hat genau dann eine L¨osung, wenn b eine Linearkombination der Kolonnen von A ist. 2.3 Die Transponierte einer Matrix; symmetrische und Hermitesche Matrizen Definition: Ist A eine m×n–Matrix, so heisst die n×m–Matrix AT mit (AT)ij :≡ (A)ji die zu A transponierte [transposed] Matrix oder die Transponierte [transpose] von A. Ist A eine komplexe Matrix, so ist A mit (A)ij :≡ (A)ij die zu A konjugiert-komplexe [complex conjugate] Matrix, und AH :≡ (A)T = AT ist die zu A konjugiert-transponierte [conjugate transpose] oder Hermitesch-transponierte6 [Hermitian transpose] Matrix7. ▲ 6Charles Hermite (24.12.1822 – 14.1.1901), franz¨osischer Mathematiker, ab 1870 Professor an der Sorbonne, bewies z.B. die Transzendenz von e. 7Oft wird AH auch die zu A adjungierte [adjoint] Matrix oder kurz die Adjungierte [adjoint] genannt und mit A∗ bezeichnet. LA-Skript 2-12 11. April 2016 c⃝M.H. Gutknecht Lineare Algebra (2009) Kapitel 2 — Matrizen und Vektoren im R n und C n Beispiele 2.13: F¨ur die Matrizen A = ( 1 2 3 4 5 6 7 8 ) , C = ( 1.2 + 3.4 i 5.6 − 7.8 i 8.7 + 6.5 i 4.3 − 2.1 i ) ist AT =     1 5 2 6 3 7 4 8     , CH = ( 1.2 − 3.4 i 8.7 − 6.5 i 5.6 + 7.8 i 4.3 + 2.1 i ) . Weiter ist zum Beispiel allgemein die Transponierte einer unteren Drei- ecksmatrix eine obere Dreiecksmatrix. Und es gilt nat¨urlich: Die Trans- ponierte eines Kolonnenvektors ist ein Zeilenvektor — und umgekehrt. Dies wird oft ausgen¨utzt, um Kolonnenvektoren Platz sparend aufzu- schreiben: x = ( x1 x2 . . . xn )T = ( 1 3 5 3 5 3 1 )T . ♦ Definition: Eine Matrix A heisst symmetrisch [symmetric], falls AT = A , d.h. (A)ij = (A)ji (∀i, j) . Eine Matrix A heisst Hermitesch [Hermitian], falls AH = A , d.h. (A)ij = (A)ji (∀i, j) . ▲ Beispiele 2.14: Die Matrizen B =   2 3 −5 3 −1 2 −5 2 7   , E =   1 2 + 3 i 4 + 5 i 2 − 3 i 6 7 + 8 i 4 − 5 i 7 − 8 i 9   sind (reell) symmetrisch bzw. (komplex) Hermitesch. ♦ Man beachte, dass die Diagonalelemente einer Hermiteschen Matrix reell sind. Ferner ist eine reelle Hermitesche Matrix nat¨urlich sym- metrisch. Die Menge der Hermiteschen Matrizen enth¨alt also als Teilmenge die reellen symmetrischen Matrizen. Meist denkt man aber an komplexe Matrizen, wenn man von Hermiteschen Matrizen spricht. Es gibt auch komplexe symmetrische Matrizen, das heisst komplexe Matrizen mit AT = A. Solche kommen z.B. in der Elektrotechnik und in der Teilchenphysik vor. Beispiel 2.15: Die Matrix   8 + 5 i 1 + 2 i 6 i 1 + 2 i 7 + 6 i 2 + 3 i 6 i 2 + 3 i 4   ist komplex symmetrisch. Die Diagonalelemente brauchen nicht reell zu sein. ♦ c⃝M.H. Gutknecht 11. April 2016 LA-Skript 2-13 Kapitel 2 — Matrizen und Vektoren im R n und C n Lineare Algebra (2009) Eine weitere, oft auftretende Symmetrieeigenschaft ist die folgende: Definition: Eine quadratische Matrix A ist schiefsymmetrisch [skew-symmetric], falls AT = −A. ▲ Beispiel 2.16: Die Matrix   0 −3 5 3 0 −4 −5 4 0   ist schiefsymmetrisch und hat wie jede andere solche Matrix lauter Nul- len in der Diagonale. ♦ F¨ur das Transponieren gelten folgende einfache Rechenregeln: Satz 2.6 (i) F¨ur jede Matrix A gilt (AT)T = A , (AH)H = A , (2.32) (ii) F¨ur jede Matrix A und jeden Skalar α gilt (α A)T = α AT , (α A)H = α AH . (2.33) (iii) F¨ur beliebige m × n–Matrizen A und B gilt (A + B)T = AT + BT , (A + B)H = AH + BH . (2.34) (iv) F¨ur jede m × n–Matrix A und jede n × p–Matrix B gilt (AB)T = BTAT , (AB)H = BHAH . (2.35) Beweis: Die Aussagen (2.32)–(2.34) sollten klar sein. Um (2.35) zu beweisen bemerken wir, dass AB eine m × p–Matrix und somit (AB)T eine p × m–Matrix ist. Das Produkt BT AT ist ebenfalls eine p × m–Matrix. F¨ur die entsprechenden Elemente gilt ((AB)T)ij = (AB)ji = n∑ k=1 ajk bki = n∑ k=1 bki ajk = n∑ k=1(BT)ik(AT)kj = (BTAT)ij. Der Beweis von (AB)H = BHAH verl¨auft v¨ollig analog. Im allgemeinen ist das Produkt zweier symmetrischer Matrizen nicht symmetrisch. Zum Beispiel ist ( 0 1 1 0 ) ( 0 1 1 2 ) = ( 1 2 0 1 ) . Aber man hat die folgenden Aussagen: LA-Skript 2-14 11. April 2016 c⃝M.H. Gutknecht Lineare Algebra (2009) Kapitel 2 — Matrizen und Vektoren im R n und C n Satz 2.7 (i) F¨ur symmetrische (quadratische) Matrizen A, B gilt: AB = BA ⇐⇒ AB symmetrisch. (2.36) (ii) F¨ur beliebige Matrizen gilt: ATA und AAT sind symmetrisch, (2.38) Analoge Aussagen gelten im Hermiteschen Fall. Beweis: (i) F¨ur symmetrische Matrizen mit AB = BA folgt nach (2.35) (AB)T = (BA)T = ATBT = AB , also ist AB symmetrisch. Ist umgekehrt AB symmetrisch, hat man AB = (AB)T = BTAT = BA . (ii) Zum Beispiel ist nach (2.32) und (2.35) (ATA)T = AT(AT)T = ATA , also ist ATA symmetrisch. Mit Satz 2.6 kann man leicht die Aussagen der S¨atze 2.3 und 2.4 betreﬀend die Interpretation der Kolonnenstruktur einer Matrix ¨ubertragen auf eine analoge Interpretation der Zeilenstruktur. Um die Zeilenstruktur einer Matrix explizit zu machen, schreiben wir auf unsere unkonventionelle Weise A ≡:      a1 a2 ... am      oder A ≡:      a1 a2 ... am      , (2.40) worin ak der k-te Zeilenvektoren von A bezeichnet. Nun k¨onnen wir die S¨atze 2.3 und 2.4 “transponieren”: Korollar 2.8 Werden die Zeilenvektoren der m × n–Matrix A und der n × p–Matrix B gem¨ass A =    a1 ... am    , B =    b1 ... bn    bezeichnet, und ist y = ( y1 . . . yn ) ein Zeilenvektor, so gilt: yB = y1b1 + y2b2 + · · · + ynbn , eT i B = bi (2.41) und AB =      a1B a2B ... amB      =      a1B a2B ... amB      . (2.42) In (2.41) ist eT i wie ¨ublich der i-te Zeilenvektor von In. c⃝M.H. Gutknecht 11. April 2016 LA-Skript 2-15 Kapitel 2 — Matrizen und Vektoren im R n und C n Lineare Algebra (2009) 2.4 Das Skalarprodukt und die Norm von Vektoren; L¨angen und Winkel Ein n–Vektor x l¨asst sich als Ortsvektor im reellen (oder komple- xen) n–dimensionalen Euklidischen8 Raum R n (bzw. C n) auﬀassen, das heisst als Pfeil, der den Ursprung O mit einem anderen Punkt X = (x1, x2, . . . , xn) verbindet. Obwohl wir gewohnt sind Punkte durch ein n-Tupel, d.h. einen Zeilenvektor zu bezeichnen, wollen wir den Vektor von nun an als Kolonnenvektor x = ( x1 x2 . . . xn )T w¨ahlen. Die Vektoraddition entspricht oﬀensichtlich dem Zusammensetzen von Pfeilen und die Multiplikation mit einer Zahl einer Verl¨angerung oder Verk¨urzung des Pfeiles. Das gilt im n–dimensionalen Raum ge- nau gleich wie im 2– und 3–dimensionalen Raum. Geometrisch ist klar, was man im R n unter der (Euklidischen) L¨ange ∥x∥ eines solchen Pfeiles versteht, und was der Winkel ϕ zwischen zwei Pfeilen x und y ist. Im R 2 gilt bekanntlich der Satz von Pythagoras9 ∥x∥ = √x2 1 + x2 2 (2.43) und der daraus folgende Cosinussatz ∥y − x∥ 2 = ∥x∥ 2 + ∥y∥ 2 − 2∥x∥∥y∥ cos ϕ . (2.44) Zum Beweis lesen wir aus einer Figur ab, dass mit a :≡ ∥x∥ , b :≡ ∥y∥ , c :≡ ∥y − x∥ gilt c2 = (a − b cos ϕ)2 + b2(sin ϕ)2 = a2 − 2ab cos ϕ + b2 cos2 ϕ + b2 sin2 ϕ = a2 + b2 − 2ab cos ϕ , was mit (2.44) identisch ist. Falls ∥x∥ ̸= 0, ∥y∥ ̸= 0 ergibt sich daraus cos ϕ = ∥x∥ 2 + ∥y∥ 2 − ∥y − x∥ 2 2∥x∥∥y∥ . (2.45) Wenn wir das ¨ubliche Skalarprodukt im R 2 deﬁnieren, ⟨x, y⟩ :≡ x1y1 + x2y2 , 8Euklid, genauer: Eukleides (um 300 v. Chr.), griechischer Mathema- tiker in Alexandrien, oft f¨alschlich als Autor der “Elemente” betrachtet, des Geometrie-Lehrbuchs, das ¨uber 2000 Jahre als Standard galt. 9Pythagoras (ca. 580 – 500 v.Chr.), griechischer Mathematiker, Astro- nom, Musikwissenschafter und Philosoph; ab ca. 530 in Kroton (Oberitalien) Begr¨under seiner Schule. LA-Skript 2-16 11. April 2016 c⃝M.H. Gutknecht Lineare Algebra (2009) Kapitel 2 — Matrizen und Vektoren im R n und C n erhalten wir wegen ∥x∥ 2 + ∥y∥ 2 − ∥y − x∥ 2 = x2 1 + x2 2 + y2 1 + y2 2 − (y1 − x1)2 − (y2 − x2)2 = 2 (x1y1 + x2y2) = 2 ⟨x, y⟩ schliesslich cos ϕ = ⟨x, y⟩ ∥x∥∥y∥ . (2.46) Wir wollen die Ergebnisse (2.43), (2.44) und (2.46) auf den R n und den C n verallgemeinern. Dazu schreiben wir wieder En statt “R n oder C n”. Definition: Das (Euklidische) Skalarprodukt oder innere Produkt [inner product] zweier Vektoren x, y ∈ En ist die Zahl ⟨x, y⟩ deﬁniert durch ⟨x, y⟩ :≡ xHy = n∑ k=1 xk yk , (2.47) was sich im Falle reeller Vektoren reduziert auf ⟨x, y⟩ :≡ xTy = n∑ k=1 xk yk . (2.48) ▲ Es gelten die folgenden Eigenschaften: Satz 2.9 F¨ur das Skalarprodukt (2.48) im R n gilt: (S1) Es ist linear im zweiten Faktor: ⟨x, y + z⟩ = ⟨x, y⟩ + ⟨x, z⟩ f¨ur alle x, y, z ∈ R n, ⟨x, α y⟩ = α ⟨x, y⟩ f¨ur alle x, y ∈ R n, α ∈ R . (S2) Es ist symmetrisch: ⟨x, y⟩ = ⟨y, x⟩ f¨ur alle x, y ∈ R n. (S3) Es ist positiv deﬁnit: ⟨x, x⟩ ≥ 0 f¨ur alle x ∈ R n, ⟨x, x⟩ = 0 =⇒ x = o . F¨ur das Skalarprodukt (2.47) im Cn gelten (S1) und (S3) analog f¨ur alle x, y, z ∈ Cn und α ∈ C, aber (S2) wird ersetzt durch: (S2’) Es ist Hermitesch: ⟨x, y⟩ = ⟨y, x⟩ f¨ur alle x, y ∈ C n. c⃝M.H. Gutknecht 11. April 2016 LA-Skript 2-17 Kapitel 2 — Matrizen und Vektoren im R n und C n Lineare Algebra (2009) Beweis: Wenn man gem¨ass der Deﬁnition ⟨x, y⟩ :≡ xHy das Ska- larprodukt als Matrizenprodukt schreibt, so folgt (S1) f¨ur Rn und Cn sofort aus den Matrix-Eigenschaften (2.14) und (2.7) in Satz 2.1. Das gilt aber nicht f¨ur (S2) und (S2’). Dort beachten wir, dass xTy bzw. xHy eine Zahl ist, also ohne Wirkung transponiert werden kann; im ¨ubrigen wenden wir (2.35) an: xTy = (xTy)T = yTx und, im komplexen Fall, xHy = (xHy)T = yT(xH)T = yHx . (Denn es ist ja yT = yH und xH = (x)T.) Schliesslich: Eigenschaft (S3) ist klar, denn ⟨x, x⟩ = xHx = n∑ k=1 xk xk = n∑ k=1 |xk|2 ≥ 0 . Das Gleichheitszeichen gilt nur, wenn alle xk null sind. Aus den Eigenschaften (S1) und (S2) bzw. (S1) und (S2’) folgt sofort: Korollar 2.10 Das Skalarprodukt (2.48) im R n ist bilinear [bi- linear], d.h. zus¨atzlich zu (S1) gilt: (S4) Es ist auch linear im ersten Faktor: ⟨w + x, y⟩ = ⟨w, y⟩ + ⟨x, y⟩ f¨ur alle w, x, y, ∈ R n, ⟨α x, y⟩ = α ⟨x, y⟩ f¨ur alle x, y ∈ R n, α ∈ R . Das Skalarprodukt (2.47) im C n ist sesquilinear [sesquilinear], d.h. zus¨atzlich zu (S1) gilt: (S4’) Es ist konjugiert-linear im ersten Faktor: ⟨w + x, y⟩ = ⟨w, y⟩ + ⟨x, y⟩ f¨ur alle w, x, y, ∈ C n, ⟨α x, y⟩ = α ⟨x, y⟩ f¨ur alle x, y ∈ C n, α ∈ C . Es ist auch klar, dass man im reellen Fall (S1) und (S2) bzw. (S1) und (S4) kombinieren kann zu ⟨x, α y + β z⟩ = α ⟨x, y⟩ + β ⟨x, z⟩ , ⟨α w + β x, y⟩ = α ⟨w, y⟩ + β ⟨x, y⟩ . Im komplexen Fall gilt nach (S1) und (S2’) bzw. (S1) und (S4’)10 : ⟨x, α y + β z⟩ = α ⟨x, y⟩ + β ⟨x, z⟩ , ⟨α w + β x, y⟩ = α ⟨w, y⟩ + β ⟨x, y⟩ . Das Skalarprodukt kann nun auch als Basis f¨ur die Deﬁnition der L¨ange eines Vektors dienen: 10Es gibt auch viele Autoren, die das komplexe Skalarprodukt linear im ersten Argument und konjugiert-linear im zweiten Argument w¨ahlen. Unsere Wahl ist hier von Vorteil und in der Physik die ¨Ubliche. LA-Skript 2-18 11. April 2016 c⃝M.H. Gutknecht Lineare Algebra (2009) Kapitel 2 — Matrizen und Vektoren im R n und C n Definition: Die L¨ange [length] oder 2–Norm [2–norm] oder Euklidische Norm [Euclidean norm] eines Vektors x ∈ En ist die nichtnegative reelle Zahl ∥x∥ deﬁniert durch ∥x∥ :≡ √⟨x, x⟩ . (2.49) ▲ Man beachte, dass nach Eigenschaft (S2) gilt ⟨x, x⟩ ≥ 0, weshalb die Wurzel wohldeﬁniert ist. Nach (2.48) gilt im reellen Fall ∥x∥ = √xTx = √ √ √ √ n∑ k=1 x2 k (2.50) und nach (2.47) im komplexen Fall ∥x∥ = √xHx = √ √ √ √ n∑ k=1 |xk|2 . (2.51) Dass diese Norm (2.49) die ¨ubliche L¨ange eines Vektors liefert, folgt durch rekursives Anwenden des Satzes von Pythagoras auf Paare von Vektoren der Form ( x1 . . . xk 0 . . . 0 )T , ( 0 . . . 0 xk+1 0 . . . 0 )T . Um die L¨ange eines komplexen Vektors z = x + i y ∈ Cn geome- trisch zu interpretieren, kann man diesen als Vektor im R 2n auﬀas- sen, denn es ist ja |zk|2 = x2 k + y2 k. Beispiele 2.17: Es ist f¨ur x := ( 1 2 2 )T ∥x∥ 2 = ⟨x, x⟩ = 1 2 + 2 2 + 2 2 = 9 , also ∥x∥ = √ ⟨x, x⟩ = √9 = 3. F¨ur den komplexen Vektoren z =     i 3 + 4 i 5 + 12 i 8 + 15 i     erhalten wir ∥z∥ 2 = 1 2 + (3 2 + 4 2) + (5 2 + 12 2) + (8 2 + 15 2) = 1 + 25 + 169 + 289 = 484 , also ∥z∥ = √484 = 22. ♦ c⃝M.H. Gutknecht 11. April 2016 LA-Skript 2-19 Kapitel 2 — Matrizen und Vektoren im R n und C n Lineare Algebra (2009) Aus der Deﬁnition der 2–Norm und den Eigenschaften des Skalar- produktes folgt eine aus dem R 2 wohlbekannte Ungleichung: Satz 2.11 F¨ur alle Paare x, y ∈ En gilt die Schwarzsche Un- gleichung [Schwarz inequality] |⟨x, y⟩| ≤ ∥x∥ ∥y∥ . (2.52) Das Gleichheitszeichen gilt genau dann, wenn y ein Vielfaches ist von x oder umgekehrt. Statt Schwarzsche Ungleichung sagt man oft Cauchy-Schwarz- Ungleichung11 [Cauchy-Schwarz inequality] oder sogar Cauchy- Bunjakovski-Schwarz-Ungleichung12 [CBS inequality]. Die Schwarzsche Ungleichung ist eine Eigenschaft des Skalarpro- duktes, nicht der Norm, denn quadriert lautet sie |⟨x, y⟩|2 ≤ ⟨x, x⟩ ⟨y, y⟩ . (2.53) Der folgende Beweis, der nur auf den Eigenschaften des Skalarpro- duktes beruht, liefert deshalb zun¨achst diese Form. Beweis: F¨ur beliebiges α ∈ E ist 0 ≤ ⟨αx + y, αx + y⟩ = αα ⟨x, x⟩ + α ⟨x, y⟩ + α⟨x, y⟩ + ⟨y, y⟩ . F¨ur x = o gilt (2.52) oﬀensichtlich, und zwar mit dem Gleichheitszei- chen. Wir d¨urfen also x ̸= o annehmen und α = − ⟨x,y⟩ ⟨x,x⟩ w¨ahlen, womit nach Multiplikation mit ⟨x, x⟩ folgt: 0 ≤ | ⟨x, y⟩ |2 − | ⟨x, y⟩ |2 − | ⟨x, y⟩ |2 + ⟨x, x⟩ ⟨y, y⟩ , also (2.53), was ¨aquivalent ist mit (2.52). Das Gleichheitszeichen gilt genau, wenn x = o oder αx + y = o ist, was gerade heisst, dass y ein Vielfaches von x ist oder umgekehrt. (Ausser wenn x = o oder y = o gilt, ist dann y ein Vielfaches von x ist und umgekehrt.) Beispiel 2.18: F¨ur beliebige reelle Zahlen x1, . . . xn, y1, . . . , yn gilt nach (2.53) ( n∑ k=1 xkyk)2 ≤ ( n∑ k=1 x2 k)( n∑ j=1 y2 j ) . (2.54) Zum Beispiel erh¨alt man f¨ur die zwei Vektoren x := ( 2 2 1 )T , y := ( 0 15 8 )T (2.55) 11Augustin Louis Cauchy (21.8.1789 – 23.5.1857), franz¨osicher Mathema- tiker, ab 1916 Professor an der Ecole Polytechnique, sp¨ater an der Sorbonne; k¨onigstreu und streng katholisch; fast 800 Publikationen, darunter wichtige Beitr¨age zur Gruppentheorie, Analysis und komplexen Funktionentheorie. Hermann Amandus Schwarz (25.1.1843 – 30.11.1921), deutscher Mathe- matiker, Professor in Z¨urich, G¨ottingen und Berlin. 12Victor Jakowlewitsch Bunjakovski [englische Transkription: Bunya- kovskii], (16.12.1804 – 12.12.1889), russischer Mathematiker in Peterburg. LA-Skript 2-20 11. April 2016 c⃝M.H. Gutknecht Lineare Algebra (2009) Kapitel 2 — Matrizen und Vektoren im R n und C n ∥x∥2 = 9, ∥y∥2 = 289, ⟨x, y⟩ = 38. Also lautet die quadrierte Schwarz- sche Ungleichung hier 1444 = 382 ≤ 9 · 289 = 2601 und die Schwarzsche Ungleichung selbst ist 38 ≤ 3 · 17 = 51. ♦ Grundlegende Eigenschaften der Norm sind die folgenden: Satz 2.12 F¨ur die 2–Norm (2.49) im En gilt: (N1) Sie ist positiv deﬁnit: ∥x∥ ≥ 0 f¨ur alle x ∈ En , ∥x∥ = 0 =⇒ x = o . (N2) Sie ist dem Betrage nach homogen: ∥α x∥ = |α| ∥x∥ f¨ur alle x ∈ En, α ∈ E . (N3) Die Dreiecksungleichung [triangle inequality] gilt: ∥x ± y∥ ≤ ∥x∥ + ∥y∥ f¨ur alle x, y ∈ En . Beweis: (N1) und (N2) folgen leicht aus den entsprechenden Eigen- schaften des Skalarproduktes. Um (N3) zu beweisen, sch¨atzen wir in ∥x ± y∥ 2 = ⟨x ± y, x ± y⟩ = ⟨x, x⟩ ± 2 ⟨x, y⟩ + ⟨y, y⟩ den gemischten Term ⟨x, y⟩ mittels der Schwarzschen Ungleichung (2.52) ab durch 2 ∥x∥ ∥y∥ ∥x ± y∥ 2 ≤ ∥x∥ 2 + 2 ∥x∥ ∥y∥ + ∥y∥2 = (∥x∥ + ∥y∥)2 . Schreibt man im Z¨ahler der Formel (2.45) f¨ur cos ϕ die Quadrate der Normen als Skalarprodukte und wendet man die Rechenregeln aus Satz 2.9 an, so kann man nun allgemein im R n und C n diesen Z¨ahler durch ein einziges Skalarprodukt ausdr¨ucken (wie wir das in (2.46) im Spezialfall des R 2 gemacht haben): ∥x∥ 2 + ∥y∥ 2 − ∥y − x∥ 2 = ∥x∥ 2 + ∥y∥ 2 − ⟨y − x, y − x⟩ = ∥x∥ 2 + ∥y∥ 2 − ⟨y, y⟩ + ⟨y, x⟩ + ⟨x, y⟩ − ⟨x, x⟩ = 2 Re ⟨x, y⟩ . Also ist ϕ = arccos Re ⟨x, y⟩ ∥x∥∥y∥ . (2.56) Im reellen Fall entf¨allt nat¨urlich der Realteil: ϕ = arccos ⟨x, y⟩ ∥x∥∥y∥ . (2.57) c⃝M.H. Gutknecht 11. April 2016 LA-Skript 2-21 Kapitel 2 — Matrizen und Vektoren im R n und C n Lineare Algebra (2009) Diese Formeln liefern den Winkel [angle] zwischen zwei Vektoren, wobei 0 ≤ ϕ ≤ π gilt. Dass sie nicht nur im R2 sondern auch im R n und im C n gelten, l¨asst sich daraus schliessen, dass die zwei Vek- toren in einer (zweidimensionalen) Ebene liegen, dass der Winkel gem¨ass (2.45) durch L¨angen ausgedr¨uckt werden kann und dass, wie wir sp¨ater sehen werden, L¨angen und Skalarprodukt nicht vom Koordinatensystem abh¨angen, solange dieses orthonormiert bleibt. (2.57) liefert auch eine neue Interpretation der Schwarzschen Un- gleichung (2.52) im R n: der Faktor, um den die linke Seite von (2.52) kleiner ist als die rechte, ist gerade | cos ϕ|. Das Gleichheitszeichen gilt in (2.52) also genau dann, wenn | cos ϕ| = 1 ist, das heisst wenn der Zwischenwinkel ϕ null oder π ist. Definition: Zwei n-Vektoren x und y sind zueinander ortho- gonal [orthogonal] (oder: stehen senkrecht aufeinander [are perpendicular]) falls ⟨x, y⟩ = 0. Wir schreiben: x ⊥ y. ▲ Beispiel 2.19: In Beispiel 2.18 erhielten wir f¨ur die zwei Vektoren (2.55): ∥x∥2 = 9, ∥y∥2 = 289, ⟨x, y⟩ = 38. Also ist cos ϕ = 38 3 · 17 = 38 51 = 0.745098.. , was ϕ = 0.7301145.. oder, in Grad, ϕ = 41.83248..◦ ergibt. ♦ Beispiel 2.20: Die Vektoren u := ( 1 2 3 4 5 6 )T , v := ( −6 5 −4 3 −2 1 )T stehen wegen ⟨u, v⟩ = −6+10−12+12−10+6 = 0 senkrecht aufeinander. ♦ Zwei senkrechte Vektoren bilden zusammen mit dem an die Spitze von y verschobenen Vektor x − y ein rechtwinkliges Dreieck. Der Satz von Pythagoras nimmt damit folgende Form an: Satz 2.13 (Satz von Pythagoras) F¨ur x, y ∈ En mit x ⊥ y gilt ∥x ± y∥ 2 = ∥x∥ 2 + ∥y∥ 2 (2.58) Beweis: Aus der Deﬁnition (2.49), den Eigenschaften (S1), (S4) und (S2) bzw. (S2’), sowie der Voraussetzung ⟨x, y⟩ = 0 folgt: ∥x ± y∥2 = ⟨x ± y, x ± y⟩ = ⟨x, x⟩ ± ⟨x, y⟩ ± ⟨y, x⟩ + ⟨y, y⟩ = ∥x∥ 2 + ∥y∥ 2 , denn ⟨y, x⟩ = ⟨x, y⟩ = 0. Neben dem Euklidischen Skalarprodukt (2.47)–(2.48) und der da- von abgeleiteten (Euklidischen) 2–Norm werden in der Praxis ab und zu noch andere Skalarprodukte und Normen gebraucht. Dabei werden auch Normen eingef¨uhrt, die nicht gem¨ass (2.49) auf einem Skalarprodukt beruhen. LA-Skript 2-22 11. April 2016 c⃝M.H. Gutknecht Lineare Algebra (2009) Kapitel 2 — Matrizen und Vektoren im R n und C n Definition: Ein Skalarprodukt oder inneres Produkt [inner product] im R n (bzw. C n) ist eine Funktion, die jedem Paar von n-Vektoren eine reelle (bzw. komplexe) Zahl zuordnet, wobei die Eigenschaften (S1)–(S3) [bzw. (S1), (S2’), (S3)] aus Satz 2.9 gelten. Eine Norm [norm] im En (d.h. R n oder C n) ist eine Funktion, die jedem n-Vektor eine nichtnegative reelle Zahl zuordnet, wobei die Eigenschaften (N1)–(N3) aus Satz 2.12 gelten. ▲ Die gebr¨auchlichsten Normen sind Spezialf¨alle der p–Norm: ∥x∥p :≡ (|x1|p + · · · + |xn|p) 1 p (1 ≤ p ≤ ∞), (2.59) wobei die F¨alle 1, 2 und ∞ von gr¨osstem Interesse sind. Dabei ist ∥x∥1 :≡ (|x1| + · · · + |xn|) , ∥x∥∞ :≡ max k=1,...,n |xk| . (2.60) Beispiel 2.21: Betrachten wir in der normalen Euklidischen Ebene die Vektoren x, f¨ur die ∥x∥1 = 1 ist, so bilden diese den Rand eines Quadrates mit Seitenl¨ange √2, dessen Diagonalen auf den Achsen liegen. Betrachten wir stattdessen die Vektoren mit ∥x∥∞ = 1, so bilden diese den Rand eines Quadrates mit Seitenl¨ange 2, dessen Seitenmittelsenk- rechten auf den Achsen liegen. Im R3 liefert ∥x∥1 = 1 ein regul¨ares Oktaeder, ∥x∥∞ = 1 dagegen einen W¨urfel. ♦ 2.5 Das ¨aussere Produkt und die orthogonale Projektion auf eine Gerade Sind ein m–Vektor x und ein n–Vektor y gegeben, so ist das Skalar- produkt (innere Produkt) ⟨x, y⟩ = xH y nur deﬁniert wenn m = n, in welchem Falle es eine 1 × 1–Matrix, d.h. ein Skalar ist. Dagegen k¨onnen wir das Produkt x yH immer bilden; es ist eine m × n–Matrix. Im rellen Fall hat man x yT =        x1 ... xi ... xm        ( y1 . . . yj . . . yn ) =        x1y1 . . . x1yj . . . x1yn ... ... ... xiy1 . . . xiyj . . . xiyn ... ... ... xmy1 . . . xmyj . . . xmyn        . Definition: Das dyadische Produkt oder ¨aussere Produkt [outer product] eines m–Vektors x und eines n–Vektors y ist die m×n–Matrix x yH bzw. im reellen Fall die Matrix x yT. c⃝M.H. Gutknecht 11. April 2016 LA-Skript 2-23 Kapitel 2 — Matrizen und Vektoren im R n und C n Lineare Algebra (2009) Satz 2.14 Eine m × n Matrix hat genau dann den Rang 1, wenn sie das ¨aussere Produkt eines m–Vektors x ̸= o und eines n– Vektors y ̸= o ist. Beweis: Hat eine Matrix A Rang 1, so gibt es eine Zeile yT, die nicht null ist, und alle andere Zeilen sind Vielfache davon (denn im Gauss’schen Algorithmus kann man Vielfache dieser Zeile von den ande- ren subtrahieren und erh¨alt dabei lauter Nullzeilen). Im komplexen Fall darf man diese Zeile auch mit yH bezeichnen. Ihr Index sei ℓ. Nennen wir den Multiplikator f¨ur die i–te Zeile xi, und setzen wir xℓ := 1 und x := ( x1 . . . xn )T, so ist gerade A = x yH, wobei x ̸= o und y ̸= o. Ist umgekehrt A = x yH mit x ̸= o, y ̸= o, so gibt es ein xℓ ̸= 0, womit die ℓ–te Zeile von A nicht null ist. W¨ahlt man diese Zeile als Pivotzeile, so kann man, f¨ur alle i ̸= ℓ, durch Subtraktion des xi/xℓ–fachen der ℓ–ten Zeile die i–te Zeile zu null machen. Es folgt, dass A (nach unserer, auf der Anwendung des Gauss–Algorithmus beruhenden Deﬁnition des Ranges) den Rang 1 hat. Das ¨aussere Produkt eines Vektors y mit sich selbst wird gebraucht f¨ur die orthogonale Projektion auf die Gerade (durch O) mit der durch y gegebenen Richtung: Satz 2.15 Die orthogonale Projektion Py x des n–Vektors x auf die durch die Vielfachen von y (̸= o) erzeugte Gerade durch den Ursprung ist gegeben durch Py x :≡ 1 ∥y∥2 y yH x = u u H x , (2.61) worin u :≡ y/∥y∥. Im Reellen kann man yH durch yT ersetzen. Beweis: Die Projektion Pyx zeichnet sich geometrisch dadurch aus, dass f¨ur alle x gilt: (i) Pyx = αy f¨ur einen von x abh¨angigen Skalar α, (ii) x − Pyx ⊥ y. Diese zwei Eigenschaften sind f¨ur den in (2.61) deﬁnierten Ausdruck f¨ur Py x zu veriﬁzieren, wobei wir annehmen d¨urfen, dass ∥y∥ = 1 und damit der Bruch wegf¨allt. Zu (i): Es ist Pyx = y (yH x), wobei in der Klammer ein Skalar steht. Also gilt (i) mit α :≡ yH x. Beachte, dass hier (2.21) angewendet wird. Zu (ii): Unter Verwendung von yH y = ∥y∥2 = 1 folgt ⟨y, x − Pyx⟩ = ⟨y, x⟩ − 〈y, y yH x〉 = yH x − yH y yH x = 0 . Py x kann man so interpretieren: Auf den zu projizierenden Vektor x wird die Rang-1–Matrix Py :≡ 1 ∥y∥2 y yH = u u H ≡: Pu (2.62) LA-Skript 2-24 11. April 2016 c⃝M.H. Gutknecht Lineare Algebra (2009) Kapitel 2 — Matrizen und Vektoren im R n und C n (von links) angewendet, wobei wieder u :≡ y/∥y∥ ist Beispiel 2.22: Gegeben seien die zwei Punkte X = (4, 8, −1) und Y = (1, 2, 2) im R3, also die zwei Vektoren x = ( 4 8 −1 )T und y = ( 1 2 2 )T. Man projiziere die Strecke OX orthogonal auf die durch die Punkte O und Y laufende Gerade. Dazu ist bloss die reelle Version der Formel (2.61) auszuwerten, wobei es am eﬃzientesten ist, zuerst yTx = 18 und ∥y∥2 = 9 zu berechnen (und nicht etwa die Matrix yyT). Dann wird Pyx = 1 ∥y∥2 y (yT x) = 2 y = ( 2 4 4 )T . Es wird also X auf X ′ = (2, 4, 4) projiziert. ♦ Die Projektionsmatrix Py aus (2.62) hat zwei interessante Eigen- schaften: PH y = Py , P2 y = Py . (2.63) Die erste bedeutet, dass Py Hermitesch oder, falls reell, symme- trisch ist. Aufgrund der zweiten bezeichnet man Py als idempo- tent [idempotent]. Diese zwei Eigenschaften sind charakteristisch f¨ur orthogonale Projektionen und k¨onnen zu deren Deﬁnition ver- wendet werden. Py ist aber eine spezielle orthogonale Projektion, weil ja auf eine Gerade projiziert wird. Im R 3 (und allgemeiner im En) kann man aber auch auf eine Ebene projizieren. Wir werden darauf zur¨uckkommen. 2.6 Matrizen als lineare Abbildungen Ist A ∈ Em×n irgend eine m × n Matrix, so kann man die ebenfalls mit A bezeichnete Abbildung A : En → Em , x ↦→ Ax (2.64) betrachten. Sie hat zwei grundlegende Eigenschaften: f¨ur alle x, ̃x ∈ En und alle γ ∈ E gilt A(x + ̃x) = Ax + Ãx , A(γx) = γAx . (2.65) Man kann sie auch zusammenfassen in A(γx + ̃x) = γ(Ax) + (Ãx) . (2.66) Wir werden in Kapitel 5 sehen, dass diese Eigenschaften per De- ﬁnition charakteristisch sind f¨ur eine lineare Abbildung [linear transformation]. W¨ahrend der Deﬁnitionsbereich der Abbildung A aus (2.64) immer der ganze Raum En ist, ist das Bild [image] von A, im A :≡ {Ax ∈ Em ; x ∈ En} (2.67) c⃝M.H. Gutknecht 11. April 2016 LA-Skript 2-25 Kapitel 2 — Matrizen und Vektoren im R n und C n Lineare Algebra (2009) im allgemeinen eine Teilmenge von Em. Es ist aber immer o ∈ im A, denn Ao = o. Beispiel 2.23: Die Projektionsmatrix Py aus (2.62) ist eine n × n Matrix, bildet also En in sich ab. Wie wir gesehen haben, stellt sie eine orthogonale Projektion auf die durch den Ursprung verlaufende Gerade mit Richtung y dar. Das bedeutet insbesondere, dass das Bild im Py gerade diese Gerade ist: im Py = {αy ; α ∈ E} . (2.68) ♦ 2.7 Die Inverse einer Matrix Zwei quadratische Matrizen gleicher Ordnung kann man immer zu- einander addieren, voneinander subtrahieren und miteinander mul- tiplizieren. Wir werden in diesem Abschnitt sehen, dass man sie oft, aber nicht immer, auch in gewissem Sinne durcheinander dividieren kann. Vergleicht man n × n–Matrizen mit reellen oder komplexen Zah- len, so nimmt die Nullmatrix On bei der Matrizen-Addition die Rolle der Null ein, und die Einheitsmatrix In ¨ubernimmt bei der Matrizen-Multiplikation die Rolle der Eins. Bei den Zahlen gibt es zu jedem α ̸= 0 ein ξ, so dass α ξ = 1 = ξ αist. Gilt dies wohl analog f¨ur quadratische Matrizen? Das heisst, gibt es zu A ̸= On eine n × n–Matrix X mit AX = In = XA ? Ein einfaches Beispiel zeigt, dass das nicht so ist: f¨ur A = ( 1 0 0 0 ) folgt bei beliebiger Wahl von X AX = ( 1 0 0 0 ) ( x11 x12 x21 x22 ) = ( x11 x12 0 0 ) ̸= I2 Definition: Eine n × n–Matrix A heisst invertierbar [inverti- ble], falls es eine n × n–Matrix X gibt, so dass AX = In = XA ist. Die Matrix X heisst Inverse [inverse] von A und wird mit A−1 bezeichnet: A A−1 = In = A−1 A . (2.69) ▲ Beispiel 2.24: Es ist ( 2 2 1 2 ) ( 1 −1 − 1 2 1 ) = ( 1 0 0 1 ) = ( 1 −1 − 1 2 1 ) ( 2 2 1 2 ) , es ist also die eine Matrix die Inverse der anderen — und umgekehrt. ♦ LA-Skript 2-26 11. April 2016 c⃝M.H. Gutknecht Lineare Algebra (2009) Kapitel 2 — Matrizen und Vektoren im R n und C n Beispiel 2.25: Die Einheitsmatrix In ist nat¨urlich invertierbar und ist ihre eigene Inverse, denn InIn = In = InIn. ♦ Satz 2.16 Ist A invertierbar, so ist die Inverse eindeutig be- stimmt. Beweis: Angenommen X und Y sind Inverse von A, dann ist X = X I = X(AY) = (XA)Y = I Y = Y . Man kann die Bedingung f¨ur die Inverse eﬀektiv abschw¨achen: Es gen¨ugt, dass A X = I oder X A = I gilt, dann folgt die andere Beziehung automatisch. Dies ist enthalten in Satz 2.17 Die folgenden Aussagen ¨uber eine n×n–Matrix A sind ¨aquivalent: i) A ist invertierbar. ii) Es gibt eine n × n–Matrix X mit AX = In. iii) Es gibt genau eine n × n–Matrix X mit AX = In. iv) A ist regul¨ar, d.h. Rang A = n. Beweis: Wir zeigen, dass (i) =⇒ (ii) =⇒ (iv) =⇒ (iii) =⇒ (i). Wir nehmen also an, A sei invertierbar, d.h. (i) gelte. Dann ist (ii) mit X = A−1 erf¨ullt. Hieraus folgt anderseits, dass das Gleichungssystem Ax = b f¨ur jedes b die L¨osung x :≡ Xb hat, denn nach dem Assozia- tivgesetz der Matrizen-Multiplikation gilt Ax = A(Xb) = (AX)b = Inb = b . Nach Korollar 1.7 folgt, dass es f¨ur jedes b genau eine L¨osung gibt und dass Rang A = n ist, also (iv) gilt. Bezeichnet ej wieder die j-te Kolonne von In, so haben dann umgekehrt die Systeme Ax = ej (j = 1, . . . , n) alle genau eine L¨osung, was bedeu- tet, dass AX = In eine eindeutige L¨osung X hat. Aus AX = In ergibt sich weiter A(X + XA − In) = AX + A(XA) − AIn = In + (AX)A − A = In + A − A = In . Da X die einzige L¨osung von AX = In ist, folgt, dass X + XA − In = X ist, also auch XA = In gilt, d.h. X ist die Inverse von A. Es gilt somit (i). Die Eigenschaften “regul¨ar” und “invertierbar” sind also ¨aquivalent, und man k¨ame deshalb mit einer der zwei Bezeichnungen aus. Meist bevorzugt man die Bezeichung regul¨ar. Als n¨achstes stellen wir Eigenschaften der Inversen zusammen. c⃝M.H. Gutknecht 11. April 2016 LA-Skript 2-27 Kapitel 2 — Matrizen und Vektoren im R n und C n Lineare Algebra (2009) Satz 2.18 Sind A und B regul¨are n × n–Matrizen, so gilt: i) A−1 ist regul¨ar und (A−1)−1 = A . (2.70) ii) AB ist regul¨ar und (AB)−1 = B−1A−1 . (2.71) iii) AT und AH sind regul¨ar und (AT)−1 = (A−1)T , (AH)−1 = (A−1)H . (2.72) Beweis: i) ergibt sich sofort aus der Deﬁnition (2.69). Zu ii): Aus (AB)(B−1A−1) = A(BB−1)A−1 = AA−1 = In und Satz 2.17 folgt, dass B−1A−1 die Inverse von AB ist. Zu iii): Gem¨ass (2.35) ist AT(A−1)T = (A−1A)T = IT n = In. Also ist wiederum nach Satz 2.17 (A−1)T die Inverse von AT. Ganz analog ergibt sich (AH)−1 = (A−1)H. Die Berechnung von A−1: Wie wir im Beweis von Satz 2.17 gesehen haben, lassen sich die n Kolonnen von A−1 berechnen als L¨osungen der n Gleichungssysteme Axk = ek, k = 1, . . . , n. Wir k¨onnen deshalb die Inverse mit Gauss- Elimination bestimmen durch gleichzeitiges L¨osen dieser n Systeme. Mit Hilfe der Inversen der Koeﬃzientenmatrix, falls sie existiert, l¨asst sich ein lineares Gleichungssystems oﬀensichtlich sofort for- mal l¨osen, indem man es von links mit A−1 multipliziert. Aus Ko- rollar 1.7 wissen wir ja schon, dass es genau eine L¨osung gibt, wenn A regul¨ar ist. Genauer: Im Beweis von Satz 2.17 haben wir die ein- deutige L¨osbarkeit des Systems direkt mit der Invertierbarkeit und Regularit¨at der Matrix verkn¨upft. Zusammen gibt das Satz 2.19 Ist A regul¨ar, so hat das Gleichungssystem Ax = b f¨ur jede rechte Seite b eine eindeutige L¨osung, und zwar ist x = A−1b . (2.73) Bemerkung: Da die Berechnung der Inversen A−1 das L¨osen von n linearen Gleichungssystemen mit der Matrix A erfordert, macht es keinen Sinn, ein System Ax = b auf diesem Weg zu l¨osen, denn der Aufwand ist deutlich gr¨osser. Das gilt auch dann noch, wenn das System f¨ur mehrere rechte Seiten zu l¨osen ist. Wir werden in Abschnitt 3.1 den Rechenaufwand diverser Varianten diskutieren. ▼ LA-Skript 2-28 11. April 2016 c⃝M.H. Gutknecht Lineare Algebra (2009) Kapitel 2 — Matrizen und Vektoren im R n und C n 2.8 Orthogonale und unit¨are Matrizen Orthogonale reelle und unit¨are komplexe Matrizen sind spezielle regul¨are quadratische Matrizen, die in vielen Anwendungen und Algorithmen vorkommen. Ihre Beliebtheit in der Numerik beruht unter anderem auf sehr guten Eigenschaften betreﬀend Rundung. Definition: Eine n × n–Matrix A heisst unit¨ar [unitary], falls AHA = In. Eine relle unit¨are Matrix heisst auch orthogonal [or- thogonal]; f¨ur sie gilt ATA = In. ▲ Satz 2.20 Sind A und B unit¨are (bzw. orthogonale) n × n– Matrizen, so gilt: i) A ist regul¨ar und A−1 = AH (bzw. A−1 = AT). ii) AAH = In (bzw. AAT = In). iii) A−1 ist unit¨ar (orthogonal). iv) AB ist unit¨ar (orthogonal). Beweis: Zu i)–iii): Aus AHA = In folgt gem¨ass Satz 2.17 (mit A := AH, X := A), dass AH invertierbar ist und (AH)−1 = A gilt. Wegen der Symmetrie in der Deﬁnition (2.69) der Inversen ist damit auch A invertierbar und A−1 = AH. Daraus folgt erstens nach Satz 2.17, dass A regul¨ar ist. Zweitens ist AAH = AA−1 = In. Drittens ergibt sich mit Hilfe von (2.32), dass (A−1)HA−1 = (AH)HA−1 = AA−1 = In; also ist A−1 unit¨ar. Zu iv): Dies folgt aus (AB)HAB = BH(AHA)B = BHB = In. Die Aussagen ¨uber orthogonale Matrizen folgen als Spezialfall. Beispiel 2.26: Givens–Rotationen13 [Givens rotations, plane ro- tations] sind einfache orthogonale Matrizen, welche eine Drehung um einen Winkel −φ in einer durch zwei Koordinatenachsen aufgespann- ten Ebene beschreiben; die anderen Koordinatenachsen bleiben fest. Im R3 bedeutet dies, dass genau eine Koordinatenachse fest bleibt. Das ist dann die Drehachse. Als Beispiel betrachten wir eine Drehung im R5 in der durch die erste und die dritte Achse aufgespannten Ebene: U13(φ) :≡       cos φ 0 sin φ 0 0 0 1 0 0 0 − sin φ 0 cos φ 0 0 0 0 0 1 0 0 0 0 0 1       , Es ist UT 13(φ) = U13(−φ), und f¨ur UT 13(φ)U13(φ) ergibt sich 13J. Wallace Givens (1910 – 1993), amerikanischer Numeriker, Direktor der Applied Mathematics Division am Argonne National Laboratory. c⃝M.H. Gutknecht 11. April 2016 LA-Skript 2-29 Kapitel 2 — Matrizen und Vektoren im R n und C n Lineare Algebra (2009)       cos φ 0 − sin φ 0 0 0 1 0 0 0 sin φ 0 cos φ 0 0 0 0 0 1 0 0 0 0 0 1             cos φ 0 sin φ 0 0 0 1 0 0 0 − sin φ 0 cos φ 0 0 0 0 0 1 0 0 0 0 0 1       = I5 . Givens–Rotationen werden in diversen Algorithmen eingesetzt als Bau- steine f¨ur kompliziertere orthogonale Transformationen. Manchmal wer- den sie auch Jacobi–Rotationen14 [Jacobi rotations] genannt. ♦ Beispiel 2.27: Householder-Matrizen15 [Householder matrices] (oder: Householder-Spiegelungen [Householder reﬂections]) bilden eine weitere spezielle Klasse orthogonaler Matrizen. Sie beschreiben die Spiegelung an einer Hyperebene (Deﬁnition sp¨ater), d.h. an einer Gera- den, falls n = 2, und an einer Ebene, wenn n = 3. Es sei u ein reeller Kolonnenvektor der L¨ange 1, d.h. uTu = 1. Dann ist das ¨aussere Produkt uuT ja die n × n–Matrix mit (uuT)ij = uiuj. Die zu u geh¨orende Householder-Matrix ist Qu :≡ I − 2uuT . (2.74) Sie l¨asst sich oﬀensichtlich durch die Projektion (2.62) auf die Richtung u ausdr¨ucken (wobei jetzt u reell und ∥u∥ = 1 ist): Qu = I − 2 Pu , wo Pu :≡ uuT . (2.75) Pu und damit auch Qu ist symmetrisch. Wir zeigen nun, dass Qu auch orthogonal ist. Wegen P2 u = u(uTu)uT = uuT = Pu, ist QT uQu = Q2 u = (I − 2 Pu)2 = I − 4 Pu + 4 P2 u = I . Hier noch ein Beispiel einer 4 × 4–Householder-Matrix: Mit u = ( 4/5 −2/5 1/5 −2/5 )T erhalten wir uTu = 1 und Pu = uuT = 1 25     16 −8 4 −8 −8 4 −2 4 4 −2 1 −2 −8 4 −2 4     , Qu = I − 2 Pu = 1 25     −7 16 −8 16 16 17 4 −8 −8 4 23 4 16 −8 4 17     . ♦ Beispiel 2.28: Permutationsmatrizen [permutation matrices] sind quadratische Matrizen, die in jeder Kolonne und in jeder Zeile genau eine Eins und sonst lauter Nullen haben. Zum Beispiel ist 14Carl Gustav Jacobi (10.12.1804 – 18.2.1851), deutscher Mathematiker, Professor in K¨onigsberg und Berlin. 15Alston Scott Householder (5.5.1904 – 4.7.1993), amerikanischer Ma- thematiker, 1946 – 1969 am Oak Ridge National Laboratory, ab 1948 Direktor der Mathematics Division, Vorreiter der mathematischen Biologie und der nu- merischen linearen Algebra. LA-Skript 2-30 11. April 2016 c⃝M.H. Gutknecht Lineare Algebra (2009) Kapitel 2 — Matrizen und Vektoren im R n und C n P :≡       0 0 1 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 0 0       eine Permutationsmatrix der Ordnung n = 5. Man veriﬁziert leicht, dass jede Permutationsmatrix orthogonal ist. Wendet man P von links auf eine Matrix A an, so enth¨alt das Produkt PA permutierte Zeilen von A. Wendet man P von rechts auf A an, so besteht das Produkt AP aus permutierten Kolonnen von A. ♦ Die durch orthogonale (reelle) und unit¨are (komplexe) Matrizen deﬁnierten linearen Abbildungen haben besondere geometrische Ei- genschaften: Satz 2.21 Die durch eine orthogonale oder unit¨are n × n Ma- trix A deﬁnierte Abbildung ist l¨angentreu [length preserving] (oder: isometrisch [isometric]) und winkeltreu [angle preser- ving], d.h. es ist f¨ur alle x, y ∈ En ∥Ax∥ = ∥x∥ , ⟨Ax, Ay⟩ = ⟨x, y⟩ . (2.76) Da gem¨ass (2.56)–(2.57) der Winkel zwischen zwei Vektoren mit- tels Skalarprodukt und Norm deﬁniert ist, folgt aus (2.76) in der Tat, dass bei der Abbildung A der Winkel zwischen zwei Vekto- ren invariant (unver¨andert) bleibt, d.h. gleich jenem zwischen den Bildvektoren ist. Beweis von Satz 2.21: Nach Voraussetzung ist AHA = I, also ⟨Ax, Ay⟩ = (Ax)H(Ay) = xHAHAy = xHy = ⟨x, y⟩ . W¨ahlt man y := x, so folgt ∥Ax∥ 2 = ⟨Ax, Ax⟩ = ⟨x, x⟩ = ∥x∥ 2 , also ∥Ax∥ = ∥x∥. 2.9 Strukturierte Matrizen Wir haben bereits mehrere Typen von Matrizen mit spezieller Struk- tur angetroﬀen, etwa die symmetrischen Matrizen, die Dreiecksma- trizen und die Rang-1–Matrizen, welche ¨aussere Produkte von zwei Vektoren sind. In der Matrizentheorie und den Anwendungen kom- men noch viele weitere Matrizen mit speziellen Strukturen vor, von denen wir hier ein paar vorstellen wollen. Eine Matrix B ist eine untere Bidiagonalmatrix [lower bidiago- nal matrix] bzw. eine obere Bidiagonalmatrix [upper bidiagonal matrix], falls (B)ij = 0 f¨ur i < j und i > j + 1 bzw. f¨ur j < i und j > i + 1. c⃝M.H. Gutknecht 11. April 2016 LA-Skript 2-31 Kapitel 2 — Matrizen und Vektoren im R n und C n Lineare Algebra (2009) Beispiel 2.29: B =     2 0 0 0 3 4 0 0 0 2 6 0 0 0 5 3     ist eine untere Bidiagonalmatrix der Ordnung 4. ♦ Eine quadratische Matrix T heisst tridiagonal [tridiagonal], d.h. ist eine Tridiagonalmatrix [tridiagonal matrix], falls (T)ij = 0 f¨ur i > j + 1 und j > i + 1. Beispiel 2.30: T =     4 2 0 0 3 5 4 0 0 8 6 7 0 0 9 0     ist tridiagonal. Nat¨urlich d¨urfen einzelne Elemente innerhalb des Bandes null sein. ♦ Eine m × n Matrix B ist eine Bandmatrix [banded matrix] mit unterer Bandbreite [lower bandwidth] p und oberer Bandbrei- te [upper bandwidth] q, wenn (B)ij = 0 falls i > j + p oder j > i + q. Die (gesamte)Bandbreite [(total) bandwidth] ist p + q + 1. Beispiel 2.31: B =       9 6 3 0 0 7 8 5 2 0 0 5 7 4 1 0 0 3 6 3 0 0 0 1 5       hat untere Bandbreite 1, obere Bandbreite 2 und gesamte Bandbreite 4. (Die Diagonale wird also nur bei der gesamten Bandbreite mitgez¨ahlt.) ♦ Tridiagonalmatrizen sind nat¨urlich spezielle Bandmatrizen mit obe- rer und unterer Bandbreite eins. Bei Bidiagonalmatrizen ist eine der Bandbreiten eins, die andere null. Eine wichtige Klasse einseitiger Bandmatrizen sind die Hessenberg- Matrizen 16 [Hessenberg matrices], die untere Bandbreite 1 haben. Beispiel 2.32: H =       9 6 3 2 1 7 8 5 2 1 0 5 7 4 1 0 0 3 6 3 0 0 0 1 5       ist eine Hessenberg-Matrix. ♦ 16Gerhard Hessenberg (16.8.1874 – 16.11.1925), deutscher Mathemati- ker, Professor in Breslau und T¨ubingen. LA-Skript 2-32 11. April 2016 c⃝M.H. Gutknecht Lineare Algebra (2009) Kapitel 2 — Matrizen und Vektoren im R n und C n Wir werden sp¨ater sehen, dass Matrizen lineare Abbildungen dar- stellen bez¨uglich einem oder zwei fest gew¨ahlten Koordinatensyste- men. Durch geeignete Wahl der Koordinaten kann man eine Matrix oft durch eine “einfachere” ersetzen. Hessenberg-, Tridiagonal- und Bidiagonalmatrizen spielen dabei eine wichtige Rolle. In vielen Anwendungen, vor allem beim numerischen L¨osen von par- tiellen Diﬀerentialgleichungen, treten Matrizen auf, die sehr gross sind, aber nur sehr wenige von Null verschiedene Elemente haben, welche nicht auf ein schmales Band um die Diagonale beschr¨ankt sind. Man nennt diese Matrizen d¨unn besetzt [sparse]. In anderen Anwendungen kommen ganz speziell struktutierte Ma- trizen vor. Die folgenden zwei Typen sind weitverbreitet. Die Matrix A = ( aij ) ist eine Toeplitz–Matrix17 [Toeplitz ma- trix], falls aij nur von der Diﬀerenz i − j abh¨angt, und es ist eine Hankel–Matrix18 [Hankel matrix], falls aij nur von der Summe i + j abh¨angt. Beispiele 2.33: Hier sind Beispiele einer Toeplitz–Matrix T und einer Hankel–Matrix H: T =       9 7 5 3 1 8 9 7 5 3 7 8 9 7 5 6 7 8 9 7 5 6 7 8 9       , H =       9 8 7 6 5 8 7 6 5 4 7 6 5 4 3 6 5 4 3 2 5 4 3 2 1       . ♦ Jede Matrix l¨asst sich durch horizontale und/oder vertikale Trenn- linien in Bl¨ocke aufteilen, die selbst Matrizen sind. Wir bezeich- nen die Matrix dann als Blockmatrix [block matrix]. Wir nen- nen die Bl¨ocke von A zum Beispiel Akℓ. Sie brauchen nicht von derselben Gr¨osse zu sein, aber f¨ur festes k sind die (nebeneinan- derliegenden) Bl¨ocke Akℓ alle gleich hoch, und f¨ur festes ℓ sind die (¨ubereinanderliegenden) Bl¨ocke Akℓ alle gleich breit. Beispiel 2.34: A =   A11 A12 A21 A22 A31 A32   =         9 8 7 6 5 7 9 5 7 6 7 6 5 4 3 5 7 3 5 4 3 5 1 3 5 1 2 3 4 5         ist eine Blockmatrix mit der speziellen Eigenschaft, dass alle Bl¨ocke Toeplitz-Struktur haben. ♦ 17Otto Toeplitz (1.8.1881 – 15.2.1940), deutscher Mathematiker, Pro- fessor in Kiel (1913–1927) und Bonn (1927–1933); 1938 Emigration nach Pal¨astina. 18Hermann Hankel (14.2.1839 – 29.8.1873), deutscher Mathematiker, Pro- fessor in Erlangen und T¨ubingen. c⃝M.H. Gutknecht 11. April 2016 LA-Skript 2-33 Kapitel 2 — Matrizen und Vektoren im R n und C n Lineare Algebra (2009) Die Bl¨ocke Ckj des Produktes C von zwei Blockmatrizen A = (Akℓ) und B = (Bℓj) lassen sich nach der Formel Ckj = ∑ ℓ Akℓ Bℓj (2.77) berechnen, vorausgesetzt dass die ℓ-te Blockkolonne von A so breit ist wie die ℓ-te Blockzeile von B hoch ist. Beispiel 2.35: Das Produkt C der Matrix A aus Beispiel 2.34 mit B = ( B11 B12 B13 B21 B22 B23 ) =       1 −1 2 −2 2 −3 −1 1 −2 2 −2 3 4 −4 5 −5 5 −6 −4 4 −5 5 −5 6 4 −4 5 −5 5 −6       kann man nach Formel (2.77) bestimmen. Zum Beispiel wird C11 = ( 9 8 7 9 ) ( 1 −1 −1 1 ) + ( 7 6 5 5 7 6 )   4 −4 −4 4 4 −4   = ( 1 −1 −2 2 ) + ( 24 −24 16 −16 ) = ( 25 −25 14 −14 ) . ♦ Blockmatrizen spielen bei der Implementation der Matrix-Opera- tionen auf Vektor- und Parallelrechnern (und mittlerweise sogar auf PCs) eine wichtige Rolle. Die Matrizen werden derart in Bl¨ocke aufgeteilt, dass die Blockgr¨osse mit der bestehenden Hardware, ins- besondere dem Cache und den Vektorregistern optimal abgestimmt ist. Dadurch l¨asst sich das zeitaufwendige Laden von Daten aus dem Speicher minimieren. F¨ur Fortran und C Programme gibt es spezielle, von den Pro- zessor-Herstellern unterst¨utzte Subroutine-Libraries f¨ur die grund- legenden Vektor- und Matrix-Operationen, die BLAS (oder, aus- geschrieben, Basic Linear Algebra Subroutines). BLAS1 enth¨alt die Vektor-Operationen, BLAS2 die Matrix-Vektor-Operationen und BLAS3 die Matrix-Matrix-Operationen. Sie sind ihrerseits in LA- PACK integriert, einer optimierten Library von Subroutinen f¨ur die numerische lineare Algebra (ohne Ber¨ucksichtigung d¨unn be- setzter Matrizen). LA-Skript 2-34 11. April 2016 c⃝M.H. Gutknecht Lineare Algebra (2009) Kapitel 3 — LR–Zerlegung Kapitel 3 Die LR–Zerlegung 3.1 Die Gauss-Elimination als LR–Zerlegung Wir kommen zur¨uck auf die Gauss-Elimination und werden hier die Reduktion einer Matrix auf Dreiecks- oder Zeilenstufenform als Matrixzerlegung interpretieren. Dazu modiﬁzieren wir unsere Bezei- chungen aus Kapitel 1: Die resultierende Zeilenstufenmatrix wird neu mit R = ( rkj ) bezeichnet und die dazugeh¨orende rechte Sei- te als c = ( c1 . . . cn )T. Diese neuen Bezeichnungen werden in den Formeln ber¨ucksichtigt, sobald die entsprechenden Elemente deﬁnert worden sind, also sobald die Pivotzeile gew¨ahlt worden ist. Wir betrachten zuerst in diesem Abschnitt den Fall eines Systems Ax = b mit regul¨arer quadratischer Matrix, in dem R ja eine regul¨are n × n–Rechtsdreicksmatrix ist: R =      r11 r12 · · · r1n 0 r22 · · · r2n ... . . . . . . ... 0 · · · 0 rnn      . Wir nehmen anf¨anglich an, dass keine Zeilenvertauschungen n¨otig sind. F¨ur den j-ten Schritt lauten die Umbenennungen und die relevanten Formeln (1.10)–(1.11b) in den neuen Bezeichnungen: rji :≡ a(j−1) ji (i = j , . . . , n) , (3.1) cj :≡ b(j−1) j , (3.2) lkj := a(j−1) kj /rjj (k = j + 1, . . . n), (3.3) a(j) ki := a(j−1) ki − lkj rji , (i = j + 1, . . . , n , k = j + 1, . . . n), (3.4) b(j) k := b(j−1) k − lkj cj (k = j + 1, . . . n) . (3.5) Dabei sind die Koeﬃzienten lkj (k > j) ja die Multiplikatoren, mit denen die Pivotzeilen multipliziert werden. Wir setzen zus¨atzlich lkj :≡ 0 (k < j), ljj :≡ 1 , (3.6) so dass L :≡ (lkj) eine n × n–Linksdreiecksmatrix ist: L =      l11 0 · · · 0 l21 l22 . . . ... ... ... . . . 0 ln1 ln2 · · · lnn      . c⃝M.H. Gutknecht 11. April 2016 LA-Skript 3-1 Kapitel 3 — LR–Zerlegung Lineare Algebra (2009) Falls k ≤ i ist, folgt aus (3.4) durch rekursive Anwendung mit j = 1, 2, . . . , k − 1 aki = a(0) ki = lk1 r1i + a(1) ki = . . . = lk1 r1i + lk2 r2i + · · · + lk,k−1 rk−1,i + a(k−1) ki . Ersetzen wir das letzte Glied gem¨ass (3.1) und (3.6) durch lkk rki, so erhalten wir aki = lk1 r1i + · · · + lk,k−1 rk−1,i + lkk rki = k∑ j=1 lkj rji . (3.7) Analog ergibt sich im Falle k > i aus (3.4) mit j = 1, 2, . . . , i − 1 aki = a(0) ki = lk1 r1i + a(1) ki = . . . = lk1 r1i + lk2 r2i + · · · + lk,i−1 ri−1,i + a(i−1) ki . Hier ersetzen wir das letzte Glied gem¨ass (3.3), so dass aki = lk1 r1i + · · · + lk,i−1 ri−1,i + lki rii = i∑ j=1 lkj rji . (3.8) Wegen der Dreiecksgestalt der Matrizen L und R lassen sich die zwei Formeln (3.7) und (3.8) zusammenfassen zu aki = n∑ j=1 lkj rji , das heisst es gilt A = LR . (3.9) Durch die Gauss-Elimination wird also die Koeﬃzientenmatrix A eines linearen Gleichungssystems implizit in ein Produkt einer Links- dreicksmatrix L und einer Rechtsdreicksmatrix R zerlegt. Man nennt deshalb diesen Hauptteil der Gauss-Elimination auch LR–Zer- legung oder, in Anlehnung an die englische Terminologie, LU– Zerlegung [LU decomposition], weil in der englischen Literatur unsere Matrix R oft mit U bezeichnet wird. Die Wirkung der Gauss-Elimination auf die Konstantenkolonne l¨asst sich analog deuten: Statt (3.7) gilt bk = lk1 c1 + · · · + lk,k−1 ck−1 + lkk ck = k∑ j=1 lkj cj , (3.10) also Lc = b . (3.11) LA-Skript 3-2 11. April 2016 c⃝M.H. Gutknecht Lineare Algebra (2009) Kapitel 3 — LR–Zerlegung Schliesslich gilt nach (1.12) f¨ur das R¨uckw¨artseinsetzen [back substitution]: xk := ( ck − n∑ i=k+1 rki xi ) 1 rkk (k = n, n − 1, . . . , 1) . (3.12) Wenn wir dies nach ck auﬂ¨osen, erhalten wir ck = n∑ i=k rki xi (k = n, n − 1, . . . , 1) , (3.13) also, weil R eine Rechtsdreieckmatrix ist, Rx = c . (3.14) So gut man dieses System mit R¨uckw¨artseinsetzen rekursiv l¨osen kann, so gut kann man auch das System (3.11) rekursiv l¨osen durch Vorw¨artseinsetzen [forward substitution]: ck := ( bk − k−1∑ j=1 lkj cj ) 1 lkk (k = 1, 2, . . . , n) , (3.15) wobei hier ja der Bruch wegf¨allt, weil nach Deﬁnition lkk = 1 ist. Mit anderen Worten, wir brauchen die Konstantenkolonne gar nicht im Eliminationsprozess mitzuf¨uhren, wir k¨onnen die rechte Seite c des reduzierten Systems gem¨ass (3.15) bestimmen. In Bezug auf die Zahl der Rechenoperationen und die Gr¨osse der Rundungsfehler ¨andert das allerdings nichts, doch kann die separate Behandlung von Vorteil sein, sowohl aus Gr¨unden der Programmstruktur als auch, weil die rechte Seite b manchmal w¨ahrend der LR–Zerlegung von A noch gar nicht bekannt ist. Beispiel 3.1: In unserem Beispiel 1.1 auf Seite 1-3 ist A =   2 −2 4 −5 6 −7 3 2 1   , b =   6 −7 9   . (3.16) Die Gauss-Elimination liefert, wie man (1.6)–(1.8) entnehmen kann, L =   1 0 0 − 5 2 1 0 3 2 5 1   , R =   2 −2 4 0 1 3 0 0 −20   . (3.17) c =   6 8 −40   , x =   1 2 2   . (3.18) Man ¨uberpr¨uft leicht, dass eﬀektiv A = LR, Lc = b und Rx = c gilt. ♦ c⃝M.H. Gutknecht 11. April 2016 LA-Skript 3-3 Kapitel 3 — LR–Zerlegung Lineare Algebra (2009) Allerdings l¨asst sich ja auch bei regul¨arer quadratischer Matrix A ein Gleichungssystem im allgemeinen nicht ohne Zeilenvertauschun- gen l¨osen. Bekanntlich h¨angen diese vom Verlauf der Rechnung ab: Nullen oder (in gewissem Sinne) relativ kleine Zahlen d¨urfen nicht als Pivot gew¨ahlt werden. Wenn wir die Vertauschungen alle am Anfang der Elimination kennen w¨urden, k¨onnten wir sie aber vorg¨angig ausf¨uhren und dann im so pr¨aparierten Eliminationssche- ma alle Pivot auf der Diagonalen w¨ahlen. Formelm¨assig kann man solche vorg¨angige Vertauschungen ausdr¨ucken durch die Multipika- tion von links mit einer Permutationsmatrix P. Beispiel 3.2: Man h¨atte in Beispiel 3.1 die Zeilen von A etwa wie folgt vertauschen k¨onnen:   0 1 0 0 0 1 1 0 0   ︸ ︷︷ ︸ P   2 −2 4 −5 6 −7 3 2 1   ︸ ︷︷ ︸ A =   −5 6 −7 3 2 1 2 −2 4   ︸ ︷︷ ︸ PA . (3.19) Nat¨urlich m¨usste man dann auch die Komponenten der rechten Seite entsprechend vertauschen:   0 1 0 0 0 1 1 0 0   ︸ ︷︷ ︸ P   6 −7 9   ︸ ︷︷ ︸ b =   −7 9 6   ︸ ︷︷ ︸ Pb . (3.20) Die LR–Zerlegung PA = ̃L ̃R mit diagonaler Pivotwahl erweist sich als m¨oglich und liefert ̃L =    1 0 0 − 3 5 1 0 − 2 5 1 14 1    , ̃R =    −5 6 −7 0 28 5 − 16 5 0 0 10 7    , (3.21) wobei nun also PA = ̃L ̃R gilt. Vor- und R¨uckw¨artseinsetzen ergibt ̃c = ̃L−1(Pb) = ( −7 24 5 20 7 )T , x = ̃R−1̃c = ( 1 2 2 )T . (3.22) Die in der LR–Zerlegung erzeugten Matrizen und auch die reduzierte rechte Seite h¨angen nat¨urlich von der Wahl der Pivotelemente (d.h. von den ausgef¨uhrten Zeilenvertauschungen) ab. Erst das Schlussresultat f¨ur x ist wieder identisch. Wir sehen hier, dass wir durch die neue Pivotwahl das grosse Element −20 in R (vgl. (3.17)) vermeiden konnten. ♦ Zusammenfassend erhalten wir aus den vorhergenden ¨Uberlegungen die folgende Aussage: LA-Skript 3-4 11. April 2016 c⃝M.H. Gutknecht Lineare Algebra (2009) Kapitel 3 — LR–Zerlegung Satz 3.1 Angewandt auf ein quadratisches Gleichungssystem Ax = b mit regul¨arer Matrix A liefert die Gauss-Elimination eine die Zeilenvertauschungen beschreibende Permutationsmatrix P, eine Rechtsdreiecksmatrix R des reduzierten Systems und ei- ne entsprechende rechte Seite c sowie, durch Zusammenziehen der Zeilenmultiplikatoren lkj, eine Linksdreiecksmatrix L, wobei alles quadratische Matrizen gleicher Ordnung sind und die Beziehungen PA = LR , Lc = Pb , Rx = c . (3.23) gelten. Ist die LR–Zerlegung PA = LR einmal berechnet, so l¨asst sich irgend ein System mit Koeﬃzientenmatrix A l¨osen durch Auﬂ¨osen von Lc = Pb nach c (Vorw¨artseinsetzen) und Auﬂ¨osen von Rx = c nach x (R¨uckw¨artseinsetzen). Bemerkung: Dass die Beziehungen (3.23) die richtige L¨osung x liefern, sieht man auch daraus, dass Ax = P−1PAx = P−1LRx = P−1Lc = P−1Pb = b . ▼ Es gibt viele Varianten der LR–Zerlegung. Wir haben in jedem Eli- minationsschritt eine neue Kolonne von L, eine neue Zeile von R und eine neue Komponente von c bestimmt und zudem das ver- kleinerte Restgleichungssystem nachgef¨uhrt. Man kann aber auch L und R direkt aus A berechnen, ohne dass man Restgleichungssy- steme aufstellt. Zum Beispiel lassen sich beide Matrizen L und R direkt kolonnenweise (oder zeilenweise) berechnen. In der Tat be- kommen wir, wenn wir wieder von Zeilenvertauschungen absehen, aus (3.7)–(3.8) sofort die nachfolgenden Rekursionen: Algorithmus 3.1 (Zeilenweise, direkte LR–Zerlegung, “regul¨arer Fall”) Zur LR–Zerlegung einer regul¨aren Matrix A berechne man f¨ur i = 1, . . . , n, mit lii := 1, lik := ( aik − k−1∑ j=1 lij rjk ) 1 rkk (k = 1, . . . , i − 1), (3.24a) rik := ( aik − i−1∑ j=1 lij rjk ) 1 lii (k = i, . . . , n). (3.24b) Zeilenvertauschungen k¨onnen dabei wie ¨ublich durch Umnumme- rierung der Elemente von A ber¨ucksichtigt werden. Will man statt den Diagonalelementen von L jene von R auf 1 normieren, so wendet man (3.24a) auch f¨ur k = i an, daf¨ur (3.24b) erst ab k = i + 1. Beispiel 3.3: Um etwa die Matrix A aus (3.16) in Beispiel 3.1 zu zerlegen in die Faktoren L und R in (3.17) kann man nach (3.24a)– (3.24b) zeilenweise wie folgt vorgehen (wir w¨ahlen dabei lii := 1): c⃝M.H. Gutknecht 11. April 2016 LA-Skript 3-5 Kapitel 3 — LR–Zerlegung Lineare Algebra (2009) r11 := a11 = 2 , r12 := a12 = −2 , r13 := a13 = 4 , l21 := a21 1 r11 = −5 · 1 2 = − 5 2 , r22 := a22 − l21 r12 = 6 − (− 5 2 ) · (−2) = 1 , r23 := a23 − l21 r13 = −7 − (− 5 2 ) · 4 = 3 , l31 := a31 1 r11 = 3 · 1 2 = 3 2 , l32 := (a32 − l31 r12) 1 r22 = ( 2 − 3 2 · (−2)) 1 1 = 5 , r33 := a33 − l31 r13 − l32 r23 = 1 − 3 2 · 4 − 5 · 3 = −20 . Genau so gut kann man die Matrizen L und R kolonnenweise bestimmen: man f¨uhrt dieselben Rechnungen aus, aber in der Reihenfolge r11, l21, l31, r12, r22, l32, r13, r23, r33. ♦ Eine weitere Option ist, die Pivotelemente in eine Diagonalmatrix D einzubringen und daf¨ur sowohl L als auch R durch Diagonalele- mente 1 zu normieren. Das erfordert nur eine kleine Modiﬁkation der Formeln (3.24a)–(3.24b). Formal bedeutet es, dass D :≡ diag (r11, r22, . . . , rnn) , R1 :≡ D −1R (3.25) und PA = LDR1 . (3.26) Dies ist die LDR–Zerlegung oder LDU–Zerlegung [LDU de- composition] von A. Wir werden in Abschnitt 3.4 auf sie zur¨uckkommen. In grossen Problemen, die auf der Diskretisation von partiellen Dif- ferentialgleichungen beruhen, spielen Versionen der LR–Zerlegung eine wichtige Rolle, die angepasst sind an die Struktur d¨unn be- setzter Matrizen, die pro Zeile nur relativ wenige Elemente haben, die nicht null sind. Das grunds¨atzliche Vorgehen beim L¨osen von Ax = b mittels LR– Zerlegung sei noch einmal zusammengefasst: Algorithmus 3.2 (Gauss-Elimination durch LR- Zerlegung, Vor- und R¨uckw¨artseinsetzen) Zum L¨osen eines regul¨aren Gleichungssystems Ax = b kann man, als Alternative zur Gauss-Elimination von Algorithmus 1.1, wie folgt vorgehen: 1. LR-Zerlegung von A zum Beispiel gem¨ass Algorithmus 1.1 (aber ohne rechte Seite b) oder gem¨ass Algorithmus 3.1; sie erzeugt die Matrizen L und R sowie die durch P darstellbare Permutation der Zeilen. 2. Vorw¨artseinsetzen: Lc = Pb auﬂ¨osen nach c. 3. R¨uckw¨artseinsetzen: Rx = c auﬂ¨osen nach x. LA-Skript 3-6 11. April 2016 c⃝M.H. Gutknecht Lineare Algebra (2009) Kapitel 3 — LR–Zerlegung Speicherbedarf und Rechenaufwand Wie man unseren Beispielen aus Kapitel 1 entnimmt, siehe et- wa Beispiel 1.9 auf Seite 1-5, lassen sich die im Laufe einer LR– Zerlegung erzeugten und noch gebrauchten Daten jederzeit in einem Datenfeld der Gr¨osse n × n unterbringen. Die nicht-null Elemente von L (ausser die auf 1 normierten Diagonalelemente) kann man n¨amlich genau dort abspeichern, wo Nullen erzwungen werden. Bei Zeilenvertauschungen braucht man eine zus¨atzliche Kolonne, um zum Beispiel die urspr¨unglichen Zeilennummern abzuspeichern. Der Rechenaufwand f¨ur die LR–Zerlegung und das Vor- und R¨uck- w¨artseinsetzen f¨ur ℓ verschiedene rechte Seiten ist genau gleich wie f¨ur die Gauss-Elimination gem¨ass Algorithmus 1.1 mit ℓ Konstan- tenkolonnen. Es kommt nicht darauf an, ob man Restgleichungssy- steme nachf¨uhrt oder die kompakten Formeln aus Algorithmus 3.1 benutzt. Es ist allerdings von Vorteil, wenn man die reziproken Pivotelemente r−1 kk berechnet und abspeichert. Es sind dann total nur n Divisionen n¨otig. (Divisionen brauchen auf vielen Computern mehr Zeit als Multiplikationen.) Im j-ten Schritt von Algorithmus 1.1 braucht man dann 1 Division f¨ur (a(j−1) jj )−1 [Formel (1.10)] n − j Multiplikationen f¨ur die lkj [Formel (1.10)] (n − j)2 Add. & Mult. f¨ur die a(j) ki [Formel (1.11a)] ℓ(n − j) Add. & Mult. f¨ur die b(j) k [Formel (1.11b)] und f¨ur den k-ten Teilschritt des ℓ-maligen R¨uckw¨artseinsetzens ℓ(n − k) Additionen f¨ur xk [Formel (1.12)] ℓ(n − k + 1) Multiplikationen f¨ur xk [Formel (1.12)] Ersetzen wir in der letzten Formel den Index k durch j, so k¨onnen wir alle Beitr¨age f¨ur festes j addieren: 1 Division (n − j)2 + (2ℓ + 1)(n − j) + ℓ Multiplikationen (n − j)2 + 2ℓ(n − j) Additionen Diese Zahlen sind zu summieren von j = 1 bis j = n (wobei es im letzten Schritt bloss eine Division im Hinblick auf das R¨uckw¨arts- einsetzen gibt). Wenn wir k := n − j setzen, so erhalten wir zum Beispiel f¨ur die Multiplikationen n−1∑ k=0 (k2 + k(2ℓ + 1) + ℓ) = n(n − 1)(2n − 1) 6 + n(n − 1)(2ℓ + 1) 2 + ℓn = 1 3n3 + ℓn2 + O(n) , c⃝M.H. Gutknecht 11. April 2016 LA-Skript 3-7 Kapitel 3 — LR–Zerlegung Lineare Algebra (2009) wobei im Term O(n) alle Beitr¨age untergebracht sind, die linear in n oder konstant sind. Der ℓn–Term f¨allt hier weg; bei der Addition ist er negativ. F¨ur ℓ = 0 erhalten wir den Rechenaufwand f¨ur die reine LR–Zerlegung. Wir k¨onnen das so zusammenfassen: Satz 3.2 Der Rechenaufwand f¨ur die LR–Zerlegung einer voll besetzten, regul¨aren n × n-Matrix betr¨agt n Divisionen (inklusi- ve der letzten, die man erst beim R¨uckw¨artseinsetzen braucht), 1 3 n3 + O(n) Multiplikationen und 1 3 n3 − 1 2n2 + O(n) Additionen. F¨ur Vor- und R¨uckw¨artseinsetzen braucht man zus¨atzlich pro rech- te Seite je n2 − n Additionen und n2 Multiplikationen. Das L¨osen eines Systems AX = B mit n rechten Seiten erfordert dementsprechend n Divisionen, 4 3 n3 + O(n) Multiplikationen und 4 3 n3 − 3 2n2 + O(n) Additionen. Der zweite Teil des Satzes gibt im wesentlichen auch den Aufwand f¨ur die Berechnung der Inversen an, wobei man allerdings in diesem Falle (wo B = I ist), noch beim Vorw¨artseinsetzen Operationen sparen kann, weil die rechten Seiten viele Nullen haben. Dabei ist der f¨uhrende Term von 4 3n3 um einen Viertel auf je n3 Additionen und Multiplikationen reduzierbar. Pivotstrategien Es ist wichtig, daran zu erinnern, dass selbst bei quadratischen Matrizen der Gauss-Algorithmus und die dazu ¨aquivalente LR– Zerlegung in einer Arithmetik mit endlicher Genauigkeit (wo Run- dungsfehler auftreten) nur dann zuverl¨assige Resultate liefert, wenn man die Pivots geeignet w¨ahlt und wenn die Matrix nicht beinahe singul¨ar ist (was immerhin gleichzeitig ¨uberpr¨uft werden kann). Die meistens benutzte Regel (Pivotstrategie [pivot strategy]) be- sagt, dass man im jten Eliminationsschritt als Pivot a(j−1) pj jenes Element w¨ahlen soll, das unter allen m − j + 1 Elementen der er- sten Kolonne der Restgleichungsmatrix das betragsm¨assig gr¨osste ist: |a(j−1) pj | = max j≤k≤m |a(j−1) kj | . (3.27) Diese Regel nennt man Kolonnenmaximumstrategie oder par- tielles Pivotieren [partial pivoting]. Damit l¨asst sich fast immer vermeiden, dass in L und R Elemente entstehen, die viel gr¨osser sind als jene von A und zu grossen Rundungsfehlern f¨uhren k¨onnen. Allerdings setzt dies voraus, dass die Zeilen von A von der gleichen Gr¨ossenordnung sind. Noch sicherer w¨are es, auch Kolonnenvertauschungen zuzulassen und in der ganzen Restgleichungsmatrix nach dem gr¨ossten oder sogar dem relativ gr¨ossten Element (im Vergleich zu den anderen derselben Zeile) zu suchen. Aber dieses vollst¨andige Pivotieren [complete pivoting] ist zu aufwendig und wird kaum je angewandt. LA-Skript 3-8 11. April 2016 c⃝M.H. Gutknecht Lineare Algebra (2009) Kapitel 3 — LR–Zerlegung 3.2 Die Gauss-Elimination als LR–Zerlegung: der allgemeine Fall Im allgemeinen Falle der Reduktion irgend einer m × n–Matrix auf Zeilenstufenform ist R von der Form in (1.28), das heisst R =            0 · · · r1,n1 · · · ∗ · · · · · · ∗ ∗ · · · ∗ 0 · · · 0 · · · r2,n2 · · · · · · ∗ ∗ · · · ∗ ... ... ... . . . ... ... ... 0 · · · 0 · · · 0 · · · rr,nr ∗ · · · ∗ 0 · · · 0 · · · 0 · · · · · · 0 0 · · · 0 ... ... ... ... ... ... 0 · · · 0 · · · 0 · · · · · · 0 0 · · · 0            , (3.28) wobei die eingezeichneten Pivotelemente r1,n1, r2,n2, . . . , rr,nr nicht null sind und wir jetzt auch noch die m¨oglichen Null-Kolonnen am linken Rand eingetragen haben. Die Formeln (3.1)–(3.5) f¨ur den j-ten Eliminationsschritt sind nun zu ersetzen durch rji :≡ a(j−1) ji (i = nj + 1, . . . , n) , (3.29) cj :≡ b(j−1) j , (3.30) lkj := a(j−1) k,nj /rj,nj (k = j + 1, . . . n), (3.31) a(j) ki := a(j−1) ki − lkj rji , (i = nj + 1, . . . , n , k = j + 1, . . . n), (3.32) b(j) k := b(j−1) k − lkj cj (k = j + 1, . . . n) . (3.33) In (3.29), (3.30) und (3.33) gibt es keine ¨Anderung, in den anderen Formeln ist zum Teil der Kolonnenindex j durch nj ersetzt worden. Die Multiplikatoren lkj passen in die m × m–Linksdreiecksmatrix L =                1 0 · · · 0 0 · · · 0 0 l21 1 . . . ... ... ... ... ... ... . . . 0 ... ... ... lr1 lr2 · · · 1 0 · · · 0 0 lr+1,1 lr+1,2 . . . lr+1,r 1 . . . ... ... ... ... ... 0 . . . 0 ... ... ... ... ... . . . 1 0 lm1 lm2 · · · lmr 0 · · · 0 1                (3.34) mit lkj :≡ 0 (1 ≤ k < j ≤ m) , (3.35) lkj :≡ 0 (r < j < k ≤ m) , (3.36) ljj :≡ 1 (1 ≤ k < j ≤ m) . (3.37) c⃝M.H. Gutknecht 11. April 2016 LA-Skript 3-9 Kapitel 3 — LR–Zerlegung Lineare Algebra (2009) Schliesslich wird die Formel (3.12) f¨ur das R¨uckw¨artseinsetzen er- setzt durch (1.27), das heisst xnk := ( b(k−1) k − n∑ i=nk+1 a(k−1) ki xi ) 1 a(k−1) k,nk , (3.38) Nun ist es nicht mehr schwierig nachzupr¨ufen, dass mit diesen De- ﬁnitionen Satz 3.1 mit geringen Aenderungen weiterhin gilt: Satz 3.3 Angewandt auf ein beliebiges m × n–System Ax = b liefert die Gauss-Elimination eine die Zeilenvertauschungen be- schreibende m × m–Permutationsmatrix P, eine m × n Zeilen- stufenmatrix R des reduzierten Systems und eine entsprechende rechte Seite c ∈ Em sowie, durch Zusammenziehen der Zeilenmul- tiplikatoren lkj, eine regul¨are m × m Linksdreiecksmatrix L, wobei gilt PA = LR , Lc = Pb , Rx = c . (3.39) Ist die LR–Zerlegung PA = LR einmal berechnet, so l¨asst sich irgend ein System mit Koeﬃzientenmatrix A l¨osen durch Auﬂ¨osen von Lc = Pb nach c (Vorw¨artseinsetzen) und Auﬂ¨osen von Rx = c nach x (R¨uckw¨artseinsetzen). Beim letzteren sind allf¨allige freie Variablen (xk mit Indizes k ̸= nj(∀j)) frei w¨ahlbar. Im Fall Rang A = r < m l¨asst sich die LR–Zerlegung von Satz 3.3 vereinfachen: da alle Elemente der letzten m − r > 0 Zeilen von R null sind und im Produkt LR auf die Elemente in den letzten m − r Kolonnen von L treﬀen, kann man sowohl diese Kolonnen als auch die Nullzeilen von R wegstreichen. Dabei wird R zu einer r × n–Matrix ̃R und L zu einer m × r–Matrix ̃L, wobei gilt ̃L ̃R = L R = P A . Das liefert das folgende Korollar von Satz 3.3: Korollar 3.4 Zu jeder m × n–Matrix A mit Rang r gibt es eine m × r–Matrix ̃L mit Einsen in der Diagonalen, eine r × n–Matrix ̃R in Zeilenstufenform und eine m × m–Permutationsmatrix P, so dass ̃L ̃R = P A . (3.40) Auch mit dieser Zerlegung ist Ax = b nat¨urlich ¨aquivalent zu ̃L ̃R x = P b und kann deshalb durch Vor- und R¨uckw¨artseinsetzen gel¨ost werden. Nun ist zuerst das m × r–System ̃L ̃c = P b zu l¨osen. Weil r < m ist, ist das aber nicht immer m¨oglich. Die ersten r Glei- chungen ergeben ̃c eindeutig durch Vorw¨artseinsetzen. Die restli- chen m − r Gleichungen sind die Vertr¨aglichkeitsbedingungen. Nun treten die Vertr¨aglichkeitsbedingungen also bereits beim Vorw¨art- seinsetzen auf. Falls sie erf¨ullt sind, k¨onnen alle L¨osungen aus dem r ×n–System ̃R x = ̃c durch R¨uckw¨artseinsetzen bestimmt werden, wobei hier wieder die freie Variablen w¨ahlbar sind, wenn n > r ist. LA-Skript 3-10 11. April 2016 c⃝M.H. Gutknecht Lineare Algebra (2009) Kapitel 3 — LR–Zerlegung Beispiel 3.4: In unserem Beispiel 1.4 auf Seite 1-11 ist m = 7 und n = 9. Gegeben sind A =           1 0 5 0 4 0 0 1 0 0 0 0 5 0 24 16 8 6 1 5 9 3 6 1 0 1 0 1 10 13 11 8 6 3 1 2 0 5 4 18 2 18 12 2 7 1 10 13 21 8 24 16 5 8 0 5 4 13 2 24 17 6 7           (3.41a) und b = ( 1 12 3 8 13 19 16 )T . (3.41b) Wir haben bei der L¨osung im zweiten Schritt die Zeilen 2 und 3, im dritten Schritt die Zeilen 3 und 4, und im vierten Schritt die Zeilen 4 und 5 vertauscht, wodurch die Permutation 2 → 5, 3 → 2, 4 → 3, 5 → 4 deﬁniert wird, die durch die folgende Permutationsmatrix P beschrieben wird: P =           1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1           . (3.42) Unserer L¨osung entnimmt man weiter, dass r = 5 ist, und, wenn man die Auswirkung der Zeilenvertauschungen auf bereits berechnete Kolonnen von L ber¨ucksichtigt, dass L =           1 0 0 0 0 0 0 1 1 0 0 0 0 0 1 2 1 0 0 0 0 0 1 3 1 0 0 0 0 0 1 4 1 0 0 1 2 3 2 1 1 0 0 1 2 3 2 0 1           , (3.43a) R =           1 0 5 0 4 0 0 1 0 0 5 4 3 2 1 0 0 0 0 0 0 5 0 4 3 0 2 0 0 0 0 0 5 3 2 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0           , (3.43b) c = ( 1 2 3 2 1 0 0 )T . (3.43c) ♦ Bemerkung: Der hier behandelte allgemeine Gauss-Algorithmus, mit dem man jede m×n–Matrix auf Zeilenstufenform bringen kann, ist prim¨ar ein theoretischer Algorithmus f¨ur exakte Arithmetik. Man muss ja entscheiden, ob eine potentielle Pivotkolonne null ist und ¨ubersprungen werden soll. Mit anderen Worten, der Rang einer Matrix ist numerisch nicht immer exakt deﬁniert. ▼ c⃝M.H. Gutknecht 11. April 2016 LA-Skript 3-11 Kapitel 3 — LR–Zerlegung Lineare Algebra (2009) 3.3 Block–LR–Zerlegung und LR–Updating Die Gauss-Elimination und ihre Interpretation als LR–Zerlegung ist auch mit Blockmatrizen machbar, deren Bl¨ocke als unteilbare Einheiten aufgefasst werden. Allerdings muss man die den Pivotele- menten entsprechenden Bl¨ocke invertieren k¨onnen, d.h. sie m¨ussen regul¨ar sein, also insbesondere quadratisch. Wir wollen nur den Fall einer regul¨aren 2 × 2 Blockmatrix betrachten, ̃A :≡ ( A B C D ) , (3.44) wobei A auch regul¨ar sein muss (und damit D sicher quadratisch). In Anlehnung an die Gauss-Elimination subtrahieren wir das CA−1– fache der ersten Blockzeile von der zweiten, was ( A B O S ) mit S :≡ D − CA−1B (3.45) ergibt. Der Block S heisst Schur–Komplement1 [Schur comple- ment]. Die Operation, die (3.44) in (3.45) ¨uberf¨uhrt, kann man als Matrixmultiplikation darstellen: ( I O −CA−1 I ) ( A B C D ) = ( A B O S ) . (3.46) Hier ist die erste 2×2 Blockmatrix ganz einfach zu invertieren; man muss bloss das Vorzeichen des (2, 1)–Blockes ¨andern: ( I O −CA−1 I )−1 = ( I O CA−1 I ) . (3.47) Damit ergibt sich aus (3.46) die Block–LR–Zerlegung [Block LR decomposition] ( A B C D ) ︸ ︷︷ ︸ ≡: ̃A = ( I O CA−1 I ) ︸ ︷︷ ︸ ≡: ̃L ( A B O S ) ︸ ︷︷ ︸ ≡: ̃R . (3.48) Wir k¨onnen diese Zerlegung noch insofern modiﬁzieren, als wir den (1, 1)–Block A auf die zwei Faktoren aufteilen k¨onnen. Ist zum Beispiel eine LR–Zelegung A = LR dieses Blockes m¨oglich und bekannt, so k¨onnen wir wegen A−1 = R−1L−1 statt (3.48) schreiben ( A B C D ) ︸ ︷︷ ︸ ≡: ̃A = ( L O CR−1 I ) ︸ ︷︷ ︸ ≡: ̃L ( R L−1B O S ) ︸ ︷︷ ︸ ≡: ̃R , (3.49) 1Issai Schur (10.1.1875 – 10.1.1941), ab 1916 Professor in Berlin, 1938 Emigration nach Pal¨astina. Seine theoretischen Arbeiten enthalten auch wich- tige Algorithmen. LA-Skript 3-12 11. April 2016 c⃝M.H. Gutknecht Lineare Algebra (2009) Kapitel 3 — LR–Zerlegung mit nun neuen Matrizen ̃L und ̃R. Die Formeln (3.48) und (3.49) haben interessante Spezialf¨alle und viele Anwendungen. Insbesondere kann man auch theoretische Er- gebnisse aus ihnen ableiten. Wir fangen mit einem solchen an. Lemma 3.5 In der 2 × 2 Blockmatrix ̃A ∈ En×n aus (3.44) sei der (1, 1)–Block A regul¨ar. Dann ist ̃A genau dann regul¨ar, wenn das Schur–Komplement S aus (3.45) regul¨ar ist. Beweis: Weil A regul¨ar ist, l¨asst sich der Schritt von (3.44) zu (3.45) durchf¨uhren, und es gilt (3.46), wobei dort die erste Matrix regul¨ar ist, denn (3.47) ist ja eine Formel f¨ur die Inverse. Weil diese Matrix regul¨ar ist, l¨asst sich f¨ur jede rechte Seite das System ( I O −CA−1 I ) ̃y = ( b c ) (3.50) l¨osen, wobei wir hier annehmen, dass diese entsprechend den Block- gr¨ossen in der Matrix aufgeteilt sei. Nun ist auch ̃A regul¨ar genau dann, wenn das Gleichungssysstem ̃Ãx = ̃y f¨ur jede rechte Seite ̃y ∈ En eine L¨osung ̃x hat, also insbesondere f¨ur jede L¨osung ̃y von (3.50). Diese ist dann eine L¨osung von ( A B O S ) ︸ ︷︷ ︸ = ̃R ̃x = ( I O −CA−1 I ) ︸ ︷︷ ︸ = ̃L−1 ( A B C D ) ̃x ︸ ︷︷ ︸ = ̃y = ( b c ) . Teilen wir nun ̃x ≡: ( xT uT )T passend in zwei Bl¨ocke auf, so folgt aus ( A B O S ) ( x u ) = ( b c ) , (3.51) dass Su = c (3.52) ist. Wir k¨onnen auf diese Weise also zu jedem c eine L¨osung dieser Gleichung konstruieren, wobei wir in (3.50) sogar b = o w¨ahlen d¨urften. Es folgt, dass S regul¨ar ist. Umgekehrt k¨onnen wir, falls S regul¨ar ist, zu jedem c eine L¨osung u von (3.52) gewinnen, womit sich mittels der ersten Blockzeile von (3.51), Ax = b − Bu , f¨ur beliebiges b ein regul¨ares System f¨ur x ergibt. Mit anderen Worten, wir k¨onnen zu jedem ̃b :≡ ( bT cT )T eine L¨osung ̃x :≡ ( xT uT )T von ̃Ãx = ̃b ﬁnden, was bedeutet, dass ̃A regul¨ar ist. (Eﬀektiv l¨osen wir hier ̃Ãx = ̃b durch Block-R¨uckw¨artseinsetzen.) Wir werden sp¨ater in Kapitel 8 ein Lemma beweisen, aus dem man die obige Aussage von Lemma 3.5 sofort ablesen kann. c⃝M.H. Gutknecht 11. April 2016 LA-Skript 3-13 Kapitel 3 — LR–Zerlegung Lineare Algebra (2009) Zwei Spezialf¨alle obiger Block–LR–Zerlegung sind besonders inter- essant: wenn entweder der (1, 1)–Block oder der (2, 2)–Block ein Skalar ist. Im ersten Fall hat man aus (3.48) oder (3.49) ( α b T c D ) ︸ ︷︷ ︸ = ̃A = ( 1 oT cα−1 I ) ︸ ︷︷ ︸ = ̃L ( α b T o S ) ︸ ︷︷ ︸ = ̃R (3.53) mit dem Schur–Komplement S = D − cα−1b T , (3.54) das eine Rang-1–Modiﬁkation von D ist. Diese zwei Formeln beschreiben oﬀenbar gerade den ersten Schritt der (gew¨ohnlichen) LR–Zerlegung, und man sieht, dass das Schur– Komplement gerade die Koeﬃzientenmatrix des ersten Restglei- chungssystems ist. Analog gilt im zweiten Fall, in dem wir uns auf (3.49) st¨utzen wol- len, also annehmen, das der (1, 1)–Block A bereits in A = LR zerlegt sei: ( A b c T δ ) ︸ ︷︷ ︸ = ̃A = ( L o c TR−1 1 ) ︸ ︷︷ ︸ = ̃L ( R L−1b oT σ ) ︸ ︷︷ ︸ = ̃R (3.55) mit dem skalaren Schur–Komplement σ = δ − c TA−1b = δ − c TR−1L−1b . (3.56) Wenn wir diese zwei Formeln rekursiv anwenden, er¨oﬀnen sie uns eine neue, rekursive Variante der LR–Zerlegung, die allerdings kei- ne Zeilenvertauschungen zul¨asst. Wir f¨uhren dazu noch geeignete Bezeichnungen ein. Definition: Die einer m × n Matrix A = ( aij ) 1≤i≤m, 1≤j≤n zugeordneten min{m, n} quadratischen Teilmatrizen Ak = ( aij ) 1≤i≤k, 1≤j≤k (k = 1, . . . , min{m, n}) (3.57) heissen f¨uhrende Hauptuntermatrizen [leading principal sub- matrices] von A. ▲ Unter Verwendung der Bezeichnungen Ak = LkRk , Ak+1 = ( Ak bk c T k ak+1,k+1 ) (3.58) k¨onnen wir Formeln (3.55)–(3.56) umschreiben in ( Ak bk c T k ak+1,k+1 ) ︸ ︷︷ ︸ =: Ak+1 = ( Lk o c T k R−1 k 1 ) ︸ ︷︷ ︸ =: Lk+1 ( Rk L−1 k bk oT σk+1 ) ︸ ︷︷ ︸ =: Rk+1 , (3.59) σk+1 := ak+1,k+1 − c T k R−1 k L−1 k bk . (3.60) LA-Skript 3-14 11. April 2016 c⃝M.H. Gutknecht Lineare Algebra (2009) Kapitel 3 — LR–Zerlegung Dabei braucht man f¨ur die Berechnung von Rk+1, σk+1 und Lk+1 gem¨ass (3.59)–(3.60) die Inversen R−1 k und L−1 k nicht, denn man kann ja L−1 k bk und (RT k )−1ck durch Vorw¨artseinsetzen berechnen. Die Rekursionen f¨ur Rk+1, σk+1 und Lk+1 brechen zusammen, so- bald σk+1 null wird, denn dann existiert R−1 k+1 nicht. In der Tat ist σk+1 das Pivotelement im (k + 1)ten Eliminationsschritt. Nachfol- gend ein Kriterium daf¨ur, dass es nicht zum Abbruch kommt. Satz 3.6 Eine m × n Matrix A vom Rang r l¨asst sich genau dann ohne Zeilenvertauschungen LR–zerlegen, wenn die r f¨uhrenden Hauptuntermatrizen Ak (k = 1, . . . , r) regul¨ar sind. Beweis: F¨ur Matrizen vom Rang 1 ist der Satz richtig, denn eine LR–Zerlegung existiert genau dann, wenn a11 nicht null ist. Nach einem Schritt ist die Zerlegung fertig. Allgemein: L¨asst sich A ohne Zeilenvertauschungen LR–zerlegen, so gilt A = LR, wobei r11, . . . , rrr ̸= 0. Dabei ist Ak = LkRk (k = 1, . . . , r), wobei die Faktoren und damit auch A1, . . . , Ar regul¨ar sind. F¨ur die Umkehrung verwenden wir Induktion nach r, wobei die Ver- ankerung (r = 1) schon erledigt ist. Wir nehmen also an, dass wenn Rang (A) ≡ r = ℓ ist und A1, . . . , Ar regul¨ar sind, die LR–Zerlegung A = LR existiert und wollen zeigen, dass das auch f¨ur r = ℓ + 1 gilt. Es gelte also Rang (A) ≡ r = ℓ+1, und es seien A1, . . . , Ar regul¨ar. Weil A1, . . . , Ar−1 regul¨ar sind, l¨asst sich Ar−1 ohne Zeilenvertauschungen LR–zerlegen, d.h. Ar−1 = Lr−1Rr−1, wobei Lr−1 und Rr−1 auch die f¨uhrenden Hauptuntermatrizen der Faktoren L und R sind, die zu einer allf¨alligen LR–Zerlegung von A geh¨oren. Da auch Ar regul¨ar ist, folgt aus Lemma 3.5, dass das Schurkomplement von Ar−1 bez¨uglich Ar nicht null ist: σ = rrr ̸= 0. Damit sind r nichtverschwindende Pivots gefunden; die allenfalls verbleibenden m − r Zeilen von R (in der Zeilenstufenform von A) enthalten lauter Nullen, und es gilt A = LR. Zusammenfassend erhalten wir: Algorithmus 3.3 (LR–Zerlegung durch Updating) Zur LR–Zerlegung einer regul¨aren Matrix A, deren n − 1 f¨uhrende Hauptuntermatrizen Ak (k = 1, . . . , n − 1) regul¨ar sind, setze man zun¨achst L1 := ( 1 ) , R1 := ( a11 ). F¨ur k = 1, . . . , n − 1 teile man dann Ak+1 gem¨ass (3.58) in vier Bl¨ocke auf und berechne σk+1 := ak+1,k+1 − c T k R−1 k L−1 k bk , (3.61a) Lk+1 := ( Lk o c T k R−1 k 1 ) , (3.61b) Rk+1 := ( Rk L−1 k bk oT σk+1 ) . (3.61c) Dabei erh¨alt man L−1 k bk und (RT k )−1ck durch Vorw¨artseinsetzen. Der Algorithmus bricht nur ab, wenn entgegen der Voraussetzung eine der Hauptuntermatrizen singul¨ar ist, und zwar ist σk = 0 (und damit Rk singul¨ar) genau dann, wenn Ak singul¨ar ist. c⃝M.H. Gutknecht 11. April 2016 LA-Skript 3-15 Kapitel 3 — LR–Zerlegung Lineare Algebra (2009) 3.4 Die Cholesky-Zerlegung In Abschnitt 3.1 haben wir darauf hingewiesen, siehe (3.25)–(3.26), dass man die LR–Zerlegung PA = LR (mit Zeilenvertauschungen) jederzeit durch eine ¨aquivalente LDR–Zerlegung PA = LDR1 er- setzen kann, wo D die Diagonale von R und R1 :≡ D−1R ist. Ist A symmetrisch oder Hermitesch und die Zerlegung ohne Zeilen- vertauschen m¨oglich, so ist dabei P = I und automatisch R1 = LT bzw. R1 = LH, so dass gilt: A = LDLT bzw. A = LDLH . (3.62) Sind zudem die Diagonalelemente dii :≡ rii von D positiv, so kann man ihre Quadratwurzeln berechnen und jene von D deﬁnieren als D 1 2 :≡ diag (√ r11, √r22, . . . , √rnn) . (3.63) Setzt man nun ̃R :≡ D 1 2 LT bzw. ̃R :≡ D 1 2 LH , (3.64) erh¨alt man die Cholesky-Zerlegung2 [Cholesky decomposition] A = ̃RT ̃R bzw. A = ̃RH ̃R . (3.65) Bei der linken Version setzt man dabei voraus, dass die Matrizen reell sind, aus einem Grund, auf den wir gleich kommen werden. Nun haben wir in Abschnitt 3.3 gesehen, dass man die LR–Zerlegung (theoretisch) ohne Zeilenvertauschungen ausf¨uhren kann genau dann, wenn die f¨uhrenden Hauptuntermatrizen Ak (k = 1, ..., n − 1) re- gul¨ar sind. Aber wann k¨onnen wir das garantieren? Und wann sind dann zudem die Diagonalelemente von D positiv? Es gibt eine wichtige Klasse reell symmetrischer oder Hermitescher Matrizen, wo das gew¨ahrleistet ist. Definition: Eine reell symmetrische n × n Matrix A heisst po- sitiv deﬁnit [positive deﬁnite] oder, kurz, eine spd Matrix, falls xTAx > 0 f¨ur alle x ∈ R n , x ̸= o . (3.66) Eine Hermitesche n × n Matrix A heisst positiv deﬁnit [positive deﬁnite] oder, kurz, eine Hpd Matrix, falls xHAx > 0 f¨ur alle x ∈ C n , x ̸= o . (3.67) Gilt (3.66) bzw. (3.67) nur mit dem ≥–Zeichen, so heisst die Matrix positiv semideﬁnit [positive semideﬁnite]. ▲ 2Andr´e Louis Cholesky (15.10.1875 – 31.10.1918), franz¨osischer Oﬃzier, der unter anderem Vermessungen in Algerien und Tunesien durchf¨uhrte und im ersten Weltkrieg als Regimentskommandant ﬁel; Ritter der Ehrenlegion. LA-Skript 3-16 11. April 2016 c⃝M.H. Gutknecht Lineare Algebra (2009) Kapitel 3 — LR–Zerlegung Lemma 3.7 Eine reell symmetrische oder Hermitesche Matrix, die positiv deﬁnit ist, ist regul¨ar. Beweis: W¨are eine spd oder Hpd Matrix A singul¨ar, g¨abe es nach Korollar 1.7 nichttriviale L¨osungen x des homogenen Systems Ax = o. F¨ur diese x w¨are aber xTAx = 0 bzw. xHAx = 0 im Widerspruch zu (3.66) und (3.67). Im folgenden betrachten wir den Hermiteschen Fall; den reell sym- metrischen erh¨alt man daraus als Spezialfall: im wesentlichen ist H durch T zu ersetzen. Es sei also A eine Hpd Matrix der Ordnung n. W¨ahlt man x ∈ Cn speziell von der Form x = ( u o ) mit u ∈ C k , u ̸= o , so sieht man, dass auch jede Hauptuntermatrix Ak positiv deﬁnit ist: 0 < xHAx = u HAku f¨ur alle u ∈ C k , u ̸= o . Insbesondere ist nach Lemma 3.7 Ak f¨ur k = 1, . . . , n regul¨ar. Dies bedeutet nach Satz 3.6, dass man A ohne Zeilen zu vertauschen LR–zerlegen kann: A = LR = LDLH, wobei L und D regul¨ar sind. Damit gilt f¨ur beliebiges x ∈ Cn, x ̸= o, dass 0 < xHAx = xHLDLHx = yHDy . (3.68) falls y :≡ LHx. Weil LT regul¨ar ist, gibt es zu jedem y ̸= o ein x ̸= o, das heisst (3.68) gilt auch f¨ur alle y ̸= o. Da D diagonal ist, bedeutet das, dass dii > 0 (i = 1, . . . , n), womit man D 1 2 und ̃R wie in (3.63) und (3.64) deﬁnieren kann. Mit anderen Worten, die Cholesky-Zerlegung von A existiert. Umgekehrt, folgt aus der Existenz der Cholesky-Zerlegung A = ̃RH ̃R sofort, dass A Hermitesch ist. Weiter gilt mit der Deﬁnition y :≡ ̃Rx, dass xHAx = xH ̃RH ̃Rx = yHy > 0 , falls y ̸= o. Dabei bedingt y = o wieder, dass x = o ist. Also ist A positiv deﬁnit. Damit ist folgender Satz bewiesen: Satz 3.8 Eine reell symmetrische oder Hermitesche Matrix A ist genau dann positiv deﬁnit, wenn sie eine Cholesky-Zerlegung (3.65) hat, worin ̃R eine (reelle bzw. komplexe) Rechtsdreiecksma- trix mit positiven Diagonalelementen ist. Es gibt wiederum verschiedene Varianten der Berechnung von ̃R: zeilenweise mit Berechnung des Schurkomplementes (analog zum Gauss-Algorithmus 1.1), oder zeilenweise direkt (analog zur LR– Zerlegung in Algorithmus 3.1, oder auf ¨ahnliche Art kolonnenwei- se, oder durch Updating wie in Algorithmus 3.3. Die zeilenweise c⃝M.H. Gutknecht 11. April 2016 LA-Skript 3-17 Kapitel 3 — LR–Zerlegung Lineare Algebra (2009) Berechnung entspricht auch der Reihenfolge der f¨uhrenden Haupt- untermatrizen. Algorithmus 3.4 (Zeilenweise, direkte Cholesky-Zerle- gung) Zur Cholesky-Zerlegung einer positiv deﬁniten reell sym- metrischen oder Hermiteschen n × n Matrix A berechne man f¨ur i = 1, . . . , n: ̃rii := √ √ √ √aii − i−1∑ j=1 |̃rji|2 , (3.69a) ̃rik := ( aik − i−1∑ j=1 ̃rji ̃rjk ) 1 ̃rii (k = i + 1, . . . , n) . (3.69b) Dabei sind die Summen leer, wenn i = 1 ist. Im letzten Schritt mit i = n entf¨allt die zweite Formel (3.69b). Anwendung auf Gleichungssysteme; Rechenaufwand Nat¨urlich kann man die Cholesky-Zerlegung genau wie die LR– Zerlegung zum L¨osen eines Gleichungssystems einsetzen. Algorithmus 3.5 (Gauss-Elimination durch Cholesky- Zerlegung, Vor- und R¨uckw¨artseinsetzen) Zum L¨osen eines Gleichungssystems Ax = b mit symmetrisch oder Hermitesche positiv deﬁniter Matrix A kann man wie folgt vorgehen: 1. Cholesky-Zerlegung A = ̃RH ̃R von A, z.B. mittels Algorith- mus 3.4. 2. Vorw¨artseinsetzen: ̃RHc = b auﬂ¨osen nach c. 3. R¨uckw¨artseinsetzen: ̃Rx = c auﬂ¨osen nach x. Weil in der Cholesky-Zerlegung und in der symmetrischen LDR– Zerlegung (3.62) nur eine Dreiecksmatrix zu bestimmen ist, redu- ziert sich der Rechenaufwand im Vergleich zur LR–Zerlegung und zur Gauss-Elimination um fast die H¨alfte. Im Detail gilt analog zu Satz 3.2 folgende Aussage: Satz 3.9 Der Rechenaufwand f¨ur die Cholesky-Zerlegung einer voll besetzten, symmetrisch oder Hermitesch positiv deﬁniten n×n- Matrix betr¨agt n Divisionen (inklusive der letzten, die man erst im R¨uckw¨artseinsetzen braucht), 1 6n3 + 1 2n2 + O(n) Multiplikationen und 1 6 n3 + O(n) Additionen. F¨ur Vor- und R¨uckw¨artseinsetzen braucht man zus¨atzlich pro rechte Seite je n2 − n Additionen und n2 + n Multiplikationen. LA-Skript 3-18 11. April 2016 c⃝M.H. Gutknecht Lineare Algebra (2009) Kapitel 4 — Vektorr¨aume Kapitel 4 Vektorr¨aume In der Mathematik triﬀt man vielerorts folgende Struktur an: Die Elemente einer bestimmten Menge lassen sich in nat¨urlicher Weise addieren und mit einer reellen (oder komplexen) Zahl multiplizieren (“strecken”), wobei das Resultat beider Operationen je wieder ein Element der Menge ist und gewisse einfache Regeln gelten. Diese Struktur, genannt Vektorraum oder linearer Raum, kann als Ver- allgemeinerung des n–dimensionalen Euklidischen Raumes, den wir in Kapitel 2 behandelt haben, aufgefasst werden. 4.1 Deﬁnition und Beispiele Definition: Ein Vektorraum [vector space] V ¨uber E ( :≡ R oder C) ist eine nichtleere Menge auf der eine Addition [Addition] x, y ∈ V ↦−→ x + y ∈ V und eine skalare Multiplikation [scalar multiplication] α ∈ E, x ∈ V ↦−→ αx ∈ V deﬁniert sind, wobei folgende Grundregeln gelten: (V1) x + y = y + x (∀x, y ∈ V ), (V2) (x + y) + z = x + (y + z) (∀x, y, z ∈ V ), (V3) es gibt ein ausgezeichnetes Element o ∈ V mit x + o = x (∀x ∈ V ), (V4) zu jedem x ∈ V gibt es ein eindeutig bestimmtes −x ∈ V mit x + (−x) = o , (V5) α(x + y) = αx + αy (∀α ∈ E, ∀x, y ∈ V ), (V6) (α + β)x = αx + βx (∀α, β ∈ E, ∀x ∈ V ), (V7) (α β)x = α(βx) (∀α, β ∈ E, ∀x ∈ V ), (V8) 1x = x (∀x ∈ V ). Die Elemente von V heissen Vektoren [vector]. Das Element o ∈ V ist der Nullvektor [zero vector]. Die Elemente von E bezeichnet man als Skalare [scalars]. Statt Vektorraum sagt man auch linea- rer Raum [linear space]. Ist E = R, heisst der Vektorraum reell, falls E = C komplex. ▲ c⃝M.H. Gutknecht 11. April 2016 LA-Skript 4-1 Kapitel 4 — Vektorr¨aume Lineare Algebra (2009) Die Grundregeln (V1)–(V8) nennt man wieder Axiome [axioms], weil sie postulierte Grundeigenschaften der mathematischen Struk- tur “Vektorraum” sind. Allein auf ihnen aufbauend wird die ganze Theorie abstrakter Vektorr¨aume aufgestellt. Will man anderseits nachweisen, dass eine bestimmte Menge zusammen mit geeignet gew¨ahlten Operationen “Addition” und “skalare Multiplikation” ef- fektiv einen Vektorraum bildet, muss man veriﬁzieren, dass wirklich in diesem Falle (V1) bis (V8) gelten. Solche Beispiele werden wir gleich betrachten. H¨auﬁg ist dabei der Nachweis der meisten Axio- me so einfach, dass es sich nicht lohnt, die Details aufzuschreiben. Die Axiome (V1)–(V4) besagen gerade, dass ein Vektorraum bez¨ug- lich der Addition eine kommutative Gruppe bildet; vgl. Seite 2-9. Wir betrachten nun eine Reihe von Beispielen von Vektorr¨aumen: Beispiel 4.1: Der Raum Rn der reellen n-Vektoren x =    x1 ... xn    , y =    y1 ... yn    , . . . mit der Addition x + y =    x1 ... xn    +    y1 ... yn    :≡    x1 + y1 ... xn + yn    (4.1a) und der skalaren Multiplikation αx = α    x1 ... xn    :≡    αx1 ... αxn    . (4.1b) ♦ Beispiel 4.2: Der Raum C[a, b] der auf dem Intervall [a, b] deﬁnierten und dort stetigen reellen Funktionen [continuous real functions] mit der (punktweisen) Addition f + g : x ∈ [a, b] ↦−→ f (x) + g(x) ∈ R (4.2a) und der (punktweisen) skalaren Multiplikation αf : x ∈ [a, b] ↦−→ αf (x) ∈ R . (4.2b) ♦ Beispiel 4.3: Der Raum Cm[a, b] der auf dem Intervall [a, b] m-mal stetig diﬀerenzierbaren Funktionen [m-times continuously diﬀeren- tiable functions] mit (4.2a) und (4.2b). Hier ist (0 < m ≤ ∞). Zudem setzt man C0[a, b] :≡ C[a, b]. ♦ LA-Skript 4-2 11. April 2016 c⃝M.H. Gutknecht Lineare Algebra (2009) Kapitel 4 — Vektorr¨aume Beispiel 4.4: Der Raum Pm aller Polynome vom Grad ≤ m [po- lynomials of degree ≤ m] mit reellen Koeﬃzienten, p(t) :≡ a0 + a1t + · · · + amtm , q(t) :≡ b0 + b1t + · · · + bmtm , . . . mit der Addition (p + q)(t) :≡ (a0 + b0) + (a1 + b1) t + · · · + (am + bm) tm (4.3a) und der skalaren Multiplikation: (αp)(t) :≡ (αa0) + (αa1) t + · · · + (αam) tm . (4.3b) Dies ist eine “algebraische” Deﬁnition der Addition und skalaren Multi- plikation f¨ur Polynome. Die Potenzen 1, t, . . . , tm sind nur “Platzhalter”; es wird nicht ben¨utzt, dass man f¨ur x einen reellen Wert einsetzen und so einen Funktionswert p(x) berechnen kann. Wir k¨onnten (4.3a), (4.3b) durch (4.2a), (4.2b) (mit p statt f , q statt g) ersetzen, was eine “analy- tische” Deﬁnition erg¨abe. Mit (4.3a), (4.3b) ist Pm “¨aquivalent” zu Rm+1; mit (4.2a), (4.2b) ist Pm dagegen ein “Teilraum” von C(R). Die genauere Bedeutung von “¨aquivalent” und “Teilraum” folgt sp¨ater. P0 ist der Raum der Polynome vom Grade 0, das heisst der reellen Konstanten. Er ist oﬀensichtlich “¨aquivalent” zu R1, das heisst zu den reellen Zahlen aufgefasst als Vektorraum. ♦ Beispiel 4.5: Auch die Menge aller Polynome, P :≡ ∞⋃ m=0 Pm (4.4) (die Vereinigung aller Mengen Pm) ist zusammen mit der durch (4.3a) und (4.3b) deﬁnierten Addition und skalaren Multiplikation ein Vektor- raum. (Bei der Addition muss man m gleich dem gr¨osseren der zwei Grade von p und q w¨ahlen.) Zu P geh¨oren Polynome beliebig hohen Grades, jedoch keine nichtabbrechenden Potenzreihen. Polynome k¨onnte man nat¨urlich auch miteinander multiplizieren, aber diese Operation lassen wir hier ausser acht. L¨asst man sie anstelle der skalaren Multiplikation zu, so bilden die Polynome einen Ring mit Eins (die Konstante 1), vgl. Seite 2-9. Anders als der Ring der n × n Matri- zen ist dieser jetzt aber kommutativ (weil die Polynom-Multiplikation kommutativ ist: p(t)q(t) = q(t)p(t)) und hat keine Nullteiler. Er hat damit die gleichen Grundeigenschaften wie der Ring der ganzen Zahlen. ♦ Beispiel 4.6: Der Raum ℓ der rellen Zahlfolgen {xk}∞ k=0 mit glied- weiser Addition {xk} + {yk} :≡ {xk + yk} (4.5a) und gliedweiser skalarer Multiplikation α{xk} :≡ {αxk} . (4.5b) ♦ c⃝M.H. Gutknecht 11. April 2016 LA-Skript 4-3 Kapitel 4 — Vektorr¨aume Lineare Algebra (2009) Zu jedem dieser Beispiele reeller Vektorr¨aume gibt es einen analog deﬁnierten komplexen Vektorraum. Zum Beispiel analog zu C[a, b] den Raum der auf [a, b] stetigen komplexwertigen Funktionen. Es sei nochmals betont, dass man f¨ur jedes Beispiel veriﬁzieren m¨usste, dass die Vektorraum-Axiome erf¨ullt sind. Aus den Vektorraum-Axiomen lassen sich eine Reihe weiterer ein- facher Regeln ableiten: Satz 4.1 Es sei V ein Vektorraum ¨uber E. F¨ur alle x, y ∈ V und alle α ∈ E gilt: 0x = o , (4.6) αo = o , (4.7) αx = o =⇒ α = 0 oder x = o , (4.8) (−α)x = α(−x) = −(αx) . (4.9) Obwohl diese Aussagen im Hinblick auf den Spezialfall V = R n trivial aussehen, sind ihre Beweise zum Teil etwa m¨uhselig. Als Beispiel eines solchen Beweises geben wir jenen f¨ur (4.6) an. Beweis von (4.6): Es ist nach (V6) zun¨achst 0x = (0 + 0) x = 0x + 0x . Nach (V4) gibt es zu y :≡ 0x einen Vektor −y mit 0x + (−y) = o. Unter Verwendung von (V2) und (V3) folgt weiter, dass o = 0x + (−y) = (0x + 0x) + (−y) (V 2) = 0x + (0x + (−y)) = 0x + o (V 3) = 0x . ¨Ahnlich beweist man Satz 4.2 Zu x, y ∈ V existiert z ∈ V mit x + z = y, wobei z eindeutig bestimmt ist und gilt: z = y + (−x). Die erste Aussage (Existenz) kann an die Stelle der Axiome (V3) und (V4) treten. Der Satz liefert auch eine Deﬁnition der Subtrak- tion in V : Definition: In einem Vektorraum V ist die Subtraktion [sub- traction] zweier Vektoren x, y ∈ V deﬁniert durch y − x :≡ y + (−x) . ▲ Ausblick: Allgemeiner kann man in der Deﬁnition des Vektor- raumes anstelle von R oder C einen beliebigen Skalarenk¨orper [ﬁeld of scalars] K zulassen und Vektorr¨aume ¨uber K deﬁnieren. Ein K¨orper ist eine weitere durch Axiome deﬁnierte mathemati- sche Struktur. Es ist ein kommutativer Ring mit Eins und ohne Nullteiler, indem auch die Division deﬁniert ist, solange der Divisor (Nenner) nicht das neutrale Element der Addition (die “Null”) ist. LA-Skript 4-4 11. April 2016 c⃝M.H. Gutknecht Lineare Algebra (2009) Kapitel 4 — Vektorr¨aume Die rellen und komplexen Zahlen, R und C, mit der ¨ublichen Ad- dition und Multiplikation sind die bekanntesten Beispiele. Obwohl wir im folgenden solche allgemeinere Vektorr¨aume nicht betrachten, f¨ugen wir hier die Deﬁnition eines K¨orpers an: Definition: Ein K¨orper [ﬁeld] ist eine nichtleere Menge K auf der eine Addition α, β ∈ K ↦−→ α + β ∈ K und eine Multiplikation α, β ∈ K ↦−→ α · β ∈ K deﬁniert sind, wobei folgende Grundregeln gelten: (K1) α + β = β + α (∀α, β ∈ K), (K2) (α + β) + γ = α + (β + γ) (∀α, β, γ ∈ K), (K3) es gibt ein ausgezeichnetes Element 0 ∈ K mit α + 0 = α (∀α ∈ K), (K4) zu jedem α ∈ K gibt es ein eindeutig bestimmtes −α ∈ K mit α + (−α) = 0 , (K5) α · β = β · α (∀α, β ∈ K), (K6) (α · β) · γ = α · (β · γ) (∀α, β, γ ∈ K), (K7) es gibt ein ausgezeichnetes Element 1 ∈ K, 1 ̸= 0, mit α · 1 = α (∀α ∈ K), (K8) zu jedem α ∈ K, α ̸= 0 gibt es ein eindeutig bestimmtes α−1 ∈ K mit α · (α−1) = 1 , (K9) α · (β + γ) = α · β + α · γ (∀α, β, γ ∈ K), (K10) (α + β) · γ = α · γ + β · γ (∀α, β, γ ∈ K). ▲ Die Axiome (K1)–(K4) bedeuten, dass K bez¨uglich der Addition eine kommutative Gruppe ist, und wegen den Axiomen (K5)–(K8) ist K\\{0} (d.h. K ohne die Null) auch eine kommutative Gruppe bez¨uglich der Multiplikation. Schliesslich sind die Axiome (K9) und (K10) die ¨ublichen Distributivgesetze. Den Multiplikationspunkt l¨asst man oft weg. Axiom (K8) hat zur Folge, dass man in einem K¨orper eine Division deﬁnieren kann: α/β :≡ α · β−1 . (4.10) L¨asst man dieses Axiom weg, so hat man anstelle eines K¨orpers einen kommutativen Ring mit Eins. ▼ c⃝M.H. Gutknecht 11. April 2016 LA-Skript 4-5 Kapitel 4 — Vektorr¨aume Lineare Algebra (2009) 4.2 Unterr¨aume, Erzeugendensysteme Definition: Eine nichtleere Teilmenge U eines Vektorraums V heisst Unterraum [subspace], falls sie bez¨uglich Addition und ska- larer Multiplikation abgeschlossen ist, d.h. falls x + y ∈ U, αx ∈ U (∀x, y ∈ U, ∀α ∈ E) . (4.11) ▲ In einem Unterraum U ⊆ V sind Addition und Multiplikation gleich deﬁniert wie in V . Zudem gelten die Axiome (V1), (V2), (V5)–(V8), weil sie in V gelten. Schliesslich ist f¨ur jedes x ∈ U auch 0x ∈ U und (−1)x ∈ U , wobei wir nach Satz 4.1 und (V8) wissen, dass 0x = o , (−1) x = −(1x) = −x . Also gelten (V3) und (V4) auch, und wir erhalten die folgende Aus- sage. Satz 4.3 Ein Unterraum ist selbst ein Vektorraum. Beispiel 4.7: Fasst man die Polynome als auf R deﬁnierte Funktionen auf, und ist 0 < m < n, so gilt P0 ⊂ Pm ⊂ Pn ⊂ P ⊂ C∞(R) ⊂ Cn(R) ⊂ Cm(R) ⊂ C(R) , (4.12) wobei nun jeder Raum Unterraum der rechts davon stehenden Vek- torr¨aume ist. ♦ Beispiel 4.8: Jeder Vektorraum V ist ein Unterraum von sich selbst. Zudem hat jeder Vektorraum als Unterraum den Raum {o}, der nur den Nullvektor enth¨alt. Dieser Nullraum ist selbst auch ein Vektorraum, der alle Axiome erf¨ullt. ♦ Beispiel 4.9: Im geometrisch interpretierten Vektorraum R3 der Orts- vektoren ist jede Ebene durch den Ursprung O und jede Gerade durch O ein Unterraum. ♦ Ein weiteres, besonders wichtiges Beispiel formulieren wir als Satz: Satz 4.4 Ist A ∈ R m×n eine reelle m×n–Matrix und L0 die Men- ge der L¨osungsvektoren x ∈ R n des homogenen Gleichungssystems Ax = o , so ist L0 ein Unterraum von R n. Beweis: Sind x, y ∈ L0, d.h. gilt Ax = o und Ay = o, so folgt A(x + y) = Ax + Ay = o , und f¨ur alle α ∈ R A(αx) = α(Ax) = o , d.h. es gilt x + y ∈ L0, αx ∈ L0. LA-Skript 4-6 11. April 2016 c⃝M.H. Gutknecht Lineare Algebra (2009) Kapitel 4 — Vektorr¨aume Dieser Satz ist eine Verallgemeinerung von Beispiel 4.9: Der L¨osungs- raum von Ax = o ist eine “Ebene” (ein Unterraum) im R n durch den Ursprung. Als n¨achstes ¨ubertragen wir den Begriﬀ der Linearkombination von En (siehe Kapitel 2, Seite 2-10) auf beliebige Vektorr¨aume. Definition: Es seien V ein Vektorraum ¨uber E, und a1, . . . , aℓ ∈ V ausgew¨ahlte Vektoren. Ein Vektor der Form x :≡ γ1a1 + · · · + γℓaℓ = ℓ∑ k=1 γkak (4.13) mit γ1, . . . , γℓ ∈ E heisst eine Linearkombination [linear combi- nation] von a1, . . . , aℓ. ▲ Die Gesamtheit aller Linearkombinationen von a1, . . . , aℓ ist oﬀen- sichtlich ein Unterraum, denn (4.11) ist f¨ur die Vektoren der Form (4.13) mit festen Vektoren a1, . . . , aℓ und freien Skalaren γ1, . . . , γℓ erf¨ullt. Definition: Die Menge aller Linearkombinationen von a1, . . . , aℓ heisst der von a1, . . . , aℓ aufgespannte (oder: erzeugte) Un- terraum [subspace spanned by a1, . . . , aℓ; span] oder die lineare H¨ulle von a1, . . . , aℓ [linear hull]. Er wird bezeichnet mit:1 span {a1, . . . , aℓ} :≡ { ℓ∑ k=1 γkak ; γ1, . . . , γℓ ∈ E } . (4.14) Der von einer unendlichen Folge oder Menge S ⊂ V erzeugte Unter- raum ist gleich der Gesamtheit aller Linearkombinationen endlich2 vieler Vektoren aus S: span S :≡ { m∑ k=1 γkak ; m ∈ N; a1, . . . , am ∈ S; γ1, . . . , γm ∈ E } . (4.15) Die Vektoren a1, . . . , aℓ in (4.14) bzw. die Menge S in (4.15) heissen Erzeugendensystem [spanning set] von span {a1, . . . , aℓ} bzw. span S. ▲ Beispiel 4.10: Die vier Vektoren a1 =   1 2 0   , a2 =   0 1 0   , a3 =   2 0 0   , a4 =   2 1 0   spannen oﬀenbar im 3-dimensionalen Raum die Koordinatenebene E12 auf, die alle Vektoren enth¨alt, deren dritte Koordinate null ist. Sie sind 1Oft schreibt man auch ⟨a1, . . . , aℓ⟩ f¨ur diesen Unterraum, was wir aber wegen des Konﬂiktes mit unserer Notation f¨ur das Skalarprodukt vermeiden. 2Man betrachtet anderswo in der Mathematik allerdings auch Vektorr¨aume und Unterr¨aume, die von Linearkombinationen unendlich vieler Vektoren er- zeugt werden; siehe den Ausblick am Schluss von Abschnitt 6.3. c⃝M.H. Gutknecht 11. April 2016 LA-Skript 4-7 Kapitel 4 — Vektorr¨aume Lineare Algebra (2009) also eine Erzeugendensystem f¨ur E12. Oﬀensichtlich w¨urden dazu schon die Vektoren a2 = e2 und a3 = 2e1 reichen; diese sind auch ein Erzeu- gendensystem f¨ur E12. In der Tat spannen irgend zwei der vier Vektoren E12 auf. Deﬁniert man die Matrix A = ( a1 a2 a3 a4 ), so ist span {a1, a2, a3, a4} = {y = A x ; x ∈ R 4} ⊂ R 3. ♦ Beispiel 4.11: Die m + 1 Monome [monomials] 1, t, t2, . . . , tm sind ein Erzeugendensystem des Raumes Pm aller Polynome vom Grade ≤ m. Oﬀensichtlich reichen weniger als m + 1 Polynome nicht. ♦ Beispiel 4.12: Die (unendlich vielen) Monome 1, t, t2, . . . bilden ein Erzeugendensystem des Raumes P aller Polynome aus Beispiel 4.5. Jedes Polynom hat ja einen endlichen Grad m und l¨asst sich deshalb als Linearkombination von 1, t, t2, . . . , tm schreiben. ♦ 4.3 Lineare Abh¨angigkeit, Basen, Dimension Die vorangehenden Beispiele lassen vermuten, dass sich gewisse Er- zeugendensysteme dadurch auszeichnen, dass sie aus einer minima- len Anzahl von Vektoren bestehen. In gr¨osseren Erzeugendensyste- men lassen sich ein Teil der gegebenen Vektoren als Linearkombi- nationen der anderen Vektoren darstellen. Definition: Die Vektoren a1, . . . , aℓ ∈ V heissen linear abh¨an- gig [linearly dependent], falls es Skalare γ1, . . . , γℓ gibt, die nicht alle 0 sind und f¨ur die gilt γ1a1 + · · · + γℓ aℓ = o . (4.16) Andernfalls, d.h. wenn (4.16) impliziert dass γ1 = · · · = γℓ = 0, sind die Vektoren linear unabh¨angig [linearly independent]. ▲ Lemma 4.5 Die ℓ ≥ 2 Vektoren a1, . . . aℓ sind linear abh¨angig ge- nau dann, wenn sich einer dieser Vektoren als Linearkombination der andern schreiben l¨asst. Beweis: Sind die Vektoren a1, . . . aℓ linear abh¨angig, gilt (4.16) mit γk ̸= 0 f¨ur ein k, 1 ≤ k ≤ ℓ; folglich ist ak = − ℓ∑ j=1 j̸=k γj γk aj . Die Umkehrung ist trivial. Beispiel 4.13: Die vier Vektoren von Beispiel 4.10 sind linear abh¨angig, denn es ist 2a1 − 4a2 − a3 + 0a4 = o bzw. a3 = 2a1 − 4a2 . ♦ LA-Skript 4-8 11. April 2016 c⃝M.H. Gutknecht Lineare Algebra (2009) Kapitel 4 — Vektorr¨aume Beispiel 4.14: Die drei Funktionen t ↦−→ 1, cos2 t, cos 2t ∈ C∞(R) sind linear abh¨angig, denn cos 2t = 2 cos2 t − 1 . ♦ Beispiel 4.15: Sowohl als formale Polynome als auch als reelle Funk- tionen sind die m + 1 Monome t ↦−→ 1, t, t2, . . . , tm ∈ Pm ⊂ C∞(R) linear unabh¨angig, denn m∑ k=0 γk tk = o impliziert γ0 = γ1 = . . . γm = 0. Die Summe ist ja selbst ein Polynom vom Grad ≤ m, muss also das Nullpolynom sein. Dieses kann man aber nicht anders als Linearkombination der Monome 1, t, t2, . . . , tm darstel- len, als wenn alle Koeﬃzienten null sind. ♦ Beispiel 4.16: Sind die drei Polynome p1(t) :≡ 1 + t2 , p2(t) :≡ 1 − t2 , p3(t) :≡ 1 + t2 + t4 (4.17) wohl linear unabh¨angig? Aus der Bedingung, dass f¨ur alle t gilt 0 = γ1p1(t) + γ2p2(t) + γ3p3(t) = γ1(1 + t2) + γ2(1 − t2) + γ3(1 + t2 + t4) = (γ1 + γ2 + γ3) + (γ1 − γ2 + γ3)t2 + γ3t4 , folgt durch Koeﬃzientenvergleich (wir wissen aus Beispiel 4.15 ja, dass die Monome 1, t2, t4 linear unabh¨angig sind): γ1 + γ2 + γ3 = 0 γ1 − γ2 + γ3 = 0 (4.18) γ3 = 0 . Dies ist ein lineares Gleichungssystem. Ein Eliminationsschritt f¨uhrt be- reits auf die LR–Zerlegung   1 1 1 1 −1 1 0 0 1   =   1 0 0 1 1 0 0 0 1     1 1 1 0 −2 0 0 0 1   , (4.19) aus der man schliesst, dass die Matrix des 3 × 3–Systems (4.18) regul¨ar ist. Also hat das homogene System nur die triviale L¨osung; das heisst γ1 = γ2 = γ3 = 0, was bedeutet, dass die Polynome p1, p2, p3 linear unabh¨angig sind. ♦ Beispiel 4.17: Sind α1, α2, . . . , αn ∈ R paarweise voneinander ver- schieden, so sind die Funktionen t ↦−→ eα1t, eα2t, . . . , eαnt ∈ C∞(R) (4.20) linear unabh¨angig. c⃝M.H. Gutknecht 11. April 2016 LA-Skript 4-9 Kapitel 4 — Vektorr¨aume Lineare Algebra (2009) Beweis: Wir d¨urfen annehmen, die αk seien der Gr¨osse nach geordnet: α1 < α2 < · · · < αn. W¨aren die Funktionen linear abh¨angig, so g¨abe es einen gr¨ossten Index k mit der Eigenschaft, dass eαkt eine Linearkombi- nation der ¨ubrigen Funktionen ist: eαkt = k−1∑ j=1 γj eαj t . Es folgt, dass f (t) :≡ k−1∑ j=1 γj e(αj −αk)t ≡ 1 . Wegen αj − αk < 0 (j = 1, . . . , k − 1) ist aber limt→∞ f (t) = 0 im Widerspruch zu f (t) ≡ 1. ♦ Die Verallgemeinerung der linearen Unabh¨angigkeit auf unendliche Mengen von Vektoren ist heikel. In der linearen Algebra geht man ¨ublicherweise zun¨achst anders vor als in sp¨ateren Anwendungen. Wir kommen in Abschnitt 6.3 auf diesen Punkt zur¨uck. Definition: Eine unendliche Menge von Vektoren heisst linear unabh¨angig [linearly independent], wenn jede endliche Teilmenge linear unabh¨angig ist. Andernfalls ist die Menge linear abh¨angig [linearly dependent]. ▲ Beispiel 4.18: Ist {αk}∞ k=0 eine Folge verschiedener reller Zahlen, so sind die unendlich vielen Funktionen t ↦−→ eαkt (k = 1, 2, . . . ) gem¨ass Beispiel 4.17 linear unabh¨angig. Wir k¨onnten hier sogar von einer beliebigen Menge verschiedener re- eller Zahlen ausgehen, zum Beispiel sogar von ganz R. Wir k¨onnten auch beliebige komplexe Exponenten zulassen, m¨ussten dann aber den Beweis anpassen. Es gibt hier oﬀenbar eine enorm grosse Zahl linear un- abh¨angiger Funktionen. ♦ Es ist nun naheliegend, unter den Erzeugendensystemen f¨ur einen Vektorraum solche auszuw¨ahlen, die m¨oglichst klein sind. Umge- kehrt m¨ochte man Mengen von linear unabh¨angigen Vektoren so gross w¨ahlen oder so erg¨anzen, dass sie den ganzen Raum erzeu- gen. Beides f¨uhrt auf den folgenden Begriﬀ. Definition: Ein linear unabh¨angiges Erzeugendensystem eines Vektorraums V heisst Basis [basis] von V . ▲ Beispiel 4.19: Die n Einheitsvektoren im En, e1 :≡        1 0 0 ... 0        , e2 :≡        0 1 0 ... 0        , . . . , en :≡        0 0 ... 0 1        (4.21) LA-Skript 4-10 11. April 2016 c⃝M.H. Gutknecht Lineare Algebra (2009) Kapitel 4 — Vektorr¨aume (das sind die n Kolonnen der Einheitsmatrix In) spannen den Raum En auf, sind also ein Erzeugendensystem von En. Oﬀensichtlich reichen we- niger als n Vektoren dazu nicht. Diese n Vektoren bilden also eine Basis des En (d.h. des Rn und des Cn). Wir nennen sie die Standardbasis [standard basis]. ♦ Beispiel 4.20: Was gibt es f¨ur andere Basen in En? Die n Kolonnen b1, b2, . . . , bn ∈ En einer n×n Matrix B bilden genau dann eine Basis von En, wenn B regul¨ar ist. In der Tat gilt nach Korollar 1.7: Ist B regul¨ar, hat Bx = c f¨ur jedes c ∈ En eine L¨osung und f¨ur c = o nur die Nulll¨osung. Das heisst gerade, dass die Kolonnen von B den Raum En erzeugen und linear unabh¨angig sind. Ist B dagegen singul¨ar, so sind seine Kolonnen weder erzeugend noch linear unabh¨angig. Sind zum Beispiel b1, b2, . . . , bn ∈ En die n Kolonnen einer oberen Dreiecksmatrix, b1 :≡        b11 0 0 ... 0        , b2 :≡        b12 b22 0 ... 0        , . . . , bn :≡        b1n b2n b3n ... bnn        und sind die Diagonalelemente b11, b22, ..., bnn alle nicht 0, so bilden diese Kolonnen von B eine Basis von En. ♦ Beispiel 4.21: Wir wissen bereits, dass die drei Polynome p1, p2, p3 aus (4.17) linear unabh¨angig sind. Sie liegen alle in P4 und sind alles ge- rade Funktionen (d.h. pk(−t) = pk(t)). Sicher bilden sie nicht eine Basis von P4, denn die Polynome t und t3 geh¨oren nicht zum von p1, p2, p3 er- zeugten Unterraum. Man k¨onnte aber vermuten, dass dieser Unterraum gleich dem Unterraum G4 aller gerader Polynome vom H¨ochstgrade 4 ist, das heisst, dass gilt3 G4 :≡ span {1, t2, t4} = span {p1, p2, p3} . (4.22) Dazu m¨ussten wir zeigen, dass ein beliebiges Polynom der Form g(t) = a + b t2 + c t4 als Linearkombination von p1, p2, p3 dargestellt werden kann. Analog zur Rechnung in Beispiel 4.16 m¨usste nun die Bedingung a + b t2 + c t4 = γ1p1(t) + γ2p2(t) + γ3p3(t) = (γ1 + γ2 + γ3) + (γ1 − γ2 + γ3)t2 + γ3t4 gelten, die durch Koeﬃzientenvergleich das inhomogene System 3Wir sind hier und anderswo etwas schludrig in der Notation und verwenden Bezeichungen wie p(t), tn auch f¨ur die Funktionen, nicht nur f¨ur die Funkti- onswerte. c⃝M.H. Gutknecht 11. April 2016 LA-Skript 4-11 Kapitel 4 — Vektorr¨aume Lineare Algebra (2009) γ1 + γ2 + γ3 = a γ1 − γ2 + γ3 = b (4.23) γ3 = c liefert. Da das homogene System (4.18) nur die triviale L¨osung hat, wis- sen wir aus Korollar 1.6, dass obiges System f¨ur beliebige rechte Seiten ( a b c )T l¨osbar ist. Also ist {p1, p2, p3} ein Erzeugendensystem f¨ur G4, d.h. (4.22) ist richtig. Wir k¨onnen dies hier dank Korollar 1.6 schliessen, ohne die Koeﬃzienten γk explizit als Funktion von a, b, c zu bestimmen. Allerdings w¨are auch das einfach, denn mittels der LR–Zerlegung (4.19) kann man das System (4.23) mit a, b, c als Parameter l¨osen. ♦ Beispiel 4.22: Die abz¨ahlbar unendlich vielen Monome t ↦−→ 1, t, t2, . . . bilden eine Basis (die Standardbasis) des Raumes P aller Polynome, vgl. (4.4). Wir sahen ja bereits, dass endlich viele Monome linear unabh¨angig sind. Nach unserer Deﬁnition ist also auch die unendliche Menge aller Monome linear unabh¨angig. Zudem ist jedes Polynom eine endliche Li- nearkombination von Monomen. ♦ Beispiel 4.23: Die unendlich vielen Zahlfolgen e1 :≡ ( 1 0 0 . . . ) , e2 :≡ ( 0 1 0 . . . ) , · · · bilden die Standardbasis des Vektorraumes ℓ0 der abbrechenden (rellen oder komplexen) Zahlfolgen. Denn oﬀensichtlich kann man jedes Ele- ment von ℓ0 als Linearkombination endlich vieler dieser Vektoren dar- stellen, und anderseits sind endlich viele dieser Vektoren auch immer linear unabh¨angig. ♦ Voraussetzung: Im folgenden beschr¨anken wir uns auf Vektor- r¨aume, die ein endliches Erzeugendensystem besitzen. F¨ur diese R¨aume sind einige grundlegende Fakten leicht herzuleiten: Lemma 4.6 Gibt es zu einem (vom Nullraum verschiedenen) Vektorraum ein endliches Erzeugendensystem, so besitzt er eine Basis, die eine Teilmenge des Erzeugendensystems ist. Beweis: Ist das Erzeugendensystem nicht ohnehin linear unabh¨angig und enth¨alt es mehr als einen Vektor, so l¨asst sich gem¨ass Lemma 4.5 einer der Vektoren durch die anderen ausdr¨ucken. Man kann also die- sen streichen; die restlichen bilden immer noch ein Erzeugendensystem. Durch Wiederholung dieses Vorgehens kann man das Erzeugendensy- stem schrittweise auf eines reduzieren, das nur noch aus n ≥ 1 linear unabh¨angigen Vektoren besteht. Dieses ist per Deﬁnition eine Basis. LA-Skript 4-12 11. April 2016 c⃝M.H. Gutknecht Lineare Algebra (2009) Kapitel 4 — Vektorr¨aume Wir zeigen als n¨achstes, dass die Zahl n der Basisvektoren eine charakteristische Gr¨osse des Vektorraumes ist. Satz 4.7 Besitzt ein Vektorraum V ein endliches Erzeugendensy- stem, so besteht jede Basis von V aus derselben Zahl von Vektoren. Definition: Die Zahl der Basisvektoren (in jeder Basis) eines Vektorraumes V mit endlichem Erzeugensystem heisst Dimensi- on [dimension] von V und wird mit dim V bezeichnet. Ein sol- cher Raum ist also endlich-dimensional [ﬁnite dimensional]. Falls dim V = n gilt, so sagt man, V sei n–dimensional [n–dimensional]. ▲ Bevor wir Satz 4.7 beweisen, betrachten wir einfache Beispiele: Beispiel 4.24: Pm hat die Standardbasis bestehend aus den m + 1 Monomen t ↦−→ tk (0 ≤ k ≤ m). Es ist also dim Pm = m + 1. ♦ Beispiel 4.25: Der Raum En mit der in Beispiel 4.19 gegebenen Stan- dardbasis e1, . . . , en hat nat¨urlich die Dimension n, und zwar ist Rn ein n–dimensionaler reeller Vektorraum, Cn ein n–dimensionaler komplexer Vektorraum. Fasst man Cn als reellen Vektorraum auf (d.h. l¨asst man nur reelle Ska- lare zu), so hat Cn die Dimension 2n. Eine einfache Basis ist      1 0 ... 0      , . . . ,      0 0 ... 1      ︸ ︷︷ ︸ n ,      i 0 ... 0      , . . . ,      0 0 ... i      ︸ ︷︷ ︸ n . Oﬀensichtlich l¨asst sich jeder komplexe n-Vektor als Linearkombination dieser 2n Vektoren schreiben, auch wenn nur reelle Koeﬃzienten zuge- lassen sind; die Vektoren erzeugen also Cn. Zudem sind sie oﬀensichtlich linear unabh¨angig. ♦ Nun kommen wir zum Beweis von Satz 4.7. Er folgt sofort aus dem Beweis von Lemma 4.6 und aus dem folgenden Lemma. Lemma 4.8 Ist {b1, . . . , bm} ein Erzeugendensystem von V , so ist jede Menge {a1, . . . , aℓ} ⊂ V von ℓ > m Vektoren linear abh¨angig. Beweis: Die ak lassen sich als Linearkombinationen von b1, . . . , bm darstellen: ak = m∑ i=1 τikbi (1 ≤ k ≤ ℓ) . (4.24) Wir betrachten nun das homogene lineare Gleichungssystem ℓ∑ k=1 τik ξk = 0 (1 ≤ i ≤ m) (4.25) bestehend aus m Gleichungen in ℓ (> m) Unbekannten ξk. Nach Korol- lar 1.5 besitzt es eine nichttriviale L¨osung ( ξ1 . . . ξℓ )T ̸= o ∈ Eℓ. c⃝M.H. Gutknecht 11. April 2016 LA-Skript 4-13 Kapitel 4 — Vektorr¨aume Lineare Algebra (2009) Mit diesen ξk bilden wir die Linearkombination a :≡ ℓ∑ k=1 ξk ak und erhalten aufgrund von (4.24) und (4.25) a = ℓ∑ k=1 ξk ( m∑ i=1 τikbi) = m∑ i=1 ( ℓ∑ k=1 τik ξk ︸ ︷︷ ︸ = 0 (∀i) )bi = o , was zeigt, dass a1, . . . , aℓ linear abh¨angig sind. Betrachten wir noch den Fall V = En, wo das Erzeugendensystem {b1, . . . , bm} und die Menge {a1, . . . , aℓ} (mit ℓ > m) aus Kolon- nenvektoren bestehen und als Kolonnen von Matrizen aufgefasst werden k¨onnen: B = ( b1 . . . bm ) , A = ( a1 . . . aℓ ) . (4.26) Mit der m × ℓ Matrix T = ( τik ) kann man dann (4.24) als A = BT (4.27) schreiben. Da m < ℓ ist, hat das homogene System Tx = o eine nichttriviale L¨osung, und f¨ur diese gilt Ax = BTx = Bo = o . (4.28) Dies bedeutet gerade, dass die Kolonnen {a1, . . . , aℓ} von A linear abh¨angig sind. Bei unserer Konstruktion einer Basis in Lemma 4.6 sind wir von einem Erzeugendensystem ausgegangen und haben dieses schritt- weise verkleinert. Man kann auch eine linear unabh¨angige Menge schrittweise zu einer Basis vergr¨ossern: Satz 4.9 Jede Menge linear unabh¨angiger Vektoren aus ei- nem Vektorraum V mit endlichem Erzeugendensystem l¨asst sich erg¨anzen zu einer Basis von V . Beweis: Gegeben ist eine linear unabh¨angige Menge M von Vektoren und ein Erzeugendensystem E. Ist span M ̸= V , so gibt es in E einen Vektor, der nicht als Linearkombination von M dargestellt werden kann. Diesen schlagen wir zus¨atzlich zu M , wodurch dim span M um eins zunimmt, aber M linear unabh¨angig bleibt. Wiederholt man dieses Vor- gehen, so erreicht man schrittweise, dass dim span M = dim span E = dim V wird. Dann ist M eine Basis. Da gem¨ass Satz 4.7 jede Basis eines endlichdimensionalen Vektor- raumes V aus gleich vielen, n¨amlich n = dim V Vektoren besteht, k¨onnen wir aus Satz 4.9 sofort folgendes schliessen: Korollar 4.10 In einem Vektorraum V endlicher Dimension ist jede Menge von n = dim V linear unabh¨angigen Vektoren eine Basis von V . LA-Skript 4-14 11. April 2016 c⃝M.H. Gutknecht Lineare Algebra (2009) Kapitel 4 — Vektorr¨aume Beweis: W¨are eine solche Menge keine Basis, so liesse sie sich nach Satz 4.9 zu einer erg¨anzen. Diese w¨urde aber mehr als dim V Vektoren enthalten, was Satz 4.7 widersprechen w¨urde. Beispiel 4.26: Dass die Polynome p1, p2, p3 aus den Beispielen 4.16 und 4.21 eine Basis des Raumes G4 der geraden Polynome vom Grade ≤ 4 bilden, k¨onnen wir nun einfacher einsehen als in Beispiel 4.21. Da die drei Monome 1, t2, t4 eine Basis von G4 bilden, ist dim G4 = 3. In Beispiel 4.16 haben wir bewiesen, dass p1, p2, p3 linear unabh¨angig sind; also bilden sie auch eine Basis. Man kann nun sowohl 1, t2, t4 als auch p1, p2, p3 zu einer Basis von P4 erg¨anzen. Beispielsweise sind die zwei Polynome t und t3 eine geeigne- te Erg¨anzung der beiden Basen, aber wir k¨onnten auch etwa das Paar t + p1(t) und t + t3 + p3(t) verwenden. Die Wahl der Monome scheint attraktiver, weil diese den Unterraum U3 der ungeraden Polynome vom H¨ochstgrade 3 aufspannen, aber als Basen von P4 sind 1, t, t2, t3, t4 und p1(t), p2(t), p3(t), t + p1(t), t + t3 + p3(t) theoretisch gleichwertig. (Beim numerischen Rechnen wird man aber gewisse Basen bevorzugen. Man muss vor allem vermeiden, dass nahezu linear abh¨angige Basen gew¨ahlt werden.) ♦ Man kann zeigen (mit Hilfe des Zornschen Lemmas, einer tiefsch¨ur- fenden mathematischen Aussage), dass jeder Vektorraum eine Basis hat, also auch jene, die kein endliches Erzeugendensystem haben. Es gilt sogar die folgende Verallgemeinerung von Satz 4.9: Satz 4.11 Jede Menge linear unabh¨angiger Vektoren aus einem Vektorraum V l¨asst sich erg¨anzen zu einer Basis von V . F¨ur die R¨aume ℓ0 und P aus den Beispielen 4.23 und 4.22 ist das kaum ¨uberraschend. Gem¨ass unseren Deﬁnitionen bilden die Vekto- ren e1, e2, . . . aber nicht eine Basis des Vektorraumes ℓ aller Zahl- folgen. Das w¨urde erfordern, dass man unendliche Linearkombina- tionen zulassen w¨urde. Der Raum ℓ hat keine “einfache” Basis im Sinne unserer Deﬁnition. Eine Vektorraumbasis von ℓ ist notwen- digerweise “sehr gross”. Wir k¨onnen keine solche Basis angeben, obwohl wir nach obigem Satz wissen, dass welche existieren. Das Gleiche gilt zum Beispiel f¨ur C[0, 1]. Wir werden in Kapitel 6 auf diese Problematik zur¨uck kommen und einen Ausblick auf die Ant- worten geben, die die Mathematik bereit h¨alt. Die N¨utzlichkeit der Basen beruht auf der n¨achsten Aussage: Satz 4.12 {b1, . . . , bn} ⊂ V ist genau dann eine Basis von V , wenn sich jeder Vektor x ∈ V in eindeutiger Weise als Linear- kombination von b1, . . . , bn darstellen l¨asst: x = n∑ k=1 ξk bk . (4.29) Definition: Die Koeﬃzienten ξk in (4.29) heissen Koordinaten [coordinates] von x bez¨uglich der Basis {b1, . . . , bn}. Der Vektor ξ = ( ξ1 . . . ξn )T ∈ En ist der Koordinatenvektor [coordina- c⃝M.H. Gutknecht 11. April 2016 LA-Skript 4-15 Kapitel 4 — Vektorr¨aume Lineare Algebra (2009) te vector]; (4.29) heisst Koordinatendarstellung [representation in cordinates] von x. ▲ Beweis von Satz 4.12: (i) Es sei {b1, . . . , bn} eine Basis von V , und es sei x ∈ V beliebig. Da {b1, . . . , bn} ein Erzeugendensystem ist, l¨asst sich x in der Form (4.29) darstellen. G¨abe es eine zweite Darstellung x = n∑ k=1 ξ′ k bk von x, so bek¨ame man durch Subtraktion n∑ k=1 (ξk − ξ′ k) bk = x − x = o , woraus wegen der linearen Unabh¨angigkeit von {b1, . . . , bn} folgen w¨urde, dass ξk − ξ′ k = 0, d.h. ξk = ξ′ k (1 ≤ k ≤ n). (ii) Es sei jedes x ∈ V auf eindeutige Weise in der Form (4.29) darstell- bar. Dann ist {b1, . . . , bn} ein Erzeugendensystem und, weil o ∈ V die eindeutige Darstellung o = 0b1 + . . . 0bn hat, sind b1, . . . , bn linear unabh¨angig. Beispiele 4.27: Oﬀensichtlich gilt f¨ur einen beliebigen n-Vektor x = ( x1 . . . xn )T ∈ En x = n∑ k=1 xkek . (4.30) Ebenso l¨asst sich jedes Polynom p vom Grade m per Deﬁnition darstellen in der Form p(t) = a0 + a1t + · · · amtm = m∑ k=0 aktk , (4.31) das heisst als Linearkombination der Standardbasis 1, t, . . . , tm. Um in den R¨aumen En und Pm zur Darstellung eines Vektors in einer anderen Basis zu kommen, wird man in der Regel diesen Vektor und die andere Basis in der Standardbasis darstellen und dann einen Koeﬃzi- entenvergleich durchf¨uhren. So gelang es uns etwa in Beispiel 4.21 das Polynom g, das allgemeine gerade Polynom vom Grad 4, in der Basis p1, p2, p3 darzustellen; in ei- nem konkreten Beispiel w¨are noch das Gleichungssystem (4.23) zu l¨osen. Im Raum En kommen wir zu diesem Koeﬃzientenvergleich automatisch, sobald wir die Komponenten der Vektoren betrachten. ♦ Die S¨atze 4.9 und 4.12 geben Anlass zu einer Aufsplittung eines Vektorraumes in zwei sich erg¨anzende Unterr¨aume. Es sei M = {b1, . . . , bl} eine Menge linear unabh¨angiger Vektoren, die den Raum V nicht erzeugen. Sie bilden eine Basis des echten Unterraumes U :≡ span M = span {b1, . . . , bl} . (4.32) LA-Skript 4-16 11. April 2016 c⃝M.H. Gutknecht Lineare Algebra (2009) Kapitel 4 — Vektorr¨aume Nach Satz 4.9 kann man M zu einer Basis von V erg¨anzen. Es sei N = {bl+1, . . . , bn} die Menge zus¨atzlicher Basisvektoren und U ′ :≡ span N = span {bl+1, . . . , bn} (4.33) der von ihnen aufgespannte Unterraum. Damit l¨asst sich die Koor- dinatendarstellung eines beliebigen x ∈ V eindeutig aufteilen in x = l∑ k=1 ξk bk ︸ ︷︷ ︸ u ∈ U + n∑ k=l+1 ξk bk ︸ ︷︷ ︸ u′ ∈ U ′ . (4.34) Definition: Zwei Unterr¨aume U und U ′ eines Vektorraumes V mit der Eigenschaft, dass jedes x ∈ V eine eindeutige Darstellung x = u + u′ mit u ∈ U , u′ ∈ U ′ (4.35) hat, heissen komplement¨ar [complementary]. Wir nennen dann V die direkte Summe [direct sum] von U und U ′ und schreiben V = U ⊕ U ′ (4.36) ▲ Beispiel 4.28: In der Situation von Beispiel 4.26 ist klar, dass man P4 als direkte Summe der geraden Polynome vom Grad 4 und der ungeraden Polynome vom Grad 3 schreiben kann: P4 = G4 ⊕ U3 . (4.37) Aber es ist auch P4 = span {p1, p2, p3} ⊕ U3 oder P4 = span {p1, p2, p3} ⊕ span {t + p1(t), t + t3 + p3(t)} . ♦ Allgemeiner kann man einen Vektorraum als direkte Summe meh- rerer Unterr¨aume darstellen. Beispiel 4.29: Wir k¨onnen die Beispiele 4.16, 4.21, 4.26 und 4.28 fortsetzen: Da p1, p2, p3 linear unabh¨angig sind, ist zum Beispiel G4 = span {p1} ⊕ span {p2} ⊕ span {p3} eine direkte Summe und ebenso P4 = span {p1} ⊕ span {p2} ⊕ span {p3} ⊕ U3 . Hier k¨onnte man U3 auch noch als eine direkte Summe von zwei eindi- mensionalen Unterr¨aumen darstellen. ♦ c⃝M.H. Gutknecht 11. April 2016 LA-Skript 4-17 Kapitel 4 — Vektorr¨aume Lineare Algebra (2009) 4.4 Basiswechsel, Koordinatentransformation Es sei {b1, . . . , bn} eine vorgegebene, “alte” Basis des Vektorraumes V , und es sei {b′ 1, . . . , b′ n} eine zweite, “neue” Basis von V . Die neuen Basisvektoren kann man in der alten Basis darstellen: b′ k = n∑ i=1 τik bi , k = 1, . . . , n . (4.38) Die n × n-Matrix T = ( τik ) heisst Transformationsmatrix [transformation matrix] des Basiswechsels [change of basis]. Beachte: In der k-ten Kolonne von T stehen gerade die Koordinaten des k-ten neuen Basisvektors (bez¨uglich der alten Basis). Solch ein Basiswechsel hat eine Koordinatentransformation [co- ordinate transformation] zur Folge, die sich so beschreiben l¨asst: Satz 4.13 Es sei ξ = ( ξ1 . . . ξn )T der Koordinatenvektor ei- nes beliebigen Vektors x ∈ V bez¨uglich der alten Basis, und es sei ξ′ = ( ξ′ 1 . . . ξ′ n )T der Koordinatenvektor dieses Vektors x bez¨uglich einer neuen, durch (4.38) gegebenen Basis, so dass n∑ i=1 ξi bi = x = n∑ k=1 ξ′ k b′ k . (4.39) Dann gilt f¨ur diese Koordinatentransformation: ξi = n∑ k=1 τik ξ′ k , i = 1, . . . , n , (4.40) d.h. es ist ξ = Tξ′ , (4.41) wobei die Transformationsmatrix T regul¨ar ist, so dass also ξ′ = T−1ξ . (4.42) Beachte: In (4.38) wird die neue Basis in der alten dargestellt, aber in (4.40) werden die alten Koordinaten von x als Funktionen der neuen beschrieben. Beweis von Satz 4.13: Nach (4.38) und (4.40) ist x = n∑ k=1 ξ′ k b′ k = n∑ k=1 ξ′ k( n∑ i=1 τik bi) = n∑ i=1 ( n∑ k=1 τik ξ′ k) ︸ ︷︷ ︸ = ξi bi = n∑ i=1 ξi bi . Weil x laut Satz 4.12 nur auf eine Art als Linearkombination der Basis- vektoren bi darstellbar ist, folgt (4.40). Der Nullvektor o wird in den beiden Basen eindeutig dargestellt durch ξ = o bzw. ξ′ = o. Fasst man (4.41) als Gleichungssystem f¨ur ξ′ auf, LA-Skript 4-18 11. April 2016 c⃝M.H. Gutknecht Lineare Algebra (2009) Kapitel 4 — Vektorr¨aume so folgt aus ξ = o also ξ′ = o, d.h. das homogene System hat nur die triviale L¨osung. Also ist T regul¨ar, und es gilt (4.42). Beispiel 4.30: Der in Beispiel 4.21 betrachtete Raum G4 der geraden Polynome vom Grad ≤ 4 hat laut der dort bewiesenen Beziehung (4.22) die zwei Basen {1, t2, t4} und {p1, p2, p3}, wobei nach (4.17) gilt p1(t) :≡ 1 + t2 , p2(t) :≡ 1 − t2 , p3(t) :≡ 1 + t2 + t4 . (4.43) Wir k¨onnen ein beliebiges Polynom g ∈ G4 je auf eindeutige Weise in den beiden Basen darstellen: g(t) = a + b t2 + c t4 = a′ p1(t) + b′ p2(t) + c′ p3(t) . (4.44) Die entsprechenden Koordinatenvektoren sind also ξ = ( a b c )T , ξ′ = ( a′ b′ c′ )T . Die neue Basis {p1, p2, p3} ist in (4.43) gerade durch die alte Basis {1, t2, t4} dargestellt; die Transformationsmatrix ist T = ( τik ) =   1 1 1 1 −1 1 0 0 1   . (4.45) Diese Matrix ist schon in (4.18) und (4.23) als Koeﬃzientenmatrix auf- getreten. Um nun zum Beispiel das in der alten Basis ausgedr¨uckte Po- lynom g(t) = 4 + 6 t2 + 8 t4 (4.46) in der neuen Basis darzustellen, m¨ussen wir (4.42) anwenden mit ξ = ( 4 6 8 )T: ξ′ =   1 1 1 1 −1 1 0 0 1   −1   4 6 8   . Wir haben in (4.18) bereits die LR-Zerlegung von T angegeben. Was zu tun bleibt, ist vor- und r¨uckw¨arts einzusetzen:   1 0 0 1 1 0 0 0 1     ω1 ω2 ω3   =   4 6 8   =⇒ ω =   ω1 ω2 ω3   =   4 2 8   ,   1 1 1 0 −2 0 0 0 1     ξ′ 1 ξ′ 2 ξ′ 3   =   4 2 8   =⇒ ξ′ =   ξ′ 1 ξ′ 2 ξ′ 3   =   −3 −1 8   . ♦ Betrachten wir Basiswechsel und Koordinatentransformation nun noch im Spezialfall V = En. Dann lassen sich die alte und die neue Basis ja als Kolonnen von n × n–Matrizen B und B′ darstellen: B = ( b1 · · · bn ) , B′ = ( b ′ 1 · · · b ′ n ) . (4.47) Damit kann der Basiswechsel (4.38) einfach geschrieben werden als B′ = B T . (4.48) c⃝M.H. Gutknecht 11. April 2016 LA-Skript 4-19 Kapitel 4 — Vektorr¨aume Lineare Algebra (2009) F¨ur die alte und die neue Koordinatendarstellung gelten dann nach (4.39) B ξ = x = B′ ξ′ . (4.49) Die Matrizen B und B′ sind regul¨ar, denn zum Nullvektor o geh¨ort in beiden Basen der eindeutige Koordinatenvektor ξ = ξ′ = o. Aus (4.49) folgt durch Einsetzen von (4.48) sofort B ξ = B T ξ′ , also, weil B regul¨ar ist, ξ = Tξ′ wie in (4.41). Beispiel 4.31: Wir wollen im R3 neben der Standardbasis {e1, e2, e3} noch die “schiefe” Basis {b′ 1, b′ 2, b′ 3} mit b′ 1 = e1 , b′ 2 = e1 + e2 , b′ 3 = e1 + e2 + e3 (4.50) einf¨uhren. Es ist in diesem Falle wegen B = I3 T =   1 1 1 0 1 1 0 0 1   = B′ . (4.51) Die Koordinaten transformieren sich also gem¨ass   ξ1 ξ2 ξ3   =   1 1 1 0 1 1 0 0 1     ξ′ 1 ξ′ 2 ξ′ 3   , und, wie man leicht veriﬁziert, ist   ξ′ 1 ξ′ 2 ξ′ 3   =   1 −1 0 0 1 −1 0 0 1     ξ1 ξ2 ξ3   . ♦ LA-Skript 4-20 11. April 2016 c⃝M.H. Gutknecht Lineare Algebra (2009) Kapitel 5 — Lineare Abbildungen Kapitel 5 Lineare Abbildungen Zum Strukturbegriﬀ “Vektorraum” geh¨ort der Abbildungsbegriﬀ “lineare Abbildung”. Eine Abbildung zwischen zwei Vektorr¨aumen heisst linear, wenn sie “die linearen Beziehungen invariant (un- ver¨andert) l¨asst”. W¨ahlt man in den Vektorr¨aumen Basen, so in- duziert eine solche Abbildung eine lineare Abbildung der Koordi- naten. Falls die R¨aume endlich-dimensional sind, wird diese durch eine Matrix repr¨asentiert. Dies ist, in K¨urze, der grundlegende Zu- sammenhang zwischen linearen Abbildungen und Matrizen. 5.1 Deﬁnition, Beispiele, Matrixdarstellung Zur Erinnerung ein paar Begriﬀe zum Thema “Abbildung” (oder “Funktion”), die nicht auf lineare Abbildungen beschr¨ankt sind: Definition: Es sei F : X → Y irgend eine Abbildung. Die Menge F (X) der Bildpunkte von F heisst Wertebereich [range] von F oder Bild [image] von F . Sie wird auch mit im F bezeichnet: F (X) = im F :≡ {F (x) ∈ Y ; x ∈ X} ⊆ Y . Ist F (X) = Y , so ist F eine Abbildung auf [on] Y und heisst auch surjektiv [surjektive]. Gilt F (x) = F (x′) =⇒ x = x′ , so heisst F eineindeutig [one-to-one] oder injektiv [injektive]. Eine Abbildungen, die surjektiv und injektiv ist, heisst “einein- deutig auf ” [one-to-one onto] oder bijektiv [bijective]. Ist F bijektiv, so ist die inverse Abbildung [inverse map, inverse mapping, inverse transformation] F −1 deﬁniert. ▲ Im allgemeinen ist jedoch F −1 nicht als Abbildung deﬁniert. Der Ausdruck F −1(Z) bezeichnet dann die Menge der Urbilder von Ele- menten z ∈ Z ⊆ Y . Von nun an wollen wir “lineare” Abbildungen betrachten. Bei sol- chen l¨asst man die Klammer um das Argument meist weg. Das heisst, man schreibt einfach F X statt F (X) und F x statt F (x). Das geht aber nicht immer: bei F (x + y) braucht es eine Klammer. Definition: Eine Abbildung F : X → Y , x ↦→ F x zwischen zwei Vektorr¨aumen X und Y (¨uber E) heisst linear [linear transforma- tion], wenn f¨ur alle x, ̃x ∈ X und alle γ ∈ E gilt F (x + ̃x) = F x + F ̃x, F (γx) = γ(F x) . (5.1) c⃝M.H. Gutknecht 11. April 2016 LA-Skript 5-1 Kapitel 5 — Lineare Abbildungen Lineare Algebra (2009) X ist der Deﬁnitionsraum [domain], Y der Bildraum [image space] der Abbildung. Ist der Bildraum Y der Skalarenk¨orper E, so bezeichnet man eine lineare Abbildung als lineares Funktional [linear functional]. Sind Deﬁnitions- und Bildraum Funktionenr¨aume, so spricht man statt von einer linearen Abbildung von einem linearen Operator [linear operator]. Ist der Deﬁnitionsraum gleich dem Bildraum, das heisst ist X = Y , so hat man eine Selbstabbildung. ▲ Die Eigenschaften (5.1) sind gleichwertig mit F (βx + γ̃x) = βF x + γF ̃x (∀x, ̃x ∈ X, ∀β, γ ∈ E). (5.2) Man beachte auch, dass die Bezeichnung “Bildraum” nicht unbe- dingt bedeutet, dass Y = F (X) ist, das heisst, dass es zu jedem y ∈ Y ein x ∈ X mit y = f (x) gibt. Beispiel 5.1: Ist A ∈ Em×n eine vorgegebene m × n–Matrix, so deﬁniert die Zuordnung x ↦→ Ax eine lineare Abbildung des En in den Em, die wir ebenfalls mit A bezeichnen: A : En → Em, x ↦→ Ax . (5.3) y ∈ Em ist genau dann Bild eines x ∈ En, wenn das Gleichungssystem Ax = y eine L¨osung x hat. ♦ Beispiel 5.2: Der ¨Ubergang von einer auf [a, b] stetig diﬀerenzierbaren Funktion f zu ihrer Ableitung f ′ deﬁniert den Ableitungsoperator, eine lineare Abbildung von C1[a, b] in C[a, b]: D : C1[a, b] → C[a, b] , f ↦→ f ′ . (5.4) Ihre Linearit¨at folgt aus (αf + βg)′ = αf ′ + βg′. Allgemeiner ist, bei festen reellen Koeﬃzienten (oder festen stetigen re- ellen Funktionen) c0, c1, . . . , cm, der Diﬀerentialoperator L : Cm[a, b] → C[a, b] , f ↦→ cmf (m) + cm−1f (m−1) + · · · + c1f ′ + c0f (5.5) ein linearer Operator, also eine lineare Abbildung. ♦ Beispiel 5.3: Bei festem t ∈ [a, b] ist die Evaluationsabbildung deﬁniert durch Et : C[a, b] → R, f ↦→ Etf :≡ f (t) (5.6) wegen (αf + βg)(t) = αf (t) + βg(t) ein lineares Funktional. Ist zum Beispiel f (t) = α + βt + γt2 , so gilt E0f = f (0) = α , E2f = f (2) = α + 2β + 4γ . ♦ LA-Skript 5-2 11. April 2016 c⃝M.H. Gutknecht Lineare Algebra (2009) Kapitel 5 — Lineare Abbildungen Beispiel 5.4: Auch der Multiplikationsoperator M : C[a, b] → C[a, b] , f ↦→ g , wo g(t) :≡ t f (t) (5.7) ist ein linearer Operator, denn ( M (βf1 + γf2))(τ ) = β tf1(t) + γ tf2(t)∣ ∣t=τ = β τ f1(τ ) + γ τ f2(τ ) = (β (M f1) + γ (M f2))(τ ) . Die lineare Abbildung F : C1[a, b] ⇒ C[a, b] , f ↦→ g , g(t) :≡ t f ′(t) + f (2) (5.8) kombiniert den Ableitungsoperator D, den Multiplikationsoperator M und die Evaluationsabbildung E2. Unter Verwendung des Verkn¨upfungs- symbols ◦ f¨ur Abbildungen k¨onnte man schreiben F = M ◦ D + E2 . (5.9) Man kann diese Abbildungen nat¨urlich analog f¨ur Funktionen deﬁnieren, die auf ganz R gegeben sind. Man beachte, dass C1[0, 1] nicht etwa ein Unterraum von C1(R) ist. Beschr¨ankt man die Abbildungen auf Unterr¨aume, kann man sich Ge- danken ¨uber den Bildraum machen. Zum Beispiel gilt E2 : Pm → R = P0 , (5.10) M : Pm → Pm+1 , (5.11) D : Pm → Pm−1 , (5.12) F : Pm → Pm , (5.13) wenn F wie in (5.9) deﬁniert ist. ♦ Wir betrachten nun eine beliebige lineare Abbildung F eines n– dimensionalen Vektorraums X in einen m–dimensionalen Vektor- raum Y . Es sei {b1, . . . , bn} eine Basis von X, {c1, . . . , cm} eine Basis von Y : X = span {b1, . . . , bn} F −−−→ Y = span {c1, . . . , cm} (dim X = n) (dim Y = m) Die Bilder F bl ∈ Y der Basis von X lassen sich als Linearkombina- tionen der ck darstellen: F bl = m∑ k=1 akl ck (l = 1, . . . , n) . (5.14) Die m × n–Matrix A = ( akl ) heisst Abbildungsmatrix von F bez¨uglich den gegebenen Basen in X und Y [matrix for F relative to the given bases in X and Y ]. c⃝M.H. Gutknecht 11. April 2016 LA-Skript 5-3 Kapitel 5 — Lineare Abbildungen Lineare Algebra (2009) Beachte: Die Abbildungsmatrix enth¨alt in der l–ten Kolonne die Koordinaten (bez¨uglich der Basis von Y ) des Bildes des l–ten Ba- sisvektors (von X). Umgekehrt geh¨ort zu jeder Matrix A = ( akl ) ∈ Em×n bei vorge- gebenen Basen in X und Y eine durch (5.14) eindeutig bestimmte lineare Abbildung F : X → Y . Durch (5.14) sind die Bilder der Basis von X bestimmt. Das Bild eines beliebigen Vektors x ∈ X ergibt sich auf Grund der geforderten Linearit¨at: Es wird ja x = n∑ l=1 ξl bl (5.15) mit dem Koordinatenvektor ξ :≡ ( ξ1 . . . ξn )T abgebildet auf y :≡ F x = F ( n∑ l=1 ξl bl) = n∑ l=1 ξl F bl = n∑ l=1 ξl m∑ k=1 akl ck = m∑ k=1 ( n∑ l=1 akl ξl) ︸ ︷︷ ︸ ≡: ηk ck , mit dem Koordinatenvektor η :≡ ( η1 . . . ηm )T: y = m∑ k=1 ηk ck . (5.16) Das Bild y = F x hat also den Koordinatenvektor    η1 ... ηm    =    a11 . . . a1n ... ... am1 . . . amn       ξ1 ... ξn    , (5.17) das heisst η = Aξ . (5.18) In Worten: Man erh¨alt den Koordinatenvektor η des Bildes y = F x, indem man die Abbildungsmatrix A auf den Koordinatenvektor ξ von x anwendet. Im Falle einer Selbstabbildung, wo X = Y und damit auch m = n und {b1, . . . , bn} = {c1, . . . , cn} gilt, ist die Abbildungsmatrix A quadratisch und die Koordinatenvektoren ξ und η in (5.18) bezie- hen sich auf dieselbe Basis. LA-Skript 5-4 11. April 2016 c⃝M.H. Gutknecht Lineare Algebra (2009) Kapitel 5 — Lineare Abbildungen Beispiel 5.5: Die vier linearen Abbildungen (5.10)–(5.13), beschr¨ankt auf den Raum P4 (d.h. m = 4), haben bez¨uglich der monomialen Basen 1, t, t2, . . . , tn in Pn (n = 0, 3, 4, 5) folgende Abbildungsmatrizen: E2 = ( 1 2 4 8 16 ) , M =         0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1         , D =     0 1 0 0 0 0 0 2 0 0 0 0 0 3 0 0 0 0 0 4     , F =       1 2 4 8 16 0 1 0 0 0 0 0 2 0 0 0 0 0 3 0 0 0 0 0 4       . ♦ Auch hier lassen sich im Falle X = En, Y = Em die Zusam- menh¨ange etwas eleganter darstellen. Die Basen schreiben wir wie- der als Kolonnen von Matrizen B und C, B = ( b1 . . . bn ) , C = ( c1 . . . cm ) , (5.19) und wir deﬁnieren das Bild F B von B durch F B :≡ ( F b1 . . . F bn ) . (5.20) Dann schreibt sich (5.14)–(5.16) neu als F B = C A , x = Bξ , y = Cη , (5.21) und es folgt Cη = y = F x = F Bξ = C Aξ . (5.22) Da C linear unabh¨angige Kolonnen hat, ist also notwendigerweise η = Aξ, wie in (5.18). Nun zur¨uck zur allgemeinen Theorie. Definition: Eine eineindeutige lineare Abbildung von X auf Y heisst Isomorphismus [isomorphism]. Ist X = Y , so heisst sie Automorphismus [automorphism]. ▲ Lemma 5.1 Ist F : X → Y ein Isomorphismus, so ist die inverse Abbildung F −1 : Y → X linear und auch ein Isomorphismus. Beweis: Es seien y, w ∈ Y , x :≡ F −1y, z :≡ F −1w und β, γ ∈ E. Dann schliessen wir aus der Linearit¨at von f , dass βF −1y + γF −1w = F −1 F (βF −1y + γF −1w) = ↑ F linear F −1(βF F −1y + γF F −1w) = F −1(βy + γw) , woraus folgt, dass F −1 linear ist. Dass mit F auch F −1 bijektiv ist, ist allgemein g¨ultig und klar. c⃝M.H. Gutknecht 11. April 2016 LA-Skript 5-5 Kapitel 5 — Lineare Abbildungen Lineare Algebra (2009) Ist {b1, . . . , bn} eine feste Basis von X, so wird gem¨ass Satz 4.12 jedem x ∈ X ein eindeutiger Koordinatenvektor ξ ∈ En zugeordnet und umgekehrt. Die Abbildung κX : X → En, x ↦→ ξ (5.23) ist also bijektiv. Sie ist zudem linear, wie man sofort sieht, also ein Isomorphismus. Wir nennen sie Koordinatenabbildung [coordi- nate mapping]. Betrachten wir nochmals die der Beziehung (5.18) zu Grunde lie- gende Situation einer linearen Abbildung F : X ⇒ Y und deren Darstellung in festen Basen von X und Y durch die Abbildungs- matrix A. Es seien κX und κY die Koordinatenabbildungen in X bzw. Y . Dann gilt das folgende kommutative Diagramm [com- mutative diagram] X F −−−−−−→ Y κX     ↓ ↑    κ−1 X κY     ↓ ↑    κ−1 Y En A −−−−−−→ Em (5.24) Es bedeutet, dass F = κ−1 Y A κX , A = κY F κ−1 X . (5.25) Dieses Diagramm illustriert auch den Tatbestand, dass F und A ¨aquivalente Darstellungen der linearen Abbildung sind, wobei aller- dings die Matrix A abh¨angig von den gew¨ahlten Basen in X and Y ist. Die Eigenschaften von F kann man meist auch in A ﬁnden. Wir k¨onnen das Diagramm erg¨anzen durch die Angabe typischer Elemente der auftretenden R¨aume: x ∈ X F −−−−−−→ y ∈ Y κX     ↓ ↑    κ−1 X κY     ↓ ↑    κ−1 Y ξ ∈ En A −−−−−−→ η ∈ Em (5.26) Im Falle einer bijektiven Abbildung, also eines Isomorphismus, k¨on- nen wir aus Lemma 5.1 und dem Diagramm leicht schliessen: Korollar 5.2 Wird der Isomorphismus F bez¨uglich festen Basen durch die Matrix A dargestellt, so ist A regul¨ar und die Umkehr- abbildung F −1 wird durch A−1 dargestellt. LA-Skript 5-6 11. April 2016 c⃝M.H. Gutknecht Lineare Algebra (2009) Kapitel 5 — Lineare Abbildungen Beweis: Die gem¨ass Lemma 5.1 existierende Umkehrabbildung F −1 werde durch die Matrix B dargestellt. Dann ist nach dem Diagramm (5.24) analog zu (5.25) B = κX F −1 κ −1 Y , also AB = κY F κ −1 X κX F −1 κ −1 Y = I . Also ist nach Satz 2.17 A invertierbar und A−1 = B. Liegt die Bildmenge einer Abbildung F im Deﬁnitionsbereich einer zweiten Abbildung G, so kann man diese bekanntlich zusammen- setzen zur Komposition G ◦ F . Wir schreiben oft einfach G F . Zur Illustration dieser Situation kann man das Diagramm 5.26 erg¨anzen: G ◦ F −−−−−−−−−−−−−−−−−−−−−−−−−−→ x ∈ X −−−−−−→ F y ∈ Y −−−−−−→ G z ∈ Z κX     ↓ ↑    κ−1 X κY     ↓ ↑    κ−1 Y κZ     ↓ ↑    κ−1 Z ξ ∈ En A −−−−−−→ η ∈ Em B −−−−−−→ ζ ∈ Ep −−−−−−−−−−−−−−−−−−−−−−−−−−→ B A (5.27) Dabei gilt: Lemma 5.3 Sind X, Y, Z Vektorr¨aume ¨uber E und F : X → Y , G : Y → Z lineare Abbildungen, so ist auch G ◦ F : X → Z linear. Sind A und B die Abbildungsmatrizen zu F und G bez¨uglich festen Basen in X, Y, Z, so hat G ◦ F die Abbildungsmatrix BA. Beweis: Die Veriﬁkation des ersten Teils ist einfach. Beim Beweis des zweiten Teils kann man sich wieder auf (5.25) st¨utzen. 5.2 Kern, Bild und Rang In diesem Abschnitt sei stets F : X → Y eine lineare Abbildung. Dabei sei dim X = n, dim Y = m. Definition: Der Kern [kernel] ker F von F ist das Urbild von o ∈ Y . ker F :≡ {x ∈ X; F x = o} ⊆ X , ▲ Lemma 5.4 ker F ist ein Unterraum von X, und im F ist ein Unterraum von Y . Beweis: (i) Es seien x, y ∈ ker F und β, γ ∈ E. Dann ist F (βx + γy) = βF x + γF y = βo + γo = o , also βx + γy ∈ ker F . c⃝M.H. Gutknecht 11. April 2016 LA-Skript 5-7 Kapitel 5 — Lineare Abbildungen Lineare Algebra (2009) (ii) Es seien z = F x ∈ im F und w = F y ∈ im F sowie β, γ ∈ E. Dann gilt βz + γw = βF x + γF y = F (βx + γy) ∈ im Y . Auf ¨ahnliche Weise zeigt man, dass allgemeiner folgendes gilt: Lemma 5.5 Ist U ein Unterraum von X, so ist dessen Bild F U ein Unterraum von Y . Ist W ein Unterraum von im F , so ist dessen Urbild F −1W ein Unterraum von X. Bemerkung: Wie auf Seite 5-1 erw¨ahnt, ist das Urbild F −1W deﬁniert durch F −1W :≡ {x ∈ X ; F x ∈ W } . F −1 ist dabei nicht notwendigerweise eine (eindeutige) Abbildung; f¨ur festes x kann die Menge F −1x aus mehr als einem Element bestehen, insbesondere hier auch aus unendlich vielen. ▼ Beispiel 5.6: F¨ur die durch eine Matrix A deﬁnierte lineare Abbil- dung, f¨ur den Ableitungsoperator D und den Diﬀerentialoperator L aus Beispiel 5.2, f¨ur die Evaluationsabbildung Et aus Beispiel 5.3, f¨ur den Multiplikationsoperator M aus Beispiel 5.4, sowie f¨ur die Koordinaten- abbildung κX ergeben sich f¨ur Kern und Bild die folgenden Unterr¨aume: ker A = L¨osungsmenge des homogenen linearen Gleichungssystems Ax = o ; im A = Menge der rechten Seiten b, f¨ur die Ax = b eine L¨osung hat; ker D = Menge der Konstanten c ∈ C1[a, b] ∼= R ; im D = C[a, b] ; ker L = Menge der L¨osungen f ∈ Cm[a, b] der homogenen linearen Diﬀerentialgleichung Lf = o, d.h. cmf (m) + · · · + c1f ′ + c0f = o ; im L = Menge der Funktionen g ∈ C[a, b], f¨ur die die inhomogene Diﬀerentialgleichung Lf = g, d.h. cmf (m) + · · · + c1f ′ + c0f = g , eine L¨osung f ∈ Cm[a, b] besitzt; ker Et = {f ∈ C[a, b]; f (t) = 0} ; im Et = R ; ker M = {o ∈ C[a, b]} = Nullfunktion ; im M = {g ∈ C[a, b]; f : τ ↦→ g(τ )/τ ∈ C[a, b]} ; ker κX = {o} ⊂ X ; im κX = En . ♦ LA-Skript 5-8 11. April 2016 c⃝M.H. Gutknecht Lineare Algebra (2009) Kapitel 5 — Lineare Abbildungen Lineare Abbildungen sind auch insofern speziell als man am Kern erkennen kann, ob sie eineindeutig sind: Satz 5.6 F ist genau dann injektiv, wenn ker F = {o} ist. Beweis: Ist F injektiv, so hat o ∈ Y ein einziges Urbild, n¨amlich o ∈ X. Ist umgekehrt ker F = {o} und x ̸= y, d.h. x − y ̸= o, so folgt F (x − y) ̸= o, also, weil F linear ist, F x ̸= F y. Wir wollen uns nun den Dimensionen der Unterr¨aume ker F ⊆ X und im F ⊆ Y zuwenden und setzen dabei dim X < ∞ voraus. Es besteht der folgende grundlegende Zusammenhang: Satz 5.7 Es gilt die Dimensionsformel dim X − dim ker F = dim im F . (5.28) Beweis: Es sei {c1, . . . , cr} eine Basis von im F . Wir bezeichnen mit b1, . . . , br einen Satz von Urbildern von c1, . . . , cr. Weiter sei {br+1, . . . , br+k} eine Basis von ker F . Wir zeigen, dass {b1, . . . , br, br+1, . . . , br+k} eine Basis von X ist, dass also r + k = dim X gilt. (i) Wir zeigen zuerst, dass {b1, . . . , br+k} ganz X erzeugt. Zu x ∈ X gibt es γ1, . . . , γr mit F x = γ1c1 + · · · + γrcr = F (γ1b1 + · · · + γrbr) . Also ist x − γ1 b1 − · · · − γrbr ∈ ker F , d.h. es gibt γr+1, . . . , γr+k mit x − γ1 b1 − · · · − γr br = γr+1 br+1 + · · · + γr+k br+k , d.h. x ∈ span {b1, . . . , br+k}. (ii) Weiter zeigen wir, dass b1, . . . , br+k linear unabh¨angig sind. Aus r+k∑ j=1 γjbj = o folgt, dass γ1c1 + · · · + γrcr = F (γ1b1 + · · · + γrbr) = F (−γr+1 br+1 − · · · − γr+k br+k︸ ︷︷ ︸ ∈ ker F ) = o . Also ist γ1 = · · · = γr = 0, denn c1, . . . , cr sind linear unabh¨angig. Unter Verwendung der Voraussetzung folgt weiter, dass r+k∑ j=r+1 γjbj = r+k∑ j=1 γjbj = o , woraus sich wegen der linearen Unabh¨angigkeit von br+1, . . . , br+k ergibt, dass γr+1 = · · · = γr+k = 0. Die Dimensionsformel gibt eine reiche Ernte. Wir bleiben zun¨achst bei den abstrakten linearen Abbildungen. Eine Reihe von Folgerun- gen in der Matrixtheorie werden wir im n¨achsten Abschnitt bespre- chen. Zun¨achst deﬁnieren wir auch f¨ur lineare Abbildungen einen Rang. c⃝M.H. Gutknecht 11. April 2016 LA-Skript 5-9 Kapitel 5 — Lineare Abbildungen Lineare Algebra (2009) Definition: Der Rang [rank] einer linearen Abbildung F ist gleich der Dimension des Bildes von F : Rang F :≡ dim im F . ▲ Damit kann man die Dimensionsformel auch so schreiben: dim X − dim ker F = Rang F . (5.29) Korollar 5.8 Es gelten die folgenden drei ¨Aquivalenzen: (i) F : X → Y injektiv ⇐⇒ Rang F = dim X (ii) F : X → Y bijektiv, d.h. ⇐⇒ Rang F = dim X Isomorphismus = dim Y (iii) F : X → X bijektiv, d.h. ⇐⇒ Rang F = dim X Automorphismus ⇐⇒ ker F = o Beweis: Aussage (i) folgt aus (5.29) und Satz 5.6. Um (ii) zu beweisen, ben¨utzt man zus¨atzlich Lemma 5.4 und die Tatsa- che, dass jeder echte Unterraum von Y (“echt” heisst ̸= Y ) geringere Di- mension als Y hat. Hieraus folgt dann: im F = Y ⇐⇒ Rang F = dim Y . (iii) ergibt sich aus (ii) als Spezialfall und aus (5.29). Definition: Zwei Vektorr¨aume X and Y heissen isomorph [iso- morphic], falls es einen Isomorphismus F : X → Y gibt. ▲ Satz 5.9 Zwei Vektorr¨aume endlicher Dimension sind genau dann isomorph, wenn sie die gleiche Dimension haben. Beweis: (i) Sind X und Y isomorph, so gibt es einen Isomorphismus F : X → Y , und nach Korollar 5.8 (ii) folgt dim X = dim Y . (ii) Ist dim X = dim Y = n, und sind {b1, . . . , bn} und {c1, . . . , cn} Basen von X und Y , so ist durch die Forderung F bk = ck (k = 1, . . . , n) eine lineare Abbildung F eindeutig deﬁniert (vgl. (5.14), wo in diesem Falle A = I ist). Dabei ist nat¨urlich im F = span {c1, . . . , cn} = Y . Zudem gilt ker F = {o}, denn ist x = ∑ ξkbk und F x = o, also o = F x = n∑ k=1 ξk F bk = n∑ k=1 ξk ck , so folgt wegen der linearen Unabh¨angigkeit der Basisvektoren ck, dass ξ1 = · · · = ξn = 0. LA-Skript 5-10 11. April 2016 c⃝M.H. Gutknecht Lineare Algebra (2009) Kapitel 5 — Lineare Abbildungen Korollar 5.10 Sind F : X → Y , G : Y → Z lineare Abbildungen (wobei dim X, dim Y < ∞), so gilt: i) Rang F G ≤ min{Rang F, Rang G} , ii) G injektiv =⇒ Rang GF = Rang F , iii) F surjektiv =⇒ Rang GF = Rang G . Beweis: (i) Die Dimensionsformel (5.29) zeigt, dass Rang G = dim im G ≤ dim Y ist, und dass analog, bei Anwendung auf die Restriktion G|im F von G auf im F , das heisst f¨ur G|im F : im F︸ ︷︷ ︸ ≡: ̃Y ⊆ Y =⇒ im GF︸ ︷︷ ︸ = G( ̃Y ) ⊆ Z folgt: Rang GF = dim im GF︸ ︷︷ ︸ = G( ̃Y ) ≤ dim im F︸ ︷︷ ︸ = ̃Y = Rang F . (5.30) Ferner ist nat¨urlich Rang GF = dim im GF ≤ dim im G = Rang G . (5.31) (ii) Ist G injektiv, so ist G|im F auch injektiv, und nach (5.28) gilt (5.30) mit dem Gleichheitszeichen. (iii) Ist F surjektiv, so ist im GF = im G; folglich gilt (5.31) mit dem Gleichheitszeichen. 5.3 Matrizen als lineare Abbildungen Es sei A = ( akl ) eine m × n–Matrix. Wir bezeichnen ihre n Kolonnen wieder mit a1, . . . , an, so dass A = ( a1 a2 . . . an ) . (5.32) Definition: Der von den Kolonnen von A aufgespannte Unter- raum R(A) :≡ span {a1, . . . , an} heisst Kolonnenraum [column space] oder Wertebereich [range] von A. Der L¨osungsraum L0 des homogenen Systems Ax = o heisst Nullraum [null space] N (A). ▲ Fasst man A als lineare Abbildung A : En → Em, x ↦→ Ax auf, so ist ker A = N (A), und es gilt mit x ≡: ( x1 . . . xn )T im A = {Ax ; x ∈ En} = { n∑ k=1 akxk ; x1, . . . , xn ∈ E} = span {a1, . . . , an} = R(A) . c⃝M.H. Gutknecht 11. April 2016 LA-Skript 5-11 Kapitel 5 — Lineare Abbildungen Lineare Algebra (2009) Wie wir bereits in Beispiel 5.6 betont haben, ist auch im A mit dem Gleichungssystem Ax = b verkn¨upft. Zusammengefasst gilt: Satz 5.11 Fasst man die Matrix A als lineare Abbildung auf, so ist das Bild von A gleich dem Kolonnenraum oder Wertebereich von A, und der Kern von A ist gleich dem Nullraum von A: im A = R(A) , ker A = N (A) . (5.33) Das Gleichungssystem Ax = b ist genau dann l¨osbar, wenn b im Kolonnenraum von A liegt. Eine allf¨allige L¨osung ist genau dann eindeutig, wenn N (A) = {o} ist, das heisst das homogene System nur die triviale L¨osung hat. Die L¨osungsmenge L0 von Ax = o haben wir bereits in Kapitel 1 studiert, siehe insbesondere Korollar 1.5. Im homogenen Fall gilt im Endschema des Gauss-Algortihmus c1 = · · · = cm = 0, das heisst die Vertr¨aglichkeitsbedingungen sind immer erf¨ullt, und die allgemeine L¨osung hat n − r frei w¨ahlbare Parameter. Nehmen wir der Einfachheit halber an, dass die ersten r Variablen x1, . . . , xr gerade die Pivotvariablen sind, d.h. dass A auf eine Matrix R mit der speziellen Zeilenstufenform R :≡             r11 ∗ ∗ · · · · · · · · · ∗ 0 r22 ∗ · · · · · · · · · ∗ ... . . . . . . ... ... . . . rrr ∗ · · · ∗ 0 · · · · · · 0 0 · · · 0 ... ... ... ... 0 · · · · · · 0 0 · · · 0             ← r (5.34) ↑ ↑ r + 1 n reduziert worden ist. (Dies ist stets durch nachtr¨agliches Umnum- merieren der Variablen erreichbar.) Dann sind also xr+1, . . . , xn frei w¨ahlbar. W¨ahlen wir f¨ur irgend ein ℓ ∈ {1, . . . , n − r} x(ℓ) k := { 1 falls k = r + ℓ , 0 falls r < k < r + ℓ or r + ℓ < k ≤ n , und bestimmen wir weiter die Komponenten x(ℓ) r , x(ℓ) r−1, . . . , x(ℓ) 1 durch R¨uckw¨artseinsetzen, so erhalten wir einen L¨osungvektor xℓ. Die so bestimmten n − r L¨osungen x1, . . . , xn−r haben die Form x1 =            ∗ ... ∗ 1 0 ... 0            ← r + 1 , x2 =            ∗ ... ∗ 0 1 ... 0            , . . . , xn−r =            ∗ ... ∗ 0 0 ... 1            (5.35) LA-Skript 5-12 11. April 2016 c⃝M.H. Gutknecht Lineare Algebra (2009) Kapitel 5 — Lineare Abbildungen und sind damit oﬀensichtlich linear unabh¨angig. Weil nur n − r Parameter w¨ahlbar sind, kann es nicht mehr linear unabh¨angige L¨osungen geben. Also bilden diese n − r Vektoren eine Basis des L¨osungsraumes L0, der folglich die Dimension n − r hat: Satz 5.12 Bezeichnet r den Rang der Matrix A und L0 den L¨osungsraum von Ax = o, so ist dim L0 ≡ dim N (A) ≡ dim ker A = n − r . (5.36) Mit dem Identit¨atssymbol ≡ wollen wir hier betonen, dass im Prin- zip drei ¨aquivalente Formulierungen vorliegen: es gilt nicht nur dim L0 = dim N (A) = dim ker A, sondern es ist aufgrund der Deﬁnitionen L0 = N (A) ≡ ker A. Beispiel 5.7: In unserem Beispiel 1.4 auf Seite 1-11 in Kapitel 1 das wir auch als Beispiel 3.4 in Kapitel 3, Seite 3-11, betrachtet haben, ist r = 5 und n − r = 9 − 5 = 4. Das zugeh¨orige homogene System wird durch die Gauss-Elimination reduziert auf die (1.18) entsprechende Zeilenstufenform x1 x2 x3 x4 x5 x6 x7 x8 x9 1 1❦ 0 5 0 4 0 0 1 0 0 0 5❦ 4 3 2 1 0 0 0 0 0 0 0 5❦ 0 4 3 0 2 0 0 0 0 0 0 5❦ 3 2 1 0 0 0 0 0 0 0 1❦ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 (5.37) aus der man leicht eine Basis von L0 konstruiert. Dazu setzt man nach- einander je eine der freien Variablen x3, x5, x8 und x9 eins, die anderen null und rechnet die Werte der restlichen Variablen aus. Man beachte, dass die Zeilenstufenform nicht die vereinfachte Gestalt (5.34) hat, und deshalb auch die Basis nicht genau die gleiche Gestalt wie in (5.35) hat, wobei hier allerdings die vielen Nullen in R zu vielen zus¨atzlichen Nullen in der Basis f¨uhren: x1 =               −5 − 4 5 1 0 0 0 0 0 0               , x2 =               −4 − 2 5 0 0 1 0 0 0 0               , x3 =               −1 − 14 125 0 8 25 0 − 2 5 0 1 0               , x4 =               0 23 125 0 − 6 25 0 − 1 5 0 0 1               . (5.38) ♦ Durch Vergleich von (5.36) mit der Dimensionsformel (5.28) sehen wir, dass r gleich dem Rang der durch A gegebenen linearen Ab- bildung ist. c⃝M.H. Gutknecht 11. April 2016 LA-Skript 5-13 Kapitel 5 — Lineare Abbildungen Lineare Algebra (2009) Bezeichnen wir noch den durch die Zeilenvektoren von A aufge- spannten Unterraum des En als Zeilenraum [row space] von A, so erhalten wir nun eine ganze Reihe ¨aquivalenter Aussagen: Satz 5.13 Der Rang einer m × n Matrix A ist gleich (i) der Anzahl Pivotelemente bei der Reduktion von A auf Zei- lenstufenform; (ii) dem Rang der linearen Abbildung A : En → Em, deﬁniert als dim im A; (iii) der Dimension des Kolonnenraums (“Kolonnenrang”), deﬁ- niert als Anzahl linear unabh¨angiger Kolonnen (∈ Em); (iv) der Dimension des Zeilenraums (“Zeilenrang”), deﬁniert als Anzahl linear unabh¨angiger Zeilen (∈ En). Beweis: (i) ist unsere Deﬁnition des Ranges r von A aus Kapitel 1. (ii) folgt aus der Dimensionsformel (5.28) und Satz 5.12: dim im A = n − dim ker A = n − (n − r) = r . Zu (iii): Nach Satz 5.11 ist im A der Kolonnenraum von A, also ist nach (ii) dessen Dimension gleich r. Zu (iv): Bei der Gauss-Elimination von Kapitel 1 l¨asst sich leicht fol- gern, dass der Zeilenraum von A identisch ist mit dem Zeilenraum der Zeilenstufenmatrix R. Dieser hat die Dimension r. Ein alternativer Beweis von Teil (iv) folgt aus der LR–Zerlegung: Der Zeilenraum von A ist der Kolonnenraum, d.h. der Bildraum von AT. Aus der Beziehung A = P−1LR = PTLR (5.39) von Satz 3.3 ergibt sich aber AT = RTLTP, wobei P und L re- gul¨ar sind, also auch LT. Hieraus folgt, dass R(AT) = R(RT). Wie erw¨ahnt ist aber nat¨urlich dim R(RT) = r. Die Aussagen (iii) und (iv) implizieren die “Formel” “Zeilenrang” = “Kolonnenrang” . Wenn man noch ber¨ucksichtigt, dass f¨ur eine komplexe Matrix nat¨urlich Rang A = Rang A gilt, folgt sofort: Korollar 5.14 Rang AT = Rang AH = Rang A . Wir haben oben eine Basis von ker A ≡ N (A) konstruiert. Wie ﬁndet man wohl eine von im A ≡ R(A)? Das ist einfach, wenn wir an die Zeilenstufenform (1.28) und die entsprechende LR–Zerlegung (3.23) denken. Wie in Beispiel 5.6 LA-Skript 5-14 11. April 2016 c⃝M.H. Gutknecht Lineare Algebra (2009) Kapitel 5 — Lineare Abbildungen vermerkt, ist R(A) ≡ im A gleich der Menge der b ∈ R m, f¨ur die Ax = b eine L¨osung x hat. Bei der Reduktion auf Zeilenstufenform wird b durch c = L−1Pb ersetzt, und anschliessend ﬁndet man alle x zu einem festen b bzw. c durch R¨uckw¨artseinsetzen. Dabei gibt es zu jedem b, f¨ur das eine L¨osung existiert, auch eine spezielle L¨osung x, in der alle freien Variablen null sind. Mit anderen Worten, wir erhalten alle c und b auch, wenn wir in R bzw. A die Kolonnen mit den freien Variablen streichen, das heisst, wenn wir nur die r Pivotkolonnen behalten, mit denen wir eine m × r Matrix ̃A bilden k¨onnen. Von diesen l¨asst sich anderseits keine streichen, ohne dass die Menge zul¨assiger Vektoren b reduziert wird. Es gilt also: Satz 5.15 F¨ur den Kolonnenraum einer m × n–Matrix A gilt: im A ≡ R(A) = R( ̃A) = span {an1, . . . , anr } , (5.40) worin an1, . . . , anr die Pivotkolonnen von A sind, und ̃A die dar- aus gebildete m × r–Matrix bezeichnet. Beispiel 5.8: Wir setzen das oben behandelte Beispiel 5.7 fort. Gem¨ass (5.37) ist r = 5 und sind x1, x2, x4, x6, x7 die Pivotvariablen, also a1, a2, a4, a6, a7 die Pivotkolonnen von A. Sie bilden gem¨ass (5.40) eine Basis des Kolonnenraumes und ergeben zusammen die Matrix ̃A: ̃A =           1 0 0 0 0 1 5 3 1 0 1 10 11 6 3 0 5 18 18 12 0 0 5 24 16 1 10 21 24 16 0 5 13 24 17           . (5.41) ♦ Wir werden in Abschnitt 7.4 eine weitere Methode zur Konstruktion einer Basis von R(A) kennen lernen, die direkt eine orthogonale Basis liefert. Sie ist im praktischen Rechnen vorzuziehen. Allgemeine Aussagen ¨uber den Rang eines Produktes AB ergeben sich sofort aus Lemma 5.3, Korollar 5.10 und Satz 5.13 (ii): Satz 5.16 Es seien A ∈ Em×n, B ∈ Ep×m. Dann gilt: (i) Rang BA ≤ min{Rang B, Rang A}, (ii) Rang B = m (≤ p) =⇒ Rang BA = Rang A, (iii) Rang A = m (≤ n) =⇒ Rang BA = Rang B. Beweis: (i) ergibt sich aus den genannten Aussagen. (ii) Bei der Anwendung von Korollar 5.10 ben¨utze man noch, dass nach Satz 5.6 und Satz 5.12 folgt B : Em → Ep injektiv ⇐⇒ ker B = {o} ⇐⇒ Rang B = m . c⃝M.H. Gutknecht 11. April 2016 LA-Skript 5-15 Kapitel 5 — Lineare Abbildungen Lineare Algebra (2009) (iii) Hier wird zus¨atzlich ben¨utzt, dass nat¨urlich gilt: A : En → Em surjektiv ⇐⇒ im A = Em ⇐⇒ Rang A = m . Im Falle von quadratische Matrizen ist vieles etwas einfacher, be- sonders wenn sie regul¨ar sind. Satz 5.16 vereinfacht sich so: Korollar 5.17 Es seien A ∈ Em×m, B ∈ Em×m. Dann gilt: (i) Rang BA ≤ min{Rang B, Rang A}, (ii) Rang B = m =⇒ Rang BA = Rang A, (iii) Rang A = m =⇒ Rang BA = Rang B. Im folgenden Satz fassen wir nochmals eine Reihe von gleichwerti- gen Aussagen zusammen. Satz 5.18 F¨ur eine quadratische Matrix A ∈ En×n sind folgende Aussagen ¨aquivalent: (i) A ist invertierbar; (ii) A ist regul¨ar; (iii) Rang A = n ; (iv) die n Kolonnenvektoren von A sind linear unabh¨angig; (v) die n Zeilenvektoren von A sind linear unabh¨angig; (vi) im A ≡ R(A) = En ; (vii) ker A ≡ N (A) = {o} ; (viii) die lineare Abbildung A : En → En ist ein Automorphismus; (ix) A ist die Transformationsmatrix einer Koordinatentransfor- mation in En. Beweis: (i) ⇐⇒ (ii) auf Grund unserer Deﬁnition von “regul¨ar”. (ii) ⇐⇒ (iii) nach Korollar 1.4 und Satz 2.15. (iii) ⇐⇒ (iv) ⇐⇒ (v) ergibt sich aus Satz 5.13 (iii) und (iv). (iv) ⇐⇒ (vi), weil n Vektoren genau dann den En erzeugen, wenn sie linear unabh¨angig sind. (iii) ⇐⇒ (vii) nach der Dimensionsformel (5.36); oder: weil beide Aus- sagen ¨aquivalent damit sind, dass Ax = o nur die triviale L¨osung hat. (iii) ⇐⇒ (viii) folgt aus Korollar 5.8 (iii) und Satz 5.13 (ii). (viii) ⇐⇒ (ix) ist ziemlich klar, soll aber genauer betrachten werden: A = ( akl ) ist Transformationsmatrix genau dann, wenn aus der “al- ten” Basis b1, . . . , bn gem¨ass (4.38), LA-Skript 5-16 11. April 2016 c⃝M.H. Gutknecht Lineare Algebra (2009) Kapitel 5 — Lineare Abbildungen b′ l :≡ n∑ k=1 akl bk (l = 1, . . . , n) (5.42) eine “neue” Basis b′ 1, . . . , b′ n hervorgeht, d.h. genau dann, wenn b′ 1, . . . , b′ n den ganzen Raum En erzeugen (womit sie automatisch linear unabh¨angig sind). Wir k¨onnen b′ l als Bild von bl bei einer linearen Abbildung F auﬀassen: F bl :≡ b′ l, vgl. (5.14). Es ist dann span {b′ 1, . . . , b′ n} = En gleichbedeutend mit Rang F = n, was gem¨ass Korollar 5.8 heisst, dass F ein Automorphismus ist. Gem¨ass (5.42) ist auch Abl = b′ l, also, wenn man A als lineare Abbildung auﬀasst, A = F . 5.4 Aﬃne R¨aume und die allgemeine L¨osung eines inhomogenen Gleichungssystems Definition: Ist U ein echter Unterraum eines Vektorraumes V und u0 ∈ V , so heisst die Menge u0 + U :≡ {u0 + u ; u ∈ U } (5.43) ein aﬃner Teilraum [aﬃne (sub)space]. Ist F : X → Y eine lineare Abbildung und y0 ∈ Y , so ist H : X → y0 + Y , x ↦→ y0 + F x (5.44) eine aﬃne Abbildung [aﬃne mapping]. ▲ Beispiel 5.9: Jede Gerade und jede Ebene im R3 ist ein aﬃner Teil- raum, auch wenn sie nicht durch den Ursprung verl¨auft. Eine Translation x ↦→ y0 + x ist eine spezielle aﬃne Abbildung. ♦ Man beachte, dass ein aﬃner Teilraum im allgemeinen kein Unter- raum ist, n¨amlich stets wenn u0 ̸∈ U gilt. Ebenso ist eine aﬃne Abbildung nicht linear, wenn y0 ̸= o ist, denn H(o) = y0 ̸= o. Dies gilt selbst, wenn y0 ∈ im F ⊂ Y und damit das Bild im H = im F ein Unterraum ist. Allgemein ist im H = y0 + im F ein aﬃner Teilraum. Aﬃne Teilr¨aume und aﬃne Abbildungen spielen eine grosse Rolle bei geometrischen Aufgaben und Anwendungen. Wir betrachten hier nur den Zusammenhang mit linearen Gleichungssystemen. Satz 5.19 Es sei x0 irgend eine L¨osung des inhomogenen Systems Ax = b, und es bezeichne L0 den L¨osungsraum des homogenen Systems Ax = o. Dann ist die L¨osungsmenge Lb von Ax = b gleich dem aﬃnen Teilraum Lb = x0 + L0 . (5.45) Mit anderen Worten: Die allgemeine L¨osung eines inhomogenen li- nearen Gleichungssystems l¨asst sich darstellen als Summe einer spe- ziellen L¨osung (Partikul¨arl¨osung) des Systems und der allgemeinen L¨osung des homogenen Systems. c⃝M.H. Gutknecht 11. April 2016 LA-Skript 5-17 Kapitel 5 — Lineare Abbildungen Lineare Algebra (2009) Analoge Aussagen gibt es f¨ur andere L¨osungsmengen linearer Be- ziehungen. Zum Beispiel f¨ur lineare Diﬀerentialgleichungen, und zwar sowohl f¨ur eine Diﬀerentialgleichung h¨oherer Ordnung als auch f¨ur Systeme erster (oder h¨oherer) Ordnung. In jedem Falle ist die L¨osungsmenge ein aﬃner Teilraum eines geeignet gew¨ahlten Funk- tionenraums. Beweis von Satz 5.19: Aus Ax0 = b und Ax = o folgt A(x0+x) = b ; es gilt also Lb ⊇ x0 +L0. Ist umgekehrt x1 irgend eine L¨osung von Ax = b, so gilt A(x1 − x0) = o, es ist also x1 − x0 ∈ L0, d.h., x1 ∈ x0 + L0 ; also gilt auch Lb ⊆ x0 + L0. Beispiel 5.10: Wir betrachten nochmals das oben behandelte Beispiel 5.7, interessieren uns nun aber f¨ur die allgemeine L¨osung des inhomo- genen Systems (1.17), die wir ¨ubrigens ja bereits in Kapitel 1 bestimmt haben, siehe (1.21). Dort haben wir insbesondere die spezielle L¨osung x0 = ( 1 43 125 0 4 25 0 − 1 5 1 0 0 )T gefunden, in der alle freien Variablen null gesetzt sind. Anderseit haben wir in (5.38) eine Basis von L0 berechnet. In Matrixschreibweise l¨asst sich damit die allgemeine L¨osung ausdr¨ucken als x = x0 + ( x1 x2 x3 x4 )     α β γ δ     (5.46) =               1 43 125 0 4 25 0 − 1 5 1 0 0               +               −5 −4 −1 0 − 4 5 − 2 5 − 14 125 23 125 1 0 0 0 0 0 8 25 − 6 25 0 1 0 0 0 0 − 2 5 − 1 5 0 0 0 0 0 0 1 0 0 0 0 1                   α β γ δ     (5.47) mit frei w¨ahlbaren Parametern α, β, γ, δ. Dies stimmt genau mit der allgemeinen L¨osung in (1.21) ¨uberein. ♦ 5.5 Die Abbildungsmatrix bei Koordinaten- transformation Es seien X und Y Vektorr¨aume der Dimensionen n bzw. m, und weiter seien F : X → Y , x ↦→ y eine lineare Abbildung, A : En → Em , ξ ↦→ η die entsprechende Abbildungsmatrix, T : En → En , ξ′ ↦→ ξ eine Transformationsmatrix im En, S : Em → Em , η′ ↦→ η eine Transformationsmatrix im Em. LA-Skript 5-18 11. April 2016 c⃝M.H. Gutknecht Lineare Algebra (2009) Kapitel 5 — Lineare Abbildungen Wir wissen aus Abschnitt 4.4 wie Basiswechsel in X und Y durch Koordinatentransformationen T und S ausgedr¨uckt werden. Wir wollen nun die Auswirkungen auf die Abbildungsmatrix A betrach- ten. Dazu gen¨ugt es, das kommutative Diagramm (5.26) nach unten zu erweitern: x ∈ X F −−−−−−→ lin. Abb. y ∈ Y κX     ↓ ↑    κ−1 X κY     ↓ ↑    κ−1 Y (Koordinaten- abbildung bzgl. “alten” Basen) ξ ∈ En A −−−−−−→ Abb.matrix η ∈ Em (Koordinaten bzgl. “alten” Basen) T−1    ↓ ↑    T S −1    ↓ ↑    S (Koordinaten- transformation) ξ′ ∈ En B −−−−−−→ Abb.matrix η′ ∈ Em (Koordinaten bzgl. “neuen” Basen) (5.48) Es gilt also y = F x , η = A ξ , ξ = T ξ′ , η = S η′ , η′ = B ξ′ . (5.49) Diesen Formeln oder dem Diagramm entnimmt man, dass f¨ur die Abbildungsmatrix B, die die Abbildung F bez¨uglich den “neuen” Basen in Em und En beschreibt, gilt B ξ′ = η′ = S −1 η = S −1 A ξ = S −1 A T ξ′ Da ξ′ beliebig ist, ergibt sich B = S −1AT, A = SBT−1 . (5.50) Aus Satz 5.16 folgt im ¨ubrigen wegen Rang S −1 = Rang T = n, dass Rang B = Rang A ist, und in ¨ahnlicher Weise folgt aus Korollar 5.10, dass Rang F = Rang A ist: Rang F = Rang A = Rang B . (5.51) Im Falle einer linearen Abbildung von X in sich, ist nat¨urlich Y = X, κY = κX, S = T. Aus (5.50) wird damit B = T−1AT, A = TBT−1 . (5.52) Da B und A Matrixdarstellungen der selben linearen Selbstabbil- dung sind, kann man erwarten, dass sie gewisse ¨ahnliche Eigenschaf- ten haben. Das wird sich best¨atigen. Es gilt folgende Terminologie: c⃝M.H. Gutknecht 11. April 2016 LA-Skript 5-19 Kapitel 5 — Lineare Abbildungen Lineare Algebra (2009) Definition: Die n × n–Matrizen A und B heissen ¨ahnlich [si- milar], falls es eine regul¨are Matrix T gibt, so dass eine der Bezie- hungen (5.52) gilt (und damit auch die andere). Der ¨Ubergang A ↦→ B = T−1AT heisst ¨Ahnlichkeitstransfor- mation [similarity transformation]. Beispiel 5.11: Wir betrachten den Ableitungsoperator D auf dem in den Beispielen 4.21 und 4.30 von Kapitel 4 betrachteten Unterraum G4 der geraden Polynome vom H¨ochstgrad 4. Er hat die monomiale Basis 1, t2, t4. F¨ur jedes p ∈ G4 ist die Ableitung p′ ein ungerades Polynom vom H¨ochstgrad 3, liegt also im entsprechenden Unterraum U3, der von den zwei Monomen t und t3 aufgespannt wird. Bez¨uglich diesen Basen wird D : p ∈ G4 ↦→ p′ ∈ U3 dargestellt durch die 2 × 3–Matrix A = ( 0 2 0 0 0 4 ) , (5.53) die man im ¨ubrigen auch aus der Matrix D in Beispiel 5.5 bekommt, indem man die erste und die dritte Zeile sowie die zweite und die vierte Kolonne streicht. In G4 ersetzen wir nun die monomiale Basis durch jene aus (4.17): p1(t) :≡ 1 + t2 , p2(t) :≡ 1 − t2 , p3(t) :≡ 1 + t2 + t4 . Diese Basistransformation wird durch die Matrix T = ( τik ) =   1 1 1 1 −1 1 0 0 1   dargestellt, wie wir ja auch schon in (4.45) gesehen haben. (Zur Erinne- rung: In den Kolonnen stehen die Koordinaten der neuen Basis bez¨uglich der alten Basis.) In U3 w¨ahlen wir als neue Basis q1(t) :≡ t , q2(t) :≡ 3t + 2t3 . (5.54) Die entsprechende Transformationsmatrix und ihre Inverse sind S = ( σik ) = ( 1 3 0 2 ) , S−1 = ( 1 − 3 2 0 1 2 ) . (5.55) Um den Ableitungsoperator bez¨uglich den Basen {p1, p2, p3} von G4 und {q1, q2} von U3 darzustellen, braucht man somit gem¨ass (5.50) die Ab- bildungsmatrix B = S−1A T = ( 1 − 3 2 0 1 2 ) ( 0 2 0 0 0 4 )   1 1 1 1 −1 1 0 0 1   = ( 2 −2 −4 0 0 2 ) . (5.56) ♦ LA-Skript 5-20 11. April 2016 c⃝M.H. Gutknecht Lineare Algebra (2009) Kapitel 5 — Lineare Abbildungen Wie weit l¨asst sich die Abbildungsmatrix durch geeignete Wahl der Basen vereinfachen? Im Falle Y = X werden wir uns sp¨ater im Zusammenhang mit dem Eigenwertproblem ausgiebig mit dieser Frage besch¨aftigen. Der Fall Y ̸= X ist einfacher, denn man kann statt einer zwei Basen geschickt w¨ahlen: Satz 5.20 Es sei F : X → Y eine lineare Abbildung, wobei dim X = n, dim Y = m, Rang F = r gelte. Dann besitzt F bez¨uglich geeignet gew¨ahlten Basen in X und Y die Abbildungs- matrix A = ( Ir O O O ) . (5.57) Beweis: Es seien {b1, . . . , bn} und {c1, . . . , cr} gleich gew¨ahlt wie im Beweis von Satz 5.7. Zudem werde {c1, . . . , cr} gem¨ass Satz 4.9 zu einer Basis {c1, . . . , cm} von Y erg¨anzt. Dann ist also F bj = { cj , j = 1, . . . , r, o , j = r + 1, . . . , n. Da in der Kolonne j der Abbildungsmatrix gerade die Koordinaten von F bj (bez¨uglich der Basis des Bildraumes) stehen, bekommen wir genau die Matrix A von (5.57). Beispiel 5.12: Im Falle des Ableitungsoperators D : p ∈ G4 ↦→ p′ ∈ U3 aus Beispiel 5.11 gen¨ugt es, die monomiale Basis von G4 umzuordnen in {t4, t2, 1} und die monomiale Basis von U3 zus¨atzlich zu skalieren: {4t3, 2t}. Dann hat man als Abbildungsmatrix statt A aus (5.53) einfach ̃A = ( 1 0 0 0 1 0 ) . (5.58) ♦ c⃝M.H. Gutknecht 11. April 2016 LA-Skript 5-21 Kapitel 5 — Lineare Abbildungen Lineare Algebra (2009) LA-Skript 5-22 11. April 2016 c⃝M.H. Gutknecht Lineare Algebra (2009) Kapitel 6 — Vektorr¨aume mit Skalarprodukt Kapitel 6 Vektorr¨aume mit Skalarprodukt Wir verallgemeinern in diesem Kapitel verschiedene in Kapitel 2 f¨ur n–Vektoren eingef¨uhrten Begriﬀe wie Norm, Skalarprodukt und Orthogonalit¨at. Wir wissen ja schon, siehe Satz 5.9, dass jeder n– dimensionale Vektorraum V ¨uber R bzw. C isomorph ist zum R n beziehungsweise C n im Sinne, dass es eine eineindeutige lineare Abbildung von V auf R n oder C n gibt. Die Existenz eines Ska- larproduktes macht einen n–dimensionalen Vektorraum aber “noch ¨ahnlicher” zum R n oder C n. Es ist zum Beispiel naheliegend, dass man in solchen R¨aumen orthogonale Basen betrachtet und an linea- ren Abbildungen interessiert ist, die L¨angen und Winkel erhalten. 6.1 Normierte Vektorr¨aume Definition: Eine Norm [norm] in einem Vektorraum V ist eine Funktion ∥.∥ : V → R , x ↦→ ∥x∥ (6.1) mit den folgenden Eigenschaften: (N1) Sie ist positiv deﬁnit: ∥x∥ ≥ 0 f¨ur alle x ∈ V , ∥x∥ = 0 =⇒ x = o . (N2) Sie ist dem Betrage nach homogen: ∥α x∥ = |α| ∥x∥ f¨ur alle x ∈ V, α ∈ E . (N3) Die Dreiecksungleichung [triangle inequality] gilt: ∥x + y∥ ≤ ∥x∥ + ∥y∥ f¨ur alle x, y ∈ V . (6.2) Ein Vektorraum mit einer Norm heisst normierter Vektorraum [normed vector space] oder normierter linearer Raum [normed linear space]. ▲ Man beachte, dass die Norm-Axiome (N1)–(N3) genau jene sind, die wir in Satz 2.12 als Eigenschaften der 2–Norm f¨ur R n und C n aufgez¨ahlt haben. Dort wurde die 2–Norm mit Hilfe des Euklidi- schen Skalarprodukt ⟨., .⟩ deﬁniert: ∥x∥ :≡ √ ⟨x, x⟩. Dies werden wir im folgenden Abschnitt verallgemeinern. Aber hier wird die Norm ohne Bezug auf ein Skalarprodukt deﬁniert. c⃝M.H. Gutknecht 11. April 2016 LA-Skript 6-1 Kapitel 6 — Vektorr¨aume mit Skalarprodukt Lineare Algebra (2009) Beispiel 6.1: Auf dem in Beispiel 4.2 betrachteten Raum C[a, b] der auf dem Intervall [a, b] deﬁnierten und dort stetigen reell- oder komplex- wertigen1 Funktionen werden durch ∥f ∥1 :≡ ∫ b a |f (t)| dt (6.3) und ∥f ∥∞ :≡ max t∈[a,b] |f (t)| (6.4) zwei verschiedene Normen deﬁniert, die L1–Norm und die L∞–Norm. Die zweite heisst auch Maximumnorm [maximum norm]. Ersetzt man in (6.4) das Maximum durch ein Supremum, so k¨onnen beide Normen auch auf einem “gr¨osseren”, unstetige Funktionen enthaltenden Vektor- raum deﬁniert werden. ♦ 6.2 Vektorr¨aume mit Skalarprodukt Definition: Ein Skalarprodukt [inner product] in einem rellen oder komplexen Vektorraum ist eine Funktion von zwei Variablen ⟨·, ·⟩ : V × V → E, x, y ↦−→ ⟨x, y⟩ (6.5) mit folgenden Eigenschaften: (S1) Es ist linear im zweiten Faktor: ⟨x, y + z⟩ = ⟨x, y⟩ + ⟨x, z⟩ f¨ur alle x, y, z ∈ V, ⟨x, α y⟩ = α ⟨x, y⟩ f¨ur alle x, y ∈ V, α ∈ E . (S2) Falls E = R, ist es symmetrisch: ⟨x, y⟩ = ⟨y, x⟩ f¨ur alle x, y ∈ V. Falls E = C, ist es Hermitesch: ⟨x, y⟩ = ⟨y, x⟩ f¨ur alle x, y ∈ V. (S3) Es ist positiv deﬁnit: ⟨x, x⟩ ≥ 0 f¨ur alle x ∈ V, ⟨x, x⟩ = 0 =⇒ x = o . V mit ⟨·, ·⟩ ist ein Vektorraum mit Skalarprodukt [inner pro- duct space]. Ist V endlichdimensional, nennt man V auch • falls E = R: Euklidischer Vektorraum2 [Euclidean vector space] oder orthogonaler Vektorraum, • falls E = C: unit¨arer Vektorraum [unitary vector space]. ▲ 1In diesem Kapitel wollen wir explizit den Fall komplexwertiger Funktionen ber¨ucksichtigen, da sie die Modiﬁkation einiger Formeln erfordern. 2Euklidische Vektorr¨aume sind also Verallgemeinerungen des R n bzw. C n. Letztere haben wir als rellen bzw. komplexen n–dimensionalen Euklidischen Raum bezeichnet. LA-Skript 6-2 11. April 2016 c⃝M.H. Gutknecht Lineare Algebra (2009) Kapitel 6 — Vektorr¨aume mit Skalarprodukt Aus (S1) und (S3) folgt, dass ⟨x, αy + βz⟩ = α ⟨x, y⟩ + β ⟨x, z⟩ , ⟨αw + βx, y⟩ = α ⟨w, y⟩ + β ⟨x, y⟩ , (6.6) wobei im Falle E = R nat¨urlich α = α und β = β ist, das Skalar- produkt also auch im ersten Argument linear ist. Bemerkung: Gem¨ass (S1) ist das Skalarprodukt linear im zwei- ten Argument. Stattdessen wird auch im Falle E = C oft Linearit¨at im ersten Argument verlangt. In der komplexen Matrizenrechnung ist aber unsere Version von Vorteil, denn im Falle von n-Vektoren gilt so direkt ⟨y, x⟩ = yHx. ▼ In diesem Kapitel sind V , X, Y, . . . stets Vektorr¨aume mit Ska- larprodukt, und zwar ist ⟨. , .⟩ oder ⟨. , .⟩V das Skalarprodukt in V , ⟨. , .⟩X jenes in X, usw. Definition: Die Norm [norm] oder L¨ange [length] eines Vektors x in einem Vektorraum V mit Skalarprodukt ist ∥x∥ :≡ √ ⟨x, x⟩ . (6.7) ▲ Man zeigt wie im Beweis von Satz 2.12, dass die in Abschnitt 6.1 angegebenen Axiome (N1)–(N3) f¨ur die Norm (6.7) stets erf¨ullt sind. Beispiel 6.2: Rn mit dem Skalarprodukt ⟨x, y⟩ :≡ xTy ist ein Eukli- discher Vektorraum. Cn mit dem Skalarprodukt ⟨x, y⟩ :≡ xHy ist ein unit¨arer Vektorraum. ♦ Beispiel 6.3: Auf dem Raum C[a, b] der auf dem Intervall [a, b] deﬁ- nierten und dort stetigen reell- oder komplexwertigen Funktionen wird durch ⟨f, g⟩ :≡ ∫ b a f (t) g(t) dt (6.8) ein Skalarprodukt deﬁniert. Die zugeh¨orige Norm ist ∥f ∥2 :≡ √ ∫ b a |f (t)|2 dt . (6.9) Ist w ∈ C[a, b] irgend eine positive reelle Gewichtsfunktion [weight function], so ist auch durch ⟨f, g⟩w :≡ ∫ b a f (t) g(t) w(t) dt (6.10) ein Skalarprodukt erkl¨art. Weiter wird auf C[−1, 1] zum Beispiel durch ⟨f, g⟩ :≡ ∫ 1 −1 f (t) g(t) dt √1 − t2 (6.11) ein Skalarprodukt deﬁniert, denn diese Integrale existieren alle. ♦ c⃝M.H. Gutknecht 11. April 2016 LA-Skript 6-3 Kapitel 6 — Vektorr¨aume mit Skalarprodukt Lineare Algebra (2009) Grundlegende Folgerungen aus den Axiomen des Skalarproduktes lassen sich genau gleich herleiten, wie wir das in Kapitel 2 f¨ur das spezielle Euklidische Skalarprodukt von R n und C n taten. Als Beispiele geben wir die allgemeine Form der Schwarzschen Unglei- chung, die Deﬁnitionen des Winkels und der Orthogonalit¨at, sowie die allgemeine Form des Satzes von Pythagoras an: Satz 6.1 F¨ur alle Paare x, y ∈ V gilt die Schwarzsche Unglei- chung (oder Cauchy-Bunjakovski-Schwarz-Ungleichung) | ⟨x, y⟩ | ≤ ∥x∥ ∥y∥ . (6.12) Das Gleichheitszeichen gilt genau dann, wenn x und y linear abh¨angig sind. Beweis: Der Beweis verl¨auft v¨ollig analog zu jenem von Satz 2.11. Definition: Der Winkel [angle] ϕ (0 ≤ ϕ ≤ π) zwischen zwei Vektoren x und y ist gegeben durch ϕ :≡ arccos ⟨x, y⟩ ∥x∥ ∥y∥ , falls E = R , (6.13) ϕ :≡ arccos Re ⟨x, y⟩ ∥x∥ ∥y∥ , falls E = C . (6.14) Zwei Vektoren x, y ∈ V sind zueinander orthogonal [orthogonal] (oder: senkrecht [perpendicular]), falls ⟨x, y⟩ = 0. Symbolisch: x ⊥ y. Zwei Teilmengen M , N ⊆ V sind zueinander orthogonal [ortho- gonal], falls ⟨x, y⟩ = 0 f¨ur alle x ∈ M und alle y ∈ N . Symbolisch: M ⊥ N . ▲ Bemerkung: M und N k¨onnen insbesondere Unterr¨aume sein. Der Nullvektor ist orthogonal zu allen Vektoren; der Nullraum ist orthogonal zu allen Unterr¨aumen. ▼ F¨ur senkrechte Vektoren kann man nun allgemein den Satz von Pythagoras beweisen, genau wie Satz 2.13: Satz 6.2 (Satz von Pythagoras) In einem Vektorraum mit Skalarprodukt gilt: x ⊥ y =⇒ ∥x ± y∥ 2 = ∥x∥ 2 + ∥y∥ 2 . (6.15) Beispiel 6.4: Wir betrachten im Raum C[0, 1], versehen mit dem Skalarprodukt (6.8) die zwei Funktionen f (t) : t ↦→ t2 und g(t) : t ↦→ t3. Hier gilt: ⟨f, g⟩ = 1/6 , ∥f ∥2 = 1/5 , ∥g∥2 = 1/7 . Der Winkel zwischen diesen Funktionen (Vektoren) betr¨agt damit ϕ = arccos ⟨f, g⟩ ∥f ∥ ∥g∥ = arccos √5 √7 6 = arccos √ 35 36 . ♦ LA-Skript 6-4 11. April 2016 c⃝M.H. Gutknecht Lineare Algebra (2009) Kapitel 6 — Vektorr¨aume mit Skalarprodukt 6.3 Orthonormalbasen Sobald die Orthogonalit¨at von Vektoren deﬁniert ist, kann man wie im R 3 orthogonale Basen, d.h. orthogonale Koordinatensyste- me verwenden. Wir ¨uberlegen uns in diesem Abschnitt, dass solche Vorteile haben und immer konstruiert werden k¨onnen. Insbesondere sind die Koordinaten bez¨uglich einer orthogonalen Basis besonders einfach zu berechnen. Satz 6.3 Eine Menge M von paarweise orthogonalen Vektoren ist linear unabh¨angig, falls o /∈ M . Beweis: Annahme: Es seien x1, . . . , xm ∈ M , γ1, . . . , γm ∈ E, und es gelte m∑ k=1 γkxk = o . Auf Grund der Linearit¨at des Skalarproduktes gilt f¨ur j = 1, . . . , m: 0 = 〈xj, m∑ k=1 γk xk〉 = m∑ k=1 γk ⟨xj, xk⟩ ︸ ︷︷ ︸ 0, falls k ̸= j = γj ⟨xj, xj⟩ , also ist γj = 0, weil ⟨xj, xj⟩ > 0. Ist dim V = n, so ergeben n paarweise orthogonale, von 0 verschie- dene Vektoren somit eine Basis von V . Definition: Eine Basis heisst orthogonal[orthogonal], falls die Basisvektoren paarweise orthogonal sind: ⟨bk, bl⟩ = 0 falls k ̸= l . Sie heisst orthonormiert oder orthonormal3 [orthonormal], wenn zus¨atzlich die Basisvektoren die L¨ange 1 haben, d.h. wenn ⟨bk, bk⟩ = 1 (∀k) . ▲ Mit dem Kronecker-Symbol [Kronecker symbol], deﬁniert durch δkl :≡ { 0, falls k ̸= l , 1, falls k = l , gilt f¨ur eine orthonormierte Basis also: ⟨bk, bl⟩ = δkl. Satz 6.4 Ist V ein n–dimensionaler Vektorraum mit Skalarpro- dukt und {b1, . . . , bn} eine Orthonormalbasis, so gilt f¨ur alle x ∈ V x = n∑ k=1 ⟨bk, x⟩ ︸ ︷︷ ︸ ≡: ξk bk . (6.16) Das heisst f¨ur die Koordinaten bez¨uglich einer Orthonormalbasis gilt einfach ξk = ⟨bk, x⟩. 3Man beachte, dass eine orthogonale Matrix orthonormale Kolonnen hat und quadratisch ist. Ist die Matrix nicht quadratisch (d.h. ist m > n), so sagt man es sei eine Matrix mit orthonormalen Kolonnen; sind diese nicht normiert, so spricht man von einer Matrix mit orthogonalen Kolonnen. c⃝M.H. Gutknecht 11. April 2016 LA-Skript 6-5 Kapitel 6 — Vektorr¨aume mit Skalarprodukt Lineare Algebra (2009) Beweis: Jedes x ∈ V l¨asst sich nach Satz 4.12 in der Form x = n∑ k=1 ξkbk darstellen, mit eindeutig bestimmten Koordinaten ξk. Bei orthonormier- ter Basis gilt wegen der Linearit¨at des Skalarproduktes im zweiten Ar- gument ⟨bj, x⟩ = 〈 bj, n∑ k=1 ξkbk〉 = n∑ k=1 ξk ⟨bj, bk⟩ = ξj . Beispiel 6.5: Ein reelles trigonometrisches Polynom vom Grad ≤ m ist eine auf ganz R deﬁnierte 2π–periodische Funktion der Form f (t) = 1 2 α0 + α1 cos(t) + β1 sin(t) + · · · + αm cos(mt) + βm sin(mt) oder f (t) = m∑′ k=0 (αk cos(kt) + βk sin(kt)) , (6.17) wobei der Strich nach dem Summenzeichen bedeutet, dass der Summand α0 cos(0t) das Gewicht 1 2 bekommt und der Summand β0 sin(0t) nicht auftritt. Wir bezeichnen die Menge dieser Polynome mit Tm. Mit der ¨ublichen, auf C(R) deﬁnierten Addition und skalaren Multiplikation ist Tm oﬀensichtlich ein Unterraum des Vektorraumes C(R), also selbst ein Vektorraum. Man veriﬁziert leicht, dass durch ⟨f, g⟩ :≡ ∫ 2π 0 f (t) g(t) dt (6.18) auf Tm ein Skalarprodukt deﬁniert ist. Wir behaupten, dass die 2m + 1 Funktionen 1 √2π , 1 √π cos(t), 1 √π sin(t), . . . , 1 √π cos(mt), 1 √π sin(mt) (6.19) eine orthonormale Basis von Tm bilden. In der Tat kann man mit Hilfe der Additionstheoreme f¨ur Sinus und Cosinus die Integrale ∫ 2π 0 cos(kt) cos(lt) dt , ∫ 2π 0 sin(kt) sin(lt) dt , als Summe bzw. Diﬀerenz der zwei Integrale 1 2 ∫ 2π 0 cos((k ± l)t) = δkl π ausdr¨ucken, w¨ahrend f¨ur alle k und l gilt ∫ 2π 0 cos(kt) sin(lt) dt = 1 2 ∫ 2π 0 sin((l −k)t)dt+ 1 2 ∫ 2π 0 sin((l +k)t)dt = 0 . LA-Skript 6-6 11. April 2016 c⃝M.H. Gutknecht Lineare Algebra (2009) Kapitel 6 — Vektorr¨aume mit Skalarprodukt Schreibt man f statt in der Form (6.17) als Linearkombination der nor- mierten Basisfunktionen (6.19), also in der Form f (t) = ξ′ 0√2π + ξ′ 1√π cos(t) + ξ′′ 1√π sin(t) + · · · + + ξ′ m√π cos(mt) + ξ′′ m√π sin(mt) , (6.20) so lassen sich die Koeﬃzienten analog zur Formel ξk = ⟨βk, x⟩ aus (6.16) bestimmen: zum Beispiel ist f¨ur k = 1, . . . , m ξ′ k = 1 √π ⟨cos(kt), f (t)⟩ = 1 √π ∫ 2π 0 f (t) cos(kt) dt . Da ξ′ k/ √π = αk ist und sich βk analog berechnen l¨asst, folgt schliesslich f¨ur die Koeﬃzienten in (6.17): αk = 1 π ∫ 2π 0 f (t) cos(kt) dt , βk = 1 π ∫ 2π 0 f (t) sin(kt) dt , (6.21) wobei die erste Formel auch f¨ur α0 gilt. ♦ Ist V = En und {b1, . . . , bn} eine orthonormierte Basis, so schreibt man (6.16) vorzugweise wieder so, dass die Skalare auf der rechten Seite der Vektoren stehen. Auf Grund der Regeln der Matrizenrech- nung gilt dann f¨ur jedes x ∈ En x = n∑ k=1 bk (b H k x) = n∑ k=1 bkb H k x = ( n∑ k=1 bkb H k ) x . (6.22) Damit ergibt sich folgende additive Zerlegung der Einheitsmatrix in eine Summe von Rang-1–Matrizen n∑ k=1 bkb H k = In . (6.23) Sie bedeutet, dass die Identit¨at als Summe von Projektionen auf die Koordinatenachsen dargestellt werden kann. In Vektorr¨aumen mit Orthonormalbasis sind die Vektoren gleich lang wie die entsprechenden Koordinatenvektoren. Laut der fol- genden Parsevalschen Formel4 gilt allgemeiner das Analoge f¨ur die Skalarprodukte in V und En: 4Marc-Antoine de Parseval des Chˆesnes (27.4.1755 – 16.8.1836), franz¨osischer Mathematiker, in Kontakt mit Fourier, Poisson, Bessel; Royalist. c⃝M.H. Gutknecht 11. April 2016 LA-Skript 6-7 Kapitel 6 — Vektorr¨aume mit Skalarprodukt Lineare Algebra (2009) Satz 6.5 (Parsevalsche Formel) Unter den Voraussetzungen von Satz 6.4 gilt mit ξk :≡ ⟨bk, x⟩ und ηk :≡ ⟨bk, y⟩ (k = 1, . . . , n): ⟨x, y⟩ = n∑ k=1 ξkηk = ξHη = ⟨ξ, η⟩ , (6.24) das heisst das Skalarprodukt zweier Vektoren in V ist gleich dem (Euklidischen) Skalarprodukt ihrer Koordinatenvektoren im En. Insbesondere gilt: ∥x∥ = ∥ξ∥ , (6.25) ∢ (x, y) = ∢ (ξ, η) , (6.26) x ⊥ y ⇐⇒ ξ ⊥ η . (6.27) Beweis: Man stellt x und y mittels Formel (6.16) dar und wendet die Eigenschaften (S1) und (S2) an: ⟨x, y⟩ = 〈 n∑ k=1 ξkbk, n∑ l=1 ηlbl 〉 = n∑ k=1 n∑ l=1 ξk ηl ⟨bk, bl⟩ ︸ ︷︷ ︸ δkl = n∑ k=1 ξkηk . Die drei weiteren Formeln folgen unmittelbar aufgrund der Deﬁnitionen der auftretenden Gr¨ossen. Noch haben wir nicht gezeigt, dass es in einem Vektorraum mit Skalarprodukt immer eine orthonormierte Basis gibt. Dass dies bei endlicher oder abz¨ahlbar-unendlicher Basis so ist, zeigt der folgende Algorithmus von Gram-Schmidt5. Wir wissen aus Lemma 4.6 und Satz 4.11, dass es in jedem Vektorraum eine Basis gibt. Der Algo- rithmus erlaubt uns, diese durch eine orthogonale Basis zu ersetzen. Algorithmus 6.1 (Gram–Schmidt–Orthogonalisierungs- verfahren) Es sei {a1, a2, . . . } eine endliche oder abz¨ahlbare, linear un- abh¨angige Menge von Vektoren. Wir berechnen eine gleich grosse Menge {b1, b2, . . . } von Vektoren rekursiv gem¨ass b1 :≡ a1 ∥a1∥ , ̃bk :≡ ak − ∑k−1 j=1 ⟨bj, ak⟩ bj , bk :≡ ̃bk ∥̃bk∥    (k = 2, 3, . . . ) . (6.28) 5Jorgen Pedersen Gram (27.6.1850 – 29.4.1916), d¨anischer Versiche- rungsmathematiker. Erhard Schmidt (14.1.1876 – 6.12.1959), aus dem Baltikum stammender deutscher Mathematiker, Sch¨uler von H.A. Schwarz und D. Hilbert; 1908 Pro- fessor in Z¨urich, 1910 in Erlangen, 1911 in Breslau, 1917 in Berlin. LA-Skript 6-8 11. April 2016 c⃝M.H. Gutknecht Lineare Algebra (2009) Kapitel 6 — Vektorr¨aume mit Skalarprodukt Satz 6.6 Die im Gram–Schmidt–Orthogonaliserungsverfahren konstruierten Vektoren b1, b2, . . . sind normiert und paarweise orthogonal, und es gilt nach k Schritten span {a1, a2, . . . , ak} = span {b1, b2, . . . , bk} . Ist {a1, a2, . . . } eine Basis von V , so ist {b1, b2, . . . } eine Ortho- normalbasis von V . Statt Gram–Schmidt–Orthogonalisierungsverfahren werden auch die Bezeichnungen Schmidtsches Orthogonalisierungsverfahren (vor allem in der deutschsprachigen Mathematik-Literatur) und klassisches Gram–Schmidt–Verfahren [classical Gram-Schmidt algorithm] (vor allem im wissenschaftlichen Rechnen) verwendet. Eine in Bezug auf die Rundungsfehler etwas genauere Version heisst modiﬁziertes Gram–Schmidt–Verfahren [modiﬁed Gram– Schmidt algorithm]. Das Verfahren wird vor allem, aber nicht nur, im Falle V = En verwendet. In diesem Fall gibt es allerdings diverse Alternativen. Wir werden sehen, dass man das Gram–Schmidt–Verfahren nicht nur f¨ur die Konstruktion einer Orthonormalbasis brauchen kann. Beweis von Satz 6.6: Es ist klar, dass ∥bk∥ = 1 (∀k). Wir m¨ussen zeigen, dass ⟨bl, bk⟩ = 0 falls k ̸= l. Wir d¨urfen die Induktionsvorausset- zung machen, dass b1, . . . , bk−1 paarweise orthogonal sind. (Falls k = 1 ist die Induktionsvoraussetzung leer; es ist deshalb keine eigentliche Ver- ankerung n¨otig.) Dann folgt f¨ur l = 1, . . . , k − 1: ⟨bl, bk⟩ = ∥̃bk∥ −1 〈bl, ̃bk〉 = ∥̃bk∥ −1 [ ⟨bl, ak⟩ − k−1∑ j=1 ⟨bj, ak⟩ δlj ︷ ︸︸ ︷ ⟨bl, bj⟩ ︸ ︷︷ ︸ = ⟨bl, ak⟩ ] = ∥̃bk∥ −1[⟨bl, ak⟩ − ⟨bl, ak⟩] = 0 . Also sind sogar b1, . . . , bk paarweise orthogonal, und mit Induktion schlies- sen wir auf die paarweise Orthogonalit¨at aller bj. Durch Induktion folgt sofort auch, dass bk ∈ span {a1, . . . , ak} , also span {b1, . . . , bk} ⊆ span {a1, . . . , ak} (∀k) . Nach Satz 6.3 sind b1, . . . , bk linear unabh¨angig, bilden also eine Basis von span {b1, . . . , bk}. Aus Dimensionsgr¨unden folgt daraus direkt, dass span {b1, . . . , bk} = span {a1, . . . , ak}, vgl. Satz 4.7. Aufgrund der letzten Aussage in Satz 6.6 gilt insbesondere: Korollar 6.7 Zu einem Vektorraum mit Skalarprodukt von endli- cher oder abz¨ahlbar unendlicher Dimension gibt es eine orthonor- mierte Basis. c⃝M.H. Gutknecht 11. April 2016 LA-Skript 6-9 Kapitel 6 — Vektorr¨aume mit Skalarprodukt Lineare Algebra (2009) Beispiel 6.6: Die Monome ak : t ↦−→ tk−1 (k = 1, 2, . . . ) bilden ja eine Basis des Vektorraumes P aller Polynome mit reellen Koeﬃzienten. (Man k¨onnte stattdessen auch komplexe zulassen.) Diese Basis ist aber nicht orthonormal bez¨uglich des Skalarproduktes ⟨p, q⟩ :≡ ∫ 1 −1 p(t) q(t) dt , denn zum Beispiel ist 〈t2, 1 〉 = 2 3 ̸= 0. Wir k¨onnen diese Basis aber mit dem Gram–Schmidt–Verfahren orthonormieren: ∥a1∥ 2 = ∫ 1 −1 1 dt = t ∣ ∣ ∣1 −1 = 2 , b1(t) = a1(t) ∥a1∥ = 1 √2 , ⟨b1, a2⟩ = 〈 1 √2 , t〉 = ∫ 1 −1 t √2 dt = t2 2 √2 ∣ ∣ ∣1 −1 = 0 , ̃b2(t) = a2(t) − ⟨b1, a2⟩ b1(t) = t − 0 = t , ∥̃b2∥ 2 = ∫ 1 −1 t2 dt = t3 3 ∣ ∣ ∣1 −1 = 2 3 , b2(t) = ̃b2(t) ∥̃b2∥ = √ 3 2 t , ⟨b1, a3⟩ = 〈 1 √2 , t2〉 = 2 3 1 √2 , ⟨b2, a3⟩ = 〈√ 3 2 t , t2〉 = 0 , ̃b3(t) = a3(t) − ⟨b1, a3⟩ b1(t) − ⟨b2, a3⟩ b2(t) = t2 − 2 3 1 √2 1 √2 = t2 − 1 3 ∥̃b3∥ 2 = ∫ 1 −1 ( t2 − 1 3 )2 dt = t5 5 − 2t3 9 t 9 ∣ ∣ ∣1 −1 = 8 45 , b3(t) = √ 45 8 (t2 − 1 3 ) , usw. Auf diese Art entsteht eine Folge {bk}∞ k=1 orthonormaler Polynome mit Grad bk = k − 1. Die umskalierten orthogonalen Polynome Pk(t) :≡ bk+1(t)/bk+1(1) heissen Legendre–Polynome6 [Legendre polynomials]. Auf analoge Art erh¨alt man bei Ben¨utzung des Skalarproduktes ⟨p, q⟩ :≡ ∫ 1 −1 p(t) q(t) √1 − t2 dt die Tschebyscheﬀ–Polynome7 [Chebyshev polynomials]. Allgemeiner kann man zu jedem endlichen oder unendlichen Intervall und jeder da- zu passenden positiven Gewichtsfunktion eine Folge von Orthogonal- polynomen [orthogonal polynomials] deﬁnieren. Zu deren Konstrukti- on gibt es aber einen eﬃzienteren Algorithmus als das Verfahren von Gram-Schmidt, denn drei aufeinanderfolgende Polynome sind immer durch eine Rekursionsformel verkn¨upft. ♦ 6Adrien Marie Legendre (18.9.1752 – 10.1.1833), franz¨osischer Mathe- matiker, ab 1755 Professor an der ´Ecole Militaire in Paris, sp¨ater an der ´Ecole Normale. 7Pafnuti Lwowitsch Tschebyscheff (14.5.1821 – 26.11.1894), russi- scher Mathematiker, ab 1857 Professor in St. Petersburg. LA-Skript 6-10 11. April 2016 c⃝M.H. Gutknecht Lineare Algebra (2009) Kapitel 6 — Vektorr¨aume mit Skalarprodukt Ausblick: Wie wir schon in Kapitel 4, Satz 4.11, erw¨ahnt ha- ben, gibt es in jedem Vektorraum (ausser {o}) eine Basis gem¨ass unserer Deﬁnition, aber diese Deﬁnition f¨uhrt nur in R¨aumen wie ℓ0 und P mit abz¨ahlbarer Basis weiter, nicht aber f¨ur die ” grossen“ Funktionenr¨aume, wie ℓ oder C[a, b]. Man verwendet f¨ur die meisten unendlich-dimensionalen R¨aume in der Mathematik eine andere Deﬁnition von Erzeugendensystem und Basis. Die Vektoren werden als unendliche Linearkombinationen der Basisvektoren dargestellt, d.h. als Reihe. Dabei m¨ussen diese Reihen in einem gewissen Sinne konvergieren. Notwendig f¨ur eine Deﬁnition der Konvergenz ist, dass ein Mass f¨ur die Diﬀerenz (ein Abstand) zweier Vektoren (bzw. Funktionen) eingef¨uhrt werden kann oder zumindest ein Umgebungsbegriﬀ. Ein entsprechender Vektorraum heisst8 i) topologischer Vektorraum [topological vector space] (mit Umgebungsbegriﬀ), ii) Banachraum9 [Banach space] (mit Vektornorm) oder iii) Hilbertraum10 [Hilbert space] (mit Skalarprodukt und zu- geh¨origer Vektornorm). In den wichtigen Beispielen sind die in diesen R¨aumen verwende- ten Basen abz¨ahlbar unendlich gross, aber i.a. viel “kleiner” als eine Vektorraumbasis, weil jeder Vektor ja nur als unendliche Linarkom- bination von Basisvektoren darstellbar sein muss, nicht als endliche Linearkombination. Beispiel 6.7: Der Hilbertraum ℓ2 der quadrat-summierbaren Fol- gen [square summable series] ist deﬁniert als Vektorraum der Folgen x = {xk} ∞ k=0 mit ∞∑ k=0 |xk|2 < ∞ (6.29) mit dem Skalarprodukt ⟨x, y⟩ = ∞∑ k=0 xk yk (6.30) und der Norm ∥x∥ = √ √ √ √ ∞∑ k=0 |xk|2 . (6.31) Die Zahlfolgen ek (1 ≤ k < ∞) aus Beispiel 4.23 bilden eine Hilbert- raum-Basis: jedes x ∈ ℓ2 ist ja eine unendliche Linearkombination dieser Basisvektoren: x = ∑ xkek. ♦ 8Bei Banach- und Hilbertr¨aumen wird noch verlangt, dass sie vollst¨andig [complete] sind, das heisst, dass mit jeder konvergierenden Folge von Vektoren auch deren Grenzwert im Raum liegt. 9Stefan Banach (30.3.1892 – 31.8.1945), polnischer Mathematiker, ab 1922 Professor in Lw´ow. 10David Hilbert (23.1.1862 – 14.2.1943), deutscher Mathematiker, ab 1895 Professor in G¨ottingen, das dank ihm neben Berlin zum zweiten deutschen mathematischen Zentrum wurde. c⃝M.H. Gutknecht 11. April 2016 LA-Skript 6-11 Kapitel 6 — Vektorr¨aume mit Skalarprodukt Lineare Algebra (2009) Beispiel 6.8: Der Hilbertraum L2 der quadrat-integrierbaren 2π– periodischen Funktionen [square integrable 2π–periodic functions] ist deﬁniert als Menge der 2π–periodischen Funktionen11 f : R −→ E mit ∫ 2π k=0 |f (t)|2 dt < ∞ (6.32) mit dem Skalarprodukt und der Norm ⟨f, g⟩ = ∫ 2π 0 f (t) g(t) dt , ∥f ∥ = √ ∫ 2π 0 |f (t)|2 dt . (6.33) Falls E = R, bilden die Funktionen {cos kt}∞ k=0 und {sin kt}∞ k=1 (6.34) zusammen eine Basis. Im komplexen Fall, E = C, verwendet man statt- dessen die Funktionen {eikt}∞ k=−∞ . (6.35) Die entsprechenden Reihendarstellungen einer Funktion f ∈ L2 heissen Fourier-Reihen12 [Fourier series] f (t) = α0 + ∞∑ k=1 (αk cos kt + βk sin kt) (6.36) bzw. f (t) = ∞∑ k=−∞ γk eikt . (6.37) Dabei gilt allerdings an Unstetigkeitsstellen von f das Gleichheitszeichen in der Regel nicht. Die Fourier-Reihen (6.36) und (6.37) sind Darstellungen der Art (6.16), aber mit einer unendlichen statt einer endlichen Reihe. In der reellen Version (6.36) gelten wie in Beispiel 6.5 f¨ur die Fourier-Koeﬃzienten [Fourier coeﬃcients] ak und bk die Formeln (6.19). Jene f¨ur die Koeﬃ- zienten γk der komplexen Version (6.37) sind noch einfacher: γk = 1 2π ∫ 2π 0 f (t) e−ikt dt . (6.38) Die Bedeutung der Fourier-Reihen liegt darin, dass man fast jede peri- odische Funktion f durch eine Fourier-Reihe darstellen kann, die “fast ¨uberall” (in einem exakt deﬁnierbaren Sinn) gegen f konvergiert. Zu (6.36) und (6.37) analoge Reihenentwicklungen (Darstellungen in ei- ner unendlichen Basis) f¨ur die Vektoren gibt es auch in allen anderen Hilbertr¨aumen mit abz¨ahlbar unendlicher Basis. Man nennt sie dort ver- allgemeinerte Fourier-Reihen [generalized Fourier series]. ♦ ▼ 11Das Integral ist allerdings hier nicht ein Riemann-Integral sondern ein all- gemeineres Lebesgue-Integral. Die Funktionen brauchen nicht stetig zu sein, sondern nur im Sinne von Lebesgue integrierbar. Henri Lebesgue (28.6.1875 – 26.7.1941), franz¨osischer Mathematiker, der bereits in seiner Dissertation eine neue Theorie des Masses aufstellte, die zum Lebesgue-Integral f¨uhrte. Professor am Coll`ege de France. 12Jean-Baptiste-Joseph Fourier (21.3.1768 – 16.5.1830), franz¨osischer Mathematiker, 1794 Professor an der ´Ecole Normale, sp¨ater Napoleons Pr¨afekt in Grenoble. LA-Skript 6-12 11. April 2016 c⃝M.H. Gutknecht Lineare Algebra (2009) Kapitel 6 — Vektorr¨aume mit Skalarprodukt 6.4 Orthogonale Komplemente Wenn wir den Satz 6.6 ¨uber das Gram–Schmidt–Verfahren mit dem fr¨uheren Satz 4.11 kombinieren, gem¨ass dem eine linear un- abh¨angige Menge von Vektoren zu einer Basis erg¨anzt werden kann, folgt sofort: Korollar 6.8 In einem Vektorraum endlicher Dimension mit Skalarprodukt kann man jede Menge orthonormaler Vektoren zu einer orthonormalen Basis erg¨anzen. Das gilt auch noch f¨ur Vektorr¨aume mit abz¨ahlbar unendlicher Ba- sis, wie z.B. den Raum P aller Polynome, aber wir wollen uns in diesem Abschnitt der Einfachheit halber auf R¨aume endlicher Di- mension beschr¨anken, wo gewisse Schwierigkeiten nicht auftreten. Bezeichnen wir wie in (4.32) und (4.33) die gegebene Menge ortho- normaler Vektoren mit M = {b1, . . . , bl} und die Erg¨anzungsmenge mit N = {bl+1, . . . , bn}, so gilt f¨ur die von diesen Mengen aufge- spannten Unterr¨aume U :≡ span M = span {b1, . . . , bl} , U ′ :≡ span N = span {bl+1, . . . , bn} , (6.39) gem¨ass (4.36) dass V = U ⊕ U ′ gilt, aber zus¨atzlich ist U ⊥ U ′. Stellen wir irgend ein x ∈ V in der Basis dar, so ist x = ξ1b1 + · · · ξlbl︸ ︷︷ ︸ ≡: u ∈ U + ξl+1bl+1 + · · · ξnbn︸ ︷︷ ︸ ≡: u′ ∈ U ′ = u + u′ mit u ⊥ U ′ , u′ ⊥ U . Man bezeichnet den Unterraum U ′ deshalb oft mit U ⊥. Oﬀensicht- lich ist er eindeutig bestimmt, denn er enth¨alt genau alle zu U orthogonale Vektoren von V . Definition: In einem endlich-dimensionalen Vektorraums V mit Skalarprodukt heisst der zu einem echten Unterraum U orthogo- nale komplement¨are Unterraum das orthogonale Komplement [orthogonal complement] von U und wird mit U ⊥ (” U senkrecht“ [“U perp”]) bezeichnet. Es wird implizit charakterisiert durch die Beziehungen V = U ⊕ U ⊥ , U ⊥ U ⊥ (6.40) oder explizit durch U ⊥ :≡ {x ∈ V ; x ⊥ U } . (6.41) Wir nennen dann V eine direkte Summe orthogonaler Kom- plemente [direct sum of orthogonal complements]. ▲ Oﬀensichtlich ist (U ⊥)⊥ = U (6.42) c⃝M.H. Gutknecht 11. April 2016 LA-Skript 6-13 Kapitel 6 — Vektorr¨aume mit Skalarprodukt Lineare Algebra (2009) und dim U + dim U ⊥ = dim V . (6.43) Die letzte Beziehung gilt analog auch f¨ur nicht-orthogonale kom- plement¨are Unterr¨aume. Wie bei der allgemeinen direkten Summe von Unterr¨aumen l¨asst sich der Begriﬀ der Summe orthogonaler Unterr¨aume sofort auf mehrere Terme ausdehnen. Zum Beispiel k¨onnte man oben den Unterraum U selbst wieder in zwei ortho- gonale Komplemente aufteilen, falls dim U > 1 ist. Beispiel 6.9: Ist b1, . . . , bn eine orthogonale Basis von V und setzt man Uk :≡ span {bk}, so gilt V = U1 ⊕ U2 ⊕ · · · ⊕ Un , Uj ⊥ Uk for j ̸= k , (6.44) das heisst V ist die direkte Summe von n zueinander orthogonalen ein- dimensionalen Unterr¨aumen aufgespannt durch die einzelnen Basisvek- toren. ♦ Eine interessante Anwendung ergibt sich im Zusammenhang mit Kern (Nullraum) und Bild (Kolonnenraum) einer Matrix. Es sei A ∈ Em×n eine beliebige Matrix und x ∈ En. Es gilt Ax = o genau dann, wenn x auf allen Zeilen von A (bzw. von A falls E = C) senkrecht steht, das heisst wenn x auf allen Kolonnen von AT (bzw. AH) senkrecht steht. Wir haben also, falls E = R, x ∈ N (A) ⇐⇒ Ax = o ⇐⇒ x ⊥ R(AT) ⇐⇒ x ∈ (R(AT))⊥ und, falls E = C, x ∈ N (A) ⇐⇒ Ax = o ⇐⇒ x ⊥ R(AH) ⇐⇒ x ∈ (R(AH))⊥ . Erinnern wir uns noch daran, dass nach Korollar 5.14 gilt Rang A = Rang AH = Rang AT, so erhalten wir: Satz 6.9 F¨ur eine komplexe m × n–Matrix mit Rang r gilt N (A) = (R(AH))⊥ ⊂ En , N (AH) = (R(A))⊥ ⊂ Em . (6.45) Insbesondere ist N (A) ⊕ R(AH) = En , N (AH) ⊕ R(A) = Em (6.46) und dim R(A) = r , dim N (A) = n − r , dim R(AH) = r , dim N (AH) = m − r . (6.47) Bei reellen Matrizen kann ¨uberall AH durch AT ersetzt werden. Definition: Die zwei Paare komplement¨arer Unterr¨aume N (A), R(A H) und N (AH), R(A) nennt man die vier fundamentalen Unterr¨aume [fundamental subspaces] der Matrix A. ▲ LA-Skript 6-14 11. April 2016 c⃝M.H. Gutknecht Lineare Algebra (2009) Kapitel 6 — Vektorr¨aume mit Skalarprodukt 6.5 Orthogonale und unit¨are Basiswechsel und Koordinatentransformationen In einem Vektorraum V mit Skalarprodukt interessiert man sich na- turgem¨ass f¨ur Basiswechsel zwischen orthonormierten Basen. Wie in §3.4 stellen wir zun¨achst die “neue” Basis {b′ 1, . . . , b′ n} in der “alten” Basis {b1, . . . , bn} dar: b′ k = n∑ i=1 τikbi , k = 1, . . . , n . (6.48) Bei einer “gew¨ohnlichen” Basistransformation ist die Transforma- tionsmatrix T = ( τik ) irgend eine regul¨are n × n-Matrix (vgl. Satz 4.13). Welche spezielle Eigenschaft hat T wohl, wenn beide Basen orthonormiert sind? Es gilt dann δkl = ⟨b′ k, b′ l⟩ = 〈 n∑ i=1 τik bi, n∑ j=1 τjl bj〉 = n∑ i=1 τik n∑ j=1 τjl ⟨bi, bj⟩ ︸ ︷︷ ︸ δij = n∑ i=1 τik τil , das heisst I = THT . (6.49) Satz 6.10 Die Transformationsmatrix einer Basistransformation zwischen orthormierten Basen ist unit¨ar (falls E = C) bzw. ortho- gonal (falls E = R). Unit¨are und orthogonale Koordinatentransformationen sind sehr praktisch, weil T−1 = TH ist, also die inverse Transformation immer bekannt ist. Damit vereinfacht sich die Formel (4.42) von Satz 4.13. Satz 6.11 Es sei die durch (6.48) deﬁnierte Basistransformation unit¨ar bzw. orthogonal, d.h. es gelte (6.49). Dann sind die alten und die neuen Koordinatenvektoren ξ bzw. ξ′ verkn¨upft gem¨ass ξ = Tξ′ , ξ′ = THξ . (6.50) Falls der zu Grunde liegende Vektorraum V der En ist, kann man die alten und neuen orthonormierten Basisvektoren wie ¨ublich als Kolonnenvektoren unit¨arer oder orthogonaler Matrizen auﬀassen: B :≡ ( b1 . . . bn ) , B′ :≡ ( b ′ 1 . . . b ′ n ) . (6.51) Aus den Formeln (6.50) wird dann B = B′ TH , B′ = BT , (6.52) wobei nun hier alle drei Matrizen unit¨ar bzw. orthogonal sind. c⃝M.H. Gutknecht 11. April 2016 LA-Skript 6-15 Kapitel 6 — Vektorr¨aume mit Skalarprodukt Lineare Algebra (2009) Hier ist zu erw¨ahnen, dass die unit¨aren bzw. orthogonalen Matrizen der Ordnung n eine Gruppe bilden. Beispiel 6.10: Drehung des Koordinatensystems in R2. Hier gilt b′ 1 = cos φ b1 + sin φ b2 , b′ 2 = − sin φ b1 + cos φ b2 , wobei beim g¨angigen Koordinatensystem b1 = e1 und b2 = e2 die Standardbasisvektoren sind. Also hat man, mit der zweidimensionalen Givens–Rotationsmatrix U(φ) aus Kapitel 2, T = ( cos φ − sin φ sin φ cos φ ) = U(−φ) , B = I , B′ = U(−φ) . F¨uhrt man im Rn eine neue Basis ein, in der alle Basisvektoren bis auf zwei von der Standardbasis ¨ubernommen werden, die verbleibenden zwei, sagen wir b′ i und b′ j, aber in dem durch ei und ej aufgespannten zweidimensionalen Raum durch eine Drehung um den Winkel −φ aus ei und ej hervorgehen, ergibt sich als Transformationsmatrix T analog die allgemeine Givens–Rotation oder Jacobi–Rotation [Givens / Jacobi rotation] Uij(−φ): Uij(−φ) =                  1 . . . 1 cos φ − sin φ 1 . . . 1 sin φ cos φ 1 . . . 1                  ← i ← j (6.53) (alle ¨ubrigen Elemente sind 0). Eine solche 5×5–Matrix haben wir bereits in Beispiel 2.26 betrachtet. ♦ Bei einer orthogonalen oder unit¨aren Basistransformation bleiben die L¨angen und Winkel im Koordinatenraum erhalten, denn gem¨ass der Parsevalschen Formel (Satz 6.5) sind sie ja gleich den entspre- chenden L¨angen und Winkel in V . Korollar 6.12 Unter den Voraussetzungen von Satz 6.11 gilt, wenn η und η′ = THη ein weiteres Paar von alten und neuen Koordinaten bezeichnet, dass ⟨ξ′, η′⟩ = ⟨ξ, η⟩ . (6.54) Insbesondere gilt ∥ξ′∥ = ∥ξ∥ , (6.55) ∢ (ξ′, η′) = ∢ (ξ, η) , (6.56) ξ′ ⊥ η′ ⇐⇒ ξ ⊥ η . (6.57) LA-Skript 6-16 11. April 2016 c⃝M.H. Gutknecht Lineare Algebra (2009) Kapitel 6 — Vektorr¨aume mit Skalarprodukt Beweis: (6.54) folgt mit Hilfe von (6.50) und (6.49) gem¨ass ⟨ξ, η⟩ = ξHη = (ξ′)HTHTη′ = (ξ′)Hη′ = 〈ξ′, η′〉 . Wir kommen nun noch zur¨uck auf die in Abschnitt 5.5 und beson- ders in Diagramm (5.48) zusammengefasste Situation einer linearen Abbildung F : X → Y , wobei wir nun voraussetzen wollen, dass X und Y Vektorr¨aume mit Skalarprodukten (·, ·)X bzw. (·, ·)Y ¨uber dem gleichen Zahlk¨orper R oder C sind (also beides Euklidische R¨aume oder beides unit¨are R¨aume). Wir wollen F bez¨uglich or- thonormalen Basen in X und Y darstellen, und dabei von einem ersten Paar gegebener, “alter” Basen auf ein zweites Paar “neuer” orthonormaler Basen ¨ubergehen. x ∈ X F −−−−−−→ lin. Abb. z ∈ Y Vektorr¨aume mit Skalarprodukt κX     ↓ ↑    κ−1 X κY     ↓ ↑    κ−1 Y ξ ∈ En A −−−−−−→ Abb.matrix η ∈ Em Koordinaten bzgl. “alten” Orthonormal- basen in X und Y T−1 = TH    ↓ ↑    T S −1 = S H    ↓ ↑    S ξ′ ∈ En B −−−−−−→ Abb.matrix η′ ∈ Em Koordinaten bzgl. “neuen” Orthonormal- basen in X und Y (6.58) Weil die Matrizen T und S der Koordinatentransformationen or- thogonal bzw. unit¨ar sind, lassen sich nun die Formeln (5.50) und (5.52) f¨ur die Transformation der Abbildungsmatrix umschreiben in B = S HAT , A = SBTH , (6.59) bzw. falls Y = X, κY = κX und S = T, B = THAT, A = TBTH . (6.60) Im zweiten Falle, wo es um eine Selbstabbildung geht, ist nach (6.60) klar, dass • wenn A Hermitesch bzw. reell symetrisch ist, so auch B, • wenn A unit¨ar bzw. orthogonal ist, so auch B. c⃝M.H. Gutknecht 11. April 2016 LA-Skript 6-17 Kapitel 6 — Vektorr¨aume mit Skalarprodukt Lineare Algebra (2009) 6.6 Orthogonale und unit¨are Abbildungen Wir wollen nun noch auf spezielle lineare Abbildungen zu sprechen kommen, die nicht nur die linearen Beziehungen, sondern auch das Skalarprodukt zweier Vektoren invariant lassen. Definition: Es seien X und Y zwei unit¨are (bzw. orthogonale) Vektorr¨aume. Eine lineare Abbildung F : X → Y heisst unit¨ar [unitary] (bzw. orthogonal [orthogonal]), falls f¨ur x, y ∈ X gilt ⟨F x, F y⟩Y = ⟨x, y⟩X . (6.61) Diese Abbildungen haben eine Reihe interessanter Eigenschaften, die sich leicht herleiten lassen und die im folgenden Satz zusam- mengefasst sind. Die ersten zwei haben wir schon in Satz 2.21 ange- troﬀen als Eigenschaften einer durch eine orthogonale oder unit¨are Matrix deﬁnierten Abbildung. Satz 6.13 F¨ur eine orthogonale oder unit¨are Abbildung F : X → Y gilt: (i) ∥F x∥Y = ∥x∥X (∀x ∈ X), d.h. F ist l¨angentreu [length preserving] (oder: isometrisch [isometric]); (ii) x ⊥ y =⇒ F x ⊥ F y, d.h. F ist winkeltreu [angle preserving]; (iii) ker F = {o}, d.h. F ist injektiv. Ist dim X = dim Y < ∞, so gilt zus¨atzlich: (iv) F ist ein Isomorphismus. (v) Ist {b1, . . . , bn} eine Orthonormalbasis von X, so ist {F b1, . . . , F bn} eine Orthonormalbasis von Y ; (vi) F −1 ist unit¨ar (bzw. orthogonal ); (vii) Die Abbildungsmatrix A bez¨uglich orthonormierten Basen in X und Y ist unit¨ar (bzw. orthogonal ). Beweis: (i) und (ii) folgen direkt aus (6.61). (iii) folgt aus F x = o ⇐⇒ ⟨F x, F x⟩Y = 0 (6.61) ⇐⇒ ⟨x, x⟩X = 0 ⇐⇒ x = o . (iv) ergibt sich aus (iii), der Dimensionsformel (5.29) und ihrem Korol- lar 5.8. (v) und (vi) folgen aus (iv) und (6.61). (vii) ist schliesslich eine Konsequenz der folgenden drei Hilfss¨atze, ange- wandt auf die aus dem Diagramm (6.58) folgende Relation A = κY F κ −1 X (die bereits in (5.25) auftrat). LA-Skript 6-18 11. April 2016 c⃝M.H. Gutknecht Lineare Algebra (2009) Kapitel 6 — Vektorr¨aume mit Skalarprodukt Weitere Aussagen ¨uber die Verkn¨upfung unit¨arer Abbildungen, die Koordinatenabbildung und die Abildungsmatrix folgen m¨uhelos: Lemma 6.14 Sind F : X → Y und G : Y → Z zwei unit¨are (oder orthogonale) Isomorphismen endlich-dimensionaler unit¨arer (bzw. orthogonaler ) Vektorr¨aume, so auch G ◦ F : X → Z. Beweis: Es ist klar, dass G ◦ F ein Isomorphismus ist. Zudem gilt (6.61) f¨ur F und G, also ⟨GF x, GF y⟩Z = ⟨F x, F y⟩Y = ⟨x, y⟩X . Lemma 6.15 Ist V ein n–dimensionaler unit¨arer (oder orthogo- naler ) Vektorraum mit Orthonormalbasis, so ist die Koordinaten- abbildung κV : V → C n (bzw. V → R n) ein unit¨arer (bzw. ortho- gonaler ) Isomorphismus. Beweis: Wir haben uns bereits bei der Deﬁnition (5.23) der Koordi- natenabbildung ¨uberlegt, dass sie ein Isomorphismus ist. Zudem ist f¨ur sie (6.61) identisch mit der Parselvalschen Formel (6.24) aus Satz 6.5, also ist sie ein unit¨arer (oder orthogonaler) Isomorphismus. Lemma 6.16 Die Matrix A ∈ C n×n (bzw. R n×n) ist genau dann unit¨ar (bzw. orthogonal ), wenn die lineare Abbildung A : C n → C n (bzw. R n → R n) unit¨ar (bzw. orthogonal) ist. Beweis: Die durch A deﬁnierte Abbildung ist unit¨ar, wenn gem¨ass (6.61) gilt: ⟨Ax, Ay⟩ = ⟨Ax, Ay⟩ (∀x, y). Hieraus folgt mit x = ek und y = el, dass (AH A)kl = eH k AHAel = (Aek)H(Ael) = ⟨Aek, Ael⟩ = ⟨ek, el⟩ = δkl , was heisst, dass die Matrix A unit¨ar ist. Umgekehrt gilt diese Formel genau dann, wenn ⟨Aek, Ael⟩ = ⟨ek, el⟩ (∀k, l). Schreibt man x = ∑ ekξk, y = ∑ elηl, so folgt aufgrund der Eigenschaften (6.6), dass auch ⟨Ax, Ay⟩ = ⟨Ax, Ay⟩ (∀x, y) gilt, also die Abbildung unit¨ar ist. 6.7 Normen von linearen Abbildungen (Operatoren) und Matrizen Definition: Es seien X und Y zwei normierte Vektorr¨aume mit den Normen ∥.∥X und ∥.∥Y . Eine lineare Abbildung (oder: ein li- nearer Operator) F : X → Y heisst beschr¨ankt [bounded], wenn es ein γF ≥ 0 gibt mit ∥F (x)∥Y ≤ γF ∥x∥X (∀x ∈ X) . Die Gesamtheit solcher linearer Abbildungen (Operatoren) F zwi- schen X und Y heisst L(X, Y ). ▲ Bemerkung: Die zwei Normen in X und Y k¨onnen gem¨ass (6.7) durch Skalarprodukte ⟨., .⟩X und ⟨., .⟩Y deﬁniert sein, m¨ussen es aber nicht. ▼ c⃝M.H. Gutknecht 11. April 2016 LA-Skript 6-19 Kapitel 6 — Vektorr¨aume mit Skalarprodukt Lineare Algebra (2009) In L(X, Y ) k¨onnen wir auf nat¨urliche Art eine Addition und eine skalare Multiplikation deﬁnieren: F + G : x ∈ X ↦→ (F + G)(x) :≡ F x + Gx (∀F, G ∈ L(X, Y )), αF : x ∈ X ↦→ (αF )(x) :≡ αF x (∀α ∈ E, ∀F ∈ L(X, Y )). (6.62) Damit wird L(X, Y ) selbst zu einem Vektorraum, wie man leicht veriﬁziert. Beispiel 6.11: Die m×n Matrizen k¨onnen als Elemente von L(En, Em) aufgefasst werden. Dabei sind Addition und skalare Multiplikation wie in Kapitel 2 durch A + B und αA deﬁniert. ♦ Nun werden wir im Vektorraum L(X, Y ) eine Norm deﬁnieren. Definition: Die auf L(X, Y ) durch die Normen ∥.∥X und ∥.∥Y induzierte Operatornorm [operator norm] ist deﬁniert durch ∥.∥ : L(X, Y ) → R , F ↦→ ∥F ∥ :≡ sup x̸=o ∥F x∥Y ∥x∥X . (6.63) Ist X = Y = En, so dass F durch eine quadratische Matrix A ge- geben ist, heisst ∥A∥ die durch die Vektornorm (in En) induzierte Matrixnorm von A. Sie ist also deﬁniert durch ∥.∥ : En×n → R , A ↦→ ∥A∥ :≡ sup x̸=o ∥Ax∥ ∥x∥ . (6.64) Verwendet man in En die Euklidische 2–Norm, so heisst die indu- zierte Matrixnorm Spektralnorm [spectral norm] oder 2–Norm [2–norm]; sie wird auch mit ∥.∥2 bezeichnet: ∥A∥2 :≡ sup x̸=o ∥Ax∥2 ∥x∥2 . (6.65) Statt “die von den Vektornormen induzierte Operator- oder Ma- trixnorm” sagt man auch “die den Vektornormen untergeordnete Operatornorm bzw. Matrixnorm [subordinate matrix norm]”. ▲ Die Deﬁnitionen (6.63)–(6.65) lassen sich vereinfachen: f¨uhrt man zum Beispiel im Bruch in (6.63) α :≡ ∥x∥X und ̃x :≡ x/α ein, so sieht man, dass wegen ∥̃x∥ = 1 und ∥F x∥Y = ∥ ∥ ∥αF ( x α )∥ ∥ ∥Y = α ∥ ∥ ∥F ( x α )∥ ∥ ∥Y = ∥x∥X ∥F ̃x∥Y folgendes gilt (wir ersetzen ̃x wieder durch x): LA-Skript 6-20 11. April 2016 c⃝M.H. Gutknecht Lineare Algebra (2009) Kapitel 6 — Vektorr¨aume mit Skalarprodukt Lemma 6.17 Die in (6.63)–(6.65) gegebenen Formeln f¨ur ∥F ∥, ∥A∥, ∥A∥2 lassen sich ersetzen durch ∥F ∥ = sup ∥x∥X =1 ∥F x∥Y , (6.66) ∥A∥ = sup ∥x∥=1 ∥Ax∥ , (6.67) ∥A∥2 = sup ∥x∥2=1 ∥Ax∥2 . (6.68) In dieser Form l¨asst sich die Operatornorm leicht geometrisch in- terpretieren: Die Restriktion ∥x∥X = 1 bzw. ∥x∥2 = 1 bedeutet ja, dass wir nur Vektoren auf der Oberﬂ¨ache der “Einheitskugel” in X oder En betrachten. Die Norm entspricht dann der maximalen L¨ange der Bildvektoren dieser Sph¨are. Diese Interpretation passt vor allem f¨ur die Spektralnorm. Man muss aber daran denken, dass bei Verwendung einer anderen als der 2–Norm in En die “Einheits- kugel” nicht eine Kugel ist. Die Spektralnorm einer Matrix oder einer linearen Abbildung ist leider im allgemeinen nicht einfach zu berechnen, aber oft ben¨otigt man eﬀektiv nur eine obere Schranke, und eine solche kann man h¨auﬁg einfach erhalten. Auf die Berechnung der Spektralnorm wer- den wir in den Abschnitten 10.6 und 11.2 zur¨uckkommen. Wie die folgende Betrachtung zeigt, ist die Berechnung einfach, wenn A eine Diagonalmatrix ist. Beispiel 6.12: F¨ur eine Diagonalmatrix D = diag (d11, . . . , dnn) ist ∥Dx∥2 2 ∥x∥2 2 = ∑n k=1 |dkk|2 |xk|2 ∑n k=1 |xk|2 ein gewichtetes Mittel der Betragsquadrate der Diagonalelemente dkk und dieses Mittel wird oﬀensichtlich maximal, wenn man alles Gewicht auf ein betragsm¨assig gr¨osstes Element legt. Ist dies dll, so w¨ahlen wir also xl = el und erhalten max x̸=o ∥Dx∥2 2 ∥x∥2 2 = ∥Del∥2 2 ∥el∥2 2 = |dll|2 1 = max 1≤k≤n |dkk|2 . Das heisst, es ist ∥D∥2 = max x̸=o ∥Dx∥2 ∥x∥2 = max 1≤k≤n |dkk| . (6.69) ♦ ¨Ahnlich wie die Vektornormen, insbesondere die bereits in Kapi- tel 2 diskutierte 2–Norm von En, hat auch die Operatornorm Ei- genschaften, die als Axiome eines allgemeineren Normbegriﬀes f¨ur Abbildungen und Matrizen dienen k¨onnen: c⃝M.H. Gutknecht 11. April 2016 LA-Skript 6-21 Kapitel 6 — Vektorr¨aume mit Skalarprodukt Lineare Algebra (2009) Satz 6.18 Die durch (6.63) deﬁnierte induzierte Operatornorm hat die folgenden Eigenschaften: (OpN1) Sie ist positiv deﬁnit: ∥F ∥ ≥ 0 (∀ F ∈ L(X, Y ) ) , ∥F ∥ = 0 =⇒ F = O . (OpN2) Sie ist dem Betrage nach homogen: ∥α F ∥ = |α| ∥F ∥ (∀ F ∈ L(X, Y ), ∀ α ∈ E ) . (OpN3) Die Dreiecksungleichung gilt: ∥F + G∥ ≤ ∥F ∥ + ∥G∥ (∀ F, G ∈ L(X, Y ) ) . (6.70) (OpN4) F¨ur zusammengesetzte Abbildungen gilt: ∥G ◦ F ∥ ≤ ∥G∥ ∥F ∥ (∀ F ∈ L(X, Y ), G ∈ L(Y, Z) ) . (6.71) (OpN5) Sie ist kompatibel mit den Vektornormen in X, Y : ∥F x∥Y ≤ ∥F ∥ ∥x∥X (∀ F ∈ L(X, Y ), ∀ x ∈ X ) . (6.72) Beweis: (OpN1): Aus der Deﬁnition (6.63) ist klar, dass ∥F ∥ ≥ 0 gilt und dass ∥F ∥ = 0 bedeutet, dass F x = o f¨ur alle x, also F die Nullabbildung O ist. (OpN2): Unter Verwendung der Normeigeschaft (N2) in Y folgt, dass ∥αF ∥ = sup ∥x∥X =1 ∥αF x∥Y = sup ∥x∥X =1 (|α| ∥F x∥Y ) = |α| sup ∥x∥X =1 ∥F x∥Y = |α| ∥F ∥ . (OpN3): Hier verwendet man u.a. die Dreiecksungleichung (N3) aus Y : ∥F + G∥ = sup ∥x∥X =1 ∥(F + G)(x)∥Y = sup ∥x∥X =1 ∥F x + Gx∥Y ≤ sup ∥x∥X =1 (∥F x∥Y + ∥Gx∥Y ) ≤ sup ∥x∥X =1 ∥F x∥Y + sup ∥x∥X =1 ∥Gx∥Y = ∥F ∥ + ∥G∥ . (OpN5): Folgt sofort daraus, dass f¨ur festes x = ̃x gilt ∥F ̃x∥Y ∥̃x∥X ≤ sup ∥x∥X =1 ∥F x∥Y ∥x∥X = ∥F ∥ . (OpN4): Wir wenden (OpN5) zun¨achst auf G, dann auf F an, umzu sehen, dass ∥(G ◦ F )(x)∥Z = ∥G(F x)∥Z ≤ ∥G∥ ∥F x∥Y ≤ ∥G∥ ∥F ∥ ∥x∥X . LA-Skript 6-22 11. April 2016 c⃝M.H. Gutknecht Lineare Algebra (2009) Kapitel 6 — Vektorr¨aume mit Skalarprodukt Dividiert man dies unter der Annahme x ̸= o durch ∥x∥X und nimmt man das Supremum ¨uber alle x ̸= o, folgt die Behauptung. F¨ur quadratische Matrizen, wo X = Y = Z := En, wird der Begriﬀ “Matrixnorm” nun allgemein wie folgt deﬁniert: Definition: Eine Matrixnorm [matrix norm] ist eine Funktion ∥.∥ : En×n → R , A ↦→ ∥A∥ (6.73) mit den (an die Situation angepassten) Eigenschaften (OpN1)– (OpN4). Gilt zus¨atzlich analog zu (OpN5) ∥Ax∥ ≤ ∥A∥ ∥x∥ f¨ur alle A ∈ En×n, x ∈ En , (6.74) so heisst die Matrixnorm kompatibel [compatible] mit der in En verwendeten Vektornorm (die nicht die 2–Norm sein muss). ▲ Hier ist ein Beispiel einer Matrixnorm, die einfacher zu berechnen ist als die Spektralnorm: Beispiel 6.13: Die Frobenius–Norm ist deﬁniert durch ∥A∥F :≡ √ √ √ √ n∑ k=1 n∑ l=1 |akl|2 . (6.75) Man m¨usste nat¨urlich beweisen, dass die Axiome (OpN1)–(OpN4) erf¨ullt sind. Wir wollen hier nur zeigen, dass diese Matrixnorm kompatibel ist mit der 2–Norm (als Vektornorm im En). Wir wenden dazu die Schwarzsche Ungleichung(2.52) f¨ur k = 1, . . . , n an auf den Zeilenvektor( ak1 · · · akn ) und einen n–Vektor x: ∥Ax∥ 2 2 = n∑ k=1 ( n∑ l=1 akl xl )2 ≤ n∑ k=1   n∑ l=1 a2 kl · n∑ j=1 x2 j   = n∑ j=1 x2 j · n∑ k=1 ( n∑ l=1 a2 kl ) = ∥x∥ 2 2 ∥A∥ 2 F . ♦ Die Genauigkeit, mit der man ein regul¨ares Gleichungssystem Ax = b auf einem Computer mit Gleitkomma–Arithmetik l¨osen kann, h¨angt ab vom Produkt ∥A∥∞ ∥A−1∥∞, worin ∥A∥∞ die von der Vektornorm ∥.∥∞ aus (2.60) induzierte Operatornorm bezeichnet. In anderen Situationen spielt das Produkt ∥A∥2 ∥A−1∥2 eine wich- tige Rolle f¨ur die erreichbare Genauigkeit einer Rechnung. Man de- ﬁniert deshalb: c⃝M.H. Gutknecht 11. April 2016 LA-Skript 6-23 Kapitel 6 — Vektorr¨aume mit Skalarprodukt Lineare Algebra (2009) Definition: Die Konditionszahl [condition number] einer re- gul¨aren Matrix A bez¨uglich einer gewissen Norm13 ∥.∥ oder, kurz, die Kondition [condition] einer regul¨aren Matrix A ist die Zahl κ(A) = ∥A∥ ∥A−1∥ . (6.76) Insbesondere ist die 2–Norm–Konditionszahl [2–norm condition number] deﬁniert als κ2(A) = ∥A∥2 ∥A−1∥2 . (6.77) ▲ Auf die Berechnung der 2–Norm–Konditionszahl werden wir eben- falls in den Abschnitten 10.6 und 11.2 zur¨uckkommen. 13Gemeint ist eine Matrixnorm oder eine Vektornorm mit der induzierten Operatornorm. LA-Skript 6-24 11. April 2016 c⃝M.H. Gutknecht Lineare Algebra (2009) Kapitel 7 — Kleinste Quadrate, QR Kapitel 7 Die Methode der kleinsten Quadrate und die QR–Zerlegung einer Matrix In diesem Abschnitt beschr¨anken wir uns auf den Vektorraum Em mit dem Standardskalarprodukt aus Abschnitt 2.4. Alles gilt aber sinngem¨ass f¨ur beliebige m-dimensionale Vektorr¨aume mit Skalar- produkt, wie sie in Kapitel 6 eingef¨uhrt worden sind. In Abschnitt 2.5 haben wir schon die orthogonale Projektion auf eine Gerade durch O, d.h. auf einen eindimensionalen Unterraum betrachtet, nun wollen wir jene auf mehrdimensionale Unterr¨aume bestimmen und verschiedene Methoden zu deren Berechnung und zur L¨osung des entsprechenden Minimumproblems kennen lernen. Dieses ist unter anderem als Problem der kleinsten Quadrate be- kannt. Dabei gehen wir nun von einer allgemeinen Deﬁnition der Projektion aus. 7.1 Orthogonalprojektionen Definition: Eine lineare Abbildung P : Em → Em heisst Pro- jektion [projection] oder Projektor [projector], falls P2 = P . (7.1) Eine Projektion heisst Orthogonalprojektion [orthogonal pro- jection] oder orthgonaler Projektor [orthgonal projector], falls zus¨atzlich gilt ker P ⊥ im P , d.h. N (P) ⊥ R(P) . (7.2) Andernfalls ist es eine schiefe [oblique] Projektion. ▲ Bemerkung: Die Bedingung (7.1) entspricht der anschaulichen Vorstellung einer Projektion: ¨Ubt man auf das Bild Py ∈ im P eines beliebigen Punktes y nochmals dieselbe Projektion aus, so bleibt Py “an Ort”: P(Py) = Py. Die Bedingung (7.2) wird anschaulich, wenn man beachtet, dass y − Py ∈ ker P, weil wegen (7.1) gilt P(y − Py) = Py − P2y = o. ▼ Lemma 7.1 Ist P ein Projektor, so ist auch I − P ein Projektor und es gilt: im (I − P) = ker P , ker (I − P) = im P . (7.3) c⃝M.H. Gutknecht 11. April 2016 LA-Skript 7-1 Kapitel 7 — Kleinste Quadrate, QR Lineare Algebra (2009) Beweis: Wegen (7.1) ist (I − P)2 = I − 2P + P2 = I − P, d.h. es gilt (7.1) auch f¨ur I − P. Ist Px = o, so ist x = x − Px = (I − P)x ∈ im (I − P). Ist umgekehrt x = (I − P)y ∈ im (I − P), so folgt Px = (P − P2)y = o. Die zweite Gleichung in (7.3) ergibt sich am einfachsten, wenn man in der ersten den Projektor P durch den Projektor I − P ersetzt. Die Deﬁnition (7.2) der Orthogonalit¨at eines Projektors kann man durch ¨aquivalente Bedingungen ersetzen: Satz 7.2 F¨ur einen Projektor P : Em → Em sind folgende Aus- sagen ¨aquivalent: (i) P ist orthogonaler Projektor; (ii) I − P ist orthogonaler Projektor; (iii) PH = P . Beweis: Die ¨Aquivalenz von (i) und (ii) folgt sofort aus Lemma 7.1. Nehmen wir also an, P und I − P seien orthogonale Projektoren. Zudem sei x ∈ ker P, so dass wegen (7.2) f¨ur alle y ∈ Em gilt 〈PHx, y〉 = ⟨x, Py⟩ = 0 , also PHx = o, d.h. x ∈ ker PH. Somit ist ker P ⊆ ker PH, aber weil Rang P = Rang PH (Korollar 5.14), muss sogar ker P = ker PH sein. Analog ist ker (I − P) = ker (I − PH), also wegen Lemma 7.1 auch im P = im PH, denn PH ist wegen (PH)2 = (P2)H = PH sicher auch ein Projektor. Da ein Projektor durch seinen Kern (der auf o abgebildet wird) und sein Bild (dessen Punkte fest bleiben), eindeutig bestimmt ist (denn Kern und Bild sind bei einer Projektion komplement¨are Un- terr¨aume: ker P ⊕ im P = Em), folgt, dass P = PH. Es sei umgekehrt P ein Projektor mit P = PH, und es seien x ∈ ker P und y ∈ im P, so dass gilt Px = o und y = Py. Dann folgt ⟨x, y⟩ = ⟨x, Py⟩ = 〈PHx, y〉 = ⟨Px, y⟩ = 0 , was gleichbedeutend ist mit (7.2), also ist P ein orthogonaler Projektor. Um eine Formel f¨ur die Orthogonalprojektion auf einen vorgegebe- nen Unterraum (“Projektionsebene”) anzugeben, wollen wir anneh- men, dieser Unterraum sei aufgespannt durch n linear unabh¨angige, gegebene Vektoren a1, . . . , an ∈ Em, und wir betrachten diese Vek- toren als Kolonnenvektoren einer m×n-Matrix A. Der vorgegebene Unterraum ist dann gerade der Kolonnenraum R(A). Es gilt: Lemma 7.3 Sind die Kolonnen einer m × n–Matrix A linear un- abh¨angig, das heisst ist Rang A = n (≤ m), so ist AHA regul¨ar. Beweis: Aus AHAx = o folgt xHAHAx = 0, d.h. ⟨Ax, Ax⟩ = 0, also Ax = o. Weil Rang A = n ist, ist aber ker A = {o}, d.h. AHA ist regul¨ar. LA-Skript 7-2 11. April 2016 c⃝M.H. Gutknecht Lineare Algebra (2009) Kapitel 7 — Kleinste Quadrate, QR Satz 7.4 Die Orthogonalprojektion PA : Em → im A ⊆ Em auf den Kolonnenraum R(A) ≡ im A einer m × n–Matrix A mit Rang n (≤ m) ist gegeben durch PA :≡ A(AHA)−1 AH . (7.4) Beweis: Zun¨achst ist P2 A = A(AHA)−1AHA (AHA)−1AH = PA , PA ist also ein Projektor. Weiter ist im PA = im A, denn es ist klar, dass im PA ⊆ im A, und andererseits existiert zu z = Ax ∈ im A ein y mit PAy = z, denn die entsprechende Gleichung f¨ur y, A(AHA)−1AHy = z = Ax l¨asst sich l¨osen, indem man y so w¨ahlt, dass (AHA)−1AHy = x, d.h. AHy = AHAx = AHz . Zum Beispiel ist y = z eine L¨osung; sie ist aber nicht eindeutig, falls n < m. Aus (7.4) sieht man weiter sofort, dass PH A = PA, also ist PA eine Orthogonalprojektion, vgl. Satz 7.2 (iii). Es ist im ¨ubrigen auch einfach zu zeigen, dass aus y ⊥ im A, d.h. yHAx = 0 (∀x), folgt, dass y ∈ ker PA. Aus Dimensionsgr¨unden gilt dann ker PA ⊥ im PA, die urspr¨ungliche Bedingung (7.2) f¨ur einen orthogonalen Projektor. Geometrisch ist klar, dass ein Projektor nicht davon abh¨angt, durch welche Vektoren man die “Projektionsebene” R(A) ≡ im A auf- spannt. Man kann das auch formal beweisen. Wir d¨urfen also ins- besondere a1, . . . , an ersetzen durch n orthonormale Vektoren, d.h. wir ersetzen A ∈ Em×n durch Q ∈ Em×n mit R(Q) = R(A) und QHQ = In. Damit vereinfacht sich die Formel (7.4) f¨ur den Projek- tor: Korollar 7.5 Die Orthogonalprojektion PQ : Em → im Q ⊆ Em auf den Kolonnenraum R(Q) ≡ im Q einer m × n–Matrix Q =( q1 · · · qn ) mit orthonormalen Kolonnen ist gegeben durch PQ :≡ Q QH . (7.5) Es gilt also f¨ur jedes y ∈ Em PQ y = Q QHy = n∑ j=1 qjqH j y = n∑ j=1 qj ⟨qj, y⟩ . (7.6) Soll eine Orthogonalprojektion auf einen Kolonnenraum R(A) be- rechnet werden, so kann man also auf zwei Arten vorgehen: Entwe- der man wendet die allgemeine Formel (7.4) an oder man ersetzt A durch eine Matrix Q mit orthonormalen Kolonnen und wendet dann die einfacheren Formeln (7.5) und (7.6) an. Auf den ¨Ubergang von A auf Q werden wir in Abschnitt 7.3 zur¨uckkommen. c⃝M.H. Gutknecht 11. April 2016 LA-Skript 7-3 Kapitel 7 — Kleinste Quadrate, QR Lineare Algebra (2009) Beispiel 7.1: Wir wollen im R4 orthogonal auf den 2-dimensionalen Unterraum projizieren, der aufgespannt wird durch die Kolonnen der Matrix A = ( a1 a2 ) =     4 6 2 −2 2 6 1 −7     . (7.7) Berechnen wir ATA = ( 25 25 25 125 ) , (ATA)−1 = 1 100 ( 5 −1 −1 1 ) , (7.8) so wird nach (7.4) PA = 1 100     4 6 2 −2 2 6 1 −7     ( 5 −1 −1 1 ) ( 4 2 2 1 6 −2 6 −7 ) (7.9) = 1 100     4 6 2 −2 2 6 1 −7     ( 14 12 4 12 2 −4 4 −8 ) (7.10) =     0.68 0.24 0.40 0.00 0.24 0.32 0.00 0.40 0.40 0.00 0.32 −0.24 0.00 0.40 −0.24 0.68     . (7.11) Anwendung des Gram-Schmidt-Verfahrens auf die zwei Kolonnen a1 und a2 gibt anderseits ∥a1∥ = 5, q1 = 1 5 ( 4 2 2 1 )T , ⟨q1, a2⟩ = 5, ̃q2 = ( 2 −4 4 −8 )T , (7.12) ∥̃q2∥ = 10, q2 = 1 5 ( 1 −2 2 −4 )T , also die folgende Matrix Q mit orthonormalen Kolonnen: Q = ( q1 q2 ) = 1 5     4 1 2 −2 2 2 1 −4     . (7.13) Damit k¨onnen wir PA = PQ nach (7.5) auch darstellen durch PA = PQ = 1 25     4 1 2 −2 2 2 1 −4     ( 4 2 2 1 1 −2 2 −4 ) . (7.14) Man k¨onnte dieses Produkt ausmultiplizieren, um wieder (7.11) zu er- halten, aber je nach dem Verh¨altnis n : m und der Zahl der zu berech- nenden Matrix-Vektor-Produkte PAx ist es eﬃzienter, den Projektor in einer Produktform (7.9), (7.10) oder (7.14) zu belassen, wobei man im allgemeinen (7.10) aus (7.9) am besten durch L¨osen von n Gleichungs- systemen berechnen w¨urde. ♦ LA-Skript 7-4 11. April 2016 c⃝M.H. Gutknecht Lineare Algebra (2009) Kapitel 7 — Kleinste Quadrate, QR Eine Orthogonalprojektion hat die Eigenschaft, dass f¨ur Punkte y, die nicht in der “Projektionsebene” liegen, f¨ur die also y /∈ im P gilt, das Bild Py der am n¨achsten bei y liegende Punkt des Unter- raumes im P ist. Dies ergibt sich sofort aus dem Satz von Pythago- ras (Satz 2.13). Die Orthogonalprojektion liefert damit die L¨osung eines linearen Approximationsproblems in der 2–Norm: Satz 7.6 F¨ur eine Orthogonalprojektion P gilt: ∥y − Py∥2 = min z∈im P ∥y − z∥2 . (7.15) Analoges gilt in einem beliebigen Vektorraum mit Skalarprodukt f¨ur die durch das Skalarprodukt deﬁnierte Norm (6.7) aufgrund des allgemeinen Satzes von Pythagoras (Satz 6.2). 7.2 Die Methode der kleinsten Quadrate Wir betrachten nun das ¨uberbestimmte lineare Gleichungssy- stem [overdetermined linear system] Ax = y (7.16) mit einer “hohen” m × n–Matrix (m > n). Es hat im allgemeinen ja keine L¨osung. Wir wissen, dass genau dann eine existiert, wenn y ∈ R(A), aber das wird hier nicht vorausgesetzt. Weil man die Gleichungen (7.16) nur bis auf einen Fehler l¨osen kann, nennt man sie Fehlergleichungen. Wenn es keine exakte L¨osung gibt, ist es naheliegend, x ∈ En so zu w¨ahlen, dass der Residuenvektor [residual vector] (kurz: das Residuum [residual]) r :≡ y − Ax (7.17) minimale Euklidische Norm (2–Norm, “L¨ange”) hat, d.h., dass die Quadratsumme ∥r∥ 2 = m∑ k=1 r2 k falls r ∈ R m (7.18) bzw. ∥r∥ 2 = m∑ k=1 |rk|2 falls r ∈ C m (7.19) minimal wird. Die entsprechende L¨osung x heisst die L¨osung im Sinne der kleinsten Quadrate [least squares solution] des ¨uber- bestimmten Systems Ax = y. Wir nehmen an, dass die Kolonnen von A linear unabh¨angig sind, also ker A = {o} gilt und AHA nach Lemma 7.3 regul¨ar ist. Auf Grund von Satz 7.6 wissen wir, dass ∥r∥ minimal ist, wenn gilt Ax = PAy, d.h. Ax = A(AHA)−1AHy . c⃝M.H. Gutknecht 11. April 2016 LA-Skript 7-5 Kapitel 7 — Kleinste Quadrate, QR Lineare Algebra (2009) Da ker A = {o}, folgt hieraus x = (AHA)−1AH y (7.20) oder AHAx = AH y . (7.21) Dies sind die Normalgleichungen [normal equations]. Man be- kommt sie, indem man die Fehlergleichungen von links mit AH mul- tipliziert. Sie bedeuten gerade, dass r = y − Ax auf den Kolonnen von A senkrecht steht: AH r = o oder r ⊥ R(A) . (7.22) Die Matrix (AHA)−1AH in (7.20) heisst Pseudoinverse [pseudo- inverse] von A. (Falls Rang A < m, ist die Deﬁnition komplizierter.) Zusammengefasst gilt: Satz 7.7 Es sei A ∈ Em×n, Rang A = n ≤ m, y ∈ Em. Dann hat das ¨uberbestimmte Gleichungssystem Ax = y eine eindeutig bestimmte L¨osung x im Sinne der kleinsten Quadrate, d.h. x mit ∥Ax − y∥ 2 = min ̃x∈En ∥Ãx − y∥ 2 . (7.23) x kann berechnet werden durch L¨osen des regul¨aren Systems (7.21) der Normalgleichungen. Der Residuenvektor r :≡ y − Ax steht senkrecht auf R(A). Beispiel 7.2: Gegeben seien die 4 × 2 Matrix A aus (7.7) und y = ( 3 6 −3 9 )T. Es soll x = ( x1 x2 )T so bestimmt werden, dass ∥r∥ = ∥y − Ax∥ = ∥ ∥ ∥ ∥ ∥ ∥ ∥ ∥     3 6 −3 9     −     4 6 2 −2 2 6 1 −7     ( x1 x2 )∥ ∥ ∥ ∥ ∥ ∥ ∥ ∥ (7.24) minimal wird. Es ist ATy = ( 27 −75 )T und ATA = ( 25 25 25 125 ) . Die Normalgleichungen (7.21) lauten also ( 25 25 25 125 ) ( x1 x2 ) = ( 27 −75 ) . (7.25) Es wird x = ( 2.10 −1.02 )T, also r = y − Ax =     3 6 −3 9     −     4 6 2 −2 2 6 1 −7     ( 2.10 −1.02 ) =     0.72 −0.24 −1.08 −0.24     (7.26) und ∥r∥ = 1.3416... . LA-Skript 7-6 11. April 2016 c⃝M.H. Gutknecht Lineare Algebra (2009) Kapitel 7 — Kleinste Quadrate, QR ♦ In der Praxis kann das L¨osen der Normalgleichungen f¨ur grosse Matrizen ungenau sein, da mit grossen Rundungsfehlern behaftet, falls die Kolonnen von A nahezu linear abh¨angig sind. Es gibt je- doch mehrere Alternativen f¨ur die Bestimmung von x. Eine haben wir bereis zur Hand: die Anwendung des Gram-Schmidt-Verfahrens. Geometrisch sollte klar sein, dass folgendes gilt: Lemma 7.8 Unter den Voraussetzungen von Satz 7.7 kann man die Kleinste-Quadrate-L¨osung x bestimmen, indem man die Ko- lonnen a1, . . . , an von A und den Vektor an+1 :≡ y dem Schmidt- schen Orthogonalisierungsprozess (6.28) unterwirft und so das Lot ̃qn+1 :≡ y − Ax = r ⊥ R(A) , (7.27) von y auf R(A) = span {a1, . . . , an} bestimmt. Das Gleichungs- system Ax = y − ̃qn+1 ist dann exakt nach x auﬂ¨osbar. Wir werden dieses Vorgehen im Abschnitt 7.3 weiterverfolgen und neu interpretieren. Statt Ax = y − ̃qn+1 zu l¨osen, werden wir dort x auf elegantere und numerisch genauere Art bestimmen. Beispiel 7.3: Wir wollen die Beispiele 7.1 und 7.2 fortsetzen, aber im Moment nur das Residuum r = y − Ax der Kleinste-Quadrate-L¨osung x bestimmen. In Beispiel 7.1 haben wir bereits die Kolonnen von A aus (7.7) orthonormalisiert und in (7.13) Q = ( q1 q2 ) erhalten. Nun ist das Lot von y auf span {a1, a2} = span {q1, q2} zu f¨allen: Es wird ⟨q1, y⟩ = 27/5 = 5.4, ⟨q2, y⟩ = −51/5 = −10.2, und damit r = ̃q3 = y − q1 ⟨q1, y⟩ − q2 ⟨q2, y⟩ =     3 6 −3 9     − 27 25     4 2 2 1     + 51 25     1 −2 2 44     =     0.72 −0.24 −1.08 −0.24     wie in (7.26). ♦ ¨Uberbestimmte lineare Gleichungssysteme treten zum Beispiel auf, wenn man Parameter eines linearen Modells (Systems) durch ei- ne Messreihe bestimmt. Gew¨ohnlich wird man viel mehr Messun- gen machen als es Parameter gibt und dann die Parameter als L¨osung im Sinne der kleinsten Quadrate bestimmen (Ausgleichs- rechnung, lineare Regression). Beispiel 7.4: F¨ur den vertikalen freien Fall eines Massenpunktes im Vakuum gilt y(t) = x1t + x2t2 , wobei t: Zeit, y(t): zur¨uckgelegter Weg, x1: Anfangsgeschwindigkeit, x2: halbe Erdbeschleunigung. c⃝M.H. Gutknecht 11. April 2016 LA-Skript 7-7 Kapitel 7 — Kleinste Quadrate, QR Lineare Algebra (2009) Es sollen x1 und x2 bestimmt werden durch Messung von y(t) zu den Zeiten t = 1, 2, 3, 4 sec. Die Messreihe ergibt: t 1 2 3 4 Sekunden y(t) 13.0 35.5 68.0 110.5 Meter . Also ist A =     1 1 2 4 3 9 4 16     , y =     13.0 35.5 68.0 110.5     . F¨ur die Normalgleichungen brauchen wir: ATA = ( 30 100 100 354 ) , ATy = ( 730.0 2535.0 ) . Zu l¨osen ist also das Normalgleichungssystem x1 x2 1 30 100 730.0 100 354 2535.0 . das x1 = 7.9355 . . . , x2 = 4.9194 . . . ergibt und die Approximation 2x2 = 9.8387... f¨ur die Erdbeschleunigung liefert. ♦ 7.3 Die QR–Zerlegung einer Matrix Wir wollen in diesem Abschnitt das Gram–Schmidt–Verfahren im Falle V = Em neu interpretieren und dann damit das Problem der kleinsten Quadrate l¨osen. Gegeben sind also zun¨achst n linear unabh¨angige Vektoren a1, . . . , an, die wir als Kolonnen einer m×n– Matrix A auﬀassen k¨onnen. Statt b1, . . . , bn nennen wir die neu konstruierten orthonormalen Vektoren q1, . . . , qn — wie schon in den Abschnitten 7.1–7.2. Wir werden sie sp¨ater als Kolonnen einer m × n–Matrix Q auﬀassen. Diese Matrix wird also orthonormierte Kolonnen haben, was heisst, dass QHQ = In (falls E = C) bzw. QTQ = In (falls E = R). (7.28) Zur Erinnerung: nur wenn m = n ist, ist Q unit¨ar oder orthogonal, womit dann zus¨atzlich gilt: QQH = In bzw. QQT = In. Wir schreiben zuerst das Gram–Schmidt–Verfahren f¨ur diese Situa- tion nochmals auf: LA-Skript 7-8 11. April 2016 c⃝M.H. Gutknecht Lineare Algebra (2009) Kapitel 7 — Kleinste Quadrate, QR Algorithmus 7.1 (klassisches Gram–Schmidt–Verfahren im Em) Es seien a1, . . . , an ∈ Em gegebene linear unabh¨angige Vektoren. Man berechne q1 := a1 ∥a1∥ , ̃qk := ak − ∑k−1 j=1 qj ⟨qj, ak⟩ , qk := ̃qk ∥̃qk∥ ,    (k = 2, . . . , n) . (7.29) Durch Vergleich mit (7.6) ist die Summe in der Formel f¨ur ̃qk nun klar als Projektion von ak auf span {q1, . . . , qk−1} erkennbar; es ist also ̃qk das Lot von ak auf span {q1, . . . , qk−1}. Auﬂ¨osen nach den gegebenen Vektoren ak liefert a1 = q1∥a1∥ , ak = qk∥̃qk∥ + k−1∑ j=1 qj ⟨qj, ak⟩ (k = 2, . . . , n). Nun deﬁnieren wir die Koeﬃzienten r11 :≡ ∥a1∥ , rjk :≡ ⟨qj, ak⟩ , j = 1, . . . , k − 1, rkk :≡ ∥̃qk∥ , } k = 2, . . . , n , (7.30) die wir erg¨anzen k¨onnen durch rjk :≡ 0 (j = k + 1, . . . , n). Damit erhalten wir f¨ur k = 1, . . . , n ak = qk rkk + k−1∑ j=1 qj rjk = k∑ j=1 qj rjk = n∑ j=1 qj rjk . (7.31) Deﬁnieren wir die bereits erw¨ahnten (m × n)–Matrizen A :≡ ( a1 . . . an ) , Q :≡ ( q1 . . . qn ) sowie die (n × n)–Rechtsdreiecksmatrix R :≡      r11 r12 . . . r1n 0 r22 . . . r2n ... . . . . . . ... 0 . . . 0 rnn      (7.32) wird aus (7.31) kurz A = QR . (7.33) Definition: Die Zerlegung (7.33) einer m × n Matrix A mit Maximalrang n ≤ m in eine m × n Matrix Q mit orthonormalen Kolonnen mal eine n × n Rechtdreiecksmatrix mit positiven Dia- gonalelementen heisst QR–Faktorisierung [QR factorization] von A. ▲ c⃝M.H. Gutknecht 11. April 2016 LA-Skript 7-9 Kapitel 7 — Kleinste Quadrate, QR Lineare Algebra (2009) Wir k¨onnen die n orthonormalen Vektoren q1, . . . , qn zu einer Or- thonormalbasis {q1, . . . , qn, . . . , qm} von Em erg¨anzen und erhalten so eine unit¨are (oder orthogonale) quadratische Matrix ̃Q :≡ ( Q Q⊥ ) :≡ ( q1 . . . qn qn+1 . . . qm ) . (7.34) Um R kompatibel zu machen mit ̃Q, k¨onnen wir R mit m − n Zeilen aus Nullen erg¨anzen zu einer m × n–Matrix ̃R: ̃R :≡ ( R O ) . (7.35) Dann gilt A = QR = ( Q Q⊥ ) ( R O ) = ̃Q ̃R . (7.36) Hierbei bilden die Kolonnen qn+1, . . . , qm von Q⊥ nat¨urlich gerade eine Orthonormalbasis des zu R(A) orthogonalen komplement¨aren Unterraumes. Definition: Die Zerlegung A = ̃Q ̃R einer m × n Matrix A mit Maximalrang n ≤ m in eine unit¨are (oder orthogonale) m × m Matrix ̃Q mal eine (rechteckige) m × n Rechtdreiecksmatrix mit positiven Diagonalelementen heisst QR–Zerlegung1 [QR decom- position] von A. ▲ Nun k¨onnen wir Lemma 7.8 zum gesuchten Satz umformulieren: Satz 7.9 Das Gram-Schmidt-Verfahren, angewandt auf die Ko- lonnen a1, . . . , an einer m × n–Matrix A liefert die QR- Faktorisierung (7.33) dieser Matrix. Erg¨anzt man A durch den m- Vektor y, so liefert das Verfahren (vor dem Normieren) zus¨atzlich den zu R(A) orthogonalen Residuenvektor r gem¨ass der Formel r = y − n∑ j=1 qj ⟨qj, y⟩ = y − QQHy . (7.37) Die L¨osung x des Kleinste–Quadrate–Problems erf¨ullt das durch R¨uckw¨artseinsetzen l¨osbare System Rx = QHy . (7.38) Beweis: Der erste Teil des Satzes ist eine Zusammenfassung unserer Herleitung von (7.33). Die Formel (7.37) f¨ur r ergibt sich aus Lemma 7.8 und den Formeln (7.29) des Gram–Schmidt–Verfahrens. Es bleibt also nur (7.38) zu zeigen. Aus Ax = y−r folgt durch Einsetzen von A = QR und Multiplikation mit QH: QHQ ︸ ︷︷ ︸ = In Rx = QHy − QHr ︸︷︷︸ = o , (7.39) 1In bezug auf die sprachliche Unterscheidung zwischen “QR–Faktorisierung” und “QR–Zerlegung” folgen wir G.W. Stewart: Matrix Algorithms, Vol. 1, SIAM 1998. Sie ist noch nicht vielerorts akzeptiert. LA-Skript 7-10 11. April 2016 c⃝M.H. Gutknecht Lineare Algebra (2009) Kapitel 7 — Kleinste Quadrate, QR wobei wir benutzt haben, dass r auf den Kolonnen von Q senkrecht steht, wie das aus der bekannten, in Satz 7.7 beschriebenen Eigenschaft der Minimall¨osung folgt. Die L¨osung eines Kleinste-Quadrate-Problems durch QR–Zerlegung der Matrix A ist weniger mit Rundungsfehlern behaftet als die L¨osung via die Normalgleichungen, und zwar vor allem, wenn die Kolonnen von A beinahe linear abh¨angig sind. Allerdings ist dann auch das klassische Gram–Schmidt–Verfahren nicht sehr exakt. Aber es gibt daneben weitere, numerisch stabilere Methoden zur QR– Zerlegung einer Matrix. Beispiel 7.5: Wir wollen zuerst das Beispiel 7.3 abschliessen. Den Formeln (7.12) aus Beispiel 7.1 entnehmen wir, dass A aus (7.7) die QR–Faktorisierung     4 6 2 −2 2 6 1 −7     ︸ ︷︷ ︸ A = 1 5     4 1 2 −2 2 2 1 −4     ︸ ︷︷ ︸ Q ( 5 5 0 10 ) ︸ ︷︷ ︸ R hat, und aus Beispiel 7.3 kennen wir schon QTy = ( 27/5 −51/5 )T = ( 5.4 −10.2 )T. Zu l¨osen bleibt also das System Rx = QTy, konkret x1 x2 1 5 5 5.4 0 10 −10.2 das x1 = 2.10, x2 = −1.02 ergibt, was wir in (7.25) schon aus den Normalgleichungen erhalten haben. ♦ Beispiel 7.6: Im Beispiel 7.4 erh¨alt man, gerundet auf vier Stellen nach dem Komma, aus dem Gram–Schmidt–Verfahren     1 1 2 4 3 9 4 16     ︸ ︷︷ ︸ A =     0.1826 −0.5133 0.3651 −0.5866 0.5477 −0.2200 0.7303 0.5866     ︸ ︷︷ ︸ Q ( 5.4772 18.2574 0 4.5461 ) ︸ ︷︷ ︸ R , ( 0.1826 0.3651 0.5477 0.7303 −0.5133 −0.5866 −0.2200 0.5866 ) ︸ ︷︷ ︸ QT     13.0 35.5 68.0 110.5     ︸ ︷︷ ︸ y = ( 133.2792 22.3637 ) ︸ ︷︷ ︸ QTy , also f¨ur das System Rx = QTy x1 x2 1 5.4772 18.2574 133.2792 0 4.5461 22.3637 was wie in Beispiel 7.4 wieder x1 = 7.9355, x2 = 4.9194 liefert. ♦ c⃝M.H. Gutknecht 11. April 2016 LA-Skript 7-11 Kapitel 7 — Kleinste Quadrate, QR Lineare Algebra (2009) 7.4 Die QR–Zerlegung mit Pivotieren Wir haben bisher bei der in Abschnitt 7.3 mit dem klassischen Gram–Schmidt–Verfahren berechneten QR–Zerlegung angenommen, dass die m × n Ausgangsmatrix A Maximalrang n hat, also insbe- sondere n ≤ m ist. In der Tat kann sonst das Verfahren (Algorith- mus 7.1) zusammenbrechen, weil ̃qk = o sein kann, womit dann qk nicht deﬁniert ist. Dies kann schon f¨ur k ≤ Rang A eintreten, wie die Beispiele   1 0 1 0 0 1 0 0 1   ,   1 2 1 2 4 1 3 6 1   zeigen, wo der Rang zwei ist, aber ̃q2 = o wird. Oﬀensichtlich m¨usste man hier die zweite und die dritte Kolonne vertauschen um einen zweiten normierten Basisvektor q2 ⊥ q1 zu ﬁnden. Mit anderen Worten, man muss Kolonnen-Pivotieren zulassen. Aber wie ﬁndet man eine zum Vertauschen geeignete Kolonne? Dazu modiﬁzieren wir das Gram–Schmidt–Verfahren noch in ande- rer Hinsicht: durch Vertauschen von Schleife und Summe in (7.29). Algorithmus 7.2 (modiﬁziertes Gram-Schmidt-Verfahren mit Kolonnen-Pivotieren) Es sei A = ( a1 . . . an ) ∈ Em×n. Man berechne: q1 := a1 ∥a1∥ , ̃qi := ai − q1 ⟨q1, ai⟩ (i = 2, . . . , n) ; w¨ahle p ≥ k mit ∥̃qp∥ ̸= 0 und vertausche Kolonnen p und k; berechne qk := ̃qk ∥̃qk∥ , ̃qi := ̃qi − qk ⟨qk, ̃qi⟩ (i = k + 1, . . . , n); ist ∥̃qk+1∥ = · · · = ∥̃qn∥ = 0 , so gilt Rang A = k und man ist fertig    (k = 2, . . . , n) . (7.40) Um zu beweisen, dass in exakter Arithmetik das modiﬁzierte und das klassisische Gram-Schmidt-Verfahren die gleichen Vektoren qk erzeugen, m¨usste man zeigen, dass in (7.40) gilt: ⟨qk, ̃qi⟩ = ⟨qk, ai⟩. Sind in Algorithmus 7.2 keine Kolonnenvertauschungen n¨otig, gilt nach wie vor A = QR, wobei nun rki :≡ ⟨qk, ̃qi⟩ f¨ur k < i. Ist Rang A < n, so werden Vertauschungen ausgef¨uhrt, und man hat AP = QR (7.41) mit einer m × m Permutationsmatrix P, der m × r Matrix Q :≡( q1 . . . qr ) mit orthonormalen Kolonnen, die eine Basis von im A = R(A) bilden, und einer rechteckigen, r × n Rechtsdreiecks- matrix R. Dabei ist r :≡ Rang A. LA-Skript 7-12 11. April 2016 c⃝M.H. Gutknecht Lineare Algebra (2009) Kapitel 8 — Determinanten Kapitel 8 Determinanten Die Determinante einer quadratischen Matrix ist ein zentraler Be- griﬀ und ein wichtiges Hilfsmittel in der klassischen Theorie der Matrizen. Wir werden sie vor allem f¨ur die Deﬁnition des soge- nannten charakteristischen Polynoms brauchen, dessen Nullstellen die Eigenwerte der Matrix sind. Die Determinante ist dagegen kaum je n¨utzlich, wenn es um nume- rische Berechnungen geht. Glaubt man sie zu brauchen, hat man in der Regel den falschen Weg gew¨ahlt, um das Problem zu l¨osen! Aber es gibt Ausnahmen, zum Beispiel Anwendungen in der Physik. Es gibt verschiedene M¨oglichkeiten, die Determinante einzuf¨uhren. Wir w¨ahlen hier die klassische Deﬁnition durch eine etwas kompli- zierte, in der Praxis unbrauchbare Formel, aus der aber alles folgt, insbesondere auch eine eﬃziente Berechnungsart. 8.1 Permutationen Wir m¨ussen uns zuerst an die Deﬁnition und die Eigenschaften der Permutationen der Zahlmenge {1, . . . , n} erinnern. Definition: Eine Permutation [permutation] von n Elementen ist eine eineindeutige Abbildung der Menge {1, . . . , n} auf sich. Die Menge aller dieser Permutationen sei Sn. Eine Transposition [transposition] ist eine Permutation, bei der nur zwei Elemente vertauscht werden. ▲ Satz 8.1 Es gibt n! Permutationen in Sn. Beweis: Jede Permutation entspricht einer Anordnung der Zahlen 1, . . . , n. F¨ur n = 1 gilt: S1 enth¨alt nur die Identit¨at, also nur 1 Ele- ment. Betrachtet wir nun in Sn jene Abbildungen, die die Reihenfolge der Zahlen 1, . . . , n − 1 nicht ver¨andern. Es gibt n von ihnen, denn man kann die Zahl n an n Stellen zwischen die restlichen einf¨ugen. Wir d¨urfen die Induktionsvoraussetzung machen, dass die restlichen n − 1 Zahlen auf (n − 1)! Arten angeordnet werden k¨onnen. Also enth¨alt Sn total n(n − 1)! = n! Elemente. Zwei Permutationen p1 und p2 kann man nat¨urlich zusammensetzen zu p2◦p1. Diese Zusammensetzung soll als Produkt von Permutatio- nen aufgefasst werden. Sn bildet mit dieser Operation eine Gruppe mit n! Elementen, die nicht kommutativ ist, wenn n ≥ 3. In der Algebra nennt man Sn die symmetrische Gruppe [symmetric group]. c⃝M.H. Gutknecht 11. April 2016 LA-Skript 8-1 Kapitel 8 — Determinanten Lineare Algebra (2009) Satz 8.2 F¨ur n > 1 kann jede Permutation p als Produkt von Transpositionen tk benachbarter Elemente dargestellt werden: p = tν ◦ tν−1 ◦ · · · ◦ t2 ◦ t1 . (8.1) Die Darstellung ist im allgemeinen nicht eindeutig, aber die An- zahl der Transpositionen ist entweder immer gerade oder immer ungerade. Beispiel 8.1: Die Permutation p : (1, 2, 3, 4) ↦→ (4, 2, 1, 3) ist darstell- bar als (1, 2, 3, 4) ↦→ (1, 2, 4, 3) ↦→ (1, 4, 2, 3) ↦→ (4, 1, 2, 3) ↦→ (4, 2, 1, 3) . Das sind vier Transpositionen benachbarter Elemente, d.h. es ist p = t4 ◦ t3 ◦ t2 ◦ t1. Die inverse Permutation p−1 ist gegeben durch p−1 = t1 ◦ t2 ◦ t3 ◦ t4, denn es ist ja t−1 k = tk. ♦ Beweis von Satz 8.2: Die Aussagen sind trivial im Falle n = 2: es gibt nur zwei Permutationen, die Identit¨at e und die Vertauschung t : {1, 2} ↦→ {2, 1}, die eine Transposition benachbarter Elemente ist. Je- de Zusammensetzung einer geraden Anzahl von Exemplaren der Trans- position t ergibt e, jedes Produkt einer ungeraden Anzahl wieder t. F¨ur den Induktionsbeweis nehmen wir an, die Aussagen seien richtig f¨ur Permutationen von 1, . . . , n − 1. Wir argumentieren ¨ahnlich wie im Beweis von Satz 8.1: ausgehend von ihrer urspr¨unglichen Position am Ende l¨asst sich die Position der Zahl n im Bild einer Permutation p : {1, . . . , n} ↦→ {p(1), . . . , p(n)} (8.2) oﬀensichtlich durch ein Produkt von Transpositionen benachbarter Ele- mente erreichen, sagen wir durch tµ ◦ tµ−1 ◦ · · · ◦ t1. Die Permutation der ¨ubrigen n − 1 Zahlen ist nach Induktionsvoraussetzung auch als ein solches Produkt darstellbar, sagen wir als tν ◦ tν−1 ◦ · · · ◦ tµ+1. Damit hat p die Darstellung (8.1). Nat¨urlich ist diese Darstellung nicht eindeutig wenn n > 1, denn f¨ur jede Vertauschung t gilt t ◦ t = e. Es seien nun p = tν ◦ tν−1 ◦ · · · ◦ t2 ◦ t1 = t′ ν′ ◦ t′ ν′−1 ◦ · · · ◦ t′ 2 ◦ t′ 1 (8.3) zwei verschiedene Darstellungen von p durch Transpositionen benach- barter Elemente. Sie brauchen nicht auf obige Art konstruiert zu sein. Wir betrachten f¨ur beliebiges x = ( x1 . . . xn )T ∈ Rn das Produkt Ψp = ∏ i>j(xp(i) − xp(j)) = ± ∏ i>j(xi − xj) = ±Ψe (8.4) Das Vorzeichen h¨angt nur ab von der Anzahl Transpositionen in der Darstellung (8.1), denn f¨ur eine beliebige Transposition t gilt Ψt = −Ψe. Aus (8.3) folgt damit, dass Ψp = (−1)νΨe = (−1)ν′Ψe , was impliziert, dass die Diﬀerenz ν − ν′ gerade ist. LA-Skript 8-2 11. April 2016 c⃝M.H. Gutknecht Lineare Algebra (2009) Kapitel 8 — Determinanten Definition: Das Signum [sign] einer Permutation p ist deﬁniert als sign p = { +1 falls ν in (8.1) gerade, −1 falls ν in (8.1) ungerade. (8.5) ▲ 8.2 Determinante: Deﬁnition, Eigenschaften Definition: Die Determinante [determinant] einer n×n–Matrix A ist deﬁniert als die Summe det A = ∑ p∈Sn sign p · a1,p(1) a2,p(2) · · · an,p(n) , (8.6) die ¨uber die n! Permutationen p : (1, 2, . . . , n) ↦→ (p(1), p(2), . . . , p(n)) l¨auft. Statt det (A) schreibt man oft auch |A|, wobei man dann die Klammern der Matrix wegl¨asst, wenn sie durch ihre Elemente gegeben ist (und n > 1 ist). ▲ Beispiel 8.2: det ( 2 1 3 4 ) = ∣ ∣ ∣ ∣ 2 1 3 4 ∣ ∣ ∣ ∣ . ♦ Man beachte, dass jeder der n! Summanden in (8.6) aus jeder Zeile und jeder Kolonne von A genau ein Element als Faktor enth¨alt; das heisst, die Faktoren sind verteilt wie die Einsen der zugeh¨origen Permutationsmatrix. Die Formel (8.6) ist f¨ur praktische Zwecke unbrauchbar, weil der Rechenaufwand mit n wie n! zunimmt; es gibt ja n! Terme in der Summe. Ein Computer, der 109 Multiplikationen pro Sekunde (1 GFLOP/s) ausf¨uhrt, br¨auchte f¨ur die Auswertung dieser Formel etwa 77 Jahre, wenn n = 20 ist, bzw. etwa 10140 Jahre, falls n = 100. Brauchbar ist die Formel f¨ur 1 ≤ n ≤ 3 und in Spezialf¨allen. Beispiel 8.3: F¨ur Matrizen der Ordnungen 1, 2 und 3 liefert die Formel (8.6) die folgenden Ausdr¨ucke: det ( a11 ) = a11 , ∣ ∣ ∣ ∣ a11 a12 a21 a22 ∣ ∣ ∣ ∣ = a11 a22 − a21 a12 , ∣ ∣ ∣ ∣ ∣ ∣ a11 a12 a13 a21 a22 a23 a31 a32 a33 ∣ ∣ ∣ ∣ ∣ ∣ = a11a22a33 + a21a32a13 + a31a12a23 − a31a22a13 − a21a12a33 − a11a32a23 . (8.7) Das letzte ist die Regel von Sarrus1. ♦ 1Pierre Sarrus (1798 – 1861), franz¨osischer Mathematiker, Professor in Strasbourg. c⃝M.H. Gutknecht 11. April 2016 LA-Skript 8-3 Kapitel 8 — Determinanten Lineare Algebra (2009) Beispiel 8.4: F¨ur eine Dreiecksmatrix reduziert sich die Summe (8.6) auf das Produkt der Diagonalelemente: ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ r11 ⋆ · · · ⋆ r22 · · · ⋆ . . . ... rnn ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ = r11 r22 · · · rnn , (8.8) ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ l11 ⋆ l22 ... ... . . . ⋆ ⋆ · · · lnn ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ = l11 l22 · · · lnn , (8.9) denn von den n! Summanden kann nur einer nicht null sein, jener von p = e. In den anderen tritt immer auch ein Paar (k, l) auf mit p(k) < k und p(l) > l, also mindesten je ein Faktor von oberhalb und einer von unterhalb der Diagonale, womit mindestens ein Faktor null ist, und damit auch das Produkt r1,p(1) r2,p(2) · · · rn,p(n). Wir werden in Satz 8.4 noch einen zweiten Beweis dieser Tatsache geben. ♦ Einige grundlegende Eigenschaften der Determinante sind leicht zu veriﬁzieren: Satz 8.3 Die durch (8.6) deﬁnierte Determinante ist ein Funk- tional det : En×n → E , A ↦→ det A , mit den folgenden Eigenschaften: i) det ist eine lineare Funktion jeder einzelnen Zeile der Ma- trix, d.h. f¨ur alle γ, γ′ ∈ E und alle l ∈ {1, . . . , n} ist det          a11 a12 · · · a1n a21 a22 · · · a2n ... ... ... γal1 + γ′a′ l1 γal2 + γ′a′ l2 · · · γaln + γ′a′ ln ... ... ... an1 an2 · · · ann          = γdet          a11 · · · a1n a21 · · · a2n ... ... al1 · · · aln ... ... an1 · · · ann          + γ′det          a11 · · · a1n a21 · · · a2n ... ... a′ l1 · · · a′ ln ... ... an1 · · · ann          . ii) Werden in A zwei Zeilen vertauscht, so wechselt det (A) das Vorzeichen. iii) det (I) = 1. LA-Skript 8-4 11. April 2016 c⃝M.H. Gutknecht Lineare Algebra (2009) Kapitel 8 — Determinanten Beweis: Eigenschaft (i) folgt sofort aus ∑ p∈Sn(sign p) a1,p(1) · · · (γal,p(l) + γ′a′ l,p(l)) · · · an,p(n) = γ ∑ p∈Sn(sign p) a1,p(1) · · · al,p(l) · · · an,p(n) + γ′ ∑ p∈Sn(sign p) a1,p(1) · · · a′ l,p(l) · · · an,p(n) . Eigenschaft (ii) ergibt sich daraus, dass eine Zeilenvertauschung in A in der Formel (8.6) dadurch kompensiert werden kann, dass man jede Permutation p mit der Transposition verkn¨upft, die dieser Zeilenvertau- schung entspricht. Dadurch ¨andert sich aber gerade das Signum jedes Termes, w¨ahrend die Produkte unver¨andert bleiben. Eigenschaft (iii) folgt als einfacher Spezialfall von Beispiel 8.4 ¨uber die Determinante von Dreiecksmatrizen. Wir ziehen direkt aus den Eigenschaften (i)–(iii) weitere Schl¨usse, die zum Teil auch leicht aus der Deﬁnition (8.6) gefolgert werden k¨onnten. Es lassen sich in der Tat die meisten weiteren theoretischen Resultate in diesem Kapitel ableiten, wenn man vom Postulat eines eindeutigen Funktionals mit diesen drei Eigenschaften ausgeht. Satz 8.4 Das Funktional det hat folgende weitere Eigenschaften: iv) Hat A eine Zeile aus lauter Nullen, so ist det A = 0. v) det (γA) = γndet A. vi) Hat A zwei gleiche Zeilen, so ist det A = 0. vii) Addiert man zu einer Zeile von A ein Vielfaches einer an- deren Zeile von A, so ¨andert sich der Wert von det A nicht. viii) Ist A eine Diagonalmatrix, so ist det A gleich dem Produkt der Diagonalelemente. ix) Ist A eine Dreiecksmatrix, so ist det A gleich dem Produkt der Diagonalelemente. Beweis: Die Eigenschaften (iv), (v), (viii) und (ix) lassen sich sofort aus der Deﬁnition (8.6) ablesen. Aber (iv) folgt auch sofort aus Eigen- schaft (i) in Satz 8.3, wenn wir l als Index dieser Zeile und γ = γ′ = 0 w¨ahlen. Ebenso folgt (v) aus dieser Eigenschaft, wenn wir sie auf jede Zeile anwenden und γ′ = 0 setzen. Eigenschaft (vi) ergibt sich direkt aus (ii). Eigenschaft (vii) folgt aus (i) und (vi). Die Regel (viii) f¨ur Diagonalmatrizen ergibt sich auch (iii) nach n–maligem Anwenden von (i). Ist R eine obere Dreiecksmatrix mit Diagonalelementen rkk ̸= 0, kann man sie durch die in (vii) genannten elementaren Zeilenoperationen in eine Diagonalmatrix ¨uberf¨uhren, ohne den Wert der Determinante zu c⃝M.H. Gutknecht 11. April 2016 LA-Skript 8-5 Kapitel 8 — Determinanten Lineare Algebra (2009) ¨andern: ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ r11 ⋆ · · · ⋆ r22 · · · ⋆ . . . ... rnn ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ = ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ r11 r22 . . . rnn ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ . (Zum Beispiel kann man hier durch Addition Vielfacher der letzten Zeile zu den anderen Zeilen deren letztes Element zu 0 machen.) Damit folgt Eigenschaft (ix) aus (viii). Sind nicht alle Diagonalelemente ungleich 0, so gibt es ein unterstes das 0 ist, sagen wir rkk. Dann kann man durch Addition Vielfacher der Zeilen k + 1 bis n erreichen, dass die k–te Zeile nur Nullen enth¨alt. Also ist det R = 0 nach (vii) und (iv). Analoges gilt f¨ur eine untere Dreiecksmatrix. Weil die in Eigenschaft (vii) genannten Zeilenoperationen die Deter- minante nicht ver¨andern, bleibt die Determinante im Verlaufe des Gauss-Algorithmus invariant, ausser dass jede Zeilenvertauschung zu einem Faktor −1 f¨uhrt. Das stimmt auch dann, wenn eine Matrix R in allgemeiner Zeilenstufenform resultiert, d.h. Rang A < n gilt. Nach Eigenschaft (ix) ist in diesem Falle aber det A = 0, w¨ahrend im regul¨aren Falle das Produkt der Diagonalelemente von R gerade das Produkt der Pivotelemente ist. Es gilt damit folgender Satz: Satz 8.5 F¨ur jede n × n–Matrix A gilt: det A ̸= 0 ⇐⇒ Rang A = n ⇐⇒ A ist regul¨ar . Wendet man auf A den Gauss-Algorithmus an und ist dabei Rang A = n, so ist det A gleich dem Produkt der Pivotelemen- te multipliziert mit (−1)ν, wobei ν die Anzahl der ausgef¨uhrten Zeilenvertauschungen bezeichnet: det A = (−1)ν n∏ k=1 rkk . (8.10) Der Faktor (−1)ν in (8.10) ist gerade das Signum der Permutation, die den Zeilenvertauschungen entspricht. Algorithmus 8.1 (Determinante via Gauss–Algorithmus) Zur Berechnung der Determinante einer n × n–Matrix A wende man den Gauss–Algorithmus auf A an. Falls er den Rang n und die obere Dreieicksmatrix R liefert, so gilt (8.10). Ist Rang A < n, so ist det A = 0. Der Gauss-Algorithmus 8.1 ist f¨ur n ≥ 4 im allgemeinen die weitaus schnellste Methode zur Berechnung der Determinante. Im wesent- lichen ist der Rechenaufwand ja bloss je 1 3 n3 Additionen und Mul- tiplikationen (vgl. Satz 3.2), was im Vergleich zu den rund n! Ad- ditionen und (n − 1)n! Multiplikationen von Deﬁnition (8.6) enorm wenig ist. Der Trick bestand in der Anwendung von Eigenschaft (vii) zur Reduktion auf obere Dreiecksform. LA-Skript 8-6 11. April 2016 c⃝M.H. Gutknecht Lineare Algebra (2009) Kapitel 8 — Determinanten Beispiel 8.5: In unserem Beispiel 1.2 haben wir das System (1.9), x2 + 4x3 = 1 2x1 + 4x2 − 4x3 = 1 4x1 + 8x2 − 3x3 = 7 mit dem Gauss-Algorithmus gel¨ost, wobei zun¨achst eine Zeilenvertau- schung 1 ↔ 2 n¨otig war und danach die Pivotelemente r11 = 2, r22 = 1 und r33 = 5 resultierten. Nach Formel (8.10) bzw. Algorithmus 8.1 ist also det A = (−1)1 · 2 · 1 · 5 = −10 , was man leicht mit der Formel von Sarrus, (8.7), ¨uberpr¨ufen kann. ♦ Es gilt sogar, dass die Eigenschaften (i)–(iii) aus Satz 8.3 charak- teristisch sind f¨ur die Determinante. Es bleibt dazu zu zeigen, dass aus den Eigenschaften (i)–(iii) die Formel (8.6) folgt. Satz 8.6 Die durch (8.6) deﬁnierte Determinante ist das einzige auf En×n deﬁnierte Funktional mit den Eigenschaften (i)–(iii) aus Satz 8.3, das heisst diese Eigenschaften sind charakteristisch f¨ur die Determinante. Beweis (Idee): Schreiben wir f¨ur l = 1, . . . , n die lte Zeile von A als al1 eT 1 + al2 eT 2 + · · · + aln eT n und wenden wir Eigenschaft (i) je f¨ur l = 1, . . . , n, also n–mal auf eine solche Summe von n Termen an, so ergibt sich, wie man leicht sieht, det A = n∑ k1=1 n∑ k2=1 · · · n∑ kn=1 a1,k1 a2,k2 · · · an,kn ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ eT k1 eT k2 ... eT kn ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ . In dieser Sume von nn Termen sind aber aufgrund der Eigenschaft (vi) nur jene Determinanten nicht null, wo (k1, . . . , kn) eine Permutation von (1, . . . , n) ist, d.h. es ist det A = ∑ p∈Sn a1,p(1) a2,p(2) · · · an,p(n) ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ eT p(1) eT p(2) ... eT p(n) ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ , Anwendung der Eigenschaft (ii) liefert schliesslich det A = ∑ p∈Sn(sign p) a1,p(1) a2,p(2) · · · an,p(n) det I , was wegen Eigenschaft (iii) mit der Formel (8.6) ¨ubereinstimmt. Mit Hilfe von Satz 8.6 k¨onnen wir nun eine weitere grundlegende Eigenschaft der Determinante beweisen. c⃝M.H. Gutknecht 11. April 2016 LA-Skript 8-7 Kapitel 8 — Determinanten Lineare Algebra (2009) Satz 8.7 F¨ur irgend zwei n × n–Matrizen A und B gilt det (AB) = det A · det B . (8.11) Beweis (Idee): Ist det B = 0, so ist nach Satz 8.5 Rang B < n und damit nach Satz 5.16 auch Rang (AB) < n, also det (AB) = 0. Wir d¨urfen also im folgenden det B ̸= 0 voraussetzen. Wir betrachten nun das durch d(A) :≡ det (AB)/det B deﬁnierte Funk- tional d und zeigen, dass es die Eigenschaften (i)–(iii) hat. F¨ur (iii) ist dies sofort klar, w¨ahrend (i) und (ii) leicht aus der Beziehung (2.42) in Korollar 2.8 folgen. Aufgrund der gem¨ass Satz 8.6 g¨ultigen Eindeutig- keit dieses Funktionals folgt dann, dass d(A) = det A sein muss. Also ist det A = det (AB)/det B. Korollar 8.8 Ist A regul¨ar, so gilt: det A−1 = (det A)−1 . (8.12) Beweis: Anwendung von Satz 8.7 auf AA−1 = I. Beispiel 8.6: Wir wissen, dass der Gauss-Algorithmus eine LR-Zer- legung P A = L R liefert. Da die Permutationsmatrix P orthogonal ist, also P−1 = PT gilt, haben wir A = PT L R und damit det A = det PT · det L · det R . (8.13) Hier ist nach Eigenschaften (ii) und (iii) aus Satz 8.3 gerade det PT = (−1)ν, wo ν wieder die Zahl der Zeilenvertauschungen im Gauss-Algo- rithmus bezeichnet (diese f¨uhren P gerade in I ¨uber). Weiter ist nach Eigenschaft (ix) aus Satz 8.4 det L = 1 , det R = n∏ k=1 rkk . Zusammen ergibt sich wieder Formel (8.10). ♦ Nun fehlt uns einzig noch eine Formel f¨ur die Determinante der transponierten Matrix. Satz 8.9 Es gilt: det AT = det A , det AH = det A . (8.14) Beweis (1. Version): Nach (8.13) und Satz 8.7 sowie dank det P = det P−1 = det PT und Eigenschaft (ix) aus Satz 8.4 ist det AT = det RT det LT det P = det P n∏ k=1 rkk n∏ j=1 ljj = det PT det L det R = det A . Zudem ist det AH = det AT = det A = det A. LA-Skript 8-8 11. April 2016 c⃝M.H. Gutknecht Lineare Algebra (2009) Kapitel 8 — Determinanten Beweis (2. Version): Bei Anwendung der Deﬁnition (8.6) bekommt man durch Umnummerierung der Indizes det AT = ∑ p∈Sn(sign p) ap(1),1 ap(2),2 · · · ap(n),n = ∑ p−1∈Sn(sign p−1) a1,p−1(1) a2,p−1(2) · · · an,p−1(n) = ∑ ̃p∈Sn(sign ̃p) a1,̃p(1) a2,̃p(2) · · · an,̃p(n) = det A . Man beachte, dass det AT = det A auch f¨ur komplexe Matrizen gilt. Beispiel 8.7: F¨ur unit¨ares U ist 1 = det I = det U det UH = |det U|2, also |det U| = 1 . F¨ur orthogonale Matrizen Q hat man speziell det Q = ±1 . ♦ Aus Satz 8.9 folgt nun unmittelbar: Korollar 8.10 Die Eigenschaften (i) und (ii) aus Satz 8.3 sowie die Eigenschaften (iv), (vi) und (vii) aus Satz 8.4 gelten auch, wenn man “Zeile” durch “Kolonne” ersetzt. 8.3 Entwicklung nach Zeilen und Kolonnen Schliesslich wollen wir eine weitere M¨oglichkeit der rekursiven Be- rechnung der Determinante herleiten, die allerdings nur in Spezi- alf¨allen eﬃzient ist, aber in solchen F¨allen interessante Formeln und Zusammenh¨ange liefern kann. Definition: Zu jedem Element akl einer n × n–Matrix A werde die (n − 1) × (n − 1)–Untermatrix A[k,l] deﬁniert durch Streichen der Zeile k und der Kolonne l von A. Der Kofaktor [cofactor] κkl von akl ist dann die Zahl κkl :≡ (−1)k+l det A[k,l] . (8.15) ▲ Beispiel 8.8: Es sei A =   5 2 1 3 6 2 1 4 3   . Dann ist A[2,1] = ( 2 1 4 3 ) , κ21 = (−1)3 ∣ ∣ ∣ ∣ 2 1 4 3 ∣ ∣ ∣ ∣ = −(2 · 3 − 4 · 1) = −2 . ♦ c⃝M.H. Gutknecht 11. April 2016 LA-Skript 8-9 Kapitel 8 — Determinanten Lineare Algebra (2009) Beispiel 8.9: F¨ur A =     1 3 5 1 2 4 6 3 3 6 4 2 1 5 3 1     ist A[2,3] =   1 3 1 3 6 2 1 5 1   , κ23 = (−1)5 ∣ ∣ ∣ ∣ ∣ ∣ 1 3 1 3 6 2 1 5 1 ∣ ∣ ∣ ∣ ∣ ∣ = −2 . ♦ Lemma 8.11 Es sei A eine Matrix, in deren l–ter Kolonne nur das Element akl ̸= 0 ist. Dann gilt det A = akl κkl . Beweis: Wir bringen zun¨achst die l–te Kolonne von A durch l − 1 Kolonnenvertauschungen so ganz nach links, dass die Reihenfolge der anderen Kolonnen erhalten bleibt. Dann bringen wir analog die k–te Zeile durch k − 1 Zeilenvertauschungen ganz nach oben. Die Determi- nante ¨andert sich dabei um einen Faktor (−1)k+l−2 = (−1)k+l, und es resultiert die Matrix     akl ⋆ · · · ⋆ o A[k,l]     . Bei Anwendung des Gauss-Algorithmus auf diese Matrix gibt es im er- sten Schritt nichts zu tun: das erste Pivotelement ist akl und die erste Restgleichungsmatrix ist A[k,l]. Weiterverarbeitung der letzteren wird det A[k,l] als Produkt der restlichen Pivotelemente ergeben. Das Pro- dukt aller Pivotelemente ist also akl det A[k,l]. Mittels Deﬁnition (8.15) ergibt sich damit gerade die Behauptung. Satz 8.12 Ist A eine n × n–Matrix, so gelten f¨ur jedes feste k ∈ {1, . . . , n} und jedes feste l ∈ {1, . . . , n} die Formeln det A = n∑ i=1 aki κki (8.16) und det A = n∑ i=1 ail κil . (8.17) Die Formel (8.16) nennt man Entwicklung nach der k-ten Zeile [expansion along row k], und die Formal (8.17) heisst Entwicklung nach der l-ten Kolonne [expansion along column l]. Beweis von Satz 8.12: Wir k¨onnen die l–te Kolonne von A als eine Summe von n Kolonnenvektoren schreiben, von denen jeder nur eine LA-Skript 8-10 11. April 2016 c⃝M.H. Gutknecht Lineare Algebra (2009) Kapitel 8 — Determinanten einzige Komponente ungleich 0 hat. Wendet man die (gem¨ass Korol- lar 8.10 geltende) Kolonnenversion von Eigenschaft (i) an auf die Ma- trix A mit der l–ten Kolonne geschrieben als diese Summe, so folgt un- ter Ber¨ucksichtigung von Lemma 8.11 gerade die Kolonnenentwicklung (8.17). Die Entwicklung (8.16) erh¨alt man aus (8.17) wiederum durch Anwen- dung von det A = det AT (Satz 8.9). Beispiel 8.10: F¨ur eine 3 × 3–Matrix gilt bei Entwicklung nach der ersten Kolonne: ∣ ∣ ∣ ∣ ∣ ∣ a11 a12 a13 a21 a22 a23 a31 a32 a33 ∣ ∣ ∣ ∣ ∣ ∣ = a11 ∣ ∣ ∣ ∣ a22 a23 a32 a33 ∣ ∣ ∣ ∣ − a21 ∣ ∣ ∣ ∣ a12 a13 a32 a33 ∣ ∣ ∣ ∣ + a31 ∣ ∣ ∣ ∣ a12 a13 a22 a23 ∣ ∣ ∣ ∣ = a11a22a33 + a21a32a13 + a31a12a23 − a31a22a13 − a21a12a33 − a11a32a23 . Dies stimmt mit der eingangs erw¨ahnten Regel von Sarrus ¨uberein. ♦ Beispiel 8.11: Entwickeln wir die bereits betrachtete Matrix A =     1 3 5 1 2 4 6 3 3 6 4 2 1 5 3 1     nach der zweiten Zeile: Wir kennen bereits κ23 = −2; weiter ist κ21 = (−1)3 ∣ ∣ ∣ ∣ ∣ ∣ 3 5 1 6 4 2 5 3 1 ∣ ∣ ∣ ∣ ∣ ∣ = −12 , κ22 = (−1)4 ∣ ∣ ∣ ∣ ∣ ∣ 1 5 1 3 4 2 1 3 1 ∣ ∣ ∣ ∣ ∣ ∣ = −2 , κ24 = (−1)6 ∣ ∣ ∣ ∣ ∣ ∣ 1 3 5 3 6 4 1 5 3 ∣ ∣ ∣ ∣ ∣ ∣ = 28 . Zusammen ergibt sich det A = a21 κ21 + a22 κ22 + a23 κ23 + a24 κ24 = 2 · (−12) + 4 · (−2) + 6 · (−2) + 3 · 28 = 40 . ♦ c⃝M.H. Gutknecht 11. April 2016 LA-Skript 8-11 Kapitel 8 — Determinanten Lineare Algebra (2009) 8.4 Determinanten von Blockdreiecksmatrizen Man k¨onnte hoﬀen, f¨ur eine 2 × 2 Blockmatrix gelte ∣ ∣ ∣ ∣ A B C D ∣ ∣ ∣ ∣ = det A det D − det B det C , aber schon ein ganz simples Beispiel wie ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ 2 0 1 0 0 2 0 1 1 0 2 0 0 1 0 2 ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ = 71 ̸= 15 = 4 · 4 − 1 · 1 zeigt, dass dies im allgemeinen falsch ist. In der speziellen Situation, wo B = O oder C = O ist, gilt aber in der Tat folgendes: Satz 8.13 F¨ur eine 2 × 2 Blockdreiecksmatrix ist ∣ ∣ ∣ ∣ A B O D ∣ ∣ ∣ ∣ = det A det D bzw. ∣ ∣ ∣ ∣ A O C D ∣ ∣ ∣ ∣ = det A det D . (8.18) Beweis: Wir betrachten den Fall der oberen Blockdreiecksmatrix und nehmen an, A sei ein m×m Block und D ein (n−m)×(n−m) Block. In unserer Deﬁnition (8.6) leisten oﬀenbar nur jene Permutationen p einen Beitrag, f¨ur die p(1), . . . , p(m) ∈ {1, . . . , m} gilt; die anderen f¨uhren auf mindestens einen Faktor aus B = O. Die relevanten Permutationen p haben also die Form (1, . . . , m; m+1, . . . n) ↦→ ( pA(1), . . . , pA(n); m+pD(1), . . . , m+pD(n−m)) mit pA ∈ Sm und pD ∈ Sn−m. Dabei ist sign p = sign pA · sign pD. Aus der Summe in (8.6) wird damit ∣ ∣ ∣ ∣ A B O D ∣ ∣ ∣ ∣ = ∑ pA∈Sm pD∈Sn−m sign pA · sign pD · a1,pA(1) · · · am,pA(m) × × d1,pD(1) · · · dn−m,pD(n−m) = ∑ pA∈Sm sign pA · a1,pA(1) · · · am,pA(m) × × ∑ pD∈Sn−m sign pD · d1,pD(1) · · · dn−m,pD(n−m) = det A det D . Durch rekursive Anwendung ergibt sich aus diesem Satz sofort: Korollar 8.14 Die Determinante einer Blockdreiecksmatrix ist gleich dem Produkt der Determinanten der diagonalen Bl¨ocke. LA-Skript 8-12 11. April 2016 c⃝M.H. Gutknecht Lineare Algebra (2009) Kapitel 9 — Eigenwerte und Eigenvektoren Kapitel 9 Eigenwerte und Eigenvektoren Eigenwerte und Eigenvektoren spielen sowohl in der Theorie als auch in den Anwendungen eine grosse Rolle. Sie erlauben einfa- che Abbildungsmatrizen von Selbstabbildungen zu ﬁnden und zum Beispiel Normalformen f¨ur quadratische Funktionen zu ﬁnden. Man kann mit ihnen aber auch lineare Diﬀerentialgleichungen mit kon- stanten Koeﬃzienten l¨osen oder, konkret, die Eigenschwingungen eines elastischen K¨orpers bestimmen, im einfachsten Falle einer schwingenden Saite. 9.1 Eigenwerte und Eigenvektoren von Matrizen und linearen Abbildungen Wir betrachten lineare Abbildungen F : V → V , x ↦−→ F x eines endlich-dimensionalen Vektorraumes V in sich und interessieren uns f¨ur Vektoren, die bei einer solchen Abbildung bloss gestreckt werden (allenfalls mit einem negativen Faktor). Definition: Die Zahl λ ∈ E heisst Eigenwert [eigenvalue] der linearen Abbildung F : V → V , falls es einen Eigenvektor [eigen- vector] v ∈ V , v ̸= o gibt, so dass F v = λv . (9.1) Ist λ ein Eigenwert, so ist der zugeh¨orige Eigenraum [Eigenspace] Eλ gleich der um den Nullvektor erweiterten Menge der Eigenvek- toren zu λ: Eλ :≡ {v ∈ V ; F v = λv} . (9.2) Die Menge aller Eigenwerte von F heisst Spektrum [spectrum] von F und wird mit σ(F ) bezeichnet. Als Abk¨urzungen f¨ur die Begriﬀe “Eigenvektor” und “Eigenwert” schreibt man oft EV [EVec] und EW [EVal]. ▲ Diese Deﬁnitionen gelten insbesondere f¨ur quadratische Matrizen: ξ ∈ En ist ein Eigenvektor zum Eigenwert λ von A ∈ En×n falls Aξ = ξλ , (9.3) wobei wir hier wieder besser den Skalar rechts schreiben, vgl. (2.21). Ist A eine Abbildungsmatrix, die eine lineare Abbildung F bez¨uglich einer gewissen Basis repr¨asentiert, so haben F und A die gleichen Eigenwerte, und die Eigenvektoren entsprechen sich auf nat¨urliche Weise: c⃝M.H. Gutknecht 11. April 2016 LA-Skript 9-1 Kapitel 9 — Eigenwerte und Eigenvektoren Lineare Algebra (2009) Lemma 9.1 Es sei F : V → V eine lineare Abbildung, κV : V → En, x ↦→ ξ eine Koordinatenabbildung von V (bez¨uglich ei- ner gew¨ahlten Basis) und A = κV F κ−1 V die entsprechende Abbil- dungsmatrix von F . Dann gilt: λ EW von F x EV von F } ⇐⇒ { λ EW von A ξ EV von A Beweis: Aus F x = λx folgt κV (F x) = κV (λx) = λκV x = λξ. Auf- grund der Deﬁnitionen von κV und A, vgl. (5.26), ist aber Aξ = κV (F x), also Aξ = λξ. Analog schliesst man unter Verwendung von κ −1 V , dass die Umkehrung gilt. Wir k¨onnen uns also bei der Diskussion von Eigenwertproblemen weitgehend auf Matrizen beschr¨anken. Der zu einem Eigenwert λ geh¨orende Eigenvektor v ist nat¨urlich nie eindeutig bestimmt: oﬀensichtlich ist mit v auch jedes (von 0 verschiedene) skalare Vielfache von v ein Eigenvektor, denn F v = λv =⇒ F (αv) = λ(αv) . Das bedeutet, dass jeder Eigenraum Eλ immer mindestens einen eindimensionalen Unterraum enth¨alt. Kann es wohl mehrere linear unabh¨angige Eigenvektoren zu einem Eigenwert geben? Der Eigenraum Eλ ist die Menge der Vektoren v, f¨ur die (F − λI)v = o (9.4) gilt, wo I die Identit¨at auf V bedeutet und F −λI deﬁniert ist durch (F −λI)x :≡ F x−λx (∀x ∈ V ). Mit F ist auch F −λI eine lineare Abbildung von V in sich, und die Eigenvektoren v (zusammen mit o) bilden gerade den Kern von F − λI. Dieser Kern besteht eben genau dann nicht nur aus dem Nullvektor, wenn λ ein Eigenwert ist. Unter Ber¨ucksichtigung von Lemma 5.4 gilt somit: Lemma 9.2 λ ist genau dann ein Eigenwert von F : V → V , wenn der Kern von F − λI nicht nur aus dem Nullvektor besteht. Der Eigenraum Eλ zu einem Eigenwert λ von F ist ein vom Null- raum verschiedener Unterraum von V , und zwar ist Eλ = ker (F − λI) . (9.5) Definition: Die geometrische Vielfachheit [geometric mul- tiplicity] eines Eigenwertes λ ist gleich der Dimension von Eλ. ▲ Beispiel 9.1: Die Identit¨at I in V hat den Eigenwert 1 und den zugeh¨origen Eigenraum E1 = V , denn f¨ur jeden Vektor gilt ja Ix = x = 1 x. Also hat der Eigenwert 1 die geometrische Vielfachheit n :≡ dim V . F¨ur die Nullmatrix O ∈ En×n ist analog E0 = En, dim E0 = n. ♦ LA-Skript 9-2 11. April 2016 c⃝M.H. Gutknecht Lineare Algebra (2009) Kapitel 9 — Eigenwerte und Eigenvektoren Beispiel 9.2: F¨ur eine Rotation im R3 um den Winkel φ um eine durch den Ursprung gehende Achse mit der Richtung a ist a ein Eigenvektor zum Eigenwert 1, denn die Punkte auf der Achse bleiben invariant (fest). F¨ur |φ| < π sind die Ortsvektoren dieser Punkte die einzigen, die fest bleiben, das heisst es ist E1 = span {a}. Ist φ = ±π, so gibt es auch noch Eigenvektoren zum Eigenwert −1: Jeder Vektor in der Ebene, die durch O geht und zu a senkrecht steht, also jedes x mit ⟨a, x⟩ = 0, ist dann ein Eigenvektor zum Eigenwert −1. Erg¨anzt man a/∥a∥ zu einer orthonormalen Basis von R3, so hat die Rotation um φ = ±π bez¨uglich dieser Basis gerade die Abbildungsmatrix   1 0 0 0 −1 0 0 0 −1   . ♦ Beispiel 9.3: F¨ur die Projektion P : R3 → R2 ⊂ R3, die in cartesi- schen Koordinaten deﬁniert ist durch P : ( x1 x2 x3 )T ↦−→ ( x1 x2 0 )T , gilt im P = { ( x1 x2 0 )T ; x1, x2 ∈ R}, ker P = { ( 0 0 x3 )T ; x3 ∈ R}. Zudem ist Px = x = 1 x falls x ∈ im P , Px = o = 0 x falls x ∈ ker P , Also hat P die Eigenwerte 1 und 0, wobei 1 die geometrische Vielfachheit 2 hat, w¨ahrend 0 die geometrische Vielfachheit 1 hat: E1 = im P , dim E1 = 2 , E0 = ker P , dim E0 = 1 . Wir werden gleich sehen, dass dies alle Eigenwerte (und damit auch alle Eigenr¨aume) sind. In den oben vorausgesetzten cartesischen Koordinaten wird die Projek- tion beschrieben durch die Diagonalmatrix   1 0 0 0 1 0 0 0 0   . V¨ollig analog verh¨alt es sich mit jeder anderen Projektion des R3 auf eine (zweidimansionale) Ebene: Die Eigenwerte sind wieder 1, 1, 0, aber die Eigenvektoren h¨angen nat¨urlich sowohl von der gew¨ahlten Projektion als auch von der gew¨ahlten Basis ab. W¨ahlt man in der Bildebene im P zwei Basisvektoren und in der Projektionsrichtung ker P den dritten, so wird die Projektion wieder durch obige Matrix beschrieben. ♦ Auf Matrizen ¨ubersetzt, lautet Lemma 9.2 wie folgt: c⃝M.H. Gutknecht 11. April 2016 LA-Skript 9-3 Kapitel 9 — Eigenwerte und Eigenvektoren Lineare Algebra (2009) Korollar 9.3 λ ist genau dann ein Eigenwert von A ∈ En×n, wenn A − λI singul¨ar ist. Der Eigenraum Eλ zu einem Eigenwert λ von A ist ein vom Null- raum verschiedener Unterraum des En, und zwar ist Eλ = ker (A − λI) , (9.6) das heisst Eλ besteht aus allen L¨osungen v des homogenen Glei- chungssystems (A − λI)v = o . (9.7) Die geometrische Vielfachheit von λ betr¨agt dim Eλ = dim ker (A − λI) = n − Rang (A − λI) . (9.8) Beweis: Das zweite Gleichheitszeichen in (9.8) folgt aus der Dimen- sionsformel (5.29); siehe auch Satz 5.12. Alles andere ergibt sich aus Lemma 9.2 und bereits bekannten ¨Aquivalenzen. Wir k¨onnen also die Eigenvektoren zu einem bekannten Eigen- wert durch L¨osen eines singul¨aren homogenen Gleichungssystems berechnen. Dabei wollen wir in der Regel eine Basis von Eλ kon- struieren, das heisst dim Eλ linear unabh¨angige L¨osungsvektoren v berechnen. Wir k¨onnen diese zudem bei Bedarf “normieren”, das heisst so w¨ahlen, dass ∥v∥ = 1 ist (in der Euklidischen Norm). Aber wie ﬁnden wir die Eigenwerte? Und wieviele gibt es? Wir m¨ussen λ so w¨ahlen, dass A − λI singul¨ar ist. Nach Satz 8.5 ist A − λI genau dann singul¨ar, wenn det (A − λI) = 0. Beispiel 9.4: F¨ur A =   −7 2 −6 12 −2 12 12 −3 11   (9.9) erhalten wir mit der Formel (8.7) von Sarrus det (A − λI) = ∣ ∣ ∣ ∣ ∣ ∣ −7 − λ 2 −6 12 −2 − λ 12 12 −3 11 − λ ∣ ∣ ∣ ∣ ∣ ∣ = (−7 − λ)(−2 − λ)(11 − λ) + 72(−2 − λ) + 36(−7 − λ) − 24(11 − λ) + 288 + 216 = (−λ)3 + (−7 − 2 + 11)λ2 − (−22 − 77 + 14 + 72 + 36 − 24)λ + 154 − 144 − 252 − 264 + 288 + 216 = −λ3 + 2λ2 + λ − 2 . Es ist also det (A − λI) = −λ3 + 2λ2 + λ − 2 ≡: χA(λ) ein kubisches Polynom χA in λ, und das gilt oﬀensichtlich analog f¨ur jede 3 × 3–Matrix. Ein solches Polynom hat im Komplexen genau drei LA-Skript 9-4 11. April 2016 c⃝M.H. Gutknecht Lineare Algebra (2009) Kapitel 9 — Eigenwerte und Eigenvektoren Nullstellen, wenn man diese mit ihrer Vielfachheit z¨ahlt. Es gibt also drei Eigenwerte, wobei aber zwei davon oder alle drei zusammenfallen k¨onnen. Zudem k¨onnen zwei Nullstellen konjugiert-komplex sein. In diesem Beispiel kann man erraten, dass λ1 = 1 eine Nullstelle von χA ist. Das Abspalten [deﬂation] dieser Nullstelle, das heisst die Division des Polynoms χA durch den Linearfaktor (1 − λ), liefert (−λ3 + 2λ2 + λ − 2) : (1 − λ) = λ2 − λ − 2 , also ein quadratisches Polynom mit den Nullstellen λ2 = 2 und λ3 = −1. Somit hat A das Spektrum σ(A) = {1, 2, −1} . ♦ Beispiel 9.5: Dass es wirklich komplexe Eigenwerte geben kann, selbst wenn A reell ist, zeigt schon das einfache Beispiel A = ( 0 −1 1 0 ) , det (A − λI) = ∣ ∣ ∣ ∣ −λ −1 1 −λ ∣ ∣ ∣ ∣ = λ2 + 1 . Die Eigenwerte sind hier rein imagin¨ar: λ1 = i , λ2 = −i . ♦ Aufgrund der Determinanten-Deﬁnition (8.6) sieht man, dass det (A − λI) = ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ a11 − λ a12 · · · a1n a21 a22 − λ · · · a2n ... ... . . . ... an1 an2 · · · ann − λ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ = (−λ)n + (a11 + · · · + ann)(−λ)n−1 + · · · + det A ≡: χA(λ) (9.10) ein Polynom χA vom Grade n in λ ist. Dessen H¨ochstkoeﬃzient ist (−1)n, der konstante Koeﬃzient ist gerade det A. Der zweith¨ochste Koeﬃzient ist das (−1)n−1–fache der Summe der Diagonalelemente von A. Definition: Das Polynom χA(λ) :≡ det (A − λI) (9.11) heisst charakteristisches Polynom [characteristic polynomial] der Matrix A ∈ En×n, und die Gleichung χA(λ) = 0 (9.12) ist die charakteristische Gleichung [characteristic equation]. c⃝M.H. Gutknecht 11. April 2016 LA-Skript 9-5 Kapitel 9 — Eigenwerte und Eigenvektoren Lineare Algebra (2009) Die Summe der Diagonalelemente von A nennt man Spur [trace] von A: Spur A :≡ a11 + a22 + · · · + ann . (9.13) ▲ Lemma 9.4 Das charakteristische Polynom χA hat die Form χA(λ) ≡: det (A − λI) = (−λ)n + Spur A (−λ)n−1 + · · · + det A . (9.14) Aus dem Vorangehenden folgt sofort: Satz 9.5 λ ∈ E ist genau dann Eigenwert der Matrix A ∈ En×n, wenn λ Nullstelle des charakteristischen Polynoms χA ist, das heisst eine L¨osung der charakteristischen Gleichung ist. Das Polynom χA hat nat¨urlich reelle Koeﬃzienten, wenn A reell ist (E = R); es hat komplexe Koeﬃzienten, wenn A komplex ist (E = C). Nach dem Fundamentalsatz der Algebra hat ein kom- plexes (oder reelles) Polynom vom exakten Grad n genau n (im allgemeinen komplexe) Nullstellen, falls man mehrfache Nullstellen mehrfach z¨ahlt. Das heisst man kann χA, das den H¨ochstkoeﬃzienten (−1)n hat, wie folgt in n Linearfaktoren zerlegen: χA(λ) = (λ1 − λ)(λ2 − λ) · · · (λn − λ) , wobei λk ∈ C, k = 1, . . . , n. Ein reelles Polynom kann aber auch nicht-reelle Nullstellen haben. Falls A reell ist, aber λ nicht reell ist, so sind auch die zugeh¨origen Eigenvektoren nicht reell. (Denn w¨are v ∈ R n und λ ̸∈ R, so w¨are Av ∈ R n, aber vλ ̸∈ R n.) In der Eigenwerttheorie fasst man deshalb in der Regel relle Ma- trizen als komplexe Matrizen auf, das heisst man l¨asst komplexe Eigenwerte und komplexe Eigenvektoren zu. Deshalb setzen wir im folgenden E = C voraus. Es wird sich zeigen, dass die bereits deﬁnierte geometrische Viel- fachheit eines Eigenwertes λ nicht unbedingt gleich seiner Vielfach- heit als Nullstelle von χA ist. Man deﬁniert deshalb: Definition: Die algebraische Vielfachheit [algebraic multi- plicity] eines Eigenwertes λ ist die Vielfachheit von λ als Nullstelle des charakteristischen Polynoms. ▲ Das Verh¨altnis zwischen algebraischer und geometrischer Vielfach- heit wird in Abschnitt 9.4 diskutiert werden. Wir k¨onnen also bei kleinen Matrizen die Eigenwerte und Eigenvek- toren nach der folgenden Methode berechnen, die aber f¨ur gr¨ossere Matrizen (n ≫ 3) unbrauchbar ist, weil zu aufwendig und zu star- ken Rundungseﬀekten unterworfen. LA-Skript 9-6 11. April 2016 c⃝M.H. Gutknecht Lineare Algebra (2009) Kapitel 9 — Eigenwerte und Eigenvektoren Algorithmus 9.1 (Berechnung von Eigenwerten und Ei- genvektoren via charakteristisches Polynom) Um die Eigenwerte und Eigenvektoren einer Matrix A ∈ Cn×n zu bestimmen, kann man theoretisch wie folgt vorgehen: 1. Berechnung des charakteristischen Polynoms χA, χA(λ) :≡ det (A − λI) . 2. Berechnung der n Nullstellen λ1, . . . , λn von χA. Die Viel- fachheit einer Nullstelle ist gleich der algebraischen Vielfach- heit dieses Eigenwertes. 3. F¨ur jeden verschiedenen Eigenwert λk: Bestimmung einer Basis des Kernes von A − λkI, des Eigenraumes zu λk. Das heisst Berechnung (maximal vieler) linear unabh¨angiger L¨osungen des singul¨aren homogenen Gleichungssystems (A − λkI) v = o . Dazu reduziert man A − λkI mit dem Gauss-Algorithmus auf Zeilenstufenform und w¨ahlt von den n − r freien Para- metern der Reihe nach immer einen ̸= 0 und die anderen 0. Die Dimension n − r des L¨osungsraumes ist gleich der geometrischen Vielfachheit dieses Eigenwertes. Beispiel 9.6: F¨ur die Matrix A =   5 −1 3 8 −1 6 −4 1 −2   (9.15) wird χA(λ) = ∣ ∣ ∣ ∣ ∣ ∣ 5 − λ −1 3 8 −1 − λ 6 −4 1 −2 − λ ∣ ∣ ∣ ∣ ∣ ∣ = (5 − λ)(−1 − λ)(−2 − λ) + 12(−1 − λ) − 6(5 − λ) + 8(−2 − λ) + 24 + 24 = −λ (λ2 − 2λ + 1 ) . Die charakteristische Gleichung l¨asst sich also schreiben als 0 = −λ ( λ2 − 2λ + 1 ) = −λ (λ − 1)2 , d.h. die Eigenwerte sind λ1 = 1 , λ2 = 1 , λ3 = 0 . (9.16) Zur Bestimmung der Eigenvektoren sind nacheinander die zwei homo- genen Gleichungssysteme ξ1 ξ2 ξ3 1 4 −1 3 0 8 −2 6 0 −4 1 −3 0 ξ1 ξ2 ξ3 1 5 −1 3 0 8 −1 6 0 −4 1 −2 0 c⃝M.H. Gutknecht 11. April 2016 LA-Skript 9-7 Kapitel 9 — Eigenwerte und Eigenvektoren Lineare Algebra (2009) durch Reduktion auf Zeilenstufenform zu l¨osen. Vertauscht man im er- sten Systems keine, im zweiten System die erste mit der dritten Zeile, so ergibt sich: ξ1 ξ2 ξ3 1 4 ✍✌ ✎☞ −1 3 0 0 0 0 0 0 0 0 0 ξ1 ξ2 ξ3 1 −4 ✍✌ ✎☞ 1 −2 0 0 1 ✍✌ ✎☞ 2 0 0 0 0 0 Das System links hat Rang 1 und die linear unabh¨angigen L¨osungen v1 = ( 1 4 0 )T und v2 = ( 3 0 −4 )T. Man kann in v2 statt die zweite die erste Komponente null setzen und bekommt dann v2 = ( 0 3 1 )T. Das System rechts hat Rang 2 und zum Beispiel die L¨osung v3 = ( −1 −2 1 )T. Wir erhalten also zu den Eigenwerten (9.16) die drei Eigenvektoren v1 =   1 4 0   , v2 =   0 3 1   , v3 =   −1 −2 1   . (9.17) ♦ Wir weisen noch darauf hin, dass die Matrix (9.15) des Beispiels 9.6 singul¨ar ist. Es gilt n¨amlich allgemein: Lemma 9.6 Eine (quadratische) Matrix A ist genau dann sin- gul¨ar, wenn sie 0 als Eigenwert hat: A singul¨ar ⇐⇒ 0 ∈ σ(A) . Beweis: Nach Satz 1.7 ist A genau dann singul¨ar, wenn das homogene System Ax = o eine nichttriviale L¨osung x hat. Dies ist aber gleichbe- deutend damit, dass x ein Eigenvektor zum Eigenwert 0 ist. Alternativer Beweis: Nach Satz 8.5 ist A genau dann singul¨ar, wenn det A = 0. Dies bedeutet aber nach (9.10), dass der konstante Term im charakterischen Polynom verschwindet, dieses also 0 als Nullstelle hat, was nach Satz 9.5 heisst, dass 0 Eigenwert von A ist. LA-Skript 9-8 11. April 2016 c⃝M.H. Gutknecht Lineare Algebra (2009) Kapitel 9 — Eigenwerte und Eigenvektoren 9.2 ¨Ahnlichkeitstransformationen; die Eigenwertzerlegung Wir betrachten nun wieder eine lineare Abbildung F : V → V , x ↦−→ F x eines Vektorraumes V der Dimension n in sich und die zugeh¨origen Abbildungsmatrizen A und B bez¨uglich zwei verschie- denen Basen {b1, . . . , bn} und {b′ 1, . . . , b′ n}. Analog zu Abschnitt 5.5, wo jedoch Urbild- und Bildraum verschieden waren, gilt das folgen- de kommutatives Diagramm: x ∈ V F −−−−−−→ lin. Abb. y ∈ V κV     ↓ ↑    κ−1 V κV     ↓ ↑    κ−1 V (Koordinaten- abbildung bzgl. “alten” Basen) ξ ∈ En A −−−−−−→ Abb.matrix η ∈ En (Koordinaten bzgl. “alten” Basen) T−1    ↓ ↑    T T−1    ↓ ↑    T (Koordinaten- transformation) ξ′ ∈ En B −−−−−−→ Abb.matrix η′ ∈ En (Koordinaten bzgl. “neuen” Basen) (9.18) Wir erinnern uns daran, vgl. (5.52), dass nach diesem Diagramm gilt B = T−1AT, A = TBT−1 , (9.19) was bedeutet, dass die n × n–Matrizen A und B ¨ahnlich [similar] sind. Der ¨Ubergang A ↦→ B = T−1AT wird als ¨Ahnlichkeits- transformation [similarity transformation] bezeichnet. Wir stellen uns nun wieder die Frage, wieweit sich die Abbildungs- matrix durch geeignete Wahl der Basis vereinfachen l¨asst, das heisst, wie weit man die Matrix A durch eine ¨Ahnlichkeitstransformation A ↦→ B = T−1AT vereinfachen kann. Diese Frage ist eng verkn¨upft mit der Eigenwerttheorie. Satz 9.7 ¨Ahnlichen Matrizen haben dasselbe charakteristische Polynom; sie haben also die gleiche Determinante, die gleiche Spur und die gleichen Eigenwerte. Sowohl die geometrische als auch die algebraische Vielfachheit ei- nes Eigenwertes ist bei ¨ahnlichen Matrizen gleich. c⃝M.H. Gutknecht 11. April 2016 LA-Skript 9-9 Kapitel 9 — Eigenwerte und Eigenvektoren Lineare Algebra (2009) Beweis: Es sei B = T−1AT. Dann gilt f¨ur die charakteristischen Po- lynome von A und B χB(λ) = det (B − λI) = det (T−1AT − λI) = det (T−1(A − λI) T) = det T−1det (A − λI)det T = det (A − λI) = χA(λ) Folglich stimmen nach Lemma 9.4 und Satz 9.5 die Determinanten von A und B, ihre Spuren und ihre Eigenwerte inklusive der algebraischen Vielfachheit ¨uberein. Dass auch die geometrische Vielfachheit ¨ubereinstimmt, folgt schon aus Lemma 9.1. Direkter sehen wir, dass aus Av = vλ folgt T−1A T︸ ︷︷ ︸ = B T−1v︸ ︷︷ ︸ ≡: w = T−1v︸ ︷︷ ︸ ≡: w λ , also Bw = wλ falls w :≡ T−1v. Umgekehrt schliesst man genauso. Das heisst es ist v Eigenvektor von A zum Eigenwert λ genau dann, wenn w Eigenvektor von B zu λ ist. Kurz, wenn wir T und T−1 als Abbildungen auﬀassen: Eλ(B) = T−1Eλ(A) , Eλ(A) = TEλ(B) (9.20) Da T regul¨ar ist, folgt dim Eλ(B) = dim Eλ(A). Gibt es in V eine Basis aus lauter Eigenvektoren (was nicht immer der Fall ist), so ist die entsprechende Abbildungsmatrix diagonal: Lemma 9.8 Eine zu F : V → V geh¨orende Abbildungsmatrix ist genau dann diagonal, wenn die gew¨ahlte Basis von V aus lauter Eigenvektoren von F besteht. Beweis: Sind v1, . . . , vn Eigenvektoren, die eine Basis bilden, und sind λ1, . . . , λn die zugeh¨origen Eigenwerte, so hat F vl = λlvl den Koordina- tenvektor ( 0 · · · 0 λl 0 · · · 0 )T (mit λl als l-ter Komponente), das heisst die Abbildungsmatrix ist Λ = diag (λ1, . . . , λn), vgl. (5.14). Dieser Schluss ist zudem umkehrbar. Definition: Eine Basis aus Eigenvektoren von F (oder A) heisst Eigenbasis [eigen basis] von F (bzw. A). ▲ Gibt es eine Eigenbasis v1, . . . , vn, so gilt also die einfache Abbil- dungsformel x = n∑ k=1 ξk vk ↦−→ F x = n∑ k=1 λk ξk vk . (9.21) LA-Skript 9-10 11. April 2016 c⃝M.H. Gutknecht Lineare Algebra (2009) Kapitel 9 — Eigenwerte und Eigenvektoren In diesem Falle gibt es also zu F eine extrem einfache Abbildungs- matrix, ¨ahnlich wie das allgemein gilt f¨ur eine lineare Abbildung F : V → W zwischen zwei verschiedenen R¨aumen, wo man in beiden die Basis frei w¨ahlen kann, vgl. Satz 5.20, wo die Diago- nalelemente sogar alle 0 oder 1 sind. Aber w¨ahrend man dort im wesentlichen nur Basen von Kern und Bild bestimmen muss, sowie die Urbilder der letzteren, ist hier die Bestimmung einer Basis aus Eigenvektoren eine rechnerisch sehr aufwendige Aufgabe, die man zudem im allgemeinen nur approximativ l¨osen kann, falls n > 3. Bereits in Lemma 9.1 haben wir gesehen, dass man sich f¨ur die Be- stimmung der Eigenwerte und Eigenvektoren einer linearen Selbst- abbildung F auf die Bestimmung der Eigenwerte und Eigenvektoren einer zugeh¨origen Abbildungsmatrix A beschr¨anken kann. F¨ur Matrizen gilt die folgende Neuformulierung von Lemma 9.8: Satz 9.9 Zu einer Matrix A ∈ En×n gibt es genau dann eine ¨ahnliche Diagonalmatrix Λ, wenn es eine Eigenbasis von A gibt. F¨ur die regul¨are Matrix V :≡ ( v1 . . . vn ) mit dieser Eigenbasis als Kolonnen gilt dann A V = V Λ , d.h. A = V Λ V−1 . (9.22) Gibt es umgekehrt eine regul¨are Matrix V ∈ En×n und eine Dia- gonalmatrix Λ ∈ En×n, so dass (9.22) gilt, so sind die Diago- nalelemente von Λ Eigenwerte von A und die Kolonnen von V entsprechende Eigenvektoren, die eine Eigenbasis bilden. Beweis: Der erste Teil des Satzes ist ein Spezialfall von Lemma 9.8. Setzen wir eine Basis aus Eigenvektoren voraus, so dass A vk = vk λk (k = 1, . . . , n) (9.23) gilt, ist klar, dass man diese n Gleichungen zur Matrix-Gleichung (9.22) zusammenfassen kann, wobei V linear unabh¨angige Kolonnen hat, also regul¨ar ist. Dieser Schluss l¨asst sich auch umkehren. Definition: A = V Λ V−1 heisst Spektralzerlegung [spectral decomposition] oder Eigenwertzerlegung [eigenvalue decomposi- tion] von A. Eine Matrix A, zu der es eine Spektralzerlegung A = V Λ V−1 mit diagonalem Λ gibt, heisst diagonalisierbar [diagonalizable]. ▲ Eine diagonalisierbare n × n–Matrix l¨asst sich auf Grund der Spek- tralzerlegung (9.22) als Summe von n Rang-1–Matrizen schreiben. Genauer gilt: c⃝M.H. Gutknecht 11. April 2016 LA-Skript 9-11 Kapitel 9 — Eigenwerte und Eigenvektoren Lineare Algebra (2009) Korollar 9.10 Ist A diagonalisierbar und zerlegen wir die Ma- trizen V und V−1 aus der Spektralzerlegung A = VΛV−1 in Kolonnen- bzw. Zeilenvektoren, V = ( v1 · · · vn ) , V−1 =    wT 1 ... wT n    , (9.24) so l¨asst sich A als Summe von n Rang-1–Matrizen darstellen: A = n∑ k=1 vkλkwT k . (9.25) Dabei gilt Avk = vkλk und wT k A = λkwT k . Definition: Ein Vektor w, f¨ur den gilt wTA = λkwT, heisst linker Eigenvektor [left eigenvector] von A. ▲ Beispiel 9.7: Die Matrix-Gleichung   −7 2 −6 12 −2 12 12 −3 11   ︸ ︷︷ ︸ A   1 0 −1 4 3 0 0 1 1   ︸ ︷︷ ︸ V =   1 0 −1 4 3 0 0 1 1   ︸ ︷︷ ︸ V   1 0 0 0 2 0 0 0 −1   ︸ ︷︷ ︸ Λ ist gleichwertig mit den Beziehungen Avk = vkλk (k = 1, . . . , 3), wenn λ1 = 1, λ2 = 2, λ3 = −1 und v1 =   1 4 0   , v2 =   0 3 1   , v3 =   −1 0 1   . Zudem ist V−1 =   1 0 −1 4 3 0 0 1 1   −1 =   −3 1 −3 4 −1 4 −4 1 −3   , also A =   1 0 −1 4 3 0 0 1 1   ︸ ︷︷ ︸ V   1 0 0 0 2 0 0 0 −1   ︸ ︷︷ ︸ Λ   −3 1 −3 4 −1 4 −4 1 −3   ︸ ︷︷ ︸ V−1 . Die Zerlegung (9.24) in Rang-1–Matrizen lautet hier A =   1 4 0   ( −3 1 −3 ) +   0 3 1   2 ( 4 −1 4 ) −   −1 0 1   ( −4 1 −3 ) , LA-Skript 9-12 11. April 2016 c⃝M.H. Gutknecht Lineare Algebra (2009) Kapitel 9 — Eigenwerte und Eigenvektoren oder ausgeschrieben A =   −3 1 −3 12 4 −12 0 0 0   +   0 0 0 24 −6 24 8 −2 8   +   −4 1 −3 0 0 0 4 −1 3   =   −7 2 −6 12 −2 12 12 −3 11   . ♦ Wie ﬁndet man aber allgemein eine Basis aus Eigenvektoren? Lie- fert unser Algorithmus 9.1 eine, falls es eine gibt? In Beispiel 9.7 gab es drei verschiedene Eigenwerte und die zu- geh¨origen Eigenvektoren sind linear unabh¨angig, denn andernfalls w¨are V nicht invertierbar. In der Tat gilt allgemein: Satz 9.11 Eigenvektoren zu verschiedenen Eigenwerten sind line- ar unabh¨angig. Beweis: Der Satz ist richtig f¨ur m = 1, da o nicht ein Eigevektor ist. Wir nehmen an, der Satz sei richtig f¨ur m Eigenvektoren v1, . . . , vm zu m verschiedenen Eigenwerten λ1, . . . , λn. Es sei nun λm+1 ein weiterer anderer Eigenwert und vm+1 (̸= o) ein zugeh¨origer Eigenvektor. W¨are vm+1 = m∑ k=1 γkvk , (9.26) so w¨urde folgen F vm+1 = m∑ k=1 γkF vk , also λm+1vm+1 = m∑ k=1 γkλkvk . Wegen (9.26) h¨atte man demnach m∑ k=1 γk(λm+1 − λk︸ ︷︷ ︸ ̸= 0 )vk = o . Da v1, . . . , vm linear unabh¨angig sind, folgt γ1 = · · · = γn = 0, im Widerspruch zu (9.26) und vm+1 ̸= o. Korollar 9.12 Sind die n Eigenwerte von F : V → V (wo n = dim V ) verschieden, so gibt es eine Basis von Eigenvektoren und die entsprechende Abbildungsmatrix ist diagonal. Das folgende Beispiel zeigt, dass die Umkehrung dieses Korollars nicht gilt: es gibt auch Matrizen mit mehrfachen Eigenwerten, die diagonalierbar sind. Das wissen wir allerdings auch schon von der Einheitsmatrix I. c⃝M.H. Gutknecht 11. April 2016 LA-Skript 9-13 Kapitel 9 — Eigenwerte und Eigenvektoren Lineare Algebra (2009) Beispiel 9.8: Wir setzen Beispiel 9.6 fort: Zur Matrix A aus (9.15) geh¨ort gem¨ass (9.16)–(9.17) die Identit¨at   5 −1 3 8 −1 6 −4 1 −2   ︸ ︷︷ ︸ A   1 0 −1 4 3 −2 0 1 1   ︸ ︷︷ ︸ V =   1 0 −1 4 3 −2 0 1 1   ︸ ︷︷ ︸ V   1 0 0 0 1 0 0 0 0   ︸ ︷︷ ︸ Λ . Die Kolonnen v1 und v2 von V sind gem¨ass Konstruktion linear un- abh¨angige Eigenvektoren zum Eigenwert λ1 = λ2 = 1 mit algebraischer und geometrischer Vielfachkeit 2. Der Eigenvektor v3 geh¨ort zu λ3 = 0 und ist gem¨ass Satz 9.11 linear unabh¨angig zu jeder Linearkombination von v1 und v2. Also sind die drei Vektoren v1, v2, v3 linear unabh¨angig, das heisst sie bilden eine Eigenbasis, und V ist regul¨ar. In der Tat ist V−1 =   5 −1 3 −4 1 −2 4 −1 3   , und somit hat A die Spektralzerlegung A =   1 0 −1 4 3 −2 0 1 1   ︸ ︷︷ ︸ V   1 0 0 0 1 0 0 0 0   ︸ ︷︷ ︸ Λ   5 −1 3 −4 1 −2 4 −1 3   ︸ ︷︷ ︸ V−1 . ♦ Es gibt leider aber auch nicht-diagonalisierbare Matrizen. Hier ist das einfachste Beispiel: Beispiel 9.9: Die Matrix A = ( 1 1 0 1 ) hat das charakteristische Polynom χA(λ) = (1 − λ)2, also den doppel- ten Eigenwert 1. Aber das Gleichungssystem (A − 1 I)v = 0 mit der Koeﬃzientenmatrix A − 1 I = ( 0 1 0 0 ) hat Rang 1. Es ist also n − r = 2 − 1 = 1, das heisst der Eigenraum E1 ist eindimensional; die geometrische Vielfachheit 1 des Eigenwertes ist damit kleiner als die algebraische Vielfachheit 2. ♦ Dieses Beispiel zeigt, dass die geometrische Vielfachheit kleiner als die algebraische sein kann, was bedeutet, dass es dann keine Eigen- basis geben kann. Wie kann man wohl diese Situation charakteri- sieren? Und ist auch das Umgekehrte m¨oglich? Es gelten die folgenden zwei grundlegenden Aussagen: Satz 9.13 F¨ur jeden Eigenwert gilt, dass die geometrische Viel- fachheit kleiner gleich der algebraischen Vielfachheit ist. Weil der Grad von χA genau n ist, sich also die algebraischen Viel- fachheiten zu n summieren, folgt sofort die Notwendigkeit der fol- genden Bedingung. LA-Skript 9-14 11. April 2016 c⃝M.H. Gutknecht Lineare Algebra (2009) Kapitel 9 — Eigenwerte und Eigenvektoren Satz 9.14 Eine Matrix ist genau dann diagonalisierbar, wenn f¨ur jeden Eigenwert die geometrische gleich der algebraischen Viel- fachheit ist. Beweis von Satz 9.13: Es sei λ0 ein fester Eigenwert mit geometrischer Vielfachheit ν. Wir w¨ahlen nun eine Basis v1, . . . , vν ∈ En von Eλ0 aus, die wir nach Satz 4.9 zu einer Basis v1, . . . , vn von En erg¨anzen k¨onnen, welche die Kolonnen einer regul¨aren Matrix V liefert. Es ist dann AV = A ( v1 . . . vn ) = ( Av1 . . . Avν Avν+1 . . . Avn ) = ( v1λ0 . . . vνλ0 Avν+1 . . . Avn ) . Wegen V−1vk = ek folgt, dass C :≡ V−1AV folgende Form hat: C :≡ V−1AV = ( e1λ0 . . . eνλ0 V−1Avν+1 . . . V−1Avn ) = ( λ0Iν E O F ) . Aus der Formel (8.6) f¨ur die Determinante kann man sehen, dass gilt det (C − λI) =det ( (λ0 − λ)Iν E O F − λIn−ν ) =det ((λ0 − λ)Iν) det (F − λIn−ν) , also nach Satz 9.7 und Determinanten-Eigenschaft v) aus Lemma 8.4 χA(λ) = χC(λ) = det (C − λI) = (λ0 − λ)νdet (F − λIn−ν) . Wir schliessen daraus, dass λ0 als Eigenwert von A mindestens die al- gebraische Vielfachheit ν hat. Beweis von Satz 9.14: Aus Lemma 9.8 wissen wir bereits, dass eine diagonale Abbildungsmatrix genau dann existiert, wenn es eine Eigen- basis gibt. Es gebe m verschiedene Eigenvektoren. Wir w¨ahlen nun in jedem Eigenraum Eλk (k = 1, . . . , m) eine Basis {vk,j ; j = 1, . . . , νk}. Sind f¨ur alle Eigenwerte geometrische und algebraische Vielfachheit gleich, so ist ν1 + · · · + νm = n, und wir behaupten, dass alle diese n Basisvek- toren linear unabh¨angig sind. Ist n¨amlich m∑ k=1 νk∑ j=1 vk,j γk,j ︸ ︷︷ ︸ ≡: ck = o , (9.27) so sind die in verschiedenen Eigenr¨aumen liegenden Vektoren ck ja ent- weder Eigenvektoren zu λk oder Nullvektoren. Wegen (9.27), das heisst c1 + · · · + cm = o, folgt aus Satz 9.11 aber, dass sie das letztere sein m¨ussen. Aus der Deﬁnition von ck sieht man dann, dass alle Koeﬃ- zienten γk,j null sein m¨ussen, denn die vk,j sind ja Basisvektoren des k-ten Eigenraumes. Wir schliessen, dass alle Vektoren vk,j zusammen eine Eigenbasis bilden. c⃝M.H. Gutknecht 11. April 2016 LA-Skript 9-15 Kapitel 9 — Eigenwerte und Eigenvektoren Lineare Algebra (2009) Gibt es umgekehrt eine Eigenbasis {bl} von A, so dass also Abl = blλl (l = 1, . . . , n) gilt, so folgt: Zu einem festen Eigenwert λk und Koeﬃzienten αl :≡ { beliebig falls λl = λk , 0 falls λl ̸= λk , (9.28) gilt f¨ur x :≡ ∑n l=1 blαl , dass Ax = A ( n∑ l=1 blαl ) = n∑ l=1 blλlαl = λk n∑ l=1 blαl = λkx . Also ist x ∈ Eλk , und die Dimension dieses Eigenraums ist wegen (9.28) gleich der algebraischen Vielfachheit des Eigenwertes. Bevor wir den Fall analysieren, wo die geometrische Vielfachheit kleiner als die algebraische ist, betrachten wir eine wichtige Klasse von diagonalisierbaren Matrizen. 9.3 Eigenwerte und Eigenvektoren symmetri- scher und Hermitescher Matrizen Die Mehrzahl der Eigenwertprobleme, die in der Praxis auftreten sind selbstadjungiert [self-adjoint], das heisst die Matrizen sind reell symmetrisch oder Hermitesch. In diesem Falle ist die Eigen- wertzerlegung einfach: Die Eigenwerte sind reell, und es gibt stets eine orthonormale Eigenbasis. Falls die Matrix selbst reell ist, ist auch die Eigenbasis reell. Satz 9.15 Ist A ∈ C n×n Hermitesch, so gilt: i) Alle Eigenwerte λ1, . . . , λn sind reell. ii) Die Eigenvektoren zu verschiedenen Eigenwerten sind paar- weise orthogonal in C n. iii) Es gibt eine orthonormale Basis des C n aus Eigenvektoren u1, . . . , un von A. iv) F¨ur die unit¨are Matrix U :≡ ( u1 . . . un ) gilt UHAU = Λ :≡ diag (λ1, . . . , λn). (9.29) LA-Skript 9-16 11. April 2016 c⃝M.H. Gutknecht Lineare Algebra (2009) Kapitel 9 — Eigenwerte und Eigenvektoren Korollar 9.16 Ist A ∈ R n×n symmetrisch, so gilt: i) Alle Eigenwerte λ1, . . . , λn sind reell. ii) Die reellen Eigenvektoren zu verschiedenen Eigenwerten sind paarweise orthogonal in R n. iii) Es gibt eine orthonormale Basis des R n aus Eigenvektoren u1, . . . , un von A. iv) F¨ur die orthogonale Matrix U :≡ ( u1 . . . un ) gilt UTAU = Λ :≡ diag (λ1, . . . , λn). (9.30) Beweis von Satz 9.15: i) und ii): Es seien u und v Eigenvektoren zu den nicht notwendigerweise verschiedenen Eigenwerten λu und λv. Dann folgt aus AH = A, dass ⟨u, Av⟩ = uHAv = uHAHv = (Au)Hv = ⟨Au, v⟩ und damit λv ⟨u, v⟩ = ⟨u, λvv⟩ = ⟨u, Av⟩ = ⟨Au, v⟩ = ⟨λuu, v⟩ = λu ⟨u, v⟩ . (9.31) Wenden wir dies an auf den Fall wo λu = λv und u = v, so folgt wegen ⟨u, u⟩ > 0, dass λv = λu. Dies bedeutet, dass alle Eigenwerte reell sind. Im Falle λu ̸= λv schreiben wir (9.31) als (λv − λu) ⟨u, v⟩ = 0. Wegen λv − λu = λv − λu ̸= 0 folgt dann, dass der zweite Faktor null sein muss. Also stehen in diesem Falle u und v senkrecht aufeinander. iii) und iv): Diese zwei Aussagen sind oﬀensichtlich ¨aquivalent. Wir k¨onnen nat¨urlich in jedem Eigenraum Eλ, weil Unterraum von Cn, ei- ne orthonormale Basis w¨ahlen (vgl. Korollar 6.7). Da nach Teil ii) die Eigenvektoren zu verschiedenen Eigenwerten senkrecht aufeinander ste- hen, folgt, dass diese Basisvektoren zusammen eine orthogonale Basis des Unterraumes bilden, der von allen Eigenvektoren aufgespannt wird; vgl. den Beweis von Satz 9.14. Was uns allerdings fehlt, ist die Gewiss- heit, dass dieser Raum der ganze Cn ist, d.h. dass f¨ur keinen Eigenwert die geometrische kleiner als die algebraische Vielfachheit ist. Oder, mit anderen Worten, dass A diagonalisierbar ist. Dies l¨asst sich mit Induk- tion beweisen; siehe z.B. Nipp/Stoﬀer Seiten 159–161. Beweis von Korollar 9.16: i) Weil jede reelle symmetrische Matrix Hermitesch ist, gelten die Aus- sagen von Satz 9.15, insbesondere Teil i). ii)–iv) Weil Matrix und Eigenwerte reell sind, kann man sich auf reelle Eigenvektoren beschr¨anken, f¨ur die das Skalarprodukt in Rn gleich jenem in Cn ist. Aus der unit¨aren Matrix mit den Kolonnen aus Eigenvektoren wird dabei eine orthogonale Matrix. c⃝M.H. Gutknecht 11. April 2016 LA-Skript 9-17 Kapitel 9 — Eigenwerte und Eigenvektoren Lineare Algebra (2009) Beispiel 9.10: Die dem Vektor w = ( 1 2 2 )T /3 zugeordnete Householder–Matrix Qw :≡ I − 2wwT = 1 9   7 −4 −4 −4 1 −8 −4 −8 1   (9.32) ist symmetrisch und orthogonal. Da die Matrix einer Spiegelung an einer 2–dimensionalen Ebene entspricht, ist auf Grund von Satz 9.13 klar, dass die Eigenwerte 1, 1, -1 sind; der Eigenraum E1 ist die Spiegelebene, der Eigenraum E−1 eine Gerade senkrecht dazu, erzeugt durch den Vektor u3 :≡ w. Um eine orthonormale Basis der Spiegelebene E1 zu ﬁnden, k¨onnten wir hier einfach w zu einer orthonormalen Basis von R3 erg¨anzen. Oder wir k¨onnen eine orthonormale Basis von E1 konstruiren, ohne dass wir vom schon bekannten Eigenvektor u3 Gebrauch machen. Die Matrix Qw − I = 1 9   −2 −4 −4 −4 −8 −8 −4 −8 −8   hat oﬀensichtlich Rang 1, ihre Zeilenstufenform liefert als einzige aus (Qw − 1 I)x = o resultierende nichttriviale Gleichung ⟨3w, x⟩ = x1 + 2x2 + 2x3 = 0 , was gerade die Bedingung ist, dass x orthogonal auf w steht. Die Wahl x2 = 1, x3 = 0 ergibt x1 = −2, also den normierten Eigenvek- tor u1 = ( −2 1 0 )T / √5. Die Wahl x2 = 0, x3 = 1 erg¨abe x1 = −2 ; dieser Vektor w¨are zwar ein Eigenvektor, aber nicht senkrecht zu u1. Um eine orthogonale Eigenbasis zu erhalten m¨ussen wir die Bedingung ⟨u1, u2⟩ = 0 als weitere Gleichung beif¨ugen: x1 + 2x2 + 2x3 = 0 , −2x1 + x2 = 0 . Hier sieht man, dass die Wahl x1 = 2 auf x2 = 4 und x3 = −5 f¨uhrt, also auf u2 = ( 2 4 −5 )T / √45. Man veriﬁziert leicht, dass in der Tat mit U :≡ ( u1 u2 u3 ) die Formel (9.30) bzw. die Spektralzerlegung Qw = UΛUT gilt (auf vier Stellen nach dem Komma gerundet): 1 9   7 −4 −4 −4 1 −8 −4 −8 1   =   −0.8944 0.2981 0.3333 0.4472 0.5963 0.6667 0.0000 −0.7454 0.6667   ×   1 0 0 0 1 0 0 0 −1     −0.8944 0.4472 0.0000 0.2981 0.5963 −0.7454 0.3333 0.6667 0.6667   . ♦ LA-Skript 9-18 11. April 2016 c⃝M.H. Gutknecht Lineare Algebra (2009) Kapitel 9 — Eigenwerte und Eigenvektoren 9.4 Die Jordansche Normalform Es stellt sich immer noch die Frage, wie weit sich eine quadratische Matrix im allgemeinen vereinfachen l¨asst. Ohne Beweis zitieren wir den folgenden Satz, der auch die Analyse der Situation liefert, wo die geometrische Vielfachheit gewisser Eigenwerte kleiner als die algebraische Vielfachheit ist. Satz 9.17 Zu einer quadratischen Matrix A ∈ Cn×n gibt es eine ¨ahnliche, blockdiagonale Matrix J :≡      J1 O · · · O O J2 · · · O ... ... . . . ... O O · · · Jµ      (9.33) mit µ Diagonalbl¨ocken Jk der Ordnung mk der bidiagonalen Form Jk = ( λk ) oder Jk =         λk 1 0 · · · 0 0 λk 1 . . . ... ... . . . . . . . . . 0 ... . . . λk 1 0 · · · · · · 0 λk         . (9.34) Dabei sind die λk nicht unbedingt verschieden. Die Matrix J ist eindeutig bestimmt bis auf die Reihenfolge der Diagonalbl¨ocke Jk. Ist A eine reelle Matrix, die nur reelle Eigenwerte hat, so kann man auch die ¨Ahnlichkeitstransformation reell w¨ahlen. Definition: Eine zu A ¨ahnliche Matrix der Form (9.33)–(9.34) heisst Jordansche Normalform [Jordan canonical form] von A. Die Bl¨ocke Jk nennen wir Jordan-Bl¨ocke [Jordan blocks]; jene mit mk > 1 sind die nicht-trivialen Jordan-Bl¨ocke. ▲ Beispiel 9.11: Ein Beispiel einer nicht-diagonalen Jordanschen Nor- malform ist J =                  5 1 0 0 0 0 0 0 0 0 0 5 1 0 0 0 0 0 0 0 0 0 5 0 0 0 0 0 0 0 0 0 0 5 0 0 0 0 0 0 0 0 0 0 5 0 0 0 0 0 0 0 0 0 0 3 1 0 0 0 0 0 0 0 0 0 3 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0                  . (9.35) Zur Hervorhebung der Bl¨ocke haben wir die 10×10–Matrix als Block- matrix geschrieben. ♦ c⃝M.H. Gutknecht 11. April 2016 LA-Skript 9-19 Kapitel 9 — Eigenwerte und Eigenvektoren Lineare Algebra (2009) Bemerkungen: 1) In der Zerlegung A = VJV−1 (9.36) mit V ≡: ( v1 v2 . . . vn ) sind jene Vektoren vl, deren Index l mit dem der ersten Kolonne eines Blockes Jk ¨ubereinstimmt, die Eigenvektoren, denn f¨ur sie gilt Avl = vlλk. F¨ur die anderen Vektoren vl im k-ten Block gilt stattdessen Avl = vlλk + vl−1 , d.h. (A − λkI)vl = vl−1 . (9.37) Diese Vektoren nennt man Hauptvektoren [principal vectors] zum Eigenwert λk. Satz 9.17 besagt also auch, dass es zu jeder n × n–Matrix A eine Basis von C n aus Eigen- und Hauptvektoren von A gibt. 2) Die Zahl µ der Bl¨ocke ist gleich der Anzahl linear unabh¨angiger Eigenvektoren. A ist diagonalisierbar genau dann, wenn µ = n. F¨ur einen bestimmten Eigenwert λ ist die algebraische Vielfachheit gleich ∑ λk=λ mk (9.38) und die geometrische Vielfachheit gleich ∑ λk=λ 1, das heisst gleich der Anzahl Bl¨ocke mit λk = λ. 3) Die Jordansche Normalform ist in der Matrizentheorie ein wichti- ges Hilfsmittel. Sie ist aber f¨ur praktische numerische Berechnungen unbrauchbar, weil sie nicht stetig von der Matrix (d.h. den Elemen- ten der Matrix) abh¨angt. Durch eine beliebig kleine ¨Anderung einer Matrix mit Jordan-Bl¨ocken kann man erreichen, dass alle Eigen- werte der Matrix verschieden werden, was bedeutet dass alle diese Jordan-Bl¨ocke verschwinden. Nat¨urlich ¨andern dabei auch die Ma- trizen V aus (9.36) unstetig. ▼ Beispiel 9.12: Die Matrix J aus (9.35) hat die vier verschiedenen Eigenwerte 5, 3, 1 und 0, von denen alle ausser 0 mehrfach sind. Die algebraischen und geometrischen Vielfachheiten der Eigenwerte sind in der nachfolgenden Tabelle zusammengefasst: EW λ alg. Vfh. geom. Vfh. Blockgr¨ossen mk 5 5 3 3, 1, 1 3 2 1 2 1 1 1 1 0 2 1 2 ♦ Satz 9.17 liefert auch sofort die folgende Verallgemeinerung von Korollar 9.10. LA-Skript 9-20 11. April 2016 c⃝M.H. Gutknecht Lineare Algebra (2009) Kapitel 9 — Eigenwerte und Eigenvektoren Korollar 9.18 Zerlegen wir die Matrizen V und V−1 aus (9.36) entsprechend den Bl¨ocken von J in vertikale bzw. horizontale Strei- fen, V =   V1 · · · Vµ   , V−1 =    WT 1 ... WT µ    (9.39) (so dass Vk ein n × mk–Block und Wk ein n × mk–Block ist), so l¨asst sich A wie folgt als Summe von µ Matrizen des Ranges mk (k = 1, . . . , µ) darstellen: A = µ∑ k=1 VkJkWT k . (9.40) Beweis: Man schreibt J als Summe von µ block-diagonalen n × n Matrizen, wobei in der k–ten nur gerade der Block Jk steht, w¨ahrend die anderen durch Nullbl¨ocke ersetzt sind. Setzt man diese Summen und die Aufteilungen aus (9.39) in A = VJV−1 ein, folgt die Behauptung sofort. W¨ahrend (9.25) in Korollar 9.10 eine additive Zerlegung einer dia- gonaliserbaren Matrix A in eine Summe von n Rang-1–Matrizen war, ist die Eigenwertzerlegung im allgemeinen Fall gem¨ass (9.40) eine additive Zerlegung in eine Summe von µ Termen, die in der Regel verschiedene R¨ange mk haben. Beispiel 9.13: Die der Matrix J aus (9.35) entsprechende Zerlegung (9.40) einer Matrix A = VJV−1 lautet in der Notation von Korol- lar 9.18: A = V1   5 1 0 0 5 1 0 0 5   WT 1 + V2 ( 5 ) WT 2 + V3 ( 5 ) WT 3 + V4 ( 3 1 0 3 ) WT 4 + V5 ( 1 ) WT 5 + V6 ( 0 1 0 0 ) WT 6 , wobei V1 = ( v1 v2 v3 ) , V2 = ( v4 ) , V3 = ( v5 ) , V4 = ( v6 v7 ) , V5 = ( v8 ) , V6 = ( v9 v10 ) , WT 1 =   wT 1 wT 2 wT 3   , WT 2 = ( wT 4 ) , WT 3 = ( wT 5 ) , WT 4 = ( wT 6 wT 7 ) , WT 5 = ( wT 8 ) , WT 6 = ( wT 9 wT 10 ) . ♦ c⃝M.H. Gutknecht 11. April 2016 LA-Skript 9-21 Kapitel 9 — Eigenwerte und Eigenvektoren Lineare Algebra (2009) LA-Skript 9-22 11. April 2016 c⃝M.H. Gutknecht Lineare Algebra (2009) Kapitel 10 — Anwendungen EW-Zerlegung Kapitel 10 Anwendungen der Eigenwertzerlegung Wir betrachten in diesem Kapitel eine Reihe von Anwendungen der Eigenwertzerlegung. Als erstes behandeln wir verschiedene Typen von Systemen von linearen Diﬀerentialgleichungen mit konstanten Koeﬃzienten. Die gefundene Theorie gibt Anlass zur Deﬁnition der Exponentialfunktion und weiterer Funktionen einer Matrix. Dann betrachten wir die Hauptachsentransformation von quadratischen Formen und ihre Anwendung auf die Charakterisierung von Kegel- schnitten und Fl¨achen zweiter Ordnung (Ellipsoiden usw.). Schliess- lich zeigen wir, dass die Spektralnorm einer Matrix A durch den gr¨ossten Eigenwert von AHA ausgedr¨uckt werden kann. 10.1 Homogene lineare Diﬀerentiagleichungen mit konstanten Koeﬃzienten Systeme erster Ordnung Gegeben sei ein System von homogenen linearen Diﬀerenti- algleichungen erster Ordnung ˙y1(t) = a11y1(t) + a12y2(t) + · · · + a1nyn(t) ... ˙yn(t) = an1y1(t) + an2y2(t) + · · · + annyn(t) (10.1) mit konstanten (zeitunabh¨angigen) Koeﬃzienten akl f¨ur die gesuchten Funktionen y1, . . . , yn. Der Punkt auf ˙yk(t) bezeichnet die Ableitung nach der Zeit t. Das System ist homogen, weil rechts nur Terme vorkommen, die in einem yk(t) linear sind und folglich die Nullfunktionen y1(t) ≡ · · · ≡ yn(t) ≡ 0 eine triviale L¨osung bilden. Eine speziﬁsche L¨osung ist erst eindeutig bestimmt durch n zus¨atzliche Bedingungen, z.B. die Anfangsbedingungen y1(0) = η1 , . . . , yn(0) = ηn , (10.2) worin η1, . . . , ηn vorgegebene Zahlen sind. Deﬁniert man A :≡ ( akl )n k,l=1 , y(t) :≡    y1(t) ... yn(t)    , y0 :≡    η1 ... ηn    , (10.3) so kann man das System (10.1) mit den Anfangsbedingungen (10.2) wie folgt in Matrixform schreiben: ˙y(t) = Ay(t) , y(0) = y0 . (10.4) c⃝M.H. Gutknecht 11. April 2016 LA-Skript 10-1 Kapitel 10 — Anwendungen EW-Zerlegung Lineare Algebra (2009) Ist A diagonalisierbar, AV = VΛ, und deﬁnieren wir z(t) :≡ V−1 y(t), so dass ˙z(t) = V−1 ˙y(t) (weil V zeitunabh¨angige Ele- mente hat), ist (10.4) ¨aquivalent zu V−1 ˙y(t) ︸ ︷︷ ︸ ˙z(t) = V−1 A V ︸ ︷︷ ︸ Λ V−1 y(t) ︸ ︷︷ ︸ z(t) . Das heisst, es gilt ˙z(t) = Λz(t) (10.5) oder ˙z1(t) = λ1z1(t) ... ˙zn(t) = λnzn(t) . (10.6) Durch Bestimmen der Eigenwerte und Eigenvektoren von A ist es uns also gelungen, das System (10.1) gekoppelter linearer Dif- ferentialgleichugen zu entkoppeln. Man nennt dieses Vorgehen die Transformationsmethode. Das entkoppelte System (10.5)/(10.6) hat die allgemeine L¨osung z1(t) = γ1eλ1t ... zn(t) = γneλnt (10.7) mit frei w¨ahlbaren Konstanten γ1, . . . , γn. Um eine kompakte Dar- stellung zu erhalten, deﬁnieren wir die Diagonalmatrix etΛ :≡ diag (eλ1 t, . . . , eλn t) (10.8) und den Konstantenvektor c = ( γ1 . . . γn )T, womit z(t) =    γ1eλ1t ... γneλnt    = etΛ c . (10.9) Die allgemeine L¨osung von (10.1) lautet damit y(t) = V z(t) = V etΛ c . (10.10) Um die spezielle L¨osung, die (10.2) erf¨ullt, zu ﬁnden, benutzen wir, dass z(0) = ( γ1 . . . γn )T = c . Aus (10.10) und y(0) = y0 folgt, dass das lineare Gleichungssystem Vc = y0 oder V    γ1 ... γn    =    η1 ... ηn    (10.11) nach c = ( γ1 . . . γn )T aufzul¨osen ist. Danach ist das berechne- te c in (10.10) einzusetzen. LA-Skript 10-2 11. April 2016 c⃝M.H. Gutknecht Lineare Algebra (2009) Kapitel 10 — Anwendungen EW-Zerlegung Beispiel 10.1: Gegeben sei das Diﬀerentialgleichungssystem 1. Ord- nung ˙y1(t) = −2y1(t) + 2y3(t) ˙y2(t) = −2y1(t) − 3y2(t) − 4y3(t) ˙y3(t) = − 3y3(t) (10.12) und die Anfangsbedingungen y1(0) = 0 , y2(0) = 0 , y3(0) = 1 . (10.13) Die (10.12) entsprechende Matrix A ist A =   −2 0 2 −2 −3 −4 0 0 −3   . (10.14) Sie hat die Eigenwertzerlegung A =   1 −2 0 −2 0 −1 0 1 0   ︸ ︷︷ ︸ V   −2 0 0 0 −3 0 0 0 −3   ︸ ︷︷ ︸ Λ   1 0 2 0 0 1 −2 −1 −4   ︸ ︷︷ ︸ V−1 . (10.15) Es w¨urde allerdings gen¨ugen, V zu kennen, V−1 br¨auchte man nicht. Weil hier der Vektor y0 der Anfangswerte gerade gleich e3 ist, ist die L¨osung c von Vc = y0 aber gerade gleich der dritten Kolonne von V−1: c = V−1 y0 = V−1 e3 = ( 2 1 −4 )T . (10.16) Nach (10.10) erh¨alt man damit als L¨osung des Anfangswertproblems (10.12)–(10.13) y(t) = V etΛ c =   1 −2 0 −2 0 −1 0 1 0     e−2t 0 0 0 e−3t 0 0 0 e−3t     2 1 −4   =   1 −2 0 −2 0 −1 0 1 0     2e−2t e−3t −4e−3t   =   2e−2t − 2e−3t −4e−2t + 4e−3t e−3t   . ♦ Wir betonen, dass die Transformationsmethode in der hier beschrie- benen Form voraussetzt, dass A diagonalisierbar ist. Die Metho- de ist erweiterbar auf nicht-diagonalisierbare Matrizen, aber dann komplizierter. Wir nehmen auch im Rest dieses Abschnittes an, dass A diagonalisierbar ist. Dagegen f¨uhren mehrfache Eigenwerte mit voller geometrischer Vielfachheit auf keine Schwierigkeiten, wie obiges Beispiel illustriert. c⃝M.H. Gutknecht 11. April 2016 LA-Skript 10-3 Kapitel 10 — Anwendungen EW-Zerlegung Lineare Algebra (2009) Systeme h¨oherer Ordnung Neben Systemen von Diﬀerentialgleichung erster Ordnung mit kon- stanten Koeﬃzienten kann man nach demselben Rezept auch linea- re Diﬀerentialgleichungen h¨oherer Ordnung mit konstan- ten Koeﬃzienten l¨osen oder sogar Systeme von solchen. Bekannt- lich lassen sich allgemein Diﬀerentialgleichungen h¨oherer Ordnung auf Systeme von Diﬀerentialgleichung erster Ordnung reduzieren. Ist die Diﬀerentialgleichung n–ter Ordnung x(n)(t) − βnx(n−1)(t) − · · · − β2 ˙x(t) − β1x(t) = 0 (10.17) mit den Anfangsbedingungen x(0) = η1 , ˙x(0) = η2 , . . . , x(n−1)(0) = ηn (10.18) gegeben, so setzen wir y1(t) :≡ x(t) , y2(t) :≡ ˙x(t) , . . . , yn(t) :≡ x(n−1)(t) , (10.19) womit (10.17)–(10.18) geschrieben werden kann in der Form (10.4), ˙y(t) = Ay(t) , y(0) = y0 , mit y und y0 wie in (10.3) und A =        0 1 0 . . . 0 0 0 1 0 ... ... . . . ... 0 0 · · · 0 1 β1 β2 · · · βn−1 βn        . (10.20) Beispiel 10.2: Gegeben sei das Anfangswertproblem 2. Ordnung ¨x(t) − ˙x(t) − 2x(t) = 0 , x(0) = 3 , ˙x(0) = 0 . (10.21) Wir setzen y1(t) :≡ x(t), y2(t) :≡ ˙x(t) und erhalten so das Anfangs- wertproblem 1. Ordnung ˙y1 = y2 ˙y2 = 2y1 + y2 mit η1 = y1(0) = 3 , η2 = y2(0) = 0 . Hier ist A = ( 0 1 2 1 ) , y0 = ( 3 0 ) , χA(λ) = ∣ ∣ ∣ ∣ −λ 1 2 1 − λ ∣ ∣ ∣ ∣ = λ2 − λ − 2 = (λ + 1)(λ − 2) . Die Eigenwerte sind also λ1 = −1, λ2 = 2. Das Auﬂ¨osen von (A − λkI)v = o (k = 1, 2) gibt nach kurzer Rechnung die Eigenvektoren v1 = ( 1 −1 )T und v2 = ( 1 2 )T, also die Matrix V = ( 1 1 −1 2 ) . LA-Skript 10-4 11. April 2016 c⃝M.H. Gutknecht Lineare Algebra (2009) Kapitel 10 — Anwendungen EW-Zerlegung Nun muss man die Anfangsbedingungen transformieren durch L¨osen von (10.11), was γ1 = 2, γ2 = 1, das heisst c = ( 2 1 )T ergibt. Damit wird z1(t) = 2e−t z2(t) = e2t und x(t) = y1(t) = 2e−t + e2t , ˙x(t) = y2(t) = −2e−t + 2e2t . Die gesuchte L¨osung ist also x(t) = 2e−t + e2t. ♦ Systeme zweiter Ordnung ohne Terme erster Ordnung Nun betrachten wir noch Systeme von homogenen linearen Diﬀerentialgleichungen zweiter Ordnung mit konstanten Koeﬃzienten und ohne erste Ableitungen. Sie haben die Form ¨y1(t) = a11y1(t) + a12y2(t) + · · · + a1nyn(t) ... ¨yn(t) = an1y1(t) + an2y2(t) + · · · + annyn(t) (10.22) Ein solches System kann man direkt mit der Transformationsmetho- de behandeln, ohne es auf ein System erster Ordnung zu reduzieren. Wir schreiben es zusammen mit den Anfangsbedingungen y1(0) = η1 , ˙y1(0) = η′ 1 , ... ... yn(0) = ηn , ˙yn(0) = η′ n , (10.23) als ¨y(t) = Ay(t) , y(0) = y0 , ˙y(0) = y1 . (10.24) Wir setzen wieder voraus, dass A diagonalisierbar ist. Zudem seien die Eigenwerte von A negativ. Dies ist typischerweise in Anwen- dungen, wo solche Diﬀerentialgleichungen auftreten, der Fall. Mit AV = VΛ und z(t) :≡ V−1y(t) erhalten wir analog zu (10.4) das entkoppelte System ¨z(t) = Λz(t) . (10.25) Schreiben wir dies komponentenweise, und f¨uhren wir die Gr¨ossen ωk :≡ √ −λk (10.26) ein (hier wird λk < 0 gebraucht), so ergibt sich ¨zk(t) + ω2 kzk(t) = 0 (k = 1, . . . , n). (10.27) Das ist die Diﬀerentialgleichung des harmonischen Oszillators [harmonic oscillator]. Sie hat bekanntlich die allgemeine L¨osung zk(t) = αk cos(ωkt) + βk sin(ωkt) (k = 1, . . . , n) (10.28) c⃝M.H. Gutknecht 11. April 2016 LA-Skript 10-5 Kapitel 10 — Anwendungen EW-Zerlegung Lineare Algebra (2009) mit freien Parametern αk und βk (k = 1, . . . , n). Die allgemeine L¨osung von (10.22) ist damit y(t) = V z(t) = V    α1 cos(ω1t) + β1 sin(ω1t) ... αn cos(ωnt) + βn sin(ωnt)    . (10.29) Durch Einf¨uhrung der Diagonalmatrizen Ω :≡ diag (ω1, . . . , ωn) , cos(Ω t) :≡ diag (cos(ω1t), . . . , cos(ωnt)) , (10.30) sin(Ω t) :≡ diag (sin(ω1t), . . . , sin(ωnt)) und der Parametervektoren a :≡ ( α1 . . . αn )T , b :≡ ( β1 . . . βn )T (10.31) k¨onnte man (10.29) auch schreiben als y(t) = V z(t) = V(cos(Ω t) a + sin(Ω t) b ) . (10.32) Um die spezielle L¨osung, die (10.23) erf¨ullt, zu ﬁnden, bemerken wir zun¨achst, dass gilt zk(0) = αk , ˙zk(0) = βkωk (k = 1, . . . , n) oder eben z(0) = a , ˙z(0) = Ω b . (10.33) Wegen y(t) = Vz(t) resultieren hier aus den Anfangsbedingungen in (10.24) zwei Gleichungssysteme mit der Matrix V, die zu l¨osen sind, um die Parameter αk und βk (k = 1, . . . , n) festzulegen: Va = y0 , Ṽb = y1 mit ̃b :≡ Ω b . (10.34) Da Ω diagonal ist, bedeutet b = Ω−1 ̃b bloss eine komponenten- weise Division durch die ωk. Beispiel 10.3: Der Einfachheit halber und um die n¨otigen ¨Anderungen zu betonen, ersetzen wir einfach in Beispiel 10.1 die erste durch die zwei- te Ableitung: ¨y1(t) = −2y1(t) + 2y3(t) ¨y2(t) = −2y1(t) − 3y2(t) − 4y3(t) ¨y3(t) = − 3y3(t) (10.35) Die Anfangsbedingungen seien nun y1(0) = 0 , y2(0) = 0 , y3(0) = 1 , ˙y1(0) = 1 , ˙y2(0) = 3 , ˙y3(0) = 0 , (10.36) LA-Skript 10-6 11. April 2016 c⃝M.H. Gutknecht Lineare Algebra (2009) Kapitel 10 — Anwendungen EW-Zerlegung Die f¨ur (10.35) relevante Matrix A ist also wieder (10.14) mit der Ei- genwertzerlegung (10.15). Aus den Eigenwerten λ1 = −2, λ2 = −3, λ3 = −3, bekommen wir Ω :≡ diag (ω1, ω2, ωn) = diag (√2, √3, √3) . Wie in (10.16) gilt a = V−1 y0 = V−1 e3 = ( 2 1 −4 )T und zudem wegen y1 = e1 + 3e2 ̃b = V−1 y1 = V−1 (e1 + 3e2) = ( 1 0 −5 )T , also b = Ω −1̃b = ( 1√2 0 −5√3 )T . In der Darstellung (10.29) lautet die gesuchte L¨osung damit y(t) =   1 −2 0 −2 0 −1 0 1 0   ︸ ︷︷ ︸ V    2 cos(√2t) + 1√2 sin(√2t) cos(√3t) −4 cos(√3t) − 5√3 sin(√3t)    ︸ ︷︷ ︸ cos(Ω t) a + sin(Ω t) b . Das Matrix-Vektor-Produkt in dieser Formel k¨onnte man noch ausmul- tiplizieren, um explizite Ausdr¨ucke f¨ur y1, y2, y3 zu bekommen. ♦ 10.2 Funktionen von Matrizen Wir gehen nochmals zur¨uck zu (10.7)–(10.10), wo wir bereits in (10.8) f¨ur die Diagonalmatrix tΛ die Exponentialfunktion deﬁniert haben durch etΛ :≡ diag (etλ1, . . . , etλn) , womit sich (10.7) schreiben liess als z(t) = etΛc . (10.37) Weil tΛ diagonal ist und weil die Potenzreihe von ex f¨ur alle x konvergiert, ist klar, dass etΛ = I + tΛ + 1 2! t2Λ2 + 1 3! t3Λ3 + · · · . Da V (tΛ)k V−1 = tk(VΛV−1)k = tkAk (f¨ur alle k ∈ N), macht es Sinn, f¨ur eine diagonalisierbare Matrix A zu deﬁnieren etA :≡ V etΛ V−1 , (10.38) weil man dann zeigen kann, dass etA = I + tA + 1 2! t2A2 + 1 3! t3A3 + · · · . (10.39) c⃝M.H. Gutknecht 11. April 2016 LA-Skript 10-7 Kapitel 10 — Anwendungen EW-Zerlegung Lineare Algebra (2009) Damit wird aus (10.10) zun¨achst y(t) = Vz(t) = VetΛc = VetΛV−1Vc und dann nach (10.38) und (10.11) y(t) = etAy0 . (10.40) Leitet man dies formal ab (d.h. wie wenn A, y0 und y bloss reelle Zahlen bzw. eine reelle Funktion w¨aren), so ergibt sich gerade ˙y(t) = AetAy0 = A y(t) , also das System (10.4), von dem wir ausgegangen sind. Analog zu (10.38) kann man auch andere Funktionen von Matrizen deﬁnieren: sofern die Matrix A = VΛV−1 diagonalisierbar ist und die Funktion f in den Eigenwerten λk der Matrix deﬁniert ist, setzt man f (Λ) :≡ diag (f (λ1), . . . , f (λn)) . (10.41) und f (A) :≡ Vf (Λ)V−1 . . (10.42) 10.3 Reelle lineare Diﬀerentialgleichungen mit komplexen Eigenwerten Im Prinzip funktioniert unser Vorgehen f¨ur Systeme von linearen Diﬀerentialgleichungen erster Ordnung auch, wenn die Matrix A komplexe Eigenwerte hat. Aber es ist nat¨urlich unsch¨on, wenn A reell ist und man die L¨osung einer rellen Diﬀerentialgleichungen mit Hilfe nicht-reeller komplexer Zahlen darstellt. Das ist auch nicht n¨otig. Wir skizzieren hier das Vorgehen nur, und zwar auf eine neue Art. Detaillierte Beispiele zum “klassischen” L¨osungsweg gibt es in Nipp/Stoﬀer, S. 181ﬀ. Wir fangen an mit einem allgemeinen Resultat ¨uber relle Matrizen. Die reelle Matrix A habe den nicht-reellen Eigenwert λ und den zu- geh¨origen (ebenfalls nicht-rellen) Eigenvektor v. Durch Konjugieren von Av = vλ folgt dann Av = vλ, also gilt zusammengefasst A ( v v ) = ( v v ) ( λ 0 0 λ ) (10.43) Nun kann man Real- und Imagin¨arteil von v ausdr¨ucken durch ( Re v Im v ) = ( v v ) K2 , wo K2 :≡ 1 2 ( 1 −i 1 i ) . (10.44) LA-Skript 10-8 11. April 2016 c⃝M.H. Gutknecht Lineare Algebra (2009) Kapitel 10 — Anwendungen EW-Zerlegung Unter Verwendung der Inversen der 2 × 2–Matrix K2, K−1 2 = ( 1 1 i −i ) = 2 KH 2 , (10.45) k¨onnen wir schliesslich (10.43) ersetzen durch A ( Re v Im v ) = A ( v v ) K2 = ( v v ) K2 2 KH 2 ( λ 0 0 λ ) K2 = 1 2 ( v v ) ( 1 −i 1 i ) ︸ ︷︷ ︸ = ( Re v Im v ) ( 1 1 i −i ) ( λ 0 0 λ ) 1 2 ( 1 −i 1 i ) ︸ ︷︷ ︸ = ( Re λ Im λ −Im λ Re λ ) . Das liefert den folgenden Satz: Satz 10.1 Die reelle Matrix A habe den nicht-reellen Eigenwert λ und den zugeh¨origen Eigenvektor v. Dann ist v ein Eigenvektor zu λ und es gilt (10.44) (wobei K−1 2 = 2 KH 2 ist), und A ( Re v Im v ) = ( Re v Im v ) ( Re λ Im λ −Im λ Re λ ) . (10.46) Nehmen wir nun an, dass die Matrix A in ˙y(t) = Ay(t) die nicht- rellen Eigenwerte λ1 und λ2 = λ1 hat sowie die zugeh¨origen Ei- genvektoren v1 und v2 = v1. Alle anderen n − 2 Eigenwerte und Eigenvektoren seien reell. (Der allgemeine Fall geht analog.) (Der allgemeine Fall l¨asst sich aber ganz analog behandeln.) Wir sind jetzt nur an L¨osungen interessiert, die reell sind. Diese bekommt man, indem man in (10.10) γ2 = γ1 und γ3, . . . , γn ∈ R w¨ahlt, denn dann ist, mit z aus (10.7), y(t) = Vz(t) = vγ1eλ1t + v γ1 eλ1t + v3γ3eλ3t + · · · + vnγneλnt ∈ R n . Deﬁnieren wir die blockdiagonale Matrix K :≡ ( K2 O O In−2 ) , so kann man diese allgemeine relle L¨osung auch schreiben als y(t) = (VK)(K−1z(t)) , (10.47) wobei jetzt hier beide Klammern nur reelle Gr¨ossen enthalten. Zur Bestimmung der Parameter aufgrund der Anfangsbedingungen ersetzen wir schliesslich (10.11), d.h. Vc = y0, durch (VK)(K−1c) = y0 , (10.48) wo nun auch wieder die Gr¨ossen in Klammer reell sind: ̃c :≡ K−1c = ( 2 Re c1 2 Im c1 c3 . . . cn )T . (10.49) c⃝M.H. Gutknecht 11. April 2016 LA-Skript 10-9 Kapitel 10 — Anwendungen EW-Zerlegung Lineare Algebra (2009) 10.4 Quadratische Formen und ihre Hauptachsentransformation Wir betrachten hier erstmals Funktionen, die in mehreren Varia- blen, etwa x1, . . . , xn quadratisch sind, und wir werden vorerst an- nehmen, dass nur quadratische Terme, aber keine linearen oder kon- stanten Terme vorkommen. Ein Beispiel ist Q(x1, x2) :≡ 4x2 1 − 24x1x2 + 11x2 2 . (10.50) Ziel wird es sein, Q als Summe oder Diﬀerenz von Quadraten dar- zustellen, wie das n¨otig ist, um festzustellen, ob die Gleichung Q(x1, x2) = const eine Ellipse oder eine Hyperbel beschreibt. F¨ur die drei Kegelschnitte [conic sections] oder Kurven zwei- ter Ordnung [quadratic curves] lauten die Gleichungen in Nor- malform, d.h. wenn die Achsen des Kegelschnittes mit den Koor- dinatenachsen zusammenfallen und der Mittel- bzw. Scheitelpunkt in 0 liegt, x2 1 a2 + x2 2 b2 = 1 Ellipse [ellipse] mit Halbachsen a und b, x2 1 a2 − x2 2 b2 = 1 nach links und rechts ge¨oﬀnete Hyper- bel [hyperbola] mit reeller Halbachse a und Asymptotensteigung ± arctan(b/a), x2 1 − 2p x2 = 0 nach oben ge¨oﬀnete Parabel [parabola] mit Halbparameter p. Dabei unterscheidet sich die Parabelgleichung massgeblich von den beiden anderen: x2 kommt nur in einem linearen Term vor. Auf diesen Fall werden wir in Abschnitt 10.5 zur¨uckkommen. Ebenso werden wir dort Ellipsen und Hyperbeln betrachten, deren Mittel- bzw. Scheitelpunkt nicht in 0 liegt. Wir gehen diese Probleme nicht nur in zwei sondern in n Variablen an. Im R 3 spricht man von Fl¨achen zweiter Ordnung [quadratic surface]. Wir deﬁnieren zuerst drei eng verkn¨upfte Funktionstypen: Definition: Eine auf R n × R n deﬁnierte symmetrische bili- neare Form [symmetric bilinear form] B ist eine Funktion der Form (x, y) ∈ R n × R n ↦−→ B(x, y) :≡ xTAy ∈ R, (10.51) mit einer reellen, symmetrischen n × n–Matrix A. Die zugeh¨orige auf R n deﬁnierte quadratische Form [quadratic form] Q ist x ∈ R n ↦−→ Q(x) :≡ xTAx ∈ R, (10.52) Eine auf C n × C n deﬁnierte Hermitesche Form [Hermitian form] B ist eine Funktion der Form (x, y) ∈ R n × R n ↦−→ B(x, y) :≡ xHAy ∈ R, (10.53) mit einer Hermiteschen n × n–Matrix A. ▲ LA-Skript 10-10 11. April 2016 c⃝M.H. Gutknecht Lineare Algebra (2009) Kapitel 10 — Anwendungen EW-Zerlegung Oﬀensichtlich gilt gerade, wenn in (10.51) und (10.52) die gleiche Matrix verwendet wird, dass Q(x) = B(x, x) . (10.54) Die Voraussetztung einer symmetrischen Matrix A erfolgt in Q oh- ne Beschr¨ankung der Allgemeinheit: Deﬁniert man Q(x) :≡ xTBx mit einer unsymmetrischen Matrix B, so kann man diese durch die symmetrische Matrix A :≡ 1 2 (B + BT) ersetzen, ohne dass die Werte von Q ¨andern. Wenn man Q(x) ausschreibt, so erkennt man, dass (10.50) verall- gemeinert wird: Q(x) :≡ xTAx = n∑ k=1 n∑ j=1 akj xk xj . (10.55) Wir wollen diese Formel f¨ur Q durch Basistransformation so umfor- men, dass keine gemischten Terme mehr auftreten. Das gelingt uns sofort mit Hilfe der Spektralzerlegung f¨ur symmetrische Matrizen (Korollar 9.16). Setzt man A = UΛUT , ̃x :≡ UTx (10.56) in (10.55) ein, resultiert wegen xTU = ̃xT Q(x) = ̃Q(̃x) :≡ ̃xTΛ̃x = n∑ k=1 λk ̃x2 k . (10.57) Ellipsen und Hyperbeln (n = 2) und Fl¨achen zweiter Ordnung (n = 3) werden, wie erw¨ahnt, durch Gleichungen der Form Q(x) = const dargestellt. Nach der Koordinatentransformation x ↦→ ̃x liegen die Hauptachsen dieser Kurven und Fl¨achen auf den Koordinatenach- sen, weshalb man von Hauptachsen-Transformation spricht. Satz 10.2 [Hauptachsen-Transformation] Jede quadratische Form Q(x) = xTAx l¨asst sich mittels einer auf der Spektralzerlegung von A beruhenden orthogonalen Basistrans- formation auf die Hauptachsen-Darstellung (10.57) bringen. Ist man bereit, zus¨atzlich die L¨ange der neuen Basisvektoren anzu- passen, so kann man in (10.57) die λk durch ±1 und 0 ersetzen: Korollar 10.3 Jede quadratische Form Q(x) = xTAx l¨asst sich durch ¨Ubergang auf eine neue orthogonale (aber im allgemeinen nicht orthonormierte) Basis mit Koordinaten y auf die folgende Form bringen: Q(x) = yTI±y = p∑ k=1 y2 k − r∑ l=p+1 y2 l . (10.58) Die Zahl p ist dabei gleich der Anzahl positiver Eigenwerte von A, und r ist der Rang von A, d.h. r − p ist die Anzahl negativer Eigenwerte. c⃝M.H. Gutknecht 11. April 2016 LA-Skript 10-11 Kapitel 10 — Anwendungen EW-Zerlegung Lineare Algebra (2009) Beweis: Man permutiert die Komponenten von x so, dass jene mit positivem λk zuerst, jene mit λk = 0 zuletzt stehen. Zudem setzt man yk :≡ √ |λk| ̃xk (k = 1, . . . , n). Definition: Das Tripel (p, r −p, n−r) aus Satz 10.3 mit der Zahl der positiven, negativen und null Eigenwerte von A heisst Tr¨agheit [inertia] von A. Die Diﬀerenz p − (r − p) der Zahl positiver und negativer Eigenwerte heisst Signatur [signature] von A. Beispiel 10.4: Der quadratischen Form Q von (10.43) entspricht die Matrix A = ( 4 −12 −12 11 ) mit der Spektralzerlegung A = UΛUT = ( 0.6 0.8 −0.8 0.6 ) ( 20 0 0 −5 ) ( 0.6 −0.8 0.8 0.6 ) . U ist eine Rotation, A hat die Tr¨agheit (1, 1, 0) und die Signatur 0. Wir erhalten Q(x1, x2) = ̃Q(̃x1, ̃x2) = 20̃x2 1 − 5̃x2 2 , (10.59) wobei ( ̃x1 ̃x2 ) = ( 0.6 −0.8 0.8 0.6 ) ( x1 x2 ) = ( 1 5 (3x1 − 4x2) 1 5 (4x1 + 3x2) ) . (10.60) Die Gleichung Q(x1, x2) = 20 hat damit die Normalform 1 20 ̃Q(̃x1, ̃x2) = 1 , d.h. ̃x2 1 − 1 4 ̃x2 2 = 1 . Dies ist eine nach links und rechts ge¨oﬀnete Hyperbel mit Scheitel- punkten (±1, 0) und ¨Oﬀnungswinkel 2 arctan(b/a) = 2 arctan(2). Im urspr¨unglichen Koordinatensystem haben die Scheitel die Koordinaten ( x1 x2 )T = U ( ±1 0 )T = ± ( 0.6 −0.8 )T. ♦ Beispiel 10.5: Zur quadratischen Form Q(x) = Q(x1, x2, x3) = 221x2 1 + 144x1x2 + 480x1x3 + 179x2 2 + 360x2x3 + 100x2 3 . (10.61) geh¨ort die Matrix A =   221 72 240 72 179 180 240 180 100   (10.62) mit der Spektralzerlegung A =   0.60 0.48 0.64 −0.80 0.36 0.48 0.00 −0.80 0.60     125 0 0 0 −125 0 0 0 500   × ×   0.60 −0.80 0.00 0.48 0.36 −0.80 0.64 0.48 0.60   . LA-Skript 10-12 11. April 2016 c⃝M.H. Gutknecht Lineare Algebra (2009) Kapitel 10 — Anwendungen EW-Zerlegung Wenn wir die neuen Koordinaten ̃x1 = 0.60x1 − 0.80x2 ̃x2 = 0.48x1 + 0.36x2 − 0.80x3 ̃x3 = 0.64x1 + 0.48x2 + 0.60x3 einf¨uhren, so erhalten wir also die Hauptachsendarstellung Q(x) = ̃Q(̃x) = 125̃x2 1 − 125̃x2 2 + 500̃x2 3 . Die Gleichung Q(x) = 500 f¨uhrt damit auf 1 500 ̃Q(̃x) = 1 4 ̃x2 1 − 1 4 ̃x2 2 + ̃x2 3 = 1 (10.63) F¨ur die Schnittkurven mit den drei Koordinatenebenen des neuen Ko- ordinatensystems erhalten wir x3 = 0 =⇒ 1 4 ̃x2 1 − 1 4 ̃x2 2 = 1 (Hyperbel) , (10.64) x2 = 0 =⇒ 1 4 ̃x2 1 + ̃x2 3 = 1 (Ellipse) , (10.65) x3 = 0 =⇒ − 1 4 ̃x2 2 + ̃x2 3 = 1 (Hyperbel) . (10.66) Eine solche F¨ache zweiter Ordnung nennt man einschaliges Hyperbo- loid [one-sheeted hyperboloid]. G¨abe es in der Summe in (10.63) zwei Minuszeichen statt nur eines, h¨atten wir ein zweischaliges Hyperbo- loid [two-sheeted hyperboloid]. W¨aren alle drei Vorzeichen positiv (und damit alle drei Schnittkurven Ellipsen), w¨are die Fl¨ache ein Ellipsoid [ellipsoid]. ♦ Man kann Korollar 10.3 auch etwas anders formulieren. Wir k¨onnen schon bei der Spektralzerlegung von A darauf achten, dass die posi- tiven Eigenwerte zuerst und die Eigenwerte null zuletzt aufgef¨uhrt werden. Deﬁnieren wir noch die Diagonalmatrizen D :≡ diag (√ |λ1|, . . . , √|λn|) , (10.67) I± :≡ diag (1, . . . , 1 ︸ ︷︷ ︸ p , −1, . . . , −1 ︸ ︷︷ ︸ r−p , 0, . . . , 0 ︸ ︷︷ ︸ n−r ) , (10.68) so l¨asst sich die Transformation von Q ja wie folgt zusammenfassen: Q(x) = xTAx = xTUΛUTx = xT UD︸︷︷︸ ≡: S︸ ︷︷ ︸ = yT I± DUT ︸ ︷︷ ︸ = S T x ︸ ︷︷ ︸ = y = yTI± y . Es wird also A zerlegt in A = S I± S T , wobei S :≡ UD (10.69) orthogonale Kolonnen hat. Definition: Zwei n × n–Matrizen A und B heissen kongruent [congruent], wenn es eine regul¨are Matrix S gibt, so dass A = S B S T . Der ¨Ubergang von B ↦→ A = S B S T (oder umgekehrt) heisst dann Kongruenztransformation [congruence transformation]. ▲ c⃝M.H. Gutknecht 11. April 2016 LA-Skript 10-13 Kapitel 10 — Anwendungen EW-Zerlegung Lineare Algebra (2009) Damit ist bereits ein Teil des folgenden Satzes bewiesen. Satz 10.4 [Tr¨agheitssatz] Zu jeder reellen symmetrische Matrix A gibt es eine kongruente Matrix I± der Form (10.68). Dabei ist das Tripel (p, r − p, n − r) gleich der Tr¨agheit von A, die unabh¨angig ist von der zugrunde liegenden Kongruenztransformation. Beweis: Was zu zeigen bleibt, ist dass p und r unabh¨angig sind von S, selbst wenn wir eine beliebige regul¨are Matrix S zulassen, und nicht nur jene von der speziellen Form S = UD wie in (10.69). Es seien A = S I± ST = S′ I′ ± (S′)T zwei verschiedene Kongruenztrans- formationen von A auf die gew¨unschte Diagonalform (10.68), und es seien (p, r − p, n − r) und (p′, r′ − p′, n − r′)r die darin auftauchenden Tripel von Indizes. Ferner seien ̃x und ̃x′ die entsprechend transformier- ten Variablen, f¨ur die gilt Q(x) = xTAx = yTI±y = (y′)TI′ ±y′ . (10.70) Schliesslich seien b1, . . . , bn und b′ 1, . . . , b′ n die diesen Koordinaten entsprechenden Basen des Rn. Die Tatsache, dass r = r′ = Rang A, folgt schon daraus, dass nach Voraussetzung S und S′ regul¨ar sind und bei einer Zusammensetzung von Abbildungen (bzw. einem Produkt von Matrizen) der Rang nicht wachsen kann; siehe Korollar 5.10 und Satz 5.16. Aus (10.70) schliesst man, dass in den Unterr¨aumen U :≡ span {b1, . . . , bp}, W :≡ span {b′ p′+1, . . . , b′ n} gilt: Q(x) > 0 f¨ur alle x ∈ U , Q(x) ≤ 0 f¨ur alle x ∈ W . Folglich ist U ∩ W = ∅, also p + (n − p′) ≤ n, d.h. p ≤ p′. Aus Sym- metriegr¨unden gilt auch p′ ≤ p, also notwendigerweise p = p′. Mit dem gleichen Argument h¨atte man auch zeigen k¨onnen, dass r − p = r′ − p′ sein muss. Da in der zuvor betrachteten speziellen Kongruenztransformation das Trippel (p, r −p, n−r) gleich der Tr¨agheit von A ist, muss dies allgemein gelten. Erlaubt man bei der Kongruenztransformation eine beliebige re- gul¨are Matrix, so l¨asst sich diese auch ohne L¨osen des Eigenwert- problemes in einem endlichen Algorithmus ﬁnden. Im wesentlichen handelt es sich dabei um eine symmetrische LR–Zerlegung mit gleichzeitigen Kolonnen- und Zeilenpermutationen, also um eine Zerlegung der Form A = PLDLTPT , (10.71) aber es gibt noch F¨alle, wo ein zus¨atzlicher Trick n¨otig sind. Einen besonders einfachen Fall, wo man ohne Permutationen auskommt, haben wir bereits in Abschnitt 3.4 betrachtet: die symmetrisch po- sitiv deﬁniten Matrizen. Da die Cholesky–Zerlegung einer solchen LA-Skript 10-14 11. April 2016 c⃝M.H. Gutknecht Lineare Algebra (2009) Kapitel 10 — Anwendungen EW-Zerlegung Matrix die Form (10.69) mit I± = I hat, folgt aus dem Tr¨agheitssatz 10.4 sofort die folgende Aussage: Satz 10.5 Eine symmetrische Matrix ist genau dann positiv de- ﬁnit, wenn alle ihre Eigenwerte positiv sind. 10.5 Quadratische Funktionen Ist ein Kegelschnitt in allgemeiner Lage, so enth¨alt seine Gleichung in der Regel auch lineare Terme, wie im Beispiel 4x2 − 24xy + 11y2 + 8x + 26y = 5 . (10.72) Auch hier m¨ochte man die Gleichung auf Normalform bringen, um Lage, Typ und Parameter des Kegelschnittes zu bestimmen. Dazu muss man jetzt das Koordinatensystem nicht nur drehen, sondern auch verschieben. Wir wollen auch dieses Problem allgemein angehen und quadrati- sche Funktionen in n (statt zwei) Variablen betrachten. Wir schreiben jetzt eine gegebene, im R n deﬁnierte, quadratische Funktion [quadratic function] F als F (x) = Q(x) − 2 b Tx − γ , (10.73) wobei Q eine quadratische Form und γ eine reelle Zahl ist. Wir wissen bereits, dass man Q(x) mittels Hauptachsentransformation auf die Normalform ̃Q(̃x) transformieren kann. Man hat dann wegen x = Ũx F (x) = ̃F (̃x) :≡ ̃Q(̃x)−2 ̃b T̃x−γ mit ̃b T :≡ b TU . (10.74) Ist zum Beispiel λk ̸= 0, so kann man den Term 2̃bk̃xk in 2 ̃b T̃x durch eine Verschiebung in der Koordinatenrichtung ̃xk wegtrans- formieren: λk ̃x2 k − 2̃bk ̃xk = λk ( ̃xk − ̃bk λk )2 − ̃b2 k λk . Deﬁnieren wir noch d :≡    d1 ... dn    mit dk :≡ { ̃bk/λk falls λk ̸= 0 , 0 falls λk = 0 , (10.75) p :≡    p1 ... pn    mit pk :≡ { 0 falls λk ̸= 0 , ̃bk falls λk = 0 , (10.76) ̃γ :≡ γ + ∑ λk̸=0 ̃b2 k λk , (10.77) c⃝M.H. Gutknecht 11. April 2016 LA-Skript 10-15 Kapitel 10 — Anwendungen EW-Zerlegung Lineare Algebra (2009) so erhalten wir schliesslich F (x) = ̃F (̃x) = ̃Q(̃x − d) − 2p T̃x − ̃γ (10.78) oder ausgeschrieben F (̃x1, . . . , ̃xn) = ∑ λk̸=0 λk (̃xk − dk)2 − 2 ∑ λk=0 pk̃xk − ̃γ . (10.79) Den zweiten Term in (10.78) kann man auch als −2 p T(̃x−d) schrei- ben, wenn man −̃γ durch −̃γ − 2p Td ersetzt. Satz 10.6 Eine auf dem Rn deﬁnierte quadratische Funktion (10.73) l¨asst sich auf die Form (10.78)–(10.79) reduzieren. Setzt man in einer Gleichung ̃F (̃x) = 0 mit ̃F aus (10.78) / (10.79) n − 2 der Komponenten ̃xk − dk von ̃x − d gleich Null, so erh¨alt man einen Kegelschnitt in Normalform oder die leere Menge. Solch ein Kegelschnitt entspricht dem Schnitt der durch F (x) = 0 deﬁnierten Kegelﬂ¨ache im R n mit einer geeignet verschobenen Koordinatene- bene im ̃x–Koordinatensystem. Oﬀenbar treten Parabeln als Schnittkurven genau dann auf, wenn es Eigenwerte gibt, die null sind. Reine Ellisoide (d.h. Ellipsen f¨ur alle Schnitte) erfordern, dass alle Eigenwerte das gleiche Vorzeichen wie γ haben m¨ussen. Ist es entgegengesetzt, so ist die L¨osungsmenge von F (x) = 0 leer. 10.6 Die Spektralnorm Nun wollen wir auf die Berechnung der in (6.65) eingef¨uhrten Spek- tralnorm oder 2–Norm einer Matrix, ∥A∥2 :≡ sup x̸=0 ∥Ax∥2 ∥x∥2 = sup ∥x∥2=1 ∥Ax∥2 , eingehen, die auch die Konditionszahl κ2(A) aus (6.76) liefert. Satz 10.7 Die Spektralnorm einer Matrix A ∈ En×n ist gleich der Wurzel aus dem gr¨ossten Eigenwert von AHA oder, ¨aquivalent, ∥A∥2 = max{ √ω ; ω Eigenwert von AHA}. (10.80) Falls A reell ist, ist nat¨urlich AHA = ATA. Beweis: Da AHA Hermitesch ist, gibt es gem¨ass Satz 9.15 eine Spek- tralzerlegung AHA = UΩUH mit unit¨arem U und diagonalem Ω. Weil AHA Hermitesch positiv semideﬁnit ist, sind dabei die Diagonalelemen- te ωk von Ω nicht negativ. Mit y :≡ UHx folgt dank ∥y∥2 = ∥x∥2: ∥A∥ 2 2 = sup ∥x∥2=1 ∥Ax∥ 2 2 = sup ∥x∥2=1 xHAHAx = sup ∥x∥2=1 xHUΩUHx = sup ∥y∥2=1 yHΩy = sup ∥y∥2=1 n∑ k=1 ωk|yk|2 = max k=1,...,n ωk . LA-Skript 10-16 11. April 2016 c⃝M.H. Gutknecht Lineare Algebra (2009) Kapitel 10 — Anwendungen EW-Zerlegung F¨ur eine Hermitesche Matrix vereinfacht sich die Formel: es ist ja AHA = A2, und allgemein sind die Eigenwerte von A2 die Qua- drate jener von A. Also ist in diesem Fall ωk = λ2 k und damit max √ωk = max |λk|. Es gilt somit: Satz 10.8 Die Spektralnorm einer Hermiteschen (oder reell sym- metrischen) Matrix A ∈ En×n ist gleich dem Betrag des gr¨ossten Eigenwertes dieser Matrix: ∥A∥2 = max{|ω| ; ω Eigenwert von A}. (10.81) Noch einfacher ist der Fall einer unit¨aren (oder orthgonalen) Matrix U: Wegen UHU = I ist stets ∥U∥2 = 1. F¨ur die 2–Norm der Inversen A−1 einer Matrix kann man analoge Formeln herleiten wie f¨ur die 2–Norm von A selbst: Satz 10.9 Die Spektralnorm der Inversen A−1 einer Matrix ist gleich dem Inversen der Wurzel aus dem kleinsten Eigenwert von AHA oder, ¨aquivalent, ∥A−1∥2 = max { 1 √ω ; ω Eigenwert von AHA} (10.82) = 1 min { √ω ; ω Eigenwert von AHA} . (10.83) Ist A Hermitesch (oder reell symmetrisch), so ist ∥A−1∥2 gleich dem Inversen des Betrages des kleinsten Eigenwertes von A: ∥A−1∥2 = max { 1 |ω| ; ω Eigenwert von A } (10.84) = 1 min {|ω| ; ω Eigenwert von A } . (10.85) Die S¨atze 10.7, 10.8 und 10.9 erlauben uns insbesondere, die in (6.77) eingef¨uhrte 2–Norm–Konditionszahl κ2(A) = ∥A∥2 ∥A−1∥2 zu berechnen. Korollar 10.10 Die 2–Norm–Konditonszahl einer Matrix A ∈ En×n ist gegeben durch κ2(A) = max {√ω ; ω Eigenwert von AHA} min { √ω ; ω Eigenwert von AHA} . (10.86) Ist A Hermitesch (oder reell symmetrisch), so vereinfacht sich die- se Formel zu κ2(A) = max {|ω| ; ω Eigenwert von A } min {|ω| ; ω Eigenwert von A } . (10.87) Insbesondere ist die Konditionszahl immer gr¨osser oder gleich 1. c⃝M.H. Gutknecht 11. April 2016 LA-Skript 10-17 Kapitel 10 — Anwendungen EW-Zerlegung Lineare Algebra (2009) Ist A singul¨ar (aber nicht die Nullmatrix), wird κ2(A) = ∞. Ist A fast singul¨ar, ist κ2(A) gross, das heisst A ist schlecht konditioniert. Beispiel 10.6: Wir betrachten wieder die bereits mehrmals verwen- dete Matrix aus (9.9), A =   −7 2 −6 12 −2 12 12 −3 11   , die das Spektrum σ(A) = {1, 2, −1} hat. Der gr¨osste Betrag eines Ei- genwertes ist hier also 2, der kleinste 1, und der Quotient aus gr¨osstem und kleinstem Eigenwertbetrag ist damit bloss 2. Aber da A nicht sym- metrisch ist, sind diese Zahlen nicht relevant f¨ur Spektralnorm und Kon- ditionszahl, denn man muss die Eigenwerte von ATA betrachten, bezie- hungsweise deren Quadratwurzel. ATA =   337 −74 318 −74 17 −69 318 −69 301   hat das Spektrum σ(ATA) = {0.0044.., 1.4063.., 653.5893..} . Es ergibt sich ∥A∥2 = √653.5893.. = 25.5654.. , ∥A−1∥2 = 1/ √0.0044.. = 15.1587.. und κ2(A) = √ λ3 λ1 = √150186.5876.. = 387.5391.. . Die Norm einer Matrix kann also viel gr¨osser sein als der gr¨osste Eigen- wert und die Kondition kann viel gr¨osser sein als der gr¨osste Quotient der Eigenwerte. ♦ Beispiel 10.7: Sehr bekannte Beipiele liefern auch die sogenannten Hilbert-Matrizen [Hilbert matrices] Hn :≡          1 1 2 1 3 . . . 1 n 1 2 1 3 1 4 . . . 1 n+1 1 3 1 4 1 5 . . . 1 n+2 ... ... ... . . . ... 1 n 1 n+1 1 n+2 . . . 1 2n−1          (10.88) Es sind Hankel–Matrizen, das heisst hij h¨angt nur von der Summe i + j ab (vgl. Abschnitt 2.9). Hankel–Matrizen sind notorisch f¨ur ihre schlech- te Kondition, und die Hilbert–Matrizen ganz besonders. Es ist zum Bei- spiel ∥H5∥2 = 1.5671 , ∥H−1 5 ∥2 = 304142.8416.. , κ2(H5) = 476607.2502.. , . ♦ LA-Skript 10-18 11. April 2016 c⃝M.H. Gutknecht Lineare Algebra (2009) Kapitel 11 — Singul¨arwertzerlegung Kapitel 11 Die Singul¨arwertzerlegung In Kapitel 9 haben wir gesehen, dass man eine quadratische Ma- trix durch geeignete Wahl der Basis in die Jordansche Normalform bringen kann. Diese ist in der Regel diagonal, insbesondere stets wenn alle Eigenwerte verschieden sind oder wenn die Matrix sym- metrisch oder Hermitesch ist. Im zweiten Falle kann man sogar die Basis orthogonal bzw. unit¨ar w¨ahlen. Bei einer linearen Abbildung F : X → Y kann man dagegen Ba- sen in beiden R¨aumen w¨ahlen. Wie wir bereits in Satz 5.20 gesehen haben, l¨asst sich so auf einfache Art erreichen, dass die Abbildungs- matrix die Form B = ( Ir O O O ) . (11.1) hat. Insbesondere l¨asst sich jede m×n Matrix A durch zwei Koor- dinatentransformationen S und T in eine Matrix B = S −1AT dieser Form ¨uberf¨uhren. Aber die einzige Information, die in der Matrix (11.1) noch enthalten ist, ist der Rang r der Abbildung. Diese Form ist in gewissem Sinne zu einfach, als dass man damit noch eine tiefe Einsicht in die Abbildung gewinnen k¨onnte. Wir werden in diesem Kapitel sehen, dass das ¨andert, wenn wir nur orthogonale bzw. unit¨are Koordinatentransformationen zulas- sen. Es resultiert dann eine sowohl f¨ur theoretische Fragen als auch in vielen Anwendungen ¨ausserst n¨utzliche Matrixfaktorisierung, die Singul¨arwertzerlegung. 11.1 Die Singul¨arwertzerlegung: Herleitung Gegeben sei irgend eine reelle oder komplexe m × n Matrix A. Die Matrix AHA ∈ En×n ist Hermitesch und positiv semideﬁnit. Sie hat also reelle, nicht-negative Eigenwerte, sagen wir σ2 k (k = 1, . . . , n) mit σk ≥ 0, und eine orthonormale Eigenbasis {v1, . . . , vn}. Es gilt also AHAV = VΣ2 n (11.2) mit der unit¨aren n×n Matrix V = ( v1 · · · vn ) und der n×n Diagonalmatrix Σ2 n :≡ diag {σ2 1, . . . , σ2 n} . c⃝M.H. Gutknecht 11. April 2016 LA-Skript 11-1 Kapitel 11 — Singul¨arwertzerlegung Lineare Algebra (2009) Dabei k¨onnen wir Eigenwerte und Eigenvektoren so ordnen, dass σ1 ≥ σ2 ≥ · · · ≥ σr > 0 , σr+1 = · · · = σn = 0 , (11.3) worin r :≡ Rang AHA ist. Multiplizieren wir (11.2) von links mit VH, so gilt wegen VHV = In, dass VHAHAV = VHVΣ2 n = Σ2 n . (11.4) In den Kolonnen vk von V ausgedr¨uckt heisst das, dass ∥Avk∥ 2 = ⟨Avk, Avk⟩ = vH k AHAvk = σ2 k (k = 1, . . . , n) , oder ∥Avk∥ = σk (k = 1, . . . , n) . (11.5) Ist r < n, so folgt insbesondere, dass Avk = o (k = r+1, . . . , n), das heisst vr+1, . . . , vn bilden eine Orthonormalbasis eines Unterraumes von ker A ≡ N (A) ⊂ En. Somit ist dim N (A) ≥ n−r, also nach der Dimensionsformel (5.36) Rang A = n − dim N (A) ≤ r. Anderseits ist aber nach den S¨atzen 5.16 (i) und 5.13 r = Rang AHA ≤ min{Rang A, Rang AH} = Rang A . Folglich gilt Rang AHA = Rang A. Also ist dim N (A) = n − r, und vr+1, . . . , vn bilden eine Orthonormalbasis von N (A). Wir unter- teilen V entsprechend in V = ( Vr V⊥ ) :≡ ( v1 . . . vr vr+1 . . . vn ) . (11.6) Die Kolonnen von Vr spannen dabei den zu N (A) orthogonalen Unterraum auf. Durch Beschr¨ankung von A auf diesen Unterraum haben wir statt (11.4) VH r AHAVr = Σ2 r :≡ diag {σ2 1, . . . , σ2 r } . (11.7) Da hier nun Σr regul¨ar ist, k¨onnen wir von links und rechts mit Σ−1 r = Σ−T r multiplizieren: (Σ−T r VH r AH) ︸ ︷︷ ︸ = UH r (AVrΣ−1 r ) ︸ ︷︷ ︸ ≡: Ur = Ir . Die m×r Matrix Ur :≡ AVrΣ−1 r (11.8) hat also orthonormale Kolonnen u1, . . . , ur ∈ Em. Wir k¨onnen diese zu einer Orthonormalbasis von Em und damit Ur zu einer unit¨aren Matrix U erg¨anzen: U = ( Ur U⊥ ) :≡ ( u1 . . . ur ur+1 . . . un ) . (11.9) Nach (11.8) ist AVr = UrΣr, zudem haben wir AV⊥ = O, was wir zusammenfassen k¨onnen in A ( Vr V⊥ ) ︸ ︷︷ ︸ = V = ( Ur U⊥ ) ︸ ︷︷ ︸ = U ( Σr O O O ) ︸ ︷︷ ︸ ≡: Σ . (11.10) LA-Skript 11-2 11. April 2016 c⃝M.H. Gutknecht Lineare Algebra (2009) Kapitel 11 — Singul¨arwertzerlegung Beachtet man noch, dass durch Transponieren die Aussage ¨uber A zu einer v¨ollig analogen Aussage ¨uber AH wird, worin U und V vertauscht sind, so ergibt sich der folgende fundamentale Satz. Satz 11.1 Zu einer komplexen m × n Matrix A vom Rang r existieren unit¨are Matrizen U = ( u1 · · · um ) und V =( v1 · · · vn ) sowie eine m×n Matrix Σ der Form Σ :≡ ( Σr O O O ) (11.11a) mit einem diagonalen r×r Block Σr :≡ diag {σ1, . . . , σr} , (11.11b) dessen Diagonalelemente positiv und gem¨ass (11.3) geordnet sind, so dass gilt A = UΣVH = r∑ k=1 ukσkvH k . (11.11c) Dabei bilden die Kolonnen von U und V orthonormale Eigenbasen von AAH bzw. AHA: AAH = UΣ2 mUH , AHA = VΣ2 nVH (11.11d) mit Σm :≡ diag {σ1, . . . , σm} , Σn :≡ diag {σ1, . . . , σn} , wo σk = 0 falls k > r. ¨Aquivalent zu (11.11c) hat man die Beziehungen AV = UΣ , AHU = VΣT , (11.11e) die zeigen, dass folgendes gilt: {u1, . . . , ur} ist Basis von im A ≡ R(A) , {v1, . . . , vr} ist Basis von im AH ≡ R(AH) , {ur+1, . . . , um} ist Basis von ker AH ≡ N (AH) , {vr+1, . . . , vn} ist Basis von ker A ≡ N (A) , (11.11f) wobei dies alles Orthonormalbasen sind. Ist A reell, so kann man U und V als reelle orthogonale Matrizen w¨ahlen, und alle Exponenten H k¨onnen durch T ersetzt werden. Definition: Die Matrixfaktorisierung (11.11c) heisst Singul¨ar- wertzerlegung [singular value decomposition] oder kurz SVD von A. Die Zahlen σ1 ≥ σ2 ≥ · · · ≥ σr > σr+1 = · · · = σmin{m,n} = 0 (11.12) heissen Singul¨arwerte [singular values] von A. (Dabei kann r = min{m, n} sein, so dass alle σk > 0 sind.) Die Vektoren u1, . . . , um sind die linken Singul¨arvektoren [left singular vectors], und die Vektoren v1, . . . , vn sind die rechten Singul¨arvektoren [right singular vectors]. ▲ c⃝M.H. Gutknecht 11. April 2016 LA-Skript 11-3 Kapitel 11 — Singul¨arwertzerlegung Lineare Algebra (2009) Bemerkung: Man beachte, dass gem¨ass (11.11c) die Singul¨ar- wertzerlegung (¨ahnlich wie die Eigenwertzerlegung einer diagonali- sierbaren quadratischen Matrix) eine additive Zerlegung einer be- liebigen Matrix A in eine Summe von r Rang-1–Matrizen liefert. Ist r ≪ min{m, n}, so ergibt diese eine eﬃziente Art der Berechung von Matrix-Vektor-Produkten Ax. ▼ Beispiel 11.1: Es sei A :=     68 −74 14 −52 46 −28 −17 −44     , (11.13) also ATA = ( 7225 −6300 −6300 10900 ) , AAT =     10100 4800 5200 2100 4800 2900 2100 2050 5200 2100 2900 450 2100 2050 450 2225     . Die Eigenwertzerlegung von ATA und AAT liefert die folgenden, in (11.11d) auftretenden Matrizen: Σn = diag (125, 50) , Σm = diag (125, 50, 0, 0) , V = ( 0.60 0.80 −0.80 0.60 ) , U =     0.8000 0.2000 −0.5488 0.1372 0.4000 −0.4000 0.5831 0.5831 0.4000 0.4000 0.5831 −0.5831 0.2000 −0.8000 −0.1372 −0.5488     , wobei in U die letzten beiden Kolonnen auf vier Stellen nach dem Kom- ma gerundet sind. Die 2 × 2–Matrix ATA hat also die Eigenwerte σ2 1 = 1252 = 15625 und σ2 2 = 502 = 2500, wogegen die 4 × 4–Matrix AAT die Eigenwerte 15625, 2500, 0, 0 hat. Insbesondere ist Rang ATA = Rang AAT = 2. W¨ahrend Σn und Σm quadratische Diagonalmatrizen sind, hat die Ma- trix Σ in der Singul¨arwertzerlegung (11.11c) die gleiche Form wie A. Dabei ist der quadratische Block Σr hier wegen r = n = 2 gleich Σn: Σ =     125 0 0 50 0 0 0 0     . Die Singul¨arwertzerlegung (11.11c) ergibt sich damit hier als LA-Skript 11-4 11. April 2016 c⃝M.H. Gutknecht Lineare Algebra (2009) Kapitel 11 — Singul¨arwertzerlegung     68 −74 14 −52 46 −28 −17 −44     ︸ ︷︷ ︸ A =     0.8000 0.2000 −0.5488 0.1372 0.4000 −0.4000 0.5831 0.5831 0.4000 0.4000 0.5831 −0.5831 0.2000 −0.8000 −0.1372 −0.5488     ︸ ︷︷ ︸ U × ×     125 0 0 50 0 0 0 0     ︸ ︷︷ ︸ Σ ( 0.60 −0.80 0.80 0.60 ) ︸ ︷︷ ︸ VH (11.14) und liefert die additive Rang-1–Zerlegung A =     0.8 0.4 0.4 0.2     ︸ ︷︷ ︸ u1 125 ︸︷︷︸ σ1 ( 0.6 −0.8 ) ︸ ︷︷ ︸ vH 1 +     0.2 −0.4 0.4 −0.8     ︸ ︷︷ ︸ u2 50 ︸︷︷︸ σ2 ( 0.8 0.6 ) ︸ ︷︷ ︸ vH 2 . (11.15) ♦ Die Schlussformeln (11.14) und (11.15) im vorangehenden Beispiel 11.1 zeigen, dass die letzten zwei Kolonnen von U eﬀektiv nicht benutzt werden. Das sieht man in der Tat allgemein schon in der Darstellung (11.10) der Singul¨arwertzerlegung oder in unserer De- ﬁnition (11.8) von Ur. Oﬀensichtlich gilt: Korollar 11.2 Bezeichnen wir wie in (11.6), (11.9) und (11.10) mit Vr die Matrix aus den r ersten Kolonnen von V, mit Ur die Matrix aus den r ersten Kolonnen von U und mit Σr die f¨uhrende r × r Hauptuntermatrix von Σ, so l¨asst sich die Singul¨arwertzer- legung (11.11c) in der kompakten Form A = UrΣrVH r = r∑ k=1 ukσkvH k (11.16) schreiben, worin die m × r Matrix U und die n × r Matrix V orthonormale Kolonnen haben und die r × r Diagonalmatrix Σr positive Diagonalelemente σ1 ≥ · · · ≥ σr > 0 hat. 11.2 Die Singul¨arwertzerlegung: Folgerungen Aus der Singul¨arwertzerlegung lassen sich leicht eine ganze Reihe von interessanten Folgerungen ziehen. Korollar 11.3 Die Unterr¨aume R(A) und N (AH) sind zuein- ander orthogonal und spannen zusammen den Bildraum Em auf. Analog sind die Unterr¨aume R(AH) und N (A) zueinander ortho- gonal und spannen zusammen den Deﬁnitionsraum En auf. c⃝M.H. Gutknecht 11. April 2016 LA-Skript 11-5 Kapitel 11 — Singul¨arwertzerlegung Lineare Algebra (2009) Beweis: Die Aussage folgt sofort aus (11.11f) und der Tatsache, dass die unit¨aren Matrizen U und V orthogonale Kolonnen haben. Die Unterr¨aume R(A) ⊥ N (AH) ⊆ Em und N (A) ⊥ R(AH) ⊆ En, die in Korollar 11.3 vorkommen und f¨ur die die Singul¨arwertzerle- gung gem¨ass (11.11f) orthonormale Basen liefert, sind gerade die vier fundamentalen Unterr¨aume der Matrix A, die wir bereits in Abschnitt 6.4 angetroﬀen haben. Weiter folgt sofort: Korollar 11.4 Es sei A ∈ Em×n, Rang A = r. Dann sind die r positiven Eigenwerte von AHA und AAH gleich, aber die Viel- fachheit des Eigenwertes 0 ist n − r bzw. m − r. Beweis: Dass die nicht-null Eigenwerte gleich sind, folgt aus (11.11d). Nun lassen sich auch Satz 10.7 ¨uber die Spektralnorm und Korol- lar 10.10 ¨uber die entsprechende Konditionszahl neu formulieren: Korollar 11.5 F¨ur die Spektralnorm einer Matrix A ∈ En×n gilt ∥A∥2 = σ1 . (11.17) Ist A regul¨ar, so gilt f¨ur die (der Spektralnorm zugeordnete) Kon- ditionszahl von A ∈ En×n κ2(A) = σ1 σn . (11.18) Die wohl wichtigste Anwendung der Singul¨arwertzerlegung ist eine beste Approximationseigenschaft der Teilsummen der Summe in (11.11e). Wir formulieren sie nur f¨ur quadratische Matrizen, sie gilt aber auch f¨ur andere, wenn man f¨ur diese die 2–Norm geeignet deﬁniert. Wir verzichten auf den Beweis. Satz 11.6 Die Matrix A ∈ En×n habe die Singul¨arwertzerlegung (11.11e) (mit m = n), und f¨ur p = 1, . . . , r sei Ap = p∑ k=1 ukσkvH k . (11.19) Dann ist Ap im Sinne der Spektralnorm die beste Approximation von A durch eine Matrix vom Rang p, das heisst es gilt ∥A − Ap∥2 = min ∥A − B∥2 , (11.20) wenn ¨uber alle Matrizen B ∈ En×n vom Rang p minimiert wird. Dabei ist ∥A − Ap∥2 = σp+1 . (11.21) Diese Approximationseigenschaft kann insbesondere auch f¨ur die Datenkompression eingesetzt werden. LA-Skript 11-6 11. April 2016 c⃝M.H. Gutknecht Index C[a, b], 4-2, 6-2, 6-3 C m[a, b], 4-2 C n, 2-4 C m×n, 2-4 En, 2-4 Em×n, 2-4 R n, 2-4 R m×n, 2-4 L(X, Y ), 6-19 Pm, 4-3 L(En, Em), 6-20 σ(F ), 9-1 m–Vektor, 2-1 n–Tupel, 2-1 n–dimensional, 4-13 ¨Ahnlichkeitstransformation, 5-20, 9-9 ¨ahnlich, 5-20, 9-9 ¨ausseres Produkt, 2-23 Abel, Niels Henrik, 2-9 Banach, Stefan, 6-11 Bunjakovski, Viktor, 2-20 Cauchy, Augustin Louis, 2-20 Cholesky, Andr´e Louis, 3-16 Euklid, 2-16 Fourier, Jean-Baptiste-Joseph, 6-12 Gauss, Carl Friedrich, 1-1 Givens, J. Wallace, 2-29 Gram, Jorgen Pedersen, 6-8 Hankel, Hermann, 2-33 Hermite, Charles, 2-12 Hessenberg, Gerhard, 2-32 Hilbert, David, 6-11 Householder, Alston Scott, 2-30 Jacobi, Carl Gustav, 2-30 Lebesgue, Henri, 6-12 Legendre, Adrien Marie, 6-10 Parseval des Chˆesnes, Marc-Antoine de, 6-7 Pythagoras, 2-16 Sarrus, Pierre, 8-3 Schmidt, Erhard, 6-8 Schur, Issai, 3-12 Toeplitz, Otto, 2-33 Tschebyscheff, Pafnuti Lwowitsch, 6-10 2–Norm, 2-19 einer Matrix, 10-16 2–Norm–Konditionszahl, 6-24 Abbildung aﬃne, 5-17 auf, 5-1 bijektive, 5-1 eineindeutige, 5-1 eineindeutige auf, 5-1 injektive, 5-1 inverse, 5-1 isometrische, 2-31, 6-18 l¨angentreue, 2-31, 6-18 lineare, 5-1 surjektive, 5-1 winkeltreue, 2-31, 6-18 Abbildungsmatrix, 5-3 Abelsche Gruppe, 2-9 Ableitungsoperator, 5-2 Absolutbetrag, 0-6 Abspalten, 9-5 Addition von Matrizen, 2-4 von Vektoren, 4-1 Adjungierte, 2-12 aﬃne Abbildung, 5-17 aﬃner Teilraum, 5-17 algebraische Vielfachheit, 9-6 Alphabet griechisches, 0-8 Axiome, 4-2 Banachraum, 6-11 Bandbreite gesamte, 2-32 obere, 2-32 untere, 2-32 i Index Lineare Algebra (2009) Bandmatrix, 2-32 Basis, 4-10 orthogonale, 6-5 orthonormale, 6-5 Standard-, 4-11 Basiswechsel, 4-18 Berechnung, 8-6 Betrag (einer komplexen Zahl), 0-6 Bidiagonalmatrix obere, 2-31 untere, 2-31 bijektiv, 5-1 Bild, 2-25, 5-1 Bildraum, 5-2 bilinear, 2-18 bilineare Form symmetrische, 10-10 BLAS, 2-34 Block–LR–Zerlegung, 3-12 Blockmatrix, 2-33, 2-34 Cache, 2-34 CBS Ungleichung, 6-4 charakteristische Gleichung, 9-5 charakteristisches Polynom, 9-5, 9-6 Cholesky-Zerlegung, 3-16, 3-18 Cosinussatz, 2-16 d¨unn besetzte Matrix, 2-33, 3-6 Deﬁnitionsraum, 5-2 Deﬂation, 0-7, 9-5 Determinante, 8-3, 8-6 Entwicklung nach Kolonne, 8-10 Entwicklung nach Zeile, 8-10 Diagonale, 2-1 diagonalisierbar, 9-11 Diagonalmatrix, 2-3 Diagramm kommutatives, 5-6 Diﬀerentialgleichungen entkoppelte, 10-5 lineare, erster Ordnung, 10-1 lineare, h¨oherer Ordnung, 10-4 lineare, zweiter Ordnung, 10-5 reelle mit komplexen Eigenwerten, 10- 8 Diﬀerentialoperator, 5-2 Dimension, 4-13 direkte Summe, 4-17 direkte Summe orthogonaler Komplemen- te, 6-13 Dreiecksgestalt, 1-3 Dreiecksmatrix obere, 2-3 untere, 2-3 Dreiecksungleichung, 2-21, 6-1, 6-22 dyadisches Produkt, 2-23 Eigenbasis, 9-10 Eigenraum, 9-1, 9-4 Eigenvektor, 9-1 linker, 9-12 Eigenwert, 9-1 Eigenwertzerlegung, 9-11 eineindeutig auf, 5-1 Einheitsmatrix, 2-3 einschaliges Hyperboloid, 10-13 elementare Zeilen-Operation, 1-6 Eliminationsschema, 1-2 Eliminationsschritt, 1-8, 1-16 Ellipse, 10-10 Ellipsoid, 10-13 endlich-dimensional, 4-13 Entwicklung nach Kolonne, 8-10 nach Zeile, 8-10 Erzeugendensystem, 4-7 Euklidische Norm, 2-19 Euklidischer Vektorraum, 6-2 Eulerschen Formel, 0-6 EV, 9-1 Evaluationsabbildung, 5-2 EW, 9-1 Faktorisierung QR–, 7-9 Fehlergleichungen, 7-5 Fl¨ache zweiter Ordnung, 10-10 Folge quadrat-summierbare, 6-11 Form bilineare, 10-10 Hermitesche, 10-10 Fourier-Koeﬃzienten, 6-12 Fourier-Reihe, 6-12 verallgemeinerte, 6-12 freie Variable, 1-9 freier Parameter, 1-9 Frobenius–Norm, 6-23 LA-Skript ii 11. April 2016 c⃝M.H. Gutknecht Lineare Algebra (2009) Index fundamentale Unterr¨aume, 6-14, 11-6 Funktion diﬀerenzierbare, 4-2 einer Matrix, 10-8 quadrat-integrierbare, 6-12 quadratische, 10-15 stetige, 4-2 Funktional lineares, 5-2 Funktionen stetige, 6-2, 6-3 Gauss-Algorithmus, 1-1 allgemeiner Fall, 1-16 regul¨arer Fall, 1-8 Gauss-Elimination, 1-1 Gauss-Elimination, 3-6, 3-18 allgemeiner Fall, 1-16 regul¨arer Fall, 1-8 geometrische Vielfachheit, 9-2 Gewichtsfunktion, 6-3 Givens–Rotation, 2-29, 6-16 Gleichung charakteristische, 9-5 Gleichungssystem, 1-1 ¨aquivalentes, 1-7 ¨uberbestimmtes, 7-5 homogenes, 1-18 inhomogenes, 1-18 lineares, 2-12 quadratisches, 1-2 regul¨ares, 1-7 singul¨ares, 1-7 Grad exakter, 0-7 Gram–Schmidt–Verfahren, 6-8 klassisches, 6-9, 7-8, 7-12 modiﬁzertes, 6-9, 7-12 griechisches Alphabet, 0-8 Gruppe, 2-9 Abelsche, 2-9 kommutative, 2-9 symmetrische, 8-1 H¨ochstgrad, 0-7 Hankel–Matrix, 2-33 harmonischer Oszillator, 10-5 Hauptachsen-Transformation, 10-11 Hauptdiagonale, 2-1 Hauptuntermatrizen f¨uhrende, 3-14 Hauptvektoren, 9-20 Hermitesche Form, 10-10 Hessenberg-Matrix, 2-32 Hilbert-Matrix, 10-18 Hilbertraum, 6-11 homogen, 1-18 Householder-Matrix, 2-30 Householder-Spiegelung, 2-30 Hpd, 3-16 Hyperbel, 10-10 Hyperboloid einschaliges, 10-13 zweischaliges, 10-13 idempotent, 2-25 Identit¨at, 2-3 imagin¨are Achse, 0-5 Imagin¨arteil, 0-5 induzierte Matrixnorm, 6-20 induzierte Operatornorm, 6-20 inhomogen, 1-18 injektiv, 5-1 inneres Produkt, 2-17, 2-23 Inverse, 2-26 inverse Abbildung, 5-1 invertierbar, 2-26 isometrisch, 2-31, 6-18 Jacobi–Rotation, 2-30, 6-16 Jordan-Block, 9-19 nicht-trivialer, 9-19 Jordansche Normalform, 9-19 K¨orper, 4-4, 4-5 Kegelschnitt, 10-10 Kellerzeile, 1-4 Kern, 5-7 Kleinste-Quadrate-L¨osung, 7-5 Koeﬃzienten eines Gleichungssystems, 1-2 Koeﬃzientenmatrix, 1-2, 2-12 Kofaktor, 8-9 Kolonne, 1-3, 2-1 Kolonnen-Pivotieren, 7-12 Kolonnenmaximumstrategie, 3-8 Kolonnenraum, 5-11 Kolonnenvektor, 2-1 kommutativ, 2-9 kommutatives Diagramm, 5-6 c⃝M.H. Gutknecht 11. April 2016 LA-Skript iii Index Lineare Algebra (2009) kommutieren, 2-8 Komplement orthogonales, 6-13 komplement¨are Unterr¨aume, 4-17 komplexe Ebene, 0-5 komplexe Zahl, 0-5 Komponente, 2-2 Kondition, 6-24 Konditionszahl, 6-24, 10-16, 10-17, 11-6 kongruent, 10-13 Kongruenztransformation, 10-13 konjugiert-komplex, 0-5 Koordinaten, 4-15 Koordinatenabbildung, 5-6 Koordinatendarstellung, 4-16 Koordinatentransformation, 4-18 Koordinatenvektor, 4-15 Kronecker-Symbol, 6-5 Kurve 2. Ordnung, 10-10 L¨ange, 2-19 l¨angentreu, 2-31, 6-18 L¨osung allgemeine, 1-14 im Sinne der kleinsten Quadrate, 7-5 nichttriviale, 1-18 triviale, 1-18 L¨osungsvektor, 2-12 LAPACK, 2-34 LDR–Zerlegung, 3-6 LDU–Zerlegung, 3-6 Legendre–Polynome, 6-10 linear abh¨angig, 4-8, 4-10 linear unabh¨angig, 4-8 unendlich viele Vektoren, 4-10 lineare Abbildung, 2-25, 5-1 beschr¨ankte, 6-19 lineare H¨ulle, 4-7 linearer Operator, 5-2 beschr¨ankter, 6-19 linearer Raum, 4-1 mit Skalarprodukt, 6-2 normierter, 6-1 vollst¨andiger, 6-11 lineares Funktional, 5-2 Linearfaktor, 0-7 Linearkombination, 2-10, 4-7 linker Eigenvektor, 9-12 LR–Zerlegung, 3-2, 3-6, 3-10, 3-18 regul¨arer Fall, 3-5 symmetrische, 10-14 Updating der, 3-15 LU–Zerlegung, 3-2 Matrix, 1-2, 2-1 2–Norm einer, 10-16 adjungierte, 2-12 Bandbreite einer, 2-32 bidiagonale, 2-31 d¨unn besetzte, 2-33, 3-6 diagonale, 2-3 Diagonale einer, 2-1 diagonalisierbare, 9-11 Dreiecks-, 2-3 Einheits-, 2-3 Elemente einer, 2-1 Gr¨osse einer, 2-1 Hankel–, 2-33 Hauptdiagonale einer, 2-1 Hermitesch positiv deﬁnite, 3-16 Hermitesch transponierte, 2-12 Hermitesche, 2-13 Hessenberg, 2-32 Inverse einer, 2-26 invertierbar, 2-26 Koeﬃzienten-, 2-12 Kolonne einer, 2-1 konjugiert transponierte, 2-12 konjugiert-komplexe, 2-12 Multiplikation mit Skalar, 2-4 Multiplikation mit Zahl, 2-4 Null-, 2-3 orthogonale, 2-29 quadratische, 2-2 regul¨are, 2-27 schiefsymmetrische, 2-14 Spalte einer, 2-1 Spektralnorm einer, 10-16 symmetrisch positiv deﬁnite, 3-16 symmetrische, 2-13 Toeplitz–, 2-33 transponierte, 2-12 tridiagonale, 2-32 unit¨are, 2-29 Vielfaches einer, 2-4 Zeile einer, 2-1 Matrix-Vektor-Produkt, 2-10 Matrixnorm, 6-23 LA-Skript iv 11. April 2016 c⃝M.H. Gutknecht Lineare Algebra (2009) Index induzierte, 6-20 kompatible, 6-23 untergeordnete, 6-20 Matrixordnung, 2-2 Matrizen ¨ahnliche, 5-20, 9-9 Addition von, 2-4 kommutierende, 2-8 kongruente, 10-13 Multiplikation von, 2-5 Produkt von, 2-5 Summe von, 2-4 Maximumnorm, 6-2 Monome, 4-8, 4-12 Multiplikation von Matrizen, 2-5 skalare, 4-1 Multiplikationsoperator, 5-3 nichttriviale L¨osung, 1-18 Norm, 2-19, 2-23, 6-1, 6-3 Euklidische, 2-19 Normalform Jordansche , 9-19 Normalgleichungen, 7-6 normierter linearer Raum, 6-1 normierter Vektorraum, 6-1 Nullmatrix, 2-3, 2-9 Nullraum, 5-11 Nullstelle, 0-7 Nullteiler, 2-9 Nullvektor, 2-3, 4-1 Operator linearer, 5-2 Operatornorm untergeordnete, 6-20 Ordnung einer Matrix, 2-2 orthogonal, 6-4 Orthogonalpolynomen, 6-10 orthogonale Basis, 6-5 orthogonale Komplement, 6-13 orthogonale Matrix, 2-29 orthogonale Vektoren, 2-22 Orthogonalprojektion, 7-1 orthonormale Basis, 6-5 Oszillator harmonischer, 10-5 Parabel, 10-10 Parallelrechner, 2-34 Parameter freier, 1-9 Parsevalsche Formel, 6-8 partielles Pivotieren, 3-8 Permutation, 8-1 Permutationsmatrix, 2-30 Pivot, 1-4 Pivotelement, 1-4, 1-8, 1-16 Pivotgleichung, 1-4 Pivotieren Kolonnen-, 7-12 partielles, 3-8 vollst¨andiges, 3-8 Pivotkolonne, 1-4 Pivotstrategie, 1-9, 3-8 Pivotvariable, 1-13 Pivotzeile, 1-4, 1-8, 1-16 Polarform, 0-6 Polynom, 0-7 charakteristisches, 9-5, 9-6 Polynome Legendre–, 6-10 orthogonale, 6-10 Tschebyscheﬀ–, 6-10 Vektorraum aller, 4-8 Vektorraum der, 4-3 vom Grad m, 4-3, 4-8 positiv deﬁnit, 3-16 positiv semideﬁnit, 3-16 Produkt ¨ausseres, 2-23 dyadisches, 2-23 inneres, 2-17 Matrix-Vektor-, 2-10 Skalar-, 2-17 Projektion, 7-1, 9-3 orthogonale, 7-1 schiefe, 7-1 Projektionen auf Koordinatenachsen, 6-7 Projektor, 7-1 orthogonaler, 7-1 Pseudoinverse, 7-6 Pythagoras Satz von, 2-16 QR–Faktorisierung, 7-9 c⃝M.H. Gutknecht 11. April 2016 LA-Skript v Index Lineare Algebra (2009) QR–Zerlegung, 7-10 quadratische Funktion, 10-15 Quadratwurzel einer Diagonalmatrix, 3-16 R¨uckw¨artseinsetzen, 1-3, 1-8, 1-16, 3-3 Rang, 1-17 einer Matrix, 1-15 eines Gleichungssystems, 1-15, 1-17 Raum aﬃner, 5-17 normierter linearer, 6-1 Realteil, 0-5 Rechenaufwand Gauss-Elimination, 3-7 LR–Zerlegung, 3-7 rechte Seite, 1-2, 2-12 rechte Seiten mehrere, 2-12 Rechtsdreiecksmatrix, 2-3 reelle Achse, 0-5 Regel von Sarrus, 8-3 Residuenvektor, 7-5 Residuum, 7-5 Restgleichungssystem, 1-4 0-tes, 1-8, 1-16 Ring, 2-9 der n × n Matrizen, 2-9 der ganzen Zahlen, 4-3 der Polynome, 4-3 mit Eins, 2-9 nicht-kommutativer, 2-9 Rotation, 2-29, 6-16, 9-3 Sarrus Regel von, 8-3 schiefsymmetrisch, 2-14 Schmidtsches Orthogonalisierungsverfah- ren, 6-9 Schur–Komplement, 3-12 Schwarz Hermann Amandus, 2-20 Schwarzsche Ungleichung, 6-4 Selbstabbildung, 5-2 senkrecht, 2-22, 6-4 sesquilinear, 2-18 Signatur, 10-12 Signum, 8-3 Singul¨arwertzerlegung, 11-3 Singul¨arvektor linker, 11-3 rechter, 11-3 Singul¨arwert, 11-3 Singul¨arwertzerlegung, 11-1 Skalar, 2-4, 4-1 Skalarenk¨orper, 4-4 Skalarprodukt, 2-17, 2-23 Spalte, 1-3, 2-1 Spaltenvektor, 2-1 spd, 3-16 Spektralnorm, 6-20, 10-16, 11-6 Spektralzerlegung, 9-11, 9-12 Spektrum, 9-1 Spur, 9-6 Standardbasis, 4-11 Subtraktion, 4-4 Summe direkte, 4-17 orthogonaler Komplemente, 6-13 surjektiv, 5-1 SVD, 11-3 symmetrische bilineare Form, 10-10 symmetrische Gruppe, 8-1 Teilmengen orthogonale, 6-4 Teilraum aﬃner, 5-17 Toeplitz–Matrix, 2-33 Tr¨agheit, 10-12 Transformation auf Hauptachsen, 10-11 Kongruenz-, 10-13 Transformationsmatrix, 4-18 Transponierte, 2-12 Transposition, 8-1 Tridiagonalmatrix, 2-32 triviale L¨osung, 1-18 Tschebyscheﬀ–Polynome, 6-10 Unbekannte, 1-2 Ungleichung Cauchy-Schwarz-, 2-20 Schwarzsche, 2-20, 6-4 unit¨ar, 2-29 unit¨are Matrix, 2-29 unit¨arer Vektorraum, 6-2 untergeordnete Operatornorm, 6-20 Unterr¨aume fundamentale, 6-14, 11-6 LA-Skript vi 11. April 2016 c⃝M.H. Gutknecht Lineare Algebra (2009) Index orthogonale, 6-4 Unterraum, 4-6 aufgespannter, 4-7 erzeugter, 4-7 Urbild, 5-1, 5-7, 5-8 Variable freie, 1-9 Vektor, 1-2, 4-1 Vektoren orthogonale, 2-22, 6-4 Vektorraum, 4-1 n–dimensionaler, 4-13 Dimension eines, 4-13 endlich-dimensionaler, 4-13 Euklidischer, 6-2 mit Skalarprodukt, 6-2 normierter, 6-1 orthogonaler, 6-2 topologischer, 6-11 unit¨arer, 6-2 vollst¨andiger, 6-11 Vektorrechner, 2-34 Vektorregister, 2-34 Vertr¨aglichkeitsbedingung, 1-15, 3-10 Vertr¨aglichkeitstest, 1-16 Vielfachheit algebraische, 9-6, 9-14, 9-15 geometrische, 9-2, 9-14, 9-15 vollst¨andig, 6-11 vollst¨andige Pivotieren, 3-8 Vorw¨artseinsetzen, 3-3 Wertebereich, 5-1, 5-11 Winkel zwischen Vektoren, 2-22, 6-4 winkeltreu, 2-31, 6-18 Zeile, 1-3, 2-1 Zeilen-Operation elementare, 1-6 Zeilenstufenform, 1-13, 1-15 Reduktion auf, 3-9 Zeilenvektor, 2-1 Zeilenvertauschung, 1-8, 1-16 Zerlegung Cholesky-, 3-16, 3-18 LDR–, 3-6 LDU–, 3-6 LR–, 3-2 LU–, 3-2 QR–, 7-10 SVD, 11-3 zweischaliges Hyperboloid, 10-13 c⃝M.H. Gutknecht 11. April 2016 LA-Skript vii","libVersion":"0.3.2","langs":""}