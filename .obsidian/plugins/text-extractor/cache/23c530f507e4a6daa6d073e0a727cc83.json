{"path":"sem5/VLSI1/VRL/extra/Top-Down-Digital-VLSI-Design/Chapter-5---Functional-Verification_2015_Top-Down-Digital-VLSI-Design.pdf","text":"CHAPTER 5 FUNCTIONAL VERIFICATION 5.1 GOALS OF DESIGN VERIFICATION The ultimate goal of design verification is to avoid the manufacturing and deployment of flawed designs. Large sums of money are wasted and precious time to market is lost when a microchip does not perform as expected. Any design is, therefore, subject to detailed verification long before manufacturing begins and to thorough testing following fabrication. One can distinguish three motivations (after the late A. Richard Newton): 1. During specification: “Is what I am asking for what is really needed?” 2. During design: “Have I indeed designed what I have asked for?” 3. During testing: “Can I tell intact circuits from malfunctioning ones?” Any of these questions can refer to two distinct circuit qualities: Functionality describes what responses a system produces at the output when presented with given stimuli at the input. In the context of digital ICs, we tend to think of logic networks and of package pins but the concept of input-to-output mapping applies to information processing systems in general. Functionality gets expressed in terms of mathematical concepts such as algorithms, equations, impulse responses, tolerance bands for numerical inaccuracies, finite state machines (FSM), and the like, but often also informally. Parametric characteristics, in contrast, relate to physical quantities measured in units such as Mbit/s, ns, V, μA, mW, pF, etc. that serve to express electrical and timing-related characteristics of an electronic circuit. Observation 5.1. Experience has shown that a design’s functionality and its parametric characteristics are best checked separately as goals, methods and tools are quite different. Top-Down Digital VLSI Design © 2015 Elsevier Inc. All rights reserved. 301 302 CHAPTER 5 FUNCTIONAL VERIFICATION 5.1.1 AGENDA Our presentation is organized accordingly with section 5.2 presenting the options for specifying a design’s functionality. The bulk of the material then is about developing a simulation strategy that maximizes the likelihood of uncovering design flaws. After having exposed the puzzling limitations of functional verification in the first part of section 5.3, we will discuss how to prepare test data sets and how to make use of assertions to render circuit models “self-checking”. How to organize simulation data and simulation runs is the subject of section 5.4, while section 5.5 gives practical advice on how to code testbenches using HDLs. Neither parametric issues nor the testing of physical parts will be addressed here. 5.2 HOW TO ESTABLISH VALID FUNCTIONAL SPECIFICATIONS 303 5.2 HOW TO ESTABLISH VALID FUNCTIONAL SPECIFICATIONS Specifications available at the outset of a project are almost always inaccurate and incomplete. While parametric characteristics are relatively easy to state, expressing complex functionalities in precise yet concise terms is much more difficult. Functional specifications are, therefore, often stated verbally or graphically. There is a serious risk with doing so, however. Warning example An ASIC had to interface with an industry-standard microprocessor bus. Specifications made reference to official documents released by the CPU manufacturer where bus read and write cycles were described in great detail along with precise timing diagrams. Although the ASIC was designed and tested with these requirements in mind, systems immediately crashed because of bus contentions when first prototypes were plugged into the target board. What had gone wrong? It was found that the ASIC worked fine as long as its chip select line was active. When deselected, however, the pad drivers failed to release the bus, i.e. they did not revert to a high- impedance state. This obvious necessity had been omitted in the specifications and, as a consequence, been ignored throughout the subsequent design and test phases. \u0002 In more general terms, the subsequent quote from [138] nicely summarizes the situation. Many computer systems fail in practice, not because they don’t meet their specifications, but because the specifications left out some unanticipated circumstances or some unusual combination of events, so that when the unexpected occurred, the system was not able to deal with it. This is not necessarily due to sloppiness or stupidity on the part of the designer or to inadequate design methodology; it is a fundamental characteristic of the design process.1 This leaves us with three important issues: “How to ascertain specifications are precise, correct and complete?” “How to make sure specifications describe the functionality that is really wanted and needed?” “How to have customers, marketing and engineers share the same understanding?” As natural language and informal sketches have been found to be inadequate, let us next discuss two approaches for arriving at more dependable specifications. 1 As testified by numerous tragedies, this applies to any kind of technical system, not just computers. Just study the conditions and failure mechanisms that have led to the sinking of RMS Titanic, the Challenger space shuttle accident, the crash of Airbus flight AF 447, or the nuclear disaster of the Fukushima power plant. 304 CHAPTER 5 FUNCTIONAL VERIFICATION 5.2.1 FORMAL SPECIFICATION Ideally, all requirements for a circuit or system could be cast into a set of formal specifications which then would serve as a starting point for a rigorous mathematical proof of correctness. Over the years, a broad variety of formalisms has been devised for capturing behavioral aspects of numerous subsystems from many different fields including truth tables, signal flow graphs, equations, state graphs, statecharts, Petri nets, and signal transition graphs (STG). A difficulty is the limited scope of each such formalism. Signal flow graphs, for instance, were developed for describing transformatorial systems, but are inadequate for modeling reactive behavior. Although Petri nets and finite state machines can, in theory, describe any kind of computation, they become unmanageable when applied to numerical computations or to highly complex situations. Most VLSI circuits, on the other hand, include diverse subsystems some of which are more of transformatorial nature (datapaths, lookup tables) and others more of reactive nature (controllers, interfaces). Relying on a single formal method for specifying the desired functionality of an entire chip or system is not normally practical. A more mundane difficulty is that mathematical formalisms are unsuitable for communicating with customers and management. Also from a practical perspective, there must exist a straightforward and foolproof way to break down a system’s specifications into specs for its various components in order to support collaborative development in a team, and to support products that comprise both hardware and software. 5.2.2 RAPID PROTOTYPING Prototyping often is the only viable compromise between strictly formal and totally informal specifica- tion. By rapid or virtual prototype we understand an algorithmic model that emulates the functionality of the target circuit but not necessarily its architectural, electrical and timing characteristics. A virtual prototype can be implemented ◦ as software code that runs on a general-purpose computer, microprocessor, or DSP, ◦ with the aid of generic software tools for system-level simulations,2 or ◦ by configuring FPGAs or other FPL devices. 2 Such as Matlab/Simulink or SystemC. 5.2 HOW TO ESTABLISH VALID FUNCTIONAL SPECIFICATIONS 305 write circuit model specifications test cases select B M EB E E model compilation E M B M SR certification of virtual prototype E I O E M adjust until all functional tests are passed test suite generation semi-automatic E S R E M behavioral languageB executable code selected stimuli expected responses E S R data transport human effort circuit modelM genuine input data genuine output data I O ✓ accept as golden model FIGURE 5.1 Successive reﬁnement of a rapid prototype and its eventual contribution toward preparing a test suite.3 The typical procedure goes: 1. Apply formal methods (e.g. equations and statecharts) to capture specifications. 2. Use them as a starting point for developing a virtual prototype. 3. Make the prototype as widely available as possible for a thorough evaluation. 4. Refine specifications and prototype until satisfied, see fig.5.1. Once a virtual prototype has been thoroughly certified in a multitude of test runs, it gets elevated to a golden model which is assumed to be free of functional errors for the purpose of the subsequent design steps. Such a model not only defines the target functionality of the circuit-to-be but also helps to prepare the expected responses from a selection of stimuli. The pros and cons of rapid prototyping are as follows. 3 Figs.5.1, 5.8 and 5.9 come as T-diagrams, a notation from compiler engineering that serves to plan the porting of programs from one machine to another [139]. As an extension, two extra symbols have been added. One stands for a test suite and the other for a piece of information-processing hardware, i.e. for a digital ASIC or an FPL device. Synthesis tools are viewed here as compilers that turn behavioral models into gate-level netlists, and gate-level simulators as interpreters that translate between netlists and executable code. E I O E P E program I O E P coded in machine language being executed thereby processing input into output on computer understanding SR E E P Ptest suite verifying program by feeding it with stimuli and checking its output against specified responses S R I OC circuit C thereby generating output O processing input I E I O E L P L L E program I O P coded being executed thereby processing input into output in source language by interpretation into Eon computer understanding E P LE P LE E L P E from source language E translation of program into machine language by compiler coded in 306 CHAPTER 5 FUNCTIONAL VERIFICATION + Demonstrations of the prospective functionality can be arranged at an early stage. + Functionality can be verified on the basis of real world data. + Shortcomings of the initial specifications are likely to get exposed in the process. + No time and money gets wasted for hardware design before prototype is approved by all, including management and customers. + Design iterations and fine tuning are not penalized by the long turnaround times associated with IC design and manufacturing. + Prototype is amenable to peer review and code inspection. + Computes expected responses for any selection of stimuli. − There is no guarantee that all critical cases get covered during prototype testing. − Slips that are related to timing or electrical problems are unlikely to be found because they are not rendered correctly by a purely functional model. − Performance of the prototype does not normally come close to that of the final VLSI chip. Observation 5.2. Rapid prototyping gives developers the opportunity to make and uncover more mistakes earlier, and so to save precious cost and time. 5.2.3 HARDWARE-ASSISTED VERIFICATION This is a particular case of rapid prototyping that leverages FPGA technology on a larger scale. Special circuit boards that carry multiple reprogrammable devices are used to extensively emulate the circuit- to-be gate by gate before committing to mask-programmed VLSI. The emulator boards connect to a host computer for control, storage of stimuli and responses, tracing of internal nodes, visualization, etc. While hardware accelerators are available commercially,4 developers of high-end microprocessors and ASICs tend to come up with proprietary systems optimized for their specific needs. Example The Bluegene/Q ASIC, a multi-processor SoC implemented in IBM’s 45 nm SOI CMOS technology and targeted to run with a 1.6 GHz clock has been emulated at a speed equivalent to a 4 MHz clock [140].5 The hardware accelerator for this 1.47 · 109 transistor circuit consists of 28 circuit boards designed around Virtex-5 LX330 FPGAs and standard RAMs. The design team felt that building custom hardware specifically for functional verification of Bluegene/Q was perfectly worth the effort as hardware emulation runs over 100 000 times faster than the logic-level software simulation of the same design. \u0002 Observation 5.3. It is just not possible to functionally verify complex high-performance designs without recurring to complex high-performance emulation tools. A hardware prototype may even be tested within the target environment, provided all surrounding equipment can be made to operate at a reduced clock rate consistent with the prototype’s execution speed. This approach is particularly helpful for locating interface problems. 4 The Cadence Palladium box and Mentor’s Veloce products are two examples. 5 Several factors make emulation considerably slower than the real target circuit: i) FPGAs are inherently slower than hardwired ICs. ii) Complex designs need to be distributed over many FPGAs or even boards. iii) External RAM may be required to emulate on-chip memory. iv) Serialization and deserialization may be required for communicating between distinct ICs and boards whereas fast on-chip busses will ultimately do the job. 5.3 PREPARING EFFECTIVE SIMULATION AND TEST VECTORS 307 5.3 PREPARING EFFECTIVE SIMULATION AND TEST VECTORS 5.3.1 A FIRST GLIMPSE AT VLSI TESTING Following fabrication, every single chip is subject to thorough tests. Most of those tests have automated test equipment (ATE) monitor the signal waveforms that result when predefined electrical waveforms are being applied to the circuit under test (CUT) over many, many clock cycles. Simulation essentially does the same, albeit with a virtual circuit model commonly referred to as model under test (MUT). Table 5.1 juxtaposes the respective terms. Simulation and testing are said to be dynamic verification techniques. This contrasts with code inspection, formal verification, equivalence checking, timing analysis, and other static verification techniques that do not depend on signals, clocks, waveforms, or test data in any way. Simulation prevails when it comes to check a design’s functional behavior. This is mainly because of the limited capabilities of today’s formal verification methods. However, simulation and testing both raise a variety couple of fundamental and practical difficulties that we are going to address in the remainder of this chapter. Let us begin by studying the question “What does it take to uncover a design flaw via I/O signals?”6 Example Consider a multiplexer buried within a large circuit. Assume that its two control inputs have been permuted by accident, see fig.5.2. Writing (0 to 1) rather than (1 downto 0) in the VHDL code, or [0:1] instead of [1:0] in SystemVerilog, for that matter, suffices. flawed design stimuli 0 instead of 1 actual response expected responses test suite ? = observe and compare 1 0 0 0 1 2 3 1 0 sensitize the bug propagate the bugged reaction two signals permuted by mistake 1 instead of 0 10 instead of 01 X Y Z 0 1 1 1 1 1 0 0 1 0 0 1 . . . . . . . . . . . . 1 . . B C A arbitrary logic sequential arbitrary logic sequential FIGURE 5.2 A typical design ﬂaw and the preconditions necessary for uncovering it. 6 By I/O signal we mean a signal that connects to a package pin or to a pad on the die so that it be driven and/or observed from externally. 308 CHAPTER 5 FUNCTIONAL VERIFICATION Three preconditions must hold for that design flaw to become manifest. 1. Bug sensitization. The design error must be made to provoke a condition other than the normal one in the circuit. In the occurrence, the permuted control inputs are driven to opposite logic values while the data input to the multiplexer are adjusted such that a logic value opposed to the correct one indeed appears at its output. 2. Bug propagation. The stimuli must permit the erroneous condition to propagate to observable nodes by causing a cascade of intermediate nodes to assume incorrect values. 3. Bug observation. The corrupted value observed on one or more of the output nodes must get checked against the logic value that a correct design is expected to produce. Unless all three conditions are met, the design flaw will have no consequence during simulation and testing, although circuits fabricated on the basis of the flawed HDL source code are almost certain to fail when put into service. \u0002 The same reasoning essentially applies to any bug that affects functional behavior. 5.3.2 FULLY AUTOMATED RESPONSE CHECKING IS A MUST In the context of simulation, bug observation implies checking the MUT output. How to do so is dictated by the volume of data. Even a fairly modest subcircuit asks for keeping track of hundreds of waveforms over thousands of clock cycles. Digging through waveform plots, event lists, tabular printouts of logic values, and similar records from simulation runs is not practical for efficiency reasons. It is also unacceptable from a quality point of view because some incorrect data value hiding within myriads of correct items is very likely to be overlooked. Observation 5.4. Purely visual inspection of simulation data is not acceptable in VLSI design. Rather, designers must arrange for the simulator software to automatically check the actual responses from the model under test against the correct ones and to report any differences. The answer is to collect the expected responses along with the pertaining stimuli in a sequence of data patterns that we call a test suite. In its most simple expression, a test suite is a set of binary vectors listed cycle by cycle that specifies what kind of responses a correct part is supposed to output when fed with certain stimuli.7 Test suite, test cases, and test vectors are often used as synonyms. The word testbench is also used in this context, but refers to a somewhat different concept. A testbench is a piece of software used to pilot a simulation run that applies stimuli, that acquires responses, and that compares the actual against the expected responses.8 A testbench is to a MUT 7 Mechanical engineers use gauges to verify the geometric conformity of manufactured parts such as to eliminate inaccurate copies before they are being put together with other components. In essence, gauges are specifications that have materialized. Similarly, a test suite serves to verify the functional correctness of some (sub)circuit. It might as well be called a functional gauge. 8 Put in IT terminology, a testbench is comparable to a software driver for some peripheral device whereas the test suite corresponds to a particular set of data. 5.3 PREPARING EFFECTIVE SIMULATION AND TEST VECTORS 309 in the simulation world what ATE is to a fabricated circuit in the physical reality. Testbench design will be the subject of section 5.5. Table 5.1 Terms used in the context of dynamic verification. In the physical reality world of simulation a design exists as fabricated circuit HDL model or netlist andisreferredtoas circuit under test (CUT). model under test (MUT). As part of prototype testing functional veriﬁcation all those stimuli and expected responses, collectively called test suite, get administered by autom. test equipment (ATE) a software testbench in search of design ﬂaws. As part of production testing fault simulation all those stimuli and expected responses, collectively called test patterns, get administered by autom. test equipment (ATE) a software testbench in search of fabrication defects. Toy example A test suite for a rising-edge-triggered Gray counter of word width w=4 with enable and asynchronous reset inputs is shown in fig.5.3. Note that it includes one stimulus/response pair per clock cycle and that both stimulus and response refer to the same cycle in each pair. How states are being encoded inside the MUT and whether the counter is actually implemented as a Medvedev or as a full Moore machine is of no importance for the input-to-output mapping.9 9 The difference is that a Medvedev machine has no output logic whereas a full Moore machine includes a non-trivial logic that translates each state into an output value. Refer to section B.1 for further explanations. 310 CHAPTER 5 FUNCTIONAL VERIFICATION Rst 0 1 2 3 4 5 6 7 8 9 . . k clock cycle 18 19 20 21 22 23 24 25 stimuli 0 1 0 1 1 1 1 1 1 . . 1 0 0 1 1 1 1 1 1 . . EnaRst 01 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 test suite4bit GraycounterCount[3:0] Clk Ena model under test eventually to be turned into silicon expected responses - 00 1 0 0100 Count[3:0] 0 0 0 0 010 1 .. 1 0 10 1 0 1 0 1 0 1 0 0 10 10 00 000 0 0 0 10 0 0 100 0 . .. ... 000 000 01 11 000 0000 01 --- FIGURE 5.3 A test suite for a 4 bit Gray counter with enable and asynchronous reset. \u0002 The test suite shown in fig.5.3 is very primitive and does not extend easily to more general situations. The concepts of test suite and testbench will thus have to be refined in numerous ways in 5.4 and later sections to make them more practically useful. Before doing so, we must address more pressing problems in sections 5.3.3 and 5.3.4. 5.3.3 ASSERTION-BASED VERIFICATION CHECKS FROM WITHIN Recall from fig.5.2 that uncovering a design flaw hidden deep in a circuit model solely via chip-level I/O signals is tedious and the outcome uncertain. Assertion-based verification comes to rescue by combining two ideas: 1. Rather than attempting to make a MUT observable via I/O signals, monitor what happens inside. After all, a simulator has access to all circuit nodes and is not limited to reading from package pins as the testing of physical parts is. 2. Instead of becoming obsessed with comparing responses, formulate functional requirements in terms of inputs, states, numerical values, outputs, or any combination thereof. A property specifies one aspect of how a specific subcircuit is supposed to behave, or conversely, how it must never behave independently of circuit context, mode of operation, and the like. A concurrent assertion is a small piece of HDL code, that gets invoked during simulation at user-defined moments of time to check whether a given property does hold or not. The outcome may generate a text message or just contribute to the statistics of a simulation run. The MUT’s functionality is not affected by the presence or absence of assertion statements. 5.3 PREPARING EFFECTIVE SIMULATION AND TEST VECTORS 311 Toy example revisited Assume our Gray counter is just a tiny subcircuit in a much larger design. As opposed to fig.5.3, many other subcircuits stand in the way between its own I/O terminals and the top-level I/O pins accessible from externally. Yet, whatever the context, any decent Gray counter must meet the unit- distance requirement “Unless the asynchronous reset is active or the counter is disabled, the next state must differ from the current state in exactly one bit position.” Listing 5.1 shows how this translates to SystemVerilog. LISTING 5.1 A Gray counter plus three assertions that report unexpected behavioral patterns during simulation. See listing 4.18 for the package referenced here. // Mission: Illustrate how to model a Medvedev machine in two processes. // Example designed to demonstrate assertion-based verification, the usage // of functions from a package, and the absence of any type conversions. // Functionality: w-bit Gray counter with enable and asynchronous reset. // Operates with double conversion: Gray->binary, increment, binary->Gray, // which is not necessarily the most economic nor the fastest solution. // Author: H.Kaeslin //----------------------------------------------------------------------------- ‘include \"../sourcecode/graydefs.sv\" // the word width to be used import grayconvPkg::*; // my own set of Gray code converter functions module graycnt ( input logic Clk_CI, Rst_RBI, Ena_SI, output logic [‘GRAYWIDTH-1:0] Count_DO ); // present state and next state logic [‘GRAYWIDTH-1:0] Count_DN, Count_DP; // computation of next state assign Count_DN = bin2gray(gray2bin(Count_DP) + 1); // updating of state always_ff @(posedge Clk_CI, negedge Rst_RBI) if ( Rst_RBI) // if active low Count_DP <= 0; // reset to all zeroes else // if rising clock if (Ena_SI) // and enable is 1 Count_DP <= Count_DN; // make next state the present one // assignment of state to output port 312 CHAPTER 5 FUNCTIONAL VERIFICATION assign Count_DO = Count_DP; // output assignment //------------------------------------------------------------------------- // set of assertions for addressing the various functional mechanisms // check asynchronous reset mechanism a_async_reset: assert property ( @(posedge Rst_RBI) Count_DO == ’{default:1’b0} ) else $error(\"Non-zero output immediately after removal of async. reset.\"); // or alternatively: $countones(Count_DO) == 0 ); // check enable/disable mechanism a_enable_disable: assert property ( @(posedge Clk_CI) disable iff ( Rst_RBI) Ena_SI |=> Count_DO == $past(Count_DO) ) else $error(\"Output has changed in spite of counting being disabled.\"); // check unit distance counting steps a_unit_distance_step: assert property ( // sampling time vvvvvvvvvvvvvv criterion vvvvvvvv of exclusion @(posedge Clk_CI) disable iff ( Rst_RBI) //precondition vvvvvv vvvvvvvvv property to be checked vvvvvvvvvv Ena_SI |=> $countones(Count_DO $past(Count_DO)) == 1 ) // optional pass action block (empty in this example) else // optional fail action block $error(\"Consecutive output patterns differ in 0 or more than 1 bit.\"); endmodule The unit-distance property is captured in the last of three assertions in this example, with the assert keyword preceded by a user-defined name. Labeling assertions is optional, but highly recommended. The interesting part follows on the next two lines of code. From an HDL point of view, a concurrent assertion is a passive process.10 As with any other process, all events that are to invoke the assertion are indicated in a sensitivity list. The subsequent disable iff (~Rst_RBI) clause precludes any action while the reset is active. The line below carries a precondition, the implication operator |=>,and the property to be checked. Operator |=> says that when the precondition to its left is met, then the property 10 That is, a process capable of producing a message and/or adding to the simulation report, but not of altering any variable’s (VHDL: signal’s) value. 5.3 PREPARING EFFECTIVE SIMULATION AND TEST VECTORS 313 to its right is to be evaluated on the next active clock event, which is perfect for a counter.11 $countones and $past are two system tasks with obvious meanings. Next come two optional action blocks separated by an else. The first is the pass statement, to be executed when the property is found to hold, the second is the fail statement, to be executed when the property is violated. A message string preceded by a severity level is standard. The severity level — one of $info, $warning, $error and $fatal — defines where to send the message and whether to proceed or to abort the current simulation run. In this example, the simulator is instructed to print the argument to the simulator window and to continue in case of a fail, while a pass will cause no further action. \u0002 It is a strong point of SystemVerilog that the language includes a comprehensive set of constructs, collectively known as SystemVerilog assertions (SVA), for expressing behavioral properties in a succinct, if somewhat cryptic way. The SVA syntax is so comprehensive, however, that the reader is referred to specialized texts such as [98] [103] for details. VHDL provides little more than the concurrent assertion statement. Luckily for VHDL aficionados, co-simulation makes it possible to combine VHDL circuit models with SystemVerilog properties, assertions, and testbenches. flawed design stimuli 0 instead of 1 actual response expected responses test suite ? = observe and compare 1 0 0 0 1 2 3 1 0 propagate the bugged reaction sensitize the bug 1 instead of 0 X Y Z 0 1 1 1 1 1 0 0 1 0 0 1 . . . . . . . . . . . . 1 . . B C A report unplanned condition two signals permuted by mistake FIGURE 5.4 Assertions resemble spies placed right into the MUT itself. In real life, a designer would hardly use an assertion to verify a behavior as trivial as an asynchronous reset. Listing 5.1 is just a toy example for illustrating the ideas behind assertion-based verification on a few lines of code. More realistic use cases follow. 11 An alternative operator |-> is available for situations where evaluation has to be immediate. 314 CHAPTER 5 FUNCTIONAL VERIFICATION • All data items entering a FIFO must eventually emanate one by one, that is in their initial order, with neither data lost nor with duplicates or bogus data added. • In a traffic light controller, no two conflicting lanes must ever see a green light at the same time, and each light must change colors according to the standard cycle. • State machines must not assume parasitic, illegal or otherwise suspect states. • Memory addresses, iteration counts, and most other variables are confined to a specific legal range and must never assume values outside that range. • Not all circuits are designed to handle all physically possible input patterns correctly. Just think of illegal instruction codes or unexpected status codes. Unforeseen inputs, symbols, or other out-of-the-ordinary conditions must be flagged should they ever occur. • Datapath operations must not result in numeric over-/underflows, out-of-range values, and other data scaling problems without the designer being alerted. • In a handshake protocol, data lines and the request and acknowledge signals have to switch in a well-defined order. Many properties relate to circuit behavior over time, i.e. to sequences of events or waveforms, if you prefer. Indeed, SVAs provide several constructs for expressing and manipulating sequences. Many assertions refer to the interface between a subcircuit and the embedding circuitry, and between a subprogram and the calling code, but they are not limited to this. Assertions can equally well be used to ascertain that the result of a numerical calculation is correct, or at least plausible and inoffensive. This can be achieved in either of two ways: a) Data produced by the MUT are cross checked against data computed by an equivalent but independently generated piece of code that is made part of the assertion. As an example, a first person may write a square root function for RTL synthesis, while a second person does the same for an assertion that double checks the output.12 b) The computation is reversed in the assertion. In our example, the procedure for computing the square root is fairly complex and thus prone to error, while checking the result is straightforward. Assertions included in simulation models are effective at providing protection against design and coding errors. They nicely complement response checking because • Feedback is immediate, see fig.5.4. There is no need for an abnormal condition to propagate to some distant node placed under constant monitoring. • The link from the manifestation of a problem to its cause is short. There is no need to trace back a mismatching output over thousands of cycles, HDL statements, or gates in an attempt to locate its place of origin. • Assertions are a lasting investment. There is no need to repeatedly adjust them when submodels are being assembled to form larger design entities. Especially the last feature is in sharp and welcome contrast to the stimulus/response pairs of a test suite. Observation 5.5. Understand assertions as executable comments that help making sure the MUT indeed operates as you intend and believe. Include an assertion into the HDL code wherever you explicitly or implicitly assume that a certain condition shall hold, or must never hold. 12 In software terminology, this is an example of N-version programming. 5.3 PREPARING EFFECTIVE SIMULATION AND TEST VECTORS 315 Properties — embedded in assertions — help to make sure synthesis code does what it is supposed to do during simulation. Properties can also contribute to formal verification as they provide extra details about how a design is thought to work and what properties are supposed to hold. Properties get ignored during synthesis, however, exactly as assert statements.13 For all enthusiasm, note that the effectiveness of assertions depends entirely on the sequence of stimuli applied to the MUT that carries them. Back to the FIFO example, the queue must be completely filled and emptied during simulation as the assertion statements otherwise get no chance to check the pointer arithmetics under diverse usage conditions. It is important to check how a full queue reacts to a write request, and an empty queue to a read. This observation brings us to an even more critical topic. 5.3.4 EXHAUSTIVE VERIFICATION REMAINS AN ELUSIVE GOAL An unfailing way — in fact the sole one — to safeguard against any possible design flaw is to verify a designs’s functional behavior in perfect detail.14 Let us see whether this is practical. Exhaustive verification calls for traversing all edges in the design’s state graph by exercising it with every possible input condition i ∈ I in every possible state s ∈ S.15 One might be tempted to think that the product |I||S| indicates the number of clock cycles cexh necessary for exhaustive verification. In almost all practical applications, traversing every edge once will necessitate traversing others twice or more, however,16 so that we must accept cexh ≥|I||S| (5.1) as a lower bound. Let wi, ws and wo denote the number of bits in the input, state, and output vector respectively. The maximum number of possible input symbols |I| then is 2wi which figure must be discounted by the parasitic — i.e. unused — input codes. An analogous reasoning holds for |S|. Although it is not possible to accurately state |I||S| in the general case, an upper bound can always be given as cexh ≥|I||S|≤ 2 wi+ws (5.2) where the “less or equal” operator holds with equality in the absence of parasitic states and input symbols, i.e. when any combination of bits is being used for encoding some legal state and input symbol respectively. 13 As an alternative, a designer is free to implement part of the sanity checks in his HDL code in RTL style such as to make them synthesize into surveillance circuitry. By having that extra circuitry activate an alarm upon detection of an out-of-the- ordinary condition, he can add self-checking capabilities to physical circuits. 14 Incidentally, note the same argument also applies to the testing of physical parts for fabrication defects. 15 Parallel edges are likely to exist, yet exhaustiveness indeed calls for checking the circuit’s behavior for every single edge, that is for every state/condition pair, unless the presence of Mealy-type outputs can be ruled out. 16 Fortunate exceptions are those cases where the state graph includes an Euler line. An (open) Euler line is a walk through a graph that runs through every edge exactly once. 316 CHAPTER 5 FUNCTIONAL VERIFICATION Let us plug in real figures to understand the practical significance. Consider the Intel 8080, an early microprocessor released in 1974 with an 8 bit datapath and almost trivial by today’s standards. Abstracting from further details we find the following word widths: input ports 8 bit data, 3 bit control, 1 bit reset wi = 12 registers 8 bit: A,B,C,D,E,H,L,IR; 16 bit: PC,SP; flags: 5 ws = 101 output ports 8 bit data, 16 bit address, 6 bit status/control wo = 30 Assume there are no parasitic states and input symbols. The minimum number of clock cycles required for exhaustive simulation then is 2113 ≈ 1034. Using test hardware running at 100 MHz, the process would run for more than 3 · 1018 years. Software simulation would take orders of magnitude longer. To our regret, we must conclude that Observation 5.6. Exhaustive verification is not practical, even for relatively modest functions. Dynamic verification, therefore, must almost always do with a partial set of test cases. The problem is to come up with a test suite of practical size and sufficient coverage. There is no cheap answer. We are thus going to discuss a number of more and less useful approaches to this problem that plagues both circuit simulation and IC testing. 5.3.5 DIRECTED VERIFICATION IS INDISPENSABLE BUT HAS ITS LIMITATIONS Exhaustive verification combines all data registers, controllers, counters, state machines, etc. into a single composite state. Also, each possible input gets aimlessly applied in each possible state, even if this contributes close to nothing towards uncovering more potential problems. The Cartesian product so obtained describes all situations the circuit might conceivably encounter but, at the same time, causes the number of test cases to explode. Testing distinct functional mechanisms separately As a way out, designers direct verification towards situations they deem critical. A pragmatic idea is to identify distinct subcircuits with fewer internal states and functional mechanisms, and to check each of them individually. In the occurrence of a CPU, this would cover circuit initialization, clock generation and distribution, the bus system including the switches involved in data and address routing, control registers, incrementing the program counter, instruction fetch and decoding, unconditional and conditional branching, the ALU in all modes of operation, status flags, accumulator and data registers, interrupt handling, stack operations, and more. Toy example revisited For simplicity, reconsider the Gray counter example. The test suite presented in fig.5.3 offers only partial coverage because it includes 26 cycles whereas (5.2) tells us 64 cycles is a lower bound for exhaustive verification.17 Why is this nevertheless a reasonable compromise between functional coverage and verification costs? 17 Finding the exact minimum is left to the reader as an exercise, see problem 1. 5.3 PREPARING EFFECTIVE SIMULATION AND TEST VECTORS 317 2 7 6 01398 54121315 14 10 11 Rst=0 parasitic state regular state enable/disable mechanism reset mechanism counting mechanism not exercised by test suite Rst=1 and Ena=0 Rst=1 and Ena=1 FIGURE 5.5 The state graph of a 4 bit Gray counter with edges colored according to the functional mechanism they implement. As observed in section 5.3.3, the desired functionality requires the working of three mechanisms, and our test suite addresses each of them separately. More specifically, the reset mechanism is being verified in cycles 0, 1 and 2, and the enable/disable mechanism in cycles 3, 4 and 5. The succession of output values is being checked against the 4 bit Gray code in cycles 4 through 20. Provided the mechanisms involved operate independently from each other, one can generalize from these partial checks and conclude that the circuit should indeed work as intended. All nodes (states) of fig.5.5 get visited, and five more vectors have been added for extra confidence. \u0002 Still, the separate testing of functional mechanisms suffers from several limitations. • The problem of combinatorial explosion persists as visiting all states rapidly becomes impractical on more substantial circuits. • One could easily come up with a modified design that allows for counting to get disabled in some of the states but not in others. Although this would clearly contravene the original specifications, the test suite of fig.5.3 would fail to uncover such a flaw. And postulating to traverse all edges (state transitions) brings us back to exhaustive verification. • Checking each mechanism and subcircuit separately holds the risk of missing problems that relate to the interaction of two (or more) of them. • Identifying subcircuits and functional mechanisms for verification requires insight into the MUT’s inner organization and operation.18 Observation 5.7. Any non-exhaustive simulation or test run is tantamount to spot checking and implies compromising between functional coverage, run time, and engineering effort. 18 A situation of limited knowledge is termed grey box probing as opposed to the black box approach of exhaustive verification that makes no assumptions about the MUT whatsoever. A situation that assumes perfect knowledge of a circuit’s inner details is referred to as clear box probing. The dilemma is this: Black box probing takes many vectors for a low probability of finding a problem. Clear box probing enables a test engineer to select test cases such as to address specific and likely problems, but may obstruct his view on other potential issues by contaminating his understanding with preconceptions from the circuit’s design phase. 318 CHAPTER 5 FUNCTIONAL VERIFICATION Ideally, an engineer would begin by enumerating all slips that might possibly occur during the design process before writing a test suite capable of sensitizing, propagating, and observing each of them. This is not possible in practice, though, because the number of potential design flaws is virtually unlimited and our imagination insufficient to list them all. Warning example A tiny portion from an incorrect ASIC design is shown in fig.5.6. The designer’s intention was to detect the zero state of a down counter by way of a 12-input nor function.19 Since no 12-input gate was available, he decided to compose the function from an 8-input and a 4-input nor gate, but mistakenly instantiated a nand gate during schematic entry. A simulation involving these four bits would have exposed the problem, but no such check was undertaken because the test suite never had the counter assume a state in excess of 18. Why did the designer refrain from exercising the upper bits? Firstly, he wanted to keep simulation runs short, and exhausting a 12 bit counter with enable and reset would have required 16 384 cycles. More importantly, however, the designer was convinced that all input bits to a zero detector are interchangeable. He concluded it was sufficient to check the subcircuit’s functioning by initializing the counter to a small number, such as 18, followed by counting backwards to zero. He was just not prepared for a problem that would challenge his preconceptions. Oup[11:0]Inp[11:0]12bitdown counter Lod Clk EnaOup[11:8]Oup[7:0] End this gate of wrong type should be NOR zero detector FIGURE 5.6 A silly little oversight that managed to slip through simulation unnoticed because of poor coverage. \u0002 What we learn from this example is that a critical difficulty of verification is to protect oneself against the unthinkable. Most examples of circuits and systems that have failed when put to service indeed confirm this. 19 Using the counter’s carry/borrow bit instead would probably have been a more economic choice anyway. 5.3 PREPARING EFFECTIVE SIMULATION AND TEST VECTORS 319 Monitoring toggle counts is of limited use A simple precautionary measure consists in collecting the toggle counts of all circuit nodes during simulation. Any node that never changes its logic value points to a weakness in the test suites chosen. In the example of fig.5.6, insisting on non-zero toggle counts would definitely have helped to recognize the stimuli as being inadequate. Yet, in spite of its utility and popularity, monitoring of node activities is far from solving all problems. A test suite may well toggle all nodes of a circuit netlist back and forth and still be insufficient, see problem 3. Also, the concept of a circuit node is meaningless before a gate-level netlist has been established. Observation 5.8. The toggling of all circuit nodes must be considered a desirable rather than a sufficient requirement for a good test suite. Automatic test pattern generation does not help either Automatic test pattern generation (ATPG) is a technique used in the context of sorting out defective ICs following manufacturing. It is important to understand that ATPG does not normally help to uncover design flaws in nonmaterial circuit models such as HDL code or gate-level netlists. This is because ATPG starts from a presumably correct netlist and produces a set of test patterns for checking for the presence of predefined faults. Fabrication defects are almost universally assumed to follow the so-called “single stuck-at fault” model whereby one circuit node at a time is assumed to be shorted to either logic 1 or 0 for that purpose. Functional verification, in contrast, questions the correctness of a circuit model by setting its logic behavior against some kind of functional specification or reference model. Monitoring code coverage helps but does not sufﬁce All decent HDL simulators can calculate code coverage figures by keeping track of how many times the individual statements in a MUT’s source code are being exercised during a simulation run. 100% code coverage implies all executable statements have been executed once or more. Observation 5.9. However useful code coverage figures are, executing all statements in an RTL or behavioral circuit model does neither imply that all states and transitions have been traversed nor that all conditions and subconditions for doing so have been checked. Also, code coverage relates to bug sensitization but neither to bug propagation nor to bug observation. Executing a flawed statement does not imply the bug must necessarily become manifest at an observable output. The subsequent case study shows that functional coverage problems can take on much more subtle forms. Warning example Electronic dimmers for incandescent lamps work by varying the duty cycle of the load current. For every half wave of the 50 or 60 Hz mains voltage, a Triac connected in series with the lamp is turned on (“fired”) at a phase angle adjustable between 0◦ and 180◦ and stays in the conducting state until the next zero crossing. A digital implementation is shown in fig.5.7. 320 CHAPTER 5 FUNCTIONAL VERIFICATION The controller accepts commands from a touch key, converts the desired luminosity into a target phase angle, and fires the Triac via an optical coupler. The trigger impulse is initiated by a comparator when the actual phase angle matches the target value. The actual angle counter is clocked at 64 times the mains frequency so that a total of 32 intensity levels are available. Synchronization with the mains is obtained through a zero-crossing detector that resets all 5 bits of the counter whenever a new half wave begins. Post-layout simulations and testing of fabricated samples on ATE confirmed circuit operation. Yet, the design of fig.5.7 is flawed. When the first prototype was plugged into the target board, the dimmer was found to function o.k. except for a slight but disturbing oscillation of luminous intensity. The problem was quickly located in the synchronization mechanism. Since only the actual angle counter is reset, the clock divider proceeds from its current but otherwise indeterminate state whenever a new half wave begins. As a consequence, the next increment impulse for the actual angle counter can arrive anytime between a zero-crossing and 1 32 half waves later. This, together with the fact that a free-running clock oscillator is being used, leads to a beat in firing angle and luminosity. Why had this flaw passed unnoticed during circuit simulation and testing? The answer is that all simulations were carried out with the clock frequency an integer multiple of the mains frequency. It just never had occurred to the designers that non-integer frequency ratios might give rise to specific behavioral phenomena. \u0002 comparator zero-crossing detector clock generator clock divider mixed-signal ASIC driver touch key phase angle lookup table command interpreter interface and debouncer target angle register actual angle counter voltage attenuator i(t) mains 230V AC u(t) firing impulses TRIAC FIGURE 5.7 Block diagram of ﬂawed digital dimmer. The above examples have demonstrated the perils of unconscious, unspoken or unjustified assumptions and misconceptions that underlie directed verification. Independently of whether the desired function- ality is embodied in a piece of hardware, of software, or both, the inconvenient truth is always the same, and we are now in a position to state it. Observation 5.10. The innate difficulty with selecting critical test cases for dynamic verification is that human beings can prepare only for events they can foresee. 5.3 PREPARING EFFECTIVE SIMULATION AND TEST VECTORS 321 Routine is the dark side of experience On this background, it is sometimes suggested that doing the functional verification right in the target environment will solve all problems. The examples below show this is not so. Warning example On its maiden flight on June 4, 1996, the Ariane 5 rocket had to be neutralized by its built-in self- destruct system at an altitude of 3500 m because excessive aerodynamic loads had ripped the solid boosters off the rocket shell after more than 30 s of seemingly normal flight [141]. Analysis of telemetry data revealed that there had been no structural failure but that the on-board computer had commanded the booster nozzles to maximum deflection two seconds before self-destruction occurred, and had so steered the rocket into an abnormal angle of attack. Why did this happen? The flight control system of Ariane 5 depends on two computerized inertial reference platforms, one active and one for backup, that provide the on-board computer with velocity and attitude information. This flight control system was a proven design that had flown with Ariane 4 for years. On that fatal morning, however, both inertial reference platforms simultaneously ceased to deliver meaningful flight data and presented the on-board computer with diagnostic bit patterns instead. Misinterpreted as they got, these garbage data caused the on-board computer to initiate a sharp change in trajectory. The underlying reason was a numeric overflow that occurred when a parameter of minor importance was converted from a 64 bit floating point number to a 16 bit signed integer combined with poor exception handling. How was it possible for such a disastrous design flaw to remain undiscovered for so long? No numeric overflow had ever occurred in an Ariane 4 flight. Yet, the Ariane 5 trajectory implied considerably higher horizontal velocity values which caused the critical parameter to accumulate beyond its habitual range. Tests for making sure that the navigational system would operate as intended in the new context had not been conducted. It was precisely the system’s excellent reliability record that led the Ariane 5 design team to believe that everything worked fine and that no extra qualification steps were necessary. \u0002 How a system responds to the things going wrong shows how good it really is. (Hans Stork, CTO of Texas Instruments) The fact that a subsystem has performed as expected when subject to certain data sets, operating conditions, parameter configurations, and the like is no guarantee for its correct functioning in similar situations. Even real-world data sometimes prove too forgiving It is often argued that the functional coverage problem is best dealt with by using genuine data collected from real-world service instead of limiting dynamic verification to a small number of artificially prepared test suites. A test with data from the anticipated flight time sequence of Ariane 5 injected into the data processing section of the inertial reference system would indeed have disclosed its fatal limitation. Similarly, a soft- or a hardware prototype of the digital dimmer ASIC embedded within the remainder of the circuitry and operated with real-world waveforms would have led designers to recognize the oversight in their design. 322 CHAPTER 5 FUNCTIONAL VERIFICATION Using actual data material is no panacea, however, because it may take an excessively large number of cycles before genuine stimuli activate some rare but critical set of circumstances where a potential misbehavior of a MUT actually becomes apparent. Warning example A case that was given world-wide publicity in the fall of 1994 was the flaw in the floating point division unit of early Pentium microprocessors [142] [143]. Due to a software problem, 5 out of 1066 table entries had been omitted from a PLA lookup-table employed in the radix-4 SRT division algorithm.20 Whether results from floating point division came out wrong or not depended on the mantissa values involved. Intel scientists estimated the fraction of the total input number space that is prone to failure to be 1.14 · 10−10 [144] which explains why it took several months before the user community eventually became aware of the problem. \u0002 We are now fully aware of the difficulties behind simulation and testing. The trouble is indeed to compose a compact set of test cases that makes a flawed design behave differently from a functionally correct one for any situation that might be relevant to the final circuit’s operation. The remainder of this section will introduce helpful techniques for that. 5.3.6 DIRECTED RANDOM VERIFICATION GUARDS AGAINST HUMAN OMISSIONS Random verification attempts to get rid of any human pre- and misconceptions that might compromise directed verification by having a random process select the test cases. Example The classical checkerboard test for memory circuits writes alternating bit patterns into adjacent memory locations and checks them upon readout. Due to the pattern’s simple and periodic nature, this does not protect against all possible design flaws and fabrication defects, however. As a consequence, it makes sense to introduce an irregular element into the procedure. \u0002 Purely random patterns are not normally the best choice, however. In the memory example, you want to make sure every location gets tested with a 0 and a 1 without having to simulate for an undetermined period of time. And in order to efficiently exercise a microprocessor, it is much more effective to randomly pick from legal opcodes and to combine them with relatively few selected data sets rather than to indiscriminately load its memory with arbitrary bit patterns. Generally speaking, you want to cover as much of the circuit’s state space as possible with just a limited number of test vectors. Observation 5.11. While random testing can be more effective at uncovering design flaws than human- selected test cases — particularly when combined with assertion-based verification and statistical coverage analysis — pattern generation needs to be biased for efficiency. 20 SRT stands for Sweeney, Robertson and Tocher who independently had invented the method in the 1950s. 5.3 PREPARING EFFECTIVE SIMULATION AND TEST VECTORS 323 Toy example revisited The idea behind directed random testing is shown in the SystemVerilog testbench of listing 5.2. LISTING 5.2 A testbench that applies directed random testing to a Gray counter. // Mission: Provide a basic example of directed random verification. // Functionality: w-bit Gray counter with enable and asynchronous reset. // Author: M. Schaffner, H. Kaeslin, B. Muheim //----------------------------------------------------------------------------- ‘include \"../sourcecode/graydefs.sv\" // the word width to be used module graycnt_tb; //------------------------------------------------------------------------- // declarations // set time unit and precision (could be overridden by the simulator) timeunit 1ns; timeprecision 1ps; // set parameters related to clocking and timing parameter NCLKS = 500; // number of clock cycles parameter CLK_PHASE_HI = 50ns; // clock high time parameter CLK_PHASE_LO = 50ns; // clock low time parameter STIM_APP_DEL = 10ns; // stimuli application delay parameter RESP_ACQ_DEL = 90ns; // response aquisition delay // declare signals logic Clk_C, Rst_RB, Ena_S; // connecting to MUT inputs logic [‘GRAYWIDTH-1:0] Count_D; // connecting to MUT output logic EndOfSim_S; // for coordination within testbench // the two tasks below ensure proper timing at the DUT’s interface // they first wait for a number of active clock edges given as argument // and then wait for application or aquisition delay respectively task appl_wcycles(int unsigned n); repeat (n) @(posedge(Clk_C)); #(STIM_APP_DEL); endtask task acq_wcycles(int unsigned n); repeat (n) @(posedge(Clk_C)); #(RESP_ACQ_DEL); endtask 324 CHAPTER 5 FUNCTIONAL VERIFICATION //------------------------------------------------------------------------- // generate clock signal always @* begin do begin Clk_C = 1; #(CLK_PHASE_HI); Clk_C = 0; #(CLK_PHASE_LO); end while (EndOfSim_S == 1’b0); end //------------------------------------------------------------------------- // instantiate MUT graycnt MUT (.Clk_CI(Clk_C), // note: the IO suffixes .Rst_RBI(Rst_RB), // stand in the way of .Ena_SI(Ena_S), // dot name or dot star .Count_DO(Count_D)); // port connections here //------------------------------------------------------------------------- // stimuli application process initial // runs just once begin : stimuli_application_p // declare local variables int randVal; bit ok; // auxiliary variable for randomize with method // initialize testbench signal EndOfSim_S = 1’b0; // initialize signals connecting to MUT appl_wcycles(1); // proceed to next application time Ena_S = 1’b0; Rst_RB = 1’b0; // initialize MUT by applying and removing asynchronous reset appl_wcycles(1); // proceed to next application time Rst_RB <= 1’b1; $display(\"circuit initialized, stimuli application starts\"); 5.3 PREPARING EFFECTIVE SIMULATION AND TEST VECTORS 325 repeat(NCLKS) begin appl_wcycles(1); // proceed to next application time //random number vvvvvvv vvvvvvv constraints vvvvvvv ok = randomize (randVal) with { randVal >= 0; randVal < 10; }; // alert designer should constraints turn out to be infeasible assert (ok) else $error(\"randomization failed\"); // disable counting in 1 out of 10 cases Ena_S <= (randVal == 0) ? 1’b0 : 1’b1; // reset count to zero in 1 out of 50 cases ok = randomize (randVal) with { randVal >= 0; randVal < 50; }; assert (ok) else $error(\"randomization failed\"); Rst_RB <= (randVal == 0) ? 1’b0 : 1’b1; end $display(\"stimuli application finished\"); EndOfSim_S = 1’b1; end //------------------------------------------------------------------------- // response acquisition process initial // runs just once begin : response_acquisition_p // declare local variables int tstCnt; // test cycles carried out int errCnt; // mismatching responses found logic [‘GRAYWIDTH-1:0] binCnt; // enabled steps since last reset logic [‘GRAYWIDTH-1:0] expResp; // expected response // initialize local variables tstCnt = 0; errCnt = 0; binCnt = ’{default:1’b0}; expResp = ’{default:1’b0}; $display(\"response acquisition started\"); 326 CHAPTER 5 FUNCTIONAL VERIFICATION do begin acq_wcycles(1); // proceed to next acquisition time if (Rst_RB == 1’b0) // reset may have been asserted binCnt = 0; // calculate the expected response by converting the number of steps // to Gray code using a piece of code independent of the MUT expResp = (binCnt >> 1) binCnt; $display(\"[test %04d]> checking whether expected ’b%b = actual ’b%b\", tstCnt,expResp,Count_D); // check response and do the reporting and accounting if ( Count_D !== expResp ) begin $error(\"expected ’b%b != actual ’b%b\",expResp,Count_D); errCnt ++; // increment incorrect responses counter end // update the number of enabled steps since last reset if (Rst_RB == 1’b0) binCnt = 0; else if (Ena_S == 1’b1) binCnt++; tstCnt++; // increment test cycle counter end while (EndOfSim_S==0); $display(\"response acquisition stopped (%4d test cycles, %4d failed)\", tstCnt,errCnt); end endmodule Instead of applying a predetermined sequence of stimuli to the MUT, the two inputs Rst_RB and Ena_S are fed with random data in a statistically controlled way. The key construct is randomize (...) with {...} used here to generate random integers constrained to an interval as indicated between the curly brackets. These numbers subsequently decide on whether to assign 0 or 1 to Rst_RB and Ena_S.A reset is specified to occur with probability of 2%, a disable with 10%. \u0002 5.3 PREPARING EFFECTIVE SIMULATION AND TEST VECTORS 327 5.3.7 STATISTICAL COVERAGE ANALYSIS PRODUCES MEANINGFUL METRICS In the code example of listing 5.2, simulation ends after a predetermined number of clock cycles. To what extent the MUT’s specifications have been exercised at that point remains an open question. The answer lies in collecting statistical data during simulation. SystemVerilog users benefit from built-in constructs for doing so. Example When added to the RTL code of listing 4.20, the lines below will count how often each state gets visited, how often each input gets applied, and — most importantly — how many times each state transition gets traversed. The numbers will be reported at the end of each simulation run, they can also be combined with predefined goals to decide when to put and end to simulation. ..... // coverage checks // ------------------------------------------------------------------------ covergroup mealy5stCg @(posedge Clk_CI); // name and when to sample state : coverpoint State_DP; // monitor the FSM state inp : coverpoint Inp_DI { // monitor the FSM input bins inp[] = { ’b00,’b01,’b10 }; // declare all its legal values } stateXinp : cross state, inp; // monitor cross coverage of the two endgroup // create an instance of the covergroup, allocating the memory required mealy5stCg mealy5stCgInst = new; ..... Note: Multiple instances of a covergroup would be required if we were to monitor multiple instances of the same (sub)circuit. \u0002 Much as assertions, statistical coverage analysis monitors what happens inside a circuit model without having to propagate any data to an observable output. And as opposed to code coverage, it enables designers to specify what they consider relevant functional features and meaningful coverage metrics. One postulate when designing a CPU, for instance, will be that each opcode involving a memory transfer shall be combined with each addressing mode, and this several times. You would further want to make sure that various ranges of memory addresses get exercised during simulation. The higher quality and confidence in the end product justifies the one-time effort for selecting and coding the various coverage points. 328 CHAPTER 5 FUNCTIONAL VERIFICATION As covergroup, coverpoint, bins, cross, and related SystemVerilog constructs come with so many options, syntax details must be obtained from specialized texts such as [98] [103]. Similarly to what has been found for assertion-based verification, VHDL per se does not offer as much support for coverage analysis and directed random stimuli generation. The IEEE 1076.2 VHDL extension includes a pseudo random number generator procedure named ieee.math_real.Uniform from which test patterns with desirable probability distributions must be obtained by way of biasing and/or filtering operations. VHDL packages and advice are available from an open source initiative named OS-VVM [145]. And again, co-simulation makes it possible to combine the exactingness of VHDL for circuit modeling with the sophisticated capabilities of SystemVerilog for verification. 5.3.8 COLLECTING TEST CASES FROM MULTIPLE SOURCES HELPS We have found that it is absolutely essential to organize functional verification with a clear and open mind. The fact that today’s VLSI circuits are so complex means that no single verification technique will suffice to uncover almost all potential design flaws, let alone to do so with a test suite of acceptable size. The conclusion is most obvious. Observation 5.12. Except for the simplest subsystems where exhaustive testing is feasible, an accept- able selection of test cases shall comprise: 1. A vast set of data that makes the MUT work in all regular regimes of operation, that exercises every functional mechanism, and that strains array and memory bounds. 2. Particular numeric data that are likely to cause uncommon arithmetic conditions including over-/underflow, division by small numbers incl. zero, sign reversal, carry, borrow, and NaN.21 3. Pathological cases that ask for exception handling and out-of-the-normal control flows, 4. Genuine data sequences collected from real-world service. 5. Properly biased random test cases in conjunction with coverage analysis. 21 NaN stands for not a number, i.e. for a binary code that does not map to any numerical value. 5.3 PREPARING EFFECTIVE SIMULATION AND TEST VECTORS 329 5.3.9 SEPARATING TEST DEVELOPMENT FROM CIRCUIT DESIGN HELPS The idea is to safeguard a design against oversights, misconceptions, and poor functional coverage by organizing manpower into two independent teams. A first team or person works towards designing the circuit while a second one prepares the properties, the assertions, and the test suite. Their respective interpretations are then crosschecked by simulating early behavioral models of the circuit to be, see fig.5.8. The goal of the circuit designers is to come up with a model that is functionally correct whereas the test engineers essentially try to prove them wrong. The pros and cons of this healthy adversarial relationship are as follows. + Mistakes made by either team are likely to be uncovered in the crosschecking phase. + The same holds for ambiguities in the initial specifications. + Having the design and the test teams work concurrently helps to cut down design time. − A chance always remains that misleading specifications get interpreted in identical but mistaken ways by both teams. − The difficulty of finding test cases of adequate coverage persists. SR write circuit model select test cases and determine supposedly correct responses B M E E M EB E model compilation B M E crosschecking by behavioral simulation SR E M data transport human effort circuit modelM behavioral languageB executable code selected stimuli expected responses E S R assertions and coverage points E EB E BE verification code compilation of adjust until functionally consistent specifications design team verification team verification aids stipulate properties and write assertions for checking them write coverage points for statistical coverage analysis FIGURE 5.8 HDL synthesis models and veriﬁcation aids being prepared by separate teams. 330 CHAPTER 5 FUNCTIONAL VERIFICATION genuine input data genuine output data circuit model I O M timing informationT physical circuitC test vectorsV layout dataL netlist, structural language behavioral language N B executable code selected stimuli expected responses E S R data transport human effortphysical circuits e l e c t r o n i c d e s i g n a u t o m a t i o n s o f t w a r ecycle-true but delayless synthesis modelssimulation models with estimated timing of increasing accuracyback-end designtestingf r o n t - e n d d e s i g nservicehardwaretest equipm.objecttoolsactionlargely algorithmic models refine until functionally correct refine until functionally correct and of acceptable performance, size and power EB E E model compilation B M E M E RTL simulation crosschecking by E MSR N M NB E E and library mapping HDL synthesis, logic optimization, B M N E E gate-level simulation SR N M prototype fabrication LLN E E place, route, and chip assembly N M timing verification TTN E E N M C circuit under test SRC I O circuit in actual use C NL E E netlist extraction N M L B M B M E M EB E E model compilation adjust until functionally consistent write RTL model design architecture, B M SR E crosschecking by behavioral simulation E MSR specifications write behavioral model select test cases and determine supposedly correct answers reused over and again FIGURE 5.9 Life cycle of a test suite during VLSI design and test in T-diagram notation. 5.4 CONSISTENCY AND EFFICIENCY CONSIDERATIONS 331 5.4 CONSISTENCY AND EFFICIENCY CONSIDERATIONS Over the last decades, telling correct designs and circuits from imperfect ones has become ever more onerous with the exploding VLSI and ULSI circuit complexities. The concern is best expressed in a quote (by Walden Rhines): The question is whether the percentage for verification time tops out at 70% (of the total engineering effort in VLSI design) or it goes to 95% in the future. Industry cannot afford to rewrite a test suite gathered with so much effort over and over again as a design matures from a virtual prototype into synthesis code, a gate-level netlist, and — finally — into a physical part. Moreover, it is absolutely essential that a MUT be checked against the same specifications throughout the entire design cycle. Fig.5.9 emphasizes the reuse of stimuli and expected responses during a VLSI design and test cycle. A problem is that the various design views and tools greatly differ in their underlying assumptions, see table 5.2. Table 5.2 Data, signal, and timing abstractions encountered during VLSI design and test. Relevant Numerical Modeling Relevant Timewise Level of data types precision of electrical time scale resolution abstraction and structures phenomena for latency Immaterial circuit models Algorithmic model abstract and essentially not an issue system-level not an issue fairly free unlimited transaction Automata theory discr. symbols not an issue not an issue abstr. cycle clock cycle Synthesis model numbers, bits, ﬁnite optional clock cycle event (HDL @ RTL) enumer. types sequence Gate-level netlist bits ﬁnite logic values clock cycle circuit Post-layout netlist delays Physical hardware Autom. test equip. bits ﬁnite discr. volt. clock cycle discr. strobes Physical circuit cont. quant. continuous The difficulty is to make sure the same stimuli and expected responses can be reused across the entire design cycle from purely algorithmic specifications to the testing of fabricated parts in spite of the many pieces of hardware and software being involved in the process. Note this is more than just a matter of format conversion. Ideally, a single test suite is reused with only minimal modifications to account for unavoidable differences in timewise and numerical resolution. Having to rewrite or to re- schedule test patterns at each development step must be avoided for reasons of quality, trustworthiness, and engineering productivity. 332 CHAPTER 5 FUNCTIONAL VERIFICATION From a hardware engineering point of view, a good simulation set-up • Is compatible with all formalisms and tools used during VLSI specification, design and test (such as automata theory, Matlab, HDLs, logic simulators, and ATE). • Translates stimuli and responses from bit-level manipulations to higher-level operations, • Consolidates simulation results such as to facilitate interpretation by humans. • Is capable of handling situations where the timewise relationship between circuit input and output is unknown or difficult to predict. • Manages with reasonable run times. • Obeys good software engineering practices (modular design, data abstraction, reuse, etc.). We are now going to discuss a couple of measures that greatly contribute towards these goals. 5.4.1 A COHERENT SCHEDULE FOR SIMULATION AND TEST A choice of utmost importance refers to the relative timing of a few key events that repeat in every stimulus/response cycle. Poor timing may cause a gate-level model to report hundreds of hold time violations per clock cycle during a simulation run, for instance, whereas a purely algorithmic model is simply not concerned with physical time. To complicate things further, engineers are often required to co-simulate an extracted gate-level netlist for one circuit block with a delayless model for some other part of the same design. Observation 5.13. To be useful for comparing circuit models across multiple levels of abstraction, a testbench must schedule all major events such as to respect the limitations imposed by all formalisms and tools involved in circuit specification, design, simulation, and test combined. Formalisms and tools are meant to include automata theory, HDLs, RTL models, gate-level netlists (whether delayless or backannotated with timing data), simulation software, and automated test equipment (ATE). In their choice of a schedule, many circuit designers and test engineers tend to be misled by the specific idiosyncrasies of one such instrument. Key events Consider a synchronous digital design22 and note that a few events repeat in every clock cycle. These key events are the same for both simulation and test, namely: • The application of a new stimulus denoted as △ (for Application), • The acquisition and evaluation of the response denoted as ⊤ (for Test) , • The recording of a stimulus/response pair for further use denoted as \u0002 (for storage), • The active clock edge symbolically denoted as ↑, and, • The passive clock edge denoted as ↓ (mandatory but of subordinate significance). When these events are ordered in an ill-advised way, the resulting schedules most often turn out to be incompatible. Exchanging test suites between software simulation and hardware testing then becomes 22 We assume the popular single-phase edge-triggered clocking discipline where the is no difference between clock cycle and computation period, see section 7.2.2 for details. 5.4 CONSISTENCY AND EFFICIENCY CONSIDERATIONS 333 very painful, if not impossible. The existence of a problem is most evident when supposedly identical simulation runs must be repeated many times over, fiddling around with the order and timing of these events just to make the schedule compatible with the automated test equipment (ATE) at hand. A suspicion always remains that such belated manipulations of test vectors cannot be trusted because any change to the sequence of events or to their timing raises the question of whether the original and the modified simulation runs are equivalent, that is, whether they are indeed capable of uncovering exactly the same set of functional flaws. A coherent stimulus/response schedule The schedule of fig.5.10 has been found to be portable across the entire VLSI design and test cycle. Its formal derivation is postponed to section 5.9. Observation 5.14. At the RTL and lower levels, any simulation set-up shall • provide a clock signal even if the MUT is of purely combinational nature, • log one stimulus/response pair per clock cycle, and • have all clock edges, all stimulus applications, and all response acquisitions occur in a strictly periodic fashion, symbolically denoted as △↓ (⊤= \u0002) ↑ . simulation time via output function via state transition function cause observable effect observable effect i(k) o(k) i(k+1) o(k+1) s(k) s(k+1) i(k+2) cycle k with its vector set k+1cycle with its vector set k+2cyclek −1cycle clock signal CLK o(k −1) FIGURE 5.10 A coherent schedule for simulation and test events with cause/effect relationships and single-phase edge- triggered clock signal waveform superimposed. As shown in fig.5.10, each computation period gets subdivided into four phases. Sample settings for a symmetric clock of a conservative 10 MHz are given below as an example. Of course, the numerical figures must be adapted to the situation at hand. 334 CHAPTER 5 FUNCTIONAL VERIFICATION cycle event with time of occurrence [ns] k ↓ = ↑ 0 10 50 90 100 1 110 150 190 200 2 210 250 290 300 ... ... ... ... ... Observe that we have elected to assign the active clock edge ↑ to the end, rather than to the beginning, of a cycle because a finite state machine takes up operation from some state — typically a well defined start state s0 — without any intervention of a clock. Further make sure to understand that a clock is needed to drive a simulation even if the circuit being modeled is of purely combinational nature. Cause/effect relationships It is important to know when and how the effects of a specific stimulus i(k) become observable. Note from fig.5.10 that o(k) = g(i(k), s(k)) is visible in the response acquired after applying i(k), while its effect on the state s(k) can be observed only in the response acquired one clock cycle later and only indirectly as o(k + 1) = g(i(k + 1), f (i(k), s(k))). All simulation set-ups that we are going to develop next adhere to this schedule, but they differ in how the bit patterns observed at the I/O pins of the MUT relate to the stimuli and expected responses that make up for the test suite. In the most simple case, illustrated in fig.5.11, both are the same and they are firmly locked to predetermined clock cycles. This is fully adequate for subcircuits of modest complexity, such as the one of fig.5.3, for instance, and also the only way to go when it comes to the testing of physical parts using automated test equipment (ATE). HDL model under testappli actual resp report A Eacqui = ? golden model stimuli generator stimuli expect resp format- and cycle-true runs once runs many times FIGURE 5.11 Elementary simulation set-up (simpliﬁed).23 23 Note that stimuli and expected responses are prepared beforehand and stored on disk from where they are retrieved at run time under control of the testbench, hence the name file-based simulation. This is generally preferred over the alternative, shown in fig.5.16b, of (re-)calculating those responses at run time with the aid of a golden model because of the computing resources wasted when many iterations are required to debug a MUT. 5.4 CONSISTENCY AND EFFICIENCY CONSIDERATIONS 335 5.4.2 PROTOCOL ADAPTERS HELP RECONCILE DIFFERENT VIEWS ON DATA AND LATENCY As MUTs grow more complex, limitations of bitwise cycle-true simulations become apparent. • Humans are easily overwhelmed by the volume of bits and bytes when confronted with raw simulation data. • I/O operations that extend over multiple clock cycles and other sophisticated transfer protocols tend to get in the way of transparency. • MUTs undergo profound changes during the development process and most architectural decisions have a dramatic impact on latency.24 What begins as a purely behavioral model is later refined into an RTL model, and ultimately becomes a gate-level netlist. Only the final model will match the physical circuit it emulates in terms of bits and clock cycles. • A golden model — and hence also the expected responses prepared using it — might not be absolutely bit-true. Many functional models do in fact ignore circuit-level details such as clock, reset, scan mode, and other test-related features. Put in other words, a semantic gap opens between the needs of overall functional verification and the reality of circuit operation. Rather than drowning engineers with tons of 0sand 1s ascribed to specific clock cycles, advanced test suites are made to express things at more meaningful levels of detail. In addition, any decent simulation set-up must support a process of incremental design where latency is subject to change several times. Example JPEG image compression in essence accepts an image frame, subdivides it into square blocks, and uses a 2D Discrete Cosine Transform (DCT) to calculate a set of spectral coefficients for each block. Those coefficients are then quantized or outright replaced by zero when their impact on the perceived image quality is only minor. For one thing, you would prefer to talk of larger data items such as image frames, blocks, and coefficient sets in the context of functional verification. A relevant operation would be the compression of one block or, alternatively, of an entire frame. Low-level details such as the reading in of pixels or the toggling of individual data bits would just distract your attention. You would not want to work at those lower levels unless forced to do so by adverse conditions. For the other thing, JPEG decoding is a combinational function that can, in theory, be computed by a delayless model, that is, with latency zero. In practice, however, typical image decoding hardware ingests one set of quantized DCT coefficients at a time and takes many clock cycles before spitting out the pixels of a block or of the assembled image, with the exact latency figure being a function of architectural decisions. \u0002 24 Just think of iterative decomposition, pipelining, loop unfolding, etc. The same applies to interfacing with RAMs, parallel ↔ serial conversions, specific input/output protocols, and the like. 336 CHAPTER 5 FUNCTIONAL VERIFICATION Observation 5.15. A testbench not only serves to drive the MUT, its more noble duties are to translate stimuli and responses across levels of abstraction, and to consolidate simulation results such as to render interpretation by humans as convenient as possible. Functional verification is all about data items being absorbed, digested and delivered. The chores of reformatting them and of applying/accepting them in a timely fashion must be carried out with a minimum of human attention. Test engineers feel little inclination to re-adjust their models and test suites when one component gets re-architected or clocked in a different way. Protocol adapters aggregate bits into more meaningful data bodies Stimuli and responses are lumped into composite data entries such as records, data packets, audio fragments, or whatever is most appropriate for the application at hand. Protocol adapters, aka bus- functional models (BFM), are inserted between test suite and MUT, see fig.5.13. An input protocol adapter accepts a high-level stimulus (an image frame in the JPEG example), breaks it down into smaller data items (e.g. blocks and pixels), and feeds those to the MUT word by word or bit by bit over a time span that may cover hundreds of clock cycles, see fig.5.12. Another adapter located downstream of the MUT does the opposite to consolidate output bits into a higher-level response (e.g. collecting bits into a JPEG coefficient set for one image). simulation time clock time zero end of time 1st stimulus-response cycle 2nd stimulus-response cycle nth stimulus-response cycle clock cycle protocol adapters consolidatedispense trans- actions simulation driven by frame pixels relevant data items (example) FIGURE 5.12 Protocol adapters help bridge the gap between bit-level models (RTL or gate-level netlist) and a higher level view of the same. 5.4 CONSISTENCY AND EFFICIENCY CONSIDERATIONS 337 HDL model under testappli actual resp report A Eacqui = ? expect respadapteradapter golden model stimuli generator stimuli not normally format- or cycle-true runs once runs many times start complete FIGURE 5.13 Simulation set-up with protocol adapters for hiding low-level details (simpliﬁed). Protocol adapters absorb latency changes across multiple circuit models The upstream adapter is designed such as to feed data to the MUT just in time and, analogously, the downstream adapter such as to wait for the next valid data item to emerge at the MUT’s output. The details depend on the MUT’s I/O interface (which functionally follows the specifications for the target circuit). ◦ The chip-to-be supports handshaking on its I/O ports, see the shaded part in fig.5.14. Each protocol adapter features the necessary control signals on the side of the MUT and participates in the handshake transfers there. ◦ The MUT’s interface includes some form of “start” signal and a status flag such as “ready/busy” that the protocol adapters drive and interprete respectively. This situation is depicted in the shaded part of fig.5.13. ◦ In the absence of any of the above provisions the protocol adapters must tacitly count clock cycles concurrently to the MUT’s internal operation and/or emulate state machines that are part of the MUT to find out when to act.25 In conclusion, the adoption of protocol adapters minimizes the need to rework stimulus/response pairs each time a modification is made to the MUT. Any change just affects the MUT itself and one or two of the adapters but neither the remainder of the testbench nor the test suite itself thereby ensuring consistency, simplifying maintenance, and improving designer productivity. Full handshaking is the cleanest and safest way to relegate clocking details and latency figures to secondary issues as simulation proceeds in an entirely data-driven manner. 25 While this may be ok for small subcircuits, we do not recommend to design larger entities in this way because the resulting circuits prove difficult to embed in a larger system. Besides, any substantial change to the MUT’s architecture is likely to necessitate adjustments to the latency parameters coded into the protocol adapters. Things become really awkward when the number of clock cycles required to complete a computation depend on numerical data values. Target specifications should be reworked in such cases. 338 CHAPTER 5 FUNCTIONAL VERIFICATION Protocol adapters can either be implemented as HDL processes of their own or merged as subfunctions into the testbench processes that handle stimulus application and response acquisition respectively. The choice is essentially a matter of software engineering. HDL appli actual resp report A Eacqui = ? expect respadapteradapter golden model stimuli generator stimuli high-level rating figs. of merit runs once runs many times model under test not normally format- or cycle-true ☞ ☞ ☞ ☞ FIGURE 5.14 Simulation set-up that calculates high-level ﬁgures of merit (simpliﬁed). 5.4.3 CALCULATING HIGH-LEVEL FIGURES OF MERIT Systems engineers do not think in terms of individual bits, they are more interested in assessing the overall correctness and performance obtained from a circuit of some given architecture. So they tend to ask: What data chunks do emanate from a MUT in response to some given set of data at the input? What do those data sets really mean? Is the MUT functionally sound? Does it meet the specifications in its current stage, or where does it need further refinements? Example In lossy image compression, the original and the reconstructed image will necessarily differ. Departures are also to be expected when a golden model calculates with floating point numbers of almost arbitrary precision whereas the MUT operates with finite precision arithmetics. System designers will be most interested to learn about peak signal-to-noise ratio (PSNR), maximum perceived color deviation, throughput, and similar overall ratings. Conversely, they do not care much about when which operation takes place or about occasional idle cycles. \u0002 Rather than just report the number of bit- or word-level inaccuracies found, a truly high-level simulation shall look back over a full simulation run and distill the essence into a couple of relevant figures of merit. A simulation set-up that does so is shown in fig.5.14. It is a good idea to carry out such calculations outside the HDL testbench and to take advantage of standard mathematics tool such as Matlab instead. Math packages not only provide high-level functions for data and signal processing and for statistics, but also offer superior means of visualization. Assertions can also be put to service to collect relevant data. 5.4 CONSISTENCY AND EFFICIENCY CONSIDERATIONS 339 5.4.4 PATTERNING SIMULATION SET-UPS AFTER THE TARGET SYSTEM In practice, most simulation set-ups for large circuits follow the organization of the target system itself as this encourages reuse of HDL and software modules, facilitates development in successive refinement steps, and so helps improve productivity. Example Fig.5.15 shows an example from wireless telecommunication where multiple antennas are being used at the transmitter and at the receiver end to improve data rate and robustness. In the simulation set-up, a behavioral model is substituted for each RAM, IF (de)modulator, PCI interface and other subfunction that collaborates with the MUT. The various HDL models for one design entity share the same interface so that they can serve as drop-in replacements for each other during the development process. The preparation of stimuli and the evaluation of responses is implemented in Matlab so that the HDL testbench code remains essentially limited to configuring, controlling and monitoring the MUT via the PCI interface. Protocol adapters take care of translating where necessary. \u0002 HSPDA = high-speed downlink packet access MIMO = multiple-input multiple-output [or antennas] PCI = peripheral component interconnect IF = intermediate frequency test suite on-chip SRAM 1 on-chip SRAM 2 co-model co-model clock generator co-model MATLAB generated with evaluated and MATLAB generated with evaluated and stimuli high-level responses and expected baseband filters MUT MiMO HSPDA transceiver waveform deconstructor complex iF modulator complex iF demodulator baseband signalspropriet. format control HSPDA protocol adapter protocol adapter co-model co-model IF signal IF signal waveform preparator stimuli actual resp waveforms compact signal Y U PCI interface stimuli actual resp payload data C PCl interface protocol adapter co-model data reformatter X V FIGURE 5.15 Example of a sophisticated simulation set-up patterned after a target system from wireless telecommunication (simpliﬁed). 340 CHAPTER 5 FUNCTIONAL VERIFICATION 5.4.5 INITIALIZATION Almost all circuits need to be initialized prior to regular operation, and the same holds for the pertaining MUTs. Activating an asynchronous reset is just the most simple case as initialization may be much more complex. A program may need to be loaded into the instruction memory, configurations need to be set, a phase-locked loop (PLL) needs to reach operational status, the outcome from a built-in self test (BIST) needs to be waited for, etc. There are two options. ◦ If done by applying stimuli vectors from the MUT’s regular output, initialization may take thousands of clock cycles, but the test suite so obtained has the benefit of being portable to automatic test equipment (ATE) with no alterations whatsoever. ◦ Doing the same through debugging aids provided by the simulator (forcing logic values upon nodes, setting initial values to registers and memories, and the like) may achieve the same circuit condition in almost no time, but is not reproducible with physical parts. 5.4.6 TRIMMING RUN TIMES BY SKIPPING REDUNDANT SIMULATION SEQUENCES Memories, counters, and state machines tend to inflate the number of stimulus/response pairs necessary to verify functionality. This is because stereotypical activities eat a lot of computation time without contributing much towards uncovering further design flaws. Much of a simulation run just reiterates the same state transitions many times over without moving on to fresh states and functional mechanisms for a long time. Examples are quite common in timers, large filters, data acquisition equipment, and data transfer protocols, but the situation is notorious in image processing and man machine interfaces. For productivity reasons, designers seek to cut back cycles that feature little or highly recurrent computational activities in a design. What follows are suggestions of what they can do. • Monitor the evolution of the MUT’s various state variables by way of properties and assertions, rather than limiting yourself to checking their effects on overall output data. • Take advantage of the scan facility to skip uninteresting portions of a simulation run. • Do the same using simulator commands to manipulate the MUT’s state variables. • Include auxiliary logic in the MUT that trims lengthy counting or waiting sequences and unacceptably large data quantities while in simulation mode. • Do the same to the synthesis model to speed up the testing of physical circuits too. • Model circuit operation on two different time scales (fine and coarse). Example Imagine you are designing a graphics accelerator chip. Instead of simulating the processing of full- screen frames of 1280 pixels x 1024 pixels exclusively, make your MUT code capable of handling smaller graphics of, say, 40 pixels x 32 pixels as well. Use this thinned-out model for most of your simulation runs, but do not forget to run a couple of full-size simulations before proceeding to back- end design and prior to tape out. \u0002 5.5 TESTBENCH CODING AND HDL SIMULATION 341 5.5 TESTBENCH CODING AND HDL SIMULATION A testbench provides the following services during a simulation run: a) Generate a periodic clock signal for driving simulation and clocked circuit models. b) Obtain stimuli vectors and apply them to the MUT at well-defined moments of time. c) Acquire the signal waveforms that emanate from the MUT as actual response vectors. d) Obtain expected response vectors and use them as a reference against which to compare. e) Establish a simulation report that lists functional discrepancies and timing violations. VHDL and SystemVerilog not only support the modeling of digital circuits, they also provide the necessary instruments for implementing simulation testbenches.26 This section gives guidelines for doing so based on the general principles established earlier in this chapter. 5.5.1 MODULARITY AND REUSE ARE THE KEYS TO TESTBENCH DESIGN File-based and golden-model-based simulation are not the only meaningful ways to process test vectors, see fig.5.16 for more options. The coding effort can be kept more reasonable by recognizing that all sorts of simulation set-ups can be readily assembled from a very small number of versatile and reusable software modules. Adapting to a new design or a new simulation set-up can so largely be confined to a couple of minor adjustments to existing code.27 Observation 5.16. With testbenches being major pieces of software, it pays to have a look at them from a software engineering perspective. Preparing verification aids is not the same as circuit design. VLSI architects are limited to a synthe- sizable subset of their HDL and primarily think in block diagram and RTL categories. Verification engineers, in contrast, must think in terms of functionality and behavioral properties, but are not restricted to any particular subset of the language as testbenches and assertions are not for synthesis. They are free to use other language constructs or to use the same constructs in totally different ways. VHDL users can resort to shared variables at this point, while adopters of SystemVerilog will likely take advantage of classes, inheritance, and various high-level synchronization mechanisms offered by that language. Observation 5.17. Verification requires a different mindset than coding synthesis models. 26 Strictly speaking, instantiating a copy of the MUT and connecting it to the corresponding testbench signals prior to simulation are the sole services that absolutely need to be realized using an HDL. All other parts of a simulation set-up could be implemented using a programming language, Matlab, or some other software tool. In practice, however, only VHDL or SystemVerilog provide the detailed timing control necessary for generating precise clock waveforms, for correctly applying stimuli to the MUT, and for acquiring the actual responses from there just in time. 27 In an earlier edition of this text [146], the expected response pickup facility and the golden model were indeed designed as drop-in replacements for each other. The idea was to simplify the transition between file-based and golden-model-based simulation as much as possible in a VHDL design environment. However, as golden models are typically established outside the HDL environment and as subsequent HDL simulations are mostly file-based for efficiency reasons, the necessary provisions that cluttered the code examples and their graphical illustrations have been dropped for the sake of simplicity. 342 CHAPTER 5 FUNCTIONAL VERIFICATION actual resp stimuli appli reportgolden model A Eacqui = ? stimuli appli expect resp golden model A Eacqui = ? stimuli appli expect resp actual resp report A Eacqui = ?model under test assertions actual resp appli reportgolden model stimuli generator A Eacqui = ? model under test model under test stimuli actual resp appli interm resp appli report A E acqui = ? A Eacqui = ? model under test model under test actual resp appli reportgolden model appli stimuli source code stimuli machine code assembler program A Eacqui = ? model under test appli actual respA Eacqui = ? stimuli generator random model under test (a) (b) (c) (f) (e) (d) (g) FIGURE 5.16 Software modules from which simulation set-ups can be assembled to serve a variety of needs. Basic simulation set-up that operates with a test suite previously stored on disk (a). Alternative set-up that generates stimuli and expected responses at run time (b). Preparing stimulus/response pairs with the aid of a golden model (c). Fully assertion-based veriﬁcation with no evaluation of responses (d). A hybrid arrangement that combines traits from (a and b) (e). A special set-up for involutory ciphers (f). Another special set-up for a situation where the stimuli exist as source code for a program-controlled processor (g).28 5.5.2 ANATOMY OF A FILE-BASED TESTBENCH Consistent with what has been said in section 5.4.1, our focus here is on the set-up for a file-based format- and cycle-true simulation. Source code organized along the lines shown in fig.5.17 is available from the book’s companion website. As a circuit example, we have chosen the Gray counter of fig.5.3. Obtaining a good understanding requires working through that code. The comments below are intended as a kind of travel guide that points to major attractions. 28 Incidentally, note that Figure 5.16c implements the procedure introduced in Figure 5.1. 5.5 TESTBENCH CODING AND HDL SIMULATION 343 All disk ﬁles stored in ASCII format File-based simulation involves at least three files, namely the stimuli, the expected responses, and a simulation report. ASCII text files are definitely preferred over binary data because the former are human-readable and platform-independent, whereas the latter are not. What’s more, the usage of ASCII characters makes it possible to take advantage of the full MVL-9 (SysVer: MVL-4) set of values for specifying stimuli and expected responses. Designers are so put in a position to check for a high- impedance condition by entering a Z as expected response, for instance, or to neutralize a response by entering a don’t care - (SysVer: X). Separate processes for stimulus application and for response acquisition Recall from section 5.4.1 that the correct scheduling of simulation events is crucial: • The application of a new stimulus denoted as △ (for Application), • The acquisition and evaluation of the response denoted as ⊤ (for Test), • The two clock edges symbolically denoted as ↑ and ↓. It would be naive to include the time of occurrence of those key events hardcoded into a multitude of wait for statements or after clauses (SysVer: # terms) dispersed throughout the testbench code. A much better solution is to assign stimulus application and response acquisition to separate processes that get periodically activated at time △ and ⊤ respectively. All relevant timing parameters are expressed as constants or as generics, thereby making it possible to adjust them from a single place in the testbench code. 29 The stimulus application process is in charge of opening, reading, and closing the stimuli file. The response acquisition process does the same with the expected responses file. In addition, it handles the simulation report file. When the stimuli file has been exhausted, the stimulus application process notifies its counterpart via an auxiliary two-valued signal named EndOfSim_S. Stimuli and responses collected in records Two measures contribute towards rendering the two processes that apply the stimuli and that acquire the responses respectively independent of the MUT and, hence, highly reusable. a) All input signals are collected into one stimulus record and, analogously, all output signals into one response record. 30 b) The subsequent operations are delegated to specialized subprograms: • All file read and write operations, • Unpacking of stimuli records (where applicable), • Unpacking of expected response records (where applicable), • Packing of actual response records (if written to file), • Response checking (actual against expected), and • Compiling a simulation report. 29 The SystemVerilog testbench in listing 5.2 achieved the same objective with a pair of tasks. 30 Bidirectional input/output signals must appear both as stimuli and as expected responses. 344 CHAPTER 5 FUNCTIONAL VERIFICATION The main processes that make part of the testbench are so put in a position to handle stimulus and response records as wholesale quantities without having to know about their detailed composition. The writing of custom code is confined to a handful of subprograms. 31 Simulation to proceed even after expected responses have been exhausted Occasionally, a designer may want to run a simulation before having prepared a complete set of expected responses. There will be more stimuli vectors than expected responses in this situation. To support this policy, the testbench has been designed such as to continue until the end of the stimuli file, no matter how many expected responses are actually available. The numbers of responses that went unchecked gets reflected in the simulation report. Stoppable clock generator A simulation run draws to a close when the processing of the last entry in the stimuli file has been completed and the pertaining response has been acquired. A mundane difficulty is to halt the simulator. There exist three alternatives for doing so in VHDL, namely a) Have the simulator stop after a predetermined amount of time, b) Cause a failure in an assert or report statement to abort the run, or c) Starve the event queue in which case simulation comes to a natural end. Alternative a) is as restrictive as b) is ugly, so c) is the choice to retain. A clock generator that can be shut down is implemented as concurrent procedure call, essentially a shorthand notation for a procedure call embedded in a VHDL process. 32 While starving the event queue also works in SystemVerilog, the customary way to end a simulation run is to use either the $stop or $finish system task listed in table 4.6. Reset treated as an ordinary stimulus bit Timingwise, the reset signal, irrespective of whether synchronous or asynchronous, gets updated at time △ like any other stimulus bit. It is, therefore, made part of the stimuli record. 31 As an example, the subprograms that check the responses against each other and that handle the reporting need to be reworked when a MUT connector gets added, dropped, or renamed. Ideally, one would prefer to do away with this dependency by having those subprograms find out themselves how to handle the response records. This is, unfortunately, not possible as VHDL lacks a mechanism for inquiring about a record’s composition at run time in analogy to the array attributes ’left, ’right, ’range and ’length. 32 The reason for using this construct rather than a regular process statement is that the clock generator is reusable, and that we want to make it available in a package. This is not otherwise possible, as the VHDL syntax does not allow a package to include process statements. testbench MutInst: graycnt(behavioral) record of actual responses record of stimuli CLK_C RespAcqui response acquisition process simrept.asc simulation report expresp.asc expected responses Add a Trace to Simulation Report Check Response Add a Failure Entry to Simulation Report Add a Summary to Simulation Report in case of mismatch at the end Release an Expected Response Record per clock cycle at time stimulus application process StimAppli stimuli.asc stimuli per clock cycle at time Release a Stimulus Record ClkGen clock generator graycnt_tb(behavioral) EndOfAppli_S auxiliary testbench signal circuit model under test memorizing or memoryless behavior RTL synthesis code first, gate-level netlist later EndOfSim_S auxiliary testbench signal FIGURE 5.17 Organization of HDL testbench code for simulation set-up of ﬁg.5.11. testbench graycnt_tb(behavioral) CLK_C(record → binary patterns)input protocol adaptor(binary patterns → record)output protocol adaptor MutInst: graycnt(behavioral) EndOfAppli_S auxiliary testbench signal RespAcqui response acquisition process simrept.asc simulation report expresp.asc expected responses Add a Trace to Simulation Report Check Response Add a Failure Entry to Simulation Report Add a Summary to Simulation Report in case of mismatch at the end Release an Expected Response Record per data entry after record of actual responses record of stimuli stimulus application process StimAppli stimuli.asc stimuli per data entry prior to Release a Stimuli Record ClkGen clock generator circuit model under test memorizing or memoryless behavior RTL synthesis code first, gate-level netlist later STA_S COP_S EndOfSim_S auxiliary testbench signal FIGURE 5.18 Organization of HDL testbench code for simulation set-up of ﬁg.5.13. 346 CHAPTER 5 FUNCTIONAL VERIFICATION 5.6 CONCLUSIONS • More often than not, what is available at the onset of a VLSI project are intentions rather than specifications. Identifying the real needs and casting them into workable instructions always is the first step in the design process. Rapid prototyping often is the only practical way to condense initial conceptions into detailed and unambiguous specs. • Simulation remains the prevalent method for functional design verification. Being a dynamic technique, it offers only limited protection against design flaws. • Properties and assertions are great for intercepting suspect behavioral patterns right where they originate. Generously include in-code sanity checks into simulation models. • Statistical coverage analysis allows designers to explicitly specify the features they want to be verified during simulation and to obtain relevant quality metrics. Much as with assertions, the detection of bugs so becomes less dependent on the propagation of flawed data to output pins. • In terms of verification support — properties, assertions, random stimuli generation, coverage analysis — SystemVerilog has clearly more to offer than VHDL, see fig.4.24. • The challenge of functional verification is to safeguard oneself against all plausible design slips without attempting exhaustive simulation. Coming up with a comprehensive collection of test cases requires foresight, care, precision, and a lot of work at the detail level. Following the guidance given in observation 5.12 should increase the likelihood of discovering problems in reasonable time. • Make sure to understand what potential design flaws might pass undetected whenever a shortcut is taken. • While establishing a verification plan, beware of preconceptions from the design process as to what situations and issues are to be considered uncritical. Insist on having persons other than the synthesis code writers select — or at least review — the verification strategy, the test cases, the assertions, the coverage analysis code, and the resulting data. • Making the same test suite work across the entire VLSI design and test cycle is a necessity as functional consistency is otherwise lost. Doing so typically implies coherent event scheduling, data abstraction, and latency absorption. The event sequence △↓ (⊤= \u0002) ↑ has been found to work across all levels of abstractions. • Keep in mind that assertions and shortcuts that involve simulator commands are not portable to the testing of physical parts using ATE. • Be prepared to spend more time on verifying functionality than on designing the circuit that implements it. There are virtually no limits to making simulation set-ups more sophisticated. The reader is referred to the specialized literature including [147] [103] [148] [129] [126] [149]. Also very helpful are industrial standards such as UVM [150] and OS-VVM [145]. 5.7 PROBLEMS 347 5.7 PROBLEMS 1. ∗ Design a test suite for exhaustive verification of the Gray counter specified below. Rst Clk Ena Count 0 -- 00...0 reset 1 ↓ - Count keep output unchanged 1 ↑ 0 Count idem 1 ↑ 1 graycode((bincode( Count)+ 1) mod 2w ) increm. Gray-coded output With how many clock cycles can you do? Indicate the formula for w-bit counters and the actual value for w = 4. How does this figure relate to the lower bound given in (5.2)? 2. ∗∗ Devise a test suite for a logic comparator function that tells whether two 6-bit vectors are the same or not. Go for a set of vectors that you consider a reasonable compromise between simulation time and functional coverage. Verify the VHDL architecture correct given below, or the gate-level circuit obtained after synthesis, against that test suite. No inconsistency must occur. Now check how your test suite performs on the flawed architectures given below. Note that the mistaken circuits named flawedy and flawedu are authentic outcomes from efforts by human designers that were using schematic entry tools. The deficiency of the fourth example flawedz, in contrast, has been built in on purpose to demonstrate the impact of an oversight during the editing of HDL code. entity compara6 is port ( ArgA_DI: in std_logic_vector(5 downto 0); ArgB_DI: in std_logic_vector(5 downto 0); Equ_DO: out std_logic ); end compara6; ------------------------------------------------------------------------- -- correct description of 6-bit logic comparator function architecture correct of compara6 is begin Equ_DO <= ’1’ when ArgA_DI=ArgB_DI else ’0’; end correct; ------------------------------------------------------------------------- -- flawed as one of the two arguments has its bits misordered -- note: a wrong ordering of ArgB_DI in the port list has the same effect architecture flawedy of compara6 is signal Bm_DI : std_logic_vector(5 downto 0); begin each_bit : for i in 5 downto 0 generate 348 CHAPTER 5 FUNCTIONAL VERIFICATION Bm_DI(i) <= ArgB_DI(5-i); end generate; Equ_DO <= ’1’ when ArgA_DI=Bm_DI else ’0’; end flawedy; ------------------------------------------------------------------------- -- mistaken translation of desired function into boolean operations architecture flawedu of compara6 is signal C1_D : std_logic_vector(5 downto 0); signal C2_D : std_logic_vector(2 downto 0); begin first_level : for i in 5 downto 0 generate C1_D(i) <= not (ArgA_DI(i) xor ArgB_DI(i)); end generate; second_level : for i in 2 downto 0 generate C2_D(i) <= not (C1_D(i) xor C1_D(i+3)); end generate; Equ_DO <= C2_D(2) xor C2_D(1) xor C2_D(0); end flawedu; ------------------------------------------------------------------------- -- corrupt due to a useless statement forgotten in the code architecture flawedz of compara6 is begin process (ArgA_DI,ArgB_DI) begin if ArgA_DI=ArgB_DI then Equ_DO <= ’1’; else Equ_DO <= ’0’; end if; if ArgA_DI=\"110011\" then Equ_DO <= ArgA_DI(0); end if; end process; end flawedz; 3. ∗ The purpose of this problem is to show that a test suite may ensure full toggling of all nodes in a gate-level circuit and still be inadequate for functional verification. To that end, find two combinational networks together with a non-exhaustive test suite such that • all nodes get toggled back and forth, • both networks comply with the test suite, and • the two networks are functionally different. 5.7 PROBLEMS 349 What are the simplest two such circuits you can think of? Generalizing to combinational n-input single-output functions, how does the number of test patterns necessary for full toggling relate to that required for exhaustive verification? 4. ∗ Consider a digital IC that communicates via a bidirectional data bus. A state machine inside the chip generates the enable signal for the pad drivers from its state and from Write/Read or some similar signal available at one of the chip’s control pins. In order to stay clear of transient drive conflicts, the bus must not be driven from externally before the on-chip drivers have actually released the bus in reaction to the control pin asking them to do so. As a consequence, both testbench and physical test equipment must observe a brief delay between updating the control signal and imposing data on the bus. Extend the precedence graph and the schedule of figs.5.10 and 5.19 accordingly. 5. ∗∗∗ Refer to the specifications for a FIFO in problem 12 of chapter 4 and assume you are the person in charge of verifying the RTL model. Begin by writing a simulation testbench that generates and applies directed random stimuli. Then develop a small set of assertions. As a minimum requirement, make sure that • the FIFO cannot get instantiated with a depth of less than two, • any attempt to read from the queue when empty causes an error message, and • the same applies for any attempt to write when the FIFO is full. 350 CHAPTER 5 FUNCTIONAL VERIFICATION 5.8 APPENDIX I: FORMAL APPROACHES TO FUNCTIONAL VERIFICATION Formal verification attempts to prove or disprove the correctness of some circuit representation by purely analytical means, i.e. without simulating the circuit’s behavior over time. A successful proof gives the designer the ultimate confidence that his design will indeed function as previously specified at some higher level of abstraction, and this irrespective of the input as there is no need to apply stimuli. Most formal verification algorithms work by converting a given design representation into a state graph, an ordered binary decision diagram (OBDD), or some other graph-type data structure before analyzing that structure and/or comparing it against similar design representations. There are different degrees of ambition, though. Equivalence checking Verifying the functional equivalence between two gate-level netlists or between a netlist and a piece of HDL code is not that difficult. Logic equations are extracted from the gate-level netlist are compared against the reference set of logic equations using theorems from switching algebra. Software tools capable of doing so are typically used to check the consistency — in regular operation mode — of a gate- level netlist with the original RTL synthesis model after test structures have been added. Other relatively minor modifications such as clock tree insertion, logic reoptimization, and conditional clocking are covered as well. While combinational subfunctions make up much of an RTL model, there are severe limitations when the checking shall be extended to sequential behavior. Automatic conformity checking of circuit models that are supposed to have equivalent external behavior but that differ in the number and/or location of registers, e.g. as a consequence from state reduction or from architectural optimizations, remains a challenging research topic [151]. Last but not least, equivalence checking always presupposes the availability of a golden model. Model checking As opposed to the above, model checking does not need any reference model, but aims at finding out whether a circuit model under all circumstances meets a set of specified criteria or properties that any meaningful implementation must satisfy. A welcome quality of model checking is that it provides a counterexample when a design violates some specification. That is, the checker indicates the specific circumstances under which the maloperation becomes manifest. A serious difficulty is the combinatorial explosion that confines the approach to subsystems with a fairly limited number of states. A detailed discussion is given in [152]. 5.8 APPENDIX I: FORMAL APPROACHES TO FUNCTIONAL VERIFICATION 351 Deductive veriﬁcation or model proving Deductive verification is closely related to theorem proving. The goal is a mathematical proof that a given circuit model or protocol indeed conforms with its formal specifications. The answer essentially is of type “true” or “false” and thus provides little clues to developers as to what is wrong with their designs. Deductive verification further suffers from the problems mentioned in section 5.2.1,but remains an active research area. The reader is referred to [153] [154] for accounts on formal verification technology. 352 CHAPTER 5 FUNCTIONAL VERIFICATION 5.9 APPENDIX II: DERIVING A COHERENT SCHEDULE FOR SIMULATION AND TEST This section serves to confirm that the simulation schedule presented in section 5.4.1 is indeed a well- founded one that conforms to the fundamental timing requirements of synchronous circuits without being unnecessarily constrained further. In order to do so, we approximate timing to a degree that makes it possible to describe how a circuit behaves when viewed from outside. 33 External timing requirements imposed by a model under test (MUT) Four sets of data propagation paths can be identified in any synchronous design that adheres to single- phase edge-triggered clocking. 34 These paths go • from inputs to outputs with no intervening registers (i → o), • from state-holding registers to outputs (s → o), • from inputs to state-holding registers (i → s), and • from state registers to state registers (s → s). (a) o(k) s(k) i(k) s(k+1) f g combinational circuitry state register CLK present input present output present state next state tho (b) tio tso tss tis tio tso tis tss tclk lo mintclk hi min ε ε 00 k-1cycle kcycle (c) 0 2 3 4 5 1topological sorting # order of eventsresponse acquisitionstimulus application recording of values active clock edge passive clock edge FIGURE 5.19 Data propagation paths through a synchronous single-phase edge-triggered circuit (a) along with the pertaining precedence graph (b), and the recommended order of events (c). Not surprisingly, we find the same four sets of paths in a Mealy automaton. This is simply because the functionality of any synchronous circuit can be modeled as a Mealy type state machine. In response to a new input or state, a physical circuit finishes re-evaluating g and f after the fresh data have propagated along all paths through the combinational logic. As input and state do not, in general, switch 33 We do not aim at modeling or even at understanding what exactly happens inside the circuit yet. A more accurate analysis will become feasible on the basis of a detailed timing model to be introduced in section 7.2.2. 34 Any asynchronous reset input can be handled like an ordinary input in this context. 5.9 APPENDIX II: DERIVING A COHERENT SCHEDULE 353 simultaneously, four delay parameters need to be introduced, namely tio, tso, tis and tss. 35 Parameter tio, for instance, denotes the time required to compute a new output o after input i has changed. The necessary precedence relations for the circuit to settle to a stationary state are 36 t⊤(k) ≥ max(t△(k) + tio , t↑(k − 1) + tso) (5.3) t↑(k) ≥ max(t△(k) + tis , t↑(k − 1) + tss) (5.4) Precedence relations captured in a constraint graph The above precedence relations can be expressed by the constraint graph of fig.5.19b where each node stands for a major event associated with clock period k. Note that there are two active clock nodes, namely one for the clock event immediately before the clock period under consideration and a second one for the clock event at its end. Each precedence relation is represented by a directed edge that runs from the earlier event to the later one. The minimum time span called for by the associated condition is indicated by the weight of that edge: Circuit delays. The aforementioned data propagation paths tio, tso, tis and tss map to a first set of four edges. State register hold time requirement. A fifth non-zero weight stipulates that new stimuli must not be applied earlier than tho after the previous active clock event. Ignoring this constraint is likely to cause hold time violations at some bistables or might otherwise interfere with the precedent state transition. 37 Clock minimum pulse widths. Two more edges of small but non-zero weight are labeled tclk hi min and tclk lo min. They indicate the minimum time spans during which the driving clock signal must remain stable. Securing a coherent vector set. There are also four edges of weight zero or close to zero. One of them leads from ⊤ to ↑ and has an infinitesimally small weight ε. It reflects the requirement that response acquisition must occur before the circuit gets any chance to change its state in reaction to the next active clock event. Three more edges define the preconditions for recording a consistent stimulus/response pair per clock cycle. 35 Most practical circuits have their set of input bits grouped into a number of vectors, each of which has its own delay parameters, and similarly for outputs. Extending our approach to cover such situations as well is left to the reader as an exercise, see problem 4. 36 Of course, precedence relations may be simpler in a given particular case, say for a combinational circuit (automaton with no state where tso, tis,and tss are not defined) or for a counter (Medvedev machine where tio is not defined and tso = tss). However, by consistently sticking to a scheme that is suitable for the most general case, we can avoid having to reorder events whenever we must move from one circuit type to another. 37 Note that tis and tss are meant to include the setup times of the registers. This explains why tsu does not appear in the constraint graph as opposed to tho which cannot be subsumed anywhere else. 354 CHAPTER 5 FUNCTIONAL VERIFICATION Solving the constraint graph Any desirable sequence must order the events such as to satisfy the above precedence relations for positive but otherwise arbitrary values of the seven timing parameters involved. Solutions are obtained from topological sorting of the precedence graph. 38 One such ordering is indicated by the underlined numbers in fig.5.19c. It corresponds to a periodic repetition of stimulus application, response acquisition, and clocking, and is symbolically denoted as △⊤ ↑ . The precedence graph also points to minor liberties. While it is true that the recording of a stimulus/re- sponse pair may take place at any time between response acquisition and the subsequent active clock edge, there is nothing to be gained from defining an extra point in time for doing so. The events of response acquisition ⊤ and recording \u0002 may as well be tied together. The passive clock edge, on the other hand, is free to float between two consecutive active edges as long as two the constraints tclk hi min and tclk lo min are respected. As a final result, the event order △↓ (⊤= \u0002) ↑ will almost always represent a workable solution. The recommended simulation schedule is depicted in fig.5.10. Anceau diagrams help visualize periodic events and timing The Anceau diagram is very convenient for visualizing events and timewise relationships that repeat periodically. Each round trip corresponds to one clock cycle (or to one computation period). The example of fig.5.20a illustrates the simulation schedule just found. tio tso tis tss (a) (b) response acquisition stimulus application active clock edge tio tso tis tss response acquisition stimulus application active clock edge FIGURE 5.20 Anceau diagram for a Mealy type circuit operated at moderate speed (a) and close to maximum speed (b). 38 The nodes of a graph are said to be in topological order if they are assigned integer numbers such that every edge leads from a smaller numbered node to a larger numbered one. 5.9 APPENDIX II: DERIVING A COHERENT SCHEDULE 355 Each arrow stands for the delay along one of the four signal propagation paths in a Mealy machine. The two short radial bars are just graphical representations of the max operators in (5.3)and (5.4) respectively. The outer one indicates when the output has settled to a new value and becomes available for acquisition. Similarly, the inner bar tells when the computation of the next state has come to an end and so determines the earliest point in time the circuit may be safely clocked. If the operating speed is to be increased, the clock period grows shorter while delays and timing conditions remain the same. Beyond a certain point, stimuli must get applied earlier and/or responses will have to be acquired later in the clock cycle. This is shown in fig.5.20b which corresponds to a circuit operating close to its maximum speed. Anceau diagrams come in handy for visualizing periodic event sequences and timing conditions. We will make use of them in later chapters in the context of clocking and input/output timing.","libVersion":"0.5.0","langs":""}