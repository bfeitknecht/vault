{"path":"sem1/LinAlg/VRL/extra/LinAlg_bb-notes_1.pdf","text":"Linear Algebra, First Part Blackboard Notes Bernd G¨artner September 15, 2023 Contents 0 Preface 3 1 Vectors and Matrices 4 1.1 Vectors and Linear Combinations . . . . . . . . . . . . . . . . . . . . . . . . . 4 1.1.1 Vector addition: v + w . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 1.1.2 Scalar multiplication: cv . . . . . . . . . . . . . . . . . . . . . . . . . . 5 1.1.3 (Linear) combination: cv + dw . . . . . . . . . . . . . . . . . . . . . . 5 1.1.4 Combining more vectors, matrix notation . . . . . . . . . . . . . . . . 6 1.1.5 Three vectors v1, v2, v3 in R 3 . . . . . . . . . . . . . . . . . . . . . . . 6 1.2 Lengths and Angles from Dot Products . . . . . . . . . . . . . . . . . . . . . 7 1.2.1 Scalar product (or dot product, inner product): v · w . . . . . . . . . 7 1.2.2 Length of a vector: ∥v∥ = √v · v . . . . . . . . . . . . . . . . . . . . . 7 1.2.3 Perpendicular (or orthogonal) vectors: v · w = 0 . . . . . . . . . . . . 7 1.3 Matrices and Their Column Spaces . . . . . . . . . . . . . . . . . . . . . . . . 8 1.3.1 Matrix-vector multiplication . . . . . . . . . . . . . . . . . . . . . . . 9 1.3.2 Column space: C(A) . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9 1.3.3 (Linear) independence of vectors . . . . . . . . . . . . . . . . . . . . . 10 1.3.4 Rank: rank(A) = number of independent columns . . . . . . . . . . 11 1.4 Matrix Multiplication AB and CR . . . . . . . . . . . . . . . . . . . . . . . . 11 1.4.1 Distributivity and associativity . . . . . . . . . . . . . . . . . . . . . . 12 1.4.2 A = CR . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12 2 Solving Linear Equations Ax = b 13 2.1 Elimination and back substitution . . . . . . . . . . . . . . . . . . . . . . . . 13 2.1.1 Back substitution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13 2.1.2 Elimination . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13 2.1.3 Elimination succeeds ⇔ the columns of A are independent . . . . . . 15 2.2 Elimination Matrices and Inverse Matrices . . . . . . . . . . . . . . . . . . . 16 2.2.1 The Inverse Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16 2.2.2 The inverse of a product AB . . . . . . . . . . . . . . . . . . . . . . . 17 2.3 Matrix Computations and A = LU . . . . . . . . . . . . . . . . . . . . . . . . 17 2.3.1 The cost of elimination . . . . . . . . . . . . . . . . . . . . . . . . . . . 17 2.3.2 The great factorization A = LU . . . . . . . . . . . . . . . . . . . . . . 19 1 2.4 Permutations and Transposes . . . . . . . . . . . . . . . . . . . . . . . . . . . 19 2.4.1 The P A = LU factorization . . . . . . . . . . . . . . . . . . . . . . . . 20 2.4.2 The transpose of A . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 2.4.3 Symmetric matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22 2.4.4 Symmetric LU-factorization . . . . . . . . . . . . . . . . . . . . . . . . 22 3 The Four Fundamental Subspaces 23 3.1 Vector Spaces and Subspaces . . . . . . . . . . . . . . . . . . . . . . . . . . . 23 3.1.1 Examples of vector spaces . . . . . . . . . . . . . . . . . . . . . . . . . 23 3.1.2 Subspaces of vector spaces . . . . . . . . . . . . . . . . . . . . . . . . 23 3.1.3 The column space of A . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 3.1.4 The columns of A span the vector space C(A) . . . . . . . . . . . . . 24 3.2 Computing the Nullspace by Elimination: A = CR . . . . . . . . . . . . . . . 25 3.2.1 Elimination column by column: the steps from A to R0 . . . . . . . . 26 3.2.2 The matrix factorization A = CR and the nullspace . . . . . . . . . . 27 3.3 The Complete Solution to Ax = b . . . . . . . . . . . . . . . . . . . . . . . . . 27 3.3.1 Number of solutions of Ax = b . . . . . . . . . . . . . . . . . . . . . . 28 3.4 Independence, Basis, and Dimension . . . . . . . . . . . . . . . . . . . . . . . 28 3.4.1 Bases (for Matrix Spaces) . . . . . . . . . . . . . . . . . . . . . . . . . 29 3.5 Dimensions of the Four Subspaces . . . . . . . . . . . . . . . . . . . . . . . . 30 4 Orthogonality 31 4.1 Orthogonality of vectors and subspaces . . . . . . . . . . . . . . . . . . . . . 31 4.1.1 Orthogonal complement V ⊥ . . . . . . . . . . . . . . . . . . . . . . . 32 4.1.2 The big picture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33 2 Chapter 0 Preface These are the blackboard notes for the ﬁrst half of the course Lineare Algebra (401-0131-00L) held at the Department of Computer Science at ETH Z ¨urich in HS23. The notes roughly correspond to what I plan to write on the tablet during my lectures (in German for the ﬁrst half of the course). The actual tablet notes will be made available after each lecture. In structure and content, the notes are based on the book Introduction to Linear Algebra (Sixth Edition) by Gilbert Strang, Wellesley - Cambridge Press, 2023. The notes are rather dense and not meant to replace full lecture notes or a book. Mainly, they should free students from the need to copy material from the blackboard. Many additional explanations (and answers to questions) will be given in the lectures. Exercises to practice the material will be published in the course Moodle and are discussed during the exercise classes. To summarize, these notes do not represent a complete and standalone Linear Algebra course; rather, they are meant to support the lectures and exercise classes. I also want to point out that Strang’s book is not part of the course’s ofﬁcial material, and there is no need for students to buy the book. With the blackboard notes, exercises, lectures, and exercises classes, the course is self-contained. Strang’s book serves as rec- ommended but optional literature. Bernd G¨artner, Z ¨urich, September 5, 2023 3 Chapter 1 Vectors and Matrices 1.1 Vectors and Linear Combinations A vector is (for now) an element of R n vector = sequence (tuple) of n real numbers xy [ 4 1 ][ −2 2 ] 0 = [ 0 0 ] 0 R 2: xy-plane xyz   −2 2 0     −2 2 3  0 =   0 0 0   0 R 3: xyz-space u =      u1 u2 ... un      , v =      v1 v2 ... vn      , w =      w1 w2 ... wn      , . . . R n R: real numbers n ∈ N (natural numbers) 0: zero vector. Vector = “movement” : go 4 steps right and 1 step up! xy [ 4 1 ] 0 [ 4 1 ] 1.1.1 Vector addition: v + w Combine the movements! R 2 : [ 2 3 ] + [ 3 −1 ] = [5 2 ] Rn :      v1 v2 ... vn      +      w1 w2 ... wn      =      v1 + w1 v2 + w2 ... vn + wn      xy [ 2 3 ][ 5 2 ][ 3 −1 ] “Parallelogram” 4 1.1.2 Scalar multiplication: cv Move c times as far! (c: the scalar) R 2 : 3 [2 1 ] = [6 3 ] R n : c      v1 v2 ... vn      =      cv1 cv2 ... cvn      xy [ 2 1 ][ 6 3 ] 1.1.3 (Linear) combination: cv + dw 5 [2 3 ] − 3 [ 3 −1 ] = [ 10 15 ] − [ 9 −3 ] = [ 1 18 ] Here: c = 5, d = −3. 5 [ 2 3 ][ 2 3 ][ 3 −1 ] −3 [ 3 −1 ][ 1 18 ] Every vector b = [b1 b2 ] is a combination of [2 3 ] and [ 3 −1 ]! Proof: we want c and d such that c [ 2 3 ] + d [ 3 −1 ] = [b1 b2 ] . “Column Picture:” Draw a parallelogram with opposite corners 0 and b and sides parallel to [2 3 ] and [ 3 −1 ]. The other two corners are c [2 3 ] and d [ 3 −1 ] . xy [ 2 3 ][ 3 −1 ][ b1 b2 ] c [ 2 3 ] d [ 3 −1 ] 0 “Row picture:” Two equations in two unknowns c and d: 2c + 3d = b1 3c − d = b2 Draw them as lines in the cd-plane. The intersection point solves both equations. cd2c + 3d = b13c − d = b2−b2 b2 3 b1 2 b1 3 5 Doesn’t always work: All combinations of [ 2 3 ] and [4 6 ] are on a line! (Exercise: What goes wrong in column and row pictures?) xy [ 2 3 ][ 4 6 ] 1.1.4 Combining more vectors, matrix notation 3 [1 2 ] + 2 [−1 3 ] − 4 [0 1 ] ︸ ︷︷ ︸ combination of 3 vectors = [ 1 8 ] [1 −1 0 2 3 1 ] ︸ ︷︷ ︸ matrix   3 2 −4   ︸ ︷︷ ︸ matrix-vector multiplication = [1 · 3 − 1 · 2 − 0 · 4 2 · 3 + 3 · 2 − 1 · 4 ] = [1 8 ] c1v1 + c2v2 + · · · + cnvn︸ ︷︷ ︸ combination of n vectors in Rm = b m rows m × n matrix ︷ ︸︸ ︷  | | | v1 v2 · · · vn | | |   n columns      c1 c2 ... cn      =   | b |   ↑ Matrix: “container for vectors” m × 1 matrix: a single vector in R m 1.1.5 Three vectors v1, v2, v3 in R 3 The combinations c1v1 + c2v2 + c3v3 form a line (vectors are collinear), a plane (vectors are coplanar), or the whole space (vectors are independent). xyz   −2 2 2     −1 1 1     −3 3 3   xyz   −2 2 0     −1 1 3     −3 3 3   xyz   −2 2 0     −1 1 3     2 3 −1   6 1.2 Lengths and Angles from Dot Products 1.2.1 Scalar product (or dot product, inner product): v · w R 2 : [1 2 ] · [ 4 6 ] = 1 · 4 + 2 · 6 = 16 R n :      v1 v2 ... vn      ·      w1 w2 ... wn      = v1w1 + v2w2 + · · · + vnwn. 1.2.2 Length of a vector: ∥v∥ = √v · v R 2 : ∥ ∥ ∥ ∥ [−4 2 ]∥ ∥ ∥ ∥ = √(−4)2 + 22 = √ 20 R n : ∥ ∥ ∥ ∥ ∥ ∥ ∥ ∥ ∥      v1 v2 ... vn      ∥ ∥ ∥ ∥ ∥ ∥ ∥ ∥ ∥ = √ v2 1 + v2 2 + · · · + v2 n Why? Pythagoras! [ −4 2 ] 0 √42 + 222490 0   −4 2 0   0 √42 + 223   −4 2 3  √ √42 + 222 + 32 = √42 + 22 + 3290 0 Unit vector: ∥u∥ = 1. For every v ̸= 0, v ∥v∥ is a unit vector. 0vu1 v ∥v∥ Standard unit vectors: ei =        0 ... 1 ... 0        ← position i e1 = [ 1 0 ] e2 = [ 0 1 ] xy 1.2.3 Perpendicular (or orthogonal) vectors: v · w = 0 [4 2 ] · [ −1 2 ] = −4 · 1 + 2 · 2 = 0. xy [ 4 2 ][ −1 2 ] 900 7 Cosine Formula: cos(α) = v · w ∥v∥∥w∥ for v, w ̸= 0. vwα Because | cos(α)| ≤ 1: Cauchy-Schwarz inequality: |v · w| ︸ ︷︷ ︸ | cos(α)|∥v∥∥w∥ ≤ ∥v∥∥w∥. Triangle inequality: ∥v + w∥ ≤ ∥v∥ + ∥w∥. “From 0 directly to v + w is shorter than via v or w.” vwv + w∥w∥∥v∥∥v + w∥0∥w∥∥v∥ Hyperplanes. If d ∈ Rn, d ̸= 0, the set {v ∈ Rn : v · d = 0} is a hyperplane: all vectors perpendicular to d. d = [ 1 3 ] 0 [ −3 1 ] xy R 2: a line xyz   −2 2 0     −1 1 3     −3 3 3  d =   3 3 0   v · d = 0 R 3: a plane 1.3 Matrices and Their Column Spaces Matrix with m rows, n columns: m × n matrix (A, B, . . .) A + B, cA: 3×2 matrix :   1 2 3 4 5 6   m×n matrix :      a11 a12 · · · a1n a21 a22 · · · a2n ... ... . . . ... am1 am2 · · · amn      [1 2 3 4 ] + [5 6 7 8 ] = [ 6 8 10 12 ] 2 [1 2 3 4 ] = [2 4 6 8 ] 0 : zero matrix, aij = 0 for all i, j Square matrix: m = n.   1 0 0 0 1 0 0 0 1     2 0 0 0 4 0 0 0 5     2 1 −3 0 4 7 0 0 5     2 0 0 1 4 0 −3 7 5     2 1 −3 1 4 7 −3 7 5   identity (symbol: I) diagonal upper triangular lower triangular symmetric aii = 1, aij = 0 if i ̸= j aij = 0 if i ̸= j aij = 0 if i > j aij = 0 if i < j aij = aji 8 1.3.1 Matrix-vector multiplication 7   1 3 5   + 8   2 4 6   ︸ ︷︷ ︸ combination =   1 2 3 4 5 6   [7 8 ] =   1 · 7 + 2 · 8 3 · 7 + 4 · 8 5 · 7 + 6 · 8   ︸ ︷︷ ︸ scalar products Ax =      a11 a12 · · · a1n a21 a22 · · · a2n ... ... . . . ... am1 am2 · · · amn      ︸ ︷︷ ︸ A      x1 x2 ... xn      ︸ ︷︷ ︸ x =      a11x1 + a12x2 + · · · + a1nxn a21x1 + a22x2 + · · · + a2nxn ... am1x1 + am2x2 + · · · + amnxn      x1v1 + x2v2 + · · · + xnvn︸ ︷︷ ︸ combination =   | | | v1 v2 · · · vn | | |   ︸ ︷︷ ︸ A, column picture      x1 x2 ... xn      =     |u1||u2|...|um|      ︸ ︷︷ ︸ A, row picture      x1 x2 ... xn      ︸ ︷︷ ︸ x =      u1 · x u2 · x ... um · x      ︸ ︷︷ ︸ scalar products 1.3.2 Column space: C(A) All combinations (“span”) of the columns. If A is m × n, C(A) = {Ax : x ∈ R n} ⊆ R m. Always: 0 ∈ C(A). C ([ 2 4 3 1 ]) = R 2 (plane, 2-dim.) xy [ 2 3 ][ 4 1 ] C ([ 2 4 3 6 ]) = { c [ 2 3 ] : c ∈ R } (a line, 1-dim.) xy [ 2 3 ][ 4 6 ] How many columns are needed to span C(A)? A =   | | | v1 v2 · · · vn | | |   Check v1, v2, . . . , vn! If vi is a combination of v1, . . . , vi−1, then vi is dependent (not needed): Every combination of v1, . . . , vi is already a combination of v1, . . . , vi−1. Proof: c1v1 + · · · + civi︸ ︷︷ ︸ combination of v1, · · · , vi = c1v1 + · · · + ci−1vi−1 + ci(d1v1 + · · · + di−1vi−1︸ ︷︷ ︸ vi ) = (c1 + cid1)v1 + · · · + (ci−1 + cidi−1)vi−1 ︸ ︷︷ ︸ combination of v1, . . . , vi−1 xy [ 2 3 ][ 4 6 ] not needed 9 Otherwise, vi is independent (needed: “adds a dimension.”) Checking order doesn’t matter: we always ﬁnd the same num- ber of independent columns (3.4). For v1 (i = 1): v1, . . . , vi−1 contains no vectors. 0 is the only combination of no vectors. (“The sum of nothing is 0”.) xy [ 2 3 ][ 4 1 ] needed 1.3.3 (Linear) independence of vectors Deﬁnition: Vectors w1, w2, . . . , wk are. . . . . . (linearly) independent if. . . (i) no vector is a combination of the previous ones. Or (ii) no vector is a combination of the other ones. Or (iii) there are no c1, c2, . . . , ck besides 0, 0, . . . , 0 such that c1w1 + c2w2 + · · · + ckwk = 0. . . . (linearly) dependent if. . . (i’) some vector is a combination of the previous ones. Or (ii’) some vector is a combination of the other ones. Or (iii’) there are some c1, c2, . . . , ck besides 0, 0, . . . , 0 such that c1w1 + c2w2 + · · · + ckwk = 0. All say the same (are equivalent): (i) ⇔ (ii) ⇔ (iii). The opposites also: (i’) ⇔ (ii’) ⇔ (iii’). Proof: (i’)⇒(ii’) (if (i’) is true, then (ii’) is true): clear (“previous ones” are “other ones”). (ii’)⇒(iii’): If wi = c1w1 + · · · + ci−1wi−1 + ci+1wi+1 + · · · + ckwk, ← (ii’) then c1w1 + · · · + ci−1wi−1 − 1wi + ci+1wi+1 + ckwk = 0. ← (iii’) (iii’)⇒(i’): If there are some c1, c2, . . . , ck besides 0, 0, . . . , 0 such that c1w1 + c2w2 + · · · + ckwk = 0 ← (iii’) take the largest i such that ci ̸= 0. Then c1w1 + c2w2 + · · · + ciwi = 0 and hence wi = −c1 ci w1 − · · · − ci−1 ci wi−1. ← (i’) The columns of a matrix A are. . . . . . independent if . . . (iii) there is no x besides 0 such that Ax = 0. . . . dependent if . . . (iii’) there is some x besides 0 such that Ax = 0. 10 1.3.4 Rank: rank(A) = number of independent columns rank ([ 2 4 3 1 ]) = 2, rank ([ 2 4 3 6 ]) = 1, rank ([ 0 0 0 0 ]) = 0. Row space: R(A). All combinations of the rows R ([ 2 4 3 1 ]) = R 2 (plane, 2-dim.) xy [ 2 4 ][ 3 1 ] R ([ 2 4 3 6 ]) = {c [ 2 4] : c ∈ R } (a line, 1-dim.) xy [ 2 4 ][ 3 6 ] In the examples, number of independent columns = number of independent rows. Coin- cidence? No (3.5)! Easy case: rank 1. Matrices of rank 1. One independent column. All columns of A are multiples of      v1 v2 ... vm      ︸ ︷︷ ︸ ̸=0 ⇒ ⇐ A =      c1v1 c2v1 · · · cnv1 c1v2 c2v2 · · · cnv2 ... ... . . . ... c1vm c2vm · · · cnvm      ︸ ︷︷ ︸ rank 1: some cj vi ̸= 0 ⇒ ⇐ All rows of A are multiples of [c1, c2, . . . , cn] ︸ ︷︷ ︸ ̸=0 1.4 Matrix Multiplication AB and CR A : m × k matrix; B : k × n matrix; AB : m × n matrix. AB =     |u1||u2|...|um|      ︸ ︷︷ ︸ A, row picture   | | | v1 v2 · · · vn | | |   ︸ ︷︷ ︸ B, column picture =      u1 · v1 u1 · v2 · · · u1 · vn u2 · v1 u2 · v2 · · · u2 · vn ... ... . . . ... um · v1 um · v2 · · · um · vn      ︸ ︷︷ ︸ mn scalar products AB = [1 2 3 4 ] [ 0 1 1 0 ] = [1 · 0 + 2 · 1 1 · 1 + 2 · 0 3 · 0 + 4 · 1 3 · 1 + 4 · 0 ] = [2 1 4 3 ] ”column exchange” BA = [0 1 1 0 ] [ 1 2 3 4 ] = [0 · 1 + 1 · 3 0 · 2 + 1 · 4 1 · 1 + 0 · 3 1 · 2 + 0 · 4 ] = [3 4 1 2 ] ”row exchange” Square matrices: usually, BA ̸= AB (matrix multiplication is not commutative). General matrices: BA can be undeﬁned (if m ̸= n), or of different size than AB. 11 Everything is matrix multiplication! Vector-vector Matrix-vector: [ 1 2 3 4 ] ︸ ︷︷ ︸ 2×2 [ 1 1 ] ︸︷︷︸ 2×1 = [ 3 7 ] ︸︷︷︸ 2×1 Scalar (inner) product: [ 1 2] ︸ ︷︷ ︸ 1×2 [3 4 ] ︸︷︷︸ 2×1 = [11 ] ︸︷︷︸ 1×1 Vector-matrix: [ 1 1] ︸ ︷︷ ︸ 1×2 [1 2 3 4 ] ︸ ︷︷ ︸ 2×2 = [ 4 6] ︸ ︷︷ ︸ 1×2 Outer product: [3 4 ] ︸︷︷︸ 2×1 [1 2] ︸ ︷︷ ︸ 1×2 = [3 6 4 8 ] ︸ ︷︷ ︸ 2×2 ← rank 1     |u1B||u2B|...|umB|      ︸ ︷︷ ︸ AB, row picture =     |u1||u2|...|um|      ︸ ︷︷ ︸ A, row picture   | | | v1 v2 · · · vn | | |   ︸ ︷︷ ︸ B, column picture =   | | | Av1 Av2 · · · Avn | | |   ︸ ︷︷ ︸ AB, column picture 1.4.1 Distributivity and associativity A(B + C) = AB + AC and (B + C)D = BD + CD (AB)C = A(BC) = ABC. More matrices: brackets don’t matter: (AB)(CD) = A((BC)D) = · · · = ABCD . Distributivity: easy Associativity: boring calculations with sums and products involving matrix entries More matrices: needs proof! 1.4.2 A = CR Finding the independent columns, revisited: columns of A A =   1 2 0 3 2 4 1 4 3 6 2 5     1 2 3     2 4 6     0 1 2     3 4 5  ====v1 1v1 2v1 3v1 v2 v3 1v3 −2v3 v4 independent? yes no yes no A =   1 0 2 1 3 2   ︸ ︷︷ ︸ C [1 2 0 3 0 0 1 −2 ] ︸ ︷︷ ︸ R C: the independent columns R: how to combine them to get all columns Rank factorization: if A has r indepen- dent columns, then A︸︷︷︸ m×n = C︸︷︷︸ m×r R︸︷︷︸ r×n . Efﬁcient computation: (3.2) R is unique: if A = CR = CR′, then C(R − R′) = 0 ⇒ Cw = 0 for every column w of R − R′ ⇒ w = 0, since the columns of C are independent (1.3.3). 12 Chapter 2 Solving Linear Equations Ax = b 2.1 Elimination and back substitution System of m linear equations in n unknowns x1, x2, . . . , xn: a11x1 + a12x2 + · · · + a1nxn = b1 a21x1 + a22x2 + · · · + a2nxn = b2 ... am1x1 + am2x2 + · · · + amnxn = bm    Ax = b :      a11 a12 · · · a1n a21 a22 · · · a2n ... ... . . . ... am1 am2 · · · amn      ︸ ︷︷ ︸ A, m×n      x1 x2 ... xn      ︸ ︷︷ ︸ x∈Rn =      b1 b2 ... bm      ︸ ︷︷ ︸ b∈Rm Given A and b, ﬁnd x! For now: m = n, A is square matrix. 2.1.1 Back substitution If A upper triangular:   2 3 4 0 5 6 0 0 7     x1 x2 x3   =   19 17 14   equation substitution solution row 3 7x3 = 14 x3 = 2 row 2 5x2 + 6x3 = 17 5x2 + 12 = 17 x2 = 1 row 1 2x1 + 3x2 + 4x3 = 19 2x1 + 11 = 19 x1 = 4 2.1.2 Elimination General case: Transform Ax = b to U x = c with same solution but upper triangular U (Gauss elimination). Then back substitution! 13 Row Operations fat number: the pivot A =   2 3 4 4 11 14 2 8 17   b =  19 55 50   subtract 2·(Row 1) from (Row 2): ↓ E21 =   1 0 0 −2 1 0 0 0 1   E21A =   2 3 4 0 5 6 2 8 17   E21b =  19 17 50   subtract 1·(Row 1) from (Row 3): ↓ E31 =   1 0 0 0 1 0 −1 0 1   E31E21A =   2 3 4 0 5 6 0 5 13   E31E21b =  19 17 31   subtract 1·(Row 2) from (Row 3): ↓ E32 =   1 0 0 0 1 0 0 −1 1   E32E31E21A ︸ ︷︷ ︸ U =   2 3 4 0 5 6 0 0 7   E32E31E21b ︸ ︷︷ ︸ c =  19 17 14   ↑ elimination matrices done! Less nice case: A =   2 3 4 4 6 14 2 8 17   b = · · · elimination in ﬁrst column: ↓ E31E21A =   2 3 4 0 0 6 0 5 13   E31E21b = · · · can’t go on with pivot 0: exchange rows 2 and 3: ↓ P23 =   1 0 0 0 0 1 0 1 0   P23E31E21A ︸ ︷︷ ︸ U =   2 3 4 0 5 13 0 0 6   P23E31E21b ︸ ︷︷ ︸ c = · · · ↑ permutation matrix done! Ugly case: A =   2 3 4 4 6 14 2 3 17   b = · · · elimination in ﬁrst column: ↓ E31E21A =   2 3 4 0 0 6 0 0 13   E31E21b = · · · no row exchange helps, give up for now!   2 3 4 0 5 6 0 0 0   also ugly! 14 Solving U x = c also solves Ax = b Same solutions before and after each row operation! x is a subtract c·(Row i) from (Row j) T : matrix of the solution or exchange rows i and j row operation before: Ax = b =⇒︸︷︷︸ do! T A︸︷︷︸ A′ x = T b︸︷︷︸ b′⇑ w w \u0000 A ︷︸︸︷ T ′A′ x = b ︷︸︸︷ T ′b′ undo! ︷︸︸︷ ⇐= A ′x = b′ : x is a T ′: matrix of the add c·(Row i) to (Row j) solution row operation or exchange rows i and j after Also holds if A is non-square. Special case: b = 0 (⇒ b′ = T b = 0): Ax = 0 ⇔ A ′x = 0 (In)dependence of columns is preserved The columns of A are dependent 1.3.3 ⇐⇒ There is x ̸= 0 such that Ax = 0 ⇔ There is x ̸= 0 such that A′x = 0 1.3.3 ⇐⇒ The columns of A ′ are dependent Ugly case in step j ⇒ the ﬁrst j columns are dependent j = 3 : ﬁnd       1 2 3 0 4 5 0 0 0 ... ... ... 0 0 0          x1 x2 x3   ︸ ︷︷ ︸ ̸=0 =        0 0 0 ... 0        equation substitution solution row 3 0x3 = 0 anything goes! x3 = 4 row 2 4x2 + 5x3 = 0 4x2 + 20 = 0 x2 = −5 row 1 x1 + 2x2 + 3x3 = 0 x1 + 2 = 0 x1 = −2 Also true in the original matrix A, because (in)dependence of columns is preserved. 2.1.3 Elimination succeeds ⇔ the columns of A are independent Elimination (allowing row exchanges) succeeds: ⇒ U has nonzero diagonal elements (pivots). ⇒ Every column of U is independent from the previous ones. ⇒ The columns of U are independent (1.3.3). ⇒ The columns of A are independent (2.1.2). Elimination fails: ⇒ The columns of some intermediate matrix are dependent (ugly case) ⇒ The columns of A are dependent. (2.1.2). 15 2.2 Elimination Matrices and Inverse Matrices Elimination: A =   2 3 4 4 11 14 2 8 17   do! −→ ←− undo! U =   2 3 4 0 5 6 0 0 7   E−1 ︷ ︸︸ ︷ E1 32E1 31E1 21 A = U Eij : do!↑      ↑    ↑  subtract 2·(Row 1) from (Row 2) subtract 1·(Row 1) from (Row 3) subtract 1·(Row 2) from (Row 3) E−1 ij : undo! A = E−1 ︷ ︸︸ ︷ E−1 21 E−1 31 E−1 32 U↑  add 2·(Row 1) to (Row 2) ↑    add 1·(Row 1) to (Row 3) ↑      add 1·(Row 2) to (Row 3) An n × n matrix M is invertible if there is an n × n matrix M −1 (the inverse of M ) such that M M −1 = M −1M = I     I =      1 0 · · · 0 0 1 · · · 0 ... ... . . . ... 0 0 · · · 1           . M · . . . : do something! M −1 · . . . : undo it ! I · . . . : do nothing! There can only be one inverse: If M X = Y M = I, then X = Y , because X = IX = (Y M )X = Y (M X) = Y I = Y. ↑ associativity (1.4.1) Case 1 × 1: M = [ x] , M −1 = [ 1 x] (if x ̸= 0). Case 2 × 2: M = [ a b c d ] , M −1 = 1 ad − bc [ d −b −c a ] (if ad − bc ̸= 0) 2.2.1 The Inverse Theorem Case n × n: A is invertible (i) ⇔ For every b ∈ R n, Ax = b has a unique solution x (ii) ⇔ the colunns of A are independent (iii) 16 Proof: (i) ⇒ (ii): if A is invertible, then • A −1b solves Ax = b: A(A −1b) = (AA−1)b = Ib = b. • Uniqueness: If Ax = b, then x = A −1b: A−1b = A −1(Ax) = (A −1A)x = Ix = x. (ii) ⇒ (iii): if Ax = 0 has a unique solution (0), the columns of A are independent (1.3.3). (iii) ⇒ (ii): If the columns of A are independent, elimination succeeds (2.1.3): Ax = b ⇔ U x = c (and U has nonzero diagonal elements). Back substitution: unique solution x. (ii) ⇒ (i): If Ax = b has a unique solution for all b, we ﬁnd v1, v2, . . . , vn such that Av1 =      1 0 ... 0      ︸︷︷︸ e1 , Av2 =      0 1 ... 0      ︸︷︷︸ e2 , . . . , Avn =      0 0 ... 1      ︸︷︷︸ en ⇒ A   | | | v1 v2 · · · vn | | |   ︸ ︷︷ ︸ B =      1 0 · · · 0 0 1 · · · 0 ... ... . . . ... 0 0 · · · 1      ︸ ︷︷ ︸ I . So AB = I. Still need BA = I to conclude that B = A−1: • AI = IA = (AB)A = A(BA), hence A(I − BA) = 0 by distributivity (1.4.1). • Columns of I − BA: w1, w2, . . . , wn. Then Awi = 0 for all i. • The columns of A are independent by (ii) ⇒ (iii). Hence wi = 0 for all i. So I − BA = 0, meaning BA = I. For any two n × n matrices A, B: If AB = I, then BA = I (Exercise). 2.2.2 The inverse of a product AB If A and B are n × n and invertible, then AB is also invertible, and (AB)−1 = B−1A −1. (”undo” works in reverse order of ”do”) Proof: (AB)(B−1A −1) = A(BB−1)A = AIA −1 = AA−1 = I. Works for more matrices: (ABC)−1 = C −1B−1A−1. 2.3 Matrix Computations and A = LU 2.3.1 The cost of elimination How many operations (·, /, +, −) are needed to solve Ax = b? 17 Elimination in step j. Subtract ℓij· (Row j) from (Row i): matrix number of entries right-hand side u11 · · · c1 0 u22 · · · c2 0 0 . . . c3 row j 0 0 · · · ujj · · · ujn ← n − j + 1 1 → cj ... row i 0 0 · · · ⋆ij · · · ⋆in ← n − j + 1 1 → ⋆i for one i for i = j + 1, . . . , n op. where? A → U b → c A → U b → c / ℓij = ⋆ij/ujj 1 (n − j) · r = ℓij· (Row j) (n − j + 1) 1 (n − j)(n − j + 1) (n − j) − (Row i) −r (n − j + 1) 1 (n − j)(n − j + 1) (n − j) Elimination in all steps j = 1, . . . , n − 1. Apply known formulas (sum of the ﬁrst inte- gers, sum of the ﬁrst square numbers): A → U : • Divisions: 1 2(n2 − n) • Multiplications / Subtractions: 1 3(n3 − n) b → c: • Multiplications / Subtractions: 1 2(n2 − n) Roughly 2 3n3 operations for A → U and n2 for b → c. Back substitution. In row j of U x = c, substitute the already known values of xj+1, . . . , xn into ujjxj + uj,j+1xj+1 + · · · + ujnxn = cj and solve for xj: xj = 1 ujj (cj − uj,j+1xj+1 − · · · − ujnxn) . op. for one j for j = n, n − 1, . . . , 1 / 1 n · (n − j) 1 2(n2 − n) − (n − j) 1 2(n2 − n) Roughly n2 operations. Solving Ax = b (for one or more b′s) takes roughly 2 3n3 operations for A → U , and roughly 2n2 operations per b (b → c, back subsitution). 18 2.3.2 The great factorization A = LU Elimination: A → U (upper triangular). Assumption for now: no row exchanges! Elimination in row i. Subtract ℓij· (Row j of U ) from (Row i): u11 · · · ← ﬁnalized (in U ) 0 u22 · · · ← ﬁnalized (in U ) 0 0 . . . ... row j 0 0 · · · ujj · · · ujn ← ﬁnalized (in U ) ... row i 0 0 · · · ⋆ij · · · ⋆in Happens in steps j = 1, . . . , i − 1. How does (Row i) change in each step? (Row i) of A initially − ℓi1 · (Row 1) of U step 1 − ℓi2 · (Row 2) of U step 2 ... − ℓi,i−1 · (Row i − 1) of U step i − 1 = (Row i) of U in the end (Row i) of A is a combination of the ﬁrst i rows of U . Matrix notation: (Row i) of A = [ ℓi1 ℓi2 · · · ℓi,i−1 1 0 · · · 0 ] ︸ ︷︷ ︸ row vector U. A =      1 ℓ21 1 ... . . . ℓn1 · · · ℓn,n−1 1      ︸ ︷︷ ︸ L, lower triangular      u11 u12 · · · u1n u22 · · · u2n . . . ... unn      ︸ ︷︷ ︸ U , upper triangular In this notation, we omit 0’s above/below the diagonal. 2.4 Permutations and Transposes A = LU fails if there are row exchanges. Is there a ﬁx? Fact: Reordering the rows of a matrix S reorders the rows of SA in the same way: 19     |w1||w2|...|wm|      ︸ ︷︷ ︸ S A =     |w1A||w2A|...|wmA|      ︸ ︷︷ ︸ SA Example: exchange rows 1, 2 of S → S′     |w2||w1|...|wm|      ︸ ︷︷ ︸ S′ A =     |w2A||w1A|...|wmA|      ︸ ︷︷ ︸ S′A Permutation matrix P : reordering (permutation) of the rows of I. P A: permutation of the rows of IA = A. P x: permutation of the the entries of x.   1 0 0 0 1 0 0 0 1     0 1 0 0 0 1 1 0 0   If P, P ′ are permutation matrices, then also P P ′: reordering twice is another reordering. There are n! = 1 · 2 · · · n permutation matrices, since n things can be ordered in n! ways: n n! orderings 1 1 1 2 2 12, 21 3 6 123, 132, 213, 231, 312, 321 4 24 1234, 1243, . . . 2.4.1 The P A = LU factorization Idea: move all row exchanges to the beginning (A → P A), then we can eliminate without row exchanges (P A = LU ). Notation: E j : do all elimination steps in column j P k, ℓ : exchange rows k and ℓ Example (↑ : move row exchange up!): A → U P A = LU E 1 P 2,5 P 2,5 P 2,5 exchange rows 2 and 5 P 2,5↑ E 1 E 1 P 3,4 and then rows 3 and 4 E 2 E 2 P 3,4↑ E 1 P 3,4 P 3,4↑ E 2 E 2 1 2 3 ← move Why it works: E j P k, ℓ has the same effect as P k, ℓ E j if k, ℓ > j.       |ujj||⋆||⋆|...|⋆|              |ujj||⋆||⋆|...|⋆|              |ujj||0||0|...|0|              |ujj||0||0|...|0|        E jE jP k ℓP k ℓ 20 2.4.2 The transpose of A A = [1 2 3 4 5 6 ] ↑ A23 ← reﬂection along ” ⧹” → A⊤ =   1 4 2 5 3 6   ↑ (A⊤)32 row i of A = column i of A⊤ Aij = (A ⊤)ji Scalar product: column j of A = row j of A⊤ (A⊤)⊤ = A v · w = v⊤ ︸︷︷︸ 1×n w︸︷︷︸ n×1 Transpose of a product: (AB)⊤ = B⊤A ⊤ AB ← reﬂection along ” ⧹” → B⊤A ⊤ : (AB)ij (B⊤A ⊤)ji              |u1||u2|...|um|      ︸ ︷︷ ︸ A   | | | v1 v2 · · · vn | | |   ︸ ︷︷ ︸ B          ij ︸ ︷︷ ︸ ui·vj =               |v1||v2|...|vn|      ︸ ︷︷ ︸ B⊤   | | | u1 u2 · · · um | | |   ︸ ︷︷ ︸ A⊤           ji ︸ ︷︷ ︸ vj ·ui Works for more matrices: (ABC)⊤ = C ⊤B⊤A ⊤. Transpose of the inverse: (A−1)⊤ = (A ⊤) −1 AA−1 = I ⇓ (A−1) ⊤A ⊤ = (AA −1)⊤ = I ⊤ = I ⇓ (A −1) ⊤ is the inverse of A ⊤ Permutation matrix: P −1 = P ⊤. Rows of P : p1, . . . , pn (reordering of rows of I). Each pi has a single 1 at a different position ⇒ pi · pi = 1, pi · pj = 0 for i ̸= j.               |p1||p2|...|pn|      ︸ ︷︷ ︸ P, row picture   | | | p1 p2 · · · pn | | |   ︸ ︷︷ ︸ P T , column picture           ij ︸ ︷︷ ︸ pi·pj = Iij ⇔ P P ⊤ = I. 21 2.4.3 Symmetric matrices S is symmetric if S = S⊤ (such S must be square). S =   2 1 −3 1 4 7 −3 7 5   If S is symmetric, then also S−1 (if it exists): (S−1) ⊤ = (S⊤)−1 = S−1. For every matrix A, both A ⊤A and AA⊤ are symmetric: (A ⊤A) ⊤ = A⊤(A ⊤) ⊤ = A ⊤A, (AA⊤)⊤ = (A ⊤) ⊤A ⊤ = AA⊤. 2.4.4 Symmetric LU-factorization Normal elimination step: subtract 2·(Row 1) from (Row 2) E21︸︷︷︸ L−1 [1 2 2 6 ] ︸ ︷︷ ︸ A, symmetric = [1 2 0 2 ] ︸ ︷︷ ︸ U Now add this extra step: subtract 2·(Column 1) from (Column 2) [1 2 0 2 ] ︸ ︷︷ ︸ U E⊤ 21︸︷︷︸ (L−1)⊤ = [ 1 0 0 2 ] ︸ ︷︷ ︸ D, diagonal U = L−1A A = LU ⇓ ⇓ D = L−1A(L⊤) −1 A = LDL⊤ ⇑ ⇑ D = U (L −1) ⊤ U = DL⊤ = U (L ⊤) −1 The general picture: product of elimination matrices ↓ A =   0 0 0 0 0 0 0 0 0 0     0 0 0 0 0 0 0 0 0 0   , U =   0 0 0 0 0 0 0 0 0 0   A   0 0 0 0 0 0 0 0 0 0   −1  0 0 0 0 0 0 0 0 0 0   = L U L −1 (exercise)   0 0 0 0 0 0 0 0 0 0     0 0 0 0 0 0 0 0 0 0   = U (L −1) ⊤ = L−1A(L−1)⊤ ← symmetric, if A is symmetric (exercise) U (L −1) ⊤=(exercise)   0 0 0 0 0 0 0 0 0 0   =   0 0 0 0 0 0 0 0 0 0     0 0 0 0 0 0 0 0 0 0     0 0 0 0 0 0 0 0 0 0   D = U (L−1)⊤ is upper triangular and symmetric ⇒ D is diagonal. D = L −1A(L −1)⊤ → A = LDL⊤ 22 Chapter 3 The Four Fundamental Subspaces 3.1 Vector Spaces and Subspaces 3.1.1 Examples of vector spaces There is more than R 2, R 3, . . . Vector space: (abstract) concept of things that we can do with vectors R 2, R 3, . . .: examples. concept number type vector space things that we can do with. . . . . . numbers: calculations! a + b, a − b, a · b, a/b . . . vectors: combinations! v + w, c · v examples N (natural numbers) Z (integers) Q (rational numbers) R (real numbers) C (complex numbers) {0, 1} (bits) ... R 2 R 3 C 3 (complex vectors) R 2×2 (2 × 2 matrices; A + B, cA (1.3)) R R (functions f : R → R) {0, 1} n (bit vectors) ... We mostly (but not only) care about R 2, R 3, . . . and their subspaces. 3.1.2 Subspaces of vector spaces V : vector space. Subspace: nonempty U ⊆ V satisfying: if v, w ∈ U and c is a scalar, then (i) v + w ∈ U (ii) cv ∈ U. Every subspace U contains 0: take some u ∈ U , then 0u = 0 ∈ U by (ii). Smallest subspace: U = {0}. Largest subspace: U = V . 23 xyzxyzxyz subspaces: line through 0 plane through 0 not a subspace: misses 0 A subspace of a vector space is itself a vector space. Two subspaces of V = R 2×2: U1: all symmetric matrices [a b b d ] U2: all diagonal matrices [a 0 0 d ] 3.1.3 The column space of A C(A) = {Ax : x ∈ R n} is a subspace of Rm: If v, w ∈ C(A) and c a scalar, then Ax = v and Ay = w for some x, y ∈ R n. Hence, (i) v + w = A(x + y ︸ ︷︷ ︸ ∈Rn ) ∈ C(A) (ii) cv = A( cx︸︷︷︸ ∈Rn ) ∈ C(A) 3.1.4 The columns of A span the vector space C(A) Span, Basis Example V : vector space C(A) S: sequence of vectors in V the columns of A S spans V : V = all combinations of S the columns span C(A) S basis of V : S independent, S spans V the independent columns: basis of C(A) V = C(A) S dependent S independent S spans V ← basis S doesn’t 24 3.2 Computing the Nullspace by Elimination: A = CR Nullspace of (m × n) matrix A: all solutions of Ax = 0 N(A) = {x ∈ R n : Ax = 0} (subspace of Rn) If all columns are independent: N(A) = {0} “Computing” a subspace: ﬁnd a basis of it! For N(A), we do this by computing A = CR (1.4.2): N ([ 1 2 3 6 ]) : x + 2y = 0 3x + 6y = 0 ⇕ x = −2y xyx = −2y A =   | | | | | | | v1 v2 v3 v4 v5 v6 v7 | | | | | | |   → C =   | | | v1 v3 v7 | | |   (the independent columns)  ↓ v1 = 1v1 v4 = r14v1 + r24v3 ↑ ↑ R =   1 r12 0 r14 r15 0 r17 1 r24 r25 0 r27 1 r37   (how to combine them to get all columns) R is in reduced row echelon form: 111110000000000e1e2e3e4e50· · · (standard unit vectors) Plan: Transform A to R using (Gauss-Jordan) elim- ination; we get C on the way. Row operations don’t change solutions (2.1.2): Ax = 0 ⇔ Rx = 0, N(A) = N(R). Read a basis of N(R) off R. The basis of N(R) Example: ”free variables” ↓ R = [ 1 2 0 3 0 0 1 −2 ] ︸ ︷︷ ︸ (1.4.2) Rx = [1 0 0 1 ] ︸ ︷︷ ︸ I [ x1 x3 ] + [2 3 0 −2 ] ︸ ︷︷ ︸ F [x2 x4 ] = 0 ⇔ [x1 x3 ] = −F [x2 x4 ] x every solution. . . . . . is a combination of two special independent ones. Since they span N(R), they are a basis.[x2 x4 ] [x1 x3 ] [ f1 f2 ] −F [ f1 f2 ] = f1     [1 0 ] −F [1 0 ]     + f2     [ 0 1 ] −F [ 0 1 ]    ===[−2f1 − 3f2 2f2 ] [ −2 0 ] [−3 2 ] 25 General case: R is (r × n). xI: the r variables for e1, e2, . . . , er xF : the n − r others (free variables) Rx = IxI + F xF = 0 ⇔ xI = −F xF general example r × n 2 × 4 xF [x2 x4 ] xI [x1 x3 ] x every solution. . . . . . is a combination of n − r special independent ones. Since they span N(R), they are a basis. xF xI f −F f = f1 ( e1 −F e1 ) + f2 ( e2 −F e2 ) + · · · + fn−r ( en−r −F en−r ) 3.2.1 Elimination column by column: the steps from A to R0 k → k + 1 (row operations) Case 1: only 0’s in blue Case 2: some ⋆ ̸= 0 in blue For k = 0, 1, . . . , n − 1: 11110000000...01111000000⋆ k columns done: k + 1 columns done: exchange rows: 1111000000?11110000000...01111000000⋆ → k + 1 columns done multiply row by 1/⋆: new row operation → 11110000001 n columns done: R0 eliminate in column k + 1: 111100000010000100000 also above pivot→ 11110000001000000 remove zero rows: R k + 1 columns done: 11110000001000010000011110000001000000 26 3.2.2 The matrix factorization A = CR and the nullspace A → R0 → R gives the same R as in A = CR (1.4.2): A = CR elimination A → R0 A =   | | | | | | | v1 v2 v3 v4 v5 v6 v7 | | | | | | |   → R0 =   | | | | | | | e1 w2 e2 w4 w5 e3 w7 | | | | | | |    ↓ v1 = 1v1 v4 = r14v1 + r24v3 ↑ ↑ (⋆) ⇐⇒ w4 = r14e1 + r24e2 ↓ R =   1 r12 0 r14 r15 0 r17 1 r24 r25 0 r27 1 r37   ← R0 =     1 r12 0 r14 r15 0 r17 1 r24 r25 0 r27 1 r37     x =           r14 0 r24 −1 0 0 0           : v4 = r14v1 + r24v3 ⇔ Ax = 0 (2.1.2) ⇐⇒ R0x = 0 ⇔ w4 = r14e1 + r24e2 (⋆) 3.3 The Complete Solution to Ax = b As in (2.1.2), apply row operations also to b (A → R0, b → c). Solutions don’t change: Ax = b ⇔ R0x = c ⇔      Rx 0 ... 0      =      d ⋆ ... ⋆      Ax = 0 ⇔ Rx = 0 If some ⋆ ̸= 0, no solution! Otherwise, solve Rx = IxI+F xF = d ⇔ xI = d−F xF x every solution. . . . . . is a particular solution of Rx = d, plus a combination of the n − r special solutions of Rx = 0 xF xI f d − F f = 0 d + f1 ( e1 −F e1 ) + f2 ( e2 −F e2 ) + · · · + fn−r ( en−r −F en−r ) 0Ax = 0Ax = b 27 3.3.1 Number of solutions of Ax = b rank (number of independent columns) = r r ≤ n ↓ ⇓ Ax = b → R0x = c → Rx = d r ≤ min(m, n) full rank: r = min(m, n) ↑ ↑ ↑ ⇑ (m × n) (m × n) (r × n) r ≤ m R0 r = n (full rank) r < n (dependent columns) invertible underdeterminedr=m(fullrank) 1 ∞ overdeterminedr<m(zerorows) ...· · · 0 or 1 0 or ∞ depending on c 3.4 Independence, Basis, and Dimension V : vector space S: sequence of vectors in V independent basis of V spans V e1, e2, . . . , en (columns of I): standard basis of R n. The columns of any invertible n × n matrix A are a basis of R n. They are independent and spanning: for every b ∈ Rn, Ax = b has a solution (2.2.1). If v1, v2, . . . , vn is a basis of V , then every v ∈ V is a unique combination. 28 Proof: if v = a1v1 + · · · + anvn = b1v1 + · · · + bnvn, then 0 = (a1 − b1)v1 + · · · + (an − bn)vn. By independence, a1 − b1 = · · · = an − bn = 0. Every basis of V has the same number of vectors. This number is the dimension dim(V ) of V . Proof (by contradiction): Suppose there is a basis v1, v2, . . . , vm and a larger basis w1, w2, . . . , wn. A basis spans V ⇒ each wj is a combination of the vi’s: wj = ⋆ v1 + ⋆ v2 + · · · + ⋆ vm ↓ ↓ ↓ vector xj with m numbers Matrix notation: [ w1 w2 · · · wn] ︸ ︷︷ ︸ B = [v1 v2 · · · vm] ︸ ︷︷ ︸ A   | | | x1 x2 · · · xn | | |   ︸ ︷︷ ︸ X, m×n rank(X) ≤ min(m, n) = m < n, so the columns of X are dependent (3.3.1): there is c ̸= 0 such that Xc = 0. Then Bc = AXc = A0 = 0 ⇔ c1w1 + c2w2 + · · · + cnwn = 0, so the wi’s are dependent and not a basis. Contradiction! Works for all vector spaces, not only (subspaces of) R n: consider A, B as 1 × m, 1 × n with vector entries (column vectors, or other objects). 3.4.1 Bases (for Matrix Spaces) vector space basis dimension R n e1, e2, . . . , en n all 2 × 2 matrices [a b c d ] [ 1 0 0 0 ] , [0 1 0 0 ] , [0 0 1 0 ] , [ 0 0 0 1 ] 4 diagonal matrices [a 0 0 d ] [ 1 0 0 0 ] , [ 0 0 0 1 ] 2 symmetric matrices [ a b b d ] [1 0 0 0 ] , [ 0 1 0 0 ] + [0 0 1 0 ] , [0 0 0 1 ] 3 {0} ∅ (empty set) 0 There are no independent vectors in {0}, so the basis must be empty. 0 is a combination of ∅ (sum of nothing = 0). 29 3.5 Dimensions of the Four Subspaces A: m × n matrix (m rows, n columns). This section: subspace of deﬁnition dimension C(A) R m combinations of the columns of A r = rank(A) R(A) = C(A ⊤) R n combinations of the rows of A = columns of A⊤ r N(A) R n solutions of Ax = 0 n − r N(A ⊤) R m solutions of A ⊤y = 0 m − r Row space R(A) = C(A ⊤) Gauss-Jordan: A → R0 by row operations: • subtract c·(Row i) from (Row j) • exchange (Row i) and (Row j) • multiply (Row i) with c ̸= 0 Exercise: Row operations don’t change the row space! R(A) = R(R0). R0: 111100000010000100000e1e2er· · · r independent rows that span the row space: basis of R(R0) dim(R(A)) = dim(R(R0)) = r zero rows For every matrix: Number of independent rows = number of independent columns! We knew this for rank-1 matrices (r = 1): (1.3.4) Nullspace N(A) Gauss-Jordan: A → R0 → R (remove zero rows of R0). Row operations don’t change solutions (2.1.2): Ax = 0 ⇔ R0x = 0 ⇔ Rx = 0 N(A) = N(R). Already found a basis of N(R) with n − r vectors (3.2). dim(N (A)) = n − r. Left nullspace N(A ⊤) As previously shown for every matrix: dim(nullspace) = number of columns − rank. Apply this to A ⊤: dim N(A ⊤) = m − dim(C(A ⊤)) = m − r. Why “left”? : all solutions of A⊤y = 0 = all solutions of y⊤A = 0⊤. 30 Chapter 4 Orthogonality 4.1 Orthogonality of vectors and subspaces Recall (1.2.3, 2.4.2): v, w ∈ R n are perpendicular or orthogonal if v · w = v⊤w = 0. [4 2 ] · [−1 2 ] ︸ ︷︷ ︸ v·w = [4 2] [−1 2 ] ︸ ︷︷ ︸ v⊤w = 0. v900w Two subspaces V and W of R n are orthogonal if v · w = 0 for all v ∈ V and all w ∈ W . xyVWxyVWzxyzVW If A is m × n: • N(A) and R(A) = C(A ⊤) are orthogonal in R n. • N(A⊤) and R(A ⊤) = C(A) are orthogonal in R m. Proof. v ∈ N(A) ⇔ Av = 0. w ∈ C(A ⊤) ⇔ w = A ⊤x. Then v⊤w = v⊤(A ⊤x) (1.4.1) = (v⊤A ⊤)x (2.4.2) = (Av) ⊤ ︸ ︷︷ ︸ 0⊤ x = 0. Same for N(A ⊤) and C(A). R ([ 1 2 3 6 ]) xy [ 1 2 ] N ([ 1 2 3 6 ]) xyx = −2y 31 Exercise: If V and W are orthogonal, V ∩ W = {0} (only the zero vector is in both). If V and W are subspaces of R n such that V ∩ W = {0}, then dim(V ) + dim(W ) ≤ n. Proof. Let k = dim(V ), ℓ = dim(W ), v1, . . . , vk a basis of V , w1, . . . , wℓ a basis of W . Suppose c1v1 + · · · + ckvk︸ ︷︷ ︸ v∈V + d1w1 + · · · dℓwℓ︸ ︷︷ ︸ w∈W (⇒−w∈W ) = 0. Then v = −w ∈ V ∩ W , so v = w = 0. v1, . . . , vk and w1, . . . , wℓ are independent ⇒ c1, . . . , ck = 0 and d1, . . . , dℓ = 0 ⇒ v1, . . . , vk, w1, . . . , wℓ are independent (1.3.3) ⇒ k + ℓ ≤ n. xyzVW dim(V ) = 2, dim(W ) = 1 4.1.1 Orthogonal complement V ⊥ V subspace of Rn. Deﬁnition: w ∈ R n is orthogonal to V if w is orthogonal to all vectors in V . V ⊥: all vectors in Rn that are orthogonal to V . Exercise: V ⊥ is a subspace. Let V, W be orthogonal subspaces of R n. The following statements are equivalent. V = N(A), W = R(A) = C(A ⊤) (i) W = V ⊥ true ⇑ (ii) dim(V ) + dim(W ) = n true: r + (n − r) = n (3.5) ⇓ (iii) every u ∈ R n can be written true as x = v + w with unique vectors v ∈ V, w ∈ W xyuvwVW Proof: v1, . . . , vk a basis of V , w1, . . . , wℓ a basis of W . (i)⇒(ii): Observation: w ∈ Rn orthogonal to V ⇔ w orthogonal to v1, . . . , vk. Let A be the matrix with rows v1, . . . , vk. Then V = C(A ⊤) (dimension k) and W = V ⊥ = N(A) (dimension n − k, 3.5). (ii)⇒(iii): As previously seen, v1, . . . , vk, w1, . . . , wℓ are independent. Since k + ℓ = n, they are a basis of R n. So u = c1v1 + · · · + ckvk︸ ︷︷ ︸ v + d1w1 + · · · dℓwℓ︸ ︷︷ ︸ w with unique scalars (3.4) ⇒ unique v, w. (iii)⇒(i): We need that W contains all vectors orthogonal to V . Let u ∈ R n be orthogonal to V . We can write u = v + w with v ∈ V, w ∈ W . Multiplying with v from the left, v⊤u︸︷︷︸ 0 = v⊤v + v⊤w︸ ︷︷ ︸ 0 ⇒ v⊤v = ∥v∥2 = 0 ⇒ v = 0 ⇒ u = w ∈ W. 32 4.1.2 The big picture N(A) : Ax = 0R(A) = C(A ⊤)N(A T )C(A)RnRmxxnullxrowbAx = bAxnullAxrowAx00 (3.3): Solutions of Ax = b = particular solution of Ax = b + solutions of Ax = 0 (4.1): N(A) and C(A ⊤), N(A ⊤) and C(A) are orthogonal subspaces. . . (4.1.1): . . . and orthogonal complements. For x ∈ R n: x = xrow + xnull (row space and nullspace components). If Ax = b, then Axrow = b, Axnull = 0. 33 Index N (natural numbers), 4 R (real numbers), 4 0 (zero vector), 4 associativity matrix multiplication, 12 back substitution, 13 basis of a vector space, 24 Cauchy-Schwarz inequality, 8 collinear vectors, 6 column space of a matrix, 9 combination of two vectors, 5 several vectors, 6 coplanar vectors, 6 cosine formula, 8 CR factorization, 12 dependent vectors, 10 diagonal magtrix, 8 dimension of a vector space, 29 dot product, 7 elimination cost, 17 failure, 15 Gauss, 13 Gauss-Jordan, 25 pivot, 14 success, 15 elimination matrix, 14 factorization CR, 12 LU, 19 free variables of nullspace, 25 full rank matrix, 28 Gauss elimination, 13 Gauss-Jordan elimination, 25 hyperplane, 8 normal vector, 8 identity matrix, 8 independent vectors, 6, 10 inner product, 7 intersecting lines, 5 inverse of permutation matrix, 21 inverse matrix, 16 of a product, 17 invertible matrix, 16 characterization, 16 left nullspace of a matrix, 30 length of a vector, 7 line, 8 linear combination, 5 of two vectors, 5 several vectors, 6 linear equation system, 13 back substitution, 13 complete solution, 27 Gauss elimination, 13 number of solutions, 28 34 overdetermined, 28 row exchange, 14 row operation, 14 underdetermined, 28 linearly dependent vectors, 10 linearly independent vectors, 10 lower triangular matrix, 8 LU-factorization, 19 symmetric, 22 with row exchanges, 20 matrix, 8 column picture, 9 column space, 9 diagonal, 8 elimination, 14 full rank, 28 identity, 8 inverse, 16 invertible, 16 left nullspace, 30 lower triangular, 8 nullspace, 25 free variables, 25 permutation, 14, 20 rank, 11 rank 1, 11 reduced row echelon form, 25 row picture, 9 row space, 11 square, 8 symmetric, 8, 22 transpose, 21 upper triangular, 8 zero, 8 matrix multiplication, 11 associativity, 12 matrix notation, 6 matrix space, 29 matrix-vector multiplication, 9 multiplication matrix-matrix, 11 matrix-vector, 9 number-matrix, 8 number-vector, 5 vector-matrix, 12 vector-vector dot product, 7 inner product, 7 outer product, 12 scalar product, 7 normal vector, 8 nullspace free variables, 25 of a matrix, 25 orthogonal complement, 32 orthogonal subspaces, 31 dimensions, 32 orthogonal vectors, 7, 31 outer product, 12 overdetermined linear equation system, 28 parallelogram, 5 permutation matrix, 14, 20 inverse, 21 perpendicular vectors, 7, 31 pivot, 14 plane, 8 Pythagoras, 7 rank of a matrix, 11 rank-1 matrix, 11 reduced row echelon form of a matrix, 25 row exchange, 14 row operation, 14 row space of a matrix, 11 scalar, 5 scalar multiplication, 5 scalar product, 7 span, 9 spanning vectors 35 of a vector space, 24 square matrix, 8 standard unit vector, 7 subspace, 23 subspaces orthogonal, 31 symmetric LU-factorization, 22 symmetric matrix, 8, 22 system of linear equations, 13 transpose of a matrix, 21 of a product, 21 of the inverse, 21 triangle inequality, 8 underdetermined linear equation system, 28 unit vector, 7 standard, 7 upper triangular matrix, 8 vector in R n, 4 length, 7 vector addition, 4 vector space basis, 24 concept, 23 dimension, 29 spanning vectors, 24 subspace, 23 vector-matrix multiplication, 12 vectors (linearly) dependent, 10 (linearly) independent, 10 collinear, 6 coplanar, 6 dot product, 7 independent, 6 inner product, 7 orthogonal, 7, 31 outer product, 12 perpendicular, 7, 31 scalar product, 7 span, 9 zero matrix, 8 zero vector, 4 36","libVersion":"0.3.2","langs":""}