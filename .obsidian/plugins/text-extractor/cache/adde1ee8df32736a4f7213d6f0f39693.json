{"path":"sem3/LinAlg/PV/pvw/LinAlg-pvw-script.pdf","text":"Skript f¨ur den Pr¨ufungsvorbereitungs Workshop Lineare Algebra Steven Battilana, Kai Zheng 30. Dezember 2021 Zusammenfassung Idee. Dieses Skript wurde in Hinblick auf die Pr ¨ufung geschrieben. Die Idee ist, dass es prinzipiell ein Kapitel pro Pr ¨ufungsaufgabe hat, wobei nur das neue Reglement ber ¨ucksichtigt wird. Um das ganze m ¨oglichst kurz zu halten wurden die Grundkenntnisse in das gleichnamige Kapitel aus- gelagert. Damit werden die anderen Kapitel aufs wesentliche beschr¨ankt. Viel Erfolg! i Inhaltsverzeichnis Inhaltsverzeichnis iii 1 Grundvoraussetzung f ¨ur die LR-Zerlegung 1 1.1 Komplexe Zahlen . . . . . . . . . . . . . . . . . . . . . . . . . . 1 1.2 Lineare Gleichungssysteme LGS . . . . . . . . . . . . . . . . . 5 1.3 Matrizen und Vektoren im Rn und Cn . . . . . . . . . . . . . . 10 1.4 Skalarprodukt . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14 1.5 Orthogonale und uni¨are Matirzen . . . . . . . . . . . . . . . . 16 1.6 Cauchy-Schwarz . . . . . . . . . . . . . . . . . . . . . . . . . . 18 1.7 Invertierbare Matrizen . . . . . . . . . . . . . . . . . . . . . . . 20 2 LR-Zerlegung 25 2.1 LR-Zerlegung (engl. LU decomposition) . . . . . . . . . . . . . 25 3 Grundvoraussetzung f ¨ur die Vektorr¨aume und linearen Abbildung 35 3.1 Vektorr¨aume . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35 3.2 Basen, Dimensionen und lineare (Un-)Abh¨anigkeit . . . . . . 36 3.3 Lineare Abbildungen . . . . . . . . . . . . . . . . . . . . . . . . 37 4 Vektorr¨aume und lineare Abbildungen 39 4.1 Untervektorr¨aume . . . . . . . . . . . . . . . . . . . . . . . . . 39 4.2 Basen, Dimensionen und lineare (Un-)Abh¨anigkeit . . . . . . 40 4.3 Lineare Abbildungen . . . . . . . . . . . . . . . . . . . . . . . . 45 5 Grundvorraussetzung f ¨ur die Methode der kleinsten Quadrate 51 5.1 Normen von linearen Abbildungen (Operatoren) und Matri- zen, Konditionszahl . . . . . . . . . . . . . . . . . . . . . . . . . 51 6 Die Methode der kleinsten Quadrate 57 6.1 Methode der kleinsten Quadrate . . . . . . . . . . . . . . . . . 57 6.1.1 Normalengleichung . . . . . . . . . . . . . . . . . . . . 58 iii Inhaltsverzeichnis 6.1.2 QR-Zerlegung . . . . . . . . . . . . . . . . . . . . . . . . 61 7 Grundvorraussetzung f ¨ur die Eigenwerte und Eigenvektoren 67 7.1 Petermutationen . . . . . . . . . . . . . . . . . . . . . . . . . . . 67 7.2 Determinanten . . . . . . . . . . . . . . . . . . . . . . . . . . . . 68 8 Eigenwerte und Eigenvektoren 75 8.1 Eigenwerte und Eigenvektoren . . . . . . . . . . . . . . . . . . 75 8.2 Spektralzerlegung, Diagonalisierbarkeit . . . . . . . . . . . . . 79 9 Addendum 85 9.1 Basiswechsel und Koordinatentransformation . . . . . . . . . 85 9.2 Orthonormalbasis und Parsevalsche Formel . . . . . . . . . . 91 9.3 Orthogonale und unit¨are Abbildungen . . . . . . . . . . . . . 92 9.4 Singul¨arwertzerlegung . . . . . . . . . . . . . . . . . . . . . . . 93 Literaturverzeichnis 99 iv Kapitel 1 Grundvoraussetzung f¨ur die LR-Zerlegung 1.1 Komplexe Zahlen Bemerkung 1.1 z2 + 1 = 0 ist ein Beispiel f ¨ur eine in R unl¨osbare Gleichung. Um eine L¨osung zu finden erweitern wir deshalb den K¨orper auf R2 und nennen dies K ¨orper (engl. Field, wird in der diskreten Mathematik im 5. Kapitel Algebra genauer behandelt) der komplexen Zahlen C. Definition 1.2 (imagin¨are Einheit) i2 = −1 i = ∧ die imagin¨are Einheit Definition 1.3 (kartesische Form) z = x + iy Definition 1.4 (Real- und Imagin¨arteil) Re(z) := x ∈ R = ∧ Realteil Im(z) := y ∈ R = ∧ Imagin¨arteil Definition 1.5 (Konjugation) Die Konjugation von z = x + iy ∈ C sei z = x − iy ∈ C. Die Konjugation hat die folgenden Eigenschaften: 1 1. Grundvoraussetzung f ¨ur die LR-Zerlegung (i) F ¨ur alle z = x + iy = (x, y) ∈ C = R gilt • z · z = (x + iy) · (x − iy) = x2 − i2y2 = x2 + y2 = ∥z∥2 . (ii) F ¨ur alle z1, z2 ∈ C gilt • z1 + z2 = z1 + z2; • z1z2 = z1 z2. Definition 1.6 (Euler Formel) eiφ = cos φ + i sin φ Definition 1.7 (Polarform) Die Polarform von z = x + iy ∈ C sei z = reiφ, Euler Formel ⇔ z = r(cos φ + i sin φ), mit r = ∥z∥, x = r cos φ, y = r sin φ. φ =    arctan( y x ), x > 0 arctan( y x ) + π, x < 0 ∧ y ≥ 0 arctan( y x ) − π, x < 0 ∧ y < 0 π 2 , x = 0 ∧ y > 0 − π 2 , x = 0 ∧ y < 0 undefiniert, x = 0 ∧ y = 0 Bemerkung 1.8 (Ausblick) z2 + 1 = 0 ist ein Beispiel f ¨ur eine in R unl¨osbare Gleichung, die in C L¨osungen hat (n¨amlich z = ±i). Allgemein gilt der Fundamentalsatz der Algebra: Jedes Polynom p(z) = zn + an−1zn−1 + · · · + a0 vom Grad n ≥ 1 hat in C eine Nullstelle. Das heisst, C ist im Unterschied zu R algebraisch vollst ¨andig. Beispiel 1.9 Berechne: 6+7i 3−8i L ¨osung: 6 + 7i 3 − 8i = 6 + 7i 3 − 8i · 3 + 8i 3 + 8i = 18 + 21i + 48i + 56i2 9 − 64i2 = 18 + 21i + 48i − 56 9 + 64 = −38 + 69i 73 Beispiel 1.10 Berechne die Polarform von z = 1 + i. L ¨osung: r = √12 + 12 = √2 φ = arctan( 1 1 ) = π 4 ⇒ z = √2ei π 4 2 1.1. Komplexe Zahlen Beispiel 1.11 Berechne die kartesische Form von 7ei π 3 . L ¨osung: 7ei π 3 = 7(cos( π 3 ) + i sin( π 3 )) = 7 · 1 2 + 7 · √3 2 i = 7 2 + 7√3 2 i Beispiel 1.12 Zeichnen Sie die folgenden Mengen grafisch in der komplexen Ebene: D := {( √2 2 (1 + i) )n : n ∈ N } L ¨osung: In Polarkoordinaten gilt (1 + i) = √2ei π 4 , also folgt ( √2 2 (1 + i) )n = ei nπ 4 . Weil eiθ = eiθ+2kπ ∀k ∈ C besteht D aus 8 Punkten, die f ¨ur n = 0, 1, 2, ..7 gefunden werden: D = {1, ei π 4 , ei 2π 4 , ei 3π 4 , ei 4π 4 , ei 5π 4 , ei 6π 4 , ei 7π 4 } 3 1. Grundvoraussetzung f ¨ur die LR-Zerlegung Bemerkung 1.13 (sollte auf eure Zusammenfassung f ¨ur die Pr ¨ufung) Im folgenden sieht ihr sch ¨one Cosinus- und Sinuswerte auf dem Einheitskreis, wo- bei die x-Richtung cos(x) und die y-Richtung sin(x) entspricht: 4 1.2. Lineare Gleichungssysteme LGS 1.2 Lineare Gleichungssysteme LGS Der allgemeine Fall hat m lineare Gleichungen, n Unbekannte und stellt ein LGS dar. Falls m > n, dann ist das LGS ¨uberbestimmt (numerisch l¨osbar) m < n, dann ist das LGS unterbestimmt (analytisch l¨osbar) m = n, sonst (analytisch l¨osbar) a11x1 + a12x2 + · · · + a1nxn = b1 a21x1 + a22x2 + · · · + a2nxn = b2 ... ... am1x1 + am2x2 + · · · + amnxn = bm    ⇒ Ax = b Wobei A =      a11 a12 · · · a1n a21 a22 · · · a2n ... ... am1 am2 · · · amn      , x =    x1 ... xn    , b =    b1 ... bm    A = ∧ Koeffizientenmatrix, x = ∧ Unbekanntenvektor, b = ∧ L¨osungsvektor (RHS) L¨osungsansatz: Gauss-Elimination Bemerkung 1.14 F ¨ur ein LGS gilt jeweils eines der folgenden Punkte: Es besitzt • genau eine L¨osung, dann nennt man es ein regul¨ares LGS • keine L¨osung, dann nennt man es ein singul¨ares LGS • ∞ viele L¨osungen, dann nennt man es ebenfalls ein singul¨ares LGS 5 1. Grundvoraussetzung f ¨ur die LR-Zerlegung Kochrezept 1.15 (Gauss-Elimination) Gegeben: LGS Ax = b (f ¨ur m < n ∨ m = n) A =      a11 a12 · · · a1n a21 a22 · · · a2n ... ... am1 am2 · · · amn      , b =    b1 ... bm    Gesucht: x =    x1 ... xn    1. Stelle die erweiterte Koeffizientenmatrix auf    a11 · · · a1n b1 ... ... ... am1 · · · amn bm    = (A b) . 2. Bringe (A b) durch Operationen der Art (I), (II), (III) in folgende Form (Zeilenstufenform, d.h. es muss nicht unbedingt die Einheitsmatrix ergeben!):    1 0 x1 . . . ... 0 1 xn    ⇔ (1 x) , wobei 1 = ∧ Einheitsmatrix (I) Zeilen vertauschen (II) Addition/Subtraktion von einer Zeile (Gleichung) zu einer anderen (III) Ver-k-fachen einer Zeile (Gleichung) mit k ∈ R \\ {0} 3. Am besten geht das, wenn ihr das folgende Verh¨altnis bildet (dies werden wir sp¨ater nochmals brauchen!) lij := ai1 ajj 6 1.2. Lineare Gleichungssysteme LGS und dies folgend nutzt x1 x2 x3 RHS ( )(i) a11 a12 a13 b1 (ii) a21 a22 a23 b2 (iii) a31 a32 a33 b3 (ii)−l21·(i) ⇝   a11 a12 a13 b1 a21 − l21a11 a22 − l21a12 a23 − l21a13 b2 − l21b1 a31 a32 a33 b3   (iii)−l31·(i) ⇝   a11 a12 a13 b1 0 ̃a22 ̃a23 ̃b2 a31 − l31a11 a32 − l31a12 a33 − l31a13 b2 − l31b1   ⇝  a11 a12 a13 b1 0 ̃a22 ̃a23 ̃b2 0 ̃a32 ̃a33 ̃b2   . . . Beispiel 1.16 L¨ose das folgende LGS: 2x1 − x2 − 2x3 = 2 4x1 − 2x2 + 2x3 = −2 8x1 − 4x2 + 6x3 = −6 L ¨osung: 2x1 − x2 − 2x3 = 2 4x1 − 2x2 + 2x3 = −2 8x1 − 4x2 + 6x3 = −6    ⇒  2 −1 −2 2 4 −2 2 −2 8 −4 6 −6   (ii)−l21·(i) ⇝  2 −1 −2 2 0 0 6 −6 8 −4 6 −6   (iii)−l31·(i) ⇝  2 −1 −2 2 0 0 6 −6 0 0 14 −14   1 6 ·(ii) ⇝  2 −1 −2 2 0 0 1 −1 0 0 14 −14   (iii)−l32·(ii) ⇝  2 −1 −2 2 0 0 1 −1 0 0 0 0   (i)− (−2) 1 ·(ii) ⇝  2 −1 0 0 0 0 1 −1 0 0 0 0   1 2 ·(i) ⇝  1 − 1 2 0 0 0 0 1 −1 0 0 0 0   =: (∗) In der 3. Zeile gibt es nur Nullen ⇒ ∞ viele L¨osungen. Mit R ¨uckw¨artseinsetzen erhalten wir von der 2. Zeile: x3 = −1 Mit R ¨uckw¨artseinsetzen erhalten wir von der 1. Zeile: x1 − 1 2 x2 = 0 ⇔ x1 = 1 2 x2 7 1. Grundvoraussetzung f ¨ur die LR-Zerlegung W¨ahle z.B. x2 = t ∈ R als freien Parameter L =      1 2 t t −1   ∣ ∣ ∣ ∣ ∣ ∣ t ∈ R    Bemerkung 1.17 Falls wir statt (∗) z.B.  1 − 1 2 0 0 0 0 1 −1 0 0 0 2   erhalten h¨atten, g¨abe es keine L¨osung, weil in der 3. Zeile 0 = 2 steht, was bekannt- lich einen Widerspruch darstellt. Beispiel 1.18 F ¨ur welche Werte von a ∈ R besitzt das folgende homogene lineare Gleichungssys- tem eine nichttriviale (von 0 verschiedene) L¨osung? x1 − x3 = 0 −2x1 + ax2 − x3 = 0 a2x1 + 2ax2 − 10x3 = 0 L ¨osung: x1 − x3 = 0 −2x1 + ax2 − x3 = 0 a2x1 + 2ax2 − 10x3 = 0    ⇒   1 0 −1 0 −2 a −1 0 a2 2a −10 0     1 0 −1 0 −2 a −1 0 a2 2a −10 0   ⇝   1 0 −1 −2 a −1 a2 2a −10   (ii)−l21·(i) ⇝   1 0 −1 0 a −3 a2 2a −10   (iii)−l31·(i) ⇝  1 0 −1 0 a −3 0 2a a2 − 10   (iii)−l32·(ii) ⇝  1 0 −1 0 a −3 0 0 a2 − 4   1. Fall: x3 ̸= 0 Die 3. Zeile gibt uns (a2 − 4)x3 = 0 ⇒ a2 − 4 = 0 ⇔ a = ±2 Wir w¨ahlen x3 =: s, s ∈ R \\ {0} als freien Parameter. Mit R ¨uckw¨artseinsetzen erhalten wir von der 2. Zeile ax2 − 3s = 0 ⇔ ax2 = 3s ⇔ x2 = 3s a 8 1.2. Lineare Gleichungssysteme LGS Mit R ¨uckw¨artseinsetzen erhalten wir von der 1. Zeile x1 − s = 0 ⇔ x1 = s Somit sind wir bereits bei der L¨osung von diesem Fall angelangt: L =      s 3s a s   ∣ ∣ ∣ ∣ ∣ ∣ s ∈ R \\ {0}, a = ±2    2. Fall: x3 = 0 Somit macht die 3. Zeile keine Aussage ¨uber a. Also m ¨ussen wir auf die 2. Zeile ausweichen. ax2 − 3x3 = 0 ⇒ ax2 − 3 · 0 = 0 ⇒ ax2 = 0 ⇒ a = 0 ∨ x2 = 0 a) a = 0, x2 ̸= 0: Wir w¨ahlen x2 = t, t ∈ R \\ {0} als freien Parameter. Mit R ¨uckw¨artseinsetzen erhalten wir von der 1. Zeile x1 = 0. Somit erhalten wir die L¨osung: L =     0 t 0   ∣ ∣ ∣ ∣ ∣ ∣ t ∈ R \\ {0}, a = 0    b) x2 = 0: Somit folgt aus der 1. Zeile: x1 = 0 Dieser Fall liefert nur die triviale L¨osung  0 0 0   und kann ausgeschlos- sen werden. Insgesamt folgt also, dass wir f ¨ur a ∈ {−2, 0, 2} nichttriviale L¨osungen erhalten. 9 1. Grundvoraussetzung f ¨ur die LR-Zerlegung 1.3 Matrizen und Vektoren im Rn und Cn Definition 1.19 Die im folgenden Bild eingekreisten Elemente heissen Pivotelement oder kurz Pi- vot. Definition 1.20 (Pivotisierung) Pivotisiere in der aktuellen Spalte j, d.h. bestimme den Index ip ∈ {j, j + 1, ..., n} mit |aip j| = max i∈{j,j+1,...,n} |aij|. Definition 1.21 In der Zeilenstufenform (ZSF) des Systems heisst die Anzahl r ∈ N der Pivotele- mente der Rang der Matrix A ∈ Rm×n und des Gleichungssystems, d.h. Rang(A) := r = ∧ # Pivotelemente. Bemerkung 1.22 Sei die Matrix A ∈ Rm×n und der Rang r ∈ N, dann gilt (i) r ≤ min{m, n}, (ii) k − r = ∧ # freie Parameter, mit k = min{m, n}. Beispiel 1.23 (zu (ii)) m < n : A =  3 0 0 −2 0 1 0 12 0 0 2 −1   ⇝ Rang(A) = 3 Hier sieht man, dass der Rang nicht mehr als 3 sein kann, da die maximal m¨ogliche Anzahl Pivotelemente gleich 3 ist. m > n : B =     1 0 −2 0 2 5 0 0 −12 0 0 0     ⇝ Rang(B) = 3 10 1.3. Matrizen und Vektoren im Rn und Cn In diesem Fall ist die letzte Zeile linear abh¨angig, d.h. mit den ersten drei Zeilen k¨onnen wir die Letzte zu Nullzeile umwandeln. Somit ist auch hier die maximal m¨ogliche Anzahl Pivotelemente gleich 3. Bemerkung 1.24 (Rechenregeln) Sei A ∈ Em×n und sei B, C ∈ En×n mit E ∈ {R, C}. (i) Rang(AT) = Rang(A) = Rang(AH) (ii) Rang(B) + Rang(C) −n ≤ Rang(BC) ≤ min{Rang(B), Rang(C)} Definition 1.25 Sei das folgende lineare Gleichungssystem (LSG) gegeben: Die im Falle m > r f ¨ur die Existenz einer L¨osung notwendigen Bedingungen cr+1 = cr+2 = ... = cm = 0 sind die Vertr¨aglichkeitsbedingungen des LGS. Definition 1.26 einer Matrix A ∈ Rm×n A =      a11 a12 · · · a1n a21 a22 · · · a2n ... ... am1 am2 · · · amn      m = ∧ Zeilen n = ∧ Spalten Bemerkung 1.27 Falls m = n, dann heisst die Matrix A quadratisch. Bemerkung 1.28 (Rechnen mit Matrizen) Sei ein Skalar (Zahl) α ∈ R, seien die Matrizen A,B ∈ Rm×n und C ∈ Rn×p, dann gilt • Addition: (A ± B)ij = (A)ij ± (B)ij 11 1. Grundvoraussetzung f ¨ur die LR-Zerlegung • Matrixmultiplikation: (AC)ij = n ∑ k=1(A)ik · (C)kj Die resultierende Matrix AC ist eine m × p Matrix. A =         a11 · · · a1n ... ... ai1 · · · ain ... ... am1 · · · amn         , C =    c11 · · · c1j · · · c1p ... ... ... cn1 · · · cnj · · · cnp    AC =     (AC)ij = ai1c1j + · · · + aincnj     • Erinnerung: Skalarprodukt (Dotproduct) a =    a1 ... an    , b =    b1 ... bn    a · b = a1b1 + · · · + anbn Beispiel 1.29 Berechne AB mit den gegebenen Matrizen. A = (2 −3 5 1 0 2 ) , B =   0 5 1 1 −2 0   L ¨osung: AB = (2 · 0 + −3 · 1 + 5 · −2 2 · 5 + −3 · 1 + 0 · 5 1 · 0 + 0 · 1 + −2 · 2 1 · 5 + 0 · 1 + 2 · 0 ) = (−13 7 −4 5 ) Bemerkung 1.30 Es gelten f ¨ur die Matrixmultiplikation die g¨angigen Rechenregeln (Assoziativit¨at, Distributivit¨at). AUSSER Kommutativit¨at !!! (AB ̸= BA) !!!!! Definition 1.31 Sei A einen relle m × n - Matrix. Die n × m - Matrix AT mit (AT)ij = Aji heisst die zu A transponierte Matrix. Definition 1.32 Sei A eine komplexe m × n - Matrix. Die n × m - Matrix AH mit AH = (A)T = AT heisst die zu A hermitesch oder konjugiert-transponierte Matrix. 12 1.3. Matrizen und Vektoren im Rn und Cn Bemerkung 1.33 (Eigenschaft von transponiert bzw. hermitesch) Sei A, B ∈ En×n, dann gilt (AB)H = BH AH (bzw. (AB)T = BT AT). Beispiel 1.34 Berechne AH der gegebenen Matrix: A =  3 + 2i −5 i 15 − 7i 0 0.5i   L ¨osung: A =  3 + 2i −5 i 15 − 7i 0 0.5i   ⇒ AH = (3−2i −i 0 −5 15 + 7i −0.5i ) Bemerkung 1.35 Wenn die untere Dreiecksmatrix L ∈ En×n gleich Null ist, dann handelt es sich um eine nilpotente Matrix. L =    0 ∗ ∗ ... . . . ∗ 0 · · · 0    ⇒ Lk = 0 f ¨ur ein k ∈ N. Beispiel 1.36 A =  0 1 1 0 0 1 0 0 0   ⇝ A3 = 0 B =   5 −3 2 15 −9 6 10 −6 4   ⇝ B2 = 0 Definition 1.37 (Hermitesch) Sei A ∈ Cn×n. Dann ist A hermitesch genau dann, wenn folgendes gilt: AH = A ⇔ (AH)ij = (A)ij ∀i, j ∈ {1, ..., n}. Beispiel 1.38 Welche der folgenden Matrix ist hermitesch: A =   1 3i 1 − 5i 2 + i 4 −6 + 2i −3 − 2i 4 + 3i 8   , B =     1 2 − 5i −8 − i 4 + 3i 2 + 5i 3 6 + 2i 1 + 13i −8 + i 6 − 2i 7 −3 − 2i 4 − 3i 1 − 13i −3 + 2i 2     L ¨osung: BH = B 13 1. Grundvoraussetzung f ¨ur die LR-Zerlegung 1.4 Skalarprodukt Definition 1.39 Ein Skalarprodukt ist eine Abbildung ⟨·, ·⟩ : En × En → E, (v, w) ↦→ ⟨v, w⟩ = ∑n k=1 vkwk so dass: ∀u, v, w ∈ En, ∀λ ∈ E: (i) ⟨v, u + λw⟩ = ⟨v, u⟩ + λ⟨v, w⟩ (linear im zweiten Faktor) (ii) ⟨v, w⟩ = ⟨w, v⟩ (symmetrisch bzw. hermitesch) (iii) ⟨v, v⟩ ≥ 0 und ⟨v, v⟩ = 0 ⇔ v = 0 (positiv semidefinit) (iv) ⟨v + λw, u⟩ = ⟨v, u⟩ + λ⟨w, u⟩ (bilinear bzw. sesquilinear) Definition 1.40 Die L¨ange oder euklidische Norm eines Vektors x ∈ En ist die nichtnegative reelle Zahl ∥x∥ definiert durch ∥x∥ := √⟨x, x⟩ = √xH x = √ n ∑ k=1 |xk|2. Bemerkung 1.41 Seien x, y ∈ En, α ∈ E, dann gilt: ⟨x, y⟩ = ∥x∥ ∥y∥ cos(α). Bemerkung 1.42 Beim Standardskarprodukt gilt das folgende: ⟨x, y⟩ = xHy ∀x, y ∈ En. Definition 1.43 Eine Matrix A ∈ En×n heisst symmetrisch, falls gilt: A = AH. Beispiel 1.44 (zum Skalarprodukt) Sei V := R2 und betrachte die Abbildung V × V → R (v, w) ↦→ ⟨v, w⟩ := vT ( 3 −1 −1 3 ) w Zeige oder widerlege das ⟨v, w⟩ ein Skalarprodukt ist. L ¨osung: Beweis: 14 1.4. Skalarprodukt Bilinearit¨at: • Linearit¨at im zweiten Argument: Seien v1, v2, w ∈ V und λ ∈ R, dann gilt wegen der Linearit¨at von Matrix-Multiplikationen ⟨w, λv1 + v2⟩ = wT ( 3 −1 −1 3 ) (λv1 + v2) = wT (( 3 −1 −1 3 ) λv1 + ( 3 −1 −1 3 ) v2 ) = λwT ( 3 −1 −1 3 ) v1 + wT ( 3 −1 −1 3 ) v2 = λ⟨w, v1⟩ + ⟨w, v2⟩. Also ist die Abbildung linear im zweiten Argument. • Die Sesquilinearit¨at im ersten Argument beweist man analog. Symmetrie: Sei v, w ∈ V. Da ⟨v, w⟩ eine reelle Zahl ist und reelle Zahlen durch Transponierung nicht ver¨andert werden, gilt ⟨v, w⟩ = ⟨v, w⟩T = wT ( 3 −1 −1 3 ) v = ⟨w, v⟩. Hierbei haben wir verwendet, dass die mittlere Matrix gleich ihrer Transpo- nierten ist, und dass f ¨ur Matrizen A, B, C von entsprechender Gr¨osse gilt (ABC)T = CT BT AT. Positive Definitheit: Sei x := (x1, x2)T ∈ V. Dann gilt ⟨x, x⟩ = 3x2 1 − 2x1x2 + 3x2 2 = (x1 − x2)2 + 2x2 1 + 2x2 2. Da alle Summanden nicht-negativ sind, ist deren Summe auch nicht-negativ. Somit ist positive Semidefinitheit gezeigt. Desweiteren ist der Ausdruck ge- nau dann Null, wenn jeder einzelne Summand verschwindet. Das ergibt die Bedingungen x1 = x2, x1 = 0 x2 = 0, welche ausschliesslich f ¨ur den Nullvektor allesamt erf ¨ullt sind. Somit ist die Abbildung sogar positiv definit. □ 15 1. Grundvoraussetzung f ¨ur die LR-Zerlegung 1.5 Orthogonale und uni¨are Matirzen Definition 1.45 • Eine komplexe n × n - Matrix A heisst unit¨ar, falls AH A = AAH = 1. • Eine reelle n × n - Matrix A heisst orthogonal, falls AT A = AAT = 1. Satz 1.46 Sind A, B ∈ En×n unit¨are (bzw. orthogonale) Matrizen, so gilt: (i) A ist regul¨ar (ii) A−1 = AH (bzw. A−1 = AT) (iii) A−1 ist unit¨ar (orthogonal) (iv) AB ist unit¨ar (orthogonal) Definition 1.47 Das Kronecker-Delta ist definiert durch: δij = {1, i = j 0, i ̸= j Definition 1.48 (Einheitsvektoren) e1 :=      1 0 ... 0      , e2 :=        0 1 0 ... 0        , ei :=             0 ... 0 1 0 ... 0             , en :=      0 ... 0 1      Beispiel 1.49 • ⟨e1, e2⟩ = 0 • ⟨e1, e1⟩ = 1 Definition 1.50 (Orthonormal) Seien a, b ∈ En. Die Vektoren a, b sind orthonormal, falls folgenden Bedingungen erf ¨ullt sind: (i) Die Vektoren sind normiert, also es gilt: ∥a∥ = 1 bzw. ∥b∥ = 1. 16 1.5. Orthogonale und uni¨are Matirzen (ii) Die Vektoren sind orthogonal, also es gilt: ⟨a, b⟩ = {1, a = b 0, a ̸= b Bemerkung 1.51 F ¨ur eine orthogonale Matrix A ∈ En×n mit der Form A = (a1|...|an) sind die Spaltenvektoren a1, ..., an paarweise orthonormal. 17 1. Grundvoraussetzung f ¨ur die LR-Zerlegung 1.6 Cauchy-Schwarz Definition 1.52 Cauchy-Schwarz Ungleichung (C.S.): |⟨v, w⟩| ≤ ∥v∥ ∥w∥, mit Gleichheit ⇔ v, w linear abh¨angig, ∀u, v ∈ En. Beweis: Seien u, v ∈ En beliebig, sei ⟨·, ·⟩ ein Skalarprodukt auf En. (i) ⟨u, v⟩ = 0: (dieser Schritt wird in Wikipedia als trivial bezeichnet) • u = 0 oder v = 0 ⇒ es folgt die Gleichheit: ⟨u, v⟩ = ∥u∥ ∥v∥ = 0 • u ̸= 0, v ̸= 0 ⇒ C.S. gilt, da ∥u∥ > 0, ∥v∥ > 0 18 1.6. Cauchy-Schwarz (ii) ⟨u, v⟩ ̸= 0: Sei u ̸= 0, v ̸= 0. Sei λ ∈ C gegeben mit λ = ⟨v,u⟩ ∥v∥2 = ⟨u,v⟩ ∥v∥2 . 0 ≤ ⟨u − λv, ⟨u − λv⟩ = ⟨u, u − λv⟩ − λ⟨v, u − λv⟩ = ⟨u, u⟩ − λ⟨u, v, ⟩ − λ(⟨v, u⟩ + λ⟨v, v⟩) = ⟨u, u⟩ − λ⟨u, v, ⟩ − λ⟨v, u⟩ + λλ⟨v, v⟩ = ⟨u, u⟩ − λ⟨u, v, ⟩ − λ⟨v, u⟩ + |λ|2⟨v, v⟩ = ⟨u, u⟩ − ⟨u, v⟩ ∥v∥2 ⟨u, v⟩ − ⟨v, u⟩ ∥v∥2 ⟨v, u⟩ + ⟨u, v⟩2 ∥v∥4 ⟨v, v⟩ = ⟨u, u⟩ − ⟨u, v⟩2 ∥v∥2 − ⟨v, u⟩2 ∥v∥2 + ⟨u, v⟩2 ∥v∥4 ⟨v, v⟩ = ∥u∥2 − ⟨u, v⟩2 ∥v∥2 − ⟨v, u⟩2 ∥v∥2 + ⟨u, v⟩2 ∥v∥4 ∥v∥2 = ∥u∥2 − ⟨u, v⟩2 ∥v∥2 − ⟨v, u⟩2 ∥v∥2 + ⟨u, v⟩2 ∥v∥2 = ∥u∥2 − ⟨v, u⟩2 ∥v∥2 ⇔ ⟨v, u⟩2 ∥v∥2 = ∥u∥2 ⇔ ⟨v, u⟩2 = ∥v∥2 ∥u∥2 ⇒ |⟨v, u⟩| = ∥v∥ ∥u∥ □ 19 1. Grundvoraussetzung f ¨ur die LR-Zerlegung 1.7 Invertierbare Matrizen Definition 1.53 Eine Matrix A ∈ En×n heisst invertierbar ⇐⇒ ∃!B ∈ En×n, so dass AB = BA = 1. 1 :=       1 0 · · · 0 0 . . . . . . ... ... . . . . . . 0 0 · · · 0 1       = ∧ Einheitsmatrix B heisst dann Inverse von A und wird meistens mit B = A−1 bezeichnet. Bemerkung 1.54 (Eigenschaften) Sei A, B ∈ En×n, dann gilt: (i) Falls A−1 existiert, dann ist A−1 eindeutig (ii) (A−1)−1 = A (iii) (AB)−1 = B−1 A−1 (iv) (AH)−1 = (A−1)H (v) (AT)−1 = (A−1)T Definition 1.55 Eine Matrix A ∈ En×n heisst symmetrisch/hermitesch, falls gilt: A = AH. Bemerkung 1.56 Sei A, B ∈ En×n, dann gilt (AB)H = BH AH (bzw. (AB)T = BT AT). Satz 1.57 Sei A ∈ En×n, dann gilt: A ist invertierbar ⇐⇒ A ist regul¨ar ⇐⇒ ∀b ∈ En, LGS Ax = b : ∃!x ∈ En s.d. x = A−1b ⇐⇒ Rang(A) = n (”A hat vollen Rang”) Definition 1.58 Ist m eine beliebige nat ¨urliche Zahl, 1 ≤ i, j ≤ m mit i ̸= j, λ ∈ E und A ∈ Em×m, so nennt man die Matrizen Pij, Si(λ), Eij(λ) ∈ Em×m Elementarmatrizen. 20 1.7. Invertierbare Matrizen (i) Zeile i mit Zeile j vertauschen, multipliziere von links die Permutationsmatrix Pij A: Pij = i-te Spalte j-te Spalte                                         1 . . . 1 0 1 i-te Zeile 1 . . . 1 1 0 j-te Zeile 1 . . . 1 Bemerkung: F ¨ur Permutationsmatrizen gilt: PT ij = P−1 ij = Pij. (ii) Zeile i mit λ ̸= 0 multiplizieren, multipliziere von links Si(λ)A: Si(λ) = i-te Spalte                       1 . . . 1 λ i-te Zeile 1 . . . 1 21 1. Grundvoraussetzung f ¨ur die LR-Zerlegung (iii) Zeile i durch (Zeile i + λ· Zeile j) erstetzten, multipliziere von links Eij(λ)A: Eij(λ) = i-te Spalte j-te Spalte                                         1 . . . 1 1 λ i-te Zeile 1 . . . 1 0 1 j-te Zeile 1 . . . 1 Bemerkung 1.59 Oben haben wir gesehen wie man Zeilenumformungen mit Links-Multiplikation von Elementarmatrizen macht. Wenn wir nun Pij, Si(λ), Eij(λ) ∈ Em×m von rechts multiplizieren, z.B. APij, dann erhalten wir Spaltenumformungen. Beispiel 1.60 Sei A ∈ R4×4 gegeben. A =     1 −2 −2 10 −3 8 1 0 1 0 5 −5 −9 5 −3 15     Vertausche die zweite mit der vierten Zeile: P24 A =     1 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0         1 −2 −2 10 −3 8 1 0 1 0 5 −5 −9 5 −3 15     =     1 −2 −2 10 −9 5 −3 15 1 0 5 −5 −3 8 1 0     . 22 1.7. Invertierbare Matrizen Kochrezept 1.61 (Berechnung der Inverse) Gegeben: A ∈ En×n Gesucht: A−1 1. Schreibe das Schema (A|1) 2. Forme A mit Hilfe von Gauss-Elimination in die Einheitsmatrix um (A|1) ⇝ · · · ⇝ (1|A−1) 3. Teste A−1 A = AA−1 = 1 Bemerkung 1.62 Sei A ∈ En×n eine Diagonalmatrix, dann gilt: A =       a11 0 · · · 0 0 . . . . . . ... ... . . . . . . 0 0 · · · 0 ann       =⇒ A−1 =       1 a11 0 · · · 0 0 . . . . . . ... ... . . . . . . 0 0 · · · 0 1 ann       . Beispiel 1.63 Gegeben: A ∈ R3×3: A =   3 2 6 1 1 3 −3 −2 −5   Gesucht: A−1 L ¨osung: ( )3 2 6 1 0 0 1 1 3 0 1 0 −3 −2 −5 0 0 1 (ii)−l21·(i) ⇝     3 2 6 1 0 0 0 1 3 1 − 1 3 1 0 −3 −2 −5 0 0 1 (iii)−l31·(i) ⇝     3 2 6 1 0 0 0 1 3 1 − 1 3 1 0 0 0 1 1 0 1 (ii)− 1 1 ·(iii) ⇝     3 2 6 1 0 0 0 1 3 0 − 4 3 1 −1 0 0 1 1 0 1 (i)− 6 1 ·(iii) ⇝     3 2 0 −5 0 −6 0 1 3 0 − 4 3 1 −1 0 0 1 1 0 1 (i)− 2 1/3 ·(ii) ⇝     3 0 0 3 −6 0 0 1 3 0 − 4 3 1 −1 0 0 1 1 0 1 23 1. Grundvoraussetzung f ¨ur die LR-Zerlegung (i)· 1 3⇝     1 0 0 1 −2 0 0 1 3 0 − 4 3 1 −1 0 0 1 1 0 1 (ii)·3 ⇝ ( )1 0 0 1 −2 0 0 1 0 −4 3 −3 0 0 1 1 0 1 =⇒ A−1 =   1 −2 0 −4 3 −3 1 0 1   24 Kapitel 2 LR-Zerlegung 2.1 LR-Zerlegung (engl. LU decomposition) Idee. Bei grossen Gleichungssystemen Ax = b mit A ∈ En×n und b, x ∈ En ist die L¨osung x meist nicht einfach zu finden. Daher wollen wir A gerne in eine einfachere Form von Dreiecksmatrizen zerlegen, d.h. Wir suchen Matrizen P, L, R ∈ En×n so dass PA = LR, P = ∧ ist eine Permutationsmatrix (enth¨alt nur 1 und 0) L = ∧ ist eine linke Dreiecksmatrix (engl. lower triangular matrix) R (engl. U) = ∧ ist eine rechte Dreiecksmatrix (engl. upper triangular matrix) L =       1 0 · · · 0 ∗ . . . . . . ... ... . . . . . . 0 ∗ · · · ∗ 1       R =       ∗ · · · · · · ∗ 0 . . . . . . ... ... . . . . . . 0 · · · 0 ∗       Bemerkung 2.1 Seien A, P, L, R ∈ En×n. P speichert die Zeilenvertauschungen, die man f ¨ur die Pivotisierung braucht, L speichert die Umformungsschritte, um A in Zeilenstufen- form zu verwandeln, R ist die Zeilenstufenform von A. Bemerkung 2.2 Seien A, P, L, R ∈ En×n und b, c, x ∈ En. Das anf¨angliche System Ax = b l¨asst sich nun einfacher l¨osen: 1. L¨ose Lc = Pb nach c durch Vorw¨artseinsetzen, d.h. (L|Pb) von oben nach unten ”Gaussen”. 2. L¨ose Rx = c nach x durch R ¨uckw¨artseinsetzen, d.h. (R|c) von untern nach oben ”Gaussen”. 25 2. LR-Zerlegung Bemerkung 2.3 Dass die Beziehung aus der obigen Bemerkung f ¨ur A, P, P−1, L, R ∈ En×n die richtige L¨osung x ∈ En liefert, sieht man auch daraus, dass Ax = P−1PAx = P−1LRx = P−1Lc = P−1Pb = b. Satz 2.4 F ¨ur eine regul¨are, quadratische Matrix A ∈ En×n existiert eine LR-Zerlegung, d.h. ∃P, L, R ∈ En×n : PA = LR. 26 2.1. LR-Zerlegung (engl. LU decomposition) Kochrezept 2.5 (LR-Zerlegung light) (d.h. ohne Pivotisierung): Annahme: keine Zeilenvertauschungen n¨otig und A hat keine Nullspalte Gegeben: A ∈ En×n Gesucht: L ∈ En×n, R ∈ En×n so dass gilt: A = LR 1. Schreibe das Schema (A|1) 2. Forme A mit Hilfe von Gauss-Elimination (OHNE Zeilenvertauschungen!) um bis ihr die Zeilenstufenform erreicht habt. Dies tut ihr alles nur auf der linken Seite der erweiterten Koeffizientenmatrix. 3. Auf der rechten Seite schreibt ihr die Verh¨altnisse auf: lij := ai1 ajj , L =       1 0 · · · 0 l21 . . . . . . ... ... . . . . . . 0 ln1 · · · lnn−1 1       Diese Schritte sehen wie folgt aus: (A|1) ⇝ · · · ⇝ (R|L) mit mehr Details: ( )a11 a12 a13 1 0 0 a21 a22 a23 0 1 0 a31 a32 a33 0 0 1 (ii)−l21·(i) ⇝ ( )a11 a12 a13 1 0 0 a21 − l21a11 a22 − l21a12 a23 − l21a13 l21 1 0 a31 a32 a33 0 0 1 (iii)−l31·(i) ⇝ ( )a11 a12 a13 1 0 0 0 ̃a22 ̃a23 l21 1 0 a31 − l31a11 a32 − l31a12 a33 − l31a13 l31 0 1 ⇝ ( )a11 a12 a13 1 0 0 0 ̃a22 ̃a23 l21 1 0 0 ̃a32 ̃a33 l31 0 1 ⇝ · · · ⇝ ( )a11 a12 a13 1 0 0 0 ̃a22 ̃a23 l21 1 0 0 0 ̂a33 l31 l32 1 27 2. LR-Zerlegung =⇒ R =  a11 a12 a13 0 ̃a22 ̃a23 0 0 ̂a33   , L =   1 0 0 l21 1 0 l31 l32 1   Nun gilt: A =  a11 a12 a13 a21 a22 a23 a31 a32 a33   =   1 0 0 l21 1 0 l31 l32 1    a11 a12 a13 0 ̃a22 ̃a23 0 0 ̂a33   = LR Beispiel 2.6 Sei A ∈ R3×3 und b ∈ Rn gegeben. A =  1 2 4 3 8 14 2 6 13   , b =   35 129 109   (i) Gesucht: L, R ∈ R3×3 : A = LR. L ¨osung: ( )1 2 4 1 0 0 3 8 14 0 1 0 2 6 13 0 0 1 (ii)−l21·(i) ⇝ ( )1 2 4 1 0 0 0 2 2 3 1 0 2 6 13 0 0 1 (iii)−l31·(i) ⇝ ( )1 2 4 1 0 0 0 2 2 3 1 0 0 2 5 2 0 1 (iii)−l32·(ii) ⇝ ( )1 2 4 1 0 0 0 2 2 3 1 0 0 0 3 2 1 1 =⇒ A =  1 2 4 3 8 14 2 6 13   =  1 0 0 3 1 0 2 1 1    1 2 4 0 2 2 0 0 3   = LR. (ii) Gesucht: x ∈ Rn : Ax = b. L ¨osung: Wir gehen vor, wie in der Bemerkung ganz oben auf Seite 4 vorgeschlagen: 1. L¨ose Lc = b:  1 0 0 35 3 1 0 129 2 1 1 109   (ii)− 3 1 ·(i) ⇝  1 0 0 35 0 1 0 24 2 1 1 109   (iii)− 2 1 ·(i) ⇝  1 0 0 35 0 1 0 24 0 1 1 39   (iii)− 1 1 ·(ii) ⇝  1 0 0 35 0 1 0 24 0 0 1 15   ⇒ c =  35 24 15   28 2.1. LR-Zerlegung (engl. LU decomposition) 2. L¨ose Rx = c:  1 2 4 35 0 2 2 24 0 0 3 15   (ii)− 2 3 ·(iii) ⇝  1 2 4 35 0 2 0 14 0 0 3 15   (i)− 4 3 ·(iii) ⇝  1 2 0 15 0 2 0 14 0 0 3 15   (i)− 2 2 ·(ii) ⇝  1 0 0 1 0 2 0 14 0 0 3 15   (ii)· 1 2⇝  1 0 0 1 0 1 0 7 0 0 3 15   (iii)· 1 3⇝  1 0 0 1 0 1 0 7 0 0 1 5   ⇒ x =  1 7 5   =⇒ L =     x1 x2 x3   ∣ ∣ ∣ ∣ ∣ ∣  1 7 5      29 2. LR-Zerlegung Kochrezept 2.7 (LR-Zerlegung full) (d.h. mit Pivotisierung): Gegeben: A ∈ En×n Gesucht: P ∈ Rn×n, L ∈ En×n, R ∈ En×n so dass gilt: PA = LR 1. Schreibe das Schema (1|A|1). Links merkt ihr euch die Zeilenvertauschun- gen, in der Mitte ‘Gausst” ihr und rechts merkt ihr euch die Verh¨altnisse. 2. Pivotisiere in der aktuellen Spalte j, d.h. bestimme den Index ip ∈ {j, j + 1, ..., n} mit |aip j| = max i∈{j,j+1,...,n} |aij|. Falls |aip j| = 0 breche ab, es existiert keine LR-Zerlegung. Sonst berechne Pij A = ̃A in der Mitte, wobei Pij ∈ Rn×n von links an die Matrix links im Schema multipliziert wird und auf der rechten Seite m ¨usst ihr entsprechend die Verh¨altnisse vertauschen (ausser im ersten Schritt). • Falls zu Beginn pivotisieren m ¨usst, sieht das Resultat vom Schritt wie folgt aus: (1|A|1) ⇝ (Pij| ̃A|1) • Sonst, sieht das Resultat vom Schritt wie folgt aus: ( )0 1 0 a11 a12 a13 1 0 0 1 0 0 0 0 ̃a23 l21 1 0 0 0 1 0 ̃a32 ̃a33 l31 0 1 pivotisiere ⇝ ( )0 1 0 a11 a12 a13 1 0 0 0 0 1 0 ̃a32 ̃a33 l31 1 0 1 0 0 0 0 ̃a23 l21 0 1 3. Forme A mit Hilfe von Gauss-Elimination (OHNE Zeilenvertauschungen!) um bis ihr die Zeilenstufenform erreicht habt. Dies tut ihr alles nur auf der linken Seite der erweiterten Koeffizientenmatrix. Taucht bei der Berechnung wieder ein Nulleintrag auf in der Hauptdiagonale, dann m ¨usst ihr wieder pivotisieren, also wiederholt den zweiten Schritt. Dann weiter ”Gaussen”. 30 2.1. LR-Zerlegung (engl. LU decomposition) 4. Auf der rechten Seite schreibt ihr die Verh¨altnisse auf: lij := ai1 ajj , L =       1 0 · · · 0 l21 . . . . . . ... ... . . . . . . 0 ln1 · · · lnn−1 1       Die Schritte sehen wie folgt aus: (1|A|1) ⇝ · · · ⇝ (P|R|L) mit mehr Details: ( )1 0 0 a11 a12 a13 1 0 0 0 1 0 a21 a22 a23 0 1 0 0 0 1 a31 a32 a33 0 0 1 (ii)−l21·(i) ⇝ ( )1 0 0 a11 a12 a13 1 0 0 0 1 0 a21 − l21a11 a22 − l21a12 a23 − l21a13 l21 1 0 0 0 1 a31 a32 a33 0 0 1 (iii)−l31·(i) ⇝ ( )1 0 0 a11 a12 a13 1 0 0 0 1 0 0 0 ̃a23 l21 1 0 0 0 1 a31 − l31a11 a32 − l31a12 a33 − l31a13 l31 0 1 ⇝ ( )1 0 0 a11 a12 a13 1 0 0 0 1 0 0 0 ̃a23 l21 1 0 0 0 1 0 ̃a32 ̃a33 l31 0 1 pivotisiere ⇝ ( )1 0 0 a11 a12 a13 1 0 0 0 0 1 0 ̃a32 ̃a33 l31 1 0 0 1 0 0 0 ̃a23 l21 0 1 =⇒ PA =  1 0 0 0 0 1 0 1 0    a11 a12 a13 a21 a22 a23 a31 a32 a33   =   1 0 0 l31 1 0 l21 0 1    a11 a12 a13 0 ̃a32 ̃a33 0 0 ̂a23   = LR 31 2. LR-Zerlegung Beispiel 2.8 Sei A ∈ R3×3 und b ∈ Rn gegeben. A =   3 1 6 −6 0 −16 0 8 −17   , b =   40 −86 −29   (i) Gesucht: L, P, R ∈ R3×3 : PA = LR. L ¨osung: ( )1 0 0 3 1 6 1 0 0 0 1 0 −6 0 −16 0 1 0 0 0 1 0 8 −17 0 0 1 pivotisiere:P21 ⇝ ( )0 1 0 −6 0 −16 1 0 0 1 0 0 3 1 6 0 1 0 0 0 1 0 8 −17 0 0 1 (ii)−l21·(i) ⇝     0 1 0 −6 0 −16 1 0 0 1 0 0 0 1 −2 − 1 2 1 0 0 0 1 0 8 −17 0 0 1 pivotisiere:P32 ⇝     0 1 0 −6 0 −16 1 0 0 0 0 1 0 8 −17 0 1 0 1 0 0 0 1 −2 − 1 2 0 1 (iii)−l32·(ii) ⇝     0 1 0 −6 0 −16 1 0 0 0 0 1 0 8 −17 0 1 0 1 0 0 0 0 1 8 − 1 2 1 8 1 =⇒ PA =  0 1 0 0 0 1 1 0 0     3 1 6 −6 0 −16 0 8 −17   =   1 0 0 0 1 0 − 1 2 1 8 1    −6 0 −16 0 8 −17 0 0 1 8   = LR (ii) Gesucht: x ∈ Rn : Ax = b. L ¨osung: Wie gehen vor, wie in der Bemerkung ganz oben auf der Seite 4 vorgeschlagen: 1. L¨ose Lc = Pb: Pb =  0 1 0 0 0 1 1 0 0     40 −86 −29   =  −86 −29 40   32 2.1. LR-Zerlegung (engl. LU decomposition)   1 0 0 −86 0 1 0 −29 − 1 2 1 8 1 40   (iii)− −1/2 1 ·(i) ⇝  1 0 0 −86 0 1 0 −29 0 1 8 1 −3   (iii)− 1/8 1 ·(ii) ⇝  1 0 0 −86 0 1 0 −29 0 0 1 5 8   ⇒ c =  −86 −29 5 8   2. L¨ose Rx = c:  −6 0 −16 −86 0 8 −17 −29 0 0 1 8 5 8   (ii)− −17 1/8 ·(iii) ⇝  −6 0 −16 −86 0 8 0 56 0 0 1 8 5 8   (i)− −16 1/8 ·(iii) ⇝  −6 0 0 −6 0 8 0 56 0 0 1 8 5 8   (i)·(− 1 6 ) ⇝  1 0 0 1 0 8 0 56 0 0 1 8 5 8   (ii)· 1 8⇝  1 0 0 1 0 1 0 7 0 0 1 8 5 8   (iii)·8 ⇝  1 0 0 1 0 1 0 7 0 0 1 5   ⇒ x =  1 7 5   =⇒ L =     x1 x2 x3   ∣ ∣ ∣ ∣ ∣ ∣  1 7 5      33 Kapitel 3 Grundvoraussetzung f¨ur die Vektorr¨aume und linearen Abbildung 3.1 Vektorr¨aume Definition 3.1 Eine Menge E zusammen mit zwei Verkn ¨upfungen + : E × E → E, (x, y) ↦→ x + y (Addition) · : E × E → E, (x, y) ↦→ x · y (Multiplikation) heisst K ¨orper, wenn ∀x, y, z ∈ E folgendes gilt: K1 E zusammen mit der Addition + ist eine abelsche Gruppe. (Ihr neutrales Element wird mit 0, das zu a ∈ E inverse Element mit −a bezeichnet; vgl. Diskrete Mathematik, 5. Algebra, Definition 5.7 und 5.8: ⟨E; +⟩ is an abelian group): (i) (Assoziativit¨at) (x + y) + z = x + (y + z) (ii) (Neutrales Element) ∃e ∈ E : x + e = e + x = x (iii) (Inverses Element) ∃x′ ∈ E : x + x′ = x′ + x = e (iv) (Abelsch ⇔ Kommutativit¨at) x + y = y + x K2 Bezeichnet E∗ := E \\ {0}, so gilt f ¨ur x, y ∈ E∗ auch x · y ∈ E∗, und E∗ zusammen mit der so erhaltenen Multiplikation ist eine abelsche Gruppe. (Ihr neutrales Element wird mit 1, das zu x ∈ E∗ inverse Element mit x−1 oder 1/x bezeichnet. Man schreibt y/x = x−1y = yx−1. Vgl. Diskrete Mathematik, 5. Algebra, Definition 5.7 und 5.8: ⟨E; ·⟩ is an abelian group) (vgl. Diskrete Mathematik, 5. Algebra: Definition 5.26 und Theorem 5.23) 35 3. Grundvoraussetzung f ¨ur die Vektorr ¨aume und linearen Abbildung Bemerkung 3.2 Meistens werden wir mit den K¨orpern R oder C arbeiten. Ein weiterer K¨orper der f ¨ur uns Informatiker bekannt ist, ist der kleinste endliche K¨orper Z2, der nur {0, 1} enth¨alt. Definition 3.3 Ein Vektorraum V ¨uber E (oder auch E-Vektorraum; VR) ist eine nichtleere Menge V zusammen mit zwei Operationen: + : V × V → V, (x, y) ↦→ x + y (Addition) · : E × V → V, (α, x) ↦→ αy (Skalarmultiplikation) so dass ∀x, y, z ∈ V und ∀α, β ∈ E gilt: V1 V zusammen mit der Addition ist eine abelsche (kommutative) Gruppe (das neutrale Element heisst Nullvektor, es wird mit 0, und das Negative wird mit −x bezeichnet; vgl. Diskrete Mathematik, 5. Algebra: ⟨V; +⟩ is an abelian group): (i) (Assoziativit¨at) (x + y) + z = x + (y + z) (ii) (Neutrales Element) ∃e ∈ V : x + e = e + x = x (iii) (Inverses Element) ∃x′ ∈ V : x + x′ = x′ + x = e (iv) (Abelsch ⇔ Kommutativit¨at) x + y = y + x V2 Die Multiplikation mit Skalaren muss in folgender Weise mit den anderen Ver- kn ¨upfungen vertr¨aglich sein: (i) (Distributivit¨at I) (α + β)x = αx + βx (ii) (Distributivit¨at II) α(x + y) = αx + βy (iii) (Assoziativit¨at) (αβ)x = α(βx) (iv) (Vertr¨aglichkeit mit 1) 1x = x Beispiel 3.4 1. n-dimensionale Vektoren bilden ¨uber E einen Vektorraum. 2. m × n-Matrizen bilden ¨uber E einen Vektorraum. 3. Pn := {Polynome in einer Variablen mit Koeffizienten in E von max. Grad n} bilden ¨uber E einen Vektorraum. 3.2 Basen, Dimensionen und lineare (Un-)Abh¨anigkeit Bemerkung 3.5 F ¨ur diesen Abschnitt werden wir folgendes annehmen. Sei V ein Vektorraum ¨uber einem K¨orper K und eine Familie (Menge) (vi)i∈I von Vektoren vi ∈ V. Ist I = {1, ..., r}, so hat man Vektoren v1, ..., vr. 36 3.3. Lineare Abbildungen Definition 3.6 F ¨ur allgemeines I definiert man spanE(vi)i∈I als die Menge all der v ∈ V die sich aus einer (von v abh¨angigen) endlichen Teilfa- milie (Teilmenge) von (vi)i∈I linear kombinieren lassen. Man nennt spanE(vi)i∈I den von der Familie (Menge) aufgespannten (oder er- zeugten) Raum. F ¨ur eine endliche Familie (v1, ..., vr) verwendet man oft die sugge- stivere Notation: spanE(v1, ..., vr) : = Ev1 + ... + Evn = {v ∈ V | ∃λ1, ..., λr ∈ E mit v = λ1v1 + ... + λrvr}. Bemerkung 3.7 Die folgenden Notationen sind ¨aquivalent: spanE(v1, ..., vr) ⇔ spanE{v1, ..., vr} ⇔ ⟨v1, ..., vr⟩. Bemerkung 3.8 Die Vektoren v1, ..., vr in der Defintion von spanE(v1, ..., vr) oben heissen Erzeu- gendensystem von V, wenn ∀a ∈ V als Linearkombination der Vektoren v1, ..., vr dargestellt werden kann. Bemerkung 3.9 Falls klar ist, welcher K¨orper gemeint ist, schreibt man nur span statt spanE. Bemerkung 3.10 Sei V ein E-Vektorraum und (vi)i∈I eine Familie (Menge) von Elementen aus V mit I = {1, ..., r}. Dann gilt: (i) span(v1, ..., vr) ist ein Untervektorraum (ii) Ist W ⊂ V ein Untervektorraum und gilt vi ∈ W f ¨ur alle i ∈ {1, ..., r} so ist span(v1, ..., vr) ⊂ W. Beispiel 3.11 • span(1, x, x2, x3) = P3 • span(x3 + x2, x2 − 1, x, x − 1, 1000) = P3 ⇒ ein Erzeugendensystem ist nicht eindeutig. 3.3 Lineare Abbildungen Definition 3.12 Eine Abbildung f : X → Y heisst injektiv, falls ∀x1, x2 ∈ X : x1 ̸= x2 ⇒ f (x1) ̸= f (x2). (In Worten: Verschiedene Elemente aus X werden auf verschiedene Bilder in Y abgebildet.) 37 3. Grundvoraussetzung f ¨ur die Vektorr ¨aume und linearen Abbildung Definition 3.13 Eine Abbildung f : X → Y heisst surjektiv, falls ∀y ∈ Y ∃x ∈ X : f (x) = y. (In Worten: Jedes Element aus Y wird von f ”getroffen”.) Definition 3.14 Eine Abbildung f : X → Y heisst bijektiv, falls ∀y ∈ Y ∃!x ∈ X : f (x) = y. (In Worten: Jedes Element aus Y wird von f genau einmal ”getroffen”.) Definition 3.15 Eine Abbildung F : V → W zwischen E-Vektorr¨aumen V und W heisst linear (genauer Homomorphismus von E-Vektorr¨aumen), wenn ∀v, w ∈ V und ∀λ ∈ E: L1 F(v + w) = F(v) + F(w) L2 F(λv) = λF(v) Diese beiden Bedingungen kann man zusammenfassen zu einer: L F(v + λw) = F(v) + λF(w). Notation. F ¨ur F : V → W linear ist F ∈ Hom(V, W). Bemerkung 3.16 Es ist ¨ublich, den Begriff Homomorphismus zu versch¨arfen: (i) F ∈ Hom(V, W) und bijektiv ⇔ Isomorphismus (Notation: V ̃=W) (ii) F ∈ Hom(V, W) und V = W ⇔ Endomorphismus (Notation: F ∈ End(V)) (iii) F ∈ End(V) und bijektiv ⇔ Automorphismus Zudem gilt: (i) ⇔ ∃G : W → V linear, so dass F ◦ G = idW, G ◦ F = idV, d.h. ∀w ∈ W : F(G(w)) = w ∀v ∈ V : G(F(v)) = v Definition 3.17 Zu jeder Basis B = {v1, ..., vn} von V gibt es genau einen Isomorphismus: ϕB : En → V, (x1, ..., xn) ↦→ ϕB(x1, ..., xn) = n ∑ k=1 xkvk = x1v1 + ... + xnvn mitϕB(ei) = vi. (In Worten: ϕB ordnet x seinen Koordinaten bez ¨uglich der Basis B zu.) 38 Kapitel 4 Vektorr¨aume und lineare Abbildungen 4.1 Untervektorr¨aume Definition 4.1 Sei V ein Vektorraum, U ⊆ V, U ̸= {}. U heisst Untervektorraum, Unterraum, linearer Teilraum (UVR), falls sie bez ¨uglich Addition und skalarer Multiplikation abgeschlossen ist, d.h. wenn ∀x, y ∈ U und ∀α ∈ E gilt: U1 x + y ∈ U U2 αx ∈ U. Bemerkung 4.2 Jeder Untervektorraum U enth¨ahlt den Nullvektor, d.h. U0 0 ∈ U. Satz 4.3 Jeder Untervektorraum ist ein Vektorraum. Beispiel 4.4 Zu zeigen: W =     x1 x2 x3   ⊆ R3 ∣ ∣ ∣ ∣ ∣ ∣ x1 + x2 + x3 = 0    ist ein Vektorraum. Bemerkung Wir haben zwei Optionen: 1. ¨Uberpr ¨ufe ob V1 und V2 von der Vektorraum Definition erf ¨ullt sind 2. Verwende den Satz von oben und zeige nur U1 und U2 Beweis: Wir f ¨uhren den Beweis mit der zweiten Option durch. Also gen ¨ugt es nach dem Satz zu zeigen, dass W ein Untervektorraum ist. Seien x =  x1 x2 x3   , y =  y1 y2 y3   ∈ W, λ ∈ E 39 4. Vektorr ¨aume und lineare Abbildungen U0  0 0 0   ∈ W, da 0 + 0 + 0 = 0, (U0 folgt trivialerweise) U1 x + y =  x1 + y1 x2 + y2 x3 + y3   ∈ W, da (x1 + y1) + (x2 + y2) + (x3 + y3) = (x1 + x2 + x3) ︸ ︷︷ ︸ x1+x2+x3=0 + (y1 + y2 + y3) ︸ ︷︷ ︸ y1+y2+y3=0 = 0 U2 λx =  λx1 λx2 λx3   ∈ W, da λx1 + λx2 + λx3 = λ (x1 + x2 + x3) ︸ ︷︷ ︸ x1+x2+x3=0 = 0 Oder statt, dass ihr U1 und U2 separat zeigt k¨onnt ihr auch die ‘all in one”Variante zeigen: U1/U2 x − λy =  x1 − λy1 x2 − λy2 x3 − λy3   ∈ W, da (x1 − λy1) + (x2 − λy2) + (x3 − λy3) = (x1 + x2 + x3) ︸ ︷︷ ︸ x1+x2+x3=0 −λ (y1 + y2 + y3) ︸ ︷︷ ︸ y1+y2+y3=0 = 0 □ 4.2 Basen, Dimensionen und lineare (Un-)Abh¨anigkeit Bemerkung 4.5 F ¨ur diesen Abschnitt werden wir folgendes annehmen. Sei V ein Vektorraum ¨uber einem K¨orper K und eine Familie (Menge) (vi)i∈I von Vektoren vi ∈ V. Ist I = {1, ..., r}, so hat man Vektoren v1, ..., vr. Definition 4.6 Seien v1, ..., vr ∈ V ausgew¨ahlte Vektoren. Ein Vektor der Form x := λ1v1, ..., λrvr = r ∑ k=1 λkvk mit λ1, ..., λr ∈ E heisst Linearkombination von v1, ..., vr. Definition 4.7 v1, ..., vr ∈ V heissen linear unabh¨angig, wenn: r ∑ k=1 λkvk = λ1v1 + ... + λrvr = 0 ⇒ λ1 = ... = λr = 0 und sonst heissen sie linear abh¨angig. 40 4.2. Basen, Dimensionen und lineare (Un-)Abh¨anigkeit Bemerkung 4.8 v1, ..., vr ∈ V heissen linear unabh¨angig genau dann, wenn kein vi sich als Linear- kombination der anderen aj mit j ̸= i schreiben l¨asst. (z.B. v1 ist keine Linearkom- bination von v2, ..., vr). Beispiel 4.9 ... •  1 2 3   ist linear abh¨angig von  2 4 6  , weil 2 ·  1 2 3   =  2 4 6  . •  1 2 3   ist linear unabh¨angig von  0 2 3  . Bei komponentenweiser Multiplikation bekommt man in der ersten Koordina- te niemals 0, wenn man 2 und 3 behalten will. Definition 4.10 Sei B = (bi)i∈I ⊆ V. B heisst Basis von V, wenn V = span(B) (V wird erzeugt von B) B = (bi)i∈I (alle bi sind untereinander linear unabh¨angig). Satz 4.11 B ist ein minimales Erzeugendensystem, d.h. span(B) = V, aber span(B \\ {bi}) ̸= V, ∀bi ∈ B. Satz 4.12 B ist ein maximal linear unabh¨angige Teilmenge von V, d.h. (bi)i∈I sind linear unabh¨angig aber (bi)i∈I ∪ {v} sind nicht mehr linear unabh¨angig, ∀v ∈ V \\ B. Bemerkung 4.13 Sind B1 und B2 Basen von V, so gilt |B1| = |B2|. Jeder Vektorraum hat eine Basis. Definition 4.14 Sei B eine Basis von V. Dann ist |B| = dim(V) (= # Basisvektoren) die Dimension von V, wobei | · | = ∧ Kardinalit¨at von einer Menge ist. Bemerkung 4.15 Falls dim(V) = n, dann gilt allgemein: • Falls k < n, sind v1, ..., vk ∈ V nicht erzeugend • Falls k > n, sind v1, ..., vk ∈ V linear abh¨angig Satz 4.16 (Basisauswahlsatz) Aus jedem endlichen Erzeugendensystem eines Vektorraumes kann man eine Basis ausw¨ahlen. Insbesondere hat jeder endlich erzeugte Vektorraum eine endliche Basis. 41 4. Vektorr ¨aume und lineare Abbildungen Satz 4.17 (Basiserg¨anzungssatz) In einem endlich erzeugten Vektorraum V seien linear unabh¨angige Vektoren w1, ..., wn gegeben. Dann kann man wn + 1, ..., wr finden so dass B = {w1, ..., wn, wn + 1, ..., wr} eine Basis von V ist. Beispiel 4.18 • dim(Rn) = n = dim(Cn) • dim(Pn) = n + 1 (B(Pn) = { 1, x, x2, ..., xn ︸ ︷︷ ︸ n+1 Basisvektoren}) • dimR(C) = 2 (Basis von C ¨uber dem K¨orper R ist z.B. {1, i}) • dimC(C) = 1 (Basis von C ¨uber dem K¨orper C ist z.B. {1}) 42 4.2. Basen, Dimensionen und lineare (Un-)Abh¨anigkeit Kochrezept 4.19 (Tricks beim Rechnen) (bei Fragen betreffend Dimension, Basis, lineare Abh¨angigkeit, etc.) Gegeben: v1, ..., vk ∈ V Gesucht: dim(V), span(v1, ..., vk) linear unabh¨angig? 1. Schreibe        vT 1 ... vT k        = A in eine Matrix mit n Zeilen. 2. F ¨uhre Gauss-Elimination auf A aus bis ihr die Zeilenstufenform erreicht habt. 3. Ziehe Fazit: • Rang(A) = dim(span(v1, ..., vk)) • Rang(A) = k ⇒ v1, ..., vk ist linear unabh¨angig • Rang(A) < k ⇒ v1, ..., vk ist linear abh¨angig • Rang(A) = dim(V) ⇒ v1, ..., vk ist erzeugend • Falls Rang(A) = dim(V) = k, dann bilden v1, ..., vk also eine Basis f ¨ur Rn Beispiel 4.20 ... Zu zeigen:  2 0 1   ,  3 5 2   ,   0 1 −1   ∈ R3 bilden eine Basis in R3. Beweis: Wissen dim(R3) = 3, und wir haben 3 Vektoren. Jetzt m ¨ussen wir nur noch zeigen, dass die Vektoren linear unabh¨angig sind.  2 0 1 3 5 2 0 1 −1   (ii)−l21(i) ⇝  2 0 1 0 5 1 2 0 1 −1   (iii)−l32(ii) ⇝  2 0 1 0 5 1 2 0 0 − 9 10   = A ⇒ Rang(A) = 3 voller Rang ⇒ Vektoren sind linear unabh¨angig und bilden somit eine Basis von R3. □ Beispiel 4.21 Ist B = {1, x, 1 + x2 + x3} eine Basis von P3? Wenn ja beweise, wenn nein, erweitere zu einer Basis. 43 4. Vektorr ¨aume und lineare Abbildungen Wissen: dim(P3) = 4 B kann keine sein, weil dim(B) = 3 < 4. Behauptung: B′ = {1, x, 1 + x2 + x3, x2} ist eine Basis von P3. Beweis: 1 = ∧ e1 x = ∧ e2 x2 = ∧ e3 x3 = ∧ e4 1 + x2 + x3 = ∧ e1 + e3 + e4     1 0 0 0 0 1 0 0 1 0 1 1 0 0 1 0     ⇝ · · · ⇝     1 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1     ⇒ linear unabh¨anging, also ist B′ eine Basis von P3 □ 44 4.3. Lineare Abbildungen 4.3 Lineare Abbildungen Definition 4.22 Eine Abbildung f : X → Y heisst injektiv, falls ∀x1, x2 ∈ X : x1 ̸= x2 ⇒ f (x1) ̸= f (x2). (In Worten: Verschiedene Elemente aus X werden auf verschiedene Bilder in Y abgebildet.) Definition 4.23 Eine Abbildung f : X → Y heisst surjektiv, falls ∀y ∈ Y ∃x ∈ X : f (x) = y. (In Worten: Jedes Element aus Y wird von f ”getroffen”.) Definition 4.24 Eine Abbildung f : X → Y heisst bijektiv, falls ∀y ∈ Y ∃!x ∈ X : f (x) = y. (In Worten: Jedes Element aus Y wird von f genau einmal ”getroffen”.) Bemerkung 4.25 Seien M(F), M(G) die darstellenden Matrizen von F : V → W isomorph und G : W → V homomorph und V, W sind endlichdimensionale Vektorr¨aume, d.h. dim(V) < ∞ und dim(W) < ∞. Dann bedeutet Bijektivit¨at von F, dass • dim(V) = dim(W) • M(F) · M(G) = M(G) · M(F) = 1dim(V) ⇔ M(F) = (M(G))−1 Definition 4.26 Sei die F ∈ Hom(V, W). • Im(F) := F(V) = {F(v)|v ∈ V} ⊂ W ist ein Untervektorraum von W und heisst Bild(F) oder Im(F). • ker(F) := {v ∈ V|F(v) = 0} ⊂ V ist ein Untervektorraum von V und heisst ker(F). Satz 4.27 Sei F : V → W linear und V, W sind Vektorr¨aume. Dann gilt: (i) F(0) = 0, die Null wird immer auf die Null abgebildet (ii) F surjektiv ⇔ Im(F) = W ⇔ dim(Im(F)) = dim(W) (iii) F injektiv ⇔ ker(F) = {0} ⇔ dim(ker(F)) = 0 (iv) F ist ein Isomorphismus ⇔ dim(V) = dim(W) = rang(F) 45 4. Vektorr ¨aume und lineare Abbildungen Definition 4.28 Der Rang der linearen Abbildung F ist definiert als: rang(F) = dim(Im(F)). Bemerkung 4.29 Der Rang der linearen Abbildung F ist gleich dem Rang ihrer Abbildungsmatrix M(F). Es gilt: rang(F) = dim(Im(F)) = rang(M(F)) = rang(M(F)T) Bemerkung 4.30 Zeilenrang = Spaltenrang: rang(M(F)) = rang(M(F)T) Achtung: Im Allgemeinen gilt: Spaltenraum ̸= Zeilenraum Beispiel 4.31 Sei F : R → R, x ↦→ 5x − 1. F ist nicht linear, da F(0) = −1 ̸= 0. Satz 4.32 Sei F : V → W linear. Dann gilt: f (0) = 0. 46 4.3. Lineare Abbildungen Kochrezept 4.33 (Abbildungsmatrix) (darstellende Matrix; Spezialfall mit Standardbasis) Gegeben: V ein Vektorraum, F : (V, A) → (W, B), v ↦→ F(v) und Basis von V mit A = {a1, ..., an} und die Standardbasis von W mit B = {b1, ..., bm} Gesucht: MA B (F) 1. Berechne f ¨ur jeden Basisvektor F(ai), i ∈ {1, ..., n} 2. Erstelle MA B (F) = (F(a1), ..., F(an) ︸ ︷︷ ︸ n-Spalten ) } m-Zeilen. Wir haben die Abbildungsmatrix von F erhalten, wobei der Definitionsbereich bez ¨uglich A und Bildbereich bez ¨uglich B gegeben ist. Beispiel 4.34 Sei F = d dt : P2 → P1, p ↦→ ˙p = dp dt . (i) Zu zeigen: F ist eine lineare Abbildung. Beweis: ∀a, b ∈ P2, λ ∈ E : F(a + λb) = d dt (a + λb) = d dt a + λ d dt b = F(a) + λF(b) □ (ii) Finde die Abbildungsmatrix MB B (F) bez ¨uglich der Monombasis B = {1, t, t2}. p = λ0 + λ1t + λ2t2 ∈ P2, p = ∧  λ0 λ1 λ2   ∈ E3 ˙p = λ1 + 2λ2t ∈ P1, ˙p = ∧   λ1 2λ2 0   ∈ E3 1 = ∧ e1 t = ∧ e2 t2 = ∧ e3 ˙p(1) = ∧ ˙p(e1) =  0 0 0   , ˙p(t) = ∧ ˙p(e2) =  1 0 0   , ˙p(t2) = ∧ ˙p(e3) =  0 2 0   47 4. Vektorr ¨aume und lineare Abbildungen =⇒ MB B (F) =  0 1 0 0 0 2 0 0 0   Beispiel 4.35 Gegeben: F : R3 → R2, F(x, y, z) = ( 2x − 3y x − 2y + 6z ) Gesucht: MA B (F) bez ¨uglich A =     1 1 0   ,  1 0 1   ,  0 1 1      , B = {(1 0 ) , (0 1 )} (B ist die Standardbasis von R2) L ¨osung: F(a1) = F  1 1 0   = ( 2 · 1 − 3 · 1 1 − 2 · 1 + 6 · 0 ) = (−1 −1 ) F(a2) = F  1 0 1   = ( 2 · 1 − 3 · 0 1 − 2 · 0 + 6 · 1 ) = (2 7 ) F(a3) = F  0 1 1   = ( 2 · 0 − 3 · 1 0 − 2 · 1 + 6 · 1 ) = (−3 4 ) ⇒ MA B (F) = (−1 2 −3 −1 7 4 ) Kochrezept 4.36 (Abbildungsmatrix (darstellende Matrix; Allgemein)) Gegeben: V ein Vektorraum, F : (V, A) → (W, B), v ↦→ F(v) und Basis von V mit A = {a1, ..., an} und die Basis von W mit B = {b1, ..., bm} Gesucht: MA B (F) 1. Schreibe F(ai), i ∈ {1, ..., n} in Koordinaten von der Basis B, das heisst finde µji ∈ E, j ∈ {1, ..., m}: F(ai) = m ∑ j=1 µjibj 48 4.3. Lineare Abbildungen 2. Erstelle MA B (F) = (F(a1), ..., F(an)) =    a11 · · · a1n ... ... am1 · · · amn    Wir haben die Abbildungsmatrix von F erhalten, wobei der Definitionsbereich bez ¨uglich A und Bildbereich bez ¨uglich B gegeben ist. Beispiel 4.37 Gegeben: G : P2 → P1, p(x) = α0 + α1x + α2x2 ↦→ 2p′(x) + 1 = (2α1 + 1) + 4α2x Gesucht: MA B (G) bez ¨uglich A = {x2 + x + 1, x + 1, x} und B = {x + 1, 1} L ¨osung: Zuerst stellen wir die Abbildung und Basen bez ¨uglich der Monombasis dar: G :  α0 α1 α2   ↦→ (2α1 + 1 4α2 ) A = ∧     1 1 1   ,  1 1 0   ,  0 1 0      B = ∧ {(1 1 ) , (1 0 )} G(a1) = G  1 1 1   = (2 · 1 + 1 4 · 1 ) = (3 4 ) Be ⇒ TBe B (3 4 ) Be = (0 1 1 −1 ) (3 4 ) Be = 4 · b1 − b2 = ( 4 −1 ) B G(a2) = G  1 1 0   = (2 · 1 + 1 4 · 0 ) = (3 0 ) Be ⇒ TBe B (3 0 ) Be = (0 1 1 −1 ) (3 0 ) Be = 0 · b1 − 3 · b2 = (0 3 ) B G(a3) = G  0 1 0   = (2 · 1 + 1 4 · 0 ) = (3 0 ) Be ⇒ TBe B (3 0 ) Be = (0 1 1 −1 ) (3 0 ) Be = 0 · b1 − 3 · b2 = (0 3 ) B 49 4. Vektorr ¨aume und lineare Abbildungen =⇒ MA B (G) = ( 4 0 0 −1 3 3 ) Bemerkung 4.38 Ein Polynom p ∈ Pn ist durch die Funktionswerte p(xi) an n + 1 paarweise ver- schiedenen Punkten xi ∈ {1, ..., n} eindeutig bestimmt. Bemerkung 4.39 Seien V, U, W E-Vektorr¨aume mit dim(V) = n, dim(U) = k und dim(W) = ℓ, dann ist die Dimensionsregel f ¨ur Verkn ¨upfungen von linearen Abbildungen: M( f ) ∈ Rℓ×n, M(g) ∈ Rk×ℓ ⇒ M(g ◦ f ) = M(g) · M( f ) ∈ (”Rk×ℓ · Rℓ×n”) = Rk×n Satz 4.40 Seien V und W zwei endlichdimensionale Vektorr¨aume eines gr¨osseren Vektorraums (endlichdimensional ⇔ dim(V) = n < ∞ und dim(W) = k < ∞) und sei f : V → W linear, dann gelten die folgenden Dimensionsformeln: • dim(V + W) = dim(V) + dim(W) − dim(V ∩ W) • n = dim(V) = dim(ker( f )) + dim(Im( f )) Bemerkung 4.41 (Eigenschaften von linearen Abbildungen) Seien V, W E-Vektorr¨aume und B = {v1, ..., vn} eine Basis von V. Sei F : V → W linear. • Im(F) = span(F(v1), ..., F(vn)), d.h. F ist eindeutig definiert durch die Wer- te der Basisvektoren • Ist F injektiv und v1, ..., vn ∈ V linear unabh¨angig, dann sind F(v1), ..., F(vn) ∈ Im(F) linear unabh¨angig • dim(F) < ∞ und F injektiv ⇒ F ist bijektiv! 50 Kapitel 5 Grundvorraussetzung f¨ur die Methode der kleinsten Quadrate 5.1 Normen von linearen Abbildungen (Operatoren) und Matrizen, Konditionszahl Definition 5.1 (Norm) ∀x, y ∈ En, ∀λ ∈ E (i) Definitheit: ∥x∥ = 0 ⇔ x = 0 (ii) absolute Homogenit¨at: ∥λx∥ = |λ| · ∥x∥ (iii) Dreiecksungleichung: ∥x ± y∥ ≤ ∥x∥ + ∥y∥ Satz 5.2 (Skript: Satz 6.18) Die durch ∥F∥ := supx̸=o ∥F(x)∥ ∥x∥ definierte induzierte Operatornorm hat die folgen- den Eigenschaften: (OpN1) Sie ist positiv definit: ∥F∥ ≥ 0 (∀F ∈ L(X, Y)), ∥F∥ = 0 ⇒ F = O. (OpN2) Sie ist dem Betrage nach homogen: ∥αF∥ = |α|∥F∥ (∀F ∈ L(X, Y), ∀α ∈ E). (OpN3) Die Dreiecksungleichung gilt: ∥F + G∥ ≤ ∥F∥ + ∥G∥ (∀F, G ∈ L(X, Y)). 51 5. Grundvorraussetzung f ¨ur die Methode der kleinsten Quadrate (OpN4) F ¨ur zusammengesetzte Abbildungen gilt: ∥G ◦ F∥ ≤ ∥G∥∥F∥ (∀F ∈ L(X, Y), ∀G ∈ L(Y, Z)). (OpN5) WICHTIG! Sie ist kompatibel mit den Vektornormen in X, Y: ∥Fx∥Y ≤ ∥F∥∥x∥X (∀ ∈ L(X, Y), ∀x ∈ X). Beweis zu Satz 5.2 (OpN1): Aus der Definition ∥F∥ := supx̸=o ∥F(x)∥ ∥x∥ ist klar, dass ∥F∥ ≥ 0 gilt und dass ∥F∥ = 0 bedeutet, dass F(x) = o f ¨ur alle x, also F die Nullabbildung O ist. (OpN2): Unter Verwendung der zweiten Eigenschaft der Vektornorm (absolute Homogenit¨at) in Y folgt, dass ∥αF∥ (i) = sup ∥x∥X=1 ∥αF(x)∥Y (ii) = sup ∥x∥X=1(|α|∥F(x)∥Y) (iii) = |α| sup ∥x∥X=1 ∥F(x)∥Y (i) = |α|∥F∥. Erkl¨arungen zu den Schritten oben: (i) Definition der induzierten Operatornorm. (ii) Zweite Eigenschaft der Vektornorm (absolute Homogenit¨at) in Y. (iii) Wir d ¨urfen den Skalar hinausziehen, da die Gleichheit erhalten bleibt. (OpN3): Hier verwendet man unter anderem die dritte Eigenschafft der Vektor- norm (die Dreiecksungleichung) aus Y: ∥F + G∥ (i) = sup ∥x∥X=1 ∥(F + G)(x)∥Y (ii) = sup ∥x∥X=1 ∥F(x) + G(x)∥Y (iii) ≤ sup ∥x∥X=1(∥F(x)∥ + ∥G(x)∥Y) (iv) = sup ∥x∥X=1 ∥F(x)∥ + sup ∥x∥X=1 ∥G(x)∥Y (i) = ∥F∥ + ∥G∥. 52 5.1. Normen von linearen Abbildungen (Operatoren) und Matrizen, Konditionszahl (i) Definition der induzierten Operatornorm. (ii) Distributivgesetz f ¨ur lineare Abbildungen. (iii) Dritte Eigenschafft der Vektornorm (die Dreiecksungleichung) aus Y. (iv) Erste Eigenschaft aus dem Satz (Rechnen mit sup/inf) (OpN5): Folgt sofort daraus, dass f ¨ur ein festes x = ̃x gilt ∥F(̃x)∥Y ∥̃x∥X (i) ≤ sup x̸=o ∥F(x)∥Y ∥x∥X (ii) = ∥F∥. (i) Entweder wir erhalten die Gleichheit oder der Bruch links ist strikt klei- ner als der von rechts. Die Gleichheit erhalten wir, falls ̃x genau das Supremum ist. Strikt kleiner erhalten wir in allen anderen F¨allen. (ii) Definition der induzierten Operatornorm. (OpN4): Wir wenden (OpN5) zun¨achst auf G, dann auf F an, umzu sehen, dass ∥G ◦ F∥ (i) = sup x̸=0 ∥(G ◦ F)(x)∥Z ∥x∥X (ii) = sup x̸=0 ∥G(F(x))∥Z ∥x∥X (iii) ≤ sup x̸=0 ∥G∥∥F(x)∥Y ∥x∥X (iii) ≤ sup x̸=0 ∥G∥∥F∥∥x∥X ∥x∥X (iv) = sup x̸=0 ∥G∥∥F∥ = ∥G∥∥F∥. (i) Definition der induzierten Operatornorm. (ii) Definition von der Komposition von Funktionen. (iii) (OpN5) (iv) K ¨urze ∥x∥X. □ Lemma 5.3 (Skript: Beispiel 6.13) Die Matrixnorm ist kompatibel mit der 2-Norm (als Vektornorm im En). 53 5. Grundvorraussetzung f ¨ur die Methode der kleinsten Quadrate Beweis vom Lemma: ∥Ax∥2 2 (i) = n ∑ k=1 ( n ∑ ℓ=1 akℓxℓ )2 (ii) ≤ n ∑ k=1 ( n ∑ ℓ=1 a2 kℓ · n ∑ j=1 x2 j ) (iii) = n ∑ j=1 x2 j · n ∑ k=1 n ∑ ℓ=1 a2 kℓ (iv) = ∥x∥2 2∥A∥2 F □ (i) Definition von der 2-Norm und Matrix-Vektor-Multiplikation (ii) Cauchy-Schwartz Ungleichung (iii) Die letzte Summe ist unabh¨angig von den Indizes k, ℓ. (iv) Definition von der 2-Norm und der Frobenius-Norm Definition 5.4 Die Maximumnorm (auch l∞ genannt) ist definiert durch: v ∈ V : ∥v∥∞ := max i {|vi|}. Definition 5.5 Es seien X und Y zwei normierte Vektorr¨aume mit den Normen ∥ · ∥X und ∥ · ∥Y. Eine lineare Abbildung (oder: ein linearer Operator) F : X → Y heisst beschr¨ankt, wenn es ein γF ≥ 0 gibt mit ∥F(x)∥Y ≤ γF∥x∥X, ∀x ∈ X. Die Gesamtheit solcher linearer Abbildungen (Operatoren) F zwischen X und Y heisst L(X, Y). Definition 5.6 (i) Die auf L(X, Y) durch die Normen ∥ · ∥X und ∥ · ∥Y induzierte Operator- norm ist definiert durch ∥ · ∥ : L(X, Y) → R F ↦→ ∥F∥ := sup x̸=o ∥F(x)∥ ∥x∥ . 54 5.1. Normen von linearen Abbildungen (Operatoren) und Matrizen, Konditionszahl (ii) Ist X = Y = En, so dass F durch eine quadratische Matrix A gegeben ist, heiss ∥A∥ die durch die Vektornorm (in En) induzierte Matrixnorm von A: ∥ · ∥ : En×n → R A ↦→ ∥A∥ := sup x̸=o ∥Ax∥ ∥x∥ . (iii) Verwendet man in En die Euklidische 2-Norm, so heisst die induzierte Ma- trixnorm Spektralnorm oder 2-Norm; sie wird auch mit ∥ · ∥2 bezeichnet: ∥A∥2 := sup x̸=o ∥Ax∥2 ∥x∥2 . Definition 5.7 Die Frobenius-Norm ist definiert durch ∥A∥F := √ n ∑ k=1 n ∑ ℓ=1 |akℓ|2. Definition 5.8 Sei x ∈ En, dann ist die Summennorm wie folgt definiert: ∥x∥1 := n ∑ i=1 |xi|. Definition 5.9 Sei A ∈ En×n, dann ist die Maximum-Matrixnorm wie folgt definiert: A =        a1 ... an        =      a11 a12 · · · a1n a21 a22 · · · a2n ... ... am1 am2 · · · amn      ∥A∥∞ := max{∥a1∥1, ..., ∥an∥1} = max{|a11| + ... + |a1n|, ..., |an1| + ... + |ann|}. Definition 5.10 Die Konditionszahl einer regul¨aren Matrix A bez ¨uglich einer gewissen Norm ∥ · ∥ (Gemeint ist eine Matrixnorm oder eine Vektornorm mit der induzierten Operator- norm) oder, kurz, die Kondition ist wie folgt definiert: κ(A) := ∥A∥∥A−1∥. Definition 5.11 Wir erweitern die obige Definition und w¨ahlen eine Norm, n¨amlich die 2-Norm. Dann ist die 2-Norm-Konditionszahl ist wie folgt definiert: κ(A)2 := ∥A∥2∥A−1∥2. 55 Kapitel 6 Die Methode der kleinsten Quadrate 6.1 Methode der kleinsten Quadrate Idee. Wir betrachten f ¨ur A ∈ Em×n, m > n das ¨uberbestimmte LGS Ax = y. Oft- mals haben solche LGS keine (exakten) L¨osungen, daher l¨ost man sie mit Hilfe von N¨ahrungsverfahren, wobei ein Fehler entsteht. Die Methode der kleinsten Quadrate (manchmal auch als lineare Ausgleichsrech- nung bezeichnet) versucht den Fehler zu minimieren. Der Residuenvektor (das Residuum, der Fehler) r := y − Ax soll minimale euklidische Norm haben, d.h. wir suchen x ∈ Rn, so dass ∥r∥2 2 = m ∑ k=1 |rk|2 = m ∑ k=1 |yk − n ∑ l=1 akl xl|2 minimal ist. Bemerkung 6.1 Die Methode der kleinsten Quadrate l¨asst sich auf allgemeine Normen erweitern. (Beispiel: S10A2) Bemerkung 6.2 Betrachte f ¨ur A ∈ Em×n, m > n das ¨uberbestimmte LGS Ax ! = y Methode der kleinsten Quadrate: Finde x ∈ En, so dass Residuum r = y − Ax minimale Norm hat. Formal ausgedr ¨uckt: arg min x∈En ∥y − Ax∥2 L¨osungsansatz: 1. Normalengleichung 2. QR-Zerlegung 57 6. Die Methode der kleinsten Quadrate 6.1.1 Normalengleichung Lemma 6.3 (Skript: Lemma 7.3) Sind die Kolonnen einer A ∈ Em×n Matrix linear unabh¨angig, d.h. ist rang(A) = n(≤ m), so ist AH A regul¨ar. Satz 6.4 (Skript: Satz 7.4) Die Orthogonalprojektion PA : Em →Im(A) ⊂ Em auf den Kolonnenraum Im(A) =Im(A) einer A ∈ Em×n Matrix mit Rang n(≤ m) ist gegeben durch: PA := A(AH A)−1 AH. Satz 6.5 (Skript: Satz 7.6) F ¨ur eine Orthogonalprojektion P ∈ En×n gilt: ∥y − Py∥2 = min z∈Im(P) ∥y − z∥2 , ∀y ∈ En Bemerkung 6.6 Angenommen die Kolonnen von A ∈ Em×n sind linear unabh¨angig, also ker(A) = {0} gilt und AH A gem¨ass Lemma 7.3 regul¨ar ist. Auf Grund von Satz 7.6 wissen wir, dass ∥r∥ minimal ist wenn gilt: Ax = PAy Satz 7.4 =⇒ Ax = A(AH A)−1 AHy ker(A)={0} =⇒ x = (AH A)−1 AHy ⇒ AH Ax = AHy Normalengleichung Definition 6.7 (Pseudoinverse) A† := (AH A)−1 AH ⇒ A† A = (AH A)−1 AH A = 1 ABER =⇒ AA† = A(AH A)−1 AH im Allgemeinem ̸= 1 Satz 6.8 (Skript: Satz 7.7) Es sei A ∈ Em×n, rang(A) = n ≤ m, y ∈ Em. Dann hat das ¨uberbestimmte Glei- chungssystem Ax = y eine eindeutig bestimmte L¨osung x im Sinne der kleinsten Quadrate, d.h. x mit ∥y − Ax∥2 2 = arg min ̃x∈En ∥y − Ãx∥2 2 x ∈ En (6.1) x kann berechnet werden durch L¨osen des regul¨aren Systems der Normalgleichun- gen (Bemerkung 6.6). Der Residuenvektor r ⊥ Im(A). 58 6.1. Methode der kleinsten Quadrate Beispiel 6.9 Gegeben: A =  0 1 1 3 2 −2   , ⇒ m = 3 > 2 = n, y =  2 2 4   Gesucht: Bestimme mit Hilfe der Normalengleichung x = (x1, x2)T, so dass ∥r∥2 2 = ∥y − Ax∥2 2 minimal ist bez ¨uglich dem Standard-Skalarprodukt ⟨x, y⟩ = xHy. L ¨osung: Ax = y ⇒ AT Ax = ATy ⇔ x = (AT A)−1 ATy (∗) ATA = (0 1 2 1 3 −2 )  0 1 1 3 2 −2   = ( 5 −1 −1 14 ) (AT A)−1 = 1 5 · 14 − (−1) · (−1) (14 1 1 5 ) = 1 69 (14 1 1 5 ) ATy = (0 1 2 1 3 −2 )  2 2 4   = (10 0 ) (∗) =⇒ x = 1 69 (14 1 1 5 ) (10 0 ) = 1 69 (140 10 ) Beispiel 6.10 Gegeben: Lege Bestm¨ogliche Gerade (d.h. der Abstand zwischen Punkt und Gerade ist minimal) duch P1, P2, P3, P4 wobei P1 = (0, 6), P2 = (1, 0), P3 = (2, 0), P4 = (3, −4) Gesucht: Geradengleichung y = ax + b, wobei a, b ∈ R2 unbekannte Parameter sind. L ¨osung: Pi = (x, y) ⇒    6 = a · 0 + b 0 = a · 1 + b 0 = a · 2 + b −4 = a · 3 + b ⇔     0 1 1 1 2 1 3 1     ︸ ︷︷ ︸ =A (a b ) ︸︷︷︸ =x =     6 0 0 −4     ︸ ︷︷ ︸ =y 59 6. Die Methode der kleinsten Quadrate Ax = y ⇒ AT Ax = ATy ⇔ x = (AT A)−1 ATy (∗) AT A = (0 1 2 3 1 1 1 1 )     0 1 1 1 2 1 3 1     = (14 6 6 4 ) (AT A)−1 = 1 14 · 4 − 6 · 6 ( 4 −6 −6 14 ) = 1 20 ( 4 −6 −6 14 ) = 1 10 ( 2 −3 −3 7 ) ATy = (0 1 2 3 1 1 1 1 )     6 0 0 −4     = (−12 2 ) (∗) =⇒ x = (AT A)−1 ATy = 1 10 ( 2 −3 −3 7 ) (−12 2 ) = 1 5 ( 2 −3 −3 7 ) (−6 1 ) = 1 5 (−15 25 ) = (−3 5 ) =⇒ Gerade: y = −3x + 5 60 6.1. Methode der kleinsten Quadrate 6.1.2 QR-Zerlegung Bemerkung 6.11 Sei A ∈ Em×n. Angenommen rang(A) = n ≤ m (voller Rang). Dann existiert eine orthogonale/unit¨are (Matrix Q = (Q1|Q2) mit Q ∈ Em×m, Q1 ∈ Em×n, Q2 ∈ Em×(m−n) und es existiert eine rechte Dreiecksmatrix R1 ∈ En×n und mit 0 ∈ E(m−n)×n, so dass A = Q · (R1 0 ) =: QR Bemerkung 6.12 Da (R1 0 ), ist A = QR = Q1R1. Bemerkung 6.13 (Zusammenhang QR-Zerlegung und kleinste Quadrate) F ¨ur das Residuum r gilt, dann r = y − n ∑ i=1⟨qi, y⟩qi, wobei {qi}n i=1 sie Spalten von Q bezeichnet. Bemerkung 6.14 Falls das betrachtete Skalarprodukt das Standard-Skalarprodukt ist, gilt: QH 1 Q1 = 1 Ax = y A=Q1R1⇐⇒ Q1R1x = y ⇔ R1x = QH 1 y ⇔ x = R−1 1 QH 1 y F ¨ur ein anderes Skalarprodukt erggibt sich eine andere Fromel f ¨ur x! 61 6. Die Methode der kleinsten Quadrate Kochrezept 6.15 (QR-Zerlegung) Gegeben: A ∈ Em×n, m > n, ⟨·, ·⟩ ein Skalarprodukt Gesucht: Q1, R1, so dass A = Q1R1. Q1: Seien a1, ..., an Spalten von A. Wende Gram-Schmidt Verfahren bez ¨uglich ⟨·, ·⟩ auch {ai}n i=1 an. Das liefert dann {q1, ..., qn}. ⇒ (q1|...|qn) = Q1 ∈ Em×n R1: 1. rij =    0, i > j ⟨qi, aj⟩, i < j ∥ ̃qi∥ = √⟨ ̃qi, ̃qi⟩, i = j wobei ̃qi den i-ten orthogonalisierten, aber noch nicht normierten Vektor bezeichnet. 2. R1 = (rij)1≤i,j≤n Beispiel 6.16 Gegeben: A =  0 1 1 3 2 −2   , ⇒ m = 3 > 2 = n, y =  2 2 4   Gesucht: Bestimme mit Hilfe der QR-Zerlegung x = (x1, x2)T, so dass ∥r∥2 2 = ∥y − Ax∥2 2 minimal ist bez ¨uglich dem Standard-Skalarprodukt ⟨x, y⟩ = xHy. L ¨osung: 62 6.1. Methode der kleinsten Quadrate (Q1) q1 = a1 ∥a1∥ = 1 √5  0 1 2   ̃q2 = a2 − ⟨q1, a2⟩q1 =   1 3 −2   − 1 √5 1 √5 ⟨  0 1 2   ,   1 3 −2  ⟩  0 1 2   =   1 3 −2   − 1 5 · (−1) ·  0 1 2   =   1 16 5 − 8 5   = 1 5   5 16 −8   q2 = ̃q2 ∥ ̃q2∥ = 1 5 5 √345   5 16 −8   = 1 √345   5 16 −8   ⇒ Q1 =    0 5√345 1√5 16√345 2√5 −8√345    (R1) r11 = ∥a1∥ = √5 r22 = ∥ ̃q2∥ =   1 5 1 5 ⟨   5 16 −8   ,   5 16 −8  ⟩   1 2 = ( 1 25 (25 + 256 + 64)) 1 2 = √345 5 r12 = ⟨q1, a2⟩ = ⟨  0 1 2   ,   1 3 −2  ⟩ = − 1 √5 r21 = 0 ⇒ R1 = (√5 − 1√5 0 √345 5 ) ⇒ A = Q1R1 63 6. Die Methode der kleinsten Quadrate Ax = y ⇒ Q1R1x = y ⇔ R1x = QT 1 y ⇔ x = R−1 1 QT 1 y R−1 1 = √5 √345 ( √345 5 1√5 0 √5 ) QT 1 = ( 0 1√5 2√5 5√345 16√345 −8√345 ) x = ( √345 5 1√5 0 √5 ) ( 0 1√5 2√5 5√345 16√345 −8√345 )  2 2 4   = ( 5 345 85 345 130 345 25 345 80 345 − 40 345 )  2 2 4   = 1 345 ( 5 85 130 25 80 −40 )  2 2 4   = 1 69 (1 17 26 5 16 −8 )  2 2 4   = 1 69 (2 + 34 + 104 10 + 32 − 32 ) = 1 69 (140 10 ) Korollar 6.17 (Skript: Korollar 10.10) Die 2-Norm-Konditonszahl (kurz: Kondition) einer Matrix A ∈ En×n ist gegeben durch κ2 = max{|ω|} min{|ω|} , wobei ω der Eigenwert von A ist. Insbesondere ist die Konditionszahl immer gr¨osser oder gleich 1. Bemerkung 6.18 Wir m¨ochten immer eine m¨oglichst kleine Kondition haben, da dann die Imple- mentierung numerisch am stabilsten ist, d.h. grosse Kondition ist schlecht und unerw ¨unscht. Bemerkung 6.19 Die QR-Zerlegung ist f ¨ur m ≥ n und Rang(A) = n eindeutig, wenn man die Vorzeichen der Diagonalelemente von der Dreiecksmatrix vorgibt. 64 6.1. Methode der kleinsten Quadrate Bemerkung 6.20 (Normalengleichung vs. QR-Zerlegung) Normalengleichung Pro: • ßch¨on ¨um von Hand zu rechnen • AH A ist hermitesch positiv definit ⇒ kann f ¨ur Cholesky-Zerlegung aus- genutzt werden Con: • Wenn A schlecht Konditioniert ist, ist AH A quadratisch schlecht kon- ditioniert. ⇒ Cholesky liefert unbrauchbare Resultate (Rundungsfehler massiv verst¨arkt, Implementiereung instabil) QR-Zerlegung Pro: • kann numerisch stabil implementiert werden (Verfahren h¨angt weniger von der Kondition ab) • Q ist Projektion der Spalten von A Con: • ”h¨asslichßum von Hand rechnen (Wurzeln) 65 Kapitel 7 Grundvorraussetzung f¨ur die Eigenwerte und Eigenvektoren 7.1 Petermutationen Definition 7.1 Eine Permutation ist eine bijektive Abbildung: p : {1, ..., n} → {1, ..., n} Definition 7.2 Die Menge aller Permutationen heisst symmetrische Gruppe Sn. Definition 7.3 Eine Permutation, welche nur zwei Elemente vertauscht heisst Transposition. Satz Jede Permutation kann als Produkt (hintereinanderschaltung von Abbildungen) von Transpositionen geschrieben werden. Bemerkung 7.4 Die Darstellung einer Permutation als Produkt von Transpositionen ist nicht ein- deutig. Aber die Anzahl ben¨otigter Transpositionen ist eindeutigerweise entweder gerade oder ungerade. Definition 7.5 Ist p ∈ Sn, so nennt man jedes Paar i, j ∈ {1, ..., n} mit i < j aber p(i) > p(j), einen Fehlstand von p. Beispiel 7.6 Gegeben: p = [1 2 3 2 3 1 ] 67 7. Grundvorraussetzung f ¨ur die Eigenwerte und Eigenvektoren Gesucht: Fehlst¨ande von p L ¨osung: Es gibt insgesamt zwei Fehlst¨ande, n¨amlich 1 < 3, aber 2 > 1, und 2 < 3, aber 3 > 1. Definition 7.7 Das Signum einer Permutation ist definiert als sign(p) = (−1)k = {1, falls p eine gerade Anzahl k von Fehlst¨anden hat −1, falls p eine ungerade Anzahl k von Fehlst¨anden hat. Definition 7.8 Man nennt p ∈ Sn: • gerade, falls sign(p) = +1, • ungerade, falls sign(p) = −1. 7.2 Determinanten Definition 7.9 Die Determinante ist eine Abbildung: det : En×n → E A ↦→ det(A) = ∑ p∈Sn sign(p) · n ∏ i=1 aip(i) Bemerkung 7.10 In der obigen Definition summieren wir ¨uber n! Summanden. Tricks um Determinanten zu berechnen. • 1 × 1-Matrix: det(a11) = |a11| = a11 • 2 × 2-Matirx: det (a b c d ) = ∣ ∣ ∣ ∣a b c d ∣ ∣ ∣ ∣ = ad − bc • 3 × 3-Matrix (Regel von Sarrus): det  a b c d e f g h i   = ∣ ∣ ∣ ∣ ∣ ∣ a b c d e f g h i ∣ ∣ ∣ ∣ ∣ ∣ = aei + b f g + cdh − gec − h f a − idb Es wird das folgende Muster bei der Regel von Sarrus angewendet: 68 7.2. Determinanten a b c a b d e f d e g h i g h + + + − − − Bemerkung 7.11 F ¨ur eine n × n-Matrix kann man die Determinante ¨uber die Definition mit Permu- tationen berechnen, aber ist sehr ineffizient (Aufwand proportional zu n!). Satz 7.12 Sei ̃A die Zeilenstufenform von A (d.h. ̃A ist eine obere/untere Dreiecksmatrix), welche man durch den Gauss-Algorithmus aus A berechnet hat. Sei k ∈ N0 die Anzahl ausgef ¨uhrter Zeilenvertauschungen: ⇒ det(A) = (−1)k · det( ̃A) = (−1)k n ∏ i=1 ̃Aii, wobei n ∏ i=1 ̃Aii das Produkt der Diagonaleintr¨age ist. Satz 7.13 (Laplace’sche Entwicklungssatz) Sei A ∈ En×n und A′ ij ∈ En−1×n−1 die Streichmatrix ohne i-te Zeile und j-te Spalte. ⇒ Entwicklung nach der i-ten Zeile: det(A) = n ∑ j=1(−1)i+j · aij · det(A′ ij) ⇒ Entwicklung nach der j-ten Spalte: det(A) = n ∑ i=1(−1)i+j · aij · det(A′ ij). Beispiel 7.14 Gegeben: A =  1 2 0 3 −1 0 1 0 5   Gesucht: det(A) L ¨osung: (1) Sarrus: det(A) = 1 · (−1) · +2 · 0 · 1 + 0 · 3 · 0 − 1 · (−1) · 0 − 0 · 0 · 1 − 5 · 3 · 2 = −5 − 5 · 3 · 2 = −5 − 30 = −35 69 7. Grundvorraussetzung f ¨ur die Eigenwerte und Eigenvektoren (2) Gauss: • ohne Zeilenvertauschung  1 2 0 3 −1 0 1 0 5   (ii)−l21·(i) ⇝  1 2 0 0 −7 0 1 0 5   (iii)−l31·(i) ⇝  1 2 0 0 −7 0 0 0 5   =: ̃A ⇒ det(A) = det( ̃A) = 1 · (−7) · 5 = −35 • mit Zeilenvertauschung  1 2 0 3 −1 0 1 0 5   Zeilenvertauschung ⇝  1 0 5 3 −1 0 1 2 0   (ii)−l21·(i) ⇝  1 0 5 0 −1 −15 1 2 0   (ii)−l31·(i) ⇝  1 0 5 0 −1 −15 0 2 −5   (ii)−l23·(i) ⇝  1 0 5 0 −1 −15 0 0 −35   =: ̃A ⇒ det(A) = (−1) · det( ̃A) = (−1) · [1 · (−1) · (−35)] = −35 (3) Laplace: (i) Entwicklung nach der letzten Spalten (am effizientesten): ∣ ∣ ∣ ∣ ∣ ∣ 1 2 0 3 −1 0 1 0 5 ∣ ∣ ∣ ∣ ∣ ∣ = 5 · ∣ ∣ ∣ ∣1 2 3 −1 ∣ ∣ ∣ ∣ = 5((−1) · 1 − 3 · 2) = 5 · (−7) = −35 (ii) Entwicklung nach der ersten Zeile (dient nur zur Illustration, da ineffizi- ent): ∣ ∣ ∣ ∣ ∣ ∣ 1 2 0 3 −1 0 1 0 5 ∣ ∣ ∣ ∣ ∣ ∣ = 1 · ∣ ∣ ∣ ∣−1 0 0 5 ∣ ∣ ∣ ∣ − 2 · ∣ ∣ ∣ ∣3 0 1 5 ∣ ∣ ∣ ∣ = 1 · (−1) · 5 − 2 · 3 · 5 = −5 − 30 = −35 70 7.2. Determinanten Satz 7.15 (Axiomatischer Zugang, Eigenschaften der Determinante) Die Abbildung det : En×n → E A ↦→ det(A) heisst Determinante, falls folgende Eigenschaften gelten: Sei A ∈ En×n eine qua- dratische Matrix, dann gilt: (D1) det(1) = 1 (D2) Hat A zwei gleiche oder lineare abh¨angige Zeilen/Spalten, so ist det(A) = 0 (D3) Linearit¨at in jeder Zeile/Spalte (in der Literatur auch n-Linearit¨at genannt) det(v1, ..., λvi + w, ..., vn) = λdet(v1, ..., vi, ..., vn) + det(v1, ..., w, ..., vn) Die Determinante hat folgende weitere Eigenschaften, die sich aus den ersten Drei herleiten lassen: (D4) det(λA) = λn det(A) f ¨ur alle λ ∈ E (D5) Ist eine Zeile/Spalte gleich Null, so ist: det(A) = 0 (D6) Vertauscht man zwei Zeilen/Spalten von A, so ¨andert sich das Vorzeichen von det(A): det(v1, ..., vi, ..., vj, ..., vn) = −det(v1, ..., vj, ..., vi, ..., vn) (D7) Entsteht B aus A durch Addition des λ-fachen (λ ̸= 0) der i-ten zur j-ten Zeile/Spalte, dann ist: det(A) = det(B) (D8) Ist A eine obere/untere Dreiecksmatrix, so ist: det(A) = n ∏ i=1 aii (D9) det(AB) = det(A)det(B) (D10) det(A) = det(A) (D11) det(A−1) = 1 det(A) 71 7. Grundvorraussetzung f ¨ur die Eigenwerte und Eigenvektoren (D12) det(AT) = det(A) (D13) Ist A = (A1 A2 0 A3 ) so ist: det(A) = det(A1) · det(A3) und analog gilt: det       B1 ∗ · · · ∗ 0 . . . . . . ... ... . . . . . . ∗ 0 · · · 0 Bn       = det(B1) · ... · det(Bn) (D14) det(A) = n ∏ i=1 λi, wobei λi ein Eigenwert von A ist. (D15) Definition der Determinante ist Basisinvariant: det(B) = det(S−1BS) = 1 det(S) · det(B) · det(S) (D16) Orthogonale/unit¨are Matrizen A haben: det(A) = ±1, weil det(AAT) = det(A)det(AT) = det(A)det(A) = det(A)2 ! = det(1) = 1 (D17) F ¨ur hermitesche Matrizen (AH = A) haben wir: det(A) ∈ R, weil det(AH) = det(AT) = det(A) = det(A) ! = det(A) Satz 7.16 • det(A) = 0 ⇔ A nicht invertierbar ⇔ rang(A) < n • Ax = b eindeutig l¨osbar ⇔ det(A) ̸= 0. 72 7.2. Determinanten Definition 7.17 Zu jedem Element akl einer n × n-Matrix A werde die (n − 1) × (n − 1)-Untermatrix A[k, l] definiert durch Streichen der Zeile k und der Kolonne l von A. Der Kofaktor κkl von akl ist dann die Zahl: κkl := (−1)k+l det (A[k,l]) . Definition 7.18 Der Grad dj des Knotens j ist wie folgt definiert: dj := n ∑ i=1(A)ij. Satz 7.19 (Satz von Kirchhoff) Der Satz von Kirchhoff besagt, dass die Anzahl der aufspannenden Teilb¨aume ei- nes Graphen gleich dem Wert eines beliebigen Kofaktors von L ist, wobei wie folgt definiert ist: L := D − A mit D = diag(d1, ..., dn), di = ∧ Grad des Knotens i und A = ∧ Adjazenzmatrix mit (Aij) = {1, falls i mit j durch eine Kante verbunden ist 0, sonst. 73 Kapitel 8 Eigenwerte und Eigenvektoren 8.1 Eigenwerte und Eigenvektoren Sei in diesem Abschnitt V ein Vektorraum ¨uber E, dim(V) = n < ∞, A ∈ En×n. Definition 8.1 • λ ∈ E heisst Eigenwert (EW) von A ⇔ ∃v ∈ V \\ {0} : Av = λv. • v ∈ V \\ {0} heisst dann Eigenvektor (EV) von A zum Eigenwert λ. • Eλ(A) = {v ∈ V|Av = λv} heisst Eigenraum von A zum Eigenwert λ. • σ(A) := {λ | λ Eigenwerte von A} heisst Spektrum von A. Bemerkung 8.2 Eλ(A) =ker(A − λ1) ist ein nichttrivialer Untervektorraum von V, d.h. es gilt: {0} ⊊ Eλ(A) (echt gr¨osser als nur der Nullraum) Bemerkung 8.3 Diese Definition l¨asst sich analog f ¨ur eine lineare Abbildung F : V → V f ¨uhren. Es gilt: λ ist ein Eigenwert von F, v ist ein Eigenvektor von F ⇔ λ ist ein Eigenwert von A, v ist ein Eigenvektor von A, wobei A die Abbildungsmatrix von F bezeichnet. Bemerkung 8.4 (Herleitung des charakteristischen Polynoms χA) Betrachte Av = λv ⇔ Av − λv = 0 ⇔ (A − λ1)v = 0 (1) v = 0 l¨ost (1), aber v = 0 ist als Eigenvektor nicht zugelassen. Wir fordern mehr L¨osungen, d.h. ∞ viele L¨osungen (da ein LGS immer 0, 1 oder ∞ viele L¨osungen 75 8. Eigenwerte und Eigenvektoren hat). Gem¨ass dem Satz aus der Erinnerung k¨onnen wir dies , indem wir det(A − λ1) ! = 0 setzen, weil det(A − λ1) Satz ⇐⇒ A − λ∞ singul¨ar ⇐⇒ (A − λ∞)v = 0 hat ∞ viele L¨osungen, da v = 0 nicht zugelassen ist. Definition 8.5 χA(λ) = det(A − λ1) heisst charakteristisches Polynom. Bemerkung 8.6 • χA(λ) hat Grad n • λ ist der Eigenwert von A ⇔ λ ist eine Nullstelle (NST) des charakteristischen Polynoms χA(λ). Bemerkung 8.7 Mitternachtsformel (auswendig)] ax2 + bx + c = 0 ⇒ x1,2 = −b ± √b2 − 4ac 2a Kochrezept 8.8 (Berechnung von Eigenwerten und Eigenvektoren) Gegeben: A ∈ En×n (falls F : V → V lineare Abbildung gegeben ist, finde zuerst die Abbildungsmatrix A) Gesucht: σ(A) (d.h. ∀ Eigenwerte von A), Eλ(A) mit ∀λ ∈ σ(A) 1) Berechne χA(λ) = det(A − λ1) 2) Setze χA(λ) ! = 0: ⇒ n Nullstellen λ1, ..., λn ⇒ σ(A) = {λ1, ..., λn} Bemerkung. λ1, ..., λn sind nicht zwingend verschieden, sondern mit Nullstel- lenvielfachheit gez¨ahlt. 76 8.1. Eigenwerte und Eigenvektoren 3) F ¨ur jeden verschiedenen Eigenwert λk bestimme die Basis von Eλk (A) = ker(A − λk(A)) mit Hilfe von der Gauss-Elimination. 4) Die Menge der Eigenvektoren ist span {⋃ k Basis von Eλk (A) } \\ {0} (Wir vereinigen alle Eigenr¨aume minus den Nullvektor) Beispiel 8.9 Gegeben: A =  1 2 3 4 3 2 0 0 5   Gesucht: Eigenwerte, Eigenvektoren von A L ¨osung: 1) χA(λ) = det(A − λ1) = det  (1 − λ) 2 3 4 (3 − λ) 2 0 0 (5 − λ)   = (1 − λ)(3 − λ)(5 − λ) − (5 − λ) · 4 · 2 = (5 − λ)[(1 − λ)(3 − λ) − 4 · 2] = (5 − λ)[3 − 3λ − λ + λ2 − 8] = (5 − λ)[λ2 − 4λ − 5] = (5 − λ)(λ − 5)(λ + 1) (allenfalls mit Hilfe der Mitternachtsformel) = −(λ − 5)(λ − 5)(λ + 1) = −(λ − 5)2(λ + 1) 2) χA(λ) ! = 0 ⇔ −(λ − 5)2(λ + 1) = 0 ⇒ λ1 = λ2 = 5 (Sp¨ater: algebraische Vielfachheit von 5 ist 2) λ3 = −1 ⇒ σ(A) = {5, −1} 77 8. Eigenwerte und Eigenvektoren 3) • λ1 = λ2 = 5: E5(A) = ker(A − 5 · 1) = ker  −4 2 3 4 −2 2 0 0 0   ”Gaussen” ⇝ ker  2 −1 0 0 0 1 0 0 0   = span     1 2 0      • λ3 = −1: E−1(A) = ker(A − (−1) · 1) = ker  2 2 3 4 4 2 0 0 6   ”Gaussen” ⇝ ker  1 1 0 0 0 1 0 0 0   = span      1 −1 0      4) Menge der Eigenvektoren: span     1 2 0   ,   1 −1 0      Bemerkung 8.10 Es w¨are mathematisch unpr¨azis in Beispiel 1 zu sagen: ”Die Eigenvektoren sind (1, 2, 0)T und (1, −1, 0)T”. Denn es gibt ∞ viele Eigenvektoren. Sei zum Beispiel v ein Eigenvektor von A zum Eigenwert λ, d.h. Av = λv ⇒ c · v ist ein Eigenvektor von A zu λ f ¨ur alle c ∈ E, da auch A(cv) = λ(cv) gilt. Meist gen ¨ugt es trotzdem, einen Basisvektor: v ∈ Eλ(A) \\ {0} als ”repr¨asentativen Eigenvektor zu nehmen. Bemerkung 8.11 Komplexwertige Nullstellen des Charakteristischen Polynoms mit reellen Koeffizi- enten treten immer paarweise auf, und zwar ist mit λ ∈ C auch λ eine Nullstelle. Somit muss eine orthogonale Matrix von ungerader Dimension mindestens einen reellen Eigenwert ±1 besitzen. Definition 8.12 Die Summe der Diagonalelemente von A ∈ En×n nennt man Spur von A: spur(A) := n ∑ i=1 aii = a11 + ... + ann. 78 8.2. Spektralzerlegung, Diagonalisierbarkeit Lemma 8.13 (Skript: Lemma 9.6) Eine (quadratische) Matrix A ist genau dann singul¨ar, wenn sie 0 als Eigenwert hat: A singu¨ar ⇔ 0 ∈ σ(A). Satz 8.14 (Skript: Satz 9.11) Eigenvektoren zu verschiedenen Eigenwerten sind linear unabh¨angig. Korollar 8.15 (Skript: Korollar 9.12) Sind die n Eigenwerte von F : V → V (mit n = dim(V)) verschieden, so gibt es eine Basis von Eigenvektoren und die entsprechende Abbildungsmatrix ist diagonal. Satz 8.16 (Skript: Satz 9.15) Ist A ∈ Cn×n hermitesch, so gilt: (i) Alle Eigenwerte λ1, ..., λn sind reell. (ii) Die Eigenvektoren zu verschiedenen Eigenwerten sind paarweise orthogonal in C. (iii) Es gibt eine orthonormale Basis des Cn aus Eigenvektoren u1, ..., un von A. (iv) F ¨ur die unit¨are Matrix U := (u1, ..., un) gilt: U H AU = Λ = diag(λ1, ..., λn). 8.2 Spektralzerlegung, Diagonalisierbarkeit Definition 8.17 Sei λ ein Eigenwert von A ∈ En×n. • Algebraische Vielfachheit von λ: aλ := ”Nullstellen vielfachheit von λ in χA(λ)” • Geometische Vielfachheit: gλ := dim(Eλ(A)) = dim(ker(A − λ1)) = n − rang(A − λ1) Definition 8.18 A heisst diagonalisierbar (es existiert eine Spektralzerlegung) ⇔ ∃V ∈ En×n, Λ ∈ En×n, mit V regul¨ar, Λ diagonal, so dass A = VΛV−1 Man sagt auch: ”A ist ¨ahnlich zu einer Diagonalmatrix”. 79 8. Eigenwerte und Eigenvektoren Satz 8.19 A ist diagonalisierbar ⇔ Die Eigenvektoren von A bilden eine Basis von V ⇔ F ¨ur alle Eigenwerte λ von A gilt gλ = aλ. Bemerkung 8.20 Eine Diagonalisiertung ist ein Basiswechsel mit Transformationsmatrizen V und V−1. Satz 8.21 Bei ¨Ahnlichkeitstransformation bleibt erhalten: • Eigenwerte und deren algebraische und geometrische Vielfachheit • Rang • Determinante • Spur • Charakteristische Polynom χA(t) • nicht die Eigenvektoren! Kochrezept 8.22 (Berechnung der Diagonalmatrix) Gegeben: A ∈ En×n diagonalisierbar Gesucht: V, Λ ∈ En×n 1) Finde die Eigenwerte λ1, ..., λn und finde die dazugeh¨origen Eigenvektoren v1, ..., vn von A (wobei vi ∈ Eλi (A) und {vi}n i=1 linear unabh¨angig). 2) Definiere die Diagonalmatrix wie folgt: Λ =       λ1 0 · · · 0 0 . . . . . . ... ... . . . . . . 0 0 · · · 0 λn       mit λ1 ≥ ... ≥ λn 3) Schreibe die Eigenvektoren vi ∈ Eλi (A) in eine Matrix: V = (v1|· · ·|vn) 80 8.2. Spektralzerlegung, Diagonalisierbarkeit 4) Test: A ? = VΛV−1 Beispiel 8.23 (Beispiel 8.9 fortgesetzt...) Sei A =  1 2 3 4 3 2 0 0 5   wie in Beispiel 1 mit χA(λ) = −(λ − 5)2(λ + 1) Gesucht: Algebraische und geometrische Vielfachheit L ¨osung: • Algebraische Vielfachheit: λ1 = 5 mit a5 = 2 und λ2 = −1 mit a−1 = 1 • Geometrische Vielfachheit: g5 = dim(E5(A)) = 1 ̸= a5 = 2 g−1 = dim(E−1(A)) = 1 = a−1 = 1 Da g5 ̸= a5 ⇒ A ist nicht diagonalisierbar. 81 8. Eigenwerte und Eigenvektoren Beispiel 8.24 Gegeben: A =   0 −1 1 −3 −2 3 −2 −2 3   Gesucht: Finde die Spektralzerlegung. L ¨osung: 1) (i) χA(λ) = det(A − λ1) = ∣ ∣ ∣ ∣ ∣ ∣ −λ −1 1 −3 −2 − λ 3 −2 −2 3 − λ ∣ ∣ ∣ ∣ ∣ ∣ = −λ(−2 − λ)(3 − λ) + (−1) · 3 · (−2) + 1 · (−3) · (−2) − (−2) · (−2 − λ) · 1 − (−2) · 3 · (−λ) − (3 − λ) · (−3) · (−1) = 6λ + 3λ2 − 2λ2 − λ3 + 6 + 6 − 4 − 2λ − 6λ − 9 + 3λ = −λ3 + λ2 + λ − 1 Bemerkung. Hier m ¨usst ihr ein Trick verwenden, indem ihr die Nullstelle erratet (teste: 1, −1, 2, −2, etc.) und dann eine Polynomdivision durchf ¨uhrt. λ1 = 1 ist eine Nullstelle: ( − t3 + t2 + t − 1 ) : (t − 1) = − t2 + 1 t3 − t2 t − 1 − t + 1 0 ⇒ χA(λ) = (λ − 1)(1 − λ2) = −(λ − 1)2(λ + 1) (ii) χA(λ) ! = 0 ⇔ 0 = −(λ − 1)2(λ + 1) ⇒ λ1 = λ2 = 1, a1 = 2 λ3 = −1, a−1 = 1 ⇒ σ(A) = {−1, 1} 82 8.2. Spektralzerlegung, Diagonalisierbarkeit (iii) • λ1 = λ2 = 1: E1(A) = ker(A − 1) = ker  −1 −1 1 −3 −3 3 −2 −2 2   ”Gaussen” ⇝ ker  −1 −1 1 0 0 0 0 0 0   = span      1 −1 0   ,  1 0 1      ⇒ g1 = 2 • λ3 = −1: E−1(A) = ker(A + 1) = ker   1 −1 1 −3 −1 3 −2 −2 4   ”Gaussen” ⇝ ker  1 −1 1 0 −4 6 0 −4 6   ”Gaussen” ⇝ ker  1 −1 1 0 2 −3 0 0 0   = span     1 3 2      ⇒ g−1 = 1 =⇒ a1 = g1 und a−1 = g−1 ⇒ A ist diagonalisierbar. (iv) Menge der Eigenvektoren: span      1 −1 0   ,  1 0 1   ,  1 3 2      2) Λ =  1 0 0 0 1 0 0 0 −1   3) V =   1 1 1 −1 0 3 0 1 2   , V−1 =   3 2 1 2 − 3 2 −1 −1 2 1 2 1 2 − 1 2   , wobei ihr die Inverse V−1 mit dem ¨ublichen Rezept berechnet. 4) Test: A = VΛV−1✓ 83 Kapitel 9 Addendum Bemerkung 9.1 Hier werden weitere Themen aufgef ¨uhrt die in der Vorlesung behandelt wurden, aber in den vorherigen Kapiteln nicht aufgelistet sind. 9.1 Basiswechsel und Koordinatentransformation Definition 9.2 Seien A = (e1, ..., en) die kanonische Basis vom Vektorraum V und B = (b1, ..., bn) eine weitere Basis von V beschrieben mit der kanonischen Basis. Dann existiert eine Transformationsmatrix mit: TB A = (b1| · · · |bn) mit ei = TB Abi, i ∈ {1, ..., n}. VA VB T A B Bemerkung 9.3 Es gilt die folgende Rechenregel: T A B = (TB A)−1 Mit der obigen Definition erhalten wir somit: T A B ei = bi, i ∈ {1, ..., n}. Satz 9.4 Sei E ein K¨orper, V ein E-Vektorraum mit dim(V) = n < ∞. Seien v ∈ V, A = {a1, ..., an}, B = {b1, ..., bn} Basen f ¨ur V. Dann existieren eindeutige λ1, ..., λn ∈ E sowie eindeutige µ1, ..., µn ∈ E, so dass v = n ∑ k=1 λkak = n ∑ k=1 µkbk. Da stellt sich die Frage wie man zwischen den Basen A und B wechseln kann, kon- kret hat man zum Beispiel die Abbildungsmatrix bez ¨uglich A gegeben und m¨ochte nun die Abbildungsmatrix bez ¨uglich B darstellen. 85 9. Addendum Definition 9.5 Zu jeder Basis B = {v1, ..., vn} von V gibt es genau einen Isomorphismus: ϕB : En → V, (x1, ..., xn) ↦→ ϕB(x1, ..., xn) = n ∑ k=1 xkvk = x1v1 + ... + xnvn mit ϕB(ei) = vi. (In Worten: ϕB ordnet x seinen Koordinaten bez ¨uglich der Basis B zu.) Definition 9.6 Seien V mit Basis A = {v1, ..., vm} und W mit Basis B = {w1, ..., wn} Vek- torr¨aume ¨uber E. Dann gibt es zu jeder linearen Abbildung f : V → W genau eine Matrix MA B ( f ), so dass MA B ( f )j = f (vj) = a1jw1 + ... + amjwm f ¨ur j = 1, ..., n. Bemerkung 9.7 Die Matrix MA B ( f ) von oben hat als j-te Spalte den Vektor der Koordinaten von f (vj) bez ¨uglich der Basis B. Bemerkung 9.8 (Wichtig) In den Spalten einer Abbildungsmatrix stehen die Bilder der Basisvektoren, d.h. MA B ( f ) = ( f (v1)| · · · | f (vm)). Bemerkung 9.9 Die Matrix MA B ( f ) kann mit Hilfe des kommutierenden Diagramms auch foglen- dermassen beschrieben werden: MA B ( f ) = ϕ−1 B ◦ f ◦ ϕA Definition 9.10 Die regul¨are Transformationsmatrix TA B mit Basen A = {v1, ..., vn}, B = {w1, ..., wn} vom Vektorraum V sieht wie folgt aus: TA B = ϕ−1 B ϕA =    t11 · · · t1n ... ... tn1 · · · tnn    , En En V ϕA T A B ϕB Dadurch kann man nun folgend beschreiben wi = t1iv1 + ... + tnivn = TA B vi, i ∈ {1, ..., n}, wobei wi bez ¨uglich B und vi bez ¨uglich A dargestellt ist: TA B vA = wB, wobei vA =    v1 ... vn    bzgl. A, wB =    w1 ... wn    bzgl. B. Bemerkung 9.11 (Rechenregeln) • TA A = 1 • TA B = (TB A)−1 86 9.1. Basiswechsel und Koordinatentransformation • λA ∈ Kn ein Koordinatenvektor bez ¨uglich A µB ∈ Kn ein Koordinatenvektor bez ¨uglich B ⇒ TA B λA = µB • f : V → V linear mit Abbildungsmatrix MA A( f ) wobei der Definitionsbe- reich und der Bildbereich bez ¨uglich A gegeben ist. Analog ist die Abbildungs- matrix MB B( f ) im Definitionsbereich und im Bildbereich bez ¨uglich B gegeben. Wir erreichen eine Basistransformation von A nach B der Abbildungsmatrix MA A( f ) mit den Transformationsmatrizen TB A, T A B : MB B( f ) = T A B MA A( f )TB A • f : V → V linear mit Abbildungsmatrix MB1 B2 ( f ) wobei der Definitions- bereich bez ¨uglich B1 und der Bildbereich bez ¨uglich B2 gegeben ist. Analog ist die Abbildugnsmatrix MB′ 1 B′ 2 ( f ) im Definitionsbereich bez ¨uglich B′ 1 und im Bildbereich bez ¨uglich B′ 2 gegeben. Wir erreichen eine Basistransformation von B1 nach B′ 1 (Definitionsbereich) bzw. von B2 nach B′ 2 (Bildbereich) der Abbil- dungsmatrix MB1 B2 ( f ) mit den Transformationsmatrizen TB2 B′ 2 , TB′ 1 B1 : MB′ 1 B′ 2 ( f ) = TB2 B′ 2 MB1 B2 ( f )TB′ 1 B1 Definition 9.12 Seien A = (e1, ..., en) die kanonische Basis vom Vektorraum V und B = (b1, ..., bn) eine weitere Basis von V beschrieben mit der kanonischen Basis. Dann existiert eine 87 9. Addendum Transformationsmatrix mit: TB A = (b1| · · · |bn) mit ei = TB Abi, i ∈ {1, ..., n}. VA VB T A B Bemerkung 9.13 Mit der obigen Definition erhalten wir somit: T A B ei = bi, i ∈ {1, ..., n}. Kochrezept 9.14 (Transformationsmatrix) (dekt alle F¨alle ab) Gegeben: A = (a1, ..., an), B = (b1, ..., bn) sind Basen von V. Gesucht: Transformationsmatrix T A B und TB A. ( )B A ⇔ ( )b1 · · · bn a1 · · · an ”Gaussen¨ohne Zeilenvertauschung ⇝ · · · ⇝ ( )1 T A B ( )A B ⇔ ( )a1 · · · an b1 · · · bn ”Gaussen¨ohne Zeilenvertauschung ⇝ · · · ⇝ ( )1 TB A Bemerkung 9.15 (Intuition) ( )B A ⇝ ( ) BB−1 ︸ ︷︷ ︸ 1 AB−1 ︸ ︷︷ ︸ =T A B ⇝ ( )1 T A B Beispiel 9.16 Gegeben: A =     1 1 0   ,  1 0 1   ,  0 1 1      , B =     3 3 4   ,  1 2 3   ,  3 4 5      Gesucht: T A B , TB A ( )3 1 3 1 1 0 3 2 4 1 0 1 4 3 5 0 1 1 (ii)−l21(i) ⇝ ( )3 1 3 1 1 0 0 1 1 0 −1 1 4 3 5 0 1 1 (iii)−l31(i) ⇝     3 1 3 1 1 0 0 1 1 0 −1 1 0 5 3 1 − 4 3 − 1 3 1 (iii)−l32(ii) ⇝     3 1 3 1 1 0 0 1 1 0 −1 1 0 0 − 2 3 − 4 3 4 3 − 2 3 (iii)·− 3 2⇝ ( )3 1 3 1 1 0 0 1 1 0 −1 1 0 0 1 2 −2 1 88 9.1. Basiswechsel und Koordinatentransformation (ii)−(iii) ⇝ ( )3 1 3 1 1 0 0 1 0 −2 1 0 0 0 1 2 −2 1 (i)−3·(iii) ⇝ ( )3 1 0 −5 7 −3 0 1 0 −2 1 0 0 0 1 2 −2 1 (i)−(ii) ⇝ ( )3 0 0 −3 6 −3 0 1 0 −2 1 0 0 0 1 2 −2 1 (i)· 1 3⇝ ( )1 0 0 −1 2 −1 0 1 0 −2 1 0 0 0 1 2 −2 1 =⇒ T A B =  −1 2 −1 −2 1 0 2 −2 1   Bemerkung. TB A = (T A B )−1 k¨onnt ihr entweder mit dem Rezept von oben berechnen oder ihr ben ¨utzt das Rezept aus der 3. ¨Ubungsstunde und berechnet die Inverse (T A B )−1 = TB A. Beispiel 9.17 Sei V = P mit Basen B = {1, x, x2} Standardbasis (Monombasis) und A = {a1, a2, a3} mit a1 = x2 a2 = (x + 1)2 = x2 + 2x + 1 a3 = (x − 1)2 = x2 − 2x + 1 a) T A B , TB A? b) Sei F = d dt : P2 → P1, p ↦→ ˙p = dp dt mit Abbildungsmatrix M(F) = MB B(F) =  0 1 0 0 0 2 0 0 0  . Was ist MA A(F)? c) Sei p(x) = 3x2 − 8x + 2 ∈ P2. Was sind die Koordinaten von p bez ¨uglich A und B? a) Da B die Standardbasis ist, gilt: T A B = (a1|a2|a3) =  0 1 1 0 2 −2 1 1 1   TB A = (T A B )−1 : ( )0 1 1 1 0 0 0 2 −2 0 1 0 1 1 1 0 0 1 Zeilenvertauschungen ⇝ ( )1 1 1 0 0 1 0 1 1 1 0 0 0 2 −2 0 1 0 89 9. Addendum (iii)−l32(ii) ⇝ ( )1 1 1 0 0 1 0 1 1 1 0 0 0 0 −4 −2 1 0 (ii)− 1 −4 (iii) ⇝     1 1 1 0 0 1 0 1 0 1 2 1 4 0 0 0 −4 −2 1 0 (i)− 1 −4 (iii) ⇝     1 1 0 − 1 2 1 4 1 0 1 0 1 2 1 4 0 0 0 −4 −2 1 0 (i)−(ii) ⇝     1 0 0 −1 0 1 0 1 0 1 2 1 4 0 0 0 −4 −2 1 0 − 1 4 (iii) ⇝     1 0 0 −1 0 1 0 1 0 1 2 1 4 0 0 0 1 1 2 − 1 4 0 ⇒ TB A = (T A B )−1 =  −1 0 1 1 2 1 4 0 1 2 − 1 4 0   b) Unter Verwendung der Rechenregel erhalten wir: MA A(F) = TB A MB B(F)T A B =  −1 0 1 1 2 1 4 0 1 2 − 1 4 0    0 1 0 0 0 2 0 0 0    0 1 1 0 2 −2 1 1 1   =   0 −2 2 1 2 3 2 − 1 2 − 1 2 1 2 3 2   c) Koordinaten von p bez ¨uglich B: pB =   2 −8 3   . Koordinaten von p bez ¨uglich A: pA = TB A pB =   1 −1 3   . Test: a1 − a2 + 3a3 = 3x2 − 8x + 2 = p(x) ✓ Definition 9.18 Zwei Matrizen A, B ∈ Em×n heissen ¨aquivalent, wenn es S ∈ Em×m und T ∈ En×n gibt mit: B = SAT−1 Falls m = n nennen wir A, B ∈ Em×n ¨ahnlich, wenn es ein S ∈ Em×m gibt mit: B = SAS−1. 90 9.2. Orthonormalbasis und Parsevalsche Formel 9.2 Orthonormalbasis und Parsevalsche Formel Definition 9.19 Sei B = {bi}i∈I eine Basis von V, ⟨·, ·⟩ Skalarprodukt auf V. • B heisst orthogonal ⇔ ⟨bk, bl⟩ = 0, ∀k ̸= l In Worten: Basisvektoren ßtehen ⊥ aufeinander”. • B heisst orthonormal (ONB) ⇔ ⟨bk, bl⟩ = δkl = {1, k = l 0, k ̸= l In Worten: Basisvektoren ßtehen ⊥ aufeinander”und haben L¨ange 1. Bemerkung 9.20 Sei V ein Vektorraum mit Skalarprodukt ⟨·, ·⟩, B = {bi}i∈I Basis von V. Per Definition von Basis kann man jedes v ∈ V schreiben als v = ∑i∈I αibi. Falls B eine ONB ist, so ist αi = ⟨bi, v⟩, ∀i ∈ I, ⇒ Koordinaten von v bez ¨uglich B sind ⟨bi, v⟩. Satz 9.21 Sei B = {b1, ..., bn} eine ONB von V. Dann gilt: • Die Koordinaten von v ∈ V sind gegeben durch ξk = ⟨bk, v⟩V, k ∈ {1, ..., n}, d.h. v = n ∑ k=1 ξkbk = n ∑ k=1⟨bk, v⟩Vbk. • Falls ξ =    ξ1 ... ξn    , η =    η1 ... ηn    die Koordinatenvektoren von v, w ∈ V sind, ξk = ⟨bk, v⟩V, ηk = ⟨bk, w⟩V, so gilt die Parsevalsche Formel: ⟨v, w⟩V = ξ Hη = ⟨ξ, η⟩ Bemerkung 9.22 Die Parsevalsche Formel besagt, dass der Winkel zwischen zwei Vektoren von einem euklidischen/unit¨aren Vektorraum V durch eine Koordinatentransformation nicht ver¨andert wird, falls wir eine orthonormale Basis haben. Somit k¨onnen wir eine Ko- ordinatentransformation von V zu E finden und den Winkel mit dem uns bekannten euklidischen Skalarprodukt (d.h. Standardskalarprodukt) berechnen. 91 9. Addendum Bemerkung 9.23 Aus der Parsevalscher Formel folgt: • ∥v∥V := √⟨v, v⟩V Parseval = ∥ξ∥ = √ n ∑ k=1 |ξk|2 • ∢V(v, w) := arccos ( ⟨v, w⟩V ∥v∥V ∥w∥V ) = ∢(ξ, η) = arccos ( ⟨ξ, η⟩ ∥ξ∥ ∥η∥ ) • v ⊥ w bez ¨uglich ⟨·, ·⟩V ⇔ ξ ⊥ η bez ¨uglich ⟨·, ·⟩ 9.3 Orthogonale und unit¨are Abbildungen Definition 9.24 Es seien X und Y zwei unit¨are (bzw. orthogonale) Vektorr¨aume. Eine lineare Abbil- dung F : X → Y heiss unit¨ar (bzw. orthogonal), falls f ¨ur x, y ∈ X gilt: ⟨F(x), F(y)⟩Y = ⟨x, y⟩X. Satz 9.25 (Skript: Satz 6.13) F ¨ur eine orthogonale oder unit¨re Abbildung F : X → F gilt: (i) ∥F(x)∥Y = ∥x∥X, d.h. F ist l¨angentreu (oder isometrisch); (ii) x ⊥ y ⇒ F(x) ⊥ F(y), d.h. F ist winkeltreu; (iii) ker(F) = {0}, d.h. F ist injektiv; Ist dim(X) = dim(Y) < ∞, so gilt zus¨atzlich: (I) F ist ein Isomorphismus; (II) Ist {b1, ..., bn} eine Orthonormalbasis (ONB) von X, so ist {F(b1), ..., F(bn)} eine ONB von Y; (III) F−1 ist unit¨ar (bzw. orthogonal; (IV) Die Abbildungsmatrix A bez ¨uglich orthonormierten Basen in X und Y ist unit¨ar (bzw. orthogonal). 92 9.4. Singul¨arwertzerlegung 9.4 Singul¨arwertzerlegung Satz 9.26 Sei A ∈ En×n symmetrisch (AT = A) oder hermitesch (AH = AT = AT = A), dann gilt: (i) A hat nur reelle Eigenwerte (ii) Eigenvektoren zu verschiedenen Eigenwerten sind paarweise orthogonal Bemerkung 9.27 Um einen Orthonormalbasis aus Eigenvektoren einer symmetisch/hermiteschen Ma- trix zu erhalten, muss man die Eigenvektoren nur normieren und gegebenenfalls mit Gram-Schmidt orthogonalisieren (macht man falls es einen Eigenwert mit geometri- scher Vielfachheit > 1 gibt). Satz 9.28 (Singul¨arwertzerlegung) Sei A ∈ Em×n, rang(A) =: r ≤ min{m, n}. Dann existieren U ∈ Em×m orthogo- nal/unit¨ar, V ∈ En×n orthogonal/unit¨ar, Σ ∈ Rm×n, wobei Σ =   Σr 0 0︸︷︷︸ r 0︸︷︷︸ n−r   } r} m − r, mit Σr =       σ1 0 · · · 0 0 . . . . . . ... ... . . . . . . 0 0 · · · 0 σr       , σ1 ≥ ... ≥ σr > 0 eine verallgemeinerte Diagonalmatrix mit Singul¨arwerten σ1, ..., σr ist, so dass A = UΣV H, d.h. A besitzt eine Singul¨arwertezerlegung. Satz 9.29 Sei A ∈ Em×n. AH A = (UΣV H)HUΣV H = V ΣH ︸︷︷︸ =ΣT; U HU︸ ︷︷ ︸ =1n da U unit¨ar ΣV H = VΣTΣV H ⇒ AH A ist ¨ahnlich zu (im Sinne von unit¨arer Spektralzerlegung) zu ΣTΣ =              σ2 1 0 · · · 0 0 · · · 0 0 . . . . . . ... ... ... ... . . . . . . 0 ... ... 0 · · · 0 σ2 r 0 · · · 0 0 · · · · · · 0 0 · · · 0 ... ... ... ... 0 · · · · · · 0 0 · · · 0              ∈ Rn×n 93 9. Addendum ⇒ σi = √λi, wobei λi die positiven reellen Eigenwerte von AH A sind. Anwendung. 1) Die Singul¨arwertzerlegung ist gewissermassen eine Verallgemeinerung von der Spektralzerlegung f ¨ur Rechtecksmatrizen. 2) Singul¨are LGS und Ausgleichsprobleme (kleinste Quadrate) l¨osen. 3) Bildkompressionsverfahren. Dabei wird ein Bilde als Matrix von Farbwerten be- trachtet, wovon die Singul¨arwertzerlegung berechnet wird. Bei der R ¨ucktransformation von UΣV H nach A werden aber nur noch die ßtark von 0 abweichendenSSin- gul¨arwerte gespeichert, im Betrag kleine Singul¨arwerte werden vernachl¨assigt. Kochrezept 9.30 (Singul¨arwertzerlegung) Gegeben: A ∈ Em×n, m ≥ n (sonst betrachte AH) Gesucht: U ∈ Em×m, Σ ∈ Rm×n, V ∈ En×n 1) Berechne B = AH A ∈ En×n 2) Berechne die Eigenwerte von B, ”nummeriereßie der Gr¨osse nach: λ1 ≥ ... ≥ λr > λr+1 = ... = λn = 0, wobei r = rang(B) = rang(AH A) 3) Σij = {σi = √λi, i = j 0, sonst 4) Bilde die Orthonormalbasis {w1, ..., wn} mit Gram-Schmidt mit Eigenvektoren {v1, ..., vn} von B. (Betrachte B = AH A ist hermitesch, d.h. verwende Bemer- kung 9.27) ⇒ V = (w1| · · · |wn) 5) ∀i ∈ {1, ..., r} definiere u1 := 1 σi Awi ⇒ ui sind orthonormal/unit¨ar Erg¨anze {u1, ..., ur} zu einer Orthonormalbasis von Em×m mit Gram-Schmidt ⇒ U = (u1|...|um) 94 9.4. Singul¨arwertzerlegung 6) Test: A ? = UΣV H ✓ Beispiel 9.31 Gegeben: A =  1 0 2 1 0 1   Gesucht: U, Σ, V so dass A = UΣV H; Zusatzfrage: Beschreibe die vier Fundamen- talr¨aume gem¨ass Satz 11.1 (siehe Skirpt). 1) B = AH A = AT A = (5 2 2 2 ) 2) Eigenwerte berechnen von B: χB(λ) = ∣ ∣ ∣ ∣5 − λ 2 2 2 − λ ∣ ∣ ∣ ∣ = (5 − λ)(2 − λ) − 4 = 10 − 2λ − 5λ + λ2 − 4 = λ2 − 7λ + 6 = (λ − 1)(λ − 6) ! = 0 ⇒ λ1 = 6 > λ2 = 1 3) σ1 = √6, σ2 = √1 = 1 ⇒ Σ =   √6 0 0 1 0 0   4) E6 = ker(B − 61) = span {(2 1 )} ⇒ v1 = (2 1 ) normieren ⇝ w1 = 1 √5 (2 1 ) E1 = ker(B − 1) = span {( 1 −2 )} ⇒ v2 = ( 1 −2 ) normieren ⇝ w2 = 1 √5 ( 1 −2 ) ⇒ V = 1 √5 (2 1 1 −2 ) 95 9. Addendum 5) u1 = 1 √6 1 √5  1 0 2 1 0 1   (2 1 ) = 1 √30  2 5 1   u2 = 1 √1 1 √5  1 0 2 1 0 1   ( 1 −2 ) = 1 √5   1 0 −2   ̃u3 = e2 − ⟨e2, u1⟩u1 − ⟨e2, u2⟩u2 =  0 1 0   − 1 √30 1 √30 ⟨  0 1 0   ,  2 5 1  ⟩  2 5 1   − 1 √5 1 √5 ⟨  0 1 0   ,   1 0 −2  ⟩   1 0 −2   =  0 1 0   − 5 30  2 5 1   − 0 ·   1 0 −2   =  0 1 0   − 1 6  2 5 1   = 1 6  −2 1 −1   u3 = ̃u3 ∥ ̃u3∥ = 1 √6  −2 1 −1   ⇒ U =    2√30 1√5 − 2√6 5√30 0 1√6 1√30 − 2√5 − 1√6    6) Test: A = UΣV H ✓ 7) Die vier Fundamentalr¨aume: im(A) = {u1, u2} im(AH) = {w1, w2} ker(AH) = {u3} ker(A) = {} 96 9.4. Singul¨arwertzerlegung Beispiel 9.32 Gegeben: A =   1 0 1 1 −1 1   Gesucht: U, Σ, V so dass A = UΣV H L¨osung: Σ =   √3 0 0 √2 0 0   , V = (1 0 0 1 ) = 12, U =    1√3 0 2√6 1√3 1√2 − 1√6 − 1√3 1√2 1√6    97 Literaturverzeichnis 99","libVersion":"0.5.0","langs":""}