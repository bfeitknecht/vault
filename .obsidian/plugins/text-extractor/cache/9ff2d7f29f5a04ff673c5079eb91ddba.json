{"path":"sem2/AuW/PV/extra/AuW-formulas.pdf","text":"ETH Z¨urich Institute of Theoretical Computer Science Prof. Rasmus Kyng Prof. Angelika Steger FS 2024 Algorithmen und Wahrscheinlichkeit Formelsammlung Notation log n Logarithmus zur Basis 2. ln n nat¨urlicher Logarithmus. Kn vollst¨andiger Graph mit n Knoten. Pn Pfad-Graph mit n Knoten und n − 1 Kanten, entspricht einem Pfad der L¨ange n − 1. Cn Kreis-Graph mit n Knoten und n Kanten. Qd d-dimensionaler Hyperw¨urfel mit 2 d Knoten. G[A] Teilgraph N (v) Nachbarschaft von v. AG Adjazenzmatrix von G. A ⊎ B disjunkte Vereinigung von A und B; G = (A ⊎ B, E) ist ein bipartiter Graph mit partiten Mengen A und B. A ⊕ B symmetrische Mengendiﬀerenz; A ⊕ B ist die Menge aller Elemente, die jeweils in einer von A und B, aber nicht in beiden Mengen liegen. deg(v) Grad von v / Anzahl Nachbarn von v. δ(G) Minimalgrad von G. ∆(G) Maximalgrad von G. χ(G) chromatische Zahl von G. E(S, T ) Menge der Kanten mit einem Endknoten in S und dem anderen in T , wobei S, T ⊆ V . G/e durch Kontraktion von e aus G entstehender Graph. E [X] Erwartungswert von X. Var [X] Varianz von X. σ[X] Standardabweichung von X. fX Dichtefunktion von X (gegebenenfalls Randdichte). FX Verteilungsfunktion von X. fX,Y gemeinsame Dichte von X und Y . FX,Y gemeinsame Verteilung von X und Y . v0v1 Liniensegment zwischen v0 und v1. C(P ) kleinster umschliessender Kreis von P . conv(S) konvexe H¨ulle von S. Wichtige Verteilungen Name Bezeichnung Wertebereich Dichte Erwartungswert Varianz Bernoulli Bernoulli(p) {0, 1} fX (i) = { p f¨ur i = 1, 1 − p f¨ur i = 0. p p(1 − p) Binomial Bin(n, p) {0, 1, . . . , n} fX (i) = (n i )p i(1 − p) n−i np np(1 − p) Geometrisch Geo(p) N fX (i) = p(1 − p) i−1 1 p 1−p p2 Poisson Po(λ) N0 fX (i) = e−λλ i i! λ λ Erwartungswert • Deﬁnition: E[X] := ∑ x∈WX x · Pr[X = x] • Linearit¨at: F¨ur a1, . . . , an, b ∈ R gilt E[a1X1 +. . .+anXn +b] = a1 E[X1]+. . .+an E[Xn]+b. • Summenformel: Ist WX ⊆ N0, dann gilt E[X] = ∑∞ i=1 Pr[X ≥ i]. • Multiplikativit¨at: F¨ur unabh¨angige X1, . . . , Xn gilt E[X1 · . . . · Xn] = E[X1] · . . . · E[Xn]. Varianz • Deﬁnition: Var[X] := E[(X − E[X]) 2] = E[X 2] − E[X] 2. • Translation: F¨ur a, b ∈ R gilt Var[a · X + b] = a 2 · Var[X]. • Standardabweichung: σ[X] := √ Var[X]. • Additivit¨at: F¨ur unabh¨angige X1, . . . , Xn gilt Var[X1 +. . .+Xn] = Var[X1]+. . .+Var[Xn]. H¨ohere Momente • k-tes Moment: E[X k]. • k-tes zentrale Moment: E[(X − E[X]) k]. Bedingte Wahrscheinlichkeiten • Deﬁnition: Ist Pr[B] > 0, so ist Pr[A|B] := Pr[A∩B] Pr[B] . • Multiplikationssatz: Ist Pr[A1 ∩ . . . ∩ An] > 0, so ist Pr[A1 ∩ · · · ∩ An] = Pr[A1] · Pr[A2|A1] · Pr[A3|A1 ∩ A2] · . . . · Pr[An|A1 ∩ · · · ∩ An−1]. • Satz von der totalen Wahrscheinlichkeit: Ist Ω = A1 ⊎ · · · ⊎ An mit Pr[A1], . . . , Pr[An] > 0, so gilt Pr[B] = ∑n i=1 Pr[B|Ai] · Pr[Ai]. • Satz von Bayes: Ist B ⊆ A1 ⊎ · · · ⊎ An mit Pr[A1], . . . , Pr[An], Pr[B] > 0, so gilt Pr[Ai|B] = Pr[Ai ∩ B] Pr[B] = Pr[B|Ai] · Pr[Ai] ∑n j=1 Pr[B|Aj] · Pr[Aj] . Unabh¨angigkeit • Deﬁnition: X1, . . . , Xn heissen genau dann unabh¨angig, wenn f¨ur alle (x1, . . . , xn) ∈ WX1 × . . . × WXn gilt: Pr[X1 = x1, . . . , Xn = xn] = Pr[X1 = x1] · . . . · Pr[Xn = xn]. • Multiplikationsformel: Sind X1, . . . , Xn unabh¨angig und Si ⊆ WXi, dann gilt: Pr[X1 ∈ S1, . . . , Xn ∈ Sn] = Pr[X1 ∈ S1] · . . . · Pr[Xn ∈ Sn]. • Transformationen: Seien fi : R → R. Wenn X1, . . . , Xn unabh¨angig sind, dann gilt dies auch f¨ur f (X1), . . . , f (Xn). • Summe: Sind X, Y unabh¨angig und Z := X + Y , so gilt fZ(z) = ∑ x∈WX fX (x) · fY (z − x). Absch¨atzungen • Boolesche Ungleichung, Union Bound: Pr [ ⋃n i=1 Ai] ≤ ∑n i=1 Pr[Ai]. • Markov: Ist WX ⊆ R≥0 und t ∈ R≥0, so ist Pr[X ≥ t] ≤ E[X] t bzw. Pr[X ≥ t · E[X]] ≤ 1 t . • Chebyshev: F¨ur t ∈ R≥0 ist Pr[|X −E[X]| ≥ t] ≤ Var[X] t2 bzw. Pr[|X −E[X]| ≥ t·σ[X]] ≤ 1 t2 . • Chernoﬀ: Seien X1, . . . , Xn unabh¨angig und Bernoulli-verteilt, X := ∑n i=1 Xi und δ ∈ [0, 1]. Dann ist Pr[X ≥ (1 + δ)E[X]] ≤ e− 1 3 δ2 E[X], Pr[X ≤ (1 − δ)E[X]] ≤ e− 1 2 δ2 E[X], Pr[X ≥ t] ≤ 2−t f¨ur t ≥ 2eE[X]. Andere S¨atze zur Wahrscheinlichkeit • Siebformel: Pr [ ⋃n i=1 Ai] = ∑n l=1(−1)l+1 ∑ 1≤i1<···<il≤n Pr[Ai1 ∩ · · · ∩ Ail ]. • Waldsche Identit¨at: Sind N und X unabh¨angig, WN ⊆ N, und sind X1, X2, . . . unabh¨angi- ge Kopien von X, dann gilt E[ ∑N i=1 Xi] = E[N ] · E[X]. Fehlerreduktionen: • Wiederholung MC: Eine N -fache Wiederholung mit N = 4ε−2 ln δ−1 steigert die Erfolgs- wahrscheinlichkeit eines Monte-Carlo-Algorithmus von 1 2 + ε auf ≥ 1 − δ. • Wiederholung MC mit einseitigem Fehler: Eine N -fache Wiederholung mit N = ε −1 ln δ−1 steigert f ˜A¼r einen Monte-Carlo-Algorithmus mit einseitigem Fehler die Erfolgs- wahrscheinlichkeit von ε auf ≥ 1 − δ. • Target Shooting: Bestimmt der Target-Shooting-Algorithmus eine Menge S ⊆ U mit N ≥ 3 |U | |S| ε −2 ln (2/δ) Versuchen, so ist die Ausgabe mit Wahrscheinlichkeit ≥ 1 − δ im Intervall [ (1 − ε) |S| |U | , (1 + ε) |S| |U | ].","libVersion":"0.3.2","langs":""}