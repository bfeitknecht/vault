{"path":"sem3/LinAlg/UE/s/LinAlg-s-u04.pdf","text":"D-INFK Linear Algebra HS 2024 Bernd G¨artner Robert Weismantel Solution for Assignment 4 1. Consider the three constraints p(−1) = 0, p(0) = 2 and p(1) = 2 that we have on p. Each of these constraints gives us an equation involving the unknowns a, b and c. In particular, we get the three equations a − b + c = 0 from p(−1) = 0 c = 2 from p(0) = 2 a + b + c = 2 from p(1) = 2 that we can also write down in matrix form   1 −1 1 0 0 1 1 1 1     a b c   =   0 2 2   . In order to solve this system, let us use the elimination method from the lecture. For this, let us define A =   1 −1 1 0 0 1 1 1 1   and b =   0 2 2   . We already highlighted the first pivot in A. After one step of elimination, we get E21A =   1 −1 1 0 0 1 1 1 1   and E21b =   0 2 2   with E21 = I as we already had a21 = 0. In the second step, we obtain E31E21A =   1 −1 1 0 0 1 0 2 0   and E31E21b =   0 2 2   with E31 =   1 0 0 0 1 0 −1 0 1  . Next, we need to permute rows 2 and 3 with the matrix P23 =   1 0 0 0 0 1 0 1 0   in order to get our next pivot. We obtain P23E31E21A =   1 −1 1 0 2 0 0 0 1   and P23E31E21b =   0 2 2   . In the last elimination step we again find that we do not need to do anything. In other words, we have E32 = I and get E32P23E31E21A =   1 −1 1 0 2 0 0 0 1   and E32P23E31E21b =   0 2 2   . We arrived at the desired upper triangular shape. It remains to use back substitution to get c = 2, b = 1 and a = −1. 1 2. Note that A is already upper triangular. Hence, we can solve the three systems Ax1 = e1, Ax2 = e2, and Ax3 = e3 to find the inverse A−1 =   | | | x1 x2 x3 | | |   . In the first system Ax1 =   a b c 0 1 d 0 0 1   x1 =   1 0 0   = e1 we find x1 =   1 a 0 0   by backwards substitution. In the second system Ax2 =   a b c 0 1 d 0 0 1   x2 =   0 1 0   = e2 we find x2 =   −b a 1 0   by backwards substitution. Finally, we find x3 =   bd−c a −d 1   in the third system Ax3 =   a b c 0 1 d 0 0 1   x3 =   0 0 1   = e3 by backward substitution. The inverse of A is hence given by A−1 =   | | | x1 x2 x3 | | |   =   1 a − b a bd−c a 0 1 −d 0 0 1   whenever a ̸= 0. In the case where a = 0, the matrix A is not invertible as its columns are not linearly independent (the first column is 0). In other words, A is invertible for all choices of a, b, c, d ∈ R where a ̸= 0. 3. a) To solve this exercise, we could use the elimination procedure for each system and then read off the solution. But the procedure would never actually eliminate anything (only reorder rows). In particular, the solutions to the system can be read off without using elimination as every row of A has a unique non-zero element. We first consider the system   0 0 1 1 0 0 0 1 0   x1 =   1 0 0   = e1. To get the 1 in e1, the third coordinate of x1 has to be 1. To get the two zeroes of e1, the other two coordinates of x1 have to be zero. Hence, we get x1 = e3. Analogously, we obtain x2 = e1 and x3 = e2 from the other two systems. 2 b) Recall that the inverse A−1 of the m × m matrix A, if it exists, is the unique matrix satisfying AA−1 = I. Now recall our three linear systems from above: Ax1 = e1 Ax2 = e2 Ax3 = e3 and consider what happens when we arrange the vectors (Ax1), (Ax2), (Ax3) as columns in a new matrix   | | | Ax1 Ax2 Ax3 | | |   =   | | | e1 e2 e3 | | |   = I. Notice how this corresponds to the column view of matrix multiplication, i.e. we have A   | | | x1 x2 x3 | | |   =   | | | Ax1 Ax2 Ax3 | | |   =   | | | e1 e2 e3 | | |   = I. From this we can conclude that the matrix X =   | | | x1 x2 x3 | | |   must be the inverse of A. In particular, we have A−1 = X =   | | | x1 x2 x3 | | |   =   | | | e3 e1 e2 | | |   . c) We proceed as above by first solving the three systems Dy1 = e1, Dy2 = e2, and Dy3 = e3. Again, the solutions can be read off directly as D has a unique non-zero entry in every row. In particular, we get the solutions y1 =   1 2 0 0   , y2 =  0 1 3 0   , y3 =   0 0 2   . As above, we conclude that the matrix Y =   | | | y1 y2 y3 | | |   is the inverse of D since we have DY = D   | | | y1 y2 y3 | | |   =   | | | Dy1 Dy2 Dy3 | | |   =   | | | e1 e2 e3 | | |   = I. Concretely, we have D−1 = Y . d) Using the same strategy as above, one could now also determine the inverse of B. But the idea of this exercise is to observe that there is a faster way. In particular, we observe that B can be obtained from A and D as DA = B. From the lecture we know that B−1 can now be computed as B−1 = (DA) −1 = A−1D−1 = XY =   0 1 0 0 0 1 1 0 0     1 2 0 0 0 1 3 0 0 0 2   =  0 1 3 0 0 0 2 1 2 0 0   . 3 4. a) Yes, the inverse of Ak exists and is given by (A−1)k. We will argue by induction over k. • Property: The inverse of Ak is given by (A−1)k. • Base case: For k = 1, the property is true because we are given that A−1 is the inverse of A. • Induction step: Fix a natural number 1 ≤ k and assume that the property is true for this k (induction hypothesis). We prove that the property is true for k + 1, i.e. we prove that the inverse of Ak+1 is (A−1)k+1. By the induction hypothesis, we have Ak(A−1)k = I. Hence, from Ak+1(A−1) k+1 = AkAA −1(A−1)k = Ak(AA−1)(A−1) k = AkI(A−1) k = Ak(A−1) k IH = I we conclude that the property is indeed true for k + 1. b) We prove this by contradiction. Assume for a contradiction that A has an inverse A−1. Since A is nilpotent, we know from Assignment 2 that AA−1 = I is nilpotent too, i.e. there exists some k ∈ N+ such that I k = (AA−1)k = 0. But we have I k = I ̸= 0 which is a contradiction. c) We prove this with the following calculation: A = AI = A(A3) = A4 = I. d) Observe that it suffices to find a 2 × 2 matrix A ̸= I with A2 = I: indeed, for even k = 2ℓ we then get Ak = A2ℓ = (A2)ℓ = I ℓ = I as well by using A2 = I. Such a matrix is sometimes called self-inverse, since we have A−1 = A. We have seen some self-inverse matrices before in this course. In particular, the 2 × 2 rotation matrix for angle ϕ = π is self-inverse. Concretely, this is the matrix A = [cos(π) − sin(π) sin(π) cos(π) ] = [−1 0 0 −1 ] and we can check that indeed A2 = I but A ̸= I. Moreover, for odd k we then have Ak = AAk−1 = AI = A ̸= I by using that k − 1 must be even. e) Again, we use rotation matrices. In particular, the rotation matrix with angle π/2 should satisfy this. The intuition is that whenever we apply this rotation matrix four times, we rotate one whole turn and hence nothing happens. For a complete solution, we check that this indeed works. For the matrix A = [ cos(π/2) − sin(π/2) sin(π/2) cos(π/2) ] = [ 0 −1 1 0 ] we get A2 = [0 −1 1 0 ]2 = [−1 0 0 −1 ] A3 = A2 [0 −1 1 0 ] = [ 0 1 −1 0 ] A4 = A3 [0 −1 1 0 ] = [1 0 0 1 ] . Now consider an arbitrary k > 4. Then k can be split as k = 4ℓ + h where ℓ, h ∈ N0 and 0 ≤ h < 4 and we get Ak = (A4)ℓAh = Ah. We conclude that Ak = I if and only if h = 0 which is the same as saying that Ak = I if and only if k ≡4 0. 4 5. a) Let x ∈ Rn be arbitrary and assume Bx = 0. Recall that Bx = x1b1 + x2b2 + · · · + xnbn where x1, x2, . . . , xn are the entries of x and b1, b2, . . . , bn are the columns of B. Hence, in order to prove that the columns of B are linearly independent, we have to prove x = 0. We achieve this with the calculation x = Ix = ABx = A0 = 0. b) Let y ∈ Rn be arbitrary and assume Ay = 0. We want to prove that y = 0. By the inverse theorem and using subtask a), we know that there exists x ∈ Rn with Bx = y. Now observe that indeed, we have y = Bx = BIx = B(ABx) = B(Ay) = 0 and hence we conclude that the columns of A are linearly independent. c) By applying the matrix A to BA − I we get A(BA − I) = ABA − A = IA − A = 0. In particular, we have Av = 0 for every column v of BA − I. But by the inverse theorem and subtask b), the equation Ay = 0 has a unique solution y. And we also know that 0 is a solution since A0 = 0. By uniqueness, we conclude that we have v = 0 for every column v of BA − I and hence BA − I = 0. 6. a) Using the formula for 2 × 2 matrices, we get L −1 = [ 1 0 −a 1 ] . b) We prove both directions individually. First, assume that we are given an arbitrary m × m lower triangular matrix L with no zeroes on its diagonal. We want to prove that L is invertible. Let v1, v2, . . . , vn be the columns of L. We claim that these vectors are linearly independent. Indeed, checking them from right to left, we can see that for any i ∈ [n], there is no way of obtaining vn−i from vn−i+1, . . . , vn: by assumption, the (n−i)-th entry of vn−i is non-zero, but by the triangular shape of L, the (n − i)-th entries in all of vn−i+1, . . . , vn are zero. This argument works for all i and we conclude that the columns of L are linearly independent. Hence, by the inverse theorem, L is invertible. The reverse direction is a bit more difficult. We provide an indirect proof. For this, let L be an arbitrary lower triangular m × m matrix with a zero on its diagonal. We want to prove that L is not invertible. Let i ∈ [m] be such that ℓii = 0. Without loss of generality, assume that this is the last zero on the diagonal, i.e. we have ℓjj ̸= 0 for all integers i < j ≤ m. If i = m, the last column of L is 0 and hence the columns of L are dependent. By the inverse theorem, this implies that L is not invertible, as desired. Thus, assume from now on i < m. Now consider the (m − i) × (m − i) submatrix L′ in the bottom right corner of L, i.e. the entries of L′ are ℓ′ jk := ℓ(i+j),(i+k). Observe that L′ is lower triangular and all entries on its diagonal are non-zero (by our choice of i). But then, we know from above (the other direction of this subtask) that L′ is invertible. By the inverse theorem, this implies that the linear system L ′x =      ℓ(i+1),i ℓ(i+2),i ... ℓm,i      5 has a solution. By adding rows of zeroes to the system, we then get that             0 0 . . . 0 ... ... . . . ... 0 0 . . . 0 ℓ(i+1),(i+1) ℓ(i+1),(i+2) . . . ℓ(i+1),m ℓ(i+2),(i+1) ℓ(i+2),(i+2) . . . ℓ(i+2),m ... ... . . . ... ℓm,(i+1) ℓm,(i+2) . . . ℓm,m             x =             0 ... 0 = ℓii ℓ(i+1),i ℓ(i+2),i ... ℓm,i             has a solution as well. In other words, the last m − i + 1 columns of L are linearly dependent. Hence, L is not invertible by the inverse theorem. c) Let L be an arbitrary m × m lower triangular matrix with inverse L−1. We want to prove that L−1 is lower triangular. Assume for a contradiction, that there exists (L−1)ij ̸= 0 for some i, j ∈ [m] with i < j. Moreover, assume without loss of generality that (L−1)ij is the rightmost non-zero entry in row i, i.e. j is maximized given i. Let u ∈ Rm be the i-th row of L−1 and let v ∈ Rm be the j-th column of L. Note that we must have u · v = (L−1L)ij = 0 since i ̸= j. But observe that by the triangular shape of L, vk = 0 for all integers 1 ≤ k < j. Recall that (L−1)ij is the rightmost non-zero entry in row i. Hence, we also have uk = 0 for all j < k ≤ n. We conclude that 0 = (L −1L)ij = u · v = u1v1 + · · · + ujvj + · · · + umvm = ujvj ̸= 0 since we have uj ̸= 0 by assumption and vj ̸= 0 from the previous subtask. This is a contradiction. Hence, L−1 must be lower triangular. d) Yes, both statements also hold for upper triangular matrices. To see this, consider an arbitrary m × m upper triangular matrix U . Then U ⊤ is lower triangular. The diagonal entries of U are non-zero if and only if the diagonal entries of U ⊤ are non-zero. Moreover, U is invertible if and only if U ⊤ is invertible, since (U −1)⊤ = (U ⊤)−1 (so if either of the inverses exists, the other exists as well). Because U ⊤ is lower triangular, we know that (U ⊤)−1, if it exists, must be lower triangular as well. In particular, the inverse of U , if it exists, must be upper triangular by U −1 = ((U −1)⊤)⊤ = ((U ⊤)−1)⊤. 6","libVersion":"0.5.0","langs":""}