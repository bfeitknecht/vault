{"path":"sem2a/PProg/VRL/slides/PProg-L08-L09-div&conq-cilk.pdf","text":"Parallel Programming Divide and Conquer, Cilk-style bounds Big Picture (Part I) 2 CPU OS JVM (Process A) Core Core Core Core OS thread OS thread OS thread OS thread OS scheduler JVM scheduler JVM thread Process B Memory Space A Memory Space B Physical Memory JVM thread JVM thread JVM threadL03-05 L06 L07 L08-09 L10-L11 L13 … … Stack Registers PC Stack Registers PC Stack Registers PC Stack Registers PC Parallel performance & algorithms L12Virtual threads Lets look at a code example: sum the elements of a list 3 Sequential Version The first step of writing a parallel program is writing a sequential version: • Helps validate our eventual parallel program is correct • by comparing results with the simpler, sequential version • Evaluate the performance of our parallel program • we write parallel programs to improve performance! 4 Adding Numbers - Sequentially public static int sum(int[] input){ int sum = 0; for(int i=0; i<input.length; i++){ sum += input[i]; } return sum; } 5 Parallelism idea Idea: Have 4 threads simultaneously sum 1/4 of the array • Warning: This is an inferior first approach • Create 4 thread objects, each given a portion of the work • Call start() on each thread object to actually run it in parallel • Wait for threads to finish using join() • Add together their 4 answers for the final result + ans ans0 ans1 ans2 ans3 67 Code example: PP-L09-01ArraySum First attempt, part 1 8 class SumThread extends java.lang.Thread { int lo; // arguments int hi; int[] arr; int ans = 0; // result SumThread(int[] a, int l, int h) { lo=l; hi=h; arr=a; } public void run() { //override must have this type for(int i=lo; i < hi; i++) ans += arr[i]; } } Because we must override a no-arguments/no-result run, we use fields to communicate across threads First attempt, continued (wrong) 9 class SumThread extends java.lang.Thread { int lo, int hi, int[] arr; // arguments int ans = 0; // result SumThread(int[] a, int l, int h) { … } public void run(){ … } // override } int sum(int[] arr){ // can be a static method int len = arr.length; int ans = 0; SumThread[] ts = new SumThread[4]; for(int i=0; i < 4; i++) // do parallel computations ts[i] = new SumThread(arr,i*len/4,(i+1)*len/4); for(int i=0; i < 4; i++) // combine results ans += ts[i].ans; return ans; } Second attempt (still wrong) 10 int sum(int[] arr){// can be a static method int len = arr.length; int ans = 0; SumThread[] ts = new SumThread[4]; for(int i=0; i < 4; i++){// do parallel computations ts[i] = new SumThread(arr,i*len/4,(i+1)*len/4); ts[i].start(); // start actually runs the thread in parallel } for(int i=0; i < 4; i++) // combine results ans += ts[i].ans; return ans; } class SumThread extends java.lang.Thread { int lo, int hi, int[] arr; // arguments int ans = 0; // result SumThread(int[] a, int l, int h) { … } public void run(){ … } // override } Third attempt (correct in spirit) 11 int sum(int[] arr){// can be a static method int len = arr.length; int ans = 0; SumThread[] ts = new SumThread[4]; for(int i=0; i < 4; i++){// do parallel computations ts[i] = new SumThread(arr,i*len/4,(i+1)*len/4); ts[i].start(); } for(int i=0; i < 4; i++) { // combine results ts[i].join(); // wait for helper to finish! ans += ts[i].ans; } return ans; } class SumThread extends java.lang.Thread { int lo, int hi, int[] arr; // arguments int ans = 0; // result SumThread(int[] a, int l, int h) { … } public void run(){ … } // override } Discussion The Thread class defines various methods you could not implement on your own • For example: start, which calls run in a new thread The join method is valuable for coordinating this kind of computation • Caller blocks until/unless the receiver is done executing (meaning the call to run finishes) • Else we would have a race condition on ts[i].ans This style of parallel programming is called fork/join Java detail: code has 1 compile error because join may throw java.lang.InterruptedException • In basic parallel code, should be fine to catch-and-exit 1214 Shared memory? Fork-join programs (thankfully) do not require much focus on sharing memory among threads But in languages like Java, there is memory being shared. In our example: • lo, hi, arr fields written by “main” thread, read by helper thread • ans field written by helper thread, read by “main” thread When using shared memory, you must avoid race conditions (we will see a more formal definition of data races, later) 15 Issues with this approach (and some workarounds) Several reasons why this is a poor parallel algorithm Reason 1: want code to be reusable and efficient across platforms 16 int sum(int[] arr){// can be a static method int len = arr.length; int ans = 0; SumThread[] ts = new SumThread[4]; for(int i=0; i < 4; i++){// do parallel computations ts[i] = new SumThread(arr,i*len/4,(i+1)*len/4); ts[i].start(); } for(int i=0; i < 4; i++) { // combine results ts[i].join(); // wait for helper to finish ans += ts[i].ans; } return ans; } 17 Code example: PP-L09-02ParameterizedThreads Issues with this approach (and some workarounds) Several reasons why this is a poor parallel algorithm Reason 1: want code to be reusable and efficient across platforms • “Forward-portable” as core count grows • So at the very least, parameterize by the number of threads 18 int sum(int[] arr, int numTs){ int ans = 0; SumThread[] ts = new SumThread[numTs]; for(int i=0; i < numTs; i++){ ts[i] = new SumThread(arr,(i*arr.length)/numTs, ((i+1)*arr.length)/numTs); ts[i].start(); } for(int i=0; i < numTs; i++) { ts[i].join(); ans += ts[i].ans; } return ans; } Issues with this approach (and some workarounds) Reason 2: want to use (only) processors “available to you now” • Not used by other programs or threads in your program • Maybe caller is also using parallelism • Available cores can change even while your threads run 19 // numThreads == numProcessors is bad // if some are needed for other things int sum(int[] arr, int numTs){ … } Issues with this approach (and some workarounds) Reason 3: Though unlikely for sum, in general subproblems may take significantly different amounts of time Example: Apply method f to every array element, but maybe f is much slower for some data items, e.g.: is a large integer prime? If we create 4 threads and all slow data is processed by 1 of them, we won’t get nearly a 4x speedup • Example of a load imbalance 2021 A Better Approach The counterintuitive (?) solution to all these problems is to use lots of threads, far more than the number of processors • But this will require changing our algorithm • And for constant-factor reasons, abandoning Java’s threads 1. Forward-portable: Lots of helpers each doing a small piece 2. Processors available: Hand out “work chunks” as you go 3. Load imbalance: No problem if slow thread scheduled early enough • Variation probably small anyway if pieces of work are small ans0 ans1 ansN ans … 22 Divide and Conquer to the Rescue! This is straightforward to implement using divide-and-conquer • Parallelism for the recursive calls 23 + + + + + + + + + + + + + + + Divide and Conquer Fundamental pattern in parallel programming, also called recursive splitting Divide and Conquer: if cannot divide: return unitary solution (stop recursion) divide problem in two solve first (recursively) solve second (recursively) combine solutions return result 24 Sequential Version: Recursive Sum public static int do_sum_rec(int[] xs, int l, int h) { int size = h-l; if (size == 1) /*check for termination criteria*/ return xs[l]; /* split array in half and call self recursively*/ int mid = size / 2; int sum1 = do_sum_rec(xs, l, l + mid); int sum2 = do_sum_rec(xs, l + mid, h); return sum1 + sum2; } 252627 Code example: PP-L09-03ParallelRecursiveSum 2829 Parallel Recursive Sum (with Threads) public class SumThread extends Thread { int[] xs; int h, l; int result; public SumThread(int[] xs, int l, int h){ super(); this.xs = xs; this.h = h; this.l =l; } public void run(){ /*Do computation and write to result*/ return; } 30 Parallel Recursive Sum (with Threads) public void run(){ int size = h-l; if (size == 1) { result = xs[l]; return; } int mid = size / 2; SumThread t1 = new SumThread(xs, l, l + mid); SumThread t2 = new SumThread(xs, l + mid, h); t1.start(); t1.join(); t2.start(); t2.join(); result = t1.result + t2.result; return; } 31 Is this OK? Parallel Recursive Sum (with Threads) public void run(){ int size = h-l; if (size == 1) { result = xs[l]; return; } int mid = size / 2; SumThread t1 = new SumThread(xs, l, l + mid); SumThread t2 = new SumThread(xs, l + mid, h); t1.start(); t2.start(); t1.join(); t2.join(); result = t1.result + t2.result; return; } 32 Remark: This doesn’t compile because join() can throw exceptions. In reality we need a try-catch block here. Result 33 Java.lang.OutOfMemoryError: unable to create new native thread One thread per parallel task model Java threads are actually quite heavyweight Java threads are mapped to OS threads (in the Oracle and most real- world implementations) In general: using one thread per (small tasks) is highly inefficient 34 Divide-and-Conquer works – (really, we’ll get there) In theory, you can divide down to single elements, do all your result- combining in parallel and get optimal speedup In practice, creating all those threads and communicating swamps the savings, so: • Use a sequential cutoff, typically around 500-1000 • Eliminates almost all the recursive thread creation (bottom levels of tree) • Do not create two recursive threads; create one and do the other “yourself” • Cuts the number of threads created by another 2x 35 Divide-and-conquer – with manual fixes (Pt. I) public void run(){ int size = h-l; if (size < SEQ_CUTOFF) for (int i=l; i<h; i++) result += xs[i]; else { int mid = size / 2; SumThread t1 = new SumThread(xs, l, l + mid); SumThread t2 = new SumThread(xs, l + mid, h); t1.start(); t2.start(); t1.join(); t2.join(); result = t1.result + t2.result; } } 36 Half the threads If a language had built-in support for fork-join parallelism, we would expect this hand-optimization to be unnecessary But the library we are using expects you to do it yourself (and the difference is surprisingly substantial) Again, no difference in theory 37 // wasteful: don’t SumThread t1 = … SumThread t2 = … t1.start(); t2.start(); t1.join(); t2.join(); result=t1.result+t2.result; // better: do // order of next 4 lines // essential – why? t1.start(); t2.run(); t1.join(); result=t1.result+t2.result; 3839 Code example: PP-L09-04RecursiveSumOpt Divide-and-conquer really works – (but it’s hard work) The key is divide-and-conquer parallelizes the result-combining • If you have enough processors, total time is height of the tree: O(log n) (optimal, exponentially faster than sequential O(n)) • Often relies on operations being associative (like +) Will write all our parallel algorithms in this style • But using special libraries engineered for this style • Takes care of scheduling the computation well 41 + + + + + + + + + + + + + + + Recap: One thread per task model Java threads are actually quite heavyweight Java threads are mapped to OS threads In general: using one thread per (small tasks) is highly inefficient 4243 Alternative approach: schedule tasks on threads Threads How many threads would you use? Tasks (Thread pool) 44 Java's executor service: managing asynchronous tasks Tasks ExecutorService Interface Implementation e.g.: ThreadPoolExecutor (Thread pool) Java's executor service:managing asynchronous tasks 46 ExecutorService User submits tasks gets back a Future .submit(Callable<T> task) → Future<T> .submit(Runnable task) → Future<?> 47 Note: Callable vs Runnable Interface Runnable: → void run() Interface Callable<T>: → T call() Does not return result Returns result ExecutorService can handle “Runnable” or “Callable” tasks: 48 Code example: PP-L10-01ExecutorHelloTask Using executor service: Hello World (creating executor, submitting) int ntasks = 1000; ExecutorService exs = Executors.newFixedThreadPool(4); for (int i=0; i<ntasks; i++) { HelloTask t = new HelloTask(\"Hello from task \" + i); exs.submit(t); } exs.shutdown(); // initiate shutdown, does not wait, but can’t submit more tasks 49 Using executor service: Hello World (task) static class HelloTask implements Runnable { String msg; public HelloTask(String msg) { this.msg = msg; } public void run() { long id = Thread.currentThread().getId(); System.out.println(msg + \" from thread:\" + id); } } 50 Using executor service: Hello World (output) … Hello from task 803 from thread:8 Hello from task 802 from thread:10 Hello from task 807 from thread:8 Hello from task 806 from thread:9 Hello from task 805 from thread:11 Hello from task 810 from thread:9 Hello from task 809 from thread:8 Hello from task 808 from thread:10 Hello from task 813 from thread:8 Hello from task 812 from thread:9 Hello from task 811 from thread:11 ... 5152 Code example: PP-L10-02RecursiveSumExecutor Recursive Sum with ExecutorService public Integer call() throws Exception { int size = h – l; if (size == 1) return xs[l]; int mid = size / 2; sumRecCall c1 = new sumRecCall(ex, xs, l, l + mid); sumRecCall c2 = new sumRecCall(ex, xs, l + mid, h); Future<Integer> f1 = ex.submit(c1); Future<Integer> f2 = ex.submit(c2); return f1.get() + f2.get(); } 53 Simple! – But does this work? If you execute the code, you will observe that it never returns (i.e., the computation is not completed) 54 Why does this happen? tasks will end up waiting eventually we will run out of threads 55 sum(0,100): t1 = spawn sum(0,50) t2 = spawn sum(50,100) t1.wait(); t2.wait() sum(0,50): t1 = spawn sum(0,25) t2 = spawn sum(25,50) t1.wait(); t2.wait() sum(50,100): t1 = spawn sum(50,75) t2 = spawn sum(75,100) t1.wait(); t2.wait() sum(0,25): t1 = spawn sum(0,12) t2 = spawn sum(12,25) t1.wait(); t2.wait() Adding Numbers ExecutorService: another approach Problem with the divide and conquer approach is that tasks create other tasks and work partitioning (splitting up work) is part of the task. A possible approach is to decouple work partitioning from solving the problem. That is we split the array into chunks (how many?) and create a task per chunk. Then, we submit tasks into ExecutorService and combine results (e.g., sum). It can be tricky to do the initial partitioning of work and final summing in parallel. 57 ans0 ans1 ansN ans … Task Parallel Programming [Cilk-style] Tasks: – execute code – spawn other tasks – wait for results from other tasks A graph is formed based on spawning tasks B A C 58 The edges mean that Task B was created by Task A and that Task C was created by Task A fib() Function public class Fibonacci { public static long fib(int n){ if (n < 2) return n; long x1 = fib(n-1); long x2 = fib(n-2); return x1 + x2; } } 59 public class Fibonacci { public static long fib(int n) { if (n < 2) return n; spawn task for fib(n-1); spawn task for fib(n-2); wait for tasks to complete return addition of task results }}} Sequential Version Parallel Version fib(4) task graph spawn join 60 step in same procedure The task graph is a directed acyclic graph (DAG) Task parallelism discussion ● Tasks can execute in parallel – but they don't have to – assignment of tasks to CPUs/cores is up to the scheduler ● Task graph is dynamic – unfolds as execution proceeds ● Intuition: wide task graph → more parallelism 62 Task parallelism: performance model 64 ● Task graph: tasks become available as computation progresses ● We can execute the graph on p processors Scheduler assign tasks to processors ● Tp: execution time on p processors Task parallelism: performance model [some reminders] 65 ● Tp: execution time on p processors ● T1: work (total amount of work) – the sum of the time cost of all nodes in graph – as if we executed graph sequentially (p=1) ● T1 / Tp → speedup Task parallelism: performance model (Bounds) ● T∞: span, critical path – Time it takes on infinite processors – longest path from root to sink ● T1 / T∞ à parallelism – “wider” is better ● Lower Bounds: – Tp ≥ T1 / P – Tp ≥ T∞ 66 On this graph, T∞ is 9 Scheduling of task graphs Scheduler is an algorithm for assigning tasks to processors Note that: Tp depends on scheduler T1 / P and T∞ are fixed 68 What is T2 for this graph? That is, we have 2 processors. T2 will be 5 with this scheduling (we have 5 time steps) 1 2 3 4 5 T2 will be 4 with this scheduling (we have 4 time steps) 1 2 3 4 13 Work stealing scheduler First used in MIT’s Cilk, now a standard method Provably: Tp = T1 / P + O(T∞) Empirically: Tp ≈ T1 / P + T∞ Guideline for parallel programs => ”Scheduling Multithreaded Computations by Work Stealing”, Blumfoe & Leiserson, MIT 70 a bound on how fast you can get on p processors with a greedy scheduler: 𝐓𝐩 ≤ 𝐓𝟏 / 𝐏 + 𝐓∞ Theorem [Graham‘68] Summary Divide and conquer for parallel programming Cilk-style task graphs, scheduling and bounds 71","libVersion":"0.3.2","langs":""}