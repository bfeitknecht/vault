{"path":"sem2/PProg/VRL/extra/PProg-summary-rböhr.pdf","text":"Abstract Parallel Programming Roman Böhringer, June 2018 Introduction Parallel Programming Page 2 of 50 Table of contents 1 Introduction ....................................................................................................................... 5 1.1 Why Parallel Programming? ........................................................................................ 5 1.2 Parallel vs. concurrent ................................................................................................. 5 (Parallel) Programming ............................................................................................................ 5 2 Threads .............................................................................................................................. 5 2.1 Thread safety .............................................................................................................. 6 2.2 Thread states .............................................................................................................. 6 3 Synchronization in Java ................................................................................................... 7 3.1 Exceptions .................................................................................................................. 7 3.2 Operations on the Monitor ........................................................................................... 7 3.3 Interface Lock.............................................................................................................. 8 3.4 Atomic operations ....................................................................................................... 8 4 Executor Service ............................................................................................................... 8 5 ForkJoin framework .......................................................................................................... 9 6 Java Memory Model .......................................................................................................... 9 Parallelism .............................................................................................................................. 10 7 Approaches to parallelism ............................................................................................. 10 7.1 Implicit / explicit parallelism ....................................................................................... 10 7.2 Vectorization ............................................................................................................. 10 7.3 Pipelining .................................................................................................................. 10 7.4 Instruction Level Parallelism ...................................................................................... 11 7.5 Architectures ............................................................................................................. 11 7.5.1 Simultaneous Multithreading (SMT), Intel Hyperthreading ................................. 11 7.5.2 Multicores .......................................................................................................... 11 7.5.3 Symmetric Multi Processing (SMPs) .................................................................. 11 7.5.4 Non-Uniform Memory Access (NUMA) .............................................................. 11 7.5.5 Distributed Memory ........................................................................................... 12 7.6 Expressing parallelism .............................................................................................. 12 7.6.1 Work partitioning / scheduling ............................................................................ 12 7.6.2 Scalability .......................................................................................................... 12 7.6.3 Parallel programming paradigms ....................................................................... 14 8 Distributed memory / message passing ........................................................................ 16 Introduction Parallel Programming Page 3 of 50 8.1 Programming models ................................................................................................ 17 8.1.1 CSP: Communicating Sequential Processes ..................................................... 17 8.1.2 Actor programming model ................................................................................. 17 8.2 Message passing libraries ......................................................................................... 18 8.2.1 MPI (Message Passing Interface) ...................................................................... 18 Concurrency ........................................................................................................................... 21 9 Managing state ................................................................................................................ 21 9.1 Mutual exclusion ....................................................................................................... 21 9.2 Atomicity ................................................................................................................... 21 9.2.1 Atomic operations .............................................................................................. 21 9.3 Types of registers ...................................................................................................... 22 10 Races ........................................................................................................................... 23 10.1 Data races ................................................................................................................. 23 10.2 Interleavings .............................................................................................................. 23 11 Locks ........................................................................................................................... 23 11.1 Decker’s Algorithm .................................................................................................... 24 11.2 Peterson Lock ........................................................................................................... 24 11.3 Filter Lock ................................................................................................................. 25 11.4 Bakery algorithm ....................................................................................................... 25 11.5 Spinlock .................................................................................................................... 26 11.5.1 Test-and-Test-and-Set (TATAS) Lock ............................................................... 26 11.5.2 TATAS with backoff ........................................................................................... 27 11.6 Locks with waiting / scheduling ................................................................................. 27 11.7 Reader / Writer locks ................................................................................................. 27 11.8 Deadlocks ................................................................................................................. 28 11.8.1 Livelock ............................................................................................................. 29 11.9 Lock granularity ......................................................................................................... 29 11.9.1 Coarse grained locking ...................................................................................... 29 11.9.2 Fine grained locking .......................................................................................... 29 11.9.3 Optimistic synchronization ................................................................................. 30 11.9.4 Lazy synchronization ......................................................................................... 30 12 Semaphores ................................................................................................................ 31 12.1 Rendezvous .............................................................................................................. 31 Introduction Parallel Programming Page 4 of 50 12.2 Implementation .......................................................................................................... 32 13 Barriers ........................................................................................................................ 32 14 Monitors ....................................................................................................................... 33 15 Lock-free programming .............................................................................................. 34 15.1 Memory reuse / ABA problem ................................................................................... 35 15.1.1 Solutions ........................................................................................................... 35 16 Transactional memory ................................................................................................ 36 16.1 Implementation .......................................................................................................... 37 17 Concurrency Theory ................................................................................................... 37 17.1 Linearizability ............................................................................................................ 37 17.2 Sequential consistency .............................................................................................. 39 17.3 Quiescent consistency .............................................................................................. 40 18 Consensus................................................................................................................... 40 Parallel algorithms / data structures ..................................................................................... 41 19 Parallel prefix-sum ...................................................................................................... 41 19.1 Parallel pack.............................................................................................................. 42 19.2 Quicksort ................................................................................................................... 42 20 Producer / Consumer Pattern..................................................................................... 43 21 Lazy Skiplist ................................................................................................................ 43 22 Lock-free stack ............................................................................................................ 45 23 Lock-free list set.......................................................................................................... 45 24 Lock-free unbounded queue ...................................................................................... 46 25 Sorting networks ......................................................................................................... 47 25.1 Bitonic sort ................................................................................................................ 49 Introduction Parallel Programming Page 5 of 50 1 Introduction 1.1 Why Parallel Programming? Moore’s Law is still somehow true (Transistors keep getting smaller & faster), but heat and power have become the primary concern in modern computer architecture. Therefore we have smaller, more efficient processors and more processors (often in one package). Furthermore, CPUs are way faster than memory access and there are limits in inherent program’s ILP so it is / was no longer affordable to increase sequential CPU performance. 1.2 Parallel vs. concurrent The key concern of parallelism is to use extra resources to solve a problem faster. The key con- cern of concurrency is to correctly and efficiently manage access to shared resources. In prac- tice, these terms are often used interchangeably. (Parallel) Programming 2 Threads Threads are independent sequences of execution. Multiple threads share the same address space, which is more vulnerable for programming mistakes because threads are not shielded from each other. But because of this, threads share resources and can communicate more eas- ily. Furthermore, context switching between threads is efficient. No change of address space is needed, no automatic scheduling and therefore no saving / reloading of PCB (Process control block; data structure containing the information to manage the scheduling of a process) state. Multiple threads can share a single CPU or run on multiple CPUs. In Java, there’s the java.lang.Thread class for managing Threads. The most important methods are: • start: method called to spawn a new thread (JVM calls run() on object) • interrupt: freeze and throw exception to thread • join: Wait for another thread (the target i.e. the thread we called join on) to terminate. • getId: Gets the thread ID • getState: Gets the thread state There are two options for creating a thread in Java: 1. Instantiate a subclass of java.lang.Thread (must override run()): class ConcurrWriter extends Thread { ... public void run() { ... } Threads Parallel Programming Page 6 of 50 } ConcurrWriter writerThread = new ConcurrWriter(); writerThread.start(); // calls ConcurrWriter.run() 2. Implement a java.lang.Runnable, pass it to the thread object: public class ConcurrReader implements Runnable { ... public void run() { ... ... code here executes concurrently with caller ... } } ConcurrReader readerThread = new ConcurrReader(); Thread t = new Thread(readerThread); t.start(); // calls ConcurrReader.run() automatically Every Java program has at least one execution thread (first execution thread calls the main function). Each call to start method of a Thread object creates a new thread, only creating the object doesn’t start it. Calling run() doesn’t start the thread either! 2.1 Thread safety Informally, thread safety implies program correctness. It is the guarantee that nothing bad ever happens regardless of thread interleavings! 2.2 Thread states Synchronization in Java Parallel Programming Page 7 of 50 3 Synchronization in Java All Objects in Java have an internal lock (inherited from java.lang.Object). A synchronized operation locks the object (only with respect to locks on same object, other code that doesn’t use locks can still modify the lock object or the protected object). The synchronized classifier in a method declaration is shorthand for a synchronized (this) {…} block surrounding the whole method. Java locks are reentrant, i.e. a thread can request to lock an object it has al- ready locked. synchronized on a static method does lock the whole class, i.e. public static synchronized getId() {…} is the same as a synchronized(getId.class) {…} around the whole method. 3.1 Exceptions When an exception inside a synchronized block is triggered, the lock is released and then the exception is caught and the exception handler is executed (if there is no exception handler, the exception is propagated back as usual). 3.2 Operations on the Monitor Locks in Java are really Monitors. Therefore, they support the usual operations: • wait(): Releases object lock, thread waits on internal queue • notify(): Wakes the highest-priority thread closest to front of object’s internal queue • notifyAll(): Wakes up all waiting threads. The threads compete (non-deterministi- cally for access to object, may not be fair) These methods can only be called inside a synchronized block. notify / notifyAll don’t release the lock, therefore the woken up thread can’t run until the code which called notify re- leases its lock. Executor Service Parallel Programming Page 8 of 50 When we check a condition and wait if the condition is not fulfilled, we should always check the condition in a while-loop (not using an if-statement) because the thread can return from a wait call for various reasons (e.g. because he got interrupted) and we cannot be sure that the condition is fulfilled after he returned. Furthermore, we have to take care that we do not hold another lock when calling wait (could lead to a deadlock because the other lock is still held). 3.3 Interface Lock Javas intrinsic locks (used by synchronized) have some limitations. There is only one implicit lock per object, they are forced to be used in blocks and provide limited flexibility. For more flex- ibility, Java offers the Lock interface: final Lock lock = new ReentrantLock(); Java Locks provide conditions that can be instantiated: Condition notFull = lock.newCondition(); They offer the await(), signal() and signalAll() methods. There is also java.util.concurrent.locks.ReentrantReadWriteLock for a Reader / Writer lock. 3.4 Atomic operations We have java.util.concurrent.atomic.AtomicBoolean (also for other data types) with the operations set(), get(), compareAndSet(boolean expect, boolean up- date) and getAndSet(boolean newValue). compareAndSet(boolean expect, boolean update) sets the Boolean to update when the current value is still expect (and returns true, false otherwise). getAndSet(boolean newValue) sets the Boolean to newValue and returns the current value. The direct mapping to hardware of these operations is not guaranteed, meaning they are not guaranteed lock-free. 4 Executor Service Because Java threads are quite heavyweight (in many implementations mapped to OS threads), using one thread per small task is highly inefficient. The alternative approach is to schedule tasks on threads in a thread pool. In Java, we use ExecutorService for that. The programmer submits a task to the ExecutorService (a Runnable that doesn’t return a result or a Callable that returns a result) and gets back a Future. ExecutorService isn’t suited well for the divide and conquer approach. In a typical parallel divide and conquer program, a tasks spawns another tasks and waits for the result (the ForkJoin framework Parallel Programming Page 9 of 50 spawned task spawns another one and so on…). In a short time, the thread pool is filled with tasks waiting for other tasks which can’t run because the thread pool is filled. 5 ForkJoin framework The ForkJoin framework is Javas implementation of the Fork / Join idiom. It is very similar to an Executor Service, but uses a work stealing scheduler. Because of this, it is well suited for divide and conquer types of programs. While threads in the pool are waiting for the results of their own spawned tasks, they can “steal” tasks from the other threads queues – therefore helping them progress and vice-versa. To use the ForkJoin framework, we need to create a ForkJoinPool and then submit a Re- cursiveTask<V> to it which in turn spawns new RecursiveTask<V> by calling fork(). To wait for the task result, join() is called on the RecursiveTask<V>. In a Recur- siveTask<V>, we need to override the compute() method. 6 Java Memory Model The compiler and the hardware is allowed to make changes that do not affect the semantics of a sequentially executed program. Therefore, modern compilers do not give guarantees that a global ordering of memory accesses is provided. The memory model of a programming lan- guage provides certain (often minimal) guarantees for the effects of memory operations. When we use volatile fields in Java, accesses do not count as data races. In detail, vola- tile in Java means: • Every read / write to the field goes straight to main memory, there are no cache incoher- ences between different cores (for this field). • There is a “happens-before” guarantee for volatile fields: o Reads from and writes to other variables cannot be reordered to occur after a write to a volatile variable, if the reads / writes originally occurred before the write to the volatile variable. The reads / writes before a write to a volatile variable are guaranteed to \"happen before\" the write to the volatile variable. Notice that it is still possible for e.g. reads / writes of other variables located after a write to a vol- atile to be reordered to occur before that write to the volatile. Just not the other way around. From after to before is allowed, but from before to after is not al- lowed. o Reads from and writes to other variables cannot be reordered to occur before a read of a volatile variable, if the reads / writes originally occurred after the read of the volatile variable. Notice that it is possible for reads of other variables that oc- cur before the read of a volatile variable can be reordered to occur after the read of the volatile. Just not the other way around. From before to after is allowed, but from after to before is not allowed. Approaches to parallelism Parallel Programming Page 10 of 50 Parallelism 7 Approaches to parallelism 7.1 Implicit / explicit parallelism Implicit parallelism is when the programmer doesn’t explicitly specify the parallelism. It is the job of the compiler or the runtime / OS to identify potential parallelism (e.g. independent memory access) and exploit it. Explicit parallelism is when the programmer explicitly specifies the paral- lelism, e.g. by creating a number of threads to solve a specific problem. 7.2 Vectorization Vectorized operations apply an operation to an entire set of values. Whereas we normally load 1 value, modify it and store it back, in vectorized code we load multiple values (using special registers), apply the modification to all the values and store all values back in memory. 7.3 Pipelining In a pipeline, we divide instructions in sequential steps and use multiple functional units to pro- cess these steps in parallel. A typical pipeline looks like this: The throughput of the pipeline is the amount of work that can be done by a system in a given period of time (CPU: number of instructions per second). The throughput is bounded by the longest stage and we can approximate it (ignoring lead-in / lead-out time) by: ! \"#$\t('()*+,-,.(/,.)0(1,-201)) Approaches to parallelism Parallel Programming Page 11 of 50 The latency is the time to perform a computation (e.g. to execute a CPU instruction). The la- tency is only constant over time if the pipeline is balanced and is then the sum of execution times of each stage. In an unbalanced pipeline, the latency grows: Pipelining often increases the latency, but significantly improves the throughput of a system. 7.4 Instruction Level Parallelism ILP is a form of implicit parallelism. The CPU executes the code in parallel but to the program- mer it appears that the code was executed sequentially. There are multiple forms of ILP: • Pipelining • Superscalar CPUs (multiple instructions per cycle) • Out-of-Order (OoO) execution (reorder program) • Speculative execution (predict results) 7.5 Architectures 7.5.1 Simultaneous Multithreading (SMT), Intel Hyperthreading In SMT, there are multiple instruction streams (“virtual cores”) per core that are exposed to the OS. The processor schedules the instructions from the streams and can take advantage of the superscalar concept (instructions from different threads can get issued in the same cycle on the same core). 7.5.2 Multicores A single processor has multiple cores with its own hardware units. The cores may share part of the cache hierarchy. 7.5.3 Symmetric Multi Processing (SMPs) There are multiple CPUs on the same system and they share the memory. There has to be a cache coherence protocol to ensure that all the caches are coherent. 7.5.4 Non-Uniform Memory Access (NUMA) The memory is distributed i.e. each processor has its own local memory but can access the memory of the other CPUs. The access time depends on if the data is in the local memory or in the memory of a different processor. Approaches to parallelism Parallel Programming Page 12 of 50 7.5.5 Distributed Memory Distributed Memory is used by large clusters. There is an interconnect network between the ma- chines and often time Message Passing (e.g. MPI) is used. 7.6 Expressing parallelism 7.6.1 Work partitioning / scheduling Work partitioning to split up work of a single program into parallel tasks. It can be done explicitly / manually (task / thread parallelism) or automatically / implicit (data parallelism). Work partition- ing is also called task / thread decomposition. Scheduling is to assign tasks to processors. This is usually done by the system and the goal is full utilization (i.e. no processor is ever idle). When the program is split up in many small tasks, we talk about fine granularity. On the other hand, if it is split up in a few big tasks, it is called coarse granularity. Fine granularity is more portable (can easily be executed in machines with more processors), better for scheduling but if the scheduling overhead is comparable to a single task, the overhead dominates. The guideline for granularity is to keep the tasks as small as possible but significantly bigger than scheduling overhead. 7.6.2 Scalability Scalability means how well a system reacts to increased load. In parallel programming, when we talk about scalability we mean the speedup when we increase processors and we look what happens when processors go to infinity. If a program scales linearly, we have linear speedup. When comparing parallel performance, we have a sequential execution time 𝑇! as a base meas- ure. We then measure the execution time 𝑇* on 𝑝 CPUs and distinguish three cases: • 𝑇* = 𝑇!/𝑝 (perfection) • 𝑇* > 𝑇!/𝑝 (performance loss, normal case) • 𝑇* < 𝑇!/𝑝 (should only happen in special cases) The (parallel) speedup 𝑆* on 𝑝 CPUs is defined as: 𝑆* = 𝑇!/𝑇* We can distinguish the three cases again: • 𝑆* = 𝑝 à linear speedup (perfection) • 𝑆* < 𝑝 à sub-linear speedup (performance loss) • 𝑆* = 𝑝 à super-linear speedup (should only happen in special cases) We get normally a sub-linear speedup because the programs may not contain enough parallel- ism, because of overheads introduced by parallelization (oftentimes associated with synchroni- zation) or / and because of architectural limitations (e.g. memory contention). 7.6.2.1 Amdahl’s Law If we divide the execution time into two categories: Approaches to parallelism Parallel Programming Page 13 of 50 • 𝑊10< = Time spent doing non-parallelizable serial work • 𝑊*-< = Time spent doing parallelizable work We get that 𝑇! = 𝑊10< + 𝑊*-< and get a lower bound for 𝑇*: 𝑇* ≥ 𝑊10< + ?@AB ?CDB Therefore, we have this upper bound for the speedup: 𝑆* ≤ 𝑊10< + 𝑊*-< 𝑊10< + 𝑊*-< 𝑝 If f is the non-parallelizable serial fraction of the total work, we have: 𝑊10< = 𝑓𝑇! and 𝑊*-< = (1 − 𝑓)𝑇!. This gives: 𝑆* ≤ 1 𝑓 + 1 − 𝑓 𝑝 Therefore, if we have infinite workers: 𝑆I ≤ 1 𝑓 The following diagram illustrates the main takeaway from Amdahl’s Law: 7.6.2.2 Gustafson’s Law In Gustafson’s Law, the run-time is constant but the problem size isn’t. More processors allow to solve larger problem in the same time because the parallel part of a program scales with the problem size. If we have a task with a sequential part 𝑓, the speedup is: Approaches to parallelism Parallel Programming Page 14 of 50 𝑆𝑝 = 𝑓 + 𝑝(1 − 𝑓)\t \t\t\t\t\t\t= 𝑝 − 𝑓(𝑝 − 1) 7.6.3 Parallel programming paradigms 7.6.3.1 Task Parallel In the task parallel paradigm, the programmer explicitly defines parallel tasks. In Cilk 1-Style task parallel programming, tasks execute code, spawn other tasks and wait for results from other tasks (fork-join idiom). A task graph (directed acyclic graph) is formed, where an edge means that the head of the edge created the tail of the edge. Tasks can be executed in parallel, but they don’t have to. It is up to the scheduler to assign the tasks to CPUs / cores. The task graph is dynamic because it unfolds as execution proceeds In- tuitively, a wider graph means more parallelism. 𝑇∞ is the time it takes on infinite processors and is called span / critical path in this context. It is the longest path from root to sink in the task graph: 1 A Programming language from MIT, designed for multithreaded parallel computing Approaches to parallelism Parallel Programming Page 15 of 50 𝑇𝑝 depends on the scheduler, whereas 𝑇∞ (and 𝑇1 and 𝑝) is fixed. In this example, we can see the difference of 𝑇𝑝 based on the scheduling: For a greedy scheduler (in a nutshell, a greedy scheduler is a scheduler in which no processor is idle if there is more work it can do,) we have: 𝑇𝑝 ≤ 𝑇1 𝑝 + 𝑇∞ MIT’s Cilk introduced the work stealing scheduler. In a work stealing scheduler, each processor in a computer system has a queue of work items (computational tasks, threads) to perform. Each work item consists of a series of instructions, to be executed sequentially, but in the course of its execution, a work item may also spawn new work items that can feasibly be Distributed memory / message passing Parallel Programming Page 16 of 50 executed in parallel with its other work. These new items are initially put on the queue of the processor executing the work item. When a processor runs out of work, it looks at the queues of other processors and \"steals\" their work items. In effect, work stealing distributes the scheduling work over idle processors, and as long as all processors have work to do, no scheduling over- head occurs. For a work stealing scheduler, the lower bound of a greedy scheduler is usually achieved, i.e. we have: 𝑇𝑝 = 𝑇1 𝑝 + 𝑂(𝑇 ∞) 7.6.3.2 Data parallel (dataflow programming model) In the dataflow programming model, the programmer defines what each task does and how the tasks are connected. Dataflow programming is declarative because the programmer only describes what, but not how. The work partitioning is done by the system. An important dataflow programming technique / library is MapReduce. A reduction is an opera- tion that produces a single answer from a collection via an associative operator (e.g. max, count, leftmost, sum, product, …). A map operates on each element of a collection inde- pendently to create a new collection of the same size. In the MapReduce framework, the pro- grammer just writes the mappers / reducers and the system takes care of distributing the data / managing fault tolerance. 8 Distributed memory / message passing The basic idea of distributed memory / message passing is to have isolated mutable state i.e. state is mutable, but not shared (each thread has its private state). Tasks cooperate via mes- sage passing. With distributed memory, state is distributed and threads exchange amounts at coarse granularity. We differentiate between two message types: • Synchronous messages: Sender blocks until message is received • Asynchronous messages: Sender does not block (fire-and-forget), message is placed into a buffer for receiver to get There are multiple programming models for concurrent message passing. Distributed memory / message passing Parallel Programming Page 17 of 50 8.1 Programming models 8.1.1 CSP: Communicating Sequential Processes CSP is a formal language defining a process algebra for concurrent systems. Functions can be linked with the operators seq (sequential) or par (parallel). Communication / Synchronization is done with message passing but naming is indirect (there are symbolic channels between sender and receiver). CSP was first implemented in Occam and the Go programming language was inspired by CSP. An example of a Go application is: 8.1.2 Actor programming model An actor is a computational agent that maps communication to: • a finite set of communications sent to other actors (messages) • a new behavior (state) • a finite set of new actors created (dynamic recofigurability) Global ordering is undefined and asynchronous message passing is used. The graph is dynami- cally configured during runtime and resources are dynamically allocated. Actor sends messages to other actors using “direct naming” (no indirection via e.g. ports / channels / queues / sockets, etc…). The actor model is implemented in various languages such as Erlang, Scala, Ruby or Akka (Scala and Java). The model is sometimes also called event-driven programming model be- cause often a program is written as a set of event handlers for events. A sample erlang application looks like this: start() -> Pid = spawn(fun() -> hello() end), Pid ! hello, Pid ! bye. hello() -> receive hello -> io:fwrite(\"Hello world\\n\"), hello(); Distributed memory / message passing Parallel Programming Page 18 of 50 bye -> io:fwrite(\"Bye cruel world\\n\"), ok end. 8.2 Message passing libraries In the 1980s, PVM (Parallel Virtual Machines) was created as a library for message passing. In the 1990s, MPI (Message Passing Interface) was created which is widely used today. 8.2.1 MPI (Message Passing Interface) MPI processes are collected into groups. Each groups can have multiple colors (sometimes called the context). The group + the color is the communicator (like a name for the group). When an MPI application starts, the group of all processes is initially given the predefined name MPI_COMM_WORLD. Within each communicator, a process is identified by a unique number called rank. For two different communicators, the same process can have two different ranks. The communicators define the communication domain of a communication operation i.e. the set of processes that are allowed to communicate with each other. A communicator does not need to contain all the processes in the system. MPI programs follow the SPMD (Single Program, Multiple Data) pattern. One program is com- piled that works on multiple data. The most important MPI functions are: • MPI_INIT – initialize the MPI library (must be the first routine called) • MPI_COMM_SIZE - get the size of a communicator • MPI_COMM_RANK – get the rank of the calling process in the communicator • MPI_SEND – send a message to another process • MPI_RECV – receive a message from another process • MPI_FINALIZE – clean up all MPI state (must be the last MPI function called by a pro- cess) When sending messages, message tags can be used to differentiate between different mes- sages being sent. Furthermore, a receiver can “filter” messages so that he only receives from the source he specified and the sender can specifiy the destination in addition to the communi- cator. Synchronous send (Ssend) waits until complete message can be accepted by receiving process before completing the send whereas asynchronous send (send) does not wait for actions to complete before returning and therefore needs a buffer. Receive is always synchronous. Send and receive are blocking by default. Blocking / Nonblocking is about local handling of data to be sent / received, a blocking call can only return after local actions are complete (though the mes- sage transfer may not have been completed; depending on synchronous / asynchronous) whereas nonblocking calls return immediately. The following code is called “unsafe”: Distributed memory / message passing Parallel Programming Page 19 of 50 It depends on the availability of system buffers in which to store the data sent until it can be re- ceived and can therefore lead to a deadlock. It can be solved by reordering the messages, ex- plicitly supplying a buffer, sending / receiving at the same time or by using non-blocking opera- tions and calling waitall afterwards. A sample application to compute pi looks like this: As we can see, each process calculates depending on its rank and the total number of pro- cesses (size). All processes except the one with rank 0 send the result, whereas the process with rank 0 receives all results and sums them together. MPI applications are started using mpiexec, e.g. mpiexec -np 16 ./test. 8.2.1.1 Collective communication Besides point-to-point communication, MPI also supports communications among groups of processors. The most important operations are: Distributed memory / message passing Parallel Programming Page 20 of 50 • Reduce: Implemented in a tree-structure. Process 1 sends to 0, 3 to 2, … Then process 2 sends their reduced value to 0, 6 to 4, … Finally process 4 sends its newest value to process 0. • Broadcast: Implemented in a tree-structure as well. Process 0 sends to 4, which starts broadcasting to other nodes as soon as the value arrives… • Allreduce: Allreduce is when not only one process needs the reduced value (e.g. the sum) but rather all need it. Allreduce is implemented in a “butterfly structure”: • Scatter / Gather: Scatter is the operation of splitting a vector and sending the com- ponents to different processes, whereas gather collects the components and forms a fector: Managing state Parallel Programming Page 21 of 50 Concurrency 9 Managing state Managing state is the main challenge for parallel programs. There are three approaches: • Immutability (data does not change, best option when possible) • Isolated mutability (data can change, but only one thread / task can access them) • Mutable / shared data (data can change and all threads / tasks can access them) Mutable / shared data is present in shared memory architectures, but concurrent accesses may lead to inconsistencies. Therefore we need to protect state by allowing only one task / thread to access it at a time (in general). Intermediate inconsistent states should not be observed. The methods to ensure mutual exclusion are locks and transactional memory (see later chapters). 9.1 Mutual exclusion A critical section is a piece of code that may be executed by at most one process at a time with the following conditions: • Mutual exclusion (statements from critical sections of two or more processes must not be interleaved) • Freedom from deadlock: If some processes are trying to enter a critical section, one of them must eventually succeed • Freedom from starvation 2: If any process tries to enter its critical section, then that pro- cess must eventually succeed Mutual exclusion is an algorithm to implement a critical section. The required properties for mu- tual exclusion are: • Safety property: At most one process executes the critical section code • Liveness: acquire_mutex must terminate in finite time when no process executes in the critical section 9.2 Atomicity An operation is atomic if no other thread can see it partly executed (“appears indivisible”). 9.2.1 Atomic operations Most modern hardware supports atomic instructions. We differentiate TAS (Test-and-set) and CAS (Compare-and-set). The semantics of them are: 2 Starvation is the repeated but unsuccessful attempt of a recently unblocked process to continue its exe- cution. Managing state Parallel Programming Page 22 of 50 9.3 Types of registers We differentiate between atomic, safe and regular registers: • Atomic register: An invocation of read / write takes effect at a single point in time. This point lies between start and end of the operation. Two operations on the same register always have a different effect time. An invocation of read returns the value written by the invocation of write with closest preceding time: • Safe Register: A Safe SWMR (Single Writer Multiple Reader) allows only one concur- rent write but multiple concurrent reads. Any read not concurrent with a write returns the current value of the register. Any read concurrent with a write can return any value of the domain of r. Races Parallel Programming Page 23 of 50 • Regular Register: Like a Safe Register, but any read concurrent with a write will either read the new or the old value (but not consistent, can sometimes return the old and then the new value). 10 Races 10.1 Data races A data race is an erroneous program behavior caused by insufficiently synchronized accesses of a shared resource by multiple threads, e.g. simultaneous read/write or write/write of the same memory location. As a guideline, you should never allow two threads to read/write or write/write the same location at the same time. Do not make any assumptions on the orders of reads or writes. 10.2 Interleavings If a second call starts before the first ends, we say the calls interleave (can also happen with one processor when a thread gest preempted / a context switch happens). A bad interleaving is an erroneous program behavior caused by an unfavorable execution order of a multithreaded algorithm that makes use of otherwise well synchronized resources. 11 Locks Locks are primitives with atomic operations: • new: Make a new lock, initially “not held” • acquire: Blocks if this lock is already currently “held” • release: Makes this lock “not held” (if more than 1 threads are blocked on the lock, ex- actly one will acquire it) A re-entrant lock (recursive lock) stores the thread that currently holds it and a count. If the cur- rent holder calls acquire, it does not block but increments the count. On release, the count is decremented and if the count is 0, the lock becomes not held. “Locking” / mutual exclusion is very easy with a single core. The only way to get bad interleav- ings is when the scheduler decides to schedule another thread while a thread is in its critical section. To do that, an interrupt request (IRQ) is emitted. It is possible to switch off IRQs before entering the critical section and to switch them back on after leaving the critical section. Then it isn’t possible to interrupt the thread while in its critical section. In the following sections, some algorithms to implement mutual exclusion with multiple threads are described. Decker’s Algorithm / Petersons’s Algorithm and the Filter lock require an atomic register, whereas the bakery locks also works with an SWMR register. In practice, mutual exclusion is not implemented like this. There’s a theorem that states that a mutual exclusion protocol must have at least as many variables as processes. Therefore, Locks Parallel Programming Page 24 of 50 implementing mutual exclusion like this would require much storage and would be inefficient. Because of this, atomic hardware operations are used for implementing mutual exclusion. 11.1 Decker’s Algorithm Decker’s Algorithm is an algorithm to ensure mutual exclusion with two processes. It uses two flags (indicating that the other process wants to enter the critical section) and a variable turn to indicate which thread is allowed to enter the critical section: Only using a turn variable wouldn’t work because we haven’t made any assumptions about pro- gress outside of the critical section i.e. process 1 (when the turn variable is initialized to 0) could wait forever because process 0 isn’t doing any progress and therefore not arriving at the critical section. 11.2 Peterson Lock Peterson Lock is another way to ensure 2 process mutual exclusion. We again have two flags and a turn or victim variable Locks Parallel Programming Page 25 of 50 11.3 Filter Lock The Filter Lock is an extension of Peterson’s Lock to n processes. The idea is that every thread knows his level in the filter level[t]. In order to enter the critical section, a thread has to ele- vate all levels. For each level, we use Petersons’s mechanism to filter at most one thread if other threads are at higher level. For every level, there is one victim[l] that has to let other pass in case of conflicts. The Filter Lock is not fair, it is first-come-first-serve. 11.4 Bakery algorithm The bakery algorithm works like the numbering system in a postal office. Every thread has a la- bel indicating when he is allowed to enter the critical section (when he has the lowest label). It is possible that multiple processes have the same label, in which case the thread id gets com- pared. Locks Parallel Programming Page 26 of 50 11.5 Spinlock It is very easy to implement a spinlock using TAS: Init(lock): lock = 0; Acquire(lock): while !TAS(lock); // wait Release(lock): lock = 0; If we have a CAS(memref a, int old, int new) operation, it is also very easy to imple- ment a spinlock: Init(lock): lock = 0; Acquire(lock): while (CAS(lock, 0, 1) != 0); Release(lock): CAS(lock, 1, 0); // result gets ignored 11.5.1 Test-and-Test-and-Set (TATAS) Lock The performance of the spinlock described above is quite poor for many threads, because all threads fight for the bus (memory bus) during the call of getAndSet() and the cache coherency protocol needs to invalidate cached copies of the lock on other processors. Therefore, we can improve performance by only calling getAndSet / compareAndSet when we noticed that the lock is available (i.e. testing first): Locks Parallel Programming Page 27 of 50 11.5.2 TATAS with backoff In a TATAS lock, there are still too many threads fighting for access to the same resource. Therefore we set a thread to sleep for a random duration when the acquisition of the lock fails. With an exponential backoff, we double the duration every time the acquisition fails. This leads to a heavy improvement in performance: 11.6 Locks with waiting / scheduling Spinlocks have some problems. They aren’t fair (missing FIFO behavior), computing resources are wasted which degrades performance (especially for long-lived contention) and there is no notification mechanism. Scheduled locks are locks that suspend the execution of threads while they wait and solve these problems. Semaphores, mutexes and monitors (normally hybrid, while threads are in the “waiting entry” queue a spinlock is used to let only one thread in to the critical section, while they are in the “waiting condition” queue they are suspended) are typically imple- mented using a scheduled lock. Scheduled locks require support from the runtime system (OS / scheduler) and the data struc- tures for them need to be protected against concurrent access (using spinlocks or lock-free). They often have a higher wakeup latency and there are hybrid solutions (try access with spinlock for a certain duration before rescheduling). 11.7 Reader / Writer locks Locks Parallel Programming Page 28 of 50 Multiple concurrent reads of the same memory location are not a problem, whereas multiple concurrent writes and multiple concurrent read & writes of the same memory location are. Fur- thermore, there are many applications that have many simultaneous read operations and only a few write operations. Therefore, there are reader / writer locks. This lock can have three states: • not held • held for writing: By one thread • held for reading: By 1 or more thread It can never be held for reading / writing at the same time. Therefore instead of the acquire / release operations, we have acquire_write (block if currently held for reading or held for writing, else set state to held for writing) / release_write (set state to not held) and ac- quire_read (block if currently held for writing, else set state to held for reading and increment readers count) / release_read (decrement readers count, if 0 set state to not held). A simple monitor-based implementation looks like this: This lock isn’t fair because it gives priority to readers (they can just enter when other readers are reading). A “fairer” model would be to let a number k of currently waiting readers pass. When the k readers have passed, the next writer may enter (if any), otherwise further readers may enter until the next writer enters (who has to wait until the current readers finish). 11.8 Deadlocks A deadlock occurs when two or more processes are mutually blocked because each process waits for another of these processes to proceed. More formally stated: Locks Parallel Programming Page 29 of 50 Deadlock detection is implemented by finding cycles in the dependency graph. To avoid dead- locks, we can do two things: • Two phase locking with retry (done when we can abort transaction without consequence, usually in databases) • Resource ordering (usually in parallel programming). The whole program has to obey this order to avoid cycles. 11.8.1 Livelock A livelock occurs when two or more processes can’t make progress because they constantly switch between the same states (in contrast to the deadlock, where they are blocked). 11.9 Lock granularity There are different approaches to lock granularity. Most of these concepts translate somehow to other data types / programs, but will be described based on a linked list. 11.9.1 Coarse grained locking Coarse grained locking is very simple, but performance is poor. We just lock the whole object on all operations. 11.9.2 Fine grained locking In the fine grained locking approach, we split the object into pieces with separate locks. There- fore we have no mutual exclusion for algorithms on disjoint pieces, which improves perfor- mance. When traversing a linked list, “hand over hand locking” is used in the fine grained lock- ing approach. In hand over hand locking, we first lock the head and the predecessor of the head. Then the head is unlocked, the third node is locked, the predecessor is unlocked, the fourth node is locked and so on… With this method, in the end the predecessor and the node to be modified / deleted is locked and it is prevented that another thread changes the next pointer of the predecessor. The disadvantages of this method are that it can lead to a potentially long sequence of locking / unlocking before the intended action can take place and one slow thread locking “early nodes” can block another thread wanting to modify “late nodes” (à no passing possible). Locks Parallel Programming Page 30 of 50 11.9.3 Optimistic synchronization In optimistic synchronization search is performed without acquiring any locks at all. If the method finds the sought-after component, it locks that component, and then checks that the component has not changed in the interval between when it was inspected and when it was locked. This technique is only worthwhile when it succeeds more often than not, why it is called optimistic. In the case of linked lists, it could happen that the sought-after node isn’t reachable anymore (got deleted) or has a new successor. Therefore we need to validate that both isn’t the case. With optimistic synchronization, traversals are wait-free and there are less lock acquisitions. But we need to traverse the list twice and the contains method needs to acquire locks. Further- more, optimistic synchronization is not starvation free (if new nodes are always added / re- moved, a thread can be delayed infinitely because validation always fails). 11.9.4 Lazy synchronization In lazy synchronization, the task of removing a component from a data structure is split into two phases: The component is logically removed by setting a tag bit and later physically removed by unlinking it from the rest of the data structure. In the case of linked lists, we use deleted markers and have the invariant that every unmarked node is reachable. When removing a node, we scan the list and lock predecessor and current (same as with the optimistic synchronization approach). Then we mark the current node as re- moved (logical deletion) and afterwards we redirect predecessors next (physical deletion). Our remove routine now looks like this (without finding node & locking): Semaphores Parallel Programming Page 31 of 50 contains is very simple and wait-free: 12 Semaphores Sometimes we need more than locks. Locks provide means to enforce atomicity via mutual ex- clusion but they lack the means for threads to communicate about changes (e.g. changes in the state). A semaphore is an integer-valued abstract data type 𝑆 with some initial value 𝑠 ≥ 0 and the fol- lowing operations: It is very easy to build a lock with a semaphore. We just initiate it with value 1. If more than 1 thread should be let into the “critical section”, we initiate it with this value. 12.1 Rendezvous It is easy to implement a Rendezvous (point in the program where two threads meet, i.e. wait for each other) using Semaphores: Barriers Parallel Programming Page 32 of 50 12.2 Implementation A semaphore can be implemented without spinning by using a process list: 13 Barriers Barriers are used when we want to synchronize a number of processes at one point in the pro- gram. An implementation of a reusable barrier using semaphores is provided below: Monitors Parallel Programming Page 33 of 50 The two semaphores are needed because otherwise, processes could pass other processes and increment count while others are still “in” the barrier. Like this, the process has to wait until count is 0 and release(barrier2) is called. 14 Monitors Monitors provide, in addition to mutual exclusion, a mechanism to check conditions with the fol- lowing semantics: If a condition does not hold: • Release the monitor lock • Wait for the condition to become true • Signaling mechanism to avoid busy-loops A monitor has an associated monitor queue (similar to semaphores). The threads in the queue are separated in “waiting entry” (they are just waiting for the lock, but not on a condition) and “waiting condition” (waiting for a signal): Lock-free programming Parallel Programming Page 34 of 50 There are different monitor semantics: • Signal and wait: Signaling process exits the monitor (goes to waiting entry queue) and passes the monitor lock to the signaled process. • Signal and continue: Signaling process continues running and the signaled process is moved to waiting entry queue. (implemented by Javas Monitors) 15 Lock-free programming Locks have several disadvantages. They are pessimistic by design (assume the worst and en- force mutual exclusion), have performance issues (overhead for each lock taken even in un- conteded case, contended case leads to significant performance degradation) and they are blocking. If a thread is delayed in a critical section, all threads have to wait. If a thread dies in the critical section, it’s even worse and they are prone to deadlocks. Lock-free programming tries to overcome these issues. We define the following progress conditions: • Lock-freedom: At least one thread always makes progress even if other threads run con- currently. Implies system wide progress but not freedom from starvation. • Wait-freedom: All threads eventually make progress. Implies freedom from starvation (and implies lock-freedom). Lock-free programming Parallel Programming Page 35 of 50 The mechanism to check for exclusive access in lock-free programming is CAS. A positive re- sult of CAS suggests that no other thread has written (not always true because of the ABA prob- lem). 15.1 Memory reuse / ABA problem When we reuse objects e.g. by a ObjectPool (and don’t rely on garbage collection), the ABA problem can occur. The definition is: The ABA problem ... occurs when one activity fails to recognize that a single memory location was modified temporarily by another activity and therefore erroneously assumes that the overall state has not been changed. An example with a concurrent stack is outlined below. Thread X reads the value A, then comes Threads Y and pops A (and puts it in the NodePool), then comes Thread Z that pushes B and Thread Z’ that pushes A again. Thread X wants to complete the pop now by calling top.compareAndSet(head, next) (where head is his local copy of A and next still the previous successor of A). This call succeeds because A is (again) the top element but the previ- ously inserted Element B is lost because Thread X uses the previous successor of A in his CAS call. 15.1.1 Solutions • DCAS: If we could check that the successor of A is still the same in the CAS in the previ- ous example, the problem wouldn’t occur. But DCAS is not available on most platforms. • Garbage collection: Slow, especially for things like a runtime kernel • Pointer tagging: Some bits of the addresses are made available for a tag. Each time a pointer is stored in a data structure, the tag is increased by one. If we have 5 bits for pointer tagging, the ABA problem is much less probable because 32 versions of each pointer exists: Transactional memory Parallel Programming Page 36 of 50 • Hazard Pointers: When a process reads a pointer, it marks it hazarduous by entering it in one of the n (=number of threads) slots of an array associated with the data structure. When finished (after the CAS), the process removes the pointer from the array. Before a process tries to reuse a pointer, it has to check if it is hazarduous (by checking all entries of the hazard array). 16 Transactional memory The goal of transactional memory is to remove the burden of synchronization from the program- mer and place it in the system (hardware / software). Furthermore, it solves the issue of locks that they aren’t composable (combining thread-safe operations is hard). In transactional memory, the programmer explicitly defines atomic code sections. The program- mer only defines that the operations should be atomic but not how (à declarative approach). The benefits of transactional memory is simpler and less error-prone code, higher-level seman- tics (what but not how), composability and they are optimistic by design. Changes made by transactions are made visible atomically i.e. other threads preserve either the initial or the final state, but not any intermediate states. Furthermore, transactions run in isolation. This means that while a transaction is running, effects from other transactions are not observed (as if the transaction takes a snapshot of the global state and operates on this snapshot). Therefore, the transactions appear serialized. Transactional Memory is heavily inspired by database transactions with the ACID properties: • Atomicity • Consistency (database remains in a consistent state) • Isolation (no mutual corruption of data) • Durability (e.g. transaction effects will survive power loss, not important in transactional memory) To implement transactional memory, the system keeps track of operations performed by each transaction and ensures atomicity / isolation properties. When a conflict occurs (e.g. read value was changed by another transaction during the run) the transaction gets aborted / rollbacked. The consistency guarantee is usually implemented by a snapshot at the beginning or early abort (abort if a value has changed). There are two different isolation levels: • Strong isolation: When shared state is accessed outside of a transaction, the transac- tional guarantees are still maintained. • Weak isolation: When shared state is accessed outside of a transaction, the transac- tional guarantees aren’t maintained (i.e. we can have inconsistencies). For nested transactions, there are also different design choices: Concurrency Theory Parallel Programming Page 37 of 50 • Flat nesting: Nested transactions are flattened to one transaction i.e. an abort of an inner transaction causes the abort of the outer transaction and an inner commit is only visible if the outer transaction commits. • Closed nesting: Similar to flat nesting, but an abort of an inner transaction does not re- sult in an abort of an outer transaction. Furthermore, when the inner transaction commits the changes are visible to the outer transactions but they are only visible to other trans- action when the outer transaction commits. Some Transactional Memory implementations (e.g. Scala STM) provide a retry method which causes the transaction to abort and retry when any of the variables that were read change. 16.1 Implementation In a clock-based STM system, we have a global clock that is read by transactions at their crea- tion and all objects have a timestamp. When a transaction commits, the clock is increased. Each transaction uses a local read-set and a local write-set holding all locally read and written objects. When the transaction calls read, it checks if the object is in the write set in which case this version is returned. Otherwise it is checked if the object time stamp is bigger than the timestamp of the transaction, in which case the transaction aborts. If the time stamp is smaller, the value is retrieved from memory and added to the read set. When the transaction calls write, the object is copied and added to the write set. When the transaction commits, all objects of the read- and write-set are locked and the system checks that all objects in the read set have a time stamp smaller than the timestamp of the transaction. If this is not true, the transaction aborts. Otherwise the clock is incremented and all elements of the write set are copied back to global memory (with the new timestamp). 17 Concurrency Theory A method call is the interval that starts with an invocation and ends with a response. The method call is pending between invocation / response. In concurrent settings, method calls can overlap and an object might never be between method calls (when there is no call on the object, we call it periods of quiescence). 17.1 Linearizability Each method should appear to take effect instantaneously between invocation and response events. An object for which this is true for all possible executions is called linearizable. The line- arization points can often be specified but they may depend on the execution (e.g. if a queue is empty, a dequeue may fail while it does not fail with a non-empty queue, leading to different lin- earization points). More formal, we have a History H that is a sequence of invocations and response. Invocations and response match, if thread names agree and object names agree. An invocation is pending if it has no matching response. A subhistory is complete when it has no pending responses. Concurrency Theory Parallel Programming Page 38 of 50 Object projections are all invocations / responses on an object in a history whereas thread pro- jections are all invocations / responses on a thread in a history: In sequential histories, method calls of different threads do not interleave. A final pending invo- cation is ok. An example of a sequential history is: In well-formed histories, all thread projections are sequential. An example of a well-formed his- tory is: Histories are equivalent, if the thread projections are identical. An example of equivalent histo- ries is: Concurrency Theory Parallel Programming Page 39 of 50 A history is legal if for every object x, H | x adheres to the sequential specification of x. A method call precedes another method call if the response event precedes the invocation event, otherwise they overlap. We denote 𝑚O →Q 𝑚! if method execution 𝑚O precedes method execution 𝑚!. A history 𝐻 is linearizable if it can be extended to a history 𝐺 by • appending zero or more responses to pending invocations that took effect • discarding zero or more pending invocations that did not take effect such that 𝐺 is equivalent to a legal sequential history 𝑆 with →T⊂→V →T⊂→V means that the “happens-before order” (method call a precedes method call b, …) of G is a subset of the “happens-before order” of S (in other words: S respects the real-time order of G). The composability theorem states that history H is linearizable if and only if for every object x, H|x is linearizable. The consequence of this is modularity (linearizability of objects can be proven in isolation). Atomic registers are linearizable with a single linearization point (they are sequentially con- sistent, every read operation yields most recently written value and for non-overlapping opera- tions, the real-time order is respected). When we use locking, the linearization points are when locks are released. 17.2 Sequential consistency A history 𝐻 is sequentially consistent if it can be extended to a history 𝐺 by • appending zero or more responses to pending invocations that took effect • discarding zero or more pending invocations that did not take effect such that 𝐺 is equivalent to a legal sequential history 𝑆. No order across threads is required, sequential consistency is therefore weaker than lineariza- bility. Only operations done by one thread need to respect program order, no need to preserve Consensus Parallel Programming Page 40 of 50 real-time order (cannot re-order operations done by the same thread but can re-order non-over- lapping operations done by different threads). In most hardware / memory models, sequential consistency is violated by default for perfor- mance reasons but can be requested explicitly (e.g. with volatile). 17.3 Quiescent consistency Programs should respect real-time order of algorithms separated by periods of quiescence. 18 Consensus Consider an object with the method decide(T value). A number of threads call c.de- cide(v). A consensus protocol is a protocol with the following requirements: • wait-free: consensus returns in finite time for each thread • consistent: all threads decide the same value • valid: the common decision value is some thread’s input A class C solves n-thread consensus if there exists a consensus protocol using any number of objects of class C and any number of atomic registers. The consensus number of C is the larg- est n such that C solves n-thread consensus. Consensus is important because it can be used to proof that certain data structures cannot be implemented with certain operations / registers. E.g. atomic registers have consensus number 1. We can’t implement a FIFO queue using atomic registers because we can implement 2- thread consensus with a FIFO queue by inserting one “red ball” and “black ball” and having an additional array where the threads write their proposed value. The one with the red ball wins and decides the value, the one with the black ball takes the value of the other thread. Therefore, a FIFO queue has consensus number 2 and can’t be implemented by atomic registers. The consensus hierarchy is as follows: Parallel prefix-sum Parallel Programming Page 41 of 50 Parallel algorithms / data structures 19 Parallel prefix-sum The prefix-sum problem is defined like this: Given an input array of n elements, produce an out- put array of n elements where the i-th element is the sum of elements 0 to i, i.e. output[i] = input[0]+input[1]+...+input[i]. The sequential algorithm doesn’t seem parallelizable, but we can write another algorithm. The algorithm does two passes: In the first pass (bottom-up), a binary tree is built where the sum under each node is noted: In the second pass (top-down), each node takes its fromLeft value and passes its left child the same fromLeft value and its right child its fromLeft plus its left child’s sum. This starts at the root node and the root node is given a fromLeft value of 0. The output at position i is then: output[i]=fromLeft+input[i] Parallel prefix-sum Parallel Programming Page 42 of 50 Both passes have 𝑂(𝑛) work and 𝑂(log 𝑛) span, therefore the total work is 𝑂(𝑛) and the total span is 𝑂(log 𝑛). 19.1 Parallel pack Given an array input, produce an array output containing only elements such that 𝑓(𝑒𝑙𝑡) is true. With parallel prefix, this can be parallelized: 1. Parallel map to compute a bit-vector for true elements: input [17, 4, 6, 8, 11, 5, 13, 19, 0, 24] bits [1, 0, 0, 0, 1, 0, 1, 1, 0, 1] 2. Parallel-prefix sum on the bit-vector: bitsum [1, 1, 1, 1, 2, 2, 3, 4, 4, 5] 3. Parallel map to produce the output (because of the parallel-prefix sum on the bit-vector, we have the positions of true elements in the bitsum array): output [17, 11, 13, 19, 24] 19.2 Quicksort If we just do the two recursive calls in parallel, our span is 𝑂(𝑛). But we can use our parallel pack method to do the partitioning by packing elements less than pivot into the left side of an Producer / Consumer Pattern Parallel Programming Page 43 of 50 array and the elements greater than pivot into the right side of an array. Because parallel pack has a span of 𝑂(log 𝑛), we get a total span of 𝑇(𝑛) = 𝑂(log 𝑛) + 1𝑇 ^/ _` = 𝑂(log_ 𝑛) 20 Producer / Consumer Pattern In the producer / consumer pattern, there is a producer (thread) and a consumer (thread) (or multiple threads). We need a synchronized mechanism to pass objects between the producer and the consumer. The producer / consumer pattern can be used to build data-flow parallel pro- grams, e.g. pipelines. To implement the producer / consumer, a circular buffer can be used as a queue. Such a buffer can be implemented with a semaphore, but is much easier to implement using a monitor: synchronized void enqueue(long x) { while (isFull()) try { wait(); } catch (InterruptedException e) { } doEnqueue(x); notifyAll(); } synchronized long dequeue() { long x; while (isEmpty()) try { wait(); } catch (InterruptedException e) { } x = doDequeue(); notifyAll(); return x; } As usual, we have to check the condition in a while-loop because we can’t be sure it is fulfilled after we got signaled. We can also use Javas Lock interface with its condition to implement the buffer. To avoid that the signal is sent when no threads are waiting, we can work with two additional variables that indicate how many threads are waiting (for enqueuing / dequeuing). 21 Lazy Skiplist A skiplist is a collection of elements (without duplicates) with the methods add, remove and find. We assume that there are many calls to find, fewer to add and much fewer to remove. Lazy Skiplist Parallel Programming Page 44 of 50 The skiplist is a sorted multi-level list where the height of a node is probabilistic (e.g. Pr[ℎ𝑒𝑖𝑔ℎ𝑡 = 𝑛] = 0.5 /) and we have two sentinels that are in the maximal level. If a node is in a level 𝑖, he is automatically in all levels 0 … 𝑖 − 1 as well. All nodes are in level 0: When searching for a value, we start at the sentinel −∞ and check if the predecessor of the highest level is smaller than our value. If this is true, we skip all nodes between the sentinel and the predecessor and continue at the predecessor (because the predecessor is smaller and therefore all nodes between are too). If this is not true, we go a level down and check again if the predecessor is smaller. This gets repeated until we find the node (or we are in the lowest level and haven’t found the node, i.e. he isn’t in the list). The search for 8 is illustrated below: The expected runtime for contains is logarithmic (with high probability) and it is wait-free. When we add a new node, we need to find the correct destination, lock all predecessors, vali- date and finally insert the node. E.g. when we add 6 with height 4: Lock-free stack Parallel Programming Page 45 of 50 When we remove a node, we find the predecessors, lock the victim, logically remove the victim (mark it), lock the predecessors and finally physically remove the node (change pointers). 22 Lock-free stack A stack is quite easy to implement lock-free, when popping / pushing we just need to make sure that the top element is still the element we retrieved: 23 Lock-free list set Implementing a list set (set based on a linked list) is more difficult because we want to atomati- cally establish consistency of two things: The next pointer and the mark bit. Java provides some sort of DCAS with AtomicMarkableReferences. One bit of the reference is reserved as a mark bit that can be checked in the compareAndSet as well. With these references, when removing a node, we first try to set the mark of the next pointer (logically deleting it). Then we do the “DCAS” on the predecessor which only succeeds when pred.next wasn’t marked / updated: Lock-free unbounded queue Parallel Programming Page 46 of 50 The physical deletion can fail, therefore other threads “clean up” when they observe a logically but not physically deleted node while traversing (if other threads would have had to wait for one thread to clean up the inconsistency, the approach would not have been lock-free). When we add a new node, we keep retrying until we succeed: 24 Lock-free unbounded queue An unbounded queue is a queue with “unlimited” (not restricted) capacity. When we want to im- plement a queue lock-free, we run into problems because we sometimes need to simultane- ously update head / tail / tail.next. We introduce a sentinel node at the start and the head pointer always points at this node. Now we only need to update tail / tail.next. The enqueue method appends the node and tries to set the tail pointer to the appended node. This can fail (other thread observed in the meantime that tail did not point to the last element and changed it) Sorting networks Parallel Programming Page 47 of 50 but this doesn’t matter because in these cases it was changed by another thread and the overall state is still consistent. When we dequeue an element, the current next element becomes the new sentinel (i.e. is “de- leted). Before advancing head, we must make sure that tail is not left referring to the sentinel node (that gets deleted). Therefore we check if first == last and the the sentinel has a successor (next != null). In this case, we need to update tail to the successor of the senti- nel: 25 Sorting networks Sorting networks are abstract devices that are very efficient for parallel execution (and imple- mentation in hardware). They are built from comparators that take two values and return the min / max of the two: Sorting networks Parallel Programming Page 48 of 50 A sorting network consists of many comparators so that each possible input sequence gets sorted: A sorting network can be constructed recursively quite easy. Given a network that sorts the numbers 𝑥!, … , 𝑥/ we just can add 𝑛 comparators in front or behind the network to construct one that sorts the numbers 𝑥!, … , 𝑥/m!. Applied recursively, this gives us a insertion sorting network and a bubble sorting network, which are the same with parallelism: Sorting networks Parallel Programming Page 49 of 50 Using these sorting networks, a computer with infinite numbers of processors requires 2𝑛 − 3 steps to sort 𝑛 numbers (longest sequence of comparators). The networks are well suited for parallel processing because the comparators on the same vertical line can be executed in paral- lel. These sorting networks are not optimal, but finding the optimal sorting network is a hard task (testing whether a candidate network is a sorting network is co-NP complete i.e. the class of problems for which there is a polynomial-time algorithm that can verify counterexamples). The zero-one-principle states that if a network with 𝑛 input lines sorts all 2 / sequences of 0s and 1s into nondecreasing order, it will sort any arbitrary sequence of 𝑛 numbers in nondecreas- ing order. The proof is quite simple, if 𝑥 is sorted by a network N, then also any monotonic func- tion of 𝑥. If 𝑥 is not sorted, then there is a monotonic function 𝑓(𝑥) that maps 𝑥 to 0s and 1s and is not sorted as well. Such a function is 𝑓(𝑥) = 0\t\tif\t𝑥 < \t𝑦, 1\totherwise (for some non-minimal y). Such a function fulfills 𝑓(𝑥) ≤ 𝑓(𝑦) whenever 𝑥 ≤ 𝑦. 25.1 Bitonic sort Bitonic sort is a parallel algorithm for sorting with a time complexity of 𝑂(𝑛log_𝑛) when he is ex- ecuted sequentially and 𝑂(log_𝑛) for the parallel case (worst case = average case = best case). A bitonic set is a set in which the sign of the gradient changes once at most, a bitonic sequence is defined as a list with no more than one local maximum / minimum (à can be wrapped around to a bitonic set): A half-cleaner is a sorting network that splits a binary bitonic sequence into a bitonic and bitonic clean (all 0s or 1s). Every number in the upper half is greater or equal than in the lower half: Sorting networks Parallel Programming Page 50 of 50 A bitonic sorting network takes a bitonic sequence as an input and sorts them with multiple half cleaners: To get the bitonic sequences, “mergers” consisting of half cleaners and bi-mergers (acts like a half-cleaner, but on two sorted sequences). So the final sorting network to sort any input looks like this:","libVersion":"0.3.2","langs":""}