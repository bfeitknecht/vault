{"path":"sem3/LinAlg/UE/s/LinAlg-s-u09.pdf","text":"D-INFK Linear Algebra HS 2024 Bernd G¨artner Robert Weismantel Solution for Assignment 9 1. a) Let a1, a2, a3 denote the columns of A. Performing the Gram-Schmidt process (Algo- rithm 5.4.9) yields q1 = a1 ∥a1∥ = 1 √2   0 1 1   =   0 1/ √2 1/ √2   q′ 2 = a2 − (a⊤ 2 q1)q1 = a2 − 1 √2 q1 =   0 0 1   − 1 √2   0 1/ √2 1/ √2   =   0 −1/2 1/2   q2 = q′ 2 ∥q′ 2∥ =   0 −1/ √2 1/ √2   q′ 3 = a3 − (a⊤ 3 q1)q1 − (a ⊤ 3 q2)q2 = a3 − √2q1 − 0q2 =   1 1 1   −   0 1 1   =   1 0 0   q3 = q′ 3 ∥q′ 3∥ = q′ 3 where q1, q2, q3 is the desired set of orthonormal vectors. b) Putting the vectors q1, q2, q3 into a matrix we obtain Q =   0 0 1 1/ √2 −1/ √ 2 0 1/ √2 1/ √2 0   and it remains to compute R. Concretely, we have R = Q ⊤A =   0 0 1 1/ √2 −1/ √2 0 1/ √2 1/ √2 0   ⊤   0 0 1 1 0 1 1 1 1   =   0 1/ √2 1/ √2 0 −1/ √2 1/ √2 1 0 0     0 0 1 1 0 1 1 1 1   =   √2 1/ √2 √2 0 1/ √2 0 0 0 1   . c) Let b1, b2, b3, b4 denote the columns of B. Performing the Gram-Schmidt process (Algo- 1 rithm 5.4.9) yields q1 = b1 ∥b1∥ = b1 q′ 2 = b2 − (b⊤ 2 q1)q1 = b2 − 2q1 = [ 0 4 0 0 ]⊤ q2 = q′ 2 ∥q′ 2∥ = [ 0 1 0 0 ]⊤ q′ 3 = b3 − (b⊤ 3 q1)q1 − (b ⊤ 3 q2)q2 = b3 − 3q1 − 5q2 = [ 0 0 7 0 ]⊤ q3 = q′ 3 ∥q′ 3∥ = q′ 3 = [0 0 1 0 ]⊤ q′ 4 = b4 − (b⊤ 4 q1)q1 − (b ⊤ 4 q2)q2 − (b⊤ 4 q3)q3 = b4 − 0q1 − 6q2 − 8q3 = [0 0 0 9 ]⊤ q4 = q′ 4 ∥q′ 4∥ = [ 0 0 0 1 ]⊤ where q1, q2, q3, q4 is the desired set of orthonormal vectors. d) This is not always true. The n × n matrix −I is a counterexample for any n ∈ N+. It already has orthonormal columns, hence Gram-Schmidt would leave it unaltered. Moreover, its columns are not exactly the standard unit vectors: the sign is wrong. Therefore, this is indeed a counterexample. Note that this is already a full solution. But we still provide a proof that the answer to the question would be yes if we had required the diagonal entries to be strictly positive (and not just non-zero). Let A be an arbitrary upper triangular n × n matrix with strictly positive entries on its di- agonal. Let a1, . . . , an denote the columns of A and let q1, . . . , qn denote the orthonormal vectors obtained from the Gram Schmidt process on a1, . . . , an. We claim that qi = ei for all i ∈ [n]. Assume for a contradiction that this is not the case and let i ∈ [n] be the smallest index such that qi ̸= ei. Note that we have a1 = ce1 for some constant c ∈ R+ and hence q1 = a1 c = e1. Hence, we must have i > 1. Observe that by definition of the Gram-Schmidt process and because the last n − i entries of ai are zero (triangular shape of A), we also get that the last n − i entries of qi are zero. We claim that the first i − 1 entries of qi are zero as well. To see this, assume for a moment that there is j < i such that the j-th entry of qi is non-zero. Then q⊤ j qi = e⊤ j qi ̸= 0 which contradicts the orthogonality of qj and qi. Hence, we conclude that the first i − 1 entries of qi are zero. In particular, we established that the only non-zero entry of qi is the i-th entry. Since qi must be a unit vector (by the Gram-Schmidt process), we get qi = ei, a contradiction. 2. Let q1, . . . , qm ∈ Rm be the columns of Q, i.e. Q =   | . . . | q1 . . . qn | . . . |   . We want to prove that Q⊤Q = I. Let i, j ∈ [m] be arbitrary and consider the standard unit vectors ei, ej ∈ Rm. By assumption, we have q⊤ i qj = (Qei)⊤(Qej) = e ⊤ i ej = δij := { 1 if i = j 0 otherwise . Using this, we get Q ⊤Q =       q⊤ 1 q1 q⊤ 1 q2 . . . q⊤ 1 qm q⊤ 2 q1 q⊤ 2 q2 . . . q⊤ 2 qm ... . . . . . . ... q⊤ mq1 q⊤ mq2 . . . q⊤ mqm       = I 2 and thus Q is orthogonal. 3. a) Consider the matrix A = [ 0 1 1 0 ] . Clearly, A is an orthogonal matrix. Moreover, A is not a rotation matrix because there is no θ ∈ R satisfying both 1 = sin(θ) and 1 = − sin(θ). b) Assume that A is orthogonal. Recall the formula for the 2 × 2 inverse A−1 = 1 ad − bc [ d −b −c a ] . Since A is orthogonal, we must have A⊤ = A−1. From this, we deduce a = d ad−bc , d = a ad−bc , c = −b ad−bc , and b = −c ad−bc . Note that ad − bc ̸= 0 since A is invertible. Assume first a ̸= 0. Then we obtain ad − bc = d a = a d since we also must have d ̸= 0. This implies |a| = |d| and |ad − bc| = 1. On the other hand, if we have a = 0 then we must have b ̸= 0 and c ̸= 0. Thus, we get ad − bc = −b c = −c b and therefore |b| = |c| and |ad − bc| = 1. c) Consider the matrix A that we get by setting a = d = √2 and b = c = 1. Clearly, we have |ad−bc| = 2−1 = 1. But A is not orthogonal since in particular, its two columns [√ 2 1]⊤ and [ 1 √2]⊤ are not orthogonal (and also they are not unit vectors). 4. a) Let a1, a2, a3 be the columns of A. We first compute all the scalar products between columns of A. In particular, we get a ⊤ 1 a1 = m, a ⊤ 1 a2 = m∑ k=1 tk, a⊤ 1 a3 = m∑ k=1 t2 k, a⊤ 2 a2 = m∑ k=1 t 2 k, a ⊤ 2 a3 = m∑ k=1 t 3 k, a⊤ 3 a3 = m∑ k=1 t4 k and therefore A⊤A =   m ∑m k=1 tk ∑m k=1 t2 k∑m k=1 tk ∑m k=1 t2 k ∑m k=1 t3 k∑m k=1 t2 k ∑m k=1 t3 k ∑m k=1 t4 k   . b) For A⊤A to be diagonal, we need to have ∑m k=1 tk = 0, ∑m k=1 t2 k = 0, and ∑m k=1 t3 k = 0. The first and last condition are not so interesting, but note that the condition ∑m k=1 t2 k = 0 implies tk = 0 for all k ∈ [m] because we clearly have t2 k ≥ 0 for all k ∈ [m]. 5. a) Let us denote the four given points by p1, p2, p3, p4, respectively. We want to find r ∈ R+ such that the sum 4∑ i=1(r − ||pi||) 2 is minimized. The key observation of this exercise is that this is the least squares objective of the linear system     1 1 1 1     [ r] =     ||p1|| ||p2|| ||p3|| ||p4||     =       2√2√ 20 9√ 10 4 .       3 Using the normal equations to solve this we get 4r = [1 1 1 1 ]     1 1 1 1     [ r] = [ 1 1 1 1 ]     ||p1|| ||p2|| ||p3|| ||p4||     = 4∑ i=1 ||pi|| and hence r = 1 4 4∑ i=1 ||pi|| = 1 4 (2 + √2 + √ 20 9 + √ 10 4 ). b) In this more general setting, we need to solve the system    1 ... 1    [r] =    ||p1|| ... ||pn||    in the least squares sense for r. Using the normal equations, this now yields nr = [ 1 . . . 1]    1 ... 1    [ r] = [ 1 . . . 1]    ||p1|| ... ||p4||    = n∑ i=1 ||pi|| and thus r = 1 n 4∑ i=1 ||pi||. 6. Observe first that the concatenation (σ ◦ π) : [n] → [n] (defined as (σ ◦ π)(i) := σ(π(i)) for all i ∈ [n]) of two bijective funtions σ, π : [n] → [n] is again bijective: Indeed, if we had σ(π(i)) = σ(π(j)) for some distinct i, j ∈ [n], this would also imply either π(i) = π(j) or π(i) ̸= π(j) but σ(p(i)) = σ(π(j)), contradicting injectivity of σ or π in either case. Thus, (σ ◦ π) is injective. Moreover, any injective function from [n] to [n] is automatically surjective. We conclude that multiplying two permutation matrices A, B ∈ Rn×n yields again a permutation matrix AB. In particular, this observation implies that the matrices P, P 2, P 3, . . . are all permutation matrices. Since there are only finitiely many permutation matrices of size n × n, there must exist distinct indices ℓ, r ∈ N such that P ℓ = P r. Multiplying both sides with (P −1)ℓ yields I = P r−ℓ. Thus, the statement holds with k = r − ℓ. 4","libVersion":"0.3.2","langs":""}