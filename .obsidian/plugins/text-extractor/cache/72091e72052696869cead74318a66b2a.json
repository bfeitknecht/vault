{"path":"sem4/CN/VRL/extra/slides/12-LinkLayer-Coding.pdf","text":"Computer Networks Lecture 12: Link Layer: Error detection and correction Adrian Perrig Network Security Group ETH Zürich Photo: ETH Zürich / Gian Marco Castelberg Where we are in the Course 2 Physical Link Network Transport Application Returning to a final topic in the Link Layer: error detection and correction Scope of the Link Layer 3 Frame • Concerns how to transfer messages over one or more connected links - Messages are frames, of limited size - Builds on the physical layer, provides service to network layer In terms of layers … 4 Actual data path Virtual data path Network Link Physical In terms of layers (2) 5 Actual data path Virtual data path Network Link Physical Typical Implementation of Layers 6 Topics Framing • Delimiting start/end of frames Error detection and correction • Handling errors Retransmissions • Handling loss Multiple Access • 802.11, classic Ethernet Switching • Modern Ethernet 7 1 2 3 4 5Already covered Framing 8 …10110 … Um? The Physical layer gives us a stream of bits. How do we interpret it as a sequence of frames? Framing Methods • We’ll look at: -Byte count (motivation) -Byte stuffing -Bit stuffing 9 In practice, the physical layer often helps to identify frame boundaries E.g., Ethernet, 802.11 10 Byte Count • First try: -Let’s start each frame with a length field! -It’s simple, and hopefully good enough … Byte Count (2) How well do you think it works? 11 Byte Count (3) Difficult to re-synchronize after framing error • Want a way to scan for a start of frame 12 Byte Stuffing 13 Better idea: • Have a special flag byte value that means start/end of frame • Replace (“stuff”) the flag inside the frame with an escape code • Complication: have to escape the escape code too! Byte Stuffing (2) 14 Rules • Replace each FLAG in data with ESC FLAG • Replace each ESC in data with ESC ESC Byte Stuffing (3) 15 Now any unescaped FLAG is the start/end of a frame Can stuff at the bit level too • Call a flag six consecutive 1s • On transmit, after five 1s in the data, insert a 0 • On receive, a 0 after five 1s is deleted Bit Stuffing 16 Bit Stuffing (2) Example: 17 Transmitted bits with stuffing Data bits Bit Stuffing (3) So how does it compare with byte stuffing? 18 Transmitted bits with stuffing Data bits Link Example: PPP over SONET 19 PPP is Point-to-Point Protocol • Widely used for link framing -E.g., it is used to frame IP packets that are sent over SONET optical links Link Example: PPP over SONET (2) Think of SONET as a bit stream, and PPP as the framing that carries an IP packet over the link 20 Protocol stacks PPP frames may be split over SONET payloads Link Example: PPP over SONET (3) 21 Framing uses byte stuffing • FLAG is 0x7E and ESC is 0x7D Link Example: PPP over SONET (4) 22 Byte stuffing method: • To stuff (unstuff) a byte, add (remove) ESC (0x7D), and XOR byte with 0x20 • Removes FLAG from the contents of the frame Error Coding Overview (§6.2) 23 Already covered Some bits will be received in error due to noise. What can we do? • Detect errors with codes • Correct errors with codes • Retransmit lost frames Reliability is a concern that cuts across the layers Approach – Add Redundancy 24 Error detection codes • Add check bits to the message bits to let some errors be detected Error correction codes • Add more check bits to let some errors be corrected Main issue is how to structure the code to detect many errors with few check bits and modest computation Motivating Example 25 A simple code to handle errors: • Send two copies! Error if different. How good is this code? How many errors can it detect/correct? How many errors will make it fail? Motivating Example (2) 26 We want to handle more errors with less overhead Will look at better codes; they are applied mathematics But, they can’t handle all errors? And they focus on accidental errors (cryptographic techniques are needed to handle malicious errors) Codeword consists of D data plus R check bits (=systematic block code) Sender: Compute R check bits based on the D data bits; send the codeword of D+R bits Using Error Codes 27 D R=fn(D) Data bits Check bits Using Error Codes (2) 28 D R Data bits Check bits R’=fn(D) =? Receiver: • Receive D+R bits with unknown errors • Recompute R check bits based on the D data bits; error if R doesn’t match R’ Intuition for Error Codes 29 All codewords Correct codewords For D data bits, R check bits: Randomly chosen codeword is unlikely to be correct; overhead is low R.W. Hamming (1915-1998) Much early work on codes: “Error Detecting and Error Correcting Codes”, BSTJ, 1950 See also: “You and Your Research”, 1986 30 Source: IEEE GHN, © 2009 IEEE 31 Hamming Distance Distance is the number of bit flips needed to change D1+R1 to D2+R2 Hamming distance of a code is the minimum distance between any pair of codewords Example: Correct codeword for 0 ➢ 000 1 ➢ 111 ➢ ➢ 000 001 010 011 110 111 101 100 Hamming distance = 3 32 Hamming Distance (2) Error detection: • For a code of Hamming distance d+1, up to d errors will always be detected A B000 000 No error, it’s 0 Good 001 An error, likely 0 Detect 010 An error, likely 0 Detect 100 An error, likely 0 Detect 011 Two errors, likely 1 Detect 101 Two errors, likely 1 Detect 110 Two errors, likely 1 Detect 111 Three errors, it’s 1 Fail 33 Hamming Distance (3) Error correction: • For a code of Hamming distance 2d+1, up to d errors can always be corrected by mapping to the closest codeword A B000 000 No error, it’s 0 001 An error, likely 0 Can be corrected 010 An error, likely 0 Can be corrected 100 An error, likely 0 Can be corrected 011 Two errors, likely 1 Can’t be corrected 101 Two errors, likely 1 Can’t be corrected 110 Two errors, likely 1 Can’t be corrected 111 Three errors, it’s 1 Can’t be corrected Error Detection (§6.2.1-6.2.2) 34 Some bits may be received in error due to noise. How do we detect this? • Parity • Checksums • CRCs Detection will let us fix the error, for example, by retransmission Simple Error Detection – Parity Bit Take D data bits, add 1 check bit that is the sum of the D bits • Sum is modulo 2 or XOR 35 Parity Bit (2) How well does parity work? • What is the distance of the code? • How many errors will it detect/correct? 36 What about larger errors? What is the probability to detect large number of errors? Idea: sum up data in N-bit words • Widely used in, e.g., TCP/IP/UDP Stronger protection than parity Checksums 37 1500 bytes 16 bits Internet Checksum Sum is defined in 1s complement arithmetic (must add back carries) • And it’s the negative sum 38 “The checksum field is the 16 bit one's complement of the one's complement sum of all 16 bit words …” – RFC 791 Internet Checksum (2) Sending: 39 0001 f203 f4f5 f6f7 +(0000) ------ 2ddf0 ddf0 + 2 ------ ddf2 220d Put zero in checksum position, add Arrange data in 16-bit words Add any carryover back to get 16 bits Negate (complement) to get sum 1 2 3 4 Internet Checksum (3) 40 0001 f203 f4f5 f6f7 +(0000) ------ 2ddf0 ddf0 + 2 ------ ddf2 220d Sending: Put zero in checksum position, add Arrange data in 16-bit words Add any carryover back to get 16 bits Negate (complement) to get sum 1 2 3 4 Internet Checksum (4) 41 0001 f203 f4f5 f6f7 + 220d ------ 2fffd fffd + 2 ------ ffff 0000 Receiving: Checksum will be non-zero, add Arrange data in 16-bit words Add any carryover back to get 16 bits Negate the result and check it is 0 1 2 3 4 Internet Checksum (5) 42 0001 f203 f4f5 f6f7 + 220d ------ 2fffd fffd + 2 ------ ffff 0000 Receiving: Checksum will be non-zero, add Arrange data in 16-bit words Add any carryover back to get 16 bits Negate the result and check it is 0 1 2 3 4 Internet Checksum (6) How well does the checksum work? • What is the distance of the code? • How many errors will it detect/correct? 43 What is probability to detect larger errors? Any systematic error that cannot be detected? ? ? ? Cyclic Redundancy Check (CRC) (§6.2.3) 44 Even stronger protection • Given n data bits, generate k check bits such that the n+k bits are evenly divisible by a generator C Example with numbers: • Message = 302, k = one digit, C = 3 CRCs (2) The approach: • It’s based on mathematics of finite fields, in which “numbers” represent polynomials • e.g., 10011010 is x7 + x4 + x3 + x1 What this means: • We work with binary values and operate using modulo 2 arithmetic 45 CRCs (3) 46 Send Procedure: 1. Extend the n data bits with k zeros 2. Divide by the generator value C 3. Keep remainder, ignore quotient 4. Adjust k check bits by remainder Receive Procedure: 1. Divide and check for zero remainder CRCs (4) Data bits: 1101011111 Check bits: C(x)=x4+x1+1 C = 10011 k = 4 47 1 0 0 1 1 1 1 0 1 0 1 1 1 1 1 CRCs (5) 48 CRCs (6) 49 Protection depends on generator • Standard CRC-32 is 1 0000 0100 1100 0001 0001 1101 1011 0111 Properties: • HD=4, detects up to triple bit errors • Also odd number of errors • And bursts of up to k bits in error • Not vulnerable to systematic errors (i.e., moving data around) like checksums Error Detection in Practice 50 CRCs are widely used on links • Ethernet, 802.11, ADSL, Cable … Checksum used in Internet • IP, TCP, UDP … but it is weak Parity • Is little used Error Correction (§6.2.3) 51 Some bits may be received in error due to noise. How do we fix them? • Hamming code • Other codes And why should we use detection when we can use correction? Why Error Correction is Hard 52 If we had reliable check bits we could use them to narrow down the position of the error • Then correction would be easy But error could be in the check bits as well as the data bits! • Data might even be correct Intuition for Error Correcting Code 53 Suppose we construct a code with a Hamming distance of at least 3 • Need ≥3 bit errors to change one valid codeword into another • Single bit errors will be closest to a unique valid codeword If we assume errors are only 1 bit, we can correct them by mapping an error to the closest valid codeword • Works for d errors if HD ≥ 2d + 1 Intuition (2) Visualization of code: 54 A B Valid codeword Error codeword Intuition (3) Visualization of code: 55 A B Single bit error from A Three bit errors to get to B Valid codeword Error codeword Hamming Code 56 Gives a method for constructing a code with a distance of 3 • Uses n = 2k – k – 1, e.g., n=4, k=3 • Put check bits in positions p that are powers of 2, starting with position 1 • Check bit in position p is parity of positions with a p term in their values Plus an easy way to correct Hamming Code (2) 57 Example: data=1010, 3 check bits • 7 bit code, check checksum bit positions 1, 2, 4 • Place the data in non-checksum positions 3, 5, 6, 7 1 0 1 0 7 6 5 4 3 2 1 Hamming Code (2) 58 Example: data=1010, 3 check bits • 7 bit code, check bit positions 1, 2, 4 • Place the data in non-checksum positions 3, 5, 6, 7 • Check bit at position 1 covers positions 1, 3, 5, 7 - Bit positions xx1 (e.g., 001, 011, 101, 111) • Check bit at position 2 covers positions 2, 3, 6, 7 - Bit positions x1x (e.g., 010, 011, 110, 111) • Check bit at position 4 covers positions 4, 5, 6, 7 - Bit positions 1xx (e.g., 100, 101, 110, 111) 1 0 1 0 0 7 6 5 4 3 2 1 p1= 0+1+1 = 0 Hamming Code (4) 59 Example: data=1010, 3 check bits • 7 bit code, check bit positions 1, 2, 4 • Place the data in non-checksum positions 3, 5, 6, 7 • Check bit at position 1 covers positions 1, 3, 5, 7 • Check bit at position 2 covers positions 2, 3, 6, 7 • Check bit at position 4 covers positions 4, 5, 6, 7 1 0 1 0 0 1 0 7 6 5 4 3 2 1 p2= 0+0+1 = 1 p4= 1+0+1 = 0 Hamming Code (5) 60 To decode: • Recompute check bits (with parity sum including the check bit) • Arrange as a binary number • Value (syndrome) tells error position • Value of zero means no error (that is the reason why there is no symbol at position 0) • Otherwise, flip bit to correct Hamming Code (6) 61 1 0 1 0 0 1 0 p1= p2= p4= Syndrome = Data = 7 6 5 4 3 2 1 Example, continued Hamming Code (7) 62 1 0 1 0 0 1 0 p1= 0+0+1+1 = 0, p2= 1+0+0+1 = 0, p4= 0+1+0+1 = 0 Syndrome = 000, no error Data = 1 0 1 0 7 6 5 4 3 2 1 Example, continued Hamming Code (8) 63 1 1 1 0 0 1 0 p1= p2= p4= Syndrome = Data = 7 6 5 4 3 2 1 Example, continued Hamming Code (9) 64 1 1 1 0 0 1 0 p1= 0+0+1+1 = 0, p2= 1+0+1+1 = 1, p4= 0+1+1+1 = 1 Syndrome = 1 1 0, flip position 6 Data = 1 0 1 0 (correct after flip!) 7 6 5 4 3 2 1 Example, continued Hamming Code (10) 65 1 0 1 0 0 1 1 p1= 1+0+1+1 = 1, p2= 1+0+0+1 = 0, p4= 0+1+0+1 = 0 Syndrome = 0 0 1, flip position 1 Data = 1 0 1 0 (correct after flip!) An error on parity bit positions can also be corrected! 7 6 5 4 3 2 1 Example, continued m3m5m6m7m11 m10 m9 p8m15 m14 m13 m12 Localizing error to top or bottom half 1xxx or 0xxx p8 = m15 Å m14 Å m13 Å m12 Å m11 Å m10 Å m9 Localizing error to x1xx or x0xx m3p4m5m6m7m11 m10 m9 p8m15 m14 m13 m12 p4 = m15 Å m14 Å m13 Å m12 Å m7 Å m6 Å m5 Localizing error to xx1x or xx0x p2m3p4m5m6m7m11 m10 m9 p8m15 m14 m13 m12 p2 = m15 Å m14 Å m11 Å m10 Å m7 Å m6 Å m3 Localizing error to xxx1 or xxx0 p1p2m3p4m5m6m7m11 m10 m9 p8m15 m14 m13 m12 p1 = m15 Å m13 Å m11 Å m9 Å m7 Å m5 Å m3 Hamming Code Example: 11 bit msgOther Error Correction Codes 67 Codes used in practice are much more involved than Hamming Convolutional codes • Take a stream of data and output a mix of the recent input bits • Makes each output bit less fragile • Decode using Viterbi algorithm (which can use bit confidence values) Low Density Parity Check • LDPC based on sparse matrices • Decoded iteratively using a belief propagation algorithm • State of the art today Invented by Robert Gallager in 1963 as part of his PhD thesis • Promptly forgotten until 1996 … Other Codes (2) – LDPC 68 Source: IEEE GHN, © 2009 IEEE Detection vs. Correction 69 Which is better will depend on the pattern of errors. For example: • 1000 bit messages with a bit error rate (BER) of 1 in 10000 Which has less overhead? • It depends! We need to know more about the errors ? ? Detection vs. Correction (2) Assume bit errors are random • Messages have 0 or maybe 1 error 70 Error correction: • Need ~10 check bits per message • Overhead: 10 bits/message Error detection: • Need ~1 check bit per message plus 1000 bit retransmission 1/10 of the time • Overhead: 1+1000/10=101 bit/message 1 Detection vs. Correction (3) 71 Assume errors come in bursts of 100 consecutively garbled bits • Only 1 or 2 messages in 1000 have errors Error correction: • Need >>100 check bits per message • Overhead: ~200 bit/message (Reed-Solomon) Error detection: • Can use 32 check bits per message plus 1000 bit resend 2/1000 of the time • Overhead: 32 + 1000/1000 * 2 = 34 bit/message 2 Detection vs. Correction (4) 72 Error correction: • Needed when errors are expected - Small number of errors are correctable • Or when there’s no time for retransmission Error detection: • More efficient when errors are not expected • And when errors are large when they do occur Error Correction in Practice 73 Heavily used in physical layer • LDPC is the future, used for demanding links like 802.11, DVB, WiMAX, LTE, power-line, … • Convolutional codes widely used in practice Error detection (with retransmission) is used in the link layer and above for residual errors Correction also used in the application layer • Called Forward Error Correction (FEC) • Normally with an erasure error model (entire packets are lost) • E.g., Reed-Solomon (CDs, DVDs, etc.)","libVersion":"0.3.2","langs":""}