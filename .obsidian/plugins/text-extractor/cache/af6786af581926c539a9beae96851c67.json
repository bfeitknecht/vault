{"path":"sem1/LinAlg/VRL/kai/LinAlg-w07-kai.pdf","text":"L in e are Algebra Übungsstunde 7 1 . Orga 2 . GA 3 . Priorisierte Wiederholung 4. Recap : AG 5. Nächste Woche 6 . Lineare Abbildungen 7.. Qu iz 11 Orga · Schaut euch 3 Blue 1 Brown an ! 2 . GA : Reflexion · [3-5 Minuten 1 Rew ind: Vorlesungen Woche s Worum gings ... 3 . Priorisierte Whl . Skalarprodukt 24· Skalarprodukt*:=xTy Liefert eine Zahl! : Il XIR\" - R · Norm = =) = Betrag\" = Länge vo m Vektor x II .11: R\" -> RR · Orthogonal y = 0 = senkrecht · Besonderheit vom Skalarprodukt und Nor m : bilden Vektoren auf Zahlen (Skalarel ab ! Liefern Eigenschaften (Länge, Winkel, ...) Orthogonal Subspaces · Sei U =Span Ebe. ..., bn 3, W = span EC1, ... m3 zwei Subspaces des Vectorspace V U, weW: u .w = 0 Es sie sind orthegonal Bsp : U = spanz[5] 3 Zwei Subspaces sind gena u dann Orthogonal wenn w = span {[8] 3 alle ihre Vektoren Orthogonal zueinander sind · Es gilt : UnW = 203 # Orthogonal Complement · Gilt zus ät zlich/dimU + dieW = die V dann nennt man sie complementary !- und dann orthogonale Komplemente falls zusätzlich der gemeinsam U = spanz[5)3e Span erzeugend ist (V spannt) : span{ba ..., br, C , ... m3 = V . (U = orthegonales Komplement vo n U, bz w. V = UQU( · Es gilt : NCAL = CCAT), bzw : RP = N(A) @C(AT) d NCAT) = CCA) * RM = NC AT(((A) =mit Projection on to Subspace · Gegeben Subspace UCV und beV, be V : #(b ) = argmillb-pll Projektion eines Vektors auf Subspace U ist genau der Vektor pe mit geringstem Abstand zu b . Merke : Projektionslinie muss senkrecht zu U sein Falls U = span {a3, a +V, a +0 : b e = b-pb) = at b ↑\" rojulb) U = span{ a3 -> A I =projectswobei b-proju(b) 1 a Falls U =span Zan, ..., an 3 : * (b) = AX , wo b eiAbI - Vispanda= A(ATA) \"ATb wenn ATA invertible 1 , a23- A1 T > Az · 1= P : Projection matrix & proju(b) Es gilt ATA ist invertible ~ symmetric falls & A hat vollen rank Einleitung : Least #Is · Wenn wir ein overdetermined LSE (mehr Zeiten als Spalten) betrachten: T = G ab mit [A(s] - T] , * ke in e so können wir mit Sicherheit sagen , da s Jb mit Ax= b hat keine Lsg . · Methode kleinster A : Zumindest aber finden wir fü Ax +b ein * sodass- 1 AX -b112 minimal! -- Le a s t Squares= Literally für ohne Lösung, die beste Apparation finden Le a s t Squares =Projektion 0· -- · egeb en: , & - mit- b 11 - minimal Das X mit der kleinsten Non II Ax-yll · Esung : Normal equations : = b auf (CA) projizieren #A = Ab LSE lösen! # IMA falls A hat vollen rank: E = (ATA) \"A b 44. Recap : AG · Eure Lösungen · Sieht nice aus! · Meine Lösung a) - 3 -- 125-2 -L 1 -- 125 -2) +- - 3 - 3 O - 12 - 2 - freie Variablen -- 1 -> O - 3 - 3L25 1. LO O 10 - => xp = t -R , bel Aus II : xz = - 1 + 2t # : xz = - 1 - ( - 1 + 2t) + t = - t # x = 6 + 2) - t) + 5( - 1 + 2t) - 2t 3 =2 = 2) t) , te R = 6 - 2t - 5 + 10t - 2t = 1 + 6t = (g) + (2)t ,te R 3 particular Solution special solutions Ax = b with xp = 0 . Ax = 0 , ( NCAI . b) Basis NCA): <(2) > Pivots - L O I Basis (CA ) : E (=, ), ( . 5). (c) R = Es ↓ A = [1375 abhängige erzeugend . c) Aus b) folgt per Def. Dimension : Merkt euch: dimN(A) = 1, din ((A) = 3 · rankA=rankA = din (CA) = dim (AT) = r Da Ae R 4 *3 und rankAT = rankA = 3 3 · dim N(A) = n-v => die N(AT) = 0 , din ((AT1 = 3 · dim N(AT) = m - r d) Da rankAT = 3 : Man hätte auch wie in den Musterlösungen die Zeilenvektoren aus vref (Al nehmen können. Basis (C AT : 3)2) , (33) , (1) Es gilt : R(rref(A)) = R)wef(All = RCA) * Achtung, gleiches gilt nicht für C , da rref und ret bloß Etenops macht bleibt der Zeitenspan . 5. Nächste Woche (8) · Least Squares · Normal equation · Linear Regression ML , Anwendung Least Squares · Orthonormal Basis · Gram-Schmidt · Moore-Penrose In v e rs e K maybe 6 . Lineare Abbildungen Einleitung : Linear Tranformations · Für Intuition (CA) 1 NCA) · Lineare Abbildungen F :X- Y , sind nix anderes als Funktionen zwischen Vector Spaces z . B . Sei F : R3 -> R2 mit F((, ) = [2] · Jetzt : Da sie Lear sind ka n n man jede Lineare Abbildung als eine At ix A schreiben ! Wir haben VFJA: FE ) A \" => A = [18 ] mit A . [] = [2] und A : 13 - 1 · Mit anderen Worten : wir können uns Matrix Vektor Multiplikationen als Abbildungen/ Transformationen vorstellen ! Z =(2) A fal↓ - - - · Betrachten wir nun den ganzen Raum : (w as passiert mit allen Vektoren des - -# Definitionsraums X=R3) wie wir sehen: durch A/F verlieren wir eine Dimension! das macht Sinn da wir vo n 3D (13) in den ZD (R) bilden . · Lass uns das generalisieren : X = R3 Y = R2- -FEA: O · Betrachten wir eine andere Abbildung Fr mit Az : Sei F2 : R3 -> R2 mit =[5]) = [8 ], Ac [888] Jetzt verlieren wir eine weitere Dimension !-- -#d X = R3 Y = R2- O - ImF = Im A = (CA) => F(>\"A : 1R3 -> R E* 1 2 O- Wie du siehst, es gibt viel zu analysieren mit Dimensionen, Räumen etc. · Hier Form al : · Sei F : X - Y mit, Es\" A : X -> Y : X Y M~ - 2 Y· Image: F(XC Im F x Y Ke rF X- · Ke rn e l : IN C A ) =G- wa s man verliert\" I · injektiv : (1) = F(x) Ex = Xz eindeutige Ich -- > E F ⑤ Im F = Y· Surjektiv : (F(X) = erreicht alle s\"- -- X dim X= dim Y L Im F = Y· Bijektiv surjektiv eineindeutig Ka t = # ut euch 3 Blue Brown an!! 7. Quiz · Kahoot . it !","libVersion":"0.3.2","langs":""}