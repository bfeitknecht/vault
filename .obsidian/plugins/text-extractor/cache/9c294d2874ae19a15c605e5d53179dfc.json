{"path":"sem2/PProg/VRL/extra/kuhn/PProg-w04-kuhn.pdf","text":"Parallel Programming Session 4 Spring 2024 Schedule Recall last 2 weeks: finished with Java Threads Assignment 3 post-discussion Theory/lecture recap Some topics you covered briefly this week Preview of assignment 4 Quiz Detail What’s the problem ? Detail Fazit: for ints, bools etc in general do it like this . Thread Basics 1 Process vs. thread Context switch & scheduler State model of the threads ( new, runnable, running, blocked, terminated ) Parallelism vs concurrency Busy waiting & join() Interrupted () (bad) interleaving, race condition critical section monitor/intrinsic lock Thread Basics 2 Bad interleaving idea of atomic operations Race condition critical section synchronize block vs. synchronize method Mutual exclusion synchronizing static methods monitor/intrinsic lock Reentrance wait(), notify(), notifyAll() state change when calling wait / notify(All)() while vs. if with wait() Post-Discussion Ex.3 A If we run taskASequential(), this prints the expected 100'000. Post-Discussion Ex.3 A. If we run taskAParallel(), this prints less than the expected 400'000. WHYYYYYYYYYYY? Post-Discussion Ex.3Post-Discussion Ex.3.APost-Discussion Ex.3 B Thread-safe version: SYNCHRONIZED SYNCHRONIZED Running taskB(), this prints the expected 400'000. Refresher Write the equivalent version of Refresher called intrinsic lock or monitior Post-Discussion Ex.3 C We print the name of the thread performing the increment Wait and Notify Recap 15 Object (lock) provides wait and notify methods (any object is a lock) wait: Thread must own object’s lock to call wait thread releases lock and is added to “waiting list” for that object thread waits until notify is called on the object notify: Thread must own object’s lock to call notify notify: Wake one (arbitrary) thread from object’s “waiting list” notifyAll: Wake all threads essentials Wait and Notify Recap 16 Spurious wake-ups and notifyAll() à wait has to be in a while loop essentials Notify vs. Notify All https://docs.oracle.com/en/java/javase/17/docs/api/java.base/java/lang/Object.html Post-Discussion Ex.3 E public class AtomicCounter implements Counter { public void increment() { ?? } public int value() { ?? } } Post-Discussion Ex.3 E public class AtomicCounter implements Counter { private AtomicInteger c = new AtomicInteger(0); public void increment() { c.incrementAndGet(); } public int value() { return c.get(); } } What’s the difference ? Int c++; AtomicInteger c.incrementAndGet(); What’s the difference ? Int c++; AtomicInteger c.incrementAndGet(); not atomic atomic What’s the difference ? An operation is atomic if no other thread can see it partly executed. What’s the difference ? An operation is atomic if no other thread can see it partially executed. What’s the difference ? An operation is atomic if no other thread can see it partially executed. Atomic as in “appears indivisible”. (not divided into get, increment and write-back) What’s the difference ? An operation is atomic if no other thread can see it partially executed. Atomic as in “appears indivisible”. (not divided into get, increment and write-back) However, does not mean it’s implemented as single instruction. Parallelism on a single core Upon till now: Sequential program -> we as programmers parallelize it f. eg. with Java Threads -> assumed multiple core to execute threads Concurrency vs Parallelism 27 Thread A Thread B Thread C Thread A Thread B Thread C Thread A Concurrent, not parallel Concurrent, parallel Not concurrent, not parallel Parallelism on a single core NOW: How do we get parallelism out of a program on a single core ? Parallelism on a single core NOW: How do we get parallelism out of a program on a single core ? Answer: Special hardware support ILP ILP: Instruction-Level-Parallelism ILP ILP: Instruction-Level-Parallelism Idea: execute independent instructions in parallel ILP ILP: Instruction-Level-Parallelism Idea: execute independent instructions in parallel Independent instructions : result of one isn’t input of the other ILP ILP: Instruction-Level-Parallelism Idea: execute independent instructions in parallel Independent instructions : result of one isn’t input of the other ILP vs multi-threading ILP: single thread ILP vs multi-threading ILP: single thread no concurrency (because single threaded) ILP vs multi-threading ILP: single thread no concurrency (because single threaded) can be combined with multi-threading (ILP for the instructions the thread executes) ILP vs multi-threading ILP: single thread no concurrency (because single threaded) can be combined with multi-threading (ILP for the instructions the thread executes) execute independent instructions in parallel ILP vs multi-threading ILP: single thread no concurrency (because single threaded) can be combined with multi-threading (ILP for the instructions the thread executes) execute independent instructions in parallel Multi-threading: multiple cores concurrency thread to execute instructions ILP vs multi-threading Lets combine both: 1.)Thread is scheduled on a core ILP vs multi-threading Lets combine both: 1.)Thread is scheduled on a core 2.)Nice to execute independent instructions within that thread in parallel (still on the same core as the thread) ->ILP ILP vs multi-threading Lets combine both: 1.)Thread is scheduled on a core 2.)Nice to execute independent instructions within that thread in parallel (ILP inside a thread) ILP: Execute independent instructions in parallel in a single core ILP vs multi-threading Lets combine both: 1.)Thread is scheduled on a core 2.)Nice to execute independent instructions within that thread in parallel (ILP inside a thread) Need hardware for that : if not available => no ILP ILP: Execute independent instructions in parallel in a single core So what is that “hardware”? Pipelining Vectorization (Super-Scalar Execution) -> see this in DDCA So what is that “hardware”? Pipelining Vectorization All about trying to execute independent instructions in “parallel” on one core Pipelining split a “process” into different stages, each with clear function. Pipelining split a “process” into different stages, each with clear function. f.eg: Washing 15min Drying 15min Folding 5min put in closet 5min Pipelining split a “process” into different stages, each with clear function f.eg: Washing 15min Drying 15min Folding 5min put in closet 5min How can we do this smart/efficiently for multiple laundry packets? Pipelining Option 1: sequential: one packet does all stages, then next packet etc. Option 2: Pipeline it, that means: f.eg: Washing 15min Drying 15min Folding 5min put in closet 5min Pipelining Start with 2nd load as soon as first load done with first stage: Pipelining Modern CPU’s do that: f.eg: Washing 15min -> fetch instruction Drying 15min -> decode it Folding 5min -> execute it put in closet 5min -> write back result What we need Each stage needs an execution unit f.eg: Washing 15min washing machine Drying 15min dryer Folding 5min person who folds the clothes put in closet 5min person who puts back clothes Pipelining Each stage needs an execution unit f.eg: fetch instruction -> Load/Store Unit decode it ->. Decoder execute it -> ALU write back result-> Load/Store Unit Pipelining Properties: Latency Throughput Throughput with lead-in /lead-out Balanced/Unbalanced Pipelining Latency: Time needed to perform computation Pipelining Latency: Time needed to perform computation Need to specify if we mean latency of one stage all stages (sum of all stages) Pipelining Latency: Time needed to perform computation Not always constant -> can increase f.eg if we have to wait until predecessor finished. Pipelining Latency: Time needed to perform computation Not always constant -> can increase f.eg if we have to wait until predecessor finished. Latency= Duration of a computation but we need to specify what we mean (stage or all stages) ! Pipelining Goal: decrease latency (short time per computation) Pipelining Throughput: Amount of work a system produces in a given time interval Pipelining Throughput: Amount of work a system produces in a given time interval “how many elements come out of the pipeline in a given interval” Pipelining “how many elements come out of the pipeline in a given interval” Pipelining “how many elements come out of the pipeline in a given interval” 1 element every 15 min Pipelining number of elements leaving the pipeline per given time interval. If we have one execution unit per stage, this holds: Pipelining Goal: decrease latency (short time per computation) increase throughput (lot of output in a given time) Pipelining Goal: decrease latency (short time per computation) increase throughput (lot of output in a given time) how long(latency) vs. how many(throughput) Different perspective Throughput we saw is for “infinite” time (we ignored lead-in/lead-out) If we know how many elements we want to process we use this: Different perspective Throughput we saw is for “infinite” time (we ignored lead-in/lead-out) If we know how many elements we want to process we use this: throughput with lead-in/lead-out = N / (overall time for n elements) Different perspective Throughput we saw is for “infinite” time (we ignored lead-in/lead-out) If we know how many elements we want to process we use this: throughput with lead-in/lead-out = N / (overall time for n elements) (If this formula goes to infinity –> it’s the same as the formula before) Unbalanced Pipelines From before, assume we get another washing machine Unbalanced PipelinesUnbalanced Pipelines Now washing only takes 5min not 10 min. Unbalanced Pipelines latency increases with every load !!! = not constant latency Unbalanced Pipelines latency increases with every load !!! = not constant latency Latency for unbalanced pipelines is infinite/unbounded Balanced Pipelines Balanced: latency is constant for all elements. ” no gaps” Rule: BALANCED <-> NO STAGE LONGER THAN FIRST STAGE Balanced Pipelines, what do we mean? There are 2 definitions of balanced: latency is constant for all elements ->” no gaps” Balanced Pipelines, what do we mean? There are 2 definitions of balanced: latency is constant for all elements ->” no gaps” all stages take the same amount of time During the exam: state which definition you use ! Pipelining: Exam Which of them are balanced ? Pipelining: ExamPipelining: ExamPipelining: Exam “Fangfrage” You operate a factory where completing a product involves 4 tasks. The dependencies between these tasks and their respective completion times can be summarized as: Exam.ple - Pipelining 81 product essentials T0 T2 T1 T3 5’ 3’ 2’ 1’ A. You plan to invest in enhancing ONE task to improve overall throughput. However, every saved minute incurs a cost. A. What is the current throughput? B. Which task do you choose, and by how much do you improve the time? C. What will be the new throughput? Pipeline: Solution 82 essentials 3’ 2’ 1’ A. You plan to invest in enhancing ONE task to improve overall throughput. However, every saved minute incurs a cost. A. What is the current throughput? 1 product / 5’ B. Which task do you choose, and by how much do you improve the time? T2, by 2’: T0 and T1 take 5’ together but can be pipelined. Reducing T2 by 2’min removes the bottleneck it currently induces. C. What will be the new throughput? 1 product / 3’ T0 T1 T2 T3 P0 P1 P2 So what is that “hardware”? Pipelining Vectorization Super-Scalar Execution Vectorization Operations on vector-like data like arrays Vectorization Operations on vector-like data like arrays independent operations of the same type VectorizationVectorization Can easily parallelize the loop: All iterations are independent of each other Vectorization Can easily parallelize the loop: All iterations are independent of each other +threads where each does some part of the array -> need n cores Vectorization Can easily parallelize the loop: All iterations are independent of each other +threads where each does some part of the array -> need n cores + now want a single core approach ? Vectorization Can easily parallelize the loop: All iterations are independent of each other +threads where each does some part of the array -> need n cores + now want a single core approach ? SIMD SIMD “Single instruction, single data” != SIMD SIMD “Single instruction, multiple data” SIMD “Single instruction, multiple data” Vector-Instructions Combine multiple instructions into a single vector-instruction Vector-Instructions “Single instruction, multiple data” Combine multiple instructions into a single vector-instruction Vector-Instructions “Single instruction, multiple data” Combine multiple instructions into a single vector-instruction Either compiler does it for us (auto-vectorization) Programmer specifies it via vector-instructions Vectorization Take –Away Instruction independence where independent instructions are of same type & on vector like data. f. eg the for-loop we saw. Vectorization Take –Away Instruction independence where independent instructions are of same type & on vector like data. f. eg the for-loop we saw. Vectorization needs hardware support f.eg multiple ALU’s Parallelism on a single core via ILP Pipelining Vectorization (Super-Scalar Execution) -> see this in DDCA Some things from last lecture p=1 p=4 Amdahl's Law Gustafson's Law Time p=1 p1 p2 p3 p4p1 p2 p3 p4 p=4 p1p1 Divide& ConquerExecutor-Service Can deadlock, if no threads to execute task Executor-Service ex.submit(Runnable task) : Submits a Runnable object for execution and returns a Future object representing that task. Executor-Service ex.submit(Runnable task) : Submits a Runnable object for execution and returns a Future object representing that task. ex.submit(Callable<T> task): Submits a value-returning task for execution and returns a Future object representing the pending results of the task. Executor-Service ex.submit(Runnable task) : Submits a Runnable object for execution and returns a Future object representing that task. ex.submit(Callable<T> task): Submits a value-returning task for execution and returns a Future object representing the pending results of the task. ex.shutdown(): previously submitted tasks are executed, but no new tasks will be accepted. Executor-ServiceFork-Join fork/join framework solves the deadlock problem other ExecutorService designed for recursive tasks Fork-Join More next weekPre-Discussion Ex.4 Task A Pipelining How does latency and/or throughput change ? Sequential vs Pipelined vs new machines Task B How many cycles needed to execute loop? one multiplication instruction with latency 6 cycles one addition instruction with latency 3 cycles for (int i = 0; i < data.length; i++) { data[i] = data[i] * data[i]; } for (int i = 0; i < data.length; i += 2) { j = i + 1; data[i] = data[i] * data[i]; data[j] = data[j] * data[j]; } Task C We need to identify potential for parallelization: check dependencies for (int i=1; i<size; i++) { // for loop: i from 1 to (size-1) if (data[i-1] > 0) // If the previous value is positive data[i] = (-1)*data[i]; // change the sign of this value } // end for loop Task C We need to identify potential for parallelization: check dependencies for (int i=0; i<size; i++) { // for loop: i from 0 to (size-1) data[i] = Math.sin(data[i]); // calculate sin() of the value } // end for loop Did I really understand? ILP ILP vs. multi-threading Can we have multi-threading& ILP? balanced/unbalanced forms of ILP(pipeline, vectorization) throughput latency ILP needs hardware support. independent instructions vectorization idea of SIMD vectorization vs. pipelining latency of unbalanced pipelines finally See you next week J","libVersion":"0.3.2","langs":""}