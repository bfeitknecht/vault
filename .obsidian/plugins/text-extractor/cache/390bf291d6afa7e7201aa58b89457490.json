{"path":"sem2/DDCA/PV/extra/pvw/DDCA PVW Script.pdf","text":"DDCA PVW SCRIPT Mark Sosman June 2024 1 Contents 1 Binary Numbers 5 1.1 Binary Counting System . . . . . . . . . . . . . . . . . . . . . . . 5 1.2 Powers of Two to Memorize . . . . . . . . . . . . . . . . . . . . . 5 1.3 Addition, Subtraction (All About One’s and Two’s Complement) 6 1.3.1 Addition . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 1.3.2 Subtraction . . . . . . . . . . . . . . . . . . . . . . . . . . 6 1.3.3 A Word of Caution . . . . . . . . . . . . . . . . . . . . . . 7 1.4 Binary Number Representations . . . . . . . . . . . . . . . . . . 7 1.4.1 Unsigned . . . . . . . . . . . . . . . . . . . . . . . . . . . 8 1.4.2 Signed-Bit Magnitude . . . . . . . . . . . . . . . . . . . . 8 1.4.3 Two’s Complement . . . . . . . . . . . . . . . . . . . . . . 8 1.5 Floating Point Numbers . . . . . . . . . . . . . . . . . . . . . . . 9 1.6 Hexadecimal Numbers . . . . . . . . . . . . . . . . . . . . . . . . 10 2 Logic, Gates, and Boolean Algebra 11 2.1 Fundamental Gates . . . . . . . . . . . . . . . . . . . . . . . . . . 11 2.2 Logical Completeness . . . . . . . . . . . . . . . . . . . . . . . . . 11 2.3 Boolean Algebra Notation . . . . . . . . . . . . . . . . . . . . . . 12 2.4 Boolean Algebra Laws . . . . . . . . . . . . . . . . . . . . . . . . 12 2.5 SoP & PoS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12 2.5.1 SoP . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12 2.5.2 PoS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12 2.6 Additional Important Gates . . . . . . . . . . . . . . . . . . . . . 13 2.7 Case Studies: NAND and NOR . . . . . . . . . . . . . . . . . . . 14 2.7.1 NAND . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14 2.7.2 NOR . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14 3 Circuits 15 3.1 Combinational . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 3.1.1 MUX . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 3.1.2 Decoder . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16 3.1.3 Half-Adder . . . . . . . . . . . . . . . . . . . . . . . . . . 16 3.1.4 Full Adder . . . . . . . . . . . . . . . . . . . . . . . . . . 18 3.1.5 Multiplier . . . . . . . . . . . . . . . . . . . . . . . . . . . 19 3.2 Sequential . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19 3.2.1 SR-Latch . . . . . . . . . . . . . . . . . . . . . . . . . . . 20 3.2.2 D-Latch . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 3.2.3 D Flip Flops . . . . . . . . . . . . . . . . . . . . . . . . . 21 3.3 Memories . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22 3.3.1 SRAM - Static RAM . . . . . . . . . . . . . . . . . . . . . 22 3.3.2 DRAM - Dynamic RAM . . . . . . . . . . . . . . . . . . . 22 3.4 Propagation and Contamination Delay . . . . . . . . . . . . . . . 22 2 4 Timing 23 4.1 What is a Clock? . . . . . . . . . . . . . . . . . . . . . . . . . . . 23 4.2 Basic Times . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23 4.2.1 Input Timing Constraints . . . . . . . . . . . . . . . . . . 23 4.2.2 Output Timing Constraints . . . . . . . . . . . . . . . . . 24 4.2.3 Formulas That Follow . . . . . . . . . . . . . . . . . . . . 25 4.3 Clock Skew . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27 4.3.1 Clock Skew Setup Time . . . . . . . . . . . . . . . . . . . 27 4.3.2 Clock Skew Hold Time . . . . . . . . . . . . . . . . . . . . 27 5 Finite State Machines 29 5.1 Basic Anatomy of an FSM . . . . . . . . . . . . . . . . . . . . . . 29 5.2 Moore . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29 5.3 Mealy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30 5.4 The Three Encodings . . . . . . . . . . . . . . . . . . . . . . . . 31 5.4.1 Binary Encoding . . . . . . . . . . . . . . . . . . . . . . . 31 5.4.2 One-Hot Encoding . . . . . . . . . . . . . . . . . . . . . . 31 5.4.3 Output Encoding . . . . . . . . . . . . . . . . . . . . . . . 31 5.5 FSM Construction in Code and Applying Encodings . . . . . . . 32 5.5.1 Binary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32 5.5.2 One-Hot Encoding . . . . . . . . . . . . . . . . . . . . . . 33 5.5.3 Output Encoding . . . . . . . . . . . . . . . . . . . . . . . 34 6 Verilog 35 6.1 Wires and Buses . . . . . . . . . . . . . . . . . . . . . . . . . . . 35 6.2 Basic Modules . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35 6.3 Structural vs Functional . . . . . . . . . . . . . . . . . . . . . . . 37 6.3.1 Bitwise and Reduction . . . . . . . . . . . . . . . . . . . . 37 6.4 Ternary Operators . . . . . . . . . . . . . . . . . . . . . . . . . . 38 6.5 Numbers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38 6.6 Always Blocks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39 7 Basic Assembly - MIPS 42 7.1 Foundations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42 7.2 Arithmetic Operations . . . . . . . . . . . . . . . . . . . . . . . . 42 7.3 Memory Operations . . . . . . . . . . . . . . . . . . . . . . . . . 43 7.4 Branching . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44 7.5 Basic Statements in Assembly . . . . . . . . . . . . . . . . . . . . 45 7.5.1 If(-Else)-Statements . . . . . . . . . . . . . . . . . . . . . 45 7.5.2 While Loops . . . . . . . . . . . . . . . . . . . . . . . . . 45 7.5.3 For-Loops . . . . . . . . . . . . . . . . . . . . . . . . . . . 46 3 8 Single Cycle Processors 48 8.1 High Level Flow . . . . . . . . . . . . . . . . . . . . . . . . . . . 48 8.2 The Basic Components . . . . . . . . . . . . . . . . . . . . . . . . 48 8.2.1 Instruction Memory . . . . . . . . . . . . . . . . . . . . . 48 8.2.2 Registers . . . . . . . . . . . . . . . . . . . . . . . . . . . 50 8.2.3 R, I, J Instructions in Detail . . . . . . . . . . . . . . . . 51 8.2.4 Data Memory . . . . . . . . . . . . . . . . . . . . . . . . . 52 8.2.5 ALU . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53 8.2.6 Basic Lifecycle of an Instruction . . . . . . . . . . . . . . 53 8.3 Performance Evaluation . . . . . . . . . . . . . . . . . . . . . . . 54 9 Pipelining 55 9.1 The Stages . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55 9.1.1 FETCH . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55 9.1.2 DECODE . . . . . . . . . . . . . . . . . . . . . . . . . . . 55 9.1.3 EXECUTE . . . . . . . . . . . . . . . . . . . . . . . . . . 55 9.1.4 MEMORY . . . . . . . . . . . . . . . . . . . . . . . . . . 55 9.1.5 WRITE-BACK . . . . . . . . . . . . . . . . . . . . . . . . 55 9.1.6 Measuring of Latency . . . . . . . . . . . . . . . . . . . . 56 9.1.7 Usual Pipeline Structure . . . . . . . . . . . . . . . . . . . 56 9.2 Special Features . . . . . . . . . . . . . . . . . . . . . . . . . . . 56 9.2.1 Scoreboarding . . . . . . . . . . . . . . . . . . . . . . . . . 56 9.2.2 Data Forwarding . . . . . . . . . . . . . . . . . . . . . . . 57 9.2.3 Internal Register File Data Forwarding . . . . . . . . . . . 57 9.3 Hazards and How to Deal With Them . . . . . . . . . . . . . . . 57 9.3.1 Control Hazards . . . . . . . . . . . . . . . . . . . . . . . 57 9.3.2 Data Hazards . . . . . . . . . . . . . . . . . . . . . . . . . 57 9.3.3 Data Hazards - Forwarding . . . . . . . . . . . . . . . . . 58 9.3.4 Data Hazards - Reordering . . . . . . . . . . . . . . . . . 58 9.3.5 Data Hazards - Stalling . . . . . . . . . . . . . . . . . . . 58 9.3.6 Data Hazards - NOP . . . . . . . . . . . . . . . . . . . . . 58 9.4 (Pipelined) Performance Evaluation . . . . . . . . . . . . . . . . 58 4 1 Binary Numbers 1.1 Binary Counting System In a decimal counting system, the number 123 does not represent ”123” as some abstract concept, but rather it is 3 · 100 + 2 · 101 + 1 · 102 (Notice how the least significant position is n0 worth, where n is the ”number” of the system.) Similarly, in binary 10011 isn’t some abstract concept ”10011” but rather 1 · 2 0 + 1 · 21 + 0 · 2 2 + 0 · 2 3 + 1 · 24 which in decimal is 1 + 2 + 0 + 0 + 16 = 19 In case clarity is lacking regarding which system is used, often a ”diacritic” subscript is added, e.g. x10 means that the number x is expressed in the decimal system, or y2 means the number y is expressed in the binary system. 1.2 Powers of Two to Memorize Here is a table with a few commonly used and very important powers of two. Power Value Power Value 0 1 1 2 2 4 3 8 4 16 5 32 6 64 7 128 8 256 9 512 10 1’024 11 2’048 Power Value Power Value 12 4’096 13 8’192 14 16’384 15 32’768 16 65’536 17 131’072 18 262’144 19 524’288 -1 0.5 -2 0.25 -3 0.125 -4 0.0625 5 1.3 Addition, Subtraction (All About One’s and Two’s Complement) 1.3.1 Addition Addition is relatively easy to perform. Keep in mind that here addition is still associative. When two zero bits are added, the result is zero. When a zero bit and a one bit are added, the result is one. When two one bits are added, the result is zero but there is a carry to the next more significant bit of 1. Example: 1 0 1 0 + 1 1 1 1 1 + (1) 0 + (1) 0 + (1) 1 1 1 0 0 1 One can verify this result manually: 10102 = 10 11112 = 15 110012 = 25 1.3.2 Subtraction Subtraction too is easy to perform: it is essentially addition but converting the second summand into its negative form, that is, its two’s complement form. Essentially, to subtract y2 from x2 one must simply perform the operation x2+!(y2) + 12 What is the two’s complement form? It’s the one’s complement form ”+ 12” What is the one’s complement form? It’s the number itself with every bit flipped, so every 1 becomes a 0 and every 0 becomes a 1 One’s complement form of 110012 example: !110012 = 001102 = 1102 One can technically omit preceding zeros but when doing addition/subtrac- tion/one’s complement/two’s complement form it is best to leave it in for the sake of clarity and alignment. Sometimes it’s even good to pad numbers so that they all have the same length (in immediate calculations). 6 Two’s complement form of 110012 example: Recall that the one’s complement form of 110012 is 001102 Recall that to obtain the two’s complement (given the one’s complement) one merely needs to add a 12, meaning the result is: 001102 + 12 = 001112 1.3.3 A Word of Caution When performing any arithmetic operations on binary numbers (particularly with two’s complement and subtraction) be mindful that overflows and under- flows may occur. For instance, 111112 − 110012 = 111112 + 001112 = 1 ′001102 The number 1 ′001102 can mean 38, but in this case 31 − 25 is hardly 38 but rather 6 - which instead the number 001102 conveniently is. Notice how all the bits with position greater than what our operation originally entailed had to be truncated in order for the result to be mathematically correct. This is called an underflow, for when the result is so small that it loops back around to being large again (in a computer). Similarly, the operation 1112 + 1012 = 11002 which when truncated to fit the form of operands yields 1002 which is 4 Keep in mind that 7 + 5 is 12 and not 4 meaning that (after truncating) an overflow has occured, meaning that the number attempted to be stored is too large for the format - so large in fact that it loops back around to representing a smaller number. Keep in mind that this has only to do with whether or not data is truncated in the form of how much storage/how many bits are allocated to store the re- sult, and not whether or not it is truncated. With truncation the operation 1012 − 1102 would also yield to an underflow (resulting in 1112) and would ac- tually need to have the two’s complement procedure reversed (to obtain 0012) and one would also have to be mindful whether or not the result would be neg- ative - in this case, it would be. 1.4 Binary Number Representations There are three main ways that binary numbers are represented, since − is not typically used (due to it translating poorly as an electric signal). 7 1.4.1 Unsigned Unsigned binary numbers are as easy as pie. Assuming n bits, all n bits are used to represent binary numbers from 0 to 2 n − 1, where the most significant bit represents 2 n−1 1.4.2 Signed-Bit Magnitude Assuming there are n bits, the bits from 20 up until 2n−2 are used to represent numbers. The bit at position n − 1 however is a flag for + or − (when it’s one, the number is interpreted as negative). For instance if there is one byte (or eight bits) at ones disposal, 123 is represented as 0111 1011 whereas −123 is represented as 1111 1011 This representation has two main problems: much more forgivably it is wasteful, as it allows −0 to exist in the form of 10 . . . 0 Much less forgivable is that it breaks arithmetic. −3 + 3 = 0 but 1 0 1 1 + 0 0 1 1 (1) 0 + (1) 0 + 0 + 1 1 1 1 0 which is −6 - as opposed to 0 1.4.3 Two’s Complement Two’s complement works just as one would expect it to: using two’s complement notation. Assuming there are n bits, the bits from 0 to n − 2 are reserved for the actual number - if it’s negative (so ”two’s complemented”) then bit n − 1 is set to 1, otherwise bit n − 1 is set to 0. This representation is neither wasteful (considering it doesn’t have double 0), and actually works with arithmetic. For example (assuming 4 bit numbers): −5 = 1011 and 3 = 0011 Quick sanity check: −5 + 3 = −2 8 1 0 1 1 + 0 0 1 1 (1) 0 + (1) 0 + 0 + 1 1 1 1 0 Here decoding involves looking at the most significant bit, which indicates that 110 has been two’s complemented. The nice thing about two’s complement is that it is self inverse, meaning that 110 must have its bits flipped to 001 and then have a 1 added, becoming 010 (which is 2) As such, 1110 is −2 in (4 bit) two’s complement, proving that two’s complement does in fact work with arithmetic. 1.5 Floating Point Numbers A number such as 123.45 is a decimal number. Recall the definition of 123 which here remains the same: the new part is the definition of .45 which is equal to 4 · 10−1 + 5 · 10−2 It works all the same in binary numbers, only that the first bit after the dot is 2−1 and so on and so on. How are (binary) floating point numbers represented in a computer? Using the IEEE 754 format. Keep in mind that a floating point number is essentially the ”scientific notation” of a number. The number is written as M · BE where M is the mantissa (the actual value), B is the base (as previously discussed), and E the exponent - so how often one has to multiply the mantissa with the base (meaning how often the base occurs in the product, the mantissa appears only once) until the actual number is reached. Typically, one bit is reserved for the sign, a handful of bits for the exponent, and another handful for the mantissa. For instance, a 32-bit floating point number (typically) has: 1 sign bit 8 exponent bits 23 fraction bits (which encode the mantissa as typically a leading 1 is implied) So a number can be encoded as 0′10000011′10100000000000000000000 which just means (1 + 0.101) · 2 10000011−BIAS which is 1.625 · 2 6 = 1.625 · 64 = 104 where BIAS is the bias of the format - in this case, 127 To calculate the bias of any other floating point format, it is 2(b−1) − 1 where b is the amount of exponent bits. Therefore the exponent bits actually have to store de + BIAS, where de is the desired exponent. 9 1.6 Hexadecimal Numbers Hexadecimal numbers are very often used in computer science, particularly be- cause one hex digit corresponds to 4 binary digits (since base 16 is equal to base 24) - which means that any n digit binary number can be represented with a ⌈ n 4 ⌉ digit hex number. In any odd cases, the leftmost digit can simply be padded with 0s, so 111112 (which is 31) simply becomes 0001 ′11112 corresponding to 1F16 Additionally, since hexadecimal numbers have sixteen digits (and the decimal only ten), sometimes letters are used, summarized in this table: Digit Value Digit Value Digit Value Digit Value 0 0 1 1 2 2 3 3 4 4 5 5 6 6 7 7 8 8 9 9 10 A,a 11 B,b 12 C,c 13 D,d 14 E,e 15 F,f 10 2 Logic, Gates, and Boolean Algebra 2.1 Fundamental Gates In circuit design and logic there are three fundamental gates: AND, a two-input one-output gate, OR, also a two-input one-output gate, and NOT, a one-input one-output gate. They are practically equivalent to the fundamental logical operators, respectively ∧, ∨, · or to the frequently seen in Java and many other C-style syntax programming languages boolean operators respectively &&, ||, ! In digital design and computer architecture the basic components that carry informations are not predicates or, ironically enough, bools, but rather signals. Much like predicates can be either true or false or bools can also have either values true or false (or in some languages 1 or 0, such as in C), signals too are discreet and can occupy only two possible values: 1 and 0, or true or false, or ON or OFF. Here the tables for how AND, OR, and NOT output given specific signals: Signal X Signal Y X AND Y 0 0 0 0 1 0 1 0 0 1 1 1 Signal X Signal Y X OR Y 0 0 0 0 1 1 1 0 1 1 1 1 Signal X NOT X 0 1 1 0 2.2 Logical Completeness One fundamental fact about the fundamental gates is that they are logically complete: this means that any possible circuit, from the humble single-memory cell to a circuit that could, in theory, allow humanity to develop things as remarkable as dyson spheres or interstellar travel can all be represented using no more than AND, OR, and NOT gates - any set that can represent any possible logical circuit is in fact logically complete. The procedure of proving that a given set of gates is logically complete is rather simple as well: one must merely be able to construct a different set of already known to be logically complete logical gates, such as the AND, OR and NOT gates. 11 2.3 Boolean Algebra Notation Typically in Boolean Algebra notation the operator X AN D Y is expressed as XY or X · Y , the operator X OR Y is expressed as X + Y , and the operator N OT X is expressed as X 2.4 Boolean Algebra Laws The laws of boolean algebra are best summarized with a table: Name Example Name Example Nullification AND X·0 = 0 Nullification OR X+1 = 1 Identity AND X·1 = X Identity OR X+0 = X Canceling AND XX = 0 Canceling OR X+X = 1 Idempotence AND X = X·X Idempotence OR X = X+X Symmetry AND X·Y = Y · X Symmetry OR X+Y = Y+X DeMorgan’s AND X · Y = X + Y DeMorgan’s OR X + Y = X · Y Transitivity AND X+(YZ) = (X+Y)(X+Z) Transitivity OR X(Y+Z) = XY + XZ 2.5 SoP & PoS Sum of Products and Product of Sums are ways to get from a truth table to a formula. 2.5.1 SoP Assume a truth table: X Y Mystery 0 0 0 0 1 1 1 0 1 1 1 1 in Sum of Products, all the products that result in a true output are summed, meaning: XY + XY + XY This can be rearranged and, with idempotence, become: XY + XY + XY + XY = (X + X)Y + X(Y + Y ) = Y + X = X + Y 2.5.2 PoS Assume a truth table: X Y Mystery 0 0 0 0 1 0 1 0 0 1 1 1 12 In Product of Sums, rows with a false output get their inverted inputs added, meaning here: (X + Y )(X + Y )(X + Y ) Expanding the two rightmost sums yields (X + Y )(XX + Y · X + XY + Y Y ) = (X + Y )(Y · X + XY ) Another expansion yields XX · Y + Y X · Y + XXY + Y XY Notice that XX · Y + Y X · Y respectively consist of parts that cancel out and XXY + Y XY consist of parts that are idempotent, meaning all that’s left is XY + XY - which once more is idempotent, leaving only XY 2.6 Additional Important Gates There are four additional gates which are very frequently used: XOR, XNOR, NAND, and NOR, with their meanings being respectively ”exclusive or”, ”in- verted exclusive or/exclusively must be of same value”, ”not and”, and ”not or”. Here the tables: Signal X Signal Y X XOR Y 0 0 0 0 1 1 1 0 1 1 1 0 Signal X Signal Y X XNOR Y 0 0 1 0 1 0 1 0 0 1 1 1 Signal X Signal Y X NAND Y 0 0 1 0 1 1 1 0 1 1 1 0 Signal X Signal Y X NOR Y 0 0 1 0 1 0 1 0 0 1 1 0 In Boolean Algebra X N AN D Y is expressed as X · Y 13 while X N OR Y is expressed as X + Y A good exercise for the reader would be to attempt to construct all the gates XOR and XNOR using only AND, OR, and NOT gates. 2.7 Case Studies: NAND and NOR 2.7.1 NAND Since the NAND gate is logically complete, a task that frequently occurs is having to translate a different circuit or expression into using purely NAND gates. This can be done relatively easily with the following conversion table: Gate Boolean Algebra NAND NOT X X · X AND X · Y (X · Y ) · (X · Y ) OR X + Y (X · X) · (Y · Y ) Notice how in order to expressed AND using only NAND (NOT AND) the same exact expression for AND is negated and then NANDed with itself. The proof of correctness (which can be done with a simple truth table) is left as an exercise to the reader. 2.7.2 NOR Since the NOR gate is also logically complete, another task that frequently occurs is having to translate a different circuit or expression into using purely NOR gates. This can be done relatively easily with the following conversion table: Gate Boolean Algebra NOR NOT X X + X AND X · Y (X + X) + (Y + Y ) OR X + Y (X + Y ) + (X + Y ) Notice how in order to expressed OR using only NOR (NOT OR) the same exact expression for OR is negated and then NORed with itself. The proof of correctness (which can be done with a simple truth table) is left as an exercise to the reader. 14 3 Circuits 3.1 Combinational A combinational circuit is a circuit where the value of the output depends ex- clusively on the inputs - like a proper mathematical function, the same inputs ALWAYS yield the same output. Almost any function that can be represented with just a humble truth table or logic, predicates, visual novel dialogue trees that do not contain cycles, etc. are or can be represented as combination cir- cuits, meaning that there really is nothing particularly crazy about them. 3.1.1 MUX The MUX or multiplexer is a certified electrical engineering classic, as it re- ceives n different signals and using ⌈log2(n)⌉ switches (or additional signals) it becomes possible to choose which specific signal of the original n to propagate and output - in other words, it’s a selector. The most basic MUX is a 2-MUX, one that receives two signals and with just one humble switch it allows one to choose which signal to propagate. Figure 1: 2:1 MUX 15 Figure 2: 4:1 MUX 3.1.2 Decoder A decoer effectively takes a binary number and selects an output such that for each number only one specific signal is true. For n input signals there are 2n output signals. Figure 3: 2:4 Decoder 3.1.3 Half-Adder A half adder is relatively simple, taking care of the sum and carry of the addition of two (respective) binary digits. 16 X Y S C 0 0 0 0 0 1 1 0 1 0 1 0 1 1 0 1 Figure 4: Half Adder 17 3.1.4 Full Adder A full adder is meant to compute the sum and carry out of two binary digits and a carry in - this is particularly important for multi bit adders, where occasionally one digit has a carry which is meant to flow into the next, and so on and so on. Figure 5: Full Adder The derivation of the truth table is left as an exercise to the reader. Single-bit full adders can be chained to create 4bit, 8bit, even 32bit adders - these are called ripple carry adders, but they do get a little slow with increasing size. 18 3.1.5 Multiplier Binary multiplication works a lot like ”regular” multiplication - at least in ex- panded form. As an example, here is the multiplication of 10102 and 10012 1 0 1 0 × 1 0 0 1 1 0 1 0 + 0 0 0 0 + 0 0 0 0 + 1 0 1 0 + 1 0 1 1 0 1 0 As one can see, there’s a lot of ”AND”ing and a lot of summing going on. A single digit multiplication is little more than binary multiplication: but it has to be ”shifted” and then added. Quick sanity check: 10102 = 1010; 10012 = 910; 1011010 = 210 + 810 + 1610 + 6410 = 1010 + 8010 = 9010 One component that is part of the raw digit (prior to all the carries being added and then propagated further) is Rn = ∑n i=0 XiYn−i Of course, depending on the multiplication, the intermediate additions may have several carry signals - so much so that it is incredibly difficult to find a closed formula for any digits. Simply keep in mind that a multiplication is little more than a bunch of ANDs, additions, and shifts. The sketching of a 2x2 and 3x3 bit multiplier is left as an (optional, very very optional) exercise to the reader. 3.2 Sequential A sequential circuit goes a little beyond combinatorical logic: while yes, the current inputs do matter, the previous inputs also matter. In other words, sequential circuits have state. In even clearer words sequential circuits have: memory. 19 3.2.1 SR-Latch Figure 6: SR latch using two cross-coupled NOR Gates Explanation of the figure: the left top input is the RESET signal, the left bottom input is the SET signal. The right top output is ”Q”, the right bottom output is ”Q The truth table: S R Output 0 0 Remains as it was previously. 0 1 Reset Q = 0 1 0 Set Q = 1 1 1 Everything is off The last possible state is practically ”not allowed” as it violates the condition by making Q = 0 = Q This means that an SR latch is ”easily broken”. 20 3.2.2 D-Latch Figure 7: D latch consisting of an SR latch Here the ”middle positioned” input is the Enable signal, which practically turns the latch on or off. The outputs are, just like in an SR latch, top right Q, bottom right Q The D input is practically the ”data” signal. As long as the Enable signal is off, Q effectively stores whatever it was previ- ously - that is, it has become memory. Enabling the Enable signal, the D/Data signal states precisely what Q is sup- posed to be: if D is off, then so is Q; if D is on, then so is Q E D Output 0 X Remains as it was previously. 1 0 (Re)set Q = 0 1 1 Set Q = 1 3.2.3 D Flip Flops Besides being beachwear, flip flops are also memory cells which are hooked up to a clock: so instead of an Enable single, the memory cell (read: Q signal/output) reads the data specifically during a clock update (in case it’s programmed to read during a rising-edge, it reads when the clock goes from 0 to 1, otherwise, if it’s falling-edge sensitive, it reads when the clock goes from 1 to 0 21 3.3 Memories 3.3.1 SRAM - Static RAM SRAM is two NOT gates chained in an almost ouroboros fashion (cross couple inverters). It’s static, but pretty volatile. Static in this case means that it’s going to stay the way it is unless it gets updated. 3.3.2 DRAM - Dynamic RAM DRAM is just a capacitor. Seriously, it’s just a capacitor. The only problem is that capacitors tend to discharge over time. Meaning that this memory, funnily enough, can’t always remember. It has to be refreshed and the fact that it changes over time, is what makes it dynamic. Better storage times require larger capacitors which require larger areas (and more resources) so it can get pretty pricey. 3.4 Propagation and Contamination Delay In the context of circuits at large, the contamination delay is the smallest delay of a circuit (so the shortest path and ”hold-up” a signal will experience while traveling). It’s called this way because it’s practically a result of a simple ”con- tamination” with the circuit. Propagation delay on the other hand is the temporal length of the (read: time it takes to traverse the) critical path of a circuit. The mnemonic here is that it is the time it takes for the ”complete propagation” of a signal from one end to another. Keep in mind: a gate is just a trivial circuit. So they too have contamination and propagation delay. To compute the propagation delay of a circuit, simply analyze what the longest time is a signal could possibly take to traverse a circuit. Contamination: same thing, just swap longest with shortest. 22 4 Timing 4.1 What is a Clock? For the longest time, timekeeping was performed by staring at the sun - there was a LOT of blindness back then, presumably. At some point, time keeping became mechanical, and back in the 1960s-1970s people decided to keep elec- trocuting a little quartz crystal, hook it up to a tuning fork, and create time keeping that works off of the piezoelectric oscillation of quartz crystals (which is usually at least a couple thousand Hertz). Keep in mind that these quartz crystals oscillate at a very consistent frequency, and use electrity, which makes them perfect for computers and are also the very basis of (computer) clocks. So a clock in a computer is little more than a little circuit-like component that electrocutes a little quartz crystal-tuning fork contraption and converts that to an electric signal (of course it can be ”manu- ally” slowed depending on the needs of a program) and thus allows man to do the impossible: implement regular refreshes and updates of memory. Inciden- tally, digital watches are technically computers (that specialize in only one to five functions) and are, despite the much lower cost, cheaper than ”traditional” quartz watches, which usually lack a computer surrounding the quartz-tuning fork-electrical signal contraption. This is just good background knowledge to keep in mind. 4.2 Basic Times Timing is everything, both while making eggs and trying to analyze circuits. It is also important to keep in mind that values of signals must be 0 or 1 - this is connected to a voltage in practice and occasionally a voltage may be in the process of changing and it can not be properly identified as either a 0 or 1 - this is called the ”Danger Zone” (with greetings to Kenny Loggins). For the purposes of this chapter, every single circuit is connected to a clock unless mentioned otherwise. Given this, there are some timing constraints that particularly sequential circuits experience: 4.2.1 Input Timing Constraints Around a clock update, an input value must be stable. This is due to most circuits in computers being constrained to working within cycles - so a certain clock update must come before a ”new” output can be computer. In order for the output to be computed normally, the input has to be stable at least a little bit before the ”actual” computation starts. 23 Figure 8: Input Constraints Prior to the clock update, the input must be stable for some time, called the setup time tsetup After the clock update, the input must be stable for yet another some time, called the hold time thold The aperture time is the some of both the setup and hold times. 4.2.2 Output Timing Constraints Usually (or at least very very often) outputs of one circuit get repurposed as inputs of others. Even if that is not the case, a circuit must still perform a computation, for which it must first receive the inputs, and then complete the computation, that is, ”reach the time” during which the outputs are ready to use. 24 Figure 9: Output Constraints The time after the clock update that the output value will begin changing is called the contamination delay tccq - keep in mind the output value between the clock update and the contamination delay passing is supposed to be sta- ble. Once the contamination delay has passed, the circuit begins to overwrite (that is, compute) the (new) output value. The time after which the circuit is GUARANTEED to be done is called the propagation delay, that is, tpcq 4.2.3 Formulas That Follow There are primarily two main formulas that follow within a chain of circuits. Once more it is imperative to clarify that while the defined delays seem to only care about a clock getting updated (so half a period), the actually relevant time frame is an entire period, so a clock getting updated and then re-updated (unupdated?) back to its ”original” value. Sometimes the beginning of the period is a rising edge (going from 0 to 1), sometimes the falling edge (going from 1 to 0). It all depends on the implementation. Primarily: TC ≥ tpcq + tpd + tsetup which means that the period (of the processor) must be at least long enough to cover the propagation delay of a computation (of the input for this circuit, as it ”begins life” as an output of a previous component) beginning at the start of the period, the propagation delay of the entire circuit (assumed to be combinational here, so this part has no clock), so that the next output can actually be com- 25 puted, and then the setup time, as the new output will be used as the next input. Figure 10: Input and Output with all Constraints The other formula: thold > tccq + tcd The hold time refers here to the time the output of the circuit (since it will be repurposed as the input for the next component) ends up being somewhat smaller than the contamination delay of the circuit input (recall, it is an out- put of a different component), and then the proper contamination delay of the circuit itself, that is, the time before it can actually work. Of course, the only reason why the hold time is smaller than the sum of both contamination delays is because it is essentially what is being overwritten - by the time the first con- tamination delay has passed the input will be getting recomputed, and by the time it is done AND the contamination delay of the circuit proper has passed (mind you, the computation and circuit’s contamination delay may somewhat overlap) the output is getting recomputed - so its function as an input is of course somewhat up in the air, temporarily. 26 4.3 Clock Skew As is the case with all other signals, the clock signal too must travel - very often it experiences delays which are very much percetible. 4.3.1 Clock Skew Setup Time For setup time the worst case is the output clock being ”functionally” earlier than the input clock, resulting in unnecessary wasting of time (tskew) with the circuit’s output. The propagation delay of the input computation works perfectly well, the propagation delay of the circuit itself as well, but the setup time ends up being unnecessarily prolonged by the skew time. This greatly affects the period, making the cycles longer and the frequency smaller. TC ≥ tpcq + tpd + tsetup + tskew Figure 11: Clock skew prolonging setup time and the period 4.3.2 Clock Skew Hold Time For hold time the worst case is the output clock being ”functionally” later than the input clock, resulting in unnecessary delay within the input computation. The contamination delay of the circuit is then prolonged by the clock skew time, but the input’s contamination delay may be practically subtraced. thold + tskew ≤ tccq + tcd tcd ≥ thold + tskew − tccq 27 Figure 12: Clock skew affecting contamination delay of the circuit 28 5 Finite State Machines 5.1 Basic Anatomy of an FSM An FSM consists of (a finite amount of) states, each with the (same number) of outgoing transations, one (per state) for each possible combination of inputs. It is important that for each state all possible transitions caused by input com- binations must be exhausted, otherwise the possibility of undefined behavior occurs, which is NEVER good. FSMs also consist of a set of possible outputs, though the set of achievable states must not necessarily exhaust the set of all possible states, that is it okay for an FSM to only achieve the outputs ”01” or ”10” even if the outputs ”00” and ”11” are also possible. The last basic part of an FSM is the reset indicator, which points towards the initial state - the initial state is also the state to which the FSM defaults to if the reset signal is activated. 5.2 Moore A Moore FSM has the property that outputs are assigned specifically to states. Assume a Moore FSM that represents a vendig machine that accepts two dif- ferent tokens: W tokens and F tokens - the vending machine dispenses either bottles of water or a fruit. Figure 13: Caption Here the output is encoded as XY, where X states whether or not Water is being dispensed, and Y whether or not a Fruit is being dispensed. 29 The input is encoded as WF, where each bit indicates whether the epony- mous taken had been inserted. Notice if neither token or both tokens are in- serted simultaneously, the machine here (by design, not by any mathematical law) defaults to the initial state, because the specification of the machine in- cludes that at most either one water bottle or one fruit may be dispensed. Of course, linguistic ambiguity may pose issues but these are not considered here. 5.3 Mealy Mealy machines typically are more compact than Moore machines, since in a Mealy machine the output is not coupled with state but rather previous state and input - which means that that the output is to be found specifically on the transition label, meaning that there is a lower amount of minimum states. Assume for example now a (Mealy FSM represented) vending machine that instead dispenses a bottle of juice, or a bottle of milk. In order to dispense anything, first a coin must be inserted and then only ONE button must be pressed: here the input is encoded as CJM, meaning ”signal for coin, signal for juice button pressed, signal for milk button pressed” Figure 14: The output is encoded as ”01 = milk dispensed; 10 = juice dispensed” The reason as to why !C appears by itself is because all combinations with !C would all yield the output 00 - nothing getting dispensed, because no coin had been inserted. Notice that there are fewer states here - in fact, this entire machine could theoertically fit onto only one state, but it would then transform into a comically illegible mess. 30 5.4 The Three Encodings There are primarily three commonly used state encodings for FSMs (keep in mind the state labels are usually encoded since writing into a computer the phrase ”Here the vending machine specifically only outputs soda because it has gotten two dollars, never you mind consisting of what change or which banknotes” usually yields poor results. In general states (or their label) are encoded using a binary number - as are the outputs and inputs. These three encodings are as follows: 5.4.1 Binary Encoding The states are encoded as an enumeration of binary numbers, obviously in in- creasing value - assume there are n states, so there are ⌈log2(n)⌉ bits necessary to represent all the states - in some cases, a couple go unused. For instance, in an FSM with 5 states, the possible state labels are 000, 001, 010, 011, 100 What is it good for? It minimizes the amount of bits necessary to store all the states, so this encoding is particularly good for memory concerns. 5.4.2 One-Hot Encoding Here the state bus (bus = bit array) has as many bits as there are states - if there are n states, there are n state bits, and for each state only one bit is true - the rest are off. For instance, if there are four states, in one-hot encoding the possible state values would be 0001, 0010, 0100, 1000 What is it good for? It minimizes the state transition logic: One only has to take into account (when transitioning) which bit of the state bus is currently ”hot” (and of course which input is currently being driven) - all those other pesky bits pose no problem in terms of logic. In terms of memory however this is likely one of the worst encodings one could choose. 5.4.3 Output Encoding Here the only thing that matters is the output, meaning: states are represented practically by their output. This one may falter if multiple states have the same output or if a Mealy machine is being used. Assuming however that an FSM has possible outputs 00, 01, 10 then the states are the exact same: there is one state labeled 00, another labeled 01, and a last one labeled 10 What is it good for? It minimizes the output logic, so much so that it is sufficiently described with ”output = state;” - the state transition logic may become a little more complicated, it is so/so regarding memory (potentially), and it is somewhat situational, but an ingenious encoding nonetheless. 31 5.5 FSM Construction in Code and Applying Encodings In order to construct an FSM, at least in order to represent it, either a graph is needed, in which case one must choose carefully whether to work with Moore or Mealy, how to best label the states, etc. In code(-like representation) however, the decisions are somewhat different: what encoding to use? How to make the state transition feasible? How to make output logic feasible? Here a worked example: Assume an FSM that computes whether or not a (bi- nary) number is currently divisible by 4 - in this case, its suffix needs to be at least two 0 Unless the reset button is hit, the new bit always gets appended to the number - so there are three states: No zero (currently divisible by neither 2 or 4), one zero (currently divisible by 2 but not by 4), and two zeroes (currently divisible by 4). Exercise to the reader: sketch this FSM. Mathematically, if the number is empty (or just 0) it is trivially divisble by 4 5.5.1 Binary Here the output only really needs one bit: whether or not the number is divisble by 4 States (Names and Values respectively) = NZ,OZ,TZ = {00, 01, 10} Input Signal Name = I Outputs = {0, 1} State Transition Logic: The state bus consists of two bits here. The LSB is 1 if and only if the previous state was 00 AND a 0 was appended to the number. N extState[0] = (!CurState[1]) · (!CurState[0]) · (!I) · (!RST ) Similarly, the only way to obtain a true MSB is if the previous state is 01 or 10 itself and a 0 is appended: N extState[1] = (!CurState[1]) · CurState[0] · (!I) · (!RST ) +CurState[1] · CurState[0] · (!I) · (!RST ) which can be simplified to N extState[1] = CurState[0] · (!I) · (!RST ) 32 Output Logic: Since there is only one output bit, and the output is true if and only if the current state is 10 it is as simple as Output = CurState[1] 5.5.2 One-Hot Encoding Here the output also only needs one bit, and as does the input. The only thing that changes ”variable” side is the state bus and its values, which then become: N Z, OZ, T Z = {001, 010, 100} State Transition Logic: Here, it is a little more involved in the number of lines of code necessary to check, which has increased by n ⌈log2(n)⌉ -fold - the difficulty however should be practically trivial. The LSB is true if and only if the current state is NZ, which is reachable only if a one had been added to either OZ or TZ: N extState[0] = (!RST ) ∗ CurState[1] ∗ I + (!RST ) ∗ CurState[2] ∗ I which can be simplified to N extState[0] = (!RST ) ∗ (CurState[1] + CurState[2]) ∗ I Keep in mind, the reason why only CurState[1] is written out instead of the complete boilerplate of (!CurState[2]) ∗ CurState[1] ∗ (!CurState[0]) is because it IS (unnecessary) boilerplate, as by virtue of one-hot encoding if one bit is true in the state bus, obviously all others are false. The middle bit is only true in the state OZ, which is reachable exclusively from NZ with one 0, meaning that it is N extState[1] = (!RST ) ∗ CurState[0] ∗ (!I) and MSB is true only if 0 has been input in OZ or if, at any point, the restart signal was triggered, meaning: N extState[2] = (!RST ) ∗ CurState[1] ∗ (!I) +RST ∗ CurState[0] + RST ∗ CurState[1] + RST ∗ CurState[2] This can be further simplified, considering the fact that the restart signal works (or at least should work) regardless of state, to N extState[2] = (!RST ) ∗ CurState[1] ∗ (!I) + RST 33 Output Logic: Here the output is 1 if and only if the current state is TZ, meaning the output logic is as simple as Output = CurState[2] 5.5.3 Output Encoding Recall that in output encoding the state label must match precisely the output of its state: here the issue is that there are three states but only two outputs. The solution: make the output consist of two bits, meaning that now the output must change to be {00, 01, 10} (there is some freedom here but counting up in binary is usually the most nice way of doing these sorts of things). Therefore ”variable declaration” part is now: States = NZ,OZ,TZ = {00, 01, 10} = Outputs (per State) Input = I State Transition Logic: Fortunately, the state transition logic here happens to be the exact same as in the binary encoding, so the code may be ”recycled” (N.B. in actual software (and non software, if applicable) development, if code is reused, please please please confine it into a specific other module or function or similar and simply instantiate functions instead), so: N extState[0] = (!CurState[1]) · (!CurState[0]) · (!I) · (!RST ) N extState[1] = CurState[0] · (!I) · (!RST ) Output Logic: Recall that in output encoding, the output and state are quite literally the same value, which means that the output logic is quite literally: Output = CurState or, for those inclined to verbose code calligraphy: Output[0] = CurState[0] Output[1] = CurState[1] 34 6 Verilog 6.1 Wires and Buses The absolute simplest piece of Verilog code is 1 wire x ; which simply defines one available bit and labels that as the signal x A little bit more complex is a bus, which is practically an ”array” (in theory contiguous in memory) of bits: 1 wire [15:0] small ; This defines 16 bits in ”one variable”. Indexing is simple, for instance, if one wants to specifically work on bit 7: 1 small [7] = x ; This would assign whatever value is (at that time) in x to the 7th bit of the bus ”short”. 6.2 Basic Modules Basic modules consist of three main parts: the module name and input/output list, the ”clarification” on which signals/busses are inputs and outputs, and a ”sentinel” that states that the module is over. 1 module doNothing ( input one , input two , output three ) ; 2 // ceci c ’ est ne pas du code 3 endmodule is equivalent to 1 module faitRien ( one , two , three ) ; 2 input one ; 3 input two ; 35 4 output three ; 5 // Verilog is not a programming language . Neither is HTML - or LaTeX 6 endmode Per file only one module may be declared. It is impossible to declare a module within a module. It is however possible to instantiate a module: take for example 1 module allOperations ( input a , b , 2 output [1:0] simpleOps , compOps , xorOps ) ; 3 assign simpleOps [0] = a & b ; 4 assign simpleOps [1] = a | b ; 5 assign compOps [0] = ~ simpleOps [0]; 6 assign compOps [1] = ~ simpleOps [1]; 7 assign xorOps [0] = a ^ b ; 8 assign xorOps [1] = ~( a ^ b ) ; 9 10 endmodule The choice to instantiate can be simple but demanding in user to remember the order: 1 someStuff allOperations (x , y , a ,b , c ) ; 2 // here : x = a in allOperations description 3 // y = b in allOperations description 4 // a = simpleOps ( requires a to be a two bit bus ) 5 // b = compOps ( same as above ) 6 // c = xorOps ( same as above ) or a little more involved but easier to read: 36 1 someStuff allOperations (. b ( y ) ,. a ( x ) ,. compOps ( b ) ,. xorOps ( c ) ,. simpleOps ( a ) ) ; 6.3 Structural vs Functional Structural modules are modules which consists out of other, smaller modules. For instance, an ALU using 1 adder adderModule32 (. x ( a ) ,. y ( b ) ,. c_in (0) ,. out ( res ) ,. c_out ( carry ) ) ; whereas functional is using... functions, such as an ALU using 1 assign negatedX = ~ x + 1; 6.3.1 Bitwise and Reduction Assume xbus is a 4-bit bus. Assume ybus is also a 4-bit bus. Assume res4 is, once more, a 4 bit bus. Assume resMini is a single wire. 1 assign res4 = xbus | ybus ; is the same as 1 assign res4 [0] = xbus [0] | ybus [0]; 2 assign res4 [1] = xbus [1] | ybus [1]; 3 assign res4 [2] = xbus [2] | ybus [2]; 4 assign res4 [3] = xbus [3] | ybus [3]; whereas 1 assign resMini = | xbus ; is the same as 37 1 assign resMini = ( xbus [0] | xbus [1]) | ( xbus [2] | xbus [3]) ; Beware, some operations can be both bitwise and ”reductions”, others are only bitwise, such as ∼, because it simply flips bits. 6.4 Ternary Operators Ternary operators, a certified code-golfer’s classic, are also an addition in Ver- ilog, useable as follows: 1 assign z = ( sel ) ? y : x ; 2 // if sel is 1 , z = y ; otherwise z = x They may also be chained. 1 assign z = ( sel [1]) ? ( sel [0] ? x [3] : x [2]) : ( sel [0] ? x [1] : x [0]) ; 6.5 Numbers Numbers are simple to express in Verilog: any number is N ′Bm where N is the number of bits, B is the base, and m is the number. For instance: 4 ′b1010 = 10102 = 1010 8 ′h12 = 0001001016 5 ′d24 = 110002 = 2410 It’s also possible to concatenate numbers/values in Verilog. For instance: 1 wire [2:0] x ; 2 assign x = 3 ’ b010 ; 3 wire [4:0] y ; 4 assign y = 5 ’ b10101 ; 5 wire [15:0] myShort ; 6 myShort = {y ,x ,x , y } 38 7 // myShort would be the number 10101010 _01010101 8 // broken into two bytes for legibility It’s also possible to define parameters/constants: 1 parameter three = 2 ’ b10 ; 2 parameter fifteen = 4 ’ b1111 ; 6.6 Always Blocks The basic anatomy of an always block consists of 1 always @ ( sensitivity list ) 2 statement1 ; 3 ... 4 statementn ; where the sensitivity list are the signals for which the always block ”watches out” for in terms of updates, leading the entire always block getting recomputed, that is, statements 1 through n getting executed again. Within an always block, the assign statement is not used. Additionally, within an always block, wires are not used either, but instead regs. A reg bus is just like a wire bus, only instead of writing ”wire[15:0] myBus” one needs to write ”reg[15:0] my Bus”, at least if myBus is meant to be updated inside of an always block. Inside of an always block information may be read from wires (but not written to). Instead, the choice lies between = (blocking) and <= (non-blocking) statements. When = is used, the value is assigned IMMEDIATELY, so 1 always @ ( x ) 2 begin 3 x = 0; 4 // x is 0 5 y = x ; 6 // y is x which is 0 7 end whilst when <= is used 39 1 always @ ( x ) 2 begin 3 x <= 1; 4 // what is x ? Is it 1? Maybe . 5 y <= x ; 6 // what is y ? Is it also 1? Maybe . It ’s not ( necessarily ) x ( just yet ) 7 end 8 // now x is 1 and now y is x ( which still may or may not be prior to getting updated 1) The reson for the last comment is that <= assignments happen in parallel, as opposed to ”sequentially”. Another important question is whether something is synchronous or asynchronous. Always blocks (usually) have the clock in their sensitive list. For instance 1 always @ ( posedge clk , set ) 2 begin 3 if ( reset == ’1 ’) x = 0; 4 else x = set 5 end here when clk goes from 0 to 1, it gets synchronously reset (so it is in sync with the clock). The set is asynchronous, because it doesn’t matter what happens with the clock, if set is updated, then so is x (unless reset is true). It is also possible to use negedges, so when a value changes from 1 to 0. No edge means it doesn’t matter ”which direction” a value changes in. If/Else statements can only be used in always blocks. As can cases. Examples: (assume a module with ”input reset, input[1:0] number, output reg even”) 1 reg x ; 2 reg y ; 3 4 always @ ( reset ) 5 begin 6 x = 0; 7 if ( reset ) y = 0; 40 8 else y <= x ; 9 end 10 always @ ( number ) 11 case ( number ) 12 2 ’ b00 : even = 1; 13 2 ’ b10 : even = 1; 14 default : even = 0; 15 endcase It has been experimentally determined that if statements and case state- ments have to be done in technically seperate always blocks, even if they have the exact same sensitivity lists. May not apply to all versions of Verilog. 41 7 Basic Assembly - MIPS 7.1 Foundations There are mainly two types of assembly operations: arithmetic, and memory - in other words, operations that are either performed by the ALU (Arithmetic Logic Unit) or by the MU (Memory Unit). In general in assembly the main two types of ”inputs” are immediates and reg- isters, with immediates being specific values (and usually being marked with a special symbol, like a dollar sign ($)) and registers being somewhat analogous to variables in ”regular” programming languages, such as ”R3” or ”R2” - usually they consist of the letter ”R” indicating register and a number which is fairly arbitrary and effectively acts as an ”address”. Assembly is a little more com- plicated than this but for the purposes of this course, this simplification should suffice. 7.2 Arithmetic Operations The two main arithmetic operations in MIPS are addition and multiplication. Keep in mind that when working with immediates, the operation gets an ”i” appended to its name. 1 addi $t0 , $t1 ,11 which means ”The value at register $t0 is (whatever value is at register $t1) + 11 Alternatively if it is absolutely imperative that a given register store ONLY a certain number, one could always write 1 addi $t0 , $0 ,11 as $0 is a register that always stores ”just” 0 If of course the goal is to add the values from two registers, one can always do 1 add $t0 , $t1 , $t2 In this example, $t0 := $t1 + $t2 The long lost cousin of addition, subtraction, also has its very own MIPS in- structions: 42 1 sub $t0 , $t1 , $t2 is $t0 := $t1 - $t2 Unforunately, there is no way to use immediates in SUB operations, unless one is determined enough to shove a specific value into yet another temp variable, as is frequently required to do in assembly languages. It is also possible to multiply, the long lost grandfather of addition, using MIPS code in fashion such as 1 mult $t0 , $t1 , $t2 in order to have $t0 := $t1·$t2 There is no instruction for multipliying an immediate - directly, at least - but it can easily be circumvented by simply storing ones immediate value in a pre- ferred register and using that inside of a regular mult operation. It is also possible to shift values (that is, divide or multiply by two by virtue of appending a 0 or taking away the LSB 1 sll $t1 $t0 3 for instance means shifting the value stored at $t0 by three positions (so if there is 101 it will become 101000, so it goes from 5 to 40) On the other hand 1 srl $t1 $t0 2 then means shifting the value stored at $t0 by two positions (so 101 will become 1) The point to remember is that this is LOGICAL shifting and is not always completely correct when dividing, but otherwise left means multiplying and right means dividing, with 2k where k is the number of bits shifted (the immediate value in the operation) 7.3 Memory Operations The choices for memory operations are rather limited, consisting almost exclu- sively of read (lw - load word) operations and write (sw - store word) operations. 43 1 lw $t0 , 4( $0 ) Loads into register $t0 the data stored at the address $0 offset by 4 - mean- ing at address (from data memory) 4 Similarly, or rather contrastingly, 1 sw $t1$ , 0 x2a ( $0 ) stores from register $t1 the data into the address (in data memory) $0 off- set by 42 - notice that offsets may be written both using decimal as well as hexadecimal numbers (recall 0x2a = 2 · 16 + a · 1 = 32 + 10 = 42) 7.4 Branching There are two kinds of branching in assembly: conditional (like in if-statements) and non-conditional - effectively jump or GOTO LABEL operations. The operation 1 beq $t0 $t1 myLabel would jump the program to the ”myLabel” segment - given that the value in $t0 is equal to the value in $t1 One the other hand 1 bne $t0 $t1 myOtherLabel would jump the program to the ”myOtherLabel” segment if $t0 is NOT equal to $t1 Related to branching the operation 1 slt $t2 $t1 $t0 checks whether $t1 < $t0 - if so, it stores 1 into $t2, otherwise it stores 0 in $t2 44 7.5 Basic Statements in Assembly Occasionally it is necessary to write statements such as for-loops, if-statements, and such directly in assembly. 7.5.1 If(-Else)-Statements Assume the statement 1 if ( x == y ) x = y - z ; 2 else x = y ; Expressed in MIPS it would be 1 bne $s0 $s1 myElseLabel 2 sub $s0 $s1 $s2 3 j finishedWithIf 4 myElseLabel : 5 addi $s0 $s1 0 6 finishedWithIf : 7 // whatever other code follows this // If one the other hand one has just a simple if statement (without any else statement) 1 if ( x == y ) x = y - z ; it becomes possible to 1 bne $s0 $s1 myContinueLabel 2 sub $s0 $s1 $s2 3 myContinueLabel : 4 // whatever other code follows this // 7.5.2 While Loops While loops are a little more involved but can all be simply divided into labels and well-placed (conditional) branches: imagine a program which calculates the largest sum of consecutive positive integers smaller than, for example, 100: 45 1 int x = 0; 2 int i = 1; 3 while ( x <= 100) { 4 x += i ; 5 i ++; 6 } 7 x -= i ; ”Translated” to MIPS: 1 addi $t0 $0 0 2 addi $t1 $0 1 3 addi $t2 $0 100 4 5 myWhile : 6 beq $t0 $2 myFinish 7 slt $t3 $t2 $t0 8 bne $t3 $0 myFinish 9 add $t0 $t0 $t1 10 addi $t1 $t1 1 11 j myWhile 12 13 myFinish : 14 sub $t0 $t0 $t1 7.5.3 For-Loops For loops (which are actually while loops pretending to be elegant) are some- what similar to implement, particularly since the last while-loop had a special ”running” variable. 1 int x = 0; 2 for ( int i = 0; i < 7; i ++) x += 6; Translated to MIPS: 1 addi $t0 $0 0 2 addi $t1 $0 0 3 46 4 myForLoop : 5 beq $t1 7 myDoneLabel 6 addi $t0 $t0 6 7 addi $t1 $t1 1 8 j myForLoop 9 10 myDoneLabel : 47 8 Single Cycle Processors 8.1 High Level Flow A single cycle processor executes each instruction in one, single cycle. From additions to memory accesses, all in one cycle. The basic flow of an instruction’s basic execution in a single cycle processor is: read the instruction (1) decode the instruction (2) retreive operands from registers or memory (if they’re not immediates (3) execute the operation (4) write the result (5) move to the next instruction (6) one must imagine the single cycle processor happy. 8.2 The Basic Components The four basic components/cornerstones of a processor are the instruction mem- ory (sometimes just memory), the registers, the data memory, and the ALU. 8.2.1 Instruction Memory The instruction memory is practically the ”list” in which the entire program is stored (as instruction). Every line of the program is 32-bits. In other words, every line of the program is 4 bytes. Memory addresses (on a 32-bit machine) are 32 bits wide. Consequently it is theoretically possible to have 4 Gigabytes worth of addressable memory. 48 Figure 15: Sketch of Instruction Memory The memory address is stored in a special reserved register called the pro- gram counter, often abbreviated to PC. Usually, when one instruction is finished and the next one must be executed, well, next, the PC is updated by +4 (since usually byte addressable memory is assumed). There are three main types of instructions: R type (read from two registers), I type instructions (using immediates in computation), and J type instructions (which are specifically for ”jumping around” in the code). J type instructions in particular are important because they allow the programmer to jump to a very specific instruction, one which may have occured earlier or will occur much much later - this is done by updating the program counter with an appropriate value. 49 Figure 16: Sketch of Program Counter Branch instructions are also I type instructions, since very often computa- tions involving immediates are involved. In a sense they’re both I type and J type instructions because do lead to the program counter getting updated with ”funky” values. However typically in branch instructions the PC is actually updated by a value (+ a certain multiple of 4 if to go ”forward”, - a certain (potentially different) multiple of 4 if to go ”backward”) 8.2.2 Registers Usually, there are 32 registers available for use - which means that the register address is 5 bits long. The register port has (usually) two read ports, one write port. 50 Figure 17: Sketch of Register File 8.2.3 R, I, J Instructions in Detail As such, R type instructions reserve a total amount of 15 bits just for registers, and tend to look as follows: {op, r1, r2, rd, shamt, f unct} where Segment Size in bits Description op 6 opcode (0 for R type) r1 5 ”first” register address r2 5 ”second” register address rd 5 destination/target register address shamt 5 shift amount (0 if it’s not a shift instruction) funct 6 addition to opcode* *: together with opcode funct indicates precisely which operation the computer is meant to perform R type instructions rely use three registers, two for inputs, one for writing. In 51 general a ”special” memory is needed with at least one read and one write port (two read ports are better because it speeds the process of reading the inputs substantially). Often two read ports are standard. Similarly, I type instructions also require two register operands: but also an immediate. There is no shifting, no funct, only: {op, r1, r2, imm} Segment Size in bits Description op 6 opcode (0 for R type) r1 5 ”first” register address r2 5 ”second” register address imm 16 immediate in 2’s complement notation Here the opcode determines the operation completely. J type instructions require only two fields: {op, addr} Segment Size in bits Description op 6 opcode (0 for R type) addr 26 address operand Jump instructions work specially because the simply update the program counter directly with 1 PC = {( PC +4) [31:28]) , addr ,2 ’ b0 } Keep in mind, the last two bits being 0 is a ”logical” consequence of memory being byte addressable. 8.2.4 Data Memory Data memory is rather uncomplicated in the sense that instruction memory stores instructions, and registers store ”quick access” to certain data - and data memory is where the rest of (potentially usable) data is located/stored. Has a single read and write port. If write enable is off, it reads address A onto RD If write enable is on, it writes data WD into address A on a rising clock edge. 52 Figure 18: Sketch of Data Memory 8.2.5 ALU The ALU performs the actual computations - the instructions, the updates to the program counter, all that good stuff. 8.2.6 Basic Lifecycle of an Instruction An instruction is fetched from from the instruction memory, which receives a signal from the program counter. The instruction signed then is split off into its constituent parts in the register file (sign extended if needed). The two resulting sources are propagated into the ALU (along with the opcode and if necessary funct signals), where the computation occurs. The result is then sent into the data memory, and if needed, back to the register file to be written. The program counter is then (usually) incremented by 4, and on the next rising clock edge, the entire process begins anew. 53 8.3 Performance Evaluation Performance - how many instructions are done? How fast are they done? There are two very important quantities within performance evaluation: the CPI (cycles per instruction) and the frequency. The CPI measures how many cycles are needed for one instruction, the fre- quency how many cycles can be executed per second. The truth is, instructions don’t always take just one cycle to complete, but usually one or more. Different instructions use different circuits - rather differ- ent parts of the main circuit - and as such, the critical path or the actual time for the computation varies - which determines the clock period, so the time from one rising (or falling) edge to the next rising edge (respectively falling). The frequency is little more than 1 clock period or time needed for one clock cycle The time taken (and thus frequency) very well may vary. In general, assume that a program consists of N instructions, needs CP I cycles for each instruction, and has a(n average) frequency f This means that N · CP I is the amount if cycles needed in total for the program - and thus N · CP I f is the time in seconds needed to execute the entire program. There’s also a measure MIPS (million instructions per second) which are com- puted by f CP I · 106 since it computes the frequency times the instructions per cycle, meaning the ”per cycle” gets canceled out, and only the measure of instructions per second is left - which then only needs to be divided by one million in order to obtain the million instructions per second. In case only the instructions per cycle (IP C) are provided, the CP I are ob- tained by 1 IP C (and similar). In case only the time of the clock cycle / (maximum) clock period (T ) is pro- vided, the frequency f is obtained by f := 1 T (and similar). 54 9 Pipelining 9.1 The Stages The stages in a pipeline are F(etch), D(ecode), E(xecute), M(emory), and W(rite-back). Usually (with exception of the execute stage) each stage takes one cycle. More detail about them coming up just about 9.1.1 FETCH The fetch stage fetches whatever the current instruction is (supposed to be) from the instruction memory. 9.1.2 DECODE The decode stage actually decodes what the inputs are. Word to the wise: usually there are two register read ports, but sometimes (in very rare cases) there is only one register read port, which means that each register address has to be accessed separately, instead of in parallel, meaning that the decode cycle may potentially take two cycles. However when only one register read port is needed, immediates needn’t be decoded, since they’re practically directly available to the rest of the circuit at large. 9.1.3 EXECUTE The execute stage is actually performed by the ALU - at least all those arith- metic operations, the occasional branch, and so on, are actually performed in the ALU. However, in some operations, such as the load word or store word operation, this stage is actually more boilerplate than functional. 9.1.4 MEMORY Usually this stage is boilerplate, as it reads from (or writes to) the rest of the memory, which is stored outside of the registers. The only operations (that were covered here) that do actually require this stage are store word (which is practically finished in the memory stage) and load word (which practically begins doing anything useful in the memory stage). While it may seem that ”only” two operations ”actually” require the memory stage, it is important to keep in mind that store and load word are crucial to any complex system actually working: all memory matters but not all memory can fit into a measly 32 slots of 32 bit memory (the registers). 9.1.5 WRITE-BACK In the write back stage, the result is finally written into the target register (using the register file), with the only exception being the store word instruction, which practically terminates in the memory stage - there it also is boilerplate. 55 9.1.6 Measuring of Latency Usually the latency of an operation (when broken down in its pipeline stages) is measured in the number of cycles which are spent ”executing” - assume an addition takes 2 execute cycles (in total taking 1 fetch, 1 decode, 2 execute, 1 memory, and 1 write-back, so 6 cycles) - it is then measured as taking 2 cycles (instead of 6). Similarly, if a subtraction takes 3 cycles in the execute stage, it too is usually said as having a latency of 3 cycles as opposed to 7. If a multiplication requires 5 execute cycles, its latency is typically measured as 5 instead of 9, and so on. Keep in mind: most operations only require 1 execute cycle - in which case, the latency is 1 cycle. Of course, some measure the latency from beginning of fetch stage to end of write-back stage. In either way of latency-measuring, stalls and so on are (once again, usually) NOT considered as part of the latency. 9.1.7 Usual Pipeline Structure Assuming no dependencies, all operations needing 5 cycles from fetch to write- back (both included), and the entire program begins at cycle 1, the first oper- ation begins fetching in cycle 1, and finishes write-back in cycle 5. The second operation begins fetching in cycle 2, finishes write-back in cycle 6. Some nth operation then begins fetching in cycle n, finishes write-back in n + 4, with each stage following sequentially. m m + 1 m + 2 m + 3 m + 4 m + 5 m + 6 m + 7 m + 8 OP n F D E M W OP n + 1 F D E M W OP n + 2 F D E M W OP n + 3 F D E M W OP n + 4 F D E M W 9.2 Special Features Most processors have special features in hardware that may allow to smooth over difficulties that appear in code (without bothering the poor software and compiler). 9.2.1 Scoreboarding Scoreboarding practically allows the processor to keep track of which operations are dependant (and how). 56 9.2.2 Data Forwarding When an instruction (B) is dependant on another (A), A may occasionally forward the result from one stage (usually once execute or memory has finished) to B, typically to the execute stage. For instance, if A’s result becomes available in cycle n, B may access it directly in cycle n + 1 Usually arithmetic instructions are finished once the execute stage concludes (since by then the ALU has finished working), so if the final cycle of the execute stage is in cycle m, the data (usually) becomes available in cycle m (and thus accessible in cycle m + 1). Instructions such as store word only have their data available in the same cycle as memory (and thus accessible in the next one) - whereas load word only concludes in the write-back stage (and has nothing available prior to the memory stage) meaning that its data (usually) becomes available in the cycle of its write-back stage, and thus accessibly in the very next cycle. Keep in mind, that usually data is forwarded to the execute stage - the fetch can work perfectly fine without knowing anything else about the operation (it only involves reading from the program counter anyway) and the decode usually works fine as well, considering that the address of the register(s) has to be decoded first anyway. 9.2.3 Internal Register File Data Forwarding Data Forwarding Requiem: Internal Register File Data Forwarding is a special feature of a processor in which data forwarding can occur internally in the register file - in other words, when data forwarding occurs, it means that data that becomes available in cycle n is also accessible in cycle n, as opposed to boring old regular data forwarding in which data available from cycle n is only accessible from cycle n + 1 9.3 Hazards and How to Deal With Them The two main hazards that occur in pipelining are control and data hazards. 9.3.1 Control Hazards Control hazards occur when the next instruction is not yet determined (think branches, those take time to compute as well, and prior to the computation being finished, the program counter can not be updated reliably, causing incredibly LONG stalls). This is usually solved with branch prediction (though not with 100% accuracy). 9.3.2 Data Hazards Data hazards occur when there are dependencies of data. 57 9.3.3 Data Hazards - Forwarding One way of dealing with those is forwarding, which has already been discussed in great detail here. 9.3.4 Data Hazards - Reordering Occasionally, compilers are smart enough to not only detect where there are dependencies: but also where there aren’t, which means that some instructions may be reordered. Keyword: some, as it is important to keep the program doing the same exact thing, meaning that the final result of the program need be the same, along with control flow and similar, both before and after reordering. So it is imperative to reorder with great care. 9.3.5 Data Hazards - Stalling Less sophisticated is stalling (sometimes called interlocking): assuming that an operation (B) requires the result of a previous operation (A), it’s perfectly fine for B to be already decoding as A is still executing, but certainly not executing. Therefore B may end up getting stalled until A’s result is finally accessible, meaning that the decode ”unit” is busy busy-waiting and waiting. Assuming now that there is another operation (C), that comes right after B, it may fetch while B is already decoding, but it has to wait for B to begin executing (and thus the decode ”unit” freeing up) prior to being able to finally to do any decoding of its own. 9.3.6 Data Hazards - NOP The absolutely least sophisticated method of overcoming data hazards is insert- ing NOPs - no operations. Absolutely nothing is done. For five cycles. These are inserted between dependent operations (even if not subsequent) in such a way that (usually) no stalling needs to occur. 9.4 (Pipelined) Performance Evaluation Usually, programs are broken down into percentages of how many types of instructions are part of the program, e.g. 0.3 R-type, 0.2 I-type arithmetic, 0.4 branching, and 0.1 memory - then usually each type is assigned a certain CPI, for instance: R-type 4 CPI, I-type 2 CPI, branching 6 CPI, memory 4 CPI. Thus the average CPI becomes: 4 · 0.3 + 2 · 0.2 + 6 · 0.4 + 4 · 0.1 so 12 + 4 + 24 + 4 10 = 44 10 = 4.4 meaning the average CPI is 4.4 Making this a little more complicated: assume instead of 6 CPI for branching 58 instruction, assume that a branch instruction takes 8 cycles when stalling, and others 2. Assuming now that branches mess up 20% of the time, the average branching CPI then becomes: 8 1 5 + 2 4 5 = 3.2 making the average CPI now 3.24 More mathematically, the average CPI is n∑ i=1 CP Ia,i · fi where there are n different types of instructions that are considered (can be a rel- atively fluid measurement, sometimes for instance loads and stores are seperate, sometimes memory is considered as one). Here fi is the frequency (measured in percent, not Hz/Hertz!) of how often the ith kind of instruction occurs within the program. Keep in mind that ∑n i=1 fi = 1 Otherwise the different kinds of operations add up to less than or more than 100%, which makes very little sense. CP Ia,i is the AVERAGE CPI of the ith kind of instruction, which becomes particularly important for instance during branches (or when an instruction has to stall due to data dependencies). Usually, CP Ia,i = m∑ k=1 fk · CP Ik,i where there are m different amounts of CPI depending on how long or what kind of stalling has to occur (see the branching example for where m = 2), CP Ik,i is the specific amount of CPI that the ith operation requires in the kth ”con- figuration”,and fk is how often this specific type of situation/”configuration” has to occur - or rather does occur. As such, the complete, hyper-mathematical formula for the average CPI, CP IA, is CP IA = n∑ i=1 CP Ia,i · fi = n∑ i=1 ( m∑ k=1 fk · CP Ik,i ) · fi 59","libVersion":"0.3.2","langs":""}