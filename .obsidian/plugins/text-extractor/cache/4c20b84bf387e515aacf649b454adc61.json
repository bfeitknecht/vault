{"path":"sem4/W&S/PV/cheatsheets/W&S-cheatsheet-BF.pdf","text":"1. Probability 1.1. Prerequisites Sigma AlgebraFor some non-empty set Ω ≠ ⌀ , 𝜎 -algebra is Σ ⊆ 𝒫 ( Ω ) containing Ω , is closed under complement and countable union for ℐ ⊆ ℕ . ⌀ , Ω ∈ Σ 𝐴 ∈ Σ ⟺ 𝐴 ∁ ∈ Σ 𝒜 = { 𝐴 𝑖 } ℐ ⊆ Σ ⟹ ⋃ 𝐴 ∈ 𝒜 𝐴 ∈ Σ It follows that 𝜎 -algebras are also closed under intersection. Easily checked examples of 𝜎 -algebras include the trivial case { ⌀ , Ω } , the simple case { ⌀ , 𝐴 , 𝐴 ∁ , Ω } , and the complete case 𝒫 ( Ω ) . Probability MeasureFunction Pr : Ω → [ 0 , 1 ] on measurable space ( Ω , Σ ) which satisfies totality and countable additivity of disjoint union for ℐ ⊆ ℕ . Pr [ Ω ] = 1 𝐴 = ∐ 𝑖 ∈ ℐ 𝐴 𝑖 ⟹ Pr [ 𝐴 ] = ∑ 𝑖 ∈ ℐ Pr [ 𝐴 𝑖 ] Monotonicity follows, 𝐴 ⊂ 𝐵 ⟹ Pr [ 𝐴 ] ≤ Pr [ 𝐵 ] . The following equalities hold for Pr [ 𝐵 ] > 0 . • Pr [ 𝐴 ∪ 𝐵 ] = Pr [ 𝐴 ] + Pr [ 𝐵 ] − Pr [ 𝐴 ∩ 𝐵 ] • Pr [ 𝐴 ∩ 𝐵 ] = Pr [ 𝐴 | 𝐵 ] Pr [ 𝐵 ] Probability SpaceTriple ( Ω , Σ , Pr ) of sample space, 𝜎 -algebra event space and probability measure. An outcome 𝜔 ∈ Ω is element of the sample space. An event 𝐴 ∈ Σ is set of outcomes. For some finite sample space the Laplace model is ( Ω , 𝒫 ( Ω ) , 𝐴 ↦ | 𝐴 | | Ω | ) . Independent Events𝐴 , 𝐵 ∈ Σ are independent, 𝐴 ⊥ 𝐵 ⟺ Pr [ 𝐴 ∩ 𝐵 ] = Pr [ 𝐴 ] Pr [ 𝐵 ] . Some (infinite) family of events ( 𝐴 𝑖 ) ℐ is independent, if every finite subset is independent. ∀ 𝒥 ⊆ ℐ . | 𝒥 | ∈ ℕ ⟹ Pr [ ⋂ 𝑗 ∈ 𝒥 𝐴 𝑗 ] = ∏ 𝑗 ∈ 𝒥 Pr [ 𝐴 𝑗 ] Conditional ProbabilityFor 𝐴 , 𝐵 ∈ Σ , probability of 𝐴 given 𝐵 defined below. Pr [ 𝐴 | 𝐵 ] = Pr [ 𝐴 ∩ 𝐵 ] Pr [ 𝐵 ] = Bayes ⎴ ⎴ ⎴ ⎴ ⎴ ⎴Pr [ 𝐵 | 𝐴 ] Pr [ 𝐴 ] Pr [ 𝐵 ] The fixed conditional Pr [   · | 𝐵 ] is a probability measure. Note that Pr [ 𝐴 | 𝐴 ] = Pr [ 𝐴 | Ω ] = 1 . 1.2. Functions Random VariableFunction 𝑋 : Ω → ℝ such that the following holds (ensures CDF). ∀ 𝑎 ∈ ℝ . { 𝜔 ∈ Ω | 𝑋 ( 𝜔 ) ≤ 𝑎 } ∈ Σ Indicator r.v. encodes the occurrence of an event, 𝟏 𝐴 ( 𝜔 ) = ⟦ 𝜔 ∈ 𝐴 ⟧ . Independent Random VariablesThe sequence of r.v.‘s 𝐗 = ( 𝑋 𝑖 ) 𝑛 is independent if the joint CDF equals the product of marginal CDFs. Abusive, but write 𝑋 ⊥ 𝑌 . ∀ 𝐱 ∈ ℝ 𝑛 . 𝐹 𝐗 ( 𝐱 ) = ∏ 𝑖 ∈ [ 𝑛 ] ∗ 𝐹 𝑋 𝑖 ( 𝑥 𝑖 ) Infinite sequence ( 𝑋 𝑖 ) ℕ of r.v.‘s independent if ∀ 𝑛 . ( 𝑋 𝑖 ) 𝑛 independent. Identically distributed if ∀ 𝑖 , 𝑗 . 𝐹 𝑋 𝑖 = 𝐹 𝑋 𝑗 . If both, ( 𝑋 𝑖 ) ℕ i.i.d. . Probability Mass Function PMF of discrete r.v. 𝑋 is function 𝑓 𝑋 : ℝ → [ 0 , 1 ] that yields probability of specific value such that the below holds. 𝑓 𝑋 ( 𝑥 ) = Pr [ 𝑋 = 𝑥 ] ∑ 𝑥 ∈ 𝑊 𝑓 𝑋 ( 𝑥 ) = 1 Probability Density Function PDF of continuous r.v. 𝑋 is non-negative function 𝑓 𝑋 : ℝ → ℝ + . Intuitively, 𝑓 𝑋 ( 𝑥 ) d 𝑥 is the probability that 𝑋 falls in [ 𝑥 , 𝑥 + d 𝑥 ] . Pr [ 𝑎 ≤ 𝑋 ≤ 𝑏 ] = Pr [ 𝑋 ≤ 𝑏 ] − Pr [ 𝑋 < 𝑎 ] = ∫ 𝑏 𝑎 𝑓 𝑋 ( 𝑥 ) d 𝑥 ∫ + ∞ − ∞ 𝑓 𝑋 ( 𝑥 ) d 𝑥 = 1 If 𝑓 𝑋 continuous at 𝑥 , then PDF is derivative of CDF 𝑓 𝑋 ( 𝑥 ) = 𝐹 ′ 𝑋 ( 𝑥 ) . Cumulative Distribution Function CDF of r.v. 𝑋 is function 𝐹 𝑋 : ℝ → [ 0 , 1 ] defined as below. 𝐹 𝑋 ( 𝑥 ) = Pr [ 𝑋 ≤ 𝑥 ] = Pr [ { 𝜔 ∈ Ω | 𝑋 ( 𝜔 ) ≤ 𝑥 } ] Some function 𝑓 is CDF if and only if it is non-decreasing , right continuous and from zero to one . ∀ 𝑥 , 𝑥 ′ . 𝑥 ≤ 𝑥 ′ ⟹ 𝑓 ( 𝑥 ) ≤ 𝑓 ( 𝑥 ′ ) ∀ 𝑥 . 𝑓 ( 𝑥 ) = lim ℎ → 0 + 𝑓 ( 𝑥 + ℎ ) lim 𝑥 → − ∞ 𝑓 ( 𝑥 ) = 0 , lim 𝑥 → + ∞ 𝑓 ( 𝑥 ) = 1 General inverse distribution is 𝐹 − 1 𝑋 ( 𝑝 ) = inf { 𝑥 ∈ ℝ | 𝐹 𝑋 ( 𝑥 ) ≥ 𝑝 } . Continuous r.v. has continuous CDF. Joint DistributionJoint CDF of 𝐗 = ( 𝑋 𝑖 ) 𝑛 defines probability that every r.v. falls into specific discrete set or continuous range. 𝐹 𝐗 ( 𝐱 ) = Pr [ 𝑋 1 ≤ 𝑥 1 , … 𝑋 𝑛 ≤ 𝑥 𝑛 ] Joint PMF for discrete r.v.‘s s.t. ∀ 𝑋 𝑖 . Pr [ 𝑋 𝑖 ∈ 𝑊 𝑖 ] = 1 with 𝑊 𝑖 ⊂ ℝ countable, then let 𝐱 ∈ ⨉ 𝑖 ∈ [ 𝑛 ] ∗ 𝑊 𝑖 . 𝑓 𝐗 ( 𝐱 ) = Pr [ 𝑋 1 = 𝑥 1 , … 𝑋 𝑛 = 𝑥 𝑛 ] Joint PDF for continuous 𝐗 exists if it yields joint CDF. 𝑓 𝐗 ( 𝐱 ) = 𝜕 𝑛 𝐹 𝐗 ( 𝐱 ) ∏ 𝑖 ∈ [ 𝑛 ] ∗ 𝜕 𝑥 𝑖 = ! ∫ 𝐱 ∈ ℝ 𝑛 𝑓 𝐗 ( 𝐱 ) ∏ 𝑖 ∈ [ 𝑛 ] ∗ d 𝑥 𝑖 Always possible to recover marginal density from joint density but only the other way if r.v.‘s are i.i.d.. Marginal DistributionFor r.v.‘s 𝐗 = ( 𝑋 𝑖 ) 𝑛 with joint PMF / joint PDF 𝑓 𝐗 , the marginal distribution of 𝑋 𝑖 is given below. 𝑓 𝑋 𝑖 ( 𝑥 ) = {{{{{ ∑ 𝐱 ∈ ℝ 𝑛 , 𝑥 𝑖 = 𝑥 𝑓 𝐗 ( 𝐱 ) 𝑋 𝑖 discrete ∫ 𝐱 ∈ ℝ 𝑛 , 𝑥 𝑖 = 𝑥 𝑓 𝐗 ( 𝐱 ) ∏ 𝑗 ∈ [ 𝑛 ] ∗ ∖ { 𝑖 } d 𝑥 𝑗 𝑋 𝑖 continuous Expectation For r.v. 𝑋 with PMF / PDF 𝑓 𝑋 , expectation 𝜇 𝑋 defines the mean as weighted average of possible outcomes. E [ 𝑋 ] = {{{{{ ∑ 𝑥 ∈ 𝑊 𝑥 𝑓 𝑋 ( 𝑥 ) 𝑋 discrete ∫ + ∞ − ∞ 𝑥 𝑓 𝑋 ( 𝑥 ) d 𝑥 𝑋 continuous • E [ 𝑎 𝑋 + 𝑌 ] = 𝑎 E [ 𝑋 ] + E [ 𝑌 ] , linearity • E [ 𝑋 𝑌 ] = E [ 𝑋 ] E [ 𝑌 ] + Cov [ 𝑋 , 𝑌 ] , multiplicity • Pr [ 𝑋 ≤ 𝑌 ] = 1 ⟹ E [ 𝑋 ] ≤ E [ 𝑌 ] , monotonicity For function 𝜑 : ℝ → ℝ the following holds. E [ 𝜑 ( 𝑋 ) ] = { ∑ 𝑥 ∈ 𝑊 𝜑 ( 𝑥 ) 𝑓 𝑋 ( 𝑥 ) 𝑋 discrete ∫ ∞ − ∞ 𝜑 ( 𝑥 ) 𝑓 𝑋 ( 𝑥 ) d 𝑥 𝑋 continuous VarianceFor r.v. 𝑋 with E [ 𝑋 2 ] < + ∞ , variance 𝜎 2 𝑋 defines the spread around the mean 𝜇 𝑋 as positive function Var : Σ → ℝ + . Var [ 𝑋 ] = E [ ( 𝑋 − 𝜇 𝑋 ) 2 ] = E [ 𝑋 2 ] − E [ 𝑋 ] 2 • Var [ 𝑋 + 𝑎 ] = Var [ 𝑋 ] , translation invariant • Var [ 𝑎 𝑋 ] = 𝑎 2 Var [ 𝑋 ] , quadraticity • Var [ 𝑎 𝑋 ± 𝑏 𝑌 ] = 𝑎 2 Var [ 𝑋 ] + 𝑏 2 Var [ 𝑌 ] ± 2 𝑎 𝑏 Cov [ 𝑋 , 𝑌 ] , multiplicity • For ( 𝑋 𝑖 ) 𝑛 pairwise independent, Var [ ∑ 𝑖 ∈ [ 𝑛 ] ∗ 𝑋 𝑖 ] = ∑ 𝑖 ∈ [ 𝑛 ] ∗ Var [ 𝑋 𝑖 ] 1 CovarianceFor r.v.‘s 𝑋 , 𝑌 with 𝜇 𝑋 , 𝜇 𝑌 the covariance measures co-movement. Cov [ 𝑋 , 𝑌 ] = E [ ( 𝑋 − 𝜇 𝑋 ) ( 𝑌 − 𝜇 𝑌 ) ] = E [ 𝑋 𝑌 ] − E [ 𝑋 ] E [ 𝑌 ] Reflexive covariance equals variance, Cov [ 𝑋 , 𝑋 ] = Var [ 𝑋 ] . Exhibits bilinearity, Cov [ 𝑎 𝑋 + 𝑏 𝑌 , 𝑍 ] = 𝑎 Cov [ 𝑋 , 𝑍 ] + 𝑏 Cov [ 𝑌 , 𝑍 ] . 𝑋 ⊥ 𝑌 ⟹ Cov [ 𝑋 , 𝑌 ] = 0 but not the other way. 1.3. Asymptotic Behavior Let ( 𝑋 𝑖 ) ℕ i.i.d. . Then define 𝑆 𝑛 = ∑ 𝑖 ∈ [ 𝑛 ] ∗ 𝑋 𝑖 and 𝑋 𝑛 = 𝑆 𝑛 𝑛 . Law of Large NumbersAssume E [ 𝑋 𝑖 ] < + ∞ . The LLN states that in the limit the sample mean equals the true mean almost surely . Pr [ lim 𝑛 → + ∞ 𝑋 𝑛 = 𝜇 𝑋 𝑖 ] = 1 Central Limit TheoremAssume E [ 𝑋 2 𝑖 ] < + ∞ . Note that E [ 𝑆 𝑛 ] = 𝑛 𝜇 and Var [ 𝑆 𝑛 ] = 𝑛 𝜎 2 . The CLT states that in the limit the sum of normalized samplesresembles the standard normal distribution. Pr [ lim 𝑛 → + ∞ 𝑆 𝑛 − 𝑛 𝜇 √ 𝑛 𝜎 2 ≤ 𝑥 ] = Φ ( 𝑥 ) 1.4. Distributions 1.4.1. Discrete BernoulliExperiment with single binary outcome and parameter 𝑝 . Notation 𝑋 ∼ Ber ( 𝑝 ) Parameters 𝑝 ∈ [ 0 , 1 ] Support 𝑘 ∈ { 0 , 1 } PMF 𝑓 𝑋 ( 𝑘 ) = { 𝑝 𝑘 = 1 1 − 𝑝 𝑘 = 0 CDF 𝐹 𝑋 ( 𝑘 ) = {{{{{ 0 𝑘 < 0 1 − 𝑝 0 ≤ 𝑘 < 1 1 𝑘 ≥ 1 Mean E [ 𝑋 ] = 𝑝 Variance Var [ 𝑋 ] = 𝑝 ( 1 − 𝑝 ) BinomialRepeat Bernoulli 𝑛 times with same parameter 𝑝 . Notation 𝑋 ∼ Bin ( 𝑛 , 𝑝 ) Parameters 𝑛 ∈ ℕ , 𝑝 ∈ [ 0 , 1 ] Support 𝑘 ∈ [ 𝑛 ] PMF 𝑓 𝑋 ( 𝑘 ) = ( 𝑛𝑘 ) 𝑝 𝑘 ( 1 − 𝑝 ) 𝑘 − 1 CDF 𝐹 𝑋 ( 𝑘 ) = ∑ 𝑖 ∈ [ 𝑘 ] ( 𝑛 𝑖 ) 𝑝 𝑖 ( 1 − 𝑝 ) 𝑖 − 1 Mean E [ 𝑋 ] = 𝑛 𝑝 Variance Var [ 𝑋 ] = 𝑛 𝑝 ( 1 − 𝑝 ) GeometricNumber of failures until success with parameter 𝑝 . Notation 𝑋 ∼ Geom ( 𝑝 ) Parameters 𝑝 ∈ [ 0 , 1 ] Support 𝑘 ∈ ℕ ∗ PMF 𝑓 𝑋 ( 𝑘 ) = 𝑝 ( 1 − 𝑝 ) 𝑘 − 1 CDF 𝐹 𝑋 ( 𝑘 ) = { 1 − ( 1 − 𝑝 ) 𝑘 𝑘 ≥ 1 0 𝑘 < 1 Mean E [ 𝑋 ] = 1 𝑝 Variance Var [ 𝑋 ] = 1 − 𝑝 𝑝 2 For ( 𝑋 𝑖 ) 𝑛 ∼ i.i.d. Ber ( 𝑝 ) r.v. min { 𝑖 ∈ ℕ | 𝑋 𝑖 = 1 } = 𝑇 ∼ Geom ( 𝑝 ) . PoissonHigh number of unlikely outcomes at rate parameter 𝜆 . Notation 𝑋 ∼ Pois ( 𝜆 ) Parameters 𝜆 ∈ ( 0 , + ∞ ) Support 𝑘 ∈ ℕ PMF 𝑓 𝑋 ( 𝑘 ) = 𝜆 𝑘 𝑒 − 𝜆 𝑘 ! CDF 𝐹 𝑋 ( 𝑘 ) = 𝑒 − 𝜆 ∑ 𝑖 ∈ [ 𝑘 ] 𝜆 𝑖 𝑖 ! Mean E [ 𝑋 ] = 𝜆 Variance Var [ 𝑋 ] = 𝜆 Equals the limit of Binomial with fixed scaled parameter. lim 𝑛 → + ∞ Bin ( 𝑛 , 𝜆 𝑛 ) = Pois ( 𝜆 ) 1.4.2. Continuous UniformEverything is equally likely Notation 𝑋 ∼ 𝒰 ( [ 𝑎 , 𝑏 ] ) Parameters 𝑎 ∈ ℝ , 𝑏 ∈ ℝ > 𝑎 Support 𝑥 ∈ ℝ PDF 𝑓 𝑋 ( 𝑥 ) = { 1 𝑏 − 𝑎 𝑥 ∈ [ 𝑎 , 𝑏 ] 0 𝑥 ∉ [ 𝑎 , 𝑏 ] CDF 𝐹 𝑋 ( 𝑥 ) = {{{{{ 0 𝑥 < 𝑎 𝑥 − 𝑎 𝑏 − 𝑎 𝑎 ≤ 𝑥 < 𝑏 1 𝑥 ≥ 𝑏 Mean E [ 𝑋 ] = 𝑎 + 𝑏 2 Variance Var [ 𝑋 ] = ( 𝑏 − 𝑎 ) 2 1 2 NormalEverything is normal. Notation 𝑋 ∼ 𝒩 ( 𝜇 , 𝜎 2 ) Parameters 𝜇 ∈ ℝ , 𝜎 2 ∈ ℝ + Support 𝑥 ∈ ℝ PDF 𝑓 𝑋 ( 𝑥 ) = 𝑒 − ( 𝑥 − 𝜇 ) 2 2 𝜎 2 √ 2 𝜋 𝜎 2 CDF 𝐹 𝑋 ( 𝑥 ) = Φ ( 𝑥 ) if 𝑋 ∼ 𝒩 ( 0 , 1 ) Mean E [ 𝑋 ] = 𝜇 Variance Var [ 𝑋 ] = 𝜎 2 ExponentialWaiting time between Poisson events. Notation 𝑋 ∼ Exp ( 𝜆 ) Parameters 𝜆 ∈ ℝ ∗+ Support 𝑥 ∈ [ 0 , + ∞ ) PDF 𝑓 𝑋 ( 𝑥 ) = 𝜆 𝑒 − 𝜆 𝑥 CDF 𝐹 𝑋 ( 𝑥 ) = 1 − 𝑒 − 𝜆 𝑥 Mean E [ 𝑋 ] = 1 𝜆 Variance Var [ 𝑋 ] = 1 𝜆 2 2 2. Statistics 2.1. Estimation Let 𝐗 = ( 𝑋 𝑖 ) 𝑛 be the (i.i.d.) r.v.‘s describing some underlying model. Then 𝐱 = 𝐗 ( 𝜔 ) = ( 𝑋 𝑖 ( 𝜔 ) ) 𝑛 denotes an observation, i.e. its realization. EstimatorIs r.v. 𝑇 : Ω → ℝ for some measurable function 𝑡 : ℝ 𝑛 → ℝ which yields estimate for parameter 𝜃 ∈ Θ based the sample 𝐱 . 𝑇 = 𝑡 ( 𝐗 ) 𝑇 ( 𝜔 ) = 𝑡 ( 𝐱 ) BiasFor estimator 𝑇 of parameter 𝜃 ∈ Θ , bias is as defined below. Bias 𝜃 [ 𝑇 ] = E 𝜃 [ 𝑇 ] − 𝜃 An estimator 𝑇 is unbiased if and only if ∀ 𝜃 ∈ Θ . Bias 𝜃 [ 𝑇 ] = 0 . Mean Square ErrorFor estimator 𝑇 of parameter 𝜃 ∈ Θ , MSE is as defined below. MSE 𝜃 [ 𝑇 ] = E 𝜃 [ ( 𝑇 − 𝜃 ) 2 ] = Var 𝜃 [ 𝑇 ] − ( Bias 𝜃 [ 𝑇 ] ) 2 For unbiased estimators, the MSE is equal to the variance. Likelihood For 𝐗 i.i.d. , likelihood of sample 𝐱 with parameter 𝜃 as below. 𝐿 ( 𝐱 ; 𝜃 ) = ∏ 𝑖 ∈ [ 𝑛 ] ∗ 𝑓 𝑋 𝑖 ( 𝑥 𝑖 ; 𝜃 ) Logarithm of likelihood transforms product to sum, 𝑙 = ln 𝐿 . Maximum Likelihood EstimatorFor 𝐗 i.i.d. , MLE of 𝜃 ∈ Θ for sample 𝐱 is defined as follows. ̂𝜃 ML = arg max 𝜃 ∈ Θ 𝐿 ( 𝐱 ; 𝜃 ) = arg max 𝜃 ∈ Θ 𝑙 ( 𝐱 ; 𝜃 ) 2.2. Testing Confidence IntervalFor some parameter 𝜃 and confidence level 𝛾 = 1 − 𝛼 with 𝛼 ∈ [ 0 , 1 ] , confidence interval for 𝐗 = ( 𝑋 𝑖 ) 𝑛 is defined as the random interval 𝐼 = [ 𝐴 , 𝐵 ] with endpoints defined by functions 𝑎 , 𝑏 : ℝ 𝑛 → ℝ such that the following holds. ∀ 𝜃 ∈ Θ . Pr 𝜃 [ 𝐴 ≤ 𝜃 ≤ 𝐵 ] ≥ 𝛾 Hypothesis For parameter space Θ , hypothesis forms two exclusive classes of the underlying model. Θ 0 , Θ A ⊂ Θ Θ 0 ∩ Θ A = ⌀ The null hypothesis 𝐻 0 states that 𝜃 ∈ Θ 0 . The alternative hypothesis 𝐻 A states that 𝜃 ∈ Θ A . Simple hypothesis has cardinality one, otherwise composite hypothesis.Typically, it holds that Θ A = Θ ∖ Θ 0 . Statistical TestPair ( 𝑇 , 𝐾 ) of statistic 𝑇 = 𝑡 ( 𝐗 ) and critical region 𝐾 ⊂ ℝ . For the observation 𝜔 the test then rejects or accepts 𝐻 0 . 𝑇 ( 𝜔 ) ∈ 𝐾 ⟹ reject 𝐻 0 𝑇 ( 𝜔 ) ∉ 𝐾 ⟹ accept 𝐻 0 Type 1 error falsely rejects 𝐻 0 with Pr 𝜃 [ 𝑇 ∈ 𝐾 ] . Type 2 error falsely accepts 𝐻 0 with Pr [ 𝑇 ∉ 𝐾 ] = 1 − Pr 𝜃 [ 𝑇 ∈ 𝐾 ] . Significance LevelTest ( 𝑇 , 𝐾 ) has significance level 𝛼 ∈ [ 0 , 1 ] if the following holds. ∀ 𝜃 ∈ Θ 0 . Pr 𝜃 [ 𝑇 ∈ 𝐾 ] ≤ 𝛼 Here 𝛼 is upper bound on probability of type 1 error, minimize . PowerTest ( 𝑡 , 𝐾 ) has power 𝛽 : Θ A → [ 0 , 1 ] as defined below. 𝛽 ( 𝜃 ) = Pr 𝜃 [ 𝑇 ∈ 𝐾 ] Here 1 − 𝛽 ( Θ A ) is the probability of type 2 error, try to maximize . This asymmetry makes it harder to reject 𝐻 0 , than fail to reject it. Thus, choose 𝐻 0 as the negation of the statement to prove. Likelihood Ratio For Θ 0 = { 𝜃 0 } , Θ A = { 𝜃 A } the likelihood ratio is defined as below. 𝑅 ( 𝐱 ) = 𝐿 ( 𝐱 , 𝜃 A ) 𝐿 ( 𝐱 , 𝜃 0 ) Likelihood Ratio TestAs 𝑅 ( 𝐗 ) grows, 𝐻 A is increasingly likely compared to 𝐻 0 . 𝑇 = 𝑅 ( 𝐗 ) 𝐾 = ( 𝑐 , + ∞ ) For significance level 𝛼 = Pr 𝜃 0 [ 𝑇 > 𝑐 ] determine suitable 𝑐 ≥ 0 . This method is optimal in power for deciding between two cases 𝜃 0 , 𝜃 A with given significance level 𝛼 by the Neyman-Pearson-Lemma. Ordered Family of Tests Family of tests ( 𝑇 , ( 𝐾 𝑡 ) 𝑘 ≥ 0 ) can be ordered with respect to 𝑇 . ∀ 𝑖 , 𝑗 ∈ ℝ + . 𝑖 ≤ 𝑗 ⟹ 𝐾 𝑖 ⊃ . 𝐾 𝑗 If the 𝑡 -th test rejects 𝐻 0 , then all previous tests also reject it. p-valueFor simple null hypothesis 𝐻 0 : 𝜃 = 𝜃 0 the p-value is r.v. as below. p-value = 𝐺 ( 𝑇 ) For 𝑡 = 𝑇 ( 𝜔 ) the p-value 𝐺 ( 𝑡 ) = Pr 𝜃 0 [ 𝑇 ∈ 𝐾 𝑡 ] is probability of observing result at least as extreme as sample, provided 𝐻 0 holds. Alternatively, lowest significance level that allows rejection of 𝐻 0 . Chi-Squared DistributionThe r.v. 𝑋 ∼ 𝜒 2𝑚 in 𝑚 degrees of freedom with density given below. 𝑓 𝑋 ( 𝑥 ) = 𝑥 𝑚 2 − 1 𝑒 − 𝑥2 2 𝑚 2 Γ ( 𝑚 2 ) ∀ 𝑛 ∈ ℕ . Γ ( 𝑛 ) = ( 𝑛 − 1 ) ! . If you are using this, good luck! Sum of SquaresLet ( 𝑋 𝑖 ) 𝑚 ∼ i.i.d. 𝒩 ( 0 , 1 ) then their squared sum is chi-squared. ∑ 𝑖 ∈ [ 𝑚 ] 𝑋 2 𝑖 = 𝑌 ∼ 𝜒 2𝑚 t-distributionThe r.v. 𝑋 ∼ 𝑡 𝑚 in 𝑚 degrees of freedom with density given below. 𝑓 𝑋 ( 𝑥 ) = Γ ( 𝑚 + 1 2 ) √ 𝑚 𝜋 Γ ( 𝑚 2 ) ( 1 + 𝑥 2 𝑚 ) − 𝑚 + 1 2 3 3. Utility 3.1. Notational Conventions • ℝ + = [ 0 , + ∞ ) • ℝ ∗+ = ( 0 , + ∞ ) • ℕ = { 0 , 1 , … } • ℕ ∗ = ℕ ∖ { 0 } • [ 𝑛 ] = { 𝑘 ∈ ℕ ∗ | 𝑘 ≤ 𝑛 } = { 1 , … , 𝑛 } • [ 𝑛 ] ∗ = [ 𝑛 ] ∖ { 0 } 3.2. Tables • ∑ 𝑖 ∈ [ 𝑛 ] ∗ 𝑖 = 𝑛 𝑛 + 1 2 • ∑ 𝑖 ∈ [ 𝑛 ] ∗ 𝑖 2 = 𝑛 ( 𝑛 + 1 ) ( 2 𝑛 + 1 ) 6 • ∑ 𝑖 ∈ [ 𝑛 ] ∗ 𝑖 3 = 𝑛 2 ( 𝑛 + 1 ) 2 4 From 𝑛 take 𝑘 With Repetition Without Repetition With Order 𝑛 𝑘 𝑛 ! ( 𝑛 − 𝑘 ) ! Without Order ( 𝑛 + 𝑘 − 1 𝑘 ) ( 𝑛𝑘 ) = 𝑛 ! 𝑛 ! ( 𝑛 − 𝑘 ) ! 𝐹 ( 𝑥 ) 𝐹 ′ ( 𝑥 ) 𝐶 0 𝑥 𝑛 , 𝑛 ≠ − 1 𝑛 𝑥 𝑛 − 1 𝑥 𝑛 + 1 𝑛 + 1 𝑥 𝑛 , 𝑛 ≠ − 1 ( 𝑎 𝑥 + 𝑏 ) 𝑛 + 1 𝑎 ( 𝑛 + 1 ) ( 𝑎 𝑥 + 𝑏 ) 𝑛 √ 𝑥 1 2 √ 𝑥 𝑛√ 𝑥 = 𝑥 1 𝑛 𝑥 1 𝑛 − 1 𝑛 2 𝑥 32 3 √ 𝑥 𝑛 𝑥 1 𝑛 + 1 𝑛 + 1 𝑛√ 𝑥 𝑒 𝑥 𝑒 𝑥 𝑎 𝑏 𝑥 𝑎 𝑏 𝑥 𝑏 ln 𝑎 𝑎 𝑏 𝑥 𝑏 ln 𝑎 𝑎 𝑏 𝑥 ln | 𝑥 | 1 𝑥 𝑥 ln | 𝑥 | − 𝑥 ln | 𝑥 | log 𝑎 | 𝑥 | 1 𝑥 ln 𝑎 = log 𝑎 𝑒 𝑥 sin 𝑥 cos 𝑥 cos 𝑥 − sin 𝑥 𝑐 𝑓 ( 𝑥 ) 𝑐 𝑓 ′ ( 𝑥 ) 𝑓 ( 𝑥 ) + 𝑔 ( 𝑥 ) 𝑓 ′ ( 𝑥 ) + 𝑔 ( 𝑥 ) ′ 𝑓 ( 𝑥 ) 𝑔 ( 𝑥 ) 𝑓 ′ ( 𝑥 ) 𝑔 ( 𝑥 ) + 𝑓 ( 𝑥 ) 𝑔 ′ ( 𝑥 ) ′ 𝑓 ( 𝑥 ) 𝑔 ( 𝑥 ) 𝑓 ′ ( 𝑥 ) 𝑔 ( 𝑥 ) − 𝑓 ( 𝑥 ) 𝑔 ′ ( 𝑥 ) ′ 𝑔 ( 𝑥 ) 2 𝑓 ( 𝑔 ( 𝑥 ) ) 𝑓 ′ ( 𝑔 ( 𝑥 ) ) 𝑔 ′ ( 𝑥 ) Integration by Substitution𝑢 = 𝑔 ( 𝑥 ) ⟹ d 𝑢 = 𝑔 ′ ( 𝑥 ) d 𝑥 ∫ 𝑎 𝑏 𝑓 ( 𝑔 ( 𝑥 ) ) 𝑔 ′ ( 𝑥 ) d 𝑥 = ∫ 𝑔 ( 𝑎 ) 𝑔 ( 𝑏 ) 𝑓 ( 𝑢 ) d 𝑢 Integration by Parts ∫ 𝑓 ( 𝑥 ) 𝑔 ′ ( 𝑥 ) d 𝑥 = 𝑓 ( 𝑥 ) 𝑔 ( 𝑥 ) − ∫ 𝑓 ′ ( 𝑥 ) 𝑔 ( 𝑥 ) d 𝑥 DI Method Compute ∫ 𝑎 ( 𝑥 ) 𝑏 ( 𝑥 ) d 𝑥 . 1. Choose functions 𝑓 ( 𝑥 ) , 𝑔 ( 𝑥 ) for differentiation and integration • ∫ 𝑓 ( 𝑥 ) 𝑔 ( 𝑥 ) d 𝑥 = ∫ 𝑎 ( 𝑥 ) 𝑏 ( 𝑥 ) d 𝑥 2. Construct DI table ± D I 0 𝑓 ( 𝑥 ) 𝑔 ( 𝑥 ) 1 𝑓 ′ ( 𝑥 ) ∫ 𝑔 ( 𝑥 ) d 𝑥 2 𝑓 ″ ( 𝑥 ) ∬ 𝑔 ( 𝑥 ) d 𝑥 d 𝑥 𝑖 … … 𝑁 𝐷 𝑁 𝐼 𝑁 3. Stop at 𝑖 = 𝑁 if any condition holds • 𝐷 𝑁 = 0 • ∫ 𝐷 𝑁 𝐼 𝑁 d 𝑥 is easy • ∫ 𝐷 𝑁 𝐼 𝑁 d 𝑥 = ∫ 𝑎 ( 𝑥 ) 𝑏 ( 𝑥 ) d 𝑥 4. Result is telescope sum of diagonals ( ↘ ), plus final row ∫ 𝑎 ( 𝑥 ) 𝑏 ( 𝑥 ) d 𝑥 = ∑ 𝑖 ∈ [ 𝑁 − 1 ] ( − 1 ) 𝑖 𝐷 𝑖 𝐼 𝑖 + 1 + ∫ 𝐷 𝑁 𝐼 𝑁 d 𝑥 Independent Distributions Approaches 𝑋 𝑖 ∼ Pois ( 𝜆 𝑖 ) ∑ 𝑖 ∈ [ 𝑛 ] ∗ 𝑋 𝑖 ∼ Pois ((( ∑ 𝑖 ∈ [ 𝑛 ] ∗ 𝜆 𝑖 ))) 𝑋 𝑖 ∼ Bin ( 𝑛 𝑖 , 𝑝 ) ∑ 𝑖 ∈ [ 𝑛 ] ∗ 𝑋 𝑖 ∼ Bin ((( ∑ 𝑖 ∈ [ 𝑛 ] ∗ 𝑛 𝑖 , 𝑝 ))) 𝑋 𝑖 ∼ 𝒩 ( 𝜇 𝑖 , 𝜎 2 𝑖 ) 𝜇 𝑍 = 𝑏 + ∑ 𝑖 ∈ [ 𝑛 ] ∗ 𝑎 𝑖 𝜇 𝑖 𝜎 𝑍 = ∑ 𝑖 ∈ [ 𝑛 ] ∗ 𝑎 2𝑖 𝜎 2 𝑖 𝑏 + ∑ 𝑖 ∈ [ 𝑛 ] ∗ 𝑎 𝑖 𝑋 𝑖 = 𝑍 ∼ 𝒩 ( 𝜇 𝑍 , 𝜎 𝑍 ) 3.3. Inequalities MarkovLet 𝑋 be non-negative and 𝑎 ∈ ℝ . Pr [ 𝑋 ≥ 𝑎 ] ≤ E [ 𝑋 ] 𝑎 ChebyshevLet E [ 𝑋 2 ] < + ∞ and 𝑎 ∈ ℝ + . Pr [ | 𝑋 − 𝜇 𝑋 | ≥ 𝑎 ] ≤ Var [ 𝑋 ] 𝑎 2 JensenLet 𝑋 be r.v. and 𝜑 : ℝ → ℝ convex . The inequality holds if defined. 𝜑 ( E [ 𝑋 ] ) ≤ E [ 𝜑 ( 𝑋 ) ] 3.4. Miscellaneous Just in case, 𝑐 = 2 9 9 ′ 7 9 2 ′ 4 5 8 𝑚 𝑠 . Inclusion Exclusion Principle Pr [ ⋃ 𝑖 ∈ [ 𝑛 ] ∗ 𝐴 𝑖 ] = ∑ 𝐽 ⊆ [ 𝑛 ] ∗ ( − 1 ) | 𝐽 | + 1 Pr [ ⋂ 𝑗 ∈ 𝐽 𝐴 𝑗 ] Binomial Theorem ( 𝑥 + 𝑦 ) 𝑛 = ∑ 𝑖 ∈ [ 𝑛 ] ( 𝑛 𝑖 ) 𝑥 𝑖 𝑦 𝑛 − 𝑖 = ∑ 𝑖 ∈ [ 𝑛 ] ( 𝑛 𝑖 ) 𝑥 𝑛 − 𝑖 𝑦 𝑖 Gamma FunctionExtension of the factorial to the reals, Γ : ℝ → ℝ as defined below. Γ ( 𝑥 ) = ∫ + ∞ 0 𝑡 𝑥 − 1 𝑒 − 𝑥 d 𝑡 Polar CoordinatesFor bivariate 𝑥 = 𝑟 cos ( 𝜃 ) , 𝑦 = 𝑟 sin ( 𝜃 ) substitution below is useful. • 𝑥 2 + 𝑦 2 = 𝑟 2 • d 𝑥 d 𝑦 = 𝑟 d 𝑟 d 𝜃 rad sin cos tan 0 0 1 0 𝜋6 12 √ 3 2 √ 3 3 𝜋4 √ 2 2 √ 2 2 1 𝜋3 √ 3 2 12 √ 3 𝜋2 1 0 N/A logarithm rules log 𝑎 ( 1 ) = 0 log 𝑎 ( 𝑥 𝑦 ) = log 𝑎 ( 𝑥 ) + log 𝑎 ( 𝑦 ) log 𝑎 ( 𝑥𝑦 ) = log 𝑎 ( 𝑥 ) − log 𝑎 ( 𝑦 ) log 𝑎 ( 𝑥 𝑟 ) = 𝑟 · log 𝑎 ( 𝑥 ) log 𝑎 ( 𝑏 ) = log 𝑐 ( 𝑏 ) log 𝑐 ( 𝑎 ) 4 4. Appendix 4.1. Recipes Standardize Normal DistributionLet 𝑋 ∼ 𝒩 ( 𝜇 , 𝜎 2 ) . Normalization yields standard normal. 𝑋 − 𝜇 𝜎 ∼ 𝒩 ( 0 , 1 ) Maximum Likelihood EstimatorFind ̂𝜃 ML for 𝐗 = ( 𝑋 𝑖 ) 𝑛 i.i.d. . 1. 𝐿 ( 𝐱 ; 𝜃 ) = ∏ 𝑖 ∈ [ 𝑛 ] ∗ 𝑓 𝑋 𝑖 ( 𝑥 𝑖 ; 𝜃 ) 2. 𝑙 ( 𝐱 ; 𝜃 ) = ln ∑ 𝑖 ∈ [ 𝑛 ] ∗ 𝑓 𝑋 𝑖 ( 𝑥 𝑖 ; 𝜃 ) = ∑ 𝑖 ∈ [ 𝑛 ] ∗ ln 𝑓 𝑋 𝑖 ( 𝑥 𝑖 ; 𝜃 ) 3. Differentiate with respect to 𝜃 4. Solve 𝑙 ′ ( 𝜃 ) = 0 for 𝜃 5. Check local maximum with 𝑙 ″ ( 𝜃 ) < 0 Confidence IntervalFind interval 𝐼 with confidence level 𝛾 = 1 − 𝛼 . 1. For symmetric distribution define 𝐼 around MLE, 𝑇 = ̂𝜃 ML • 𝐼 = [ 𝑇 − 𝑐 , 𝑇 + 𝑐 ] • 𝛾 = Pr 𝜃 [ 𝑇 − 𝑐 ≤ 𝜃 ≤ 𝑇 + 𝑐 ] 2. Rewrite to single instance of estimator 𝑇 • Pr 𝜃 [ 𝜃 − 𝑐 ≤ 𝑇 ≤ 𝜃 + 𝑐 ] 3. Convert 𝑇 to known distribution, e.g. Φ 4. Use CDF to calculate probability w.r.t. 𝑐 and set equal to 𝛾 5. Invert CDF and solve for 𝑐 Define TestFor given probability model. 1. Form hypotheses 𝐻 0 , 𝐻 A , with aim to disprove the 𝐻 0 2. Calculate statistic 𝑇 (e.g. MLE) and determine form of 𝐾 3. Set significance level 𝛾 = 1 − 𝛼 4. Solve 𝛼 = Pr 𝜃 0 [ 𝑇 ∈ 𝐾 ] for 𝐾 • Choose 𝜃 0 ∈ Θ 0 s.t. ∀ 𝜃 ∈ Θ 0 . Pr 𝜃 [ 𝑇 ∈ 𝐾 ] ≤ Pr 𝜃 0 [ 𝑇 ∈ 𝐾 ] 5. Perform the test, i.e. insert data into 𝑇 6. Decide on hypothesis and optionally determine p-value 4.1.1. Suitable Tests for Hypotheses at Significance Level 𝛼 Let 𝐗 = ( 𝑋 𝑖 ) 𝑛 i.i.d. with E [ 𝑋 𝑖 ] < + ∞ and 𝜇 = E [ 𝑋 𝑖 ] , 𝜎 2 = Var [ 𝑋 𝑖 ] . Let 𝑆 2 = 1 𝑛 − 1 ∑ 𝑖 ∈ [ 𝑛 ] ∗ ( 𝑋 𝑖 − 𝑋 ) 2 . Test Mean, Known Variance, Normal Distribution 𝐗 ∼ i.i.d. 𝒩 ( 𝜃 , 𝜎 2 ) , 𝜎 2 known. 𝐻 0 : 𝜃 = 𝜃 0 𝑇 = 𝑆 𝑛 − 𝑛 𝜃 0√ 𝑛 𝜎 2 ∼ Pr 𝜃 0 𝒩 ( 0 , 1 ) 𝐻 A : 𝜃 A > 𝜃 0 ⟹ 𝐾 = ( 𝑐 ← , + ∞ ) , 𝑐 ← = Φ − 1 ( 1 − 𝛼 ) = 𝑧 1 − 𝛼 𝐻 A : 𝜃 A < 𝜃 0 ⟹ 𝐾 = ( − ∞ , 𝑐 → ) , 𝑐 → = Φ − 1 ( 𝛼 ) 𝐻 A : 𝜃 A ≠ 𝜃 0 ⟹ 𝐾 = ( − ∞ , − 𝑐 ↔ ) ∪ ( 𝑐 ↔ , + ∞ ) , 𝑐 ↔ = Φ − 1 ( 1 − 𝛼2 ) Test Mean, Unknown Variance, Normal Distribution 𝐗 ∼ i.i.d. 𝒩 ( 𝜇 , 𝜎 2 ) . 𝐻 0 : 𝜃 = 𝜃 0 ∈ { 𝜇 0 } × ℝ ∗+ 𝑇 = 𝑆 𝑛 − 𝑛 𝜇 0√ 𝑛 𝑆 2 ∼ 𝑃 𝜃 0 𝑡 𝑛 − 1 𝐻 A : 𝜇 A > 𝜇 0 ⟹ 𝐾 = ( 𝑐 ← , + ∞ ) , 𝑐 ← = 𝑡 𝑛 − 1 , 1 − 𝛼 𝐻 A : 𝜇 A < 𝜇 0 ⟹ 𝐾 = ( − ∞ , 𝑐 → ) , 𝑐 → = 𝑡 𝑛 − 1 , 𝛼 𝐻 A : 𝜇 A ≠ 𝜇 0 ⟹ 𝐾 = ( − ∞ , − 𝑐 ↔ ) ∪ ( 𝑐 ↔ , + ∞ ) , 𝑐 ↔ = 𝑡 𝑛 − 1 , 1 − 𝛼 2 Test Variance, Unknown Mean, Normal Distribution 𝐗 ∼ i.i.d. 𝒩 ( 𝜇 , 𝜎 2 ) . 𝐻 0 : 𝜎 2 = 𝜎 2 0 𝑇 = ( 𝑛 − 1 ) 𝑆 2 𝑛 𝜎 2 0 ∼ 𝜒 2𝑛 − 1 𝐻 A : 𝜇 A > 𝜇 0 ⟹ 𝐾 = ( 𝑐 ← , + ∞ ) , 𝑐 ← = 𝜒 2𝑛 − 1 , 1 − 𝛼 𝐻 A : 𝜇 A < 𝜇 0 ⟹ 𝐾 = ( − ∞ , 𝑐 → ) , 𝑐 → = 𝜒 2𝑛 − 1 , 𝛼 𝐻 A : 𝜇 A ≠ 𝜇 0 ⟹ 𝐾 = ( − ∞ , − 𝑐 ↔ ) ∪ ( 𝑐 ↔ , + ∞ ) , 𝑐 ↔ = 𝜒 2𝑛 − 1 , 1 − 𝛼 2 Test Mean, Known Variance, Unknown Distribution 𝐗 i.i.d. , 𝜎 2 known. 𝐻 0 : 𝜃 = 𝜃 0 𝑇 = 𝑆 𝑛 − 𝑛 𝜃 0√ 𝑛 𝜎 2 ∼ 𝑃 𝜃 0 𝒩 ( 0 , 1 ) by the CLT 𝐻 A : 𝜇 A > 𝜇 0 ⟹ 𝐾 = ( 𝑐 ← , + ∞ ) , 𝑐 ← = 𝑧 1 − 𝛼 𝐻 A : 𝜇 A < 𝜇 0 ⟹ 𝐾 = ( − ∞ , 𝑐 → ) , 𝑐 → = 𝑧 𝛼 𝐻 A : 𝜇 A ≠ 𝜇 0 ⟹ 𝐾 = ( − ∞ , − 𝑐 ↔ ) ∪ ( 𝑐 ↔ , + ∞ ) , 𝑐 ↔ = 𝑧 1 − 𝛼 2 Test Mean, Unknown Variance, Unknown Distribution 𝐗 i.i.d. , 𝜎 2 unknown. 𝐻 0 : 𝜃 = 𝜃 0 ∈ { 𝜇 0 } × ℝ ∗+ 𝑇 = 𝑆 𝑛 − 𝑛 𝜇 0√ 𝑛 𝑆 2 ∼ 𝑃 𝜃 0 𝒩 ( 0 , 1 ) by CLT 𝐻 A : 𝜇 A > 𝜇 0 ⟹ 𝐾 = ( 𝑐 ← , + ∞ ) , 𝑐 ← = 𝑧 1 − 𝛼 𝐻 A : 𝜇 A < 𝜇 0 ⟹ 𝐾 = ( − ∞ , 𝑐 → ) , 𝑐 → = 𝑧 𝛼 𝐻 A : 𝜇 A ≠ 𝜇 0 ⟹ 𝐾 = ( − ∞ , − 𝑐 ↔ ) ∪ ( 𝑐 ↔ , + ∞ ) , 𝑐 ↔ = 𝑧 1 − 𝛼 2 4.2. Examples Common EstimatorsLet 𝐗 = ( 𝑋 𝑖 ) 𝑛 and 𝑋 = 1 𝑛 ∑ 𝑖 ∈ [ 𝑛 ] ∗ 𝑋 𝑖 . Bernoulli ̂𝑝 = 𝑋 Binomial ̂𝑝 = 𝑋 𝑚 for 𝐗 ∼ i.i.d. Bin ( 𝑝 , 𝑚 ) with 𝑚 known Poisson ̂𝜆 = 𝑋 Normal ̂𝜇 = 𝑋 ̂𝜎 2 = {{{{{ 1 𝑛 ∑ 𝑖 ∈ [ 𝑛 ] ∗ ( 𝑋 𝑖 − ̂𝜇 ) 2 biased MLE 1 𝑛 − 1 ∑ 𝑖 ∈ [ 𝑛 ] ∗ ( 𝑋 𝑖 − ̂𝜇 ) 2 unbiased Exponential ̂𝜆 = 1 𝑋 Uniform ̂𝑎 = min ( 𝐗 ) ̂𝑏 = max ( 𝐗 ) 56","libVersion":"0.5.0","langs":""}