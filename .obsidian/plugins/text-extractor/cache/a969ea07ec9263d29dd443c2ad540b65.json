{"path":"sem3/LinAlg/UE/s/LinAlg-s-u12.pdf","text":"D-INFK Linear Algebra HS 2024 Bernd G¨artner Robert Weismantel Solution for Assignment 12 1. a) One could solve this by first computing A and then computing its real eigenvalues. But in this case, it is not hard to guess eigenvectors of A. In particular, choosing x = y = 1 yields A [ 1 1 ] = [ 1 1 ] and hence 1 is an eigenvalue of A. Moreover, guessing x = 1 and y = −1 yields A [ −1 1 ] = [ 1 −1 ] and hence −1 is an eigenvalue of A. We conclude that the two real eigenvalues of A are 1 and −1. Since A is a 2 × 2 matrix, there cannot be more thant 2 eigenvalues. b) The simplest choice is the diagonal matrix A =   0 0 0 0 1 0 0 0 2   . Indeed, we have Ae1 = 0, Ae2 = e2, and Ae3 = 2e3. Hence, A has the desired eigenvalues. It does not have any other eigenvalues because a 3 × 3 matrix can have at most 3 eigenvalues. c) Consider the matrix A from the previous subtask and the basis v1 = e1 + e2, v2 = e1 − e2, v3 = e3 of R3. In particular, consider the matrix V =   | | | v1 v2 v3 | | |   =   1 1 0 1 −1 0 0 0 1   with inverse V −1 =   1 2 1 2 0 1 2 − 1 2 0 0 0 1   and define B as B = V AV −1 =   1 2 − 1 2 0 1 2 1 2 0 0 0 2   . The matrices A and B are similar and hence have the same eigenvalues. 2. a) Let v be an eigenvector of AB corresponding to eigenvalue λ. Observe that (BA)Bv = B(AB)v = Bλv = λBv. Hence, if Bv ̸= 0 then Bv is an eigenvector of BA with corresponding eigenvalue λ. Otherwise, we must have Bv = 0 and hence λ = 0. But this implies that B is not full rank which also means that BA is not full rank. Thus, λ = 0 must be an eigenvalue of BA. 1 b) Let v1, . . . , vn ∈ Rn be a complete set of real eigenvectors of AB. By subtask a) and the additional assumption that B is invertible, we know that the vectors Bv1, . . . , Bvn ∈ Rn are all real eigenvectors of BA. Moreover, the n vectors Bv1, . . . , Bvn are linearly independent since B is invertible and because the vectors v1, . . . , vn are linearly independent. Hence, Bv1, . . . , Bvn form a basis of Rn and therefore they are a complete set of real eigenvectors of BA. c) From subtask b), we know that if AB has a complete set of real eigenvectors, then so does BA. Using subtask b) again with matrices A and B exchanged (we can do this because here A is invertible as well), we also get that if BA has a complete set of real eigenvectors, then so does AB. This proves the claim. d) Consider the matrices A = [1 0 0 0 ] and B = [0 1 0 0 ] with AB = [0 1 0 0 ] and BA = [ 0 0 0 0 ] . The matrix BA has a complete set of real eigenvectors because N(BA) = R2. However, the matrix AB does not have a complete set of real eigenvectors: the eigenvalue 0 appears with algebraic multiplicity 2, but AB only has rank 1 and hence dim(N(AB)) = 1. In other words, the geometric multiplicity of eigenvalue 0 is only 1. We conclude that there is no complete set of real eigenvectors for AB. One way to attempt to find such an example is to start with a matrix that is not diagonalizable such as C = [ 0 1 0 0 ]. Then, introduce variables for the entries of A, B and write down the equations that need to be satisfied such that AB = C. Finally, we also know that an easy matrix that is diagonalizable is the zero-matrix 0, so we also want BA = 0. Now try to solve for the entries of A and B. 3. By definition of the sequence, we get the relation [ −1 6 1 0 ] [ an−1 an−2 ] = [ an an−1 ] for all n ≥ 2. We start by finding the eigenvalues of the matrix A = [ −1 6 1 0 ] . The characteristic polynomial p(λ) of A is given by p(λ) = ∣ ∣ ∣ ∣−1 − λ 6 1 −λ ∣ ∣ ∣ ∣ = (−1 − λ)(−λ) − 6 = λ 2 + λ − 6 = (λ − 2)(λ + 3) and hence has roots λ1 = 2 and λ2 = −3. By solving the linear systems (A − λ1I)x = 0 and (A − λ2I)x = 0 we find the two corresponding eigenvectors (you might also be able to guess them) v1 = [2 1 ] and v2 = [ 3 −1 ] . 2 Now define an = [ an an−1 ] for n ≥ 1 and observe that a1 = [a1 a0 ] = [1 1 ] = 4 5 v1 − 1 5 v2. Hence, we get an = An−1a1 = An−1 4 5 v1 − An−1 1 5 v2 = 4 5 λ n−1 1 v1 − 1 5 λn−1 2 v2 = [2n−1 4 5 2 − (−3)n−1 1 5 3 2n−1 4 5 + (−3)n−1 1 5 ] for all n ≥ 1. We conclude that we have an = 2 n−1 4 5 2 − (−3) n−1 1 5 3 = 4 5 2n + 1 5 (−3) n. for all n ∈ N0 (it is easy to check that it actually holds for n = 0 as well). 4. a) We know that A and B are similar. Hence, there exists an invertible matrix S with B = S−1AS. Recall that we must have det(S−1)det(S) = 1. Therefore, we get det(A−zI) = det(S−1)det(A−zI)det(S) = det(S−1(A−zI)S) = det(B−zS−1IS) = det(B−zI), as desired. b) Since λ1, . . . , λn are distinct, we know from Proposition 7.1.8 that any n corresponding real eigenvectors v1, . . . , vn ∈ Rn of A are linearly independent. In other words, A has a com- plete set of real eigenvectors v1, . . . , vn. By assumption, B also has this complete set of real eigenvectors v1, . . . , vn. Let V be the matrix with columns v1, . . . , vn. From Theorem 7.2.1 it follows that A = V ΛAV −1 for some diagonal matrix ΛA ∈ Rn×n. Analogously, it follows that B = V ΛBV −1 for some diagonal matrix ΛB ∈ Rn×n. Now observe that since both ΛA and ΛB are diagonal matrices, we have ΛAΛB = ΛBΛA. Thus, we get AB = (V ΛAV −1)(V ΛBV −1) = V ΛA(V −1V )ΛBV −1 = V ΛAΛBV −1 = V ΛBΛAV −1 = V ΛB(V −1V )ΛAV −1 = (V ΛAV −1)(V ΛBV −1) = BA which concludes the proof. c) By similarity of A and B, there exists an invertible matrix S ∈ Rn×n with A = SBS−1. By similarity of B and C, there exists an invertible matrix T ∈ Rn×n with B = T CT −1. Hence, we get A = SBS−1 = ST CT −1S−1 = P CP −1 where P = ST is invertible with inverse P −1 = T −1S−1. We conclude that A and C are similar. d) Since A has n distinct real eigenvalues, it must have a complete set of real eigenvectors and hence it must be diagonalizable (by Proposition 7.1.8 and Theorem 7.2.1), i.e. there exists an invertible matrix V ∈ Rn×n and a diagonal matrix Λ ∈ Rn×n with A = V ΛV −1. Analogously, B is diagonalizable with B = W ΛW −1 for some invertible matrix W ∈ 3 Rn×n. Note that we can assume that the diagonalization of both A and B use the same diagonal matrix Λ because A and B are assumed to have the same eigenvalues. We observe that this means that A is similar to Λ and that Λ is similar to B. By using the statement from the previous subtask for A, Λ, B, it follows that A is similar to B. e) Let λ ∈ R be an arbitrary real eigenvalue of A with corresponding eigenvector v ∈ Rn. By similarity of A and B, there exists an invertible matrix S ∈ Rn×n with B = SAS−1. Now consider the vector w = Sv. We have Bw = SAS−1(Sv) = SAv = λSv = λw and hence w is a real eigenvector of B with corresponding real eigenvalue λ. Since λ was arbitrary, we conclude that every real eigenvalue of A is also a real eigenvalue of B. By a symmetric argument (swapping the roles of A and B above) we get that every real eigenvalue of B is also a real eigenvalue of A. We conclude that A and B have the same set of real eigenvalues. 5. a) Consider the rotation Φ. Applying Φ to a vector should leave the second coordinate un- changed but rotate in the plane given by the first and third coordinate. In particular, deleting the second row and second column should give us the corresponding rotation matrix in two dimensions. Hence, the matrix A is given by A =   cos π 3 0 sin π 3 0 1 0 − sin π 3 0 cos π 3   = 1 2   1 0 √3 0 2 0 − √3 0 1   . Note that this solution assumes that the direction of the rotation is given by the right-hand rule in a right-handed coordinate system, as explained e.g. on Wikipedia1. b) The vector [ 3 4 0 ]⊤ ∈ R3 is orthogonal to the plane P . Normalizing it yields the normal vector n = [ 3 5 4 5 0 ]⊤ . Now consider an arbitrary vector x ∈ R3. We can split it into x = x⊥ + x| with x⊥, x| ∈ R3 such that x⊥ · n = 0 and x| · n = ||x||| (i.e. x| is collinear with n). In particular, we then must have x⊥ ∈ P . Hence, reflecting x on the plane P should yield Ψ (x) = −x| + x⊥. This works for any x ∈ R3 and plugging in e1, e2, and e3 for x will give you the desired matrix. To avoid this computation, let us recall that we have previously projected vectors to a plane. In particular, the projection matrix here would be I − nn⊤. From that, one might now guess that the reflection matrix for reflection on the plane P should be I − 2nn⊤ (the term −nn⊤ in the projection gets rid of the x| part, hence the term −2nn⊤ should negate the x| part). Indeed, we can check that (I − 2nn⊤)x = (I − 2nn ⊤)(x⊥ + x|) = (x⊥ + x|) − 2x| = x⊥ − x| as desired. Hence, we get B = I − 2nn⊤ = 1 25   7 −24 0 −24 −7 0 0 0 25   . 1https://en.wikipedia.org/wiki/Right-hand_rule 4 c) Both matrices are square, hence it suffices to check AA⊤ = I and BB⊤ = I. Indeed, we have AA⊤ = 1 4   1 + 3 0 0 0 4 0 0 0 3 + 1   =   1 0 0 0 1 0 0 0 1   and BB⊤ = 1 625   72 + 242 0 0 0 242 + 72 0 0 0 252   =   1 0 0 0 1 0 0 0 1   . d) As we have observed before, a rotation should leave the axis of rotation unchanged. In our case, the vector e2 ∈ R3 is on this axis of rotation. Indeed, we get Ae2 = e2. Hence, e2 is a real eigenvector of A with corresponding eigenvalue 1. As it turns out, this is the only real eigenvalue of A and all other real eigenvectors of A are multiples of e2 (i.e. all vectors on the axis of rotation). e) Consider the vector n ∈ R3 from before. As we observed, we have Ψ (n) = Bn = −n and hence n is a real eigenvector of B with corresponding eigenvalue −1. Similarly, vectors that lie in the plane P should not be affected by the reflection. Indeed, consider for example the vector v = [ −4 3 0]⊤. We have v·n = 0 and hence v ∈ P and Bv = (I −2nn⊤)v = v. Hence, v is a real eigenvector of B with corresponding eigenvalue 1. The matrix B does not have any other (distinct) eigenvalues because, as it turns out, the eigenvalue 1 appears with algebraic multiplicity two. (But it would be possible to find another real eigenvector corresponding to eigenvalue 1 that is linearly independent from v.) This corresponds to the fact that all vectors in P (a two-dimensional object) are eigenvectors of B. 6. a) Note that 0 is an eigenvalue of A. In particular, the nullspace of A is non-trivial (i.e. contains vectors other than 0). Hence, A is not full rank and not invertible. By Proposition 6.0.11, this means that det(A) = 0. b) The given vector is a multiple of v3. In particular, it is the vector 18v3. Thus, we have A(18v3) = 18(Av3) = 0 which means that the vector belongs to the nullspace of A. c) Let V be the 3 × 3 matrix with columns v1, v2, v3. Since the three vectors are orthonormal, V must be an orthogonal matrix, i.e. we have V V ⊤ = V ⊤V = I. We want to find the matrix A just from the information about its eigenvectors and eigenvalues. In particular, we know that Av1 = v1, Av2 = −v2, and Av3 = 0. We can summarize this in the equation AV = V D where D ∈ R3×3 is the diagonal matrix D =   1 0 0 0 −1 0 0 0 0   . Multiplying from the right by V −1 = V ⊤ yields A = V DV ⊤. 5 Hence, we can now calculate A as follows: A = V DV ⊤ = 1 81   1 −4 8 8 4 1 −4 7 4     1 0 0 0 −1 0 0 0 0     1 8 −4 −4 4 7 8 1 4   = 1 81   1 −4 8 8 4 1 −4 7 4     1 8 −4 4 −4 −7 0 0 0   = 1 81   −15 24 24 24 48 −60 24 −60 −33   . 7. The key insight of this exercise is to look at the real eigenvalues of A. For this, we first compute the characteristic polynomial p(λ) = ∣ ∣ ∣ ∣ ∣ ∣ −λ 1 3 1 2 −λ 0 0 1 3 −λ ∣ ∣ ∣ ∣ ∣ ∣ = −λ3 + 1 2 λ + 1 2 . We can guess one of the roots of this polynomial to be 1. Hence, we obtain p(λ) = (λ − 1)(−λ2 − λ − 1 2 ) and it turns out that the other roots of p are complex-valued. Hence, the only real eigenvalue of A is 1. Recall that our goal is to find an initial population that yields a stable population over time. The idea here is that an eigenvector corresponding to eigenvalue 1 is a suitable choice. Such an eigen- vector can be found in N(A − I) and one example would be to choose v0 = [ 6 3 1 ]⊤ . Indeed, we get Av0 =  0 1 3 1 2 0 0 0 1 3 0     6 3 1   =   6 3 1   , and hence vt = Atv0 = v0 for all t ∈ N0. We conclude that this choice of initial population yields a stable population. 6","libVersion":"0.3.2","langs":""}