{"path":"sem3/LinAlg/UE/s/LinAlg-u03-s.pdf","text":"D-INFK Linear Algebra HS 2023 Afonso Bandeira Bernd G¨artner Solution for Assignment 3 1. a) We will use the elimination procedure on A in order to get the upper triangular matrix U and the lower triangular matrix L. First, we multiply A with E21 =   1 0 0 − 1 2 1 0 0 0 1   and E31 =   1 0 0 0 1 0 −1 0 1   to get E31E21A =   2 −12 6 0 2 −2 0 1 −11   . We also note down the coefficients ℓ21 = 1 2 and ℓ31 = 1 for L. Note that these are just the negated entries of E21 and E31, respectively. Next, we multiply this with E32 =   1 0 0 0 1 0 0 − 1 2 1   and get E32E31E21A =   2 −12 6 0 2 −2 0 0 −10   =: U which is upper triangular. We also write down ℓ32 = 1 2 . From the lecture we know that we can obtain L as L =   1 0 0 ℓ21 1 0 ℓ31 ℓ32 1   =  1 0 0 1 2 1 0 1 1 2 1   . Indeed, checking LU =  1 0 0 1 2 1 0 1 1 2 1     2 −12 6 0 2 −2 0 0 −10   =   2 −12 6 1 −4 1 2 −11 −5   = A we conclude that this is a valid LU factorization of A. b) Since L is lower triangular, we can start substituting from the top. In particular, writing out the equation gives Ly =  1 0 0 1 2 1 0 1 1 2 1     y1 y2 y3   ! =   4 4 25   = b. Hence, we get y1 = 4, y2 = 4 − 2 = 2, and y3 = 25 − 1 − 4 = 20. c) We first write down the system again as U x =   2 −12 6 0 2 −2 0 0 −10     x1 x2 x3   ! =   4 2 20   = y. By using back substitution we obtain x3 = 20 −10 = −2, x2 = 2−4 2 = −1, and x1 = 4−12+12 2 = 2. d) Using the results from the previous two subtasks we get Ax LU = LU x = L(U x) c) = Ly b) = b. 2. a) To solve this exercise, we could use the elimination procedure for each system and then read off the solution. But the procedure would never actually eliminate anything (only reorder rows). In particular, the solutions to the system can be read off without using elimination as every row of A has a unique non-zero element. We first consider the system   0 0 1 1 0 0 0 1 0   x1 =   1 0 0   = e1. To get the 1 in e1, the third coordinate of x1 has to be 1. To get the two zeroes of e1, the other two coordinates of x1 have to be zero. Hence, we get x1 = e3. Analogously, we obtain x2 = e1 and x3 = e2 from the other two systems. b) Recall that the inverse A−1 of the n × n matrix A, if it exists, is the unique matrix satisfying AA−1 = I. Now recall our three linear systems from above: Ax1 = e1 Ax2 = e2 Ax3 = e3 and consider what happens when we arrange the vectors (Ax1), (Ax2), (Ax3) as columns in a new matrix   | | | Ax1 Ax2 Ax3 | | |   =   | | | e1 e2 e3 | | |   = I. Notice how this corresponds to the column view of matrix multiplication, i.e. we have A   | | | x1 x2 x3 | | |   =   | | | Ax1 Ax2 Ax3 | | |   =   | | | e1 e2 e3 | | |   = I. From this we can conclude that the matrix X =   | | | x1 x2 x3 | | |   must be the inverse of A. In particular, we have A−1 = X =   | | | x1 x2 x3 | | |   =   | | | e3 e1 e2 | | |   . c) We proceed as above by first solving the three systems Dy1 = e1, Dy2 = e2, and Dy3 = e3. Again, the solutions can be read off directly as D has a unique non-zero entry in every row. In particular, we get the solutions y1 =   1 2 0 0   , y2 =  0 1 3 0   , y3 =   0 0 2   . As above, we conclude that the matrix Y =   | | | y1 y2 y3 | | |   is the inverse of D since we have DY = D   | | | y1 y2 y3 | | |   =   | | | Dy1 Dy2 Dy3 | | |   =   | | | e1 e2 e3 | | |   = I. Concretely, we have D−1 = Y . d) Using the same strategy as above, one could now also determine the inverse of B. But the idea of this exercise is to observe that there is a faster way. In particular, we observe that B can be obtained from A and D as DA = B. From the lecture we know that B−1 can now be computed as B−1 = (DA)−1 = A−1D−1 = XY =   0 1 0 0 0 1 1 0 0     1 2 0 0 0 1 3 0 0 0 2   =  0 1 3 0 0 0 2 1 2 0 0   . 3. a) Yes, the inverse of Ak exists and is given by (A−1)k. We will argue by induction over k. • Property: The inverse of Ak is given by (A−1)k. • Base case: For k = 1, the property is true because we are given that A−1 is the inverse of A. • Induction step: Fix a natural number 1 ≤ k and assume that the property is true for this k (induction hypothesis). We prove that the property is true for k + 1, i.e. we prove that the inverse of Ak+1 is (A−1)k+1. By the induction hypothesis, we have Ak(A−1)k = I. Hence, from A k+1(A−1) k+1 = AkAA −1(A −1)k = Ak(AA−1)(A−1) k = AkI(A−1) k = Ak(A−1) k IH = I we conclude that the property is indeed true for k + 1. b) We prove this by contradiction. Assume for a contradiction that A has an inverse A−1. Since A is nilpotent, we know from Assignment 2 that AA−1 = I is nilpotent too, i.e. there exists some k ∈ N+ such that I k = (AA−1)k = 0. But we have I k = I ̸= 0 which is a contradiction. c) We prove this with the following calculation: A = AI = A(A 3) = A 4 = I. d) Observe that it suffices to find a 2 × 2 matrix A ̸= I with A2 = I: indeed, for even k = 2ℓ we then get Ak = A2ℓ = (A2)ℓ = I ℓ = I as well by using A2 = I. Such a matrix is sometimes called self-inverse, since we have A−1 = A. We have seen some self-inverse matrices before in this course. In particular, the 2 × 2 rotation matrix for angle ϕ = π is self-inverse. Concretely, this is the matrix A = [ cos(π) − sin(π) sin(π) cos(π) ] = [ −1 0 0 −1 ] and we can check that indeed A2 = I but A ̸= I. Moreover, for odd k we then have A k = AAk−1 = AI = A ̸= I by using that k − 1 must be even. e) Again, we use rotation matrices. In particular, the rotation matrix with angle π/2 should satisfy this. The intuition is that whenever we apply this rotation matrix four times, we rotate one whole turn and hence nothing happens. For a complete solution, we check that this indeed works. For the matrix A = [cos(π/2) − sin(π/2) sin(π/2) cos(π/2) ] = [0 −1 1 0 ] we get A2 = [0 −1 1 0 ]2 = [−1 0 0 −1 ] A3 = A2 [ 0 −1 1 0 ] = [ 0 1 −1 0 ] A4 = A3 [ 0 −1 1 0 ] = [ 1 0 0 1 ] . Now consider an arbitrary k > 4. Then k can be split as k = 4ℓ + h where ℓ, h ∈ N0 and 0 ≤ h < 4 and we get Ak = (A4)ℓAh = Ah. We conclude that Ak = I if and only if h = 0 which is the same as saying that Ak = I if and only if k ≡4 0. 4. In order to prove that S′ is a subset of a hyperplane of Rn+1, we need to find a vector d′ ∈ Rn+1 such that w · d′ = 0 for all w ∈ S′. To achieve this, consider what we know about an arbitrary w ∈ S′: It must be of the form w =        v1 v2 ... vn 1        for some v ∈ S. So for the scalar product w · d′ we get w · d′ = w1d ′ 1 + w2d ′ 2 + · · · + wn+1d ′ n+1 = v1d ′ 1 + v2d ′ 2 + · · · + vnd ′ n + d ′ n+1. By v ∈ S we know that v · d = c. So if we now choose d′ =        d1 d2 ... dn −c        we get w · d′ = v1d ′ 1 + v2d ′ 2 + · · · + vnd ′ n + d ′ n+1 = (v · d) − c = (v · d − c) = 0 as desired. This works for any w ∈ S′ and hence S′ is a subset of the hyperplane {v ∈ Rn+1 : v · d′}. 5. a) Let x ∈ Rn be arbitrary and assume Bx = 0. Recall that Bx = x1b1 + x2b2 + · · · + xnbn where x1, x2, . . . , xn are the entries of x and b1, b2, . . . , bn are the columns of B. Hence, in order to prove that the columns of B are linearly independent, we have to prove x = 0. We achieve this with the calculation x = Ix = ABx = A0 = 0. b) Let y ∈ Rn be arbitrary and assume Ay = 0. We want to prove that y = 0. By the inverse theorem and using subtask a), we know that there exists x ∈ Rn with Bx = y. Now observe that indeed, we have y = Bx = BIx = B(ABx) = B(Ay) = 0 and hence we conclude that the columns of A are linearly independent. c) By applying the matrix A to BA − I we get A(BA − I) = ABA − A = IA − A = 0. In particular, we have Av = 0 for every column v of BA − I. But by the inverse theorem and subtask b), the equation Ay = 0 has a unique solution y. And we also know that 0 is a solution since A0 = 0. By uniqueness, we conclude that we have v = 0 for every column v of BA − I and hence BA − I = 0.","libVersion":"0.3.2","langs":""}