{"path":"HS23/LinAlg/PV/cheatsheets/LinAlg-cheatsheet-nico.pdf","text":"LinAlg Cheatsheet Robin Frauenfelder - robinfr@ethz.ch Version: 26. Juni 2021 Dieses Cheatsheet wurde im Laufe meiner ¨Ubungstunde im MAVT Basisjsahr 18/19 erstellt. bearbeitet von Christophe Maier(cmaier) und Katharina Jaeger(kjaeger) 1. Lineare Gleichungssysteme 1.1 Zeilenstufenform Jedes lineare Gleichungssystem kann durch wiederholtes Ausf¨uhren folgender drei Rechenoperationen in die sogenannte Zeilenstufenform gebracht werden (Gaussalgorithmus): ‚ Zeilen vertauschen ‚ Ein Vielfaches einer Zeile zu einer anderen addieren ‚ Eine Zeile mit beliebigem Skalar ‰ 0 multiplizieren x1 x2 x3 ¨ ¨ ¨ xj ¨ ¨ ¨ xn 1 f ˚ ˚ ¨ ¨ ¨ ˚ ¨ ¨ ¨ ˚ b1 0 0 f ¨ ¨ ¨ ˚ ¨ ¨ ¨ ˚ b2 0 0 0 ¨ ¨ ¨ ˚ ¨ ¨ ¨ ˚ b1 . . . . . . . . . . . . . . . . . . 0 0 0 ¨ ¨ ¨ f ¨ ¨ ¨ ˚ br 0 0 0 ¨ ¨ ¨ 0 ¨ ¨ ¨ 0 br`1 . . . . . . . . . . . . . . . . . . 0 0 0 ¨ ¨ ¨ 0 ¨ ¨ ¨ 0 bmm:AnzahlGleichungenr:Rang n: Anzahl Unbekannte Keine L¨osung: r † m und bi ‰ 0 @i ° r Eindeutige L¨osung: r “ n “ m Unendlich L¨osungen: r † n und bi “ 0 @i ° r Anzahl freie Parameter: n ´ r Ax = b f¨ur beliebiges b l¨osbar: Voller Rang: r “ m oder m “ n, und Ax “ 0 hat nur die triviale L¨osung x “ 0 Ax = b f¨ur beliebiges b eindeutig l¨osbar: Voller Rang: r “ m und gleich viele Gleichungen wie Unbekannte: m “ n 1.2 Homogenes Lineares Gleichungssystem x1 x2 ¨ ¨ ¨ xn 1 a11 a12 ¨ ¨ ¨ a1n 0 a21 a22 ¨ ¨ ¨ a2n 0 . . . . . . . . . . . . am1 am2 ¨ ¨ ¨ amn 0Nullvektor ‚ Hat immer die triviale L¨osung x “ 0 ‚ Hat ausschliesslich die triviale L¨osung, wenn Rang vollst¨andig (r “ n, detpAq ‰ 0) ‚ Hat zus¨atzlich nichttriviale L¨osungen, wenn Rang nicht vollst¨andig (r † n, detpAq “ 0) 2. Matrizen 2.1 Matrixschreibweise Ein lineares Gleichungssystem kann in Matrixschreibweise dar- gestellt werden: x1 x2 1 a11 a12 b1 a21 a22 b2 a31 a32 b3 ñ ¨ ˝a11 a12 a21 a22 a31 a32 ˛ ‚¨ ˆx1 x2 ˙ “ ¨ ˝b1 b2 b3 ˛ ‚ 2.2 Matrixmultiplikation Matrizen k¨onnen auf folgende Weise miteinander multipliziert werden: A ¨ B “ C ñ cik “ ∞n j“1 aij ¨ bjkN x LM x NAM x LCB Assoziativ- & Distributivgesetz: pA ¨ Bq ¨ C “ A ¨ pB ¨ Cq pA ` Bq ¨ C “ A ¨ C ` B ¨ C A ¨ pC ` Dq “ A ¨ C ` A ¨ D Achtung! Kommutativgesetz gilt nicht! i.A. A ¨ B ‰ B ¨ A 2.3 Identit¨atsmatrix Die Identit¨ats- oder Einheitsmatrix ist eine quadratische Ma- trix, deren Hauptdiagonalenelemente 1 und deren Ausserdia- gonalelemente 0 sind. In “ ¨ ˚ ˚ ˚ ˝ 1 0 ¨ ¨ ¨ 0 0 1 ¨ ¨ ¨ 0 . . . . . . . . . 0 0 ¨ ¨ ¨ 1 ˛ ‹ ‹ ‹ ‚ P Rnˆn Rechenregel: A mˆn ¨ In “ Im ¨ A mˆn “ Amˆn 2.4 Diagonal- und Dreiecksmatrizen Eine Diagonalmatrix ist eine quadratische Matrix, deren Ele- mente ausserhalb der Hauptdiagonalen 0 sind. D “ diagpd1, d2, d3q “ ¨ ˝d1 0 0 0 d2 0 0 0 d3 ˛ ‚ Eine Dreiecksmatrix ist eine quadratische Matrix, deren Ele- mente entweder oberhalb oder unterhalb der Hauptdiagonalen Null sind. Man unterscheidet zwischen einer Rechtsdreiecks- matrix und einer Linksdreiecksmatrix. R “ ¨ ˝r11 r12 r13 0 r22 r23 0 0 r33 ˛ ‚ L “ ¨ ˝l11 0 0 l21 l22 0 l31 l32 l33 ˛ ‚ F¨ur Diagonal- und Dreiecksmatrizen gilt: ‚ detpAq “ a11 ¨ a22 ¨ a33 ¨ ¨ ¨ ann ‚ eigpAq “ ta11, a22, ¨ ¨ ¨ , annu 2.5 Transponierte Die Transponierte einer Matrix erh¨alt man, indem man sie an ihrer Diagonalen ” spiegelt“. Bsp: ¨ ˝a b c d e f ˛ ‚ T “ ˆa c e b d f ˙ Rechenregeln: pA ` BqT “ AT ` BT pA ¨ BqT “ BT ¨ AT pc ¨ AqT “ c ¨ AT pAT qT “ A pAT q´1 “ pA´1qT rangpAT q “ rangpAq detpAT q “ detpAq eigpAT q “ eigpAq 2.6 Inverse Die Inverse A ´1 von A macht eine Multiplikation mit A r¨uckg¨angig. Multipliziert man A mit A´1, erh¨alt man die Iden- tit¨atsmatrix. A ¨ v “ w ô A´1 ¨ w “ v A ´1 ¨ A “ A ¨ A ´1 “ I Eigenschaften: ‚ Nur quadratische Matrizen k¨onnen invertierbar sein. ‚ Eine invertierbare Matrix nennt man regul¨ar, eine nicht in- vertierbare singul¨ar. ‚ Die Inverse ist eindeutig. ‚ A ist invertierbar ñ A hat vollen Rang ‚ A ist invertierbar ñ AT ist invertierbar ‚ A ist symmetrisch ñ A´1 ist symmetrisch ‚ A ist eine Dreiecksmatrix ñ A ´1 ist eine Dreiecksmatrix ‚ A ist invertierbar ñ detpAq ‰ 0 ‚ A ist invertierbar ñ kein Eigenwert ⁄ “ 0 ‚ A und B sind invertierbar ùñ AB ist invertierbar Gauss-Jordan Algorithmus: Methode zur Bestimmung der Inversen. Man schreibt die Ma- trix und die Identit¨at nebeneinander auf und f¨uhrt den Gaus- salgorithmus gleich auf beiden Seiten aus, sodass am Ende auf der linken Seite die Identit¨atsmatrix steht. Tipp: Erzeuge zuerst durch ” nach unten gaussen“ links eine Rechtsdreiecksmatrix, dann durch ” nach oben gaussen“ eine Diagonalmatrix, und am Ende durch Zeilenmultiplikation die Identit¨atsmatrix. ¨ ˝1 2 0 1 0 0 2 3 0 0 1 0 3 4 1 0 0 1 ˛ ‚ ñ ¨ ˝1 0 0 ´3 2 0 0 1 0 2 ´1 0 0 0 1 1 ´2 1 ˛ ‚ A A´1 Adjunktenformel f¨ur 2x2-Matrizen: ˆa b c d ˙´1 “ 1 ad ´ bc ¨ ˆ d ´b ´c a ˙ Rechenregeln: I ´1 “ I pA´1q´1 “ A pAkq´1 “ pA´1qk pc ¨ Aq´1 “ c ´1 ¨ A´1 pA ¨ Bq´1 “ B´1 ¨ A´1 pAT q´1 “ pA´1qT rangpA´1q “ rangpAq detpA ´1q “ detpAq ´1 eigpA´1 “ eigpAq´1 2.7 Orthogonale Matrizen Eine quadratische Matrix ist orthogonal, wenn sie aus zueinan- der orthogonalen Spaltenvektoren der L¨ange 1 besteht. Dies ist der Fall, wenn eine Matrix mit der Transponierten multipli- ziert die Identit¨at ergibt. QT ¨ Q “ I Multipliziert man einen Vektor mit einer orthogonalen Matrix, kann sich seine Orientierung ¨andern, jedoch nicht seine L¨ange. Abbildungen sind deshalb kongruent. Eigenschaften: ‚ Q orthogonal ô Spalten/Zeilen von Q sind zueinander or- thogonale Vektoren der L¨ange 1. ‚ Nur quadratische Matrizen k¨onnen orthogonal sein. ‚ A und B orthogonal ñ A ¨ B orthogonal ‚ Q orthogonal ô Q´1 orthogonal ‚ Q´1 “ Q T ‚ |detpQq| “ 1 Drehungs- und Spiegelungsmatrizen: Drehung um Ursprung im R 2: Rp–q “ ˆcosp–q ´sinp–q sinp–q cosp–q ˙ Drehung um x-Achse im R3: Rxp–q “ ¨ ˝1 0 0 0 cosp–q ´sinp–q 0 sinp–q cosp–q ˛ ‚ Drehung um y-Achse im R3: Ryp–q “ ¨ ˝ cosp–q 0 sinp–q 0 1 0 ´sinp–q 0 cosp–q ˛ ‚ Drehung um z-Achse im R3: Rzp–q “ ¨ ˝cosp–q ´sinp–q 0 sinp–q cosp–q 0 0 0 1 ˛ ‚ 2.8 LR-Zerlegung A “ L ¨ R Mit der LR-Zerlegung kann man eine quadratische Matrix A in das Produkt einer Linksdreiecksmatrix L sowie einer Rechts- dreiecksmatrix R zerlegen. Dies erm¨oglicht ein eﬃzienteres L¨osen von Ax “ bi mit vielen verschiedenen bi. Bsp: L¨ose Ax “ b durch LR-Zerlegung von A “ ˆ2 5 4 12 ˙ Vorgehen: 1 Bringe A durch Zeilensubtraktion in Dreiecksform. Bei erzeugten Nullstellen speichert man, das Wievielfache ei- ner anderen Zeile von dieser Zeile subtrahiert wurde. Bsp: ˆ2 5 4 12 ˙ ùñ ˜ 2 5 2 2 ¸ 2 : Von dieser Zeile wurde das 2-fache einer anderen subtrahiert. 2 Bestimme L und R. L besteht aus den markierten Ein- tr¨agen und 1 auf der Diagonale, R aus den nichtmar- kierten Eintr¨agen. Bsp: ˜ 2 5 2 2 ¸ ùñ L “ ˆ1 0 2 1 ˙, R “ ˆ2 5 0 2 ˙ 3 L¨ose Ly “ b (einfach, da L eine Dreiecksmatrix). 4 L¨ose Rx “ y (einfach, da R eine Dreiecksmatrix). LRP-Zerlegung mit Permutationsmatrix P P ¨ A “ L ¨ R Manchmal ist es notwendig, dass man bei 1 zus¨atzlich Zeilen vertauschen kann. Dies wird durch eine Permutationsmatrix P m¨oglich. Hierzu schreibe man zu Beginn die Identit¨atsmatrix neben A, und macht mit dieser alle Zeilenvertauschungen mit: Bsp: ˆ1 0 1 2 0 1 3 4 ˙ ùñ ˆ0 1 3 4 1 0 1 2 ˙ Auf der linken Seite steht am Ende die Permutationsmatrix P . L und R werden auf die gleiche Weise wie ¨ublich bestimmt. Bei 3 l¨ose man nun Ly “ P b, bei 4 weiterhin Rx “ y. 2.9 Symmetrische Matrizen Eine symmetrische Matrix ist eine quadratische Matrix, de- ren Eintr¨age spiegelsymmetrisch bez¨uglich der Hauptdiagona- len sind. Dies ist der Fall, wenn sie gleich ihrer Transponierten ist: S “ ST Eigenschaften: ‚ A T A und AAT sind immer symmetrisch. ‚ Die Eigenwerte von S sind alle reell. ‚ Ist x ein Eigenvektor von S zum Eigenwert ⁄, so sind auch konjpxq, Repxq, Impxq Eigenvektoren zum selben Eigen- wert ⁄ ‚ Die Eigenvektoren zu unterschiedlichen Eigenwerten sind or- thogonal zueinander. ‚ S ist halbeinfach, also diagonalisierbar. ‚ S besitzt eine orthonormale Eigenbasis. ‚ Transformationsmatrix T in Eigenbasis kann orthogonal gew¨ahlt werden. 3. Determinante 3.1 Deﬁnition Determinante Die Determinante ist eine Zahl, die einer quadratischen Ma- trix zugeordnet wird und aus ihren Eintr¨agen berechnet werden kann. Die folgenden Spalten/zeileneigenschaften sind Teil ihrer Deﬁnition. Zeileneigenschaften:aa ‚ Vertauscht man zwei Zeilen von A, so ¨andert sich das Vor- zeichen der Determinante. ‚ Addiert man ein Vielfaches einer Zeile zu einer anderen, so ¨andert sich die Determinante nicht. Spalteneigenschaften: ‚ Vertauscht man zwei Spalten von A, so ¨andert sich das Vor- zeichen der Determinante. ‚ Addiert man ein Vielfaches einer Spalte zu einer anderen, so ¨andert sich die Determinante nicht. Folgerungen aus Zeilen/Spalteneigenschaften: ‚ Hat A zwei lin. abh. Zeilen/Spalten, so gilt detpAq “ 0. ‚ Hat A eine Nullzeile/spalte, so gilt detpAq “ 0 ‚ detp– ¨ Anˆnq “ –n ¨ detpAq detpAq “ 0 ‚ A ¨ x “ b hat keine oder unendl. L¨osungen ‚ A ¨ x “ 0 hat unendl. L¨osungen ‚ Rang(A) ¡ n 3.2 Rechenregeln Determinante Neben den Zeilen/Spalteneigenschaften von 3.1 gelten folgen- de Rechenregeln: detpABq “ detpAq ¨ detpBq detpA T q “ detpAq detpdiagpd1, d2, ¨ ¨ ¨ , dnqq “ d1 ¨ d2 ¨ ¨ ¨ dn detpDreiecksmatrixq “ d1 ¨ d2 ¨ ¨ ¨ dn detpA ´1q “ 1{detpAq 3.3 Berechnungsmethoden Determinante Es gibt verschiedene Methoden, die Determinante zu bestim- men. Je nach Matrix eignen sich unterschiedliche Rechnungs- wege oder Kombinationen davon. Fertige Formeln Eignen sich nur bei kleinen Matrizen. Meistens f¨ur 3x3-Matrix bereits zu kompliziert. 1x1: |a| “ a 2x2: - - -a b c d- - - “ ad ´ cb 3x3: - - - - a b c d e f g h i - - - - “ aei ` bf g ` cdh ´ gec ´ hf a ´ idb Laplace’scher Entwicklungssatz: Bei den meisten Matrizen ineﬃzient. Kann jedoch bei Matrix mit vielen Nullen in einer Zeile oder Spalte geschickt angewen- det werden. 1 Zeile oder Spalte ausw¨ahlen (dort wo viele Nullen). 2 Jedem Element dieser Zeile/Spalte ein Vorzeichen zu- ordnen (Schachbrett). 3 F¨ur jedes Element die zugeh¨orige Zeile und Spalte strei- chen und Unterdeterminante bestimmen. 4 Jede Unterdeterminante mit zugeh¨origem Element und Vorzeichen multiplizieren und addieren. Bsp: Entwicklung nach erster Spalte: - - - - 1 2 1 3 8 5 0 3 2 - - - - “ ` 1 ¨ - - -8 5 3 2- - - ´ 3 ¨ - - -2 1 3 2- - - ` 0 ¨ ˝` ´ ` ´ ` ´ ` ´ ` ˛ ‚ Anwenden von Zeilen/Spalteneigenschaften Durch vertauschen von Spalten/Zeilen (Vorzeichen¨anderung) oder Zeilen/Spaltenaddition (Determinante bleibt gleich) l¨asst sich die Matrix oft in eine einfachere Form bringen. Blocksatz Oft in Kombination mit ” Anwenden von Zeilen/Spalteneigen- schaften“ n¨utzlich. LR-Zerlegung Nur sinnvoll, wenn LR-Zerlegung bereits vorliegt. detpAq “ p´1q#Zeilenvertauschungen ¨ r11 ¨ r22 ¨ ¨ ¨ rnn Determinante mit Nullstellen A “ ¨ ˚ ˚ ˚ ˝ 1 7 0 0 0 3 a 0 0 0 4 6 b 2 0 1 ´3 2 b 0 2 1 7 3 c ˛ ‹ ‹ ‹ ‚ Betrachte als A “ ¨ ˚ ˚ ˚ ˚ ˝ - - -1 7 3 a - - - 0 0 ´ ´ ´ - - -b 2 2 b- - - 0 ´ ´ ´ ´ ´ ´ c ˛ ‹ ‹ ‹ ‹ ‚ detpAq “ - - -1 7 3 a - - - ¨ - - -b 2 2 b- - - ¨ c 3.4 Wichtige Zusammenh¨ange Folgende Aussagen sind f¨ur Anˆn ¨aquivalent: ‚ rangpAq “ n ‚ Das LGS Ax “ b ist f¨ur beliebiges b l¨osbar. ‚ Das LGS Ax “ b besitzt genau eine L¨osung. ‚ Das homogene LGS Ax “ 0 besitzt nur die triviale L¨osung. ‚ Die Zeilen/Spalten von A sind linear unabh¨angig. ‚ A ist invertierbar. ‚ detpAq ‰ 0 ‚ Die Spalten von A bilden eine Basis in Rn. ‚ Der Kern von A besteht nur aus dem Nullvektor. ‚ Kein Eigenwert von A ist 0. 4. Vektorr¨aume 4.1 Deﬁnition Vektorraum Sei V eine Menge von Objekten. V heisst Vektorraum, wenn eine innere Operation (Kombination von zwei Objekten) und eine ¨aussere Operation (Kombination eines Objekts mit einem Skalar) deﬁniert sind, und folgende Axiome gelten: Innere Operation: À : V ˆ V Ñ V pa, bq ﬁÑ a À b ¨Aussere Operation: Ä : K ˆ V Ñ V p–, aq ﬁÑ – Ä a Axiome: (A1) @u, v P V : (A2) @u, v, w P V : (A3) D0 P V, @u P V : (A4) @u P V, D ´ u P V : (M1) @–, — P R, @u P V : (M2) @–, — P R, @u, v P V : (M3) @u P V : u À v “ v À u pu À vq À w “ u À pv À wq u À 0 “ u u À p´uq “ 0 p– ¨ —q Ä u “ – Ä p— Ä uq p– ` —q Ä u “ p– Ä uq À p— Ä uq – Ä pu À vq “ p– Ä uq À p– Ä vq 1 Ä u “ u 4.2 Deﬁnition Unterraum Eine nichtleere Teilmenge eines Vektorraums V heisst Unter- raum von V, falls: 1 @a, b P U : a À b P U 2 @a P U, @– P K : – Ä a P U ‚ Ein Unterraum ist selber ein Vektorraum. ‚ Ein Unterraum muss den Nullvektor enthalten! 4.3 Linearkombination Eine Linearkombination ist eine Summe von mit Skalaren xi multiplizierten Vektoren vi. (vi P V, xi P K) w “ x1 ¨ v1 ` x2 ¨ v2 ` ¨ ¨ ¨ ` xn ¨ vn ‚ w “ V ¨ x hat L¨osung (V “ pvp1q, vp2q, ¨ ¨ ¨ , vpnqq) ùñ w ist Linearkombination von vi. 4.4 Lineare Unabh¨angigkeit ∞ xi ¨ vi “ 0 Die Vektoren vi sind linear unabh¨angig, falls die Summe ∞ nur die triviale L¨osung x1 “ x2 “ ¨ ¨ ¨ “ xi “ 0 hat. Pr¨ufen, ob Vektoren linear unabh¨angig: 1 Matrix mit Vektoren als Spalten erstellen: V “ pvp1q, vp2q, ¨ ¨ ¨ , vpnqq 2 Der Rang ist die Anzahl der linear unabh¨angigen Vekto- ren. rangpV q “ n ùñ Vektoren sind linear unabh¨angig. schiefsymmetrische -> Anhang 4.5 Span, Erzeugendensystem und Basis Die lineare H¨ulle spanpv1, v2, ¨ ¨ ¨ , vnq ist die Menge aller endlichen Linearkombinationen der vi mit Skalaren aus R. Falls f¨ur einen Vektorraum gilt spanpv1, v2, ¨ ¨ ¨ , vnq “ V , heisst tv1, v2, ¨ ¨ ¨ , vnu ein Erzeugendensystem von V. Falls ein Erzeugendensystem f¨ur V aus linear unabh¨angigen Vektoren besteht, heisst es Basis von V. Jeder Vektor kann eindeutig als Linearkombination von Basisvektoren dargestellt werden. Aus Erzeugendensystem Basis ﬁnden: 1 Matrix aufstellen, deren Zeilen aus den transponierten erzeugenden Vektoren besteht. 2 Mit Gaussalgorithmus in Zeilenstufenform bringen. Da- durch wird lineare Abh¨angigkeit eliminiert. 3 Die Ausgangsvektoren der Pivotspalten sind Basisvekto- ren. 4.6 Basiswechsel Sei V n ein Vektorraum mit Basen Q “ tq1, q2, . . . , qnu und W “ tw1, w2, . . . , wnu. Sei v ein Vektor P V . Basiswechsel von rvsq nach rvsw durchf¨uhren: 1 ¨Ubergangsmatrix: TqÑw “ prq1sw, . . . , rqnswq 2 rvsw “ TqÑw ¨ rvsq Tipps: ‚ TwÑq “ T ´1 qÑw ‚ Meist ist eine der beiden Basen die Standardbasis S. Die ¨Ubergangsmatrix TqÑs ist dann sehr einfach bestimmbar. Die entegengesetzte ¨Ubergangsmatrix wird am schnellsten durch invertieren gefunden. ‚ Basiswechsel f¨ur Matrizen: rAsw “ TqÑw ¨ rAsq ¨ T ´1 qÑw ‚ Falls T orthogonal: T ´1 “ T T 4.7 Koordinaten Sei V ein Vektorraum mit Basis B “ tb1, ¨ ¨ ¨ , bnu. Dann kann jeder Vektor x P V in eindeutiger Weise als Linearkom- bination x “ ∞n i“1 xi ¨ bi dargestellt werden. Die Koeﬃzienten x1, ¨ ¨ ¨ , xn heissen Ko- ordinaten von x bez¨uglich der Basis B. 4.8 Dimension Besitzt der Vektorraum V eine Basis B “ b 1, b2, ..., b n, so heißt n die Dimension von V und man schreibt dimpV q “ n. 5. Lineare Abbildungen 5.1 Deﬁnition Lineare Abbildung Eine Abbildung F heisst linear, falls @x, y P V, @– P K 1 F px ` yq “ F pxq ` F pyq 2 F p– ¨ xq “ – ¨ F pxq ‚ Eine Abbildung ist linear ùñ bildet 0 auf 0 ab! ‚ Eine Abbildung zwischen endlichdimensionalen VR ist linear ñ kann mit einer m ˆ n-Matrix A mit Hilfe der Matri- zenmultiplikation dargestellt werden. ‚ Ist A P Rnxn und a P Rn, dann heißt F : Rn Ñ Rn, x Ñ A ¨ x ` a eine aﬃn lineare Abbildung. 5.2 Kern und Bild einer Matrix Kern: Der Kern einer Matrix ist die Menge aller Vektoren, die durch Multiplikation auf den Nullvektor abgebildet werden. KernpAq “ tx P Rn|A ¨ x “ 0u ‚ Kern bestimmen: Gleichungssystem Ax “ 0 l¨osen. Wenn Rang nicht voll ist, gibt es unendlich viele L¨osungen. Der L¨osungsraum (am besten dargestellt als Linearkombi- nation von mit Parameter multiplizierten Vektoren) ist der Kern der Matrix. Bild: Das Bild einer Matrix A ist die Menge aller Bildvekto- ren, also aller m¨oglichen ” Ergebnisse“einer Multiplikation von A mit einem beliebigen Vektor. BildpAq “ ty P Rm| Dx P R n sodass y “ A ¨ xu ‚ Bild bestimmen: Bild “ spanta p1q, ap2q . . . a pnqu Ist nach einem Erzeugendensystem gefragt, reicht es, ein- fach die Spaltenvektoren hinzuschreiben. Wenn kerpAq schon berechnet: im(A) = Pivotspalten der Matrix Achtung: Die Spaltenvektoren sind immer ein Erzeugenden- system des Bildes, jedoch nicht unbedingt eine Basis! Um Basis zu erstellen: Siehe 4.5 Zusammenh¨ange ‚ dimpBildpAqq “ RangpAq ‚ f¨ur Amˆn: dimpBildpAqq ` dimpKernpAqq “ n ‚ BildpAq K KernpAT q ‚ Fredholm Alternative: Ax “ b ist l¨osbar (b liegt im Bild) genau dann, wenn b senkrecht auf allen L¨osungen des ad- jungierten LGS AT ¨ y “ 0 steht. 5.3. Abbildungsmatrix aus gegebener Abbildung Idee: Wir bilden zuerst die Basisvektoren ab und konstruieren uns aus den Ergebnissen unsere Matrix. Bsp: P2 Ñ P1 : ppxq ﬁÑ p1pxq 1 Finde Basis f¨ur Vektorraum aus dem man abbildet und f¨ur Vektorraum in den man abbildet. Bsp: Basis f¨ur P2 “ tx 2, x, 1u Basis f¨ur P1 “ tx, 1u 2 ¨Uberlege, was nach gegebener Abbildungsvorschrift mit den Basisvektoren passiert und schreibe die Ergebnisse in Vektorschreibweise. Bsp: px 2q1 “ 2 ¨ x “ `2 0 ˘T pxq1 “ 1 “ `0 1 ˘T p1q1 “ 0 “ `0 0 ˘T 3 Resultate von Punkt 2 sind Spalten der gesuchten Ma- trix. (Multiplikation mit Basisvektor = Extraktion von Spalte) Bsp: A “ ˆ2 0 0 0 1 0 ˙ Bei Projektion eines Vektors v auf einen Unterraum F gilt: projF pvq “ xv, F1yF1 ` xv, F2yF2 ` xv, F3yF3 6. Eigenwertproblem 6.1 Deﬁnition Eigenwerte und Eigenvektoren ⁄ P C heisst Eigenwert von A nˆn, falls dieser f¨ur einen be- stimmten Vektor v A ¨ v “ ⁄ ¨ v erf¨ullt. Der zum Eigenwert ⁄ zugeh¨orige Vektor v P Cn heisst Eigenvektor. 6.2 Eigenwerte bestimmen 1 Bestimme die Determinante detpA ´ ⁄ ¨ Iq Das Resultat ist das ”charakteristische Polynom” pp⁄q vom Grad n 2 Bestimme die Nullstellen: pp⁄q “ 0. Die Nullstellen ⁄i heissen Eigenwerte. Berechnung ¨uberpr¨ufen ‚ SpurpAq “ a11 ` a22 ` . . . ` ann “ ∞ ⁄i ‚ detpAq “ ± ⁄i Es gilt: ‚ Die Eigenwerte einer Dreiecksmatrix sind die Elemente der Diagonalen. ‚ Ist ⁄ ein Eigenwert von A, so ist ⁄ ´1 ein Eigenwert von A´1. ‚ Ist ⁄ ein Eigenwert von A, so ist ⁄ n ein Eigenwert von An. 6.3 Eigenvektoren bestimmen Nach dem Bestimmen der Eigenwerte k¨onnen die zugeh¨origen Eigenvektoren bestimmt werden. 1 Setze einen Eigenwert ⁄k in pA ´ ⁄k ¨ Iq ein 2 L¨ose das Gleichungssystem pA ´ ⁄k ¨ Iq ¨ x “ 0 3 Das Gleichungssystem hat unendlich viele L¨osungen. Man erh¨alt einen oder mehrere mit freien Parametern multiplizierte Eigenvektoren vk. Eigenschaften ‚ Eigenvektoren sind per Deﬁnition ‰ 0 ‚ Eigenvektoren sind linear unabh¨angig. ‚ Komplex konjugierte Eigenwerte haben komplex konjugierte Eigenvektoren (spart Zeit bei Berechnung). ‚ Falls A orthogonal: Die Eigenvektoren von verschiedenen ⁄ von A sind immer orthogonal. 6.4 Algebraische und geometrische Vielfachheit 1 § gVfh. von ⁄ § algVfh. von ⁄ § n Algebraische Vielfachheit Die algebraische Vielfachheit ist die Vielfachheit einer Nullstel- le im charakteristischen Polynom pp⁄q beim jeweiligen Eigen- wert ⁄. Bsp: pp⁄q “ p⁄ ´ 3q2 ¨ p⁄ ´ 2q ùñ ⁄ “ 3 hat algVfh. 2 und ⁄ “ 2 hat algVfh. 1 Geometrische Vielfachheit Die geometrische Vielfachheit von ⁄ ist die Anzahl der zum EW geh¨origen EV = Anzahl der freien Parameter. 6.5 (Halb)einfache Matrizen und Eigenbasis Einfachheit ‚ Eine Matrix ist halbeinfach ô jedes ⁄ hat algVh = gVfh ‚ Eine Matrix ist einfach ô jedes ⁄ hat algVh = gVfh = 1 Es gilt: ‚ Ist A halbeinfach, so ist auch A n und AT halbeinfach ‚ Ist A einfach, so ist An nicht einfach Eigenbasis: Die Eigenvektoren einer Matrix A P Cnˆn bilden einen Basis f¨ur C n ñ die Matrix ist halbeinfach. 6.6 Diagonalisierbarkeit Eine quadratische Matrix A heisst diagonalisierbar, falls eine regul¨are Matrix T existiert, sodass D “ T ´1AT eine Diago- nalmatrix ist. A halbeinfach ñ A diagonalisierbar Matrix diagonalisieren (Basiswechsel in Eigenbasis) 1 Bestimme die Eigenwerte ⁄i und die Eigenvektoren vi 2 Die Matrix D “ diagp⁄1, . . . , ⁄nq ist eine Diagonal- matrix mit den Eigenwerten auf der Diagonalen. 3 Die Matrix T “ pv1, . . . , vnq hat die Eigenvektoren als Spalten (Gleiche Reihenfolge wie bei D!). 4 Bestimme T ´1. Falls EV orthonormal T ´1 “ T T Potenzen und Exponentialfunktion Potenzen/Exponentialfunktionen von diagonalisierbaren Ma- trizen k¨onnen einfach berechnet werden: ‚ Ak “ pT DT ´1qk “ T ¨ diagp⁄ k 1 , . . . , ⁄ k nq ¨ T ´1 ‚ e A “ e T DT ´1 “ T ¨ diagpe⁄1 , . . . , e⁄n q ¨ T ´1 6.7 ¨Ahnlichkeit A und B heissen ¨ahnlich, falls f¨ur eine beliebige Matrix T gilt: A “ T ´1BT ¨Ahnliche Matrizen haben: ‚ die gleichen Eigenwerte ‚ die gleiche Determinante Satz: Ist v ein EV von A zum EW ⁄, so ist y “ T ´1v ein EV von B zum selben EW. 6.8 ¨Spektrum Das Spektrum ist deﬁniert als die Menge aller Eigenvektoren von A. 6.9 Eigenwertproblem symmetrischer Matrizen Sei A PR nxn symmetrisch, dann gilt: ‚ alle Eigenwerte von A sind reell ‚ Eigenvektoren sind zu den versch. EW orthogonal ‚ A ist halbeinfach (diagonalisiserbar) ‚ A besitzt eine ONB (Spalten von T normieren Ñ Gram- Schmidt) ‚ Es gibt eine orthogonale Matrix T , sodass T ´1AT “ T T AT “ diagp⁄1, ⁄2, ..., ⁄nq, Spalten normieren! 7. Normen 7.1 Deﬁnition Vektornorm Eine Norm im Vektorraum V ordnet jedem Vektor v eine relle Zahl ||v|| zu und kann so als eine Art Mass verstanden werden. Die Norm ist unabh¨angig von der Basis. Sie muss folgende Bedingungen erf¨ullen: 1 ||v|| • 0 und ||v|| “ 0 ô v “ 0 2 ||– ¨ v|| “ |–| ¨ ||v|| 3 ||v ` w|| § ||v|| ` ||w|| 7.2 Lp-Normen im Rn Allgemeine Lp-Norm: ||v||p “ pa∞ |vi|p Beispiele ‚ L1-Norm: ||v||1 “ |v|1 ` |v|2 ` . . . ` |v|n ‚ L2-Norm: ||v||2 “ b |v|2 1 ` |v|2 2 ` . . . ` |v|2 n (Euklkid) ‚ L8-Norm: ||v||8 “ maxp|v|1, |v|2, . . . , |v|nq (Maxi- mum) 7.3 Lp-Normen f¨ur Funktionen Allgemeine Lp-Norm: ||f ||p “ p≥b a |f pxq|pdxq 1{p Beispiele ‚ L1-Norm: ||f ||1 “ ≥b a |f pxq|dx (Gibt den Betrag der Fl¨ache unter der Kurve an) ‚ L8-Norm: ||f ||8 “ maxp|f pxq| : x P pa, bqq (Gibt den maximalen Ausschlag an) 7.4 Matrixoperatornormen ||A|| “ maxt||x||2 “1u “ ||Ax||2 Beispiele ‚ A quadratisch: ||A||2 “ a ⁄maxvonAT ¨ A ‚ A symmetrisch: ||A||2 “ |⁄maxvonA| ‚ A orthogonal: ||A||2 “ 1 ‚ A regul¨ar: ||A´1||2 “ 1b ⁄minvonAT ¨A ‚ regul¨ar symmetrisch: ||A´1||2 “ 1 minp|⁄i |q ‚ ||A||1 = maximale Spaltensummennorm = Maximum der Addition des Betrages der einzelnen Eintr¨age der Spalte ‚ ||A||8 = maximale Zeilensummennorm = Maximum der Addition des Betrages der einzelnen Eintr¨age der Zeile ‚ Ist A symmetrisch, so gilt: ||A||1 “ ||AT ||8 “ ||A||8 8. Skalarprodukt 8.1 Deﬁnition Skalarprodukt Ein Skalarprodukt ordnet jedem Paar x, y von Vektoren eine Zahl xx, yy zu. Es muss folgende Bedingungen erf¨ullen: 1 xx, y ` zy “ xx, yy ` xx, zy 2 xx, –yy “ –xx, yy 3 xx, yy “ xy, xy 4 xx, xy • 0 und xx, xy “ 0 ô x “ 0 Beispiele f¨ur Skalarprodukte ‚ Standardskalarprodukt auf Rn: xx, yy “ xT ¨ y ‚ Funktionenskalarprodukt: xf, gy “ ≥b a f pxqgpxqdx 8.2 Von Skalarprodukt induzierte Norm Aus einem Skalarpodukt kann eine Norm induziert werden. Dieser Ausdruck erf¨ullt alle Axiome f¨ur eine Norm (siehe 7.1.) ||x|| “ axx, xy 8.3 Orthogonalit¨at und Orthogonalprojektion Zwei Vektoren sind orthogonal, falls xx, yy “ 0. Notation: x K y Die Orthogonalprojektion des Vektors x auf Vektor y ist: z “ xx,yy xy,yy ¨ y 8.4 Gram-Schmidtsches Orthonormalisierungsverfahren Ziel des Gram-Schmidtschen Orthonormalisierungsverfahrens ist, aus einer beliebigen Basis eine sogenannte Orthonormal- basis zu erzeugen. Bei einer Orthonormalbasis sind alle Basisvektoren: ‚ orthogonal zueinander: xbi, bj y “ 0 ‚ Einheitsvektoren: ||bi|| “ a xbi, biy “ 1 Orthonormalisierungsverfahren durchf¨uhren: F¨ur die Durchf¨uhrung ben¨otigt man eine beliebige Basis, sowie ein beliebiges Skalarprodukt (meistens gegeben). 1 W¨ahle beliebigen ersten Basisvektor b1 und normiere mit von Skalarprodukt induzierter Norm. e1 “ b1 ||b1 || “ b1?xb1 ,b1 y 2 W¨ahle zweiten Basisvektor b2. Zuerst zu b1 parallelen Teil abziehen, und dann normieren. e1 2 “ b2 ´ xb2, e1y ¨ e1 e2 “ e1 2 ||e1 2 || “ e1 2b xe1 2 ,e1 2 y 3 Wiederhole f¨ur jeden weiteren Basisvektor bi: e 1 i “ bi ´ xbi, e1y ¨ e1 ´ xbi, e2y ¨ e2 ´ . . . ´ xbi, ei´1y ¨ ei´1 ei “ e1 i ||e1 i || “ e1 ib xe1 i ,e1 i y 8.5 Abbildung eines Vektors in einer ONB Abbildung von V “ tv1, v2, ..., vnu bez¨uglich der ONB B “ tb1, b2, ..., bnu rV sB “ ¨ ˚ ˝ xV, b1y xV, b2y ... xV, bny ˛ ‹ ‚ 9. Quadratische Formen 9.1 Deﬁnition Quadratische Form Quadratische Formen sind bestimmte Funktionen, die mit einer symmetrischen Matrix A und einem Vektor x P R n gebildet werden. Es kommen maximal quadratische Terme vor. qapx1, x2, . . . , xnq “ qpxq “ x T Ax ‚ Quadratische Formen, die mit einer diagonalen Matrix ge- bildet werden (siehe qc) nennt man rein quadratisch. ‚ Die symmetrische Matrix A legt die Gestalt der entstehen- den Fl¨ache fest. Beispiele: qbpx1, x2q “ px1 x2q ˆ1 2 2 1 ˙ ˆx1 x2 ˙ “ x 2 1 ` 4x1x2 ` x 2 2 qcpx1, x2, x3q “ px1 x2 x3q ¨ ˝2 0 0 0 3 0 0 0 1 ˛ ‚ ¨ ˝x1 x2 x3 ˛ ‚ “ 2x2 1 ` 3x2 2 ` 1x2 3 9.2 Deﬁnitheit einer quadratischen Form Eine quadratische Form heisst: ‚ positiv deﬁnit: qpxq ° 0 @x ‰ 0 ‚ negativ deﬁnit: qpxq † 0 @x ‰ 0 ‚ positiv semideﬁnit: qpxq • 0 @x ‰ 0 ‚ negativ semideﬁnit: qpxq § 0 @x ‰ 0 ‚ indeﬁnit: sonst Um die Deﬁnitheit einer quadratische Form zu bestimmen, be- stimme man die Deﬁnitheit der zugeh¨origen symmetrischen Matrix A (siehe 9.3) 9.3 Deﬁnitheit einer symmetrischen Matrix Variante 1: Bestimmung der Eigenwerte Die erste M¨oglichkeit ist, die Deﬁnitheit durch die Eigenwerte zu bestimmen. Eine symmetrische Matrix heisst: ‚ positiv deﬁnit: Alle ⁄ ° 0 ‚ negativ deﬁnit: Alle ⁄ † 0 ‚ positiv semideﬁnit: Alle ⁄ • 0 ‚ negativ semideﬁnit: Alle ⁄ § 0 ‚ indeﬁnit: sonst Variante 2: Hurwitz-Kriterium Die zweite M¨oglichkeit ist, die Deﬁnitheit durch Bestimmung von Unterdeterimanten zu bestimmen: A “ ¨ ˝a b c d e f g h i ˛ ‚ ùñ A1 “ paq, A2 “ ˆa b d e ˙, A3 “ ¨ ˝a b c d e f g e h ˛ ‚ ‚ positiv deﬁnit: Alle detpAiq ° 0 f¨ur i “ 1, . . . , n ‚ negativ deﬁnit: Alle detpAiq † 0 f¨ur i “ 1, 3, 5, . . . Alle detpAiq ° 0 f¨ur i “ 2, 4, 6, . . . 9.4 Extrema einer quadratischen Form Kritische Punkte ﬁnden: Man setze den Gradienten der quadratischen Form gradpqpxqq “ p dq dx1 , dq dx2 , . . . , dq dxn qT “ 0. Durch L¨osen des Gleichungssystems erh¨alt man die Koordinaten der kritischen Punkte. Kritische Punkte zuordnen: 1 Bilde Hessesche Matrix in der richtigen Dimension f¨ur jeden kritischen Punkt: Bsp: H2x2 “ ¨ ˝ dqpxq2 d2 x1 dqpxq2 dx1 x2 dqpxq2 dx1 x2 dqpxq2 d2 x2 ˛ ‚ 2 Bestimme Deﬁnitheit der Matrix (siehe 9.3). positiv deﬁnit ùñ lokales Minimum negativ deﬁnit ùñ lokales Maximum indeﬁnit ùñ Sattelpunkt 9.5 Kegelschnitte und Quadriken Setzt man eine quadratische Form in eine Gleichung folgender Form ein (x P R n, a P Rn, b P R), erh¨alt man eine sogenann- te Quadrik: x T Ax ` a T x ` b “ 1 Ist die quadratische Form zweidimensional, erh¨alt man einen sogenannten Kegelschnitt, ist sie dreidimensional erh¨alt man eine Fl¨ache zweiten Grades. Durch Hauptachsentransformation und Translation ﬁndet man heraus, um welche Art Kegelschnitt/Quadrik es sich handelt. 9.6 Hauptachsentransformation einer quadr. Form Wir k¨onnen durch zwei Koordinatentransformationen (Dre- hung y “ T x und Verschiebung z “ y ` c) jede quadratische Form rein quadratisch machen. W¨ahrend der Koordinatenvektor x die quadratische Form in der Standardbasis darstellt, stellt der Koordinatenvektor z die quadratische Form in der neuen Basis dar. Die Basis, in der qpxq rein quadratisch wird, ist die Eigenbasis der zugeh¨origen symmetrischen Matrix A. Bsp: qpxq “ x2 1 ` x 2 2 ´ 3x2 3 ´ 6x1x2 Vorgehen: Je nach Aufgabe m¨ussen nicht alle Punkte durchgef¨uhrt wer- den. F¨ur ausschliesslich Hauptachsentransformation reicht 1-3. 1 Man bestimme die symmetrische Matrix A P R nˆn, sodass qpxq “ x T Ax Trick: ax 2 1 ` bx1x2 ` cx 2 2 ñ A “ ˆ a b{2 b{2 c ˙ s Bsp: A “ ¨ ˝ 1 ´3 0 ´3 1 0 0 0 ´3 ˛ ‚ 2 Man diagonalisiere die Matrix A (siehe 6.6) und bestim- me die Transformationsmatrix T . Da A symmetrisch ist, kann T orthogonal gew¨ahlt werden und T ´1 “ T T . T orthogonal w¨ahlen! Spalten von T normieren, falls zwei Eigenvektoren zum gleichen Eigenwert: 8.4 Bsp: D “ ¨ ˝´3 0 0 0 ´2 0 0 0 4 ˛ ‚, T “ ¨ ˝0 1{? 2 ´1{?2 0 1? 2 1?2 1 0 0 ˛ ‚ 3 Multipliziere aus: qpyq “ yT ¨ D ¨ y. Wir haben nun unsere Hauptachsentransformation durchgef¨uhrt. Bsp: yT Dy “ ´3y2 1 ´ 2y2 2 ` 4y2 3 4 Falls in Aufgabe gefragt: Bringe Quadrik qpxq ` aT x ` b “ 1 in Normalform. Bestimme a P Rn und b P R. Bsp: qpxq ` 2x3 ´ 1 3 “ 1 ñ a “ ¨ ˝0 0 2 ˛ ‚, b “ ´ 1 3 5 Schreibe Quadrik in transformierter Form (ausmultipli- zieren): yT Dy ` a T T y ` b “ 1 Bsp: yT Dy ` a T T y ` b “ ´3y2 1 ´ 2y2 2 ` 4y2 3 ` 2y1 ´ 1 3 “ 1 6 Falls noch lineare Terme ¨ubrig: Erg¨anze quadratisch Bsp: 0 “ ´3y2 1 ´ 2y2 2 ` 4y2 3 ` 2y1 ´ 4 3 “ ´3py2 1 ´ 2 3 y1q ´ 2y2 2 ` 4y2 3 ´ 4 3 “ ´3ppy1 ´ 2 2¨3 q2 ´ p 2 2¨3 q2q ´ 2y2 2 ` 4y2 3 ´ 4 3 “ ´3py1 ´ 2 2¨3 q2 ´ 2y2 2 ` 4y2 3 ´ 4 3 ` 3 ¨ p 1 3 q2 “ ´3py1 ´ 1 3 q2 ´ 2y2 2 ` 4y2 3 ´ 1 Durchf¨uhrung der zweiten Koordinatentransformation z “ y ` c (Verschiebung). Man bestimme Vektor c. Danach enth¨alt die Gleichung nur noch rein quadratische Terme. Bsp: c “ ¨ ˝´1{3 0 0 ˛ ‚ ùñ Qpzq “ ´3z2 1 ´ 2z2 2 ` 4z2 3 7 Falls gefragt: Gib die zusammengesetzte Koordinaten- transformation an: z “ T T x ` c Welche Hauptachse schneidet qpxq “ a ° 0 nicht? Die mit dem negativen Eigenwert. Jeder Vektor auf dieser Ach- se gibt in qpxq eingesetzt eine negative Zahl. Wie skizziere ich die Quadrik in Normalform? In Normalform ist es nicht schwer, mehrere Punkte einzusetzen und dann Linien durchzuziehen. Wie skizziere ich die Quadrik im urspr¨unglichen System? Skizziere zuerst in Normalform und transformiere Skizze mit Drehungsmatrix T und Verschiebungsvektor c. Welche Punkte sind dem Ursprung am n¨achsten? Falls Koordinatentransformation nur aus Drehung y “ T x bestand, sind die gleichen Punkte dem Ursprung am n¨achsten wie in der Normalform. 1 Man schreibe die Funktion in folgender Form: qpxq “ xT Ax ` aT ¨ x ` b, wobei A P Rnˆn, a P Rn, b P R Bsp: yT Dy ` aT T y ` b “ ´3y2 1 ´ 2y2 2 ` 4y2 3 ` 2y1 ´ 1 3 10. Kleinste Quadrate 10.1 Kleinste Quadrate Mit dem Prinzip der ” kleinsten Quadrate“ kann man zwar ¨uberbestimmte Gleichungssysteme nicht l¨osen, man kann je- doch eine m¨oglichst ” gute“ L¨osung ﬁnden, indem man den qua- dratischen Fehler minimiert. Bsp: 2x1 ` 3x2 “ 6 3x1 ` 4x2 “ 8 2x1 ` 1x2 “ 3 ñ ¨uberbestimmt Wir bilden die Diﬀerenz (= Fehler) aus der rechten und der linken Seite und nennen sie Residuenvektor r: 2x1 ` 3x2 ´ 6 “ r1 3x1 ` 4x2 ´ 8 “ r2 2x1 ` 1x2 ´ 3 “ r3 Wir suchen px1 x2qT , sodass ||r|| minimal. ñ quadratischer Fehler minimal Vorgehen: Meist ist das Gleichungssystem in Aufgabe bereits in Form A ¨ x ´ c “ r gegeben (siehe oben). 1 Man bestimme A und c Bsp: A “ ¨ ˝2 3 3 4 2 1 ˛ ‚, c “ ¨ ˝6 8 3 ˛ ‚ 2 Man berechne A T A und AT c Bsp: AT A “ ˆ17 20 20 2 ˙ , AT c “ ˆ42 53 ˙ 3 Man l¨ose das Gleichungssystem A T A ¨ x “ A T c Bsp: ˆ17 20 20 2 ˙ ¨ ˆx1 x2 ˙ “ ˆ42 53 ˙ ñ x “ ˆ 2.67 ´0.17 ˙ 10.2 QR-Zerlegung Mit der QR-Zerlegung kann eine beliebige Matrix A P Rmˆn in das Produkt einer orthogonalen Matrix Q P Rnˆn und einer oberen Rechtsdreiecksmatrix R P Rmˆn verwandelt werden: A “ Q ¨ R Vorgehen: Wir wollen nacheinander alle Elemente unterhalb der Haupt- diagonalen von A eliminieren. 1 Man w¨ahle zu eliminierendes Element und benenne es aij . Bsp: A “ ¨ ˝1 0 0 1 1 1 ˛ ‚ ùñ a31 soll eliminiert werden 2 Lese i, j ab und notiere ajj , aij Bsp: i “ 3, j “ 1 ùñ ajj “ 1, aij “ 1 3 Berechne w “ b a2 jj ` a2 ij Bsp: w “ ?12 ` 12 “ ?2 4 Man ﬁnde die richtige Rotationsmatrix Q 1T . Man neh- me zuerst die Identit¨atsmatrix I P Rmˆm und set- ze iii “ cosp–q, iij “ ´ sinp–q, iji “ sinp–q, ijj “ cosp–q. Bsp: I “ ¨ ˝1 0 0 0 1 0 0 0 1 ˛ ‚ ñ Q1T “ ¨ ˝ cosp–q 0 sinp–q 0 1 0 ´ sinp–q 0 cosp–q ˛ ‚ 5 Setze in Rotationsmatrix sinp–q “ aij w und cosp–q “ ajj w Bsp: Q 1T “ ¨ ˝ 1{?2 0 1{ ?2 0 1 0 ´1{? 2 0 1{?2 ˛ ‚ 6 Berechne Q 1T ¨ A “ A 1 Bsp: ¨ ˝ 1{?2 0 1{ ?2 0 1 0 ´1{? 2 0 1{?2 ˛ ‚¨ ¨ ˝1 0 0 1 1 1 ˛ ‚ “ ¨ ˝ ?2 1{?2 0 1 0 1{?2 ˛ ‚ 7 Falls A 1 keine obere Dreiecksmatrix, wiederhole (ﬁnde Q 2T etc.) bis alle n¨otigen Elemente eliminiert. Bsp: Q 2T “ ¨ ˝1 0 0 0 ?2{?3 1{?3 0 ´1{?3 ?2{?3 ˛ ‚, A2 “ ¨ ˝ ?2 1{?2 0 ?3{?2 0 0 ˛ ‚ 8 Wenn A2 “ R gefunden, berechne Q “ pQ2T ¨ Q 1T qT ùñ A “ Q ¨ R Kleinste Quadrate mit QR-Zerlegung L¨ost man ein Optimierungsproblem mit dem Computer, liefert das in 10.1 beschriebene Verfahren ungenaue L¨osungen (da nu- merisch instabil). Das L¨osungsverfahren mittels QR-Zerlegung ist besser. In Aufgabe nur machen, wenn explizit verlangt! Vorgehen: 1 Man bestimme A und c wie bei 10.1. Bsp: A “ ¨ ˝1 0 0 1 1 1 ˛ ‚, c “ ¨ ˝1 0 1 ˛ ‚ 2 Man f¨uhre die QR-Zerlegung durch A “ QR Bsp: Q “ ¨ ˝ 1{?2 1{?6 1{?3 0 ?2{?3 ´1{?3 ´1{? 2 1{?6 1{?3 ˛ ‚, R “ ¨ ˝ ?2 1{?2 0 ?3{?2 0 0 ˛ ‚ 3 Man berechne d “ Q T ¨ c Bsp: d “ Q T ¨ c “ ¨ ˝ 0?2{?3 2{? 3 ˛ ‚ 4 Man berechne l¨ose das Gleichungssystem R0 ¨ x “ d0, wobei R0 die extrahierte Dreiecksmatrix aus R ist und d0 die dazugeh¨origen oberen Eintr¨age von d Bsp: R0 “ ˆ?2 1{?2 0 ?3{?2 ˙ , d0 “ ˆ 0?2{?3 ˙ 5 Falls nach dem minimalen Fehler gefragt ist, berechne ||d1||2 Bsp: ||d1||2 “ b p 2?3 q2 “ 2?3 11. Lineare Diﬀ’gleichungssysteme 11.1 L¨osen von homogenem Diﬀ’gleichungssystem Man sucht eine L¨osung f¨ur ein System von Diﬀerentialglei- chungen, gegeben in folgender Form: ¨ ˝y1 1ptq y1 2ptq y1 3ptq ˛ ‚ “ ¨ ˝a11 a12 a13 a21 a22 a23 a31 a32 a33 ˛ ‚¨ ¨ ˝y1ptq y2ptq y3ptq ˛ ‚, ¨ ˝y1p0q y2p0q y3p0q ˛ ‚ Die Anfangsbedingungen yp0q sowie die Matrix A sei bekannt, gesucht ist yptq. Das Problem kann durch Transformation in Eigenbasis (Ent- kopplung) gel¨ost werden. Bsp: ˆy1 1ptq y1 2ptq ˙ “ ˆ3 2 1 4 ˙ ¨ ˆy1ptq y2ptq ˙ , ˆy1p0q y2p0q ˙ “ ˆ3 6 ˙ Vorgehen: (Transformation in Eigenbasis z “ T ´1 ¨ y) 1 Man diagonalisiere die Matrix A “ T DT ´1 (siehe 6.6) und bestimme die Transformationsmatrix T . Bsp: D “ ˆ2 0 0 5 ˙ , T “ ˆ´2 1 1 1 ˙ 2 Sei t piq die i-te Spalte von T und dii der i-te Diagonal- eintrag von D. Die L¨osung des Diﬀ’gleichungssystems lautet dann: yptq “ z1p0q ¨ t p1q ¨ e d11t ` z2p0q ¨ t p2q ¨ e d22t ` . . . Bsp: ˆy1ptq y2ptq ˙ “ z1p0q ¨ ˆ´2 1 ˙ ¨ e2t ` z2p0q ¨ ˆ1 1 ˙ ¨ e 5t 3 Variante 1: Bestimme T ´1, danach zp0q “ T ´1 ¨ yp0q. Variante 2: Bestimme zp0q durch L¨osen des Gleichungs- systems T ¨ zp0q “ yp0q. Falls keine Anfangsbedingungen gegeben, zip0q “ Ci. Bsp: ˆ´1{3 1{3 1{3 2{3 ˙ ¨ ˆ3 6 ˙ “ ˆ1 5 ˙ “ ˆz1p0q z2p0q ˙ 11.2 Umwandlung h¨ohere Ordnung in System 1. Ordnung Man will eine Diﬀerentialgleichung h¨oherer Ordnung in ein Dif- ferentialgleichungssystem 1. Ordnung umwandeln: Bsp: y3ptq ` 4 ¨ y2ptq ` 2 ¨ y1ptq ´ 3yptq “ 0 yp0q “ 1, y1p0q “ 3, y2p0q “ 2 Vorgehen: 1 Man substituiere y “ y0, y1 “ y1 etc. Die h¨ochste Ableitung lasse man stehen. Bsp: y3ptq ` 4 ¨ y2ptq ` 2 ¨ y1ptq ´ 3y0ptq “ 0 2 Man ersetze h¨ochste Ableitung durch einfache Ableitung mit Substitution. Bsp: y1 2ptq ` 4 ¨ y2ptq ` 2 ¨ y1ptq ´ 3y0ptq “ 0 3 Durch die Substitution hat man automatisch ein Diﬀ’gleichungssystem erster Ordnung erzeugt: Bsp: y1 0 “ y1 y1 1 “ y2 y1 2 “ 3 ¨ y0 ´ 2 ¨ y1 ´ 4 ¨ y2 L¨ose dies, wie in 11.1. Die gesuchte L¨osung ist y “ y0, also nur eine Zeile! 4 Zum Schluss substituiere noch die Anfangsbedingungen Bsp: y0p0q “ 1, y1p0q “ 3, y2p0q “ 2 11.3 L¨osen von inhomogenem Diﬀ’gleichungssystem Man hat bereits mit dem in 11.1 beschriebenen Verfahren die L¨osung yhptq f¨ur das homogene Diﬀ’gleichungssystem y1 “ A ¨ y gefunden. Jetzt sucht man die L¨osung f¨ur das inhomogene System: y1 “ A ¨ y ` b: Das Prinzip ist, dass man eine partikul¨are L¨osung ypptq ﬁndet, die die Diﬀ’gleichung sicher erf¨ullt. Die allgemeine L¨osung ist dann yptq “ yhptq ` ypptq Vorgehen: 1 Man nimmt an, dass die partikul¨are L¨osung ypptq kon- stant ist. Daraus folgt, dass y1 pptq “ 0. Man l¨ose also das Gleichungssystem A ¨ yp “ ´b 2 Man addiere die homogene und die Partikul¨are L¨osung zusammen: yptq “ yhptq ` ypptq 11.4 Bedingungen im Unendlichen F¨ur das Bestimmen der Konstanten Ci sind nicht immer nur Anfangsbedingungen yip0q gegeben, sondern manchmal auch Bedingungen wie lim tÑ8 yiptq “ a. Bsp: Bestimme Ci von yptq “ C1 ` 3C2e´t ` C3 ¨ e2t mit yp0q “ 2 und lim tÑ8 yptq “ 5 Vorgehen: 1 Verlangt eine Bedingung, dass yptq im Unendlichen be- schr¨ankt sein soll, setze Konstanten vor Exponential- funktionen mit positiven Exponenten null. Bsp: Zweite Bedingung lim tÑ8 yptq “ 5 ñ C3 “ 0 2 Man bestimme weitere Konstanten, indem man t Ñ 8 einsetzt. Bsp: lim tÑ8 yptq “ C1 “ 5 3 Man bestimme die ¨ubrigen Konstanten, indem man t “ 0 einsetzt. Bsp: yp0q “ 5 ` 3C2 “ 2 ñ C2 “ ´1 11.5 Komplexe L¨osungen Die Eigenwerte von A k¨onnen auch komplex sein. Ist dies der Fall, so gilt: Bsp: Gegeben sei y2pxq “ ´5ypxq ` 4y1pxq. Finde die allgemeine L¨osung des Systems. Vorgehen: 1 Substitution: Bsp: y0 “ y, y1 “ y1 2 H¨ochste Ableitung ist durch die DGL gegeben: Bsp: y1 1pxq “ ´5y0pxq ` 4y1pxq 3 Dadurch ergibt sich diese DGL-System mit zugeh¨origer Matrix: Bsp: y1 0 “ y1, y1 1 “ ´5y0 ` 4y1 ˆy1 0 y1 1 ˙ “ ˆ 0 1 ´5 4 ˙ ¨ ˆy0 y1 ˙ 4 Mittels Diagonalisierung der Matrix A ergeben sich die Eigenwerte und -vektoren: Bsp: ⁄1 “ 2 ` i, ⁄2 “ 2 ´ iv⁄1 “ ˆ2 ´ i 5 ˙, v⁄2 “ ˆ2 ` i 5 ˙ 5 Daraus erh¨alt man die allgemeine L¨osung: Bsp: ˆy0 y1 ˙ “ c1ep2`iqx ¨ ˆ2 ´ i 5 ˙ ` c2ep2´iqx ¨ ˆ2 ` i 5 ˙ 6 Es gilt: e ˘i “ cospxq ˘ i ¨ sinpxq Bsp:ˆy0 y1 ˙ “ c1e2x ¨ pcospxq ` i ¨ sinpxqq ¨ ˆ2 ´ i 5 ˙ ` c2e2x ¨ pcospxq ´ i ¨ sinpxqq ¨ ˆ2 ` i 5 ˙ 7 Auch hier ist nur die erste Zeile die gesuchte L¨osung: y0 “ ypxq “ c1e–x ¨ pcosp—xq ` i ¨ sinp—xqq ¨ v1,y0 ` c2e–x ¨ pcosp—xq ´ i ¨ sinp—xqq ¨ v2,y0 ypxq “ e –xpc1 ¨ cosp—xq ` c2sinp—xqq Bsp: y0 “ ypxq “ c1e2x ¨ pcospxq ` i ¨ sinpxqq ` c2e2x ¨ pcospxq ´ i ¨ sinpxqq ypxq “ e 2xpc1 ¨ cospxq ` c2sinpxqq 12. Pers¨onliche Erg¨anzungen 12.1 Beispiel L-R Zerlegung A “ ¨ ˝ 2 ´1 ´3 6 1 ´10 ´2 ´7 8 ˛ ‚; b “ ¨ ˝ 4 ´1 25 ˛ ‚ ñ L/R- Schreibweise¨ ˝ 1 0 0 2 ´1 ´3 ´ 1 0 6 1 ´10 ´ ´ 1 ´2 ´7 8 ˛ ‚ L R ¨ ˝ 1 0 0 2 ´1 ´3 3 1 0 0 4 ´1 ´1 ´ 1 0 ´8 5 ˛ ‚ L R ¨ ˝ 1 0 0 2 ´1 ´3 3 1 0 0 4 ´1 ´1 ´2 1 0 0 3 ˛ ‚ L R L ¨ y “ b ¨ ˝ 1 0 0 3 1 0 ´1 ´2 1 ˛ ‚¨ ¨ ˝y1 y2 y3 ˛ ‚ “ ¨ ˝ 4 ´1 25 ˛ ‚ ñ y “ ¨ ˝ 4 ´13 3 ˛ ‚ R ¨ x “ y ¨ ˝2 ´1 ´3 0 4 ´1 0 0 3 ˛ ‚¨ ¨ ˝x1 x2 x3 ˛ ‚ “ ¨ ˝ 4 ´13 3 ˛ ‚ ñ x “ ¨ ˝ 2 ´3 1 ˛ ‚ 12.2 Beispiel P-L-R-Zerlegung ¨ ˝1 0 0 1 0 0 5 1 2 0 1 0 0 1 0 2 2 5 1 0 0 1 0 0 1 ´4 0 6 ˛ ‚ P L R ¨ ˝1 0 0 1 0 0 5 1 2 0 1 0 2 5 1 0 0 0 1 5 0 0 1 ´4 5 0 1 0 4 5 38 5 ˛ ‚ P L R ¨ ˝1 0 0 1 0 0 5 1 2 0 0 1 ´4 5 1 0 0 4 5 38 5 0 1 0 2 5 0 1 0 0 1 5 ˛ ‚ P L R ñ L¨y “ P ¨ b ñ R ¨x “ y 12.3 Beispiel Unterraum Sei V “ Rn und A eine nxn-Matrix. Wir betrachten als Teilmenge U von V die L¨osungsmenge A ¨ x “ 0. Ist U ein Unterraum von V? Seien a,b zwei L¨osungen von A ¨ x “ 0. Dann gilt A ¨ a “ 0 und A ¨ b “ 0. ñ Bedingung 1: A ¨ pa ` bq “ A ¨ a ` A ¨ b “ 0 ñ erf¨ullt ñ Bedingung 2: A ¨ p– ¨ aq “ – ¨ A ¨ a “ – ¨ 0 “ 0 ñ erf¨ullt U ist also ein Unterraum von V. 12.4 Beispiel Basiswechsel Matrix Seien Q “ pe1, e2q die Standardbasis und W “ pe1 ` e2, e1 ´ e2q und rF sQ “ ˆ1 2 3 4 ˙, berechne rF sW ! TW ÑQ “ prw1sQ, rw2sQ, ..., rwnsQq “ ˆ1 ´1 1 1 ˙, T ´1 W ÑQ “ TQÑW “ ... “ ˆ 1 2 1 2 ´1 2 1 2 ˙ rF sW “ TW ÑQ ¨ rF sQ ¨ TQÑW “ ˆ 1 2 1 2 ´1 2 1 2 ˙ ¨ ˆ1 2 3 4 ˙ ¨ ˆ1 ´1 1 1 ˙ “ ˆ2 3 1 1 ˙ ¨ ˆ1 ´1 1 1 ˙ “ ˆ5 1 2 0 ˙ 12.5 Beispiel Linearit¨at Ist die folgende Abbildung linear? F1 : P3 Ñ P3, ppxq Ñ ppxq ` 1 F1pppxq`qpxqq “ ppxq`qpxq`1 “ rppxq`1s`rqpxq`1s Ñ keine lineare Abbildung 12.6 Berechnung von A kx Sei A P Cnˆn diagonalisierbar und x P Cn und man will y “ Akx berechnen, so muss man folgende Schritte durch- laufen: 1 L¨ose das Eigenwertproblem von A, so dass T ´1AT “ D 2 L¨ose das LGS T ¨ z “ x nach z 3 Berechne Dk, dann gilt w “ Dk ¨ z 4 Dann ist y “ T ¨ w 5 Falls A symmetrisch ist, kann man auch T orthogonal w¨ahlen, in diesem Fall gilt Ak “ T DkT T 6 Falls A nicht symmetrisch ist, kann man T nicht ortho- gonal w¨ahlen, es gilt A k “ T DkT ´1 7 Die Eigenwerte von Ak sind ⁄ k 1 , ⁄ k 2 , ..., ⁄ k n 8 Jetzt kann man mit der neu berechneten Matrix das LGS l¨osen Beispiel: A “ ¨ ˚ ˝ 2 0 0 1 0 2 0 0 0 0 2 0 1 0 0 2 ˛ ‹ ‚ Berechne A 3 1 Eigenwertproblem l¨osen (wurde schon gemacht): T “ ¨ ˚ ˚ ˝ 0 0 1?2 1?2 1 0 0 0 0 1 0 0 0 0 1?2 ´ 1?2 ˛ ‹ ‹ ‚ D “ ¨ ˚ ˝ 2 0 0 0 0 2 0 0 0 0 3 0 0 0 0 1 ˛ ‹ ‚ 2 2. - 4. wird nicht ben¨otigt 3 Schritt 5. A 3 “ T D3T T ¨ ˚ ˚ ˝ 0 0 1?2 1?2 1 0 0 0 0 1 0 0 0 0 1?2 ´ 1?2 ˛ ‹ ‹ ‚ ¨ ¨ ˚ ˝ 2 0 0 0 0 2 0 0 0 0 3 0 0 0 0 1 ˛ ‹ ‚ 3 ¨ ¨ ˚ ˚ ˝ 0 1 0 0 0 0 1 0 1?2 0 0 1?2 1?2 0 0 ´ 1?2 ˛ ‹ ‹ ‚ (yn-1' = yn) “ ¨ ˚ ˚ ˝ 0 0 1?2 1?2 1 0 0 0 0 1 0 0 0 0 1?2 ´ 1?2 ˛ ‹ ‹ ‚ ¨ ¨ ˚ ˝ 8 0 0 0 0 8 0 0 0 0 27 0 0 0 0 1 ˛ ‹ ‚ 3 ¨ ¨ ˚ ˚ ˝ 0 1 0 0 0 0 1 0 1?2 0 0 1?2 1?2 0 0 ´ 1?2 ˛ ‹ ‹ ‚ “ ¨ ˚ ˚ ˝ 0 0 27? 2 1?2 8 0 0 0 0 8 0 0 0 0 27? 2 ´ 1?2 ˛ ‹ ‹ ‚¨ ¨ ˚ ˚ ˝ 0 1 0 0 0 0 1 0 1?2 0 0 1?2 1?2 0 0 ´ 1?2 ˛ ‹ ‹ ‚ “ ¨ ˚ ˝ 14 0 0 13 0 8 0 0 0 0 8 0 13 0 0 14 ˛ ‹ ‚ 12.7 Berechnung von e A Sei A P R nˆn symetrisch dann gilt e A “ 8ÿ n“0 An n! ; T ´1AT “ D Daraus entsteht: eA “ ... “ T ¨ diagped1 , ..., e dn q ¨ T ´1 Besondere Eigenschaften: ‚ e AT “ peAqT ‚ Falls e tA stetig diﬀ’bar: d dt petAq “ A ¨ etA ‚ peAq´1“e´A ‚ e P ´1AP “ P ´1eAP ‚ detpe Aq “ e spurpAq Beispiel: Sei A “ ˆ5 ´6 3 ´4 ˙ ‚ Eigenwertproblem l¨osen PAp⁄q “ detpA ´ ⁄Iq “ detpˆ5 ´ ⁄ ´6 3 ´4 ´ ⁄ ˙ “ p⁄ ´ 2qp⁄ ` 1q ñ ⁄1 “ 2, ⁄2 “ ´1 E⁄1 “ ... “ ˆ2 1 ˙ E⁄2 “ ... “ ˆ1 1 ˙ ñ D “ ˆ2 0 0 ´1 ˙ , T “ ˆ2 1 1 1 ˙ T ´1 “ 1 detpT q ¨ ˆ d ´b ´c a ˙ “ ˆ 1 ´1 ´1 2 ˙ ‚ eA “ ... “ T ¨ diagped1 , ..., edn qT ´1 eA “ ˆ2 1 1 1 ˙ ¨ ˆe 2 0 0 e ´1 ˙ ¨ ˆ 1 ´1 ´1 2 ˙ “ ˆ2e2 e´1 e2 e´1 ˙ ¨ ˆ 1 ´1 ´1 2 ˙ “ ˆ2e´2 ´ e ´1 ´2e2 ` 2e´1 e2 ´ e ´1 ´e 2 ` 2e´1 ˙ 12.8 Beispiel lineares inhomogenes DGL System Ein inhomogenes lineares Gleichungssystem ist von der Form y1 “ A ¨ y ` b Verfahren: 1 Man l¨ost das homogene DGL-System gem¨ass bekannten Strukturen “ yhpxq 2 Man muss nun irgendeine passende partikul¨are L¨osung yppxq ﬁnden 3 Die Gesamtl¨osung ist ypxq “ yhpxq`yppxq Beispiel: Gegeben sei y1 1 “ ´y1 ` y2 y1 2 “ y1 ´ y2 ` y3 y1 3 “ y2 ´ y3 ` 1 Es gilt ¨ ˝y1 y2 y3 ˛ ‚ 1 “ ¨ ˝´1 1 0 1 ´1 1 0 1 ´1 ˛ ‚¨ ¨ ˝y1 y2 y3 ˛ ‚ ⁄1 “ ´1, ⁄2 “ ´1 ` ?2, ⁄3 “ ´1 ´ ?2 E⁄1 “ ¨ ˝ 1 0 ´1 ˛ ‚, E⁄2 “ ¨ ˝ 1? 2 1 ˛ ‚, E⁄3 “ ¨ ˝ 1 ´ ?2 1 ˛ ‚ und somit yhpxq “ c1e´x ¨ ˝ 1 0 ´1 ˛ ‚ `c2ep1` ?2qx ¨ ˝ 1?2 1 ˛ ‚` c3ep´1` ?2qx ¨ ˝ 1 ´ ?2 1 ˛ ‚ Nun zu yppxq y1 p “ ¨ ˝0 0 0 ˛ ‚ “ ¨ ˝´1 1 0 1 ´1 1 0 1 ´1 ˛ ‚¨ ¨ ˝y1 y2 y3 ˛ ‚´ ¨ ˝0 0 0 ˛ ‚ Also ¨ ˝´1 1 0 1 ´1 1 0 1 ´1 ˛ ‚¨ ¨ ˝y1 y2 y3 ˛ ‚ “ ¨ ˝0 0 1 ˛ ‚ Ñ yp “ ¨ ˝1 1 0 ˛ ‚ Mit ypxq “ yhpxq ` yppxq folgt yhpxq “ c1e ´x ¨ ˝ 1 0 ´1 ˛ ‚` c2ep1` ?2qx ¨ ˝ 1?2 1 ˛ ‚` c3ep´1` ?2qx ¨ ˝ 1 ´ ?2 1 ˛ ‚` ¨ ˝1 1 0 ˛ ‚ 13. Anhang 13.1 Givens-Rotationsmatrizen Drehung um x-Achse:¨ ˝1 0 0 0 cosp–q ´sinp–q 0 sinp–q cosp–q ˛ ‚ Drehung um y-Achse:¨ ˝ cosp–q 0 sinp–q 0 1 0 ´sinp–q 0 cosp–q ˛ ‚ Drehung um z-Achse:¨ ˝cosp–q ´sinp–q 0 sinp–q cosp–q 0 0 0 1 ˛ ‚ 13.2 Abbildungsmatrizen Spiegelung an x=y Diagonalebene:¨ ˝0 1 0 1 0 0 0 0 1 ˛ ‚ Orthogonalprojektion auf die y-Achse:¨ ˝1 0 0 0 0 0 0 0 1 ˛ ‚","libVersion":"0.3.2","langs":""}