{"path":"sem2/PProg/VRL/slides/PProg-L22-consistency-models.pdf","text":"spcl.inf.ethz.ch @spcl_eth TORSTEN HOEFLER Parallel Programming List-based sets and more source: xkcd.com spcl.inf.ethz.ch @spcl_eth ▪ Lock granularity in practice ▪ Fun with lists ☺ ▪ Fine-gained locking ▪ Principle of locking subregions of structures ▪ Optimistic locking/synchronization ▪ Principle of re-checking conditions after locking ▪ Lazy structure updates/synchronization ▪ Principle of atomic markers (deletion, linked) ▪ Decouple structure update from detection by other threads through markers Summary/recap last lecture 2 spcl.inf.ethz.ch @spcl_eth ▪ Lock-free datastructures ▪ Warmup with a stack ▪ Bounded queues in practice (as OS structure) ▪ The ABA problem ▪ (Really somewhat of a wart following CAS) ▪ LL/SC-like approaches don’t have this issue ▪ We nevertheless need to know it ▪ Avoiding the ABA problem Learning goals today 3 spcl.inf.ethz.ch @spcl_eth compare old with data at memory location if and only if data at memory equals old overwrite data with new return previous memory value (in Java: return whether CAS succeeded) int CAS (memref a, int old, int new) oldval = mem[a]; if (old == oldval) mem[a] = new; return oldval; CAS (again)atomic CAS is more powerful than TAS as we will see later CAS can be implemented wait-free (!) by hardware [1]. 4[1] Kurth et al.: ATUNs: Modular and Scalable Support for Atomic Operations in a Shared Memory Multiprocessor, DAC’20 spcl.inf.ethz.ch @spcl_eth public class CasCounter { private AtomicInteger value; public int getVal() { return value.get(); } // increment and return new value public int inc() { int v; do { v = value.get(); } while (!value.compareAndSet(v, v+1)); return v+1; } } Mechanism (a) read current value v (b) modify value v' (c) try to set with CAS (d) return if success restart at (a) otherwise Positive result of CAS of (c) suggests that no other thread has written between (a) and (c) Non-blocking counter Deadlock/Starvation? Assume one thread dies. Does this affect other threads? What happens if some processes see the same value? 5 Why not “guarantees”? spcl.inf.ethz.ch @spcl_eth Positive result of CAS suggests that no other thread has written It is not always true, as we will find out (→ ABA problem). However, it is still THE mechanism to check for exclusive access in lock-free programming. Sidenotes: ▪ maybe transactional memory will become competitive at some point ▪ LL/SC or variants thereof provide stronger semantics avoiding ABA Handle CAS with care 6 spcl.inf.ethz.ch @spcl_eth Lock-Free Stack 7 spcl.inf.ethz.ch @spcl_eth public static class Node { public final Long item; public Node next; public Node(Long item) { this.item = item; } public Node(Long item, Node n) { this.item = item; next = n; } } Stack Node item next item next item next NULL 8 spcl.inf.ethz.ch @spcl_eth public class BlockingStack { Node top = null; synchronized public void push(Long item) { top = new Node(item, top); } synchronized public Long pop() { if (top == null) return null; Long item = top.item; top = top.next; return item; } } Blocking Stack top item next item next item next NULL 9 spcl.inf.ethz.ch @spcl_eth public class ConcurrentStack { AtomicReference<Node> top = new AtomicReference<Node>(); public void push(Long item) { … } public Long pop() { … } } Non-blocking Stack top item next item next item next NULL 10 spcl.inf.ethz.ch @spcl_eth public Long pop() { Node head, next; do { head = top.get(); if (head == null) return null; next = head.next; } while (!top.compareAndSet(head, next)); return head.item; } Pop A B C NULL top head next 11 spcl.inf.ethz.ch @spcl_eth public void push(Long item) { Node newi = new Node(item); Node head; do { head = top.get(); newi.next = head; } while (!top.compareAndSet(head, newi)); } Push A B C NULL top head newi 12 spcl.inf.ethz.ch @spcl_eth Lock-free programs are deadlock-free by design. How about performance? n threads 100,000 push/pop operations 10 times What's the benefit? 0 5000 10000 15000 20000 25000 30000 35000 40000 45000 50000 0 20 40 60 80 100 120 140 locked/ blocking lock-free #threads time (ms) 13 public void push(Long item) { Node newi = new Node(item); Node head; do { head = top.get(); newi.next = head; } while (!top.compareAndSet(head, newi)); } spcl.inf.ethz.ch @spcl_eth A lock-free algorithm does not automatically provide better performance than its blocking equivalent! Atomic operations are expensive and contention can still be a problem. → Backoff, again. Performance 14 spcl.inf.ethz.ch @spcl_eth With backoff 0 5000 10000 15000 20000 25000 30000 35000 40000 45000 50000 0 20 40 60 80 100 120 140 locked/ blocking lock-free lock-free with backoff ☺ #threads time (ms) 15 spcl.inf.ethz.ch @spcl_eth LOCK FREE LIST-BASED SET (NOT SKIP LIST!) Some of the material from \"Herlihy: Art of Multiprocessor Programming\" 16 spcl.inf.ethz.ch @spcl_eth A: remove(c) B: add(b') ok? Does this work? a b c d A: CAS(b.next,c,d) b' CAS decides who wins → this seems to work B: CAS(b.next,c,b') So does this CAS approach work generally?? 17 spcl.inf.ethz.ch @spcl_eth A: remove(c) B: remove(b) c not deleted!  Another scenario a b c d CASCAS 18 spcl.inf.ethz.ch @spcl_eth A: remove(c) B: add(c') c' not added!  Mark bit approach? a b c d B: CAS(c.next,d,c') c' A: CAS(b.next,c,d) A: CAS(c.mark,false,true) B: c.mark ? 19 spcl.inf.ethz.ch @spcl_eth The difficulty that arises in this and many other problems is: ▪ We cannot (or don't want to) use synchronization via locks ▪ We still want to atomically establish consistency of two things Here: mark bit & next-pointer The problem 20 spcl.inf.ethz.ch @spcl_eth Java.util.concurrent.atomic AtomicMarkableReference<V> { boolean attemptMark(V expectedReference, boolean newMark) boolean compareAndSet(V expectedReference, V newReference, boolean expectedMark, boolean newMark) V get(boolean[] markHolder) V getReference() boolean isMarked() set(V newReference, boolean newMark) } The Java solution DCAS on V and mark 21 address F mark bitreference 2 64Bytes=562,949,953,421,312 Petabytes spcl.inf.ethz.ch @spcl_eth ▪ Atomically ▪ Swing reference and ▪ Update flag ▪ Remove in two steps ▪ Set mark bit in next field ▪ Redirect predecessor’s pointer The algorithm using AtomicMarkableReference 22 spcl.inf.ethz.ch @spcl_eth A: remove(c) Algorithm idea a b c d 1. try to set mark (c.next) 2. try CAS( [b.next.reference, b.next.marked], [c,unmarked], [d,unmarked]); ①Mark ②DCAS 23 Why “try to”? How can it fail? What then? spcl.inf.ethz.ch @spcl_eth A: remove(c) B: remove(b) c remains marked  (logically deleted) It helps! a b c d 1. try to set mark (c.next) 2. try CAS( [b.next.reference, b.next.marked], [c,unmarked], [d,unmarked]); 1. try to set mark (b.next) 2. try CAS( [a.next.reference, a.next.marked], [b,unmarked], [c,unmarked]); ①Mark ②DCAS fails! ①Mark ②DCAS 24 spcl.inf.ethz.ch @spcl_eth Q: what do you do when you find a “logically” deleted node in your path? A: finish the job. CAS the predecessor’s next field Proceed (repeat as needed) Traversing the list a c d 25 spcl.inf.ethz.ch @spcl_eth public Window find(Node head, int key) { Node pred = null, curr = null, succ = null; boolean[] marked = {false}; boolean snip; while (true) { pred = head; curr = pred.next.getReference(); boolean done = false; while (!done) { marked = curr.next.get(marked); succ = marked[1:n]; // pseudo-code to get next ptr while (marked[0] && !done) { // marked[0] is marked bit if pred.next.compareAndSet(curr, succ, false, false) { curr = succ; marked = curr.next.get(marked); succ = marked[1:n]; } else done = true; } if (!done && curr.key >= key) return new Window(pred, curr); pred = curr; curr = succ; } } } Find node class Window { public Node pred; public Node curr; Window(Node pred, Node curr) { this.pred = pred; this.curr = curr; } }loop over nodes until position found if marked nodes are found, delete them, if deletion fails restart from the beginning 26 spcl.inf.ethz.ch @spcl_eth public boolean remove(T item) { Boolean snip; while (true) { Window window = find(head, key); Node pred = window.pred, curr = window.curr; if (curr.key != key) { return false; } else { Node succ = curr.next.getReference(); snip = curr.next.attemptMark(succ, true); if (!snip) continue; pred.next.compareAndSet(curr, succ, false, false); return true; } } } Remove Find element and prev element from key If no such element -> return false Otherwise try to logically delete (set mark bit). Try to physically delete the element, ignore result If no success, restart from the very beginning a b c d ① ② ① ② 27 spcl.inf.ethz.ch @spcl_eth public boolean add(T item) { boolean splice; while (true) { Window window = find(head, key); Node pred = window.pred, curr = window.curr; if (curr.key == key) { return false; } else { Node node = new Node(item); node.next = new AtomicMarkableRef(curr, false); if (pred.next.compareAndSet(curr, node, false, false)) return true; } } } Add Find element and prev element from key If element already exists, return false Otherwise create new node, set next / mark bit of the element to be inserted and try to insert. If insertion fails (next set by other thread or mark bit set), retry 28 spcl.inf.ethz.ch @spcl_eth ▪ We used a special variant of DCAS (double compare and swap) in order to be able check two conditions at once. This DCAS was possible because one bit was free in the reference. ▪ We used a lazy operation in order to deal with a consistency problem. Any thread is able to repair the inconsistency. If other threads would have had to wait for one thread to cleanup the inconsistency, the approach would not have been lock-free! ▪ This «helping» is a recurring theme, especially in wait-free algorithms where, in order to make progress, threads must help others (that may be off in the mountains ☺) Observations 29 spcl.inf.ethz.ch @spcl_eth LOCK FREE UNBOUNDED QUEUE 30 spcl.inf.ethz.ch @spcl_eth At the heart of an Operating System is a scheduler. A scheduler basically moves tasks between queues (or similar data structures) and selects threads to run on a processor core. Scheduling decisions usually happen when threads are created threads end threads block / wait threads unblock 31 Motivation: a Lock-Free Operating System Kernel Processor Core Processor Core Processor Core wait for x wait for y run run run ready Queue spcl.inf.ethz.ch @spcl_eth Data structures of a runtime or kernel need to be protected against concurrent access by ▪ threads and ▪ interrupt service routines on different cores. Conventionally, (spin-)locks are employed for protection. The granularity varies. 32 Motivation: a Lock-Free Operating System Kernel Processor Core Processor Core Processor Core run run run wait for x wait for y ready spcl.inf.ethz.ch @spcl_eth If we want to protect scheduling queues in a lock-free way, we obviously need - an implementation of a lock-free unbounded queue We will again meet the problem of transient inconsistencies - If we want to use the queues in a scheduler, usually we cannot rely on Garbage Collection, we need to reuse elements of the queue This will lead to a difficult problem, the ABA problem 33 Motivation: a Lock-Free Operating System Kernel spcl.inf.ethz.ch @spcl_eth public class Node<T> { public T item; public Node<T> next; public Node(T item) { this.item = item; next = null } } 34 Queue Node item next item next item next NULL spcl.inf.ethz.ch @spcl_eth public class BlockingQueue<T> { Node<T> head, tail; public synchronized void Enqueue(T item) { } public synchronized T Dequeue() { } } 35 Blocking Queue item next item next item next NULL head tail spcl.inf.ethz.ch @spcl_eth public synchronized void Enqueue(T item) { Node<T> node = new Node<T>(item); if (tail != null) tail.next = node; else head = node; tail = node; } case tail = null case tail != null 36 Enqueue node node node node node node head tail new ① ② head tail new ① ② spcl.inf.ethz.ch @spcl_eth 37 Dequeue case head == tail case head != tail node node node node node node head tail① ② node head tail ① public synchronized T Dequeue() { T item = null; Node<T> node = head; if (node != null) { item = node.item; head = node.next; if (head == null) tail = null; } return item; } spcl.inf.ethz.ch @spcl_eth It turns out that when we want to implement a lock-free queue like this, we run into problems because of potentially simultaneous updates of ▪ head ▪ tail ▪ tail.next How to solve this? 38 Observation spcl.inf.ethz.ch @spcl_eth 39 Idea: Sentinel at the front S node node node node node head tail spcl.inf.ethz.ch @spcl_eth 40 Sentinel at the front: Enqueue S node node node node node head tail node ① ② S head tail node ① ② operations • read/write tail.next • read/write tail empty Q nonempty Q spcl.inf.ethz.ch @spcl_eth 41 Sentinel at the front: Dequeue S node node node node node head tail S head tail node ① Read value ① Read value ② ② 1 element n elements operations • reading head.next • read/write head spcl.inf.ethz.ch @spcl_eth Still have to update two pointers at a time! ▪ But enqueuers work on tail and dequeuers on head Possible inconsistency? ▪ tail might (transiently) not point to the last element What's the problem with this? ▪ Unacceptable that any thread has to wait for the consistency to be established -- this would be locking camouflaged Solution ▪ Threads help making progress 42 Does this help? spcl.inf.ethz.ch @spcl_eth public class Node<T> { public T item; public AtomicReference<Node> next; public Node(T item) { next = new AtomicReference<Node>(null); this.item = item; } public void SetItem(T item) { this.item = item; } } 43 Queue Node needs Atomic next pointer item next item next item next NULL spcl.inf.ethz.ch @spcl_eth public class NonBlockingQueue extends Queue { AtomicReference<Node> head = new AtomicReference<Node>(); AtomicReference<Node> tail = new AtomicReference<Node>(); public NonBlockingQueue() { Node node = new Node(null); head.set(node); tail.set(node); } public void Enqueue(T item); public T Dequeue(); } 44 Queue item next item next item next NULL spcl.inf.ethz.ch @spcl_eth Enqueuer ▪ read tail into last ▪ then tries to set last.next: CAS(last.next, null, new) ▪ If unsuccessful retry! ▪ If successful, try to set tail without retry CAS(tail, last, new) Dequeuer ▪ read head into first ▪ read first.next into next ▪ if next is available, read the item value of next ▪ try to set head from first to next CAS(head, first, next) ▪ If unsuccessful, retry! 45 Protocol: Initial Version node tail node ① ② S node head ① Read value ② spcl.inf.ethz.ch @spcl_eth 46 Protocol node tail node ① ② How can this be unsuccessful? 1. Some other thread has written last.next just before me 2. I have read a stale version of tail either a) because I just missed the update of other thread b) because the other thread failed in updating tail, for example because it has died If the thread dies before calling this, tail is never updated. Enqueuer ▪ read tail into last ▪ then tries to set last.next: CAS(last.next, null, new) ▪ If unsuccessful retry! ▪ If successful, try to set tail without retry CAS(tail, last, new) spcl.inf.ethz.ch @spcl_eth 47 Protocol How can this be unsuccessful? 1. another thread has already removed next S node head ① Read value ② Dequeuer ▪ read head into first ▪ read first.next into next ▪ if next is available, read the item value of next ▪ try to set head from first to next CAS(head, first, next) ▪ If unsuccessful, retry! spcl.inf.ethz.ch @spcl_eth ▪ Thread A enqueues an element to an empty list, but has not yet adapted tail ▪ Thread B dequeues (the sentinel) ▪ Now tail points to a dequeued element. 48 One more possible inconsistency S tail node head S tail node head spcl.inf.ethz.ch @spcl_eth public void enqueue(T item) { Node node = new Node(item); while(true) { Node last = tail.get(); Node next = last.next.get(); if (next == null) { if (last.next.compareAndSet(null, node)) { tail.compareAndSet(last, node); return; } } else tail.compareAndSet(last, next); } } 49 Final solution: enqueue Create the new node Read current tail as last and last.next as next Try to set last.next from null to node, if success then try to set tail Ensure progress by advancing tail pointer if required and retry Help other threads to make progress ! spcl.inf.ethz.ch @spcl_eth public T dequeue() { while (true) { Node first = head.get(); Node last = tail.get(); Node next = first.next.get(); if (first == last) { if (next == null) return null; else tail.compareAndSet(last, next); } else { T value = next.item; if (head.compareAndSet(first, next)) return value; } } } 50 Final solution: dequeue Read head as first, tail as last and first.next as next Check if queue looks empty (1) really empty: return (2) next available: advance last pointer If queue is not empty, memorize value on next element and try to remove current sentinel Retry if removal was unsuccessful Help other threads to make progress ! spcl.inf.ethz.ch @spcl_eth REUSE AND THE ABA PROBLEM 51 spcl.inf.ethz.ch @spcl_eth public class ConcurrentStack { AtomicReference<Node> top = new AtomicReference<Node>(); public void push(Long item) { … } public Long pop() { … } } 52 For the sake of simplicity: back to the stack ☺ item next item next item next NULL top spcl.inf.ethz.ch @spcl_eth public Long pop() { Node head, next; do { head = top.get(); if (head == null) return null; next = head.next; } while (!top.compareAndSet(head, next)); return head.item; } 53 pop A B C NULL top head next Memorize \"current stack state\" in local variable head Action is taken only if \"the stack state\" did not change spcl.inf.ethz.ch @spcl_eth public void push(Long item) { Node newi = new Node(item); Node head; do { head = top.get(); newi.next = head; } while (!top.compareAndSet(head, newi)); } 54 push A B C NULL top head newi Memorize \"current stack state\" in local variable head Action is taken only if \"the stack state\" did not change spcl.inf.ethz.ch @spcl_eth Assume we do not want to allocate for each push and maintain a node pool instead (e.g., inside the OS or in an unmanaged language). Does this work? 55 Node reuse public class NodePool { AtomicReference<Node> top new AtomicReference<Node>(); public void put(Node n) { … } public Node get() { … } } public class ConcurrentStackP { AtomicReference<Node> top = newAtomicReference<Node>(); NodePool pool = new NodePool(); ... } spcl.inf.ethz.ch @spcl_eth public Node get(Long item) { Node head, next; do { head = top.get(); if (head == null) return new Node(item); next = head.next; } while (!top.compareAndSet(head, next)); head.item = item; return head; } public void put(Node n) { Node head; do { head = top.get(); n.next = head; } while (!top.compareAndSet(head, n)); } 56 NodePool put and get Only difference to Stack above: NodePool is in-place. A node can be placed in one and only one in-place data structure. This is ok for a global pool. So far this works. spcl.inf.ethz.ch @spcl_eth public void push(Long item) { Node head; Node new = pool.get(item); do { head = top.get(); new.next = head; } while (!top.compareAndSet(head, new)); } public Long pop() { Node head, next; do { head = top.get(); if (head == null) return null; next = head.next; } while (!top.compareAndSet(head, next)); Long item = head.item; pool.put(head); return item; } 57 Using the node pool spcl.inf.ethz.ch @spcl_eth ▪ run n consumer and producer threads ▪ each consumer / producer pushes / pops 10,000 elements and records sum of values ▪ if a pop returns an \"empty\" value, retry ▪ do this 10 times with / without node pool ▪ measure wall clock time (ms) ▪ check that sum of pushed values == sum of popped values 58 Experiment spcl.inf.ethz.ch @spcl_eth nonblocking stack without reuse n = 1, elapsed= 15, normalized= 15 n = 2, elapsed= 110, normalized= 55 n = 4, elapsed= 249, normalized= 62 n = 8, elapsed= 843, normalized= 105 n = 16, elapsed= 1653, normalized= 103 n = 32, elapsed= 3978, normalized= 124 n = 64, elapsed= 9953, normalized= 155 n = 128, elapsed= 24991, normalized= 195 nonblocking stack with reuse n = 1, elapsed= 47, normalized= 47 n = 2, elapsed= 109, normalized= 54 n = 4, elapsed= 312, normalized= 78 n = 8, elapsed= 577, normalized= 72 n = 16, elapsed= 1747, normalized= 109 n = 32, elapsed= 2917, normalized= 91 n = 64, elapsed= 6599, normalized= 103 n = 128, elapsed= 12090, normalized= 94 59 Result (of one particular run) yieppieh... spcl.inf.ethz.ch @spcl_eth nonblocking stack with reuse n = 1, elapsed= 62, normalized= 62 n = 2, elapsed= 78, normalized= 39 n = 4, elapsed= 250, normalized= 62 n = 8, elapsed= 515, normalized= 64 n = 16, elapsed= 1280, normalized= 80 n = 32, elapsed= 2629, normalized= 82 Exception in thread \"main\" java.lang.RuntimeException: sums of pushes and pops don't match at stack.Measurement.main(Measurement.java:107) nonblocking stack with reuse n = 1, elapsed= 48, normalized= 48 n = 2, elapsed= 94, normalized= 47 n = 4, elapsed= 265, normalized= 66 n = 8, elapsed= 530, normalized= 66 n = 16, elapsed= 1248, normalized= 78 [and does not return] 60 But other runs ... why? spcl.inf.ethz.ch @spcl_eth 61 ABA Problem A NULL top head next Thread X in the middle of pop: after read but before CAS Thread Y pops A A NULL top Thread Z pushes B B NULL top Thread Z' pushes A B NULL Thread X completes pop A NULL top head next BA time Pool Pool top public void push(Long item) { Node head; Node new = pool.get(item); do { head = top.get(); new.next = head; } while (!top.compareAndSet(head, new)); } public Long pop() { Node head, next; do { head = top.get(); if (head == null) return null; next = head.next; } while (!top.compareAndSet(head, next)); Long item = head.item; pool.put(head); return item; } spcl.inf.ethz.ch @spcl_eth \"The ABA problem ... occurs when one activity fails to recognize that a single memory location was modified temporarily by another activity and therefore erroneously assumes that the overall state has not been changed.\" 62 The ABA-Problem A X observes Variable V as A B meanwhile V changes to B ... A .. and back to A A X observes A again and assumes the state is unchanged time spcl.inf.ethz.ch @spcl_eth DCAS (double compare and swap) not available on most platforms (we have used a variant for the lock-free list set) Garbage Collection relies on the existence of a GC much too slow to use in the inner loop of a runtime kernel can you implement a lock-free garbage collector relying on garbage collection? Pointer Tagging does not cure the problem, rather delay it can be practical Hazard Pointers Transactional memory (later) 63 How to solve the ABA problem? spcl.inf.ethz.ch @spcl_eth ABA problem usually occurs with CAS on pointers Aligned addresses (values of pointers) make some bits available for pointer tagging. Example: pointer aligned modulo 32 → 5 bits available for tagging Each time a pointer is stored in a data structure, the tag is increased by one. Access to a data structure via address x – (x mod 32) This makes the ABA problem very much less probable because now 32 versions of each pointer exist. 64 Pointer Tagging MSB 00000XXXXXXXX... spcl.inf.ethz.ch @spcl_eth The ABA problem stems from reuse of a pointer P that has been read by some thread X but not yet written with CAS by the same thread. Modification takes place meanwhile by some other thread Y. Idea to solve: ▪ before X reads P, it marks it hazarduous by entering it in one of the n (n= number threads) slots of an array associated with the data structure (e.g., the stack) ▪ When finished (after the CAS), process X removes P from the array ▪ Before a process Y tries to reuse P, it checks all entries of the hazard array 65 Hazard Pointers spcl.inf.ethz.ch @spcl_eth public class NonBlockingStackPooledHazardGlobal extends Stack { AtomicReference<Node> top = new AtomicReference<Node>(); NodePoolHazard pool; AtomicReferenceArray<Node> hazarduous; public NonBlockingStackPooledHazardGlobal(int nThreads) { hazarduous = new AtomicReferenceArray<Node>(nThreads); pool = new NodePoolHazard(nThreads); } } 66 Hazard Pointers null null null null null null null null null null null null 0 nThreads-1 spcl.inf.ethz.ch @spcl_eth boolean isHazardous(Node node) { for (int i = 0; i < hazarduous.length(); ++i) if (hazarduous.get(i) == node) return true; return false; } void setHazardous(Node node) { hazarduous.set(id, node); // id is current thread id } 67 Hazard Pointers null null null null null null y null x null null 0 nThreads-1id hd spcl.inf.ethz.ch @spcl_eth public int pop(int id) { Node head, next = null; do { do { head = top.get(); setHazardous(head); } while (head == null || top.get() != head); next = head.next; } while (!top.compareAndSet(head, next)); setHazardous(null); int item = head.item; if (!isHazardous(head)) pool.put(id, head); return item; } 68 Hazard Pointers public void push(int id, Long item) { Node head; Node newi = pool.get(id, item); do{ head = top.get(); newi.next = head; } while (!top.compareAndSet(head, newi)); } This ensures that no other thread is already past the CAS and has not seen our hazard pointer null null null null null null y null x null null 0 nThreads-1id hd spcl.inf.ethz.ch @spcl_eth The ABA problem also occurs on the node pool. Solutions: Thread-local node pools ▪ No protection necessary ▪ Does not help when push/pop operations are not well balanced Hazard pointers on the global node pool ▪ Expensive operation for node reuse ▪ Equivalent to code above: node pool returns a node only when it is not hazarduous Hybrid of both ▪ Node-local + global node pool 69 How to protect the Node Pool? spcl.inf.ethz.ch @spcl_eth The Java code above does not really improve performance in comparison to memory allocation plus garbage collection. But it demonstrates how to solve the ABA problem principally. The hazard pointers are placed in thread-local storage. When thread-local storage can be replaced by processor-local storage, it scales better*. 70 Remarks * e.g., in Florian Negele, Combining Lock-Free Programming with Cooperative Multitasking for a Portable Multiprocessor Runtime System, PhD Thesis, ETH Zürich 2014 spcl.inf.ethz.ch @spcl_eth Lock-free programming: new kind of problems in comparison to lock-based programming: ▪ Atomic update of several pointers / values impossible, leading to new kind of problems and solutions, such as threads that help each other in order to guarantee global progress ▪ ABA problem (which may disappear with a garbage collector) 71 Lessons Learned spcl.inf.ethz.ch @spcl_eth • algorithms to implement critical sections and locks • hardware support for implementing critical sections and locks • how to reason about concurrent algorithms using state diagrams • high-level constructs such as semaphores and monitors that raise the level of abstraction • lock-free implementations that require Read-Modify-Write operations Recap: we have seen … Literature: Herlihy: Chapter 3.1 - 3.6 72 spcl.inf.ethz.ch @spcl_eth developed a clear overview of the theoretical concepts and notions behind such as • consistency • linearizability • consensus • a language to talk formally about concurrency I have been very hand-wavy when answering some tricky questions • now that you appreciate the complexity Let us introduce some non-trivial formalism to capture it But: we have not (yet) … 73 spcl.inf.ethz.ch @spcl_eth class WaitFreeQueue { volatile int head = 0, tail = 0; AtomicReferenceArray<T>[] items = new AtomicReferenceArray<T>(capacity); public boolean enq(T x) { if (tail – head == capacity) return false; items.set((tail+1) % capacity, x); tail++; return true; } public T deq() { if (tail - head == 0) return null; int x = items.get((head+1) % capacity); head++; return x; } } Example: Single-Enqueuer/Dequeuer bounded FIFO queue e a b c d tail head % capacity head 74 Given that there is only one enqueuer and one dequeuer process. Is the implementation of the FIFO queue from above correct? Why/why not? For a concurrent, locking queue it is easier to argue. Why/why not? spcl.inf.ethz.ch @spcl_eth An object (e.g., in Java or C++) is a container for data and provides • a set of methods to manipulate data An object has a well defined • state being modified during method invocations Well-established as Floyd-Hoare logic to prove correctness ▪ Defining the objects behavior in terms of a set of pre- and postconditions for each method is inherently sequential Can we carry that forward to a parallel formulation? Sequential Objects – Sequential Specifications (you know this) 75 spcl.inf.ethz.ch @spcl_eth A method call is the interval that starts with an invocation and ends with a response. A method call is called pending between invocation and response. Method Calls Thread q.enq(7) invocation response 76 spcl.inf.ethz.ch @spcl_eth Sequential vs. Concurrent Sequential Concurrent Meaningful state of objects only between method calls. Method calls can overlap. Object might never be between method calls. Exception: periods of quiescence. Methods described in isolation. All possible interactions with concurrent calls must be taken into account. Can add new methods without affecting older methods. Must take into account that everything can interact with everything else. \"Global clock\" \"Object/thread clock\" 77 spcl.inf.ethz.ch @spcl_eth time Blocking Queue Behavior Thread A q.deq() lock unlockdeq Thread B q.enq(…) lock unlockenq enq deq With locking it becomes simple to argue: things become sequential. Can we formalize this? 78 Which thread got the lock first? spcl.inf.ethz.ch @spcl_eth Linearizability 79 “What's the difference between theory and practice? Well, in theory there is none.” - folklore spcl.inf.ethz.ch @spcl_eth Each method should appear to take effect instantaneously between invocation and response events. An object for which this is true for all possible executions is called linearizable. The object is correct if the associated sequential behavior is correct. Linearizability 80 spcl.inf.ethz.ch @spcl_eth Is this particular execution linearizable? A q.enq(x) B q.deq() →y q.deq() →x time 81 q.enq(y) spcl.inf.ethz.ch @spcl_eth Yes A q.enq(x) B q.enq(y) q.deq() →y q.deq() →x time 82 spcl.inf.ethz.ch @spcl_eth Linearizable? A q.enq(x) B q.enq(y) q.deq() →y time 83 spcl.inf.ethz.ch @spcl_eth 84 No A q.enq(x) B q.enq(y) q.deq() →y time x is first in queue spcl.inf.ethz.ch @spcl_eth 85 Linearizable ? A q.enq(x) B q.eny(y) q.deq() →y q.deq() →x time spcl.inf.ethz.ch @spcl_eth 86 Yes A q.enq(x) B q.eny(y) q.deq() →y q.deq() →x time spcl.inf.ethz.ch @spcl_eth 87 And yes, another scenario. A q.enq(x) B q.eny(y) q.deq() →y q.deq() →x time spcl.inf.ethz.ch @spcl_eth 88 Read/Write Register Example A write(0) B time write(2) write(1) read()→1 spcl.inf.ethz.ch @spcl_eth 89 Linearizable! A write(0) B write(1) time write(2) read()→1 spcl.inf.ethz.ch @spcl_eth 90 Linearizable? A write(0) B time write(2)read()→1 write(1) read()→1 spcl.inf.ethz.ch @spcl_eth 91 No A write(0) B time write(2)read()→1 write(1) must have happened write(1) read()→1 spcl.inf.ethz.ch @spcl_eth ▪ We talk about executions in order to abstract away from actual method content. ▪ A simplification you need to revert (mentally?) for analyzing codes ▪ The linearization points can often be specified, but they may depend on the execution (not only the source code). ▪ Example: if the queue is empty, a dequeue may fail, while it does not fail with a non-empty queue Remark public int deq() throws EmptyException { if (tail == head) throw new EmptyException(); int x = items.get(head++ % capacity); return x; } 92 spcl.inf.ethz.ch @spcl_eth Split method calls into two events. Notation: Invocation Response A q.enq(x) A q: void More formal thread object method arguments thread object result 93 spcl.inf.ethz.ch @spcl_eth History H = sequence of invocations and responses A q.enq(3) A q:void A q.enq(5) H B p.enq(4) B p:void B q.deq() B q:3 Invocations and response match, if thread names agree and object names agree An invocation is pending if it has no matching response. A subhistory is complete when it has no pending responses. 94 History spcl.inf.ethz.ch @spcl_eth Object projections A q.enq(3) A q:void A q.enq(5) B p.enq(4) B p:void B q.deq() B q:3 Thread projections A q.enq(3) A q:void A q.enq(5) B p.enq(4) B p:void B q.deq() B q:3 Projections H|q = H|B = 95 spcl.inf.ethz.ch @spcl_eth A q.enq(3) A q:void A q.enq(5) B p.enq(4) B p:void B q.deq() B q:3 Complete subhistory History H without its pending invocations. Complete subhistories complete (H) = 96 spcl.inf.ethz.ch @spcl_eth A q.enq(3) A q:void B p.enq(4) B p:void B q.deq() B q:3 A q:enq(5) Sequential history: ▪ Method calls of different threads do not interleave. ▪ A final pending invocation is ok. Sequential histories 97 spcl.inf.ethz.ch @spcl_eth H= A q.enq(3) B p.enq(4) B p:void B q.deq() A q:void B q:3 Well formed histories Well formed history: Per thread projections sequential H|A = A q.enq(3) A q:void H|B = B p.enq(4) B p:void B q.deq() B q:3 98 spcl.inf.ethz.ch @spcl_eth H= A q.enq(3) B p.enq(4) B p:void B q.deq() A q:void B q:3 G = A q.enq(3) A q:void B p.enq(4) B p:void B q.deq() B q:3 Equivalent histories H and G equivalent: H|A = G|A H|B = G|B 99 spcl.inf.ethz.ch @spcl_eth Sequential specification tells if a single-threaded, single object history is legal Example: pre- / post conditions A sequential history H is legal, if ▪ for every object x ▪ H|x adheres to the sequential specification of x Legal histories 100 spcl.inf.ethz.ch @spcl_eth A method call precedes another method call if the response event precedes the invocation event A q.enq(3) B p.enq(4) B p:void A q:void B q.deq() B q:3 if no precedence then method calls overlap A q.enq(3) B p.enq(4) B p:void B q.deq() A q:void B q:3 Precedence 101 spcl.inf.ethz.ch @spcl_eth Given: history 𝐻 and method executions 𝑚0 and 𝑚1 on 𝐻 Definition: 𝒎𝟎 →𝑯 𝒎𝟏 means 𝒎𝟎 precedes 𝒎𝟏 →𝑯 is a relation and implies a partial order on H. The order is total when H is sequential. Notation 102 spcl.inf.ethz.ch @spcl_eth History 𝐻 is linearizable if it can be extended to a history 𝐺 ▪ appending zero or more responses to pending invocations that took effect ▪ discarding zero or more pending invocations that did not take effect such that G is equivalent to a legal sequential history 𝑆 with →𝑮 ⊂ →𝑺 Linearizability 103 spcl.inf.ethz.ch @spcl_eth 104 Invocations that took effect … ? A q.enq(x) B q.deq() →x C flag.read() → ? cannot be removed because B already took effect into account can be removed, nobody relies on this spcl.inf.ethz.ch @spcl_eth →𝑮 = 𝒂 → 𝒄, 𝒃 → 𝒄 →𝑺 = 𝒂 → 𝒃, 𝒂 → 𝒄, 𝒃 → 𝒄 →𝑮 ⊂ →𝑺 ? What does this mean? A a B b c time →𝑮 →𝑺 →𝑺 105 In other words: S respects the real-time order of G Linearizability: limitation on the possible choice of S spcl.inf.ethz.ch @spcl_eth Composability Theorem History H is linearizable if and only if for every object x H|x is linearizable Consequence: Modularity • Linearizability of objects can be proven in isolation • Independently implemented objects can be composed Composability 106 spcl.inf.ethz.ch @spcl_eth Memory location for values of primitive type (boolean, int, ...) • operations read and write Linearizable with a single linearization point, i.e. • sequentially consistent, every read operation yields most recently written value • for non-overlapping operations, the realtime order is respected. Recall: Atomic Registers 107 spcl.inf.ethz.ch @spcl_eth public T deq() throws EmptyException { lock.lock(); try { if (tail == head) throw new EmptyException(); T x = items[head % items.length]; head++; return x; } finally { lock.unlock(); } } Reasoning About Linearizability (Locking) head tail Linearization points are when locks are released 108 spcl.inf.ethz.ch @spcl_eth class WaitFreeQueue { volatile int head = 0, tail=0; AtomicReferenceArray<T>[] items = new AtomicReferenceArray<T>(capacity); public boolean enq (T x) { if (tail – heap == capacity) return false; items.set((tail+2) % capacity, x); tail++; return true; } public T deq() { if (tail - head == 0) return null; int x = items.get((head+1) % capacity); head++; return x; } } Reasoning About Linearizability (Wait-free example) Linearization point for (only one) enqueuer Linearization point for (only one) dequeuer head tail 109 Linearization point Linearization point spcl.inf.ethz.ch @spcl_eth public T dequeue() { while (true) { Node first = head.get(); Node last = tail.get(); Node next = first.next.get(); if (first == last) { if (next == null) return null; else tail.compareAndSet(last, next); } else { T value = next.item; if (head.compareAndSet(first, next)) return value; } } } Reasoning About Linearizability (Lock-free example) Linearization point Linearization point Linearization point 110 spcl.inf.ethz.ch @spcl_eth Identify one atomic step where the method “happens” ▪ Critical section ▪ Machine instruction Does not always work ▪ Might need to define several different steps for a given method ▪ Linearizability summary: ▪ Powerful specification tool for shared objects ▪ Allows us to capture the notion of objects being “atomic” Linearizability Strategy & Summary 111 spcl.inf.ethz.ch @spcl_eth Sequential Consistency 112 spcl.inf.ethz.ch @spcl_eth History 𝐻 is sequentially consistent if it can be extended to a history 𝐺 ▪ appending zero or more responses to pending invocations that took effect ▪ discarding zero or more pending invocations that did not take effect such that G is equivalent to a legal sequential history 𝑆. (Note that →𝐺 ⊂ →𝑆 is not required, i.e., no order across threads required) (Sequential Consistency is weaker than Linearizability) Alternative: Sequential Consistency 113 spcl.inf.ethz.ch @spcl_eth ▪ Require that operations done by one thread respect program order ▪ No need to preserve real-time order ▪ Cannot re-order operations done by the same thread ▪ Can re-order non-overlapping operations done by different threads ▪ Often used to describe multiprocessor memory architectures Alternative: Sequential Consistency 114 spcl.inf.ethz.ch @spcl_eth Example A q.enq(x) B q.enq(y) q.deq() →y time 115 spcl.inf.ethz.ch @spcl_eth Not linearizable A q.enq(x) B q.enq(y) q.deq() →y time x is first in queue 116 spcl.inf.ethz.ch @spcl_eth Yet sequentially consistent! A q.enq(x) B q.enq(y) q.deq() →y time 117 spcl.inf.ethz.ch @spcl_eth Sequential Consistency is not a local property (and thus we lose composability…) Theorem Can somebody remind me what “composability” meant? 118 spcl.inf.ethz.ch @spcl_eth Proof by Example: FIFO Queue H = A p.enq(x) B q.enq(y) A p:void A q.enq(x) B q:void B p.enq(y) A q:void A p.deq() B p:void B q.deq(); A p:y B q:x A p.enq(x) B q.enq(y) p.deq() →y p.enq(y) time q.enq(x) q.deq() →x 119 spcl.inf.ethz.ch @spcl_eth H|q sequentially consistent H = A p.enq(x) B q.enq(y) A p:void A q.enq(x) B q:void B p.enq(y) A q:void A p.deq() B p:void B q.deq(); A p:y B q:x A p.enq(x) B q.enq(y) p.deq() →y p.enq(y) time q.enq(x) q.deq() →x 120 spcl.inf.ethz.ch @spcl_eth H|p sequentially consistent H = A p.enq(x) B q.enq(y) A p:void A q.enq(x) B q:void B p.enq(y) A q:void A p.deq() B p:void B q.deq(); A p:y B q:x A p.enq(x) B q.enq(y) p.deq() →y p.enq(y) time q.enq(x) q.deq() →x 121 spcl.inf.ethz.ch @spcl_eth Ordering imposed by H|q and H|p ➔ H is not sequentially consistentA p.enq(x) B q.enq(y) p.deq() →y p.enq(y) time q.enq(x) q.deq() →x 122 spcl.inf.ethz.ch @spcl_eth Another example: Flags Each object update (H|x and H|y) is sequentially consistent Entire history is not sequentially consistent A x.write(1) B y.write(1) y.read()→0 x.read() →0 123 spcl.inf.ethz.ch @spcl_eth Reminder: Consequence for Peterson Lock (Flag Principle) Sequential Consistency → At least one of the processes A and B read flag[1-id] = true. If both processes read flag = true then both processes eventually read the same value for victim(). A flag[0].write(true) B flag[1].write(true) flag[1].read()→ ? flag[0].read() → ? flag[id] = true; victim = id; while (flag[1-id] && victim == id); victim.write(0) victim.write(1) victim.read() → ? victim.read() → ? 124 spcl.inf.ethz.ch @spcl_eth Another idea: Programs should respect real-time order of algorithms separated by periods of quiescence. Side Remark: Quiescent Consistency A q.deq() → X B q.size() → n q.enq(X) … quiescence… 125 In other words: quiescent consistency requires non-overlapping methods to take effect in their real-time order! spcl.inf.ethz.ch @spcl_eth Quiescent consistency is incomparable to sequential consistency: part I This example is sequentially consistent but not quiescently consistent Side Remark: Quiescent Consistency A B q.deq() → Xq.enq(Y) q.deq() → Yq.enq(X) 126 spcl.inf.ethz.ch @spcl_eth Quiescent consistency is incomparable to sequential consistency: part II This example is quiescently consistent but not sequentially consistent (note that initially the queue is empty) Side Remark: Quiescent Consistency A q.deq() → X B q.size() → 1 q.enq(X) 127 There is no quiescence, thus anything may happen! spcl.inf.ethz.ch @spcl_eth This pattern Write mine, read yours is exactly the flag principle Heart of mutual exclusion ▪ Peterson ▪ Bakery, etc. Sequential Consistency seems non- negotiable! … but: Many hardware architects think that sequential consistency is too strong Too expensive to implement in modern hardware Assume that flag principle Violated by default Honored by explicit request (e.g., volatile) Discussion Recall our discussions at the beginning! Recall our short discussion of caches 128 spcl.inf.ethz.ch @spcl_eth Memory hierarchy ▪ On modern multiprocessors, processors do not read and write directly to memory. ▪ Memory accesses are very slow compared to processor speeds. ▪ Instead, each processor reads and writes directly to a cache. While writing to memory ▪ A processor can execute hundreds, or even thousands of instructions. ▪ Why delay on every memory write? ▪ Instead, write back in parallel with rest of the program. Recall: Memories and caches 129 spcl.inf.ethz.ch @spcl_eth To read a memory location, load data into cache. To write a memory location update cached copy, lazily write cached data back to memory “Flag-violating” history is actually OK processors delay writing to memory until after reads have been issued. Otherwise unacceptable delay between read and write instructions. Writing to memory = mailing a letter Vast majority of reads & writes Not for synchronization No need to idle waiting for post office If you want to synchronize Announce it explicitly Pay for it only when you need it Recall: Memory operations 130 spcl.inf.ethz.ch @spcl_eth Explicit Memory barrier instruction Flush unwritten caches Bring caches up to date Compilers often do this for you Entering and leaving critical sections Implicit In Java, can ask compiler to keep a variable up-to-date with volatile keyword Also inhibits reordering, removing from loops & other optimizations Synchronization 131 spcl.inf.ethz.ch @spcl_eth Weaker than sequential consistency But you can get sequential consistency at a price [1] Concept of linearizability more appropriate for high-level software Real-World Hardware Memory [1]: H. Schweizer, M. Besta, T. Hoefler: Evaluating the Cost of Atomic Operations on Modern Architectures, ACM PACT’15 132 spcl.inf.ethz.ch @spcl_eth Linearizability Operation takes effect instantaneously between invocation and response Uses sequential specification, locality implies composablity Good for high level objects Sequential Consistency Not composable Harder to work with in software development Good way to think about hardware models Linearizability vs. Sequential Consistency 133 spcl.inf.ethz.ch @spcl_eth Consensus Literature: Herlihy: Chapter 5.1-5.4, 5.6-5.8 134 spcl.inf.ethz.ch @spcl_eth Consider an object c with the following interface public interface Consensus<T> { T decide (T value); } A number of threads call c.decide(v) with an input value v each. Consensus P consensus object R Q 135 spcl.inf.ethz.ch @spcl_eth Requirements on consensus protocol • wait-free: consensus returns in finite time for each thread • consistent: all threads decide the same value • valid: the common decision value is some thread's input ➔ linearizability of consensus must be such that first thread's decision is adopted for all threads. Consensus protocol P consensus object R Q 136 spcl.inf.ethz.ch @spcl_eth Consensus A c.decide(x)→x B c.decide(y) →x C c.decide(z)→x time 137 spcl.inf.ethz.ch @spcl_eth A class C solves n-thread consensus if there exists a consensus protocol using any number of objects of class C and any number of atomic registers. Consensus number of C: largest n such that C solves n-thread consensus. Consensus number 138 spcl.inf.ethz.ch @spcl_eth Theorem: Atomic Registers have consensus number 1. [Proof: Herlihy, Ch. 5, presented later if we have time!] Corollary: There is no wait-free implementation of n-thread consensus, n>1, from read-write registers Atomic registers 139 spcl.inf.ethz.ch @spcl_eth Theorem: Compare-And-Swap has infinite consensus number. How to prove this? Compare and swap/set 140 spcl.inf.ethz.ch @spcl_eth class CASConsensus { private final int FIRST = -1; private AtomicInteger r = new AtomicInteger(FIRST); // supports CAS private AtomicIntegerArray proposed; // suffices to be atomic register … // constructor (allocate array proposed etc.) public Object decide (Object value) { int i = ThreadID.get(); proposed.set(i, value); if (r.compareAndSet(FIRST, i)) // I won return proposed.get(i); // = value else return proposed.get(r.get()); } } Proof by construction 141 spcl.inf.ethz.ch @spcl_eth Theorem: There is no wait-free implementation of a FIFO queue with atomic registers How to prove this now? Proof follows. How to use this? Wait-free FIFO queue 142 Hint: They have consensus number 1! spcl.inf.ethz.ch @spcl_eth Can a FIFO queue implement two-thread consensus? proposed array FIFO queue with red and black balls8 red ball black ball 143 spcl.inf.ethz.ch @spcl_eth Protocol: Write value to array 0 1 0 144 spcl.inf.ethz.ch @spcl_eth Protocol: Take next item from queue 0 0 1 8 145 spcl.inf.ethz.ch @spcl_eth Protocol: Take next Item from Queue 0 1 I got the red ball, so I will decide my value I got the black ball, so I will decide the other’s value from the array 8 146 spcl.inf.ethz.ch @spcl_eth If one thread gets the red ball Then the other gets the black ball Winner decides her own value Loser can find winner’s value in array Because threads write array Before dequeueing from queue Why does this work? 147 spcl.inf.ethz.ch @spcl_eth Given A consensus protocol from queue and registers Assume there exists A queue implementation from atomic registers Substitution yields: A wait-free consensus protocol from atomic registers However: atomic registers have consensus number 1 Wait-free queue implementation from atomic registers? 148 spcl.inf.ethz.ch @spcl_eth We know • Wait-free FIFO queues have consensus number 2 • Test-And-Set, getAndSet, getAndIncrement have consensus number 2 • CAS has consensus number ∞ → wait-free FIFO queues, wait-free RMW operations and CAS cannot be implemented with atomic registers! Why consensus is important 149 spcl.inf.ethz.ch @spcl_eth 1 Read/Write Registers 2 getAndSet, getAndIncrement, … FIFO Queue LIFO Stack . . ∞ CompareAndSet, … Multiple Assignment The Consensus Hierarchy 150 spcl.inf.ethz.ch @spcl_eth Squaring the circle Geometric way to construct a square with the same area as a given circle with compass and straightedge using a finite number of steps. There is an algebraic proof that no such construction exists. People tried it for hundreds of years, some still try it today. Apparently they do not believe the mathematical proof. Let's not do the same mistake in our field...: provably there is no way to construct certain wait-free algorithms with atomic registers. Don't even try. Importance of Consensus by Analogy r=1 𝜋 151 spcl.inf.ethz.ch @spcl_eth Motivation for Transactional Memory 152 spcl.inf.ethz.ch @spcl_eth Motivation: programming with locks is too difficult Lock-free programming is even more difficult... Goal: remove the burden of synchronization from the programmer and place it in the system (hardware / software) Transactional Memory in a nutshell Literature: -Herlihy Chapter 18.1 – 18.2. -Herlihy Chapter 18.3. interesting but too detailed for this course. 153 spcl.inf.ethz.ch @spcl_eth Deadlocks: threads attempt to take common locks in different orders What is wrong with locking? 154 spcl.inf.ethz.ch @spcl_eth Convoying: thread holding a resource R is descheduled while other threads queue up waiting for R What is wrong with locking? 155 spcl.inf.ethz.ch @spcl_eth Priority Inversion: lower priority thread holds a resource R that a high priority thread is waiting on What is wrong with locking? 156 spcl.inf.ethz.ch @spcl_eth Association of locks and data established by convention. The best you can do is reasonably document your code! What is wrong with locking? 157 spcl.inf.ethz.ch @spcl_eth Example: Unbounded Queue (FIFO) What is wrong with CAS? sentinel node node node node node head tail value value value value value public class LockFreeQueue<T> { private AtomicReference<Node> head; private AtomicReference<Node> tail; public void enq(T item); public T deq(); } public class Node { public T value; public AtomicReference<Node> next; public Node(T v) { value = v; next = new AtomicReference<Node>(null); } } 158 spcl.inf.ethz.ch @spcl_eth Enqueue S node node node node node head tail new ① ② CAS next CAS tail value value value value value value Two CAS operations → half finished enqueue visible to other processes 159 spcl.inf.ethz.ch @spcl_eth Dequeue S node node node node node head tail ① ② value value value value value read value CAS head 160 spcl.inf.ethz.ch @spcl_eth public class LockFreeQueue<T> { .. public void enq(T item) { Node node = new Node(item); while(true){ Node last = tail.get(); Node next = last.next.get(); if (last == tail.get()) { if (next == null) if (last.next.compareAndSet(next, node)) { tail.compareAndSet(last, node); return; } else tail.compareAndSet(last, next); } } } } Code for enqueue Half finished insert may happen! Help other processes with finishing operations (→ lock-free) 161 spcl.inf.ethz.ch @spcl_eth public class LockFreeQueue<T> { .. public void enq(T item) { Node node = new Node(item); while(true) { Node last = tail.get(); Node next = last.next.get(); if (multiCompareAndSet({last.next, tail},{next, last},{node, node}) return; } } } Code with hypothetical DCAS This code ensures consistency of both next and last: operation either fails completely without effect or the effect happens atomically 162 spcl.inf.ethz.ch @spcl_eth class Account { private final Integer id; // account id private Integer balance; // account balance Account(int id, int balance) { this.id = new Integer(id); this.balance = new Integer(balance); } synchronized void withdraw(int amount) { // assume that there are always sufficient funds... this.balance = this.balance – amount; } synchronized void deposit(int amount) { this.balance = this.balance + amount; } } More problems: Bank account 163 spcl.inf.ethz.ch @spcl_eth void transfer_unsafe(Account a, Account b, int amount) { a.withdraw(amount); b.deposit(amount); } Bank account transfer (unsafe) Transfer does not happen atomically A thread might observe the withdraw, but not the deposit 164 spcl.inf.ethz.ch @spcl_eth void transfer_deadlock(Account a, Account b, int amount) { synchronized (a) { synchronized (b) { a.withdraw(amount); b.deposit(amount); } } } Bank account transfer (can cause a deadlock) 165 Concurrently executing: ▪ transfer_deadlock(a, b) ▪ transfer_deadlock(b, a) Might lead to a deadlock spcl.inf.ethz.ch @spcl_eth void transfer(Account a, Account b, int amount) { if (a.id < b.id) { synchronized (a) { synchronized (b) { a.withdraw(amount); b.deposit(amount); } } } else { synchronized (b) { synchronized (a) { a.withdraw(amount); b.deposit(amount); } } } } Bank account transfer (lock ordering to avoid deadlock) 166 spcl.inf.ethz.ch @spcl_eth void transfer_elegant(Account a, Account b, int amount) { Account first, second; if (a.id < b.id) { first = a; second = b; } else { first = b; second = a; } synchronized (first) { synchronized (second) { a.withdraw(amount); b.deposit(amount); } } } Bank account transfer (slightly better ordering version) Code for the actual operationCode for synchronization 167 spcl.inf.ethz.ch @spcl_eth Ensuring ordering (and correctness) is really hard (even for advanced programmers) ▪ rules are ad-hoc, and not part of the program ▪ (documented in comments at best-case scenario) Locks are not composable ▪ how can you combine n thread-safe operations? ▪ internal details about locking are required ▪ big problem, especially for programming “in the large” Lack of composability 168 spcl.inf.ethz.ch @spcl_eth Locks are pessimistic ● worst is assumed ● performance overhead paid every time Locking mechanism is hard-wired to the program ● synchronization / rest of the program cannot be separated ● changing synchronization scheme → changing all of the program Problems using locks (cont'd) 169 spcl.inf.ethz.ch @spcl_eth What the programmer actually meant to say is: atomic { a.withdraw(amount); b.deposit(amount); } → This is the idea behind transactional memory also behind locks, isn’t it? The difference is the execution! Solution: atomic blocks (or transactions) I want these operations to be performed atomically! 170 atomic { a.withdraw(amount); b.deposit(amount); } spcl.inf.ethz.ch @spcl_eth Programmer explicitly defines atomic code sections Programmer is concerned with: what: what operations should be atomic but, not how: e.g., via locking the how is left to the system (software, hardware or both) (declarative approach) Transactional Memory (TM) 171 spcl.inf.ethz.ch @spcl_eth ▪ simpler and less error-prone code ▪ higher-level (declarative) semantics (what vs. how) ▪ composable ▪ analogy to garbage collection (Dan Grossman. 2007. \"The transactional memory / garbage collection analogy\". SIGPLAN Not. 42, 10 (October 2007), 695-706.) ▪ optimistic by design (does not require mutual exclusion) TM benefits 172 spcl.inf.ethz.ch @spcl_eth changes made by a transaction are made visible atomically other threads preserve either the initial or the final state, but not any intermediate states Note: locks enforce atomicity via mutual exclusion, while transactions do not require mutual exclusion TM semantics: Atomicity 173 spcl.inf.ethz.ch @spcl_eth Transactions run in isolation ▪ while a transaction is running, effects from other transactions are not observed ● as if the transaction takes a snapshot of the global state when it begins and then operates on that snapshot TM semantics: Isolation 174 spcl.inf.ethz.ch @spcl_eth Serializability TXA Thread 0 Thread 1 TXB TXA TXB as if: Executed Sequentially (transactions appear serialized) 175 spcl.inf.ethz.ch @spcl_eth Transactional Memory is heavily inspired by database transactions ACID properties in database transactions: ● Atomicity ● Consistency (database remains in a consistent state) ● Isolation (no mutual corruption of data) ● Durability (e.g., transaction effects will survive power loss → stored in disk) Transactions in databases 176","libVersion":"0.3.2","langs":""}