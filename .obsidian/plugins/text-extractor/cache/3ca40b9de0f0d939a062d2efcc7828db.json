{"path":"sem2a/PProg/VRL/slides/PProg-L07-concepts.pdf","text":"Parallel Programming Basic Concepts in Parallelism Big Picture (Part I) 2 CPU OS JVM (Process A) Core Core Core Core OS thread OS thread OS thread OS thread OS scheduler JVM scheduler JVM thread Process B Memory Space A Memory Space B Physical Memory JVM thread JVM thread JVM threadL03-05 L06 L07 L08-09 L10-L11 L13 ‚Ä¶ ‚Ä¶ Stack Registers PC Stack Registers PC Stack Registers PC Stack Registers PC Parallel performance & algorithms L12Virtual threads Expressing Parallelism ‚óè Work partitioning ‚Äì Split up work of a single program into parallel tasks ‚óè Can be done: ‚Äì Explicitly / Manually (task/thread parallelism) ‚Äì User explicitly expresses tasks/threads ‚Äì Implicit parallelism: ‚Äì Done automatically by the system (e.g., in data parallelism) ‚Äì User expresses an operation and the system does the rest 3 Work Partitioning & Scheduling ‚óè work partitioning ‚Äì split up work into parallel tasks/threads ‚Äì (done by user) ‚Äì A task is a unit of work ‚Äì also called: task/thread decomposition ‚óè scheduling ‚Äì assign tasks to processors ‚Äì (typically done by the system) ‚Äì goal: full utilization (no processor is ever idle) work Processors scheduling work partitioning 4 # of chunks should be larger than the # of processors Task/Thread Granularity work work Coarse granularity Fine granularity 5 Coarse vs Fine granularity ‚óè Fine granularity: ‚Äì more portable (can be executed in machines with more processors) ‚Äì better for scheduling ‚Äì but: if scheduling overhead is comparable to a single task ‚Üí overhead dominates 7 Task granularity guidelines ‚óè As small as possible ‚óè but, significantly bigger than scheduling overhead ‚Äì system designers strive to make overheads small 8 Scalability An overloaded concept: e.g., how well a system reacts to increased load, for example, clients in a server In parallel programming: ‚Äì speedup when we increase processors ‚Äì what will happen if processors ‚Üí ‚Äì a program scales linearly ‚Üí linear speedup ‚àû 9 Parallel Performance Sequential execution time: T1 Execution time Tp on p CPUs ‚Äì Tp = T1 / p (perfection) ‚Äì Tp > T1 / p (performance loss, what normally happens) ‚Äì Tp < T1 / p (sorcery!) 10 (parallel) Speedup (parallel) speedup Sp on p CPUs: Sp = T1 / Tp ‚óè Sp = p ‚Üí linear speedup (perfection) ‚óè Sp < p ‚Üí sub-linear speedup (performance loss) ‚óè Sp > p ‚Üí super-linear speedup (sorcery!) ‚óè Efficiency: Sp / p 11 Absolute versus Relative Speed-up Relative speedup (Efficiency): relative improvement from using P execution units. (Baseline: serialization of the parallel algorithm). Sometimes there is a better serial algorithm that does not parallelize well. In these cases it is fairer to use that algorithm for T1 (absolute speedup). Using an unnecessarily poor baseline artificially inflates speedup and efficiency. 15 (parallel) speedup graph example 16 why Sp < p? ‚óè Programs may not contain enough parallelism ‚Äì e.g., some parts of program might be sequential ‚óè overheads introduced by parallelization ‚Äì typically associated with synchronization ‚óè architectural limitations ‚Äì e.g., memory contention 17 Question: Parallel program: ‚Äì sequential part: 20% ‚Äì parallel part: 80% (assume it scales linearly) ‚Äì T1 = 10 What is T8 ? What is the speedup S8 ? Sequential part Parallel part 18 Answer: ‚óè ùëá! = 10 ‚óè ùëá\" = 3 ‚óè ùëÜ\" = ùëá!/ùëá\" = 10/3 = 3.33 Sequential part Parallel part 80% 20% 21 Amdahl‚Äôs Law 22 ‚Ä¶the effort expended on achieving high parallel processing rates is wasted unless it is accompanied by achievements in sequential processing rates of very nearly the same magnitude. ‚Äî Gene Amdahl Amdahl‚Äôs Law ‚Äì Ingredients Execution time T1 of a program falls into two categories: ‚Ä¢ Time spent doing non-parallelizable serial work ‚Ä¢ Time spent doing parallelizable work Call these ùëä#$% and ùëäùëùùëéùëü respectively 23 Amdahl‚Äôs Law ‚Äì Ingredients Given ùëÉ workers available to do parallelizable work, the times for sequential execution and parallel execution are: ùëá! = ùëä#$% + ùëä&'% And this gives a bound on speed-up: ùëá& ‚â• ùëä#$% + ùëä&'% ùëÉ 24 Amdahl‚Äôs Law Plugging these relations into the definition of speedup yields Amdahl's Law: ùëÜ& ‚â§ ùëä#$% + ùëä&'% ùëä#$% + ùëä&'% ùëÉ 25 Amdahl‚Äôs Law - Corollary If f is the non-parallelizable serial fractions of the total work, then the following equalities hold: ùëä#$% = ùíáùëá!, ùëä&'% = 1 ‚àí ùíá ùëá! which gives: ùëÜ& ‚â§ 1 ùíá + 1 ‚àí ùíá ùëÉ 26 ùëÜ! ‚â§ ùëä\"#$ + ùëä!%$ ùëä\"#$ + ùëä!%$ ùëÉWhat happens if we have infinite workers? ùëÜ( ‚â§ 1 ùëì 28 Amdahl‚Äôs Law Illustrated 29 Amdahl‚Äôs Law Illustrated 30 Amdahl‚Äôs Law Illustrated 31 Amdahl‚Äôs Law Illustrated 32 Speedup 33 Efficiency 34 Remarks about Amdahl's Law ‚óè It concerns maximum speedup (Amdahl was an optimist (or pessimist?)) ‚Äì architectural constraints will make factors worse ‚Ä¢ But his law is mostly bad news (as it puts a limit on scalability) ‚óè takeaway: all non-parallel parts of a program (no matter how small) can cause problems ‚Ä¢ Amdahl‚Äôs law shows that efforts required to further reduce the fraction of the code that is sequential may pay off in large performance gains. ‚Ä¢ Hardware that achieves even a small decrease in the percent of things executed sequentially may be considerably more efficient 35 Gustafson‚Äôs Law ‚óè An alternative (optimistic) view to Amdahl's Law Observations: ‚óè consider problem size ‚óè run-time, not problem size, is constant ‚óè more processors allows to solve larger problems in the same time ‚óè parallel part of a program scales with the problem size 36 Gustafson‚Äôs Law 37 Gustafson‚Äôs Law 38 Gustafson‚Äôs Law 39 Gustafson‚Äôs Law 40 Gustafson's Law ‚óè ùëì: sequential part (no speedup) 41 ùëä = ùëù 1 ‚àí ùëì ùëá!\"## + ùëìùëá!\"## http://link.springer.com/referenceworkentry/10.1007%2F978-0-387-09766-4_78 ùëÜ$ = ùëì + ùëù 1 ‚àí ùëì = ùëù ‚àí ùëì(ùëù ‚àí 1) Amdahl's vs Gustafson's Law p=4 p=4 Amdahl's Law Gustafson's Law 43 Summary ‚óè Parallel speedup ‚óè Amdahl's and Gustafson's law ‚óè Parallelism: task/thread granularity 44","libVersion":"0.3.2","langs":""}