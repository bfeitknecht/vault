{"path":"sem3/LinAlg/UE/s/LinAlg-u08-s.pdf","text":"D-INFK Linear Algebra HS 2023 Afonso Bandeira Bernd G¨artner Solution for Assignment 8 1. a) Let a1, a2, a3 denote the columns of A. Performing the Gram-Schmidt process (Algorithm 4.4.10) yields q1 = a1 ∥a1∥ = 1 √2   0 1 1   =   0 1/ √2 1/ √2   q′ 2 = a2 − (a ⊤ 2 q1)q1 = a2 − 1 √2 q1 =   0 0 1   − 1 √2   0 1/ √2 1/ √2   =   0 −1/2 1/2   q2 = q′ 2 ∥q′ 2∥ =   0 −1/ √2 1/ √2   q′ 3 = a3 − (a ⊤ 3 q1)q1 − (a ⊤ 3 q2)q2 = a3 − √2q1 − 0q2 =   1 1 1   −   0 1 1   =   1 0 0   q3 = q′ 3 ∥q′ 3∥ = q′ 3 where q1, q2, q3 is the desired set of orthonormal vectors. b) Putting the vectors q1, q2, q3 into a matrix we obtain Q =   0 0 1 1/ √ 2 −1/ √2 0 1/ √ 2 1/ √2 0   and it remains to compute R. Concretely, we have R = Q ⊤A =   0 0 1 1/ √ 2 −1/ √2 0 1/ √ 2 1/ √2 0   ⊤   0 0 1 1 0 1 1 1 1   =   0 1/ √2 1/ √2 0 −1/ √2 1/ √2 1 0 0     0 0 1 1 0 1 1 1 1   =   √2 1/ √2 √2 0 1/ √2 0 0 0 1   . c) Let b1, b2, b3, b4 denote the columns of B. Performing the Gram-Schmidt process (Algorithm 4.4.10) yields q1 = b1 ∥b1∥ = b1 q′ 2 = b2 − (b⊤ 2 q1)q1 = b2 − 2q1 = [ 0 4 0 0 ]⊤ q2 = q′ 2 ∥q′ 2∥ = [0 1 0 0 ]⊤ q′ 3 = b3 − (b⊤ 3 q1)q1 − (b⊤ 3 q2)q2 = b3 − 3q1 − 5q2 = [0 0 7 0 ]⊤ q3 = q′ 3 ∥q′ 3∥ = q′ 3 = [0 0 1 0 ]⊤ q′ 4 = b4 − (b⊤ 4 q1)q1 − (b⊤ 4 q2)q2 − (b ⊤ 4 q3)q3 = b4 − 0q1 − 6q2 − 8q3 = [0 0 0 9 ]⊤ q4 = q′ 4 ∥q′ 4∥ = [0 0 0 1 ]⊤ where q1, q2, q3, q4 is the desired set of orthonormal vectors. d) This is not always true. The n × n matrix −I is a counterexample for any n ∈ N+. It already has orthonormal columns, hence Gram-Schmidt would leave it unaltered. Moreover, its columns are not exactly the standard unit vectors: the sign is wrong. Therefore, this is indeed a counterexample. Note that this is already a full solution. But we still provide a proof that the answer to the question would be yes if we had required the diagonal entries to be strictly positive (and not just non-zero). Let A be an arbitrary upper triangular n × n matrix with strictly positive entries on its diagonal. Let a1, . . . , an denote the columns of A and let q1, . . . , qn denote the orthonormal vectors obtained from the Gram Schmidt process on a1, . . . , an. We claim that qi = ei for all i ∈ [n]. Assume for a contradiction that this is not the case and let i ∈ [n] be the smallest index such that qi ̸= ei. Note that we have a1 = ce1 for some constant c ∈ R+ and hence q1 = a1 c = e1. Hence, we must have i > 1. Observe that by definition of the Gram-Schmidt process and because the last n − i entries of ai are zero (triangular shape of A), we also get that the last n − i entries of qi are zero. We claim that the first i − 1 entries of qi are zero as well. To see this, assume for a moment that there is j < i such that the j-th entry of qi is non-zero. Then q⊤ j qi = e⊤ j qi ̸= 0 which contradicts the orthogonality of qj and qi. Hence, we conclude that the first i − 1 entries of qi are zero. In particular, we established that the only non-zero entry of qi is the i-th entry. Since qi must be a unit vector (by the Gram-Schmidt process), we get qi = ei, a contradiction. 2. We prove this by showing that the columns of P are orthonormal, i.e. they all have unit length and they are pairwise orthogonal. Clearly, all columns have unit length since they are standard unit vectors. Thus, it remains to show pairwise orthogonality. Let i ̸= j with i, j ∈ [n] be arbitrary. We want to show that the scalar product ep(i) · ep(j) is 0. We observe that we have ep(i) · ep(j) = 0 if and only if p(i) ̸= p(j). Moreover, by injectivity of p and since i ̸= j, we must have p(i) ̸= p(j). Hence, we conclude that the two vectors are orthogonal. This holds for any i ̸= j and hence we conclude that the columns of P are pairwise orthogonal. 3. a) Recall that Rθ = [cos(θ) − sin(θ) sin(θ) cos(θ) ] and hence R⊤ θ = [ cos(θ) sin(θ) − sin(θ) cos(θ) ] . In order to prove that Rθ is orthogonal, it suffices to show R⊤ θ Rθ = I. For this, we need the trigonometric identity sin 2(θ) + cos2(θ) = 1 to calculate R⊤ θ Rθ = [ cos2(θ) + sin2(θ) cos(θ) sin(θ) − sin(θ) cos(θ) sin(θ) cos(θ) − cos(θ) sin(θ) sin2(θ) + cos2(θ) ] = [1 0 0 1 ] = I. Hence, Rθ is indeed orthogonal. b) Consider the matrix A = [0 1 1 0 ] . Clearly, A is an orthogonal matrix. Moreover, A is not a rotation matrix because there is no θ ∈ R satisfying both 1 = sin(θ) and 1 = − sin(θ). c) Assume that A is orthogonal. Recall the formula for the 2 × 2 inverse A−1 = 1 ad − bc [ d −b −c a ] . Since A is orthogonal, we must have A⊤ = A−1. From this, we deduce a = d ad−bc , d = a ad−bc , c = −b ad−bc , and b = −c ad−bc . Note that ad − bc ̸= 0 since A is invertible. Assume first a ̸= 0. Then we obtain ad − bc = d a = a d since we also must have d ̸= 0. This implies |a| = |d| and |ad − bc| = 1. On the other hand, if we have a = 0 then we must have b ̸= 0 and c ̸= 0. Thus, we get ad − bc = −b c = −c b and therefore |b| = |c| and |ad − bc| = 1. d) Consider the matrix A that we get by setting a = d = √2 and b = c = 1. Clearly, we have |ad − bc| = 2 − 1 = 1. But A is not orthogonal since in particular, its two columns [√2 1]⊤ and [ 1 √2]⊤ are not orthogonal (and also they are not unit vectors). 4. a) We obtained the new datapoints from the old one by shifting them along the t-axis. Imagine a line that optimally fits the old datapoints. By shifting this line the same amount in the direction of the t-axis, we should get a line that optimally fits the new datapoints. In other words, it should be possible to obtain the optimal line for the new datapoints from the optimal line for the old datapoints by a shift along the t-axis. This should also work the other way around, i.e. given an optimal line for the new datapoints we should be able to shift the line by −c in the direction of the t-axis to get an optimal line for the old datapoints. It remains to think about this shifting operation in terms of the parameters ααα and ααα′. The main insight here is that shifting a line does not change its slope. Hence we expect to have α1 = α′ 1 but not α0 = α′ 0. b) We compute the scalar product and check that it yields zero. Concretely, we have    1 ... 1   ·    t′ 1 ... t′ m    = m∑ k=1 t ′ m = m∑ k=1(tm+c) = m∑ k=1(tk− 1 m m∑ i=1 ti) = m∑ k=1 tk−m 1 m m∑ i=1 ti = m∑ k=1 tk− m∑ i=1 ti = 0 and hence the columns of A′ are indeed orthogonal. c) We prove that for all ααα′ ∈ R2 and ααα = ααα′ + [cα′ 1 0 ] , we have ||A′ααα′ − b||2 = ||Aααα − b||2. From this, it follows that ααα′ is optimal if and only if ααα is optimal. Thus, consider an arbitrary ααα′ ∈ R2 and let ααα = ααα′ + [ cα′ 1 0 ]. We compute ||A′ααα′ − b|| 2 = m∑ k=1(bk − (α′ 0 + α′ 1t ′ k)) 2 = m∑ k=1(bk − (α′ 0 + α′ 1(tk + c))) 2 (t′ k = tk + c) = m∑ k=1(bk − ((α0 − cα′ 1) + α′ 1(tk + c))) 2 (α′ 0 = α0 − cα′ 1) = m∑ k=1(bk − ((α0 − cα1) + α1(tk + c)))2 (α′ 1 = α1) = m∑ k=1(bk − (α0 − cα1 + α1tk + α1c)) 2 = m∑ k=1(bk − (α0 + α1tk)) 2 = ||Aααα − b|| 2 and therefore conclude the proof. d) By the closed form solution and subtask b), we get that the optimal ααα′ satisfies [α′ 0 α′ 1 ] = [ 1 m ∑m k=1 bk ( ∑m k=1 t′ kbk)/( ∑m k=1 t′ k2) ] . By subtask c), the optimal ααα is hence given by [α0 α1 ] = [ 1 m ∑m k=1 bk + c( ∑m k=1 t′ kbk)/( ∑m k=1 t′ k2) ( ∑m k=1 t′ kbk)/( ∑m k=1 t′ k2) ] where c = − 1 m ∑m k=1 tk and t′ k = tk + c. This is a closed form solution because the right-hand side can be directly computed from the datapoints. 5. a) Let a1, a2, a3 be the columns of A. We first compute all the scalar products between columns of A. In particular, we get a ⊤ 1 a1 = m, a ⊤ 1 a2 = m∑ k=1 tk, a⊤ 1 a3 = m∑ k=1 t2 k, a⊤ 2 a2 = m∑ k=1 t2 k, a ⊤ 2 a3 = m∑ k=1 t 3 k, a⊤ 3 a3 = m∑ k=1 t4 k and therefore A ⊤A =   m ∑m k=1 tk ∑m k=1 t2 k∑m k=1 tk ∑m k=1 t2 k ∑m k=1 t3 k∑m k=1 t2 k ∑m k=1 t3 k ∑m k=1 t4 k   . b) For A⊤A to be diagonal, we need to have ∑m k=1 tk = 0, ∑m k=1 t2 k = 0, and ∑m k=1 t3 k = 0. The first and last condition are not so interesting, but note that the condition ∑m k=1 t2 k = 0 implies tk = 0 for all k ∈ [m] because we clearly have t2 k ≥ 0 for all k ∈ [m].","libVersion":"0.3.2","langs":""}