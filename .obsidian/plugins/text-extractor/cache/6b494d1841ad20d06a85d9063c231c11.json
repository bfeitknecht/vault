{"path":"sem3/A&D/VRL/script/A&D-script-w09-graph-algorithms.pdf","text":"Skript zur Vorlesung Algorithmen und Datenstrukturen Graphenalgorithmen: Fortsetzung Herbstsemester 2024 Stand: 11. Dezember 2024 Johannes Lengler David Steurer Inhaltsverzeichnis 1 K¨urzeste Wege: All Pairs Shortest Path 1 1.1 Motivation und Problemdefinition . . . . . . . . . . . . . . . . . . . 1 1.2 Das All-Pairs-Shortest-Path-Problem . . . . . . . . . . . . . . . . . . 1 1.3 Floyd-Warshall-Algorithmus . . . . . . . . . . . . . . . . . . . . . . . 2 1.3.1 Rekursion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2 1.3.2 Der Algorithmus . . . . . . . . . . . . . . . . . . . . . . . . . 3 1.3.3 Laufzeit . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 1.3.4 Korrektheit . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 1.3.5 Bestimmung der k¨urzesten Wege . . . . . . . . . . . . . . . . 4 1.3.6 Negative Zyklen . . . . . . . . . . . . . . . . . . . . . . . . . 4 1.3.7 Zusammenfassung . . . . . . . . . . . . . . . . . . . . . . . . 5 1.3.8 Allgemeine Antwort f¨ur k¨urzeste Wege . . . . . . . . . . . . . 6 1.4 Algorithmus von Johnson . . . . . . . . . . . . . . . . . . . . . . . . 6 1.4.1 Idee . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 1.4.2 Ver¨anderung der Kantengewichte . . . . . . . . . . . . . . . . 6 1.4.3 Wahl der H¨ohen . . . . . . . . . . . . . . . . . . . . . . . . . 7 1.4.4 Berechnung der H¨ohen . . . . . . . . . . . . . . . . . . . . . . 8 1.4.5 Der Algorithmus . . . . . . . . . . . . . . . . . . . . . . . . . 8 1.4.6 Laufzeit . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9 2 Matrizen und Graphen 11 2.1 Wege der L¨ange k . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11 2.1.1 Rekursionen . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12 2.1.2 Matrixmultiplikation . . . . . . . . . . . . . . . . . . . . . . . 12 2.2 Anwendungen . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14 2.2.1 Anzahl der Dreiecke . . . . . . . . . . . . . . . . . . . . . . . 14 2.2.2 Kreise der L¨ange 4 . . . . . . . . . . . . . . . . . . . . . . . . 14 2.2.3 Erreichbarkeit . . . . . . . . . . . . . . . . . . . . . . . . . . . 14 2.3 Ausblick: Erweiterungen der Matrixmultiplikation . . . . . . . . . . . 15 i ii Kapitel 1 K¨urzeste Wege: All Pairs Shortest Path In den vorherigen Kapiteln haben wir uns mit dem Problem besch¨aftigt, k¨urzeste Wege von einem gegebenen Startknoten S zu allen anderen Knoten in einem Graphen zu finden (One-to-all). Dieses Kapitel besch¨aftigt sich mit einer Erweiterung dieses Problems, bei der wir f¨ur alle Paare von Knoten die k¨urzesten Wege finden wollen. 1.1 Motivation und Problemdefinition Wir erinnern uns zun¨achst an die Algorithmen, die wir f¨ur One-to-all kennengelernt haben. Kosten Algorithmus Laufzeit alle Kosten 1 Breitensuche O(m + n) c(e) ≥ 0 Dijkstra O((m + n) log n) c(e) ∈ R Bellman-Ford O(nm) Tabelle 1.1 ¨Ubersicht ¨uber die Laufzeiten verschiedener Algorithmen f¨ur One-to-all k¨urzes- te Wege. Dabei ist n die Anzahl der Knoten und m die Anzahl der Kanten im Graphen. Wir haben gesehen, dass Dijkstra mit Laufzeit O((m + n) log n) der effizientes- te Algorithmus f¨ur One-to-all ist1, sofern die Kantengewichte nicht-negativ sind. Bellman-Ford hingegen kann auch mit negativen Kantengewichten umgehen, hat aber eine schlechtere Laufzeit von O(nm). Wir k¨onnen diese Algorithmen verwen- den, um das All-Pairs-Shortest-Path-Problem zu l¨osen, indem wir sie f¨ur jeden Knoten als Startknoten einmal ausf¨uhren. Dies f¨uhrt zu einer Gesamtlaufzeit von O(n(m + n) log n)) bzw. O(n2m). Das Ziel dieses Kapitels ist es, effizientere Algo- rithmen zu finden, die das Problem in geringerer Zeit l¨osen. 1.2 Das All-Pairs-Shortest-Path-Problem Formal betrachten wir das folgende Problem: Definition 1.1 (All-Pairs-Shortest-Path-Problem). Gegeben sei ein Graph G = (V, E) mit einer Kostenfunktion c : E → R, wobei |V | = n und |E| = m ist. F¨ur jedes Paar u, v ∈ V wollen wir einen k¨urzesten u-v-Weg finden, also einen Weg von u nach v mit minimalen Kosten. 1Mit Fibonacci Heaps kann die Laufzeit sogar noch etwas auf O(m + n log n) verbessert werden. 1 2 K¨urzeste Wege: All Pairs Shortest Path Beachten Sie, dass das Problem nicht wohldefiniert ist, wenn der Graph negative Zyklen enth¨alt. In diesem Fall k¨onnen wir die Kosten eines Weges beliebig klein machen, indem wir den negativen Zyklus immer wieder durchlaufen. Die folgende Tabelle fasst die Laufzeiten der Algorithmen f¨ur One-to-all und All-Pairs-Shortest-Path zusammen. Wir werden in diesem Kapitel die Algorithmen von Floyd-Warshall und Johnson im Detail besprechen. Kosten Algorithmus Laufzeit alle Kosten 1 n× Breitensuche O(n(m + n)) c(e) ≥ 0 n× Dijkstra O(n(m + n) log n)) c(e) ∈ R n× Bellman-Ford O(n2m) c(e) ∈ R Floyd-Warshall O(n3) c(e) ∈ R Johnson O(n(m + n) log n)) (falls keine negativen Zyklen) Tabelle 1.2 ¨Ubersicht ¨uber die Laufzeiten verschiedener Algorithmen f¨ur All-Pairs- Shortest-Path. 1.3 Floyd-Warshall-Algorithmus Der Floyd-Warshall-Algorithmus ist ein sehr eleganter Algorithmus, der auf dyna- mischer Programmierung basiert. Um ihn zu verstehen, betrachten wir zun¨achst das folgende Teilproblem: Definition 1.2 (Teilproblem des Floyd-Warshall-Algorithmus). Sei G = (V, E) ein gerichteter Graph mit Knotenmenge V = {1, . . . , n} und 1 ≤ i ≤ n und mit Kanten- kosten c(e) ∈ R, e ∈ E. F¨ur u, v ∈ V sei di u,v die L¨ange eines k¨urzesten Weges von u nach v, dessen Zwischenknoten (das heisst alle Knoten ausser dem Startknoten u und dem Endknoten v) aus der Menge {1, . . . , i} stammen. 2 1 3 4 2 1 3 1 Abb. 1.1 Beispiel zur Definition von di u,v: F¨ur den dargestellten Graphen ist d 2 2,4 = 5. Der einzige k¨urzeste Weg von 2 nach 4, der nur die Zwischenknoten 1 und 2 benutzt, ist der Weg ¨uber die Kante (2, 1). Der Weg ¨uber die Kante (2, 3) benutzt den Knoten 3 als Zwischenknoten und z¨ahlt daher nicht f¨ur d2 2,4. 1.3.1 Rekursion Die zentrale Idee des Floyd-Warshall-Algorithmus ist eine Rekursion, die di u,v mit- hilfe von Werten d i−1 u′,v′ berechnet. Dazu betrachten wir einen k¨urzesten Weg P von 1.3 Floyd-Warshall-Algorithmus 3 u nach v, dessen Zwischenknoten in {1, . . . , i} liegen, und unterscheiden drei F¨alle: 1. Der Knoten i kommt auf dem Weg P nicht vor. In diesem Fall ist P auch ein k¨urzester Weg von u nach v, dessen Zwischenknoten in {1, . . . , i − 1} liegen. Also gilt di u,v = di−1 u,v . 2. Der Knoten i kommt auf dem Weg P genau einmal vor. In diesem Fall k¨onnen wir P in zwei Teile zerlegen: Einen Weg von u nach i und einen Weg von i nach v. Da i nur einmal vorkommt, benutzt keiner dieser beiden Teilwege den Knoten i als Zwischenknoten. Also sind die beiden Teilwege k¨urzeste Wege von u nach i bzw. von i nach v, deren Zwischenknoten in {1, . . . , i − 1} liegen. Also gilt di u,v = d i−1 u,i + d i−1 i,v . 3. Der Knoten i kommt auf dem Weg P mehrfach vor. In diesem Fall enth¨alt P einen Zyklus, der den Knoten i enth¨alt. Sofern der Graph keine negativen Zyklen enth¨alt, k¨onnen wir diesen Zyklus entfernen und erhalten einen k¨urze- ren Weg von u nach v. Das ist ein Widerspruch zur Annahme, dass P ein k¨urzester Weg ist. Diesen Fall k¨onnen wir also ignorieren, sofern der Graph keine negativen Zyklen enth¨alt. Fassen wir diese drei F¨alle zusammen, so erhalten wir die folgende Rekursion, falls der Graph keine negativen Zyklen enth¨alt: d i u,v = min(d i−1 u,v , d i−1 u,i + d i−1 i,v ). Als Rekursionsanfang verwenden wir d 0 u,v =    0 falls u = v ist, c(u, v) falls (u, v) ∈ E ist, ∞ sonst. 1.3.2 Der Algorithmus Die Rekursion liefert direkt einen Algorithmus, der auf dynamischer Programmie- rung basiert. Wir berechnen die Werte di u,v f¨ur i = 1, . . . , n. In jedem Schritt i k¨onnen wir die Werte di u,v mithilfe der Werte di−1 u,v berechnen. Floyd-Warshall(G = (V, E), c) Floyd-Warshall 1 d0 u,v ←    0 falls u = v ist, c(u, v) falls (u, v) ∈ E ist, ∞ sonst ▷ Initialisierung 2 for i ← 1, . . . , n do 3 for u ← 1, . . . , n do 4 for v ← 1, . . . , n do 5 di u,v ← min(di−1 u,v , d i−1 u,i + d i−1 i,v ) 6 return d 4 K¨urzeste Wege: All Pairs Shortest Path 1.3.3 Laufzeit Die Laufzeit des Floyd-Warshall-Algorithmus ist offensichtlich O(n3), da wir drei ineinander verschachtelte Schleifen haben, die jeweils n Schritte lang sind. Wie so oft kann man auch hier den Speicherbedarf erheblich reduzieren, indem man die Werte di u,v nur jeweils f¨ur das aktuelle i speichert. Hier ist das mit einer verbl¨uffend einfachen Ver¨anderung im Pseudocode m¨oglich: Wir lassen die Indizes i und i − 1 einfach weg, ersetzen also Zeile 5 durch du,v ← min(du,v, du,i + di,v) und entfernen den Index 0 in Zeile 1. Dieser Trick funktioniert bei anderen Algorithmen nat¨urlich in der Regel nicht, aber hier kann man sich ¨uberlegen, dass immer noch alle Werte korrekt berechnet werden. 1.3.4 Korrektheit Die Korrektheit des Floyd-Warshall-Algorithmus folgt direkt aus der Invariante, dass nach dem i-ten Durchlauf der ¨ausseren Schleife f¨ur alle u, v ∈ V gilt: du,v enth¨alt die L¨ange eines k¨urzesten Weges von u nach v, dessen Zwischenknoten aus {1, . . . , i} stammen. Diese Invariante kann mit vollst¨andiger Induktion nach i bewiesen werden. Der Induktionsanfang folgt direkt aus dem Rekursionsanfang. Der Induktionsschritt folgt aus der Rekursion, die wir zuvor hergeleitet haben. Am Ende des Algorithmus, also nach dem n-ten Durchlauf der ¨ausseren Schleife, enth¨alt du,v also die L¨ange eines k¨urzesten Weges von u nach v, dessen Zwischenk- noten aus {1, . . . , n} stammen. Da dies alle Knoten des Graphen sind, ist dies ein k¨urzester Weg von u nach v ohne Einschr¨ankungen. 1.3.5 Bestimmung der k¨urzesten Wege Der Floyd-Warshall-Algorithmus, wie er oben angegeben ist, berechnet nur die L¨angen der k¨urzesten Wege, nicht aber die Wege selbst. Wir k¨onnen diese jedoch leicht durch R¨uckverfolgung rekonstruieren. Dazu speichern wir f¨ur jedes Paar (u, v) den Knoten i, der im letzten Schritt zu einer Verbesserung von du,v gef¨uhrt hat. Der k¨urzeste Weg von u nach v l¨asst sich dann rekursiv aus den k¨urzesten Wegen von u nach i und von i nach v zusammensetzen. 1.3.6 Negative Zyklen Wir haben bereits erw¨ahnt, dass der Floyd-Warshall-Algorithmus nur korrekt ist, falls der Graph keine negativen Zyklen enth¨alt. Falls doch, so ist die Rekursion im dritten Fall nicht mehr korrekt, und der Algorithmus kann zu falschen Ergebnissen f¨uhren. Tats¨achlich k¨onnen wir den Algorithmus sogar verwenden, um festzustellen, ob der Graph negative Zyklen enth¨alt. Der Schl¨ussel dazu ist die Beobachtung, dass der Algorithmus auch alle Wege ber¨ucksichtigt, die aus genau einer Schleife bestehen. Andere Wege werden nur teilweise ber¨ucksichtigt. Mithilfe dieser Beobachtung k¨onnen wir den folgenden Satz beweisen: Theorem 1.3. Ein Graph G = (V, E) enth¨alt genau dann einen negativen Zyklus, wenn es einen Knoten v ∈ V gibt mit dn v,v < 0. Beweis. Enth¨alt der Graph einen negativen Zyklus, so enth¨alt er auch einen einfachen negativen Zyklus, das heisst einen Zyklus, der keinen 1.3 Floyd-Warshall-Algorithmus 5 Knoten mehrfach besucht (ausser dem Start- und Endknoten, die ja identisch sind). Zum Beispiel ist der k¨urzeste negative Zyklus (mit der minimalen Zahl an Knoten) einfach: g¨abe es dort einen mehrfach auf- tretenden Knoten, k¨onnten wir ihn an diesem Knoten in zwei k¨urzere Zyklen aufspalten, von denen mindestens einer negativ sein muss. Sei C nun ein einfacher negativer Zyklus und sei j der maximale Index aller Knoten in C. Sei P ein Weg von j zu einem beliebigen Knoten i ∈ C, der nur in C verl¨auft. Sei R der restliche Weg in C von i zur¨uck nach j. Dann sind P und R k¨urzeste Wege von j nach i bzw. von i nach j, deren Zwischenknoten in {1, . . . , j − 1} liegen (denn C ist einfach und j der maximale Index). Das heisst, es gilt d j−1 j,i = c(P ), d j−1 i,j = c(R). Daher gilt d j j,j ≤ d j−1 j,i + d j−1 i,j = c(P ) + c(R) = c(C) < 0. Da der Wert von di j,j in den darauffolgenden Iterationen des Algorithmus nur kleiner werden kann, gilt also auch dn j,j < 0. Nehmen wir nun umgekehrt an, dass dn v,v < 0 f¨ur einen Knoten v ∈ V gilt. Dann k¨onnen wir die k¨urzesten Wege, die der Algorithmus berech- net, zur¨uckverfolgen und erhalten einen Weg von v nach v der L¨ange dn v,v < 0, also einen negativen Zyklus. Beachten Sie, dass der Algorithmus von Floyd-Warshall keine vollst¨andige Cha- rakterisierung der negativen Zyklen gibt. Falls dv,v < 0 f¨ur ein v ∈ V , so enth¨alt der Graph zwar einen negativen Zyklus, der v enth¨alt. Die Umkehrung gilt aber nicht: Ist dv,v = 0 f¨ur ein v ∈ V , so kann der Graph trotzdem einen negativen Zyklus enthalten, der v enth¨alt. 1 3 2 1 1 -52 Abb. 1.2 Hier berechnet die Rekursion d3 1,1 = min{d 2 1,1, d 2 1,3 + d 2 3,1} = min{0, 3} = 0, obwohl 1 sich in einem negativen Zyklus befindet und d 3 1,1 daher nicht definiert ist (und h¨ochstens d3 1,1 = −∞ Sinn ergeben w¨urde). Der Algorithmus gbt trotzdem keinen negativen Wert f¨ur d3 1,1 aus. 1.3.7 Zusammenfassung Fassen wir die Anwendung des Floyd-Warshall-Algorithmus zusammen: 1. Wir lassen den Floyd-Warshall-Algorithmus laufen. 6 K¨urzeste Wege: All Pairs Shortest Path 2. Falls es ein v ∈ V gibt mit dv,v < 0, so enth¨alt der Graph einen negativen Zyklus, und die Ausgabe des Algorithmus ist falsch. 3. Andernfalls enth¨alt der Graph keine negativen Zyklen, und die Ausgabe des Algorithmus ist korrekt. 1.3.8 Allgemeine Antwort f¨ur k¨urzeste Wege Ist dn u,v endlich, so gibt der Algorithmus also die L¨ange eines k¨urzesten Weges von u nach v zur¨uck. Ist dn u,v = ∞, so gibt es keinen Weg von u nach v. Es bleibt der Fall, dass dn u,v endlich ist, der Graph aber einen negativen Zyklus enth¨alt. In diesem Fall m¨ussen wir vorsichtig sein. Gibt es einen Knoten w, der sowohl von u als auch von v aus erreichbar ist und f¨ur den dw,w < 0 ist, dann enth¨alt der Graph einen negativen Zyklus, den wir von u aus erreichen und von dem aus wir v erreichen k¨onnen. In diesem Fall ist die L¨ange eines k¨urzesten Weges von u nach v nicht definiert, denn wir k¨onnen die Kosten beliebig klein machen, indem wir den negativen Zyklus immer wieder durchlaufen. Andernfalls ist dn u,v die L¨ange eines k¨urzesten Weges von u nach v, denn kein Weg von u nach v kann einen negativen Zyklus benutzen. In der Praxis sind Graphen mit negativen Zyklen oft nicht sinnvoll. Beispielsweise macht es keinen Sinn, in einem Navigationssystem einen Weg zu berechnen, der immer wieder denselben Kreis f¨ahrt. 1.4 Algorithmus von Johnson Der Algorithmus von Johnson ist ein alternativer Algorithmus zum L¨osen des All- Pairs-Shortest-Path-Problems, falls wir schon vorab wissen, dass der Graph keine negativen Zyklen enth¨alt. Er basiert auf dem Algorithmus von Dijkstra und hat eine Laufzeit von O(n(m + n) log n)). Damit ist er asymptotisch genauso schnell wie n-mal Dijkstra, aber er kann im Gegensatz zu Dijkstra auch mit negativen Kantengewichten umgehen. 1.4.1 Idee Die Grundidee des Algorithmus von Johnson ist es, die Kantengewichte des Graphen so zu ver¨andern, dass alle Kantengewichte nicht-negativ sind und die k¨urzesten Wege erhalten bleiben. Sobald wir dies erreicht haben, k¨onnen wir den Algorithmus von Dijkstra n-mal anwenden, um die k¨urzesten Wege zwischen allen Knotenpaaren zu berechnen. 1.4.2 Ver¨anderung der Kantengewichte Ein naiver Ansatz, die Kantengewichte zu ver¨andern, besteht darin, alle Kantenge- wichte um eine Konstante c zu erh¨ohen, die so gew¨ahlt ist, dass das kleinste Kan- tengewicht danach 0 ist. Dieser Ansatz funktioniert jedoch nicht, da er die k¨urzesten Wege im Graphen ver¨andern kann. Das Problem dieses Ansatzes ist, dass er verschiedene Wege unterschiedlich be- handelt, je nachdem wie lang sie sind. Ein Weg der L¨ange k wird um kc teurer, w¨ahrend ein Weg der L¨ange ℓ um ℓc teurer wird. F¨ur k ̸= ℓ sind dies verschiedene Werte, daher kann sich die Ordnung der Wege bez¨uglich ihrer Kosten ¨andern. 1.4 Algorithmus von Johnson 7 s v t -2 1 0 +2 s v t 0 3 2 Abb. 1.3 Beispiel, das zeigt, dass der naive Ansatz, die Kantengewichte um eine Kon- stante zu erh¨ohen, die k¨urzesten Wege ver¨andern kann. Der k¨urzeste Weg von S nach T f¨uhrt im linken Graphen ¨uber v, im rechten Graphen jedoch direkt von S nach T . Die L¨osung besteht darin, sogenannte teleskopierende Summen zu verwenden. Dazu w¨ahlen wir f¨ur jeden Knoten v ∈ V eine reelle Zahl h(v), die wir die H¨ohe von v nennen. Die neuen Kantengewichte definieren wir dann als c ′(u, v) = c(u, v) + h(u) − h(v). Betrachten wir nun einen Weg P = (v0, v1, . . . , vk) von s = v0 nach t = vk. Die Kosten von P bez¨uglich der neuen Kantengewichte c′ betragen c ′(P ) = k−1∑ i=0 c ′(vi, vi+1) = k−1∑ i=0(c(vi, vi+1) + h(vi) − h(vi+1)). Da sich alle Terme h(vi) f¨ur i = 1, . . . , k − 1 gegenseitig aufheben, erhalten wir c ′(P ) = c(P ) + h(s) − h(t). Das heisst, die Kosten eines Weges von s nach t bez¨uglich der neuen Kantengewichte c′ unterscheiden sich von den Kosten bez¨uglich der alten Kantengewichte c nur um die Konstante h(s)−h(t). Diese Konstante h¨angt nur von den Start- und Endknoten ab, nicht aber vom Weg selbst. Daher bleibt der k¨urzeste Weg zwischen zwei Knoten auch nach der Transformation der Kantengewichte der k¨urzeste Weg. 1.4.3 Wahl der H¨ohen Um den Algorithmus von Dijkstra anwenden zu k¨onnen, m¨ussen alle Kantengewichte nicht-negativ sein. Wir m¨ussen also die H¨ohen h(v) so w¨ahlen, dass c′(u, v) ≥ 0 f¨ur alle (u, v) ∈ E gilt. Die Idee von Johnson ist es, einen neuen Knoten z zum Graphen hinzuzuf¨ugen und f¨ur jeden Knoten v ∈ V eine Kante (z, v) mit Gewicht 0 hinzuzuf¨ugen. Die H¨ohe h(v) definieren wir dann als die L¨ange eines k¨urzesten Weges von z nach v. Mit dieser Definition von h(v) gilt f¨ur alle (u, v) ∈ E: h(v) ≤ h(u) + c(u, v). Denn h(v) ist die L¨ange eines k¨urzesten Weges von z nach v, und h(u) + c(u, v) ist die L¨ange eines Weges von z ¨uber u nach v. Umformen dieser Ungleichung liefert: c(u, v) + h(u) − h(v) ≥ 0 ⇔ c ′(u, v) ≥ 0. 8 K¨urzeste Wege: All Pairs Shortest Path NEU u v G 0 0 c((u, v)) h(u) h(v) Abb. 1.4 Illustration der Idee von Johnson: Wir f¨ugen einen neuen Knoten z zum Gra- phen hinzu und verbinden ihn mit allen anderen Knoten durch Kanten mit Gewicht 0. Die H¨ohe h(v) eines Knotens v ist dann die L¨ange eines k¨urzesten Weges von z nach v. Also sind alle Kantengewichte c′(u, v) nicht-negativ, und wir k¨onnen den Algorith- mus von Dijkstra anwenden. 1.4.4 Berechnung der H¨ohen Um die H¨ohen h(v) zu berechnen, wenden wir den Algorithmus von Bellman-Ford an, um das One-to-All-Shortest-Paths-Problem im um z erweiterten Graphen mit Startknoten z zu l¨osen. Die L¨ange des k¨urzesten Weges von z nach v ist dann h(v). Beachten Sie, dass der Algorithmus von Bellman-Ford nur funktioniert, wenn der Graph keine negativen Zyklen enth¨alt. Falls doch, so ist der Algorithmus von Johnson nicht anwendbar. s v t -2 1 0 s v t z -2 1 0 0 0 0 s v t 0 0 1 Abb. 1.5 Beispiel f¨ur den Algorithmus von Johnson. Links ist der Originalgraph darge- stellt, in der Mitte der um den Knoten z erweiterte Graph und rechts der Graph mit den transformierten Kantengewichten c ′. Man beachte, dass der k¨urzeste Weg von S nach T in allen drei Graphen ¨uber V verl¨auft. 1.4.5 Der Algorithmus Der Algorithmus von Johnson l¨asst sich wie folgt zusammenfassen: 1. Erzeuge einen neuen Knoten z und f¨ur jeden Knoten v ∈ V eine neue Kante {z, v} mit Gewicht 0. 2. Berechne h(v) f¨ur alle v ∈ V mit dem Algorithmus von Bellman-Ford, wobei z der Startknoten ist. 1.4 Algorithmus von Johnson 9 3. Berechne c′(u, v) = c(u, v) + h(u) − h(v) f¨ur alle (u, v) ∈ E. 4. Wende n-mal den Algorithmus von Dijkstra an, um die k¨urzesten Wege bez¨uglich der Kantengewichte c′ zu finden. 5. Ist d′(u, v) die L¨ange eines k¨urzesten Pfades bez¨uglich der Kosten c′, so hat dieser Pfad bez¨uglich der Kosten c die L¨ange d(u, v) = d′(u, v) − h(u) + h(v). Da der Korrekturterm −h(u) + h(v) f¨ur alle Pfade identisch ist, bleibt ein k¨urzester Pfad f¨ur c′ auch ein k¨urzester Pfad f¨ur c. 1.4.6 Laufzeit Die Laufzeit des Algorithmus von Johnson setzt sich wie folgt zusammen: 1. Das Erzeugen von z und den neuen Kanten ben¨otigt Zeit O(n). 2. Bellman-Ford ben¨otigt Zeit O(nm). 3. Das Berechnen der neuen Kantengewichte c′ ben¨otigt Zeit O(m). 4. Das n-malige Anwenden von Dijkstra ben¨otigt Zeit O(n(m + n) log n)). Insgesamt betr¨agt die Laufzeit des Algorithmus von Johnson also O(n(m+n) log n)). F¨ur d¨unn besetzte Graphen, also Graphen mit m ≪ n2, ist dies besser als die Laufzeit des Floyd-Warshall-Algorithmus, die O(n3) betr¨agt. 10 K¨urzeste Wege: All Pairs Shortest Path Kapitel 2 Matrizen und Graphen Wir haben Matrizen bereits verwendet, um Graphen darzustellen. Die Adjazenz- matrix eines Graphen G = (V, E) ist eine n × n-Matrix AG, wobei n = |V | die Anzahl der Knoten in G ist. Der Eintrag in Zeile i und Spalte j ist 1, falls die Kante (i, j) in E enthalten ist, und 0 sonst. In diesem Kapitel werden wir sehen, dass der Zusammenhang zwischen Matrizen und Graphen tats¨achlich noch viel enger ist. 2.1 Wege der L¨ange k Wir nehmen wieder an, dass die Knotenmenge V = {1, . . . , n} ist. Wir betrachten Wege von i nach j der L¨ange k. Damit meinen wir, dass die Wege genau k Kanten enthalten. i s j Weg der L¨ange k − 1 Abb. 2.1 Jeder Weg der L¨ange k von i nach j l¨asst sich zerlegen in einen Weg der L¨ange k − 1 von i zu einem Knoten s und eine Kante von s nach j. Jeder solche Weg hat die folgende Form: Er startet bei i, besucht k − 1 weitere Knoten und endet bei j. Insbesondere gibt es einen vorletzten Knoten s auf diesem Weg. Wir k¨onnen diesen Weg also zerlegen in einen Weg der L¨ange k − 1 von i nach s und eine Kante von s nach j. Diese einfache Beobachtung ist der Schl¨ussel zu den folgenden ¨Uberlegungen. Wir wollen die folgenden drei Fragen beantworten: 1. Gibt es einen Weg von i nach j der L¨ange k? 2. Was sind die minimalen Kosten eines Weges von i nach j der L¨ange k? 3. Wie viele Wege von i nach j der L¨ange k gibt es? F¨ur die zweite Frage wollen wir hier annehmen, dass es keine negativen Zyklen gibt. Um diese Fragen zu beantworten, betrachten wir die folgenden drei Gr¨ossen: L(k) i,j = { 1 falls es einen Weg von i nach j der L¨ange k gibt, 0 sonst. M (k) i,j = minimale Kosten eines Weges von i nach j der L¨ange k (oder ∞, falls es keinen solchen Weg gibt). N (k) i,j = Anzahl der Wege von i nach j der L¨ange k. 11 12 Matrizen und Graphen 2.1.1 Rekursionen Die Zerlegung, die wir oben beschrieben haben, liefert uns direkt Rekursionsformeln f¨ur L, M und N : Existenz eines Weges Es gibt genau dann einen Weg von i nach j der L¨ange k, falls es einen Knoten s ∈ {1, . . . , n} gibt, so dass es einen Weg von i nach s der L¨ange k − 1 gibt und eine Kante von s nach j. Das heisst, es gilt L (k) i,j = n⋁ s=1 (L (k−1) i,s ∧ L (1) s,j ) . Dabei steht ⋁ f¨ur die logische Oder-Operation und ∧ f¨ur die logische Und-Operation. Der Basiswert f¨ur k = 1 ist L(1) i,j = 1, falls es die Kante (i, j) gibt, und 0 sonst. Somit ist die Matrix L(1) also identisch zur Adjazenzmatrix AG. Minimale Kosten Die minimalen Kosten eines Weges von i nach j der L¨ange k sind das Minimum der Kosten aller Wege, die sich in einen Weg der L¨ange k − 1 von i nach s und eine Kante von s nach j zerlegen lassen. Das heisst, es gilt M (k) i,j = n min s=1 (M (k−1) i,s + M (1) s,j ) . Der Basiswert ist M (1) i,j = c(i, j), falls es eine Kante von i nach j mit Kosten c(i, j) gibt. Sonst setzen wir M (1) i,j = ∞. Anzahl der Wege Um die Anzahl der Wege von i nach j der L¨ange k zu berechnen, summieren wir ¨uber alle Knoten s die Anzahl der Wege von i nach s der L¨ange k −1, die wir mit der Kante (s, j) fortsetzen k¨onnen. Das heisst, es gilt N (k) i,j = n∑ s=1 (N (k−1) i,s · N (1) s,j ) . Hier ist der Basiswert wieder N (1) i,j = 1, falls die Kanten (i, j) existiert, und 0 sonst. Die Matrix N (1) ist also wieder identisch zur Adjazenzatrix AG. Wir sehen, dass alle drei Formeln eine ¨ahnliche Struktur aufweisen. Der wesent- liche Unterschied sind die beiden verwendeten Operationen: Im ersten Fall ∨ und ∧, im zweiten Fall min und +, im dritten Fall + und ·. Wir werden sp¨ater auf diese ¨Ahnlichkeiten zur¨uckkommen. Zun¨achst schauen wir uns die dritte Formel genauer an. 2.1.2 Matrixmultiplikation Wir bemerken, dass die Rekursionsformel f¨ur N (k) i,j eine starke ¨Ahnlichkeit zur Formel f¨ur die Matrixmultiplikation hat. Seien A und B zwei n × n-Matrizen. Dann ist das Produkt C = A · B definiert durch Ci,j = n∑ s=1 Ai,s · Bs,j. 2.2 Wege der L¨ange k 13 Vergleichen wir diese Formel mit der Rekursionsformel f¨ur N (k) i,j , so sehen wir, dass wir die Anzahl der Wege von i nach j der L¨ange k durch Matrixmultiplikation berechnen k¨onnen! Dazu betrachten wir die Matrizen N (k−1) und N (1) = AG, wobei N (k−1) die Matrix ist, deren Eintr¨age die Anzahl der Wege der L¨ange k − 1 zwischen allen Knotenpaaren enth¨alt. Dann ist N (k) = N (k−1) · AG. F¨ur k = 1 ist N (1) = AG. Das heisst, wir k¨onnen die Matrizen N (k) iterativ berechnen: N (1) = AG, N (2) = N (1) · AG = A2 G, N (3) = N (2) · AG = A3 G, ... N (k) = N (k−1) · AG = Ak G. Hierbei bezeichnet Ak G die k-te Potenz der Matrix AG. Wir formulieren diese Beobachtung als Satz: Theorem 2.1. Sei G = (V, E) ein Graph mit Adjazenzmatrix AG. Dann ist der Eintrag (i, j) in Ak G die Anzahl der Wege von i nach j in G der L¨ange k. Der Beweis dieses Satzes erfolgt mit vollst¨andiger Induktion nach k. Der Induk- tionsanfang ist trivial. Der Induktionsschritt folgt direkt aus der Rekursionsformel f¨ur N k i,j und der Definition der Matrixmultiplikation. Beweis. Wir beweisen den Satz mit vollst¨andiger Induktion nach k. Induktionsanfang (k = 1) Die Aussage des Satzes f¨ur k = 1 ist genau die Definition der Adjazenzmatrix: Ai,j ist 1 falls (i, j) ∈ E, und 0 sonst. Induktionsschritt (k → k + 1) Die Induktionshypothese ist, dass der Eintrag (i, j) in Ak G die Anzahl der Wege von i nach j in G der L¨ange k ist. Wir m¨ussen zeigen, dass der Eintrag (i, j) in Ak+1 G die Anzahl der Wege von i nach j in G der L¨ange k + 1 ist. Nach Definition der Matrixmultiplikation ist (Ak+1 G )i,j = (Ak G · AG)i,j = n∑ s=1(A k G)i,s · (AG)s,j. Nach Induktionshypothese ist (Ak G)i,s die Anzahl der Wege von i nach s der L¨ange k. Der Term (AG)s,j ist 1 falls (s, j) ∈ E, und 0 sonst. Also ist (Ak G)i,s · (AG)s,j die Anzahl der Wege von i nach j der L¨ange k + 1, deren vorletzter Knoten s ist. Summieren wir ¨uber alle m¨oglichen Knoten s, so erhalten wir die Anzahl aller Wege von i nach j der L¨ange k + 1, also (Ak+1 G )i,j, wie behauptet. 14 Matrizen und Graphen 2.2 Anwendungen Wir k¨onnen diesen Satz verwenden, um interessante Fragen ¨uber Graphen zu beant- worten. 2.2.1 Anzahl der Dreiecke Wir betrachten zun¨achst die Frage, wie viele Dreiecke in einem Graphen G enthalten sind. Dabei nehmen wir an, dass der Graph gerichtet ist und keine Schleifen enth¨alt, also keine Kanten der Form (v, v). Ein Dreieck ist ein Zyklus der L¨ange 3. Nach dem obigen Satz ist der Eintrag (i, i) in A3 G die Anzahl der Wege von i nach i der L¨ange 3. Da der Graph keine Schleifen enth¨alt, ist jeder solche Weg ein Dreieck. Der Eintrag (i, i) in A3 G entspricht also der Anzahl Dreiecke, die den Knoten i enthalten. Summieren wir nun ¨uber alle i, so z¨ahlen wir jedes Dreieck dreimal, weil jedes Dreieck ja f¨ur jeden der drei Knoten gez¨ahlt wird. Daher ist die Anzahl der Dreiecke in G gleich 1 3 · n∑ i=1(A3 G)i,i = 1 3 · Spur(A3 G). Dabei ist die Spur einer Matrix die Summe ihrer Diagonalelemente. 2.2.2 Kreise der L¨ange 4 K¨onnen wir auch Kreise der L¨ange 4 z¨ahlen? Wieder wollen wir annehmen, dass wir einen gerichteten Graphen ohne Schleifen haben. Ein guter Startwert ist sicherlich die Spur der Matrix A4 G. Aber hier m¨ussen wir etwas aufpassen. Im Eintrag (i, i) dieser Matrix z¨ahlen wir alle Wege der L¨ange 4 von i nach i. Betrachten wir einen solchen Weg i − u − v − w − i. Dann sind u und w verschieden von i, weil es keine Schleifen gibt. Aber der Knoten v k¨onnte mit i identisch sein, wir z¨ahlen also f¨alschlicherweise auch Wege der Form i − u − i − w − i. Wie viele solcher Wege gibt es? Dazu m¨ussen wir wissen, wie viele Wege der L¨ange 2 es von i nach i gibt. Denn wenn es k solcher Wege gibt, dann haben wir k Wahlm¨oglichkeiten f¨ur das erste Teilst¨uck i − u − i und k M¨oglichkeiten f¨ur das zweite Teilst¨uck i − w − i, insgesamt also k2 viele Kombination. Diese m¨ussen wir von (A4 G)i,i abziehen, damit wir nur Kreise ¨ubrig behalten. Den Wert k k¨onnen wir in (A2 G)i,i nachschauen. Die Zahl der Kreise der L¨ange 4, die i enthalten, ist daher (A4 G)i,i − ((A2 G)i,i)2. Dabei bezieht sich im zweiten Term das Quadratzeichen an AG auf das Matrixprodukt, w¨ahrend das ¨aussere Quadratzeichen normales Quadrieren einer ganzen Zahl anzeigt. Summieren wir nun ¨uber alle i, so z¨ahlen wir jeden Kreis vierfach, sodass wir insgesamt die folgende Formel erhalten: Zahl der Kreise der L¨ange 4 = 1 4 · n∑ i=1 ((A4 G)i,i − ((A2 G)i,i) 2) = 1 4 · (Spur(A4 G) − n∑ i=1((A2 G)i,i)2). 2.2.3 Erreichbarkeit Als zweite Anwendung betrachten wir die Frage, ob man in einem gerichteten Gra- phen G von jedem Knoten aus alle anderen Knoten erreichen kann. 2.3 Ausblick: Erweiterungen der Matrixmultiplikation 15 Dazu verwenden wir den folgenden Trick: Wir f¨ugen zu G alle Schleifen hinzu, also alle Kanten der Form (v, v). Dies ver¨andert die Erreichbarkeit von Knoten nicht. Allerdings k¨onnen wir jetzt in einem Knoten “warten”, indem wir die Schleife an diesem Knoten verwenden. Angenommen, es gibt einen Weg von u nach v in G. Dann gibt es auch einen Weg von u nach v der L¨ange h¨ochstens n − 1, denn wir k¨onnen vermeiden, dass Knoten auf dem Weg mehrfach vorkommen. Durch die zus¨atzlichen Schleifen k¨onnen wir diesen Weg verl¨angern, indem wir in v warten, bis der Weg L¨ange n − 1 hat. Also gibt es in dem um die Schleifen erweiterten Graphen genau dann einen Weg von u nach v, wenn es in A n−1 G einen Eintrag ungleich 0 an Position (u, v) gibt. Wir k¨onnen also die Frage, ob man in G von jedem Knoten aus alle anderen Knoten erreichen kann, beantworten, indem wir An−1 G berechnen und pr¨ufen, ob alle Eintr¨age ungleich 0 sind. Wir werden im n¨achsten Semester aber noch effizientere Methoden kennen lernen, um dieses Problem zu l¨osen. Laufzeit der Berechnung von An−1 G Wir haben gesehen, dass wir die Anzahl der Wege der L¨ange k und damit auch die Erreichbarkeit von Knoten durch Potenzieren der Adjazenzmatrix berechnen k¨onnen. Die naive Methode, An−1 G zu berechnen, besteht darin, n − 2 Matrixmulti- plikationen durchzuf¨uhren. Da jede Matrixmultiplikation Zeit O(n3) ben¨otigt, hat die naive Methode eine Laufzeit von O(n4). Durch iteriertes Quadrieren k¨onnen wir die Laufzeit auf O(n3 log n) verbessern.1 Dazu berechnen wir Ak G mit einer top-down Rekursion entweder ¨uber die Formel Ak G = Ak/2 G · Ak/2 G (falls k gerade) oder ¨uber die Formel Ak G = A(k−1)/2 G · A (k−1)/2 G · AG. In jeder Iteration halbieren wir das ben¨otigte k, deshalb brauchen wir nur O(log n) Schritte. Mit dem Strassen-Algorithmus k¨onnen wir die Laufzeit der Matrixmultiplikati- on auf O(n2.807) verbessern. Damit hat die Berechnung von An−1 G durch iteriertes Quadrieren eine Laufzeit von O(n2.807 log n). Es ist ein offenes Problem, ob die Laufzeit der Matrixmultiplikation noch weiter verbessert werden kann. Der aktuelle Rekord liegt bei O(n2.3715). Allerdings ist dieser Algorithmus so komplex, dass er in der Praxis nicht verwendet wird. 2.3 Ausblick: Erweiterungen der Matrixmultiplikation Kehren wir zur¨uck zu den drei Rekursionsformeln aus Sektion 2.1.1: L (k) i,j = n⋁ s=1 (L (k−1) i,s ∧ L (1) s,j ) . M (k) i,j = n min s=1 (M (k−1) i,s + M (1) s,j ) . N (k) i,j = n∑ s=1 (N (k−1) i,s · N (1) s,j ) . 1Informatik-Studierende der ETH kennen das Verfahren aus den ¨Ubungen und aus der Vorlesung zur Linearen Algebra. 16 Matrizen und Graphen Wie schon festgestellt, weisen alle drei Formeln die gleich Struktur auf, aber mit unterschiedlichen Operationen: Im ersten Fall ∨ und ∧, im zweiten Fall min und +, im dritten Fall + und ·. Es stellt sich heraus, dass alle drei Paare von Operationen sogenannte Halbringe bilden, das heisst, dass sie die Assoziativit¨atsgesetze und das Distributivgesetz erf¨ullen.2 Man kann sich ¨uberlegen, dass die meisten Matrizenoperationen f¨ur beliebige Halbringe Sinn machen. Insbsondere gilt immer noch das Assoziativgesetz. Damit kann man den Trick des iterierten Quadrierens auch f¨ur die ersten beiden F¨alle anwenden. Wir k¨onnen also nicht nur alle N (n) i,j in Zeit O(n3 log n) ausrechnen, son- dern ebenfalls alle L (n) i,j und alle M (n) i,j .3 Diese Betrachtungsweise ¨offnet die T¨ur f¨ur weitreichende Verallgemeinerungen von Algorithmen, die uns schon bekannt sind. 2Informatik-Studierende der ETH kennen das Konzept des Rings aus der Diskreten Mathematik. Ein Halbring ist fast ein Ring, nur dass wir darauf verzichten, dass es f¨ur die erste Operation Inverse gibt. Die erste Operation wird traditionellerweise Addition genannt, ist hier aber nur im dritten Fall die herk¨ommliche Addition auf R. 3Aber Strassen’s Algorithmus benutzt eine Minus-Operation, benutzt also, dass es ein additives Inverses gibt. Diese Verbesserung funktioniert also nur f¨ur Ringe, nicht f¨ur Halbringe.","libVersion":"0.3.2","langs":""}