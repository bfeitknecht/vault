{"path":"sem3/LinAlg/VRL/LinAlg-script-1.pdf","text":"Linear Algebra, First Part Lecture Notes Bernd G¨artner September 7, 2024 Contents 0 Introduction 4 0.1 About these notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 0.2 About computer science (and mathematics) . . . . . . . . . . . . . . . . . . . 5 0.3 About linear algebra . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 1 Vectors 10 1.1 Vectors and linear combinations . . . . . . . . . . . . . . . . . . . . . . . . . . 10 1.1.1 Vector addition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11 1.1.2 Scalar multiplication . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14 1.1.3 Linear combinations . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14 1.1.4 Affine, conic, and convex combinations . . . . . . . . . . . . . . . . . 18 1.1.5 Defining the dots: sequences, sums, sets, and vectors . . . . . . . . . 19 1.2 Scalar products, lengths and angles . . . . . . . . . . . . . . . . . . . . . . . . 21 1.2.1 Scalar product . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22 1.2.2 Euclidean norm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23 1.2.3 Cauchy-Schwarz inequality . . . . . . . . . . . . . . . . . . . . . . . . 26 1.2.4 Angles . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28 1.2.5 Triangle inequality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30 1.3 Linear independence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31 1.3.1 Definition and examples . . . . . . . . . . . . . . . . . . . . . . . . . . 31 1.3.2 Alternative definitions . . . . . . . . . . . . . . . . . . . . . . . . . . . 33 1.3.3 Span of vectors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35 2 Matrices 38 2.1 Matrices and linear combinations . . . . . . . . . . . . . . . . . . . . . . . . . 38 2.1.1 Matrix-vector multiplication . . . . . . . . . . . . . . . . . . . . . . . 41 2.1.2 Column space and rank . . . . . . . . . . . . . . . . . . . . . . . . . . 42 2.1.3 Row space and transpose . . . . . . . . . . . . . . . . . . . . . . . . . 45 2.1.4 Rank-1 matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47 2.2 Matrix multiplication . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48 2.2.1 Everything is matrix multiplication . . . . . . . . . . . . . . . . . . . 51 2.2.2 Distributivity and associativity . . . . . . . . . . . . . . . . . . . . . . 52 2.2.3 CR decomposition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55 1 2.3 Matrices and linear transformations . . . . . . . . . . . . . . . . . . . . . . . 57 2.3.1 Matrices as functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57 2.3.2 Linear transformations . . . . . . . . . . . . . . . . . . . . . . . . . . . 59 2.3.3 The matrix of a linear transformation . . . . . . . . . . . . . . . . . . 63 2.3.4 Linear transformations and matrix multiplication . . . . . . . . . . . 64 2.3.5 Kernel and Image . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65 3 Solving Linear Equations Ax = b 67 3.1 Systems of linear equations . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67 3.1.1 The PageRank algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . 68 3.1.2 Computer vectors and matrices . . . . . . . . . . . . . . . . . . . . . . 71 3.2 Gauss elimination . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 72 3.2.1 Back substitution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 72 3.2.2 Elimination . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73 3.2.3 Success and failure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77 3.2.4 Runtime . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 81 3.3 Inverse matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 85 3.3.1 Definition and basic properties . . . . . . . . . . . . . . . . . . . . . . 86 3.3.2 The Inverse Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . 88 3.4 LU and LUP decomposition . . . . . . . . . . . . . . . . . . . . . . . . . . . . 90 3.4.1 LU decomposition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 90 3.4.2 Permutations and permutation matrices . . . . . . . . . . . . . . . . . 94 3.4.3 LUP decomposition . . . . . . . . . . . . . . . . . . . . . . . . . . . . 98 3.5 Gauss-Jordan elimination . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 102 3.5.1 (Reduced) row echelon form . . . . . . . . . . . . . . . . . . . . . . . 102 3.5.2 Direct solution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 103 3.5.3 Elimination . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 104 3.5.4 Runtime . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 109 3.5.5 Computing the CR decomposition . . . . . . . . . . . . . . . . . . . . 110 4 The Four Fundamental Subspaces 113 4.1 Vector spaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 113 4.1.1 Definition and examples . . . . . . . . . . . . . . . . . . . . . . . . . . 114 4.1.2 Subspaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 117 4.2 Bases and dimension . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 121 4.2.1 Bases . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 121 4.2.2 The Steinitz exchange lemma . . . . . . . . . . . . . . . . . . . . . . . 125 4.2.3 Dimension . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 127 4.3 Computing the fundamental subspaces . . . . . . . . . . . . . . . . . . . . . 129 4.3.1 Column space . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 129 4.3.2 Row space . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 130 4.3.3 Nullspace . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 132 4.3.4 Left nullspace . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 135 2 4.3.5 The solution space of Ax = b . . . . . . . . . . . . . . . . . . . . . . . 137 4.3.6 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 139 Bibliography 142 Index 143 3 Chapter 0 Introduction 0.1 About these notes These are the lecture notes for the first part of the course Lineare Algebra (401-0131-00L) held at the Department of Computer Science at ETH Z ¨urich in HS24. These notes can be considered as a full version of what I plan to write on the tablet during the lectures. Lecture plans will be made available before each lecture. The tablet notes (in German) reflect the reality and will be made available after each lecture. To- gether with the explanations (and answers to questions) given in the lectures, they will contain the “essentials”, but in order to fully understand them, it can be helpful to look up more details and additional explanations in the lecture notes. In content, the lecture notes are based on the book Introduction to Linear Algebra (Sixth Edition) by Gilbert Strang, Wellesley - Cambridge Press, 2023 [Str23]. The main difference is that the approach taken here is more rigorous than Strang’s. Strang introduces the material on an intuitive level, guided by many examples; this pro- vides a great informal introduction to Linear Algebra. What we add here (hopefully with- out losing the intuition) are formal definitions of concepts, as well as mathematical state- ments with proofs for the key results. Strang’s book is not part of the course’s official material, and there is no need for stu- dents to buy the book (it doesn’t have an official electronic version). With the lectures, lecture plans, tablet notes, lecture notes, exercises, and exercises classes, the course is self- contained. Strang’s book and others mentioned on the course web page serve as optional literature. Z ¨urich, September 3, 2024 Bernd G¨artner 4 0.2 About computer science (and mathematics) As a first semester student of computer science, you may wonder why one of the first things you see is mathematics, and in particular linear algebra. In order to answer this, we first need to answer a different question: What is computer science? Computer scientists are frequently approached for help with installing computers, getting the internet to work, and similar technical tasks. To many people, a computer sci- entist is simply an information technology expert. But this point of view is fundamentally wrong. The computer scientist Mike Fellows has explained this very well already in 1991, using a simple analogy: Computer science is not about machines in the same way that astronomy is not about telescopes. There is an essential unity of mathematics and computer science [Fel93]. The first sentence of this quote (often misattributed to Edward Dijkstra) is well-known and gets the main point across: computer science is not mainly about computers. Every- one agrees that computers, just like telescopes, are great tools, but they merely help us in achieving some goals, they are not the goals themselves. It is true that astronomers are involved in building telescopes, and computer scientists are involved in building com- puters. Generally, if you need a tool that you cannot buy off the shelf, you will team up with people that can build it for you, and this needs expertise from both sides. But in the end, you want to use the tool for something, so you are not mainly interested in the tool itself. The second part of the quote is less known, but not less important. It indicates that computer science and mathematics are very strongly connected. To understand this, we need to say what computer science actually is. If you ask the internet for a definition of computer science, you get many wrong answers that start with “the study of comput- ers. . . ”, even from serious sources. The Wikipedia article about computer science starts differently and does not mention computers in the first sentence: 1 Computer science is the study of computation, information, and automa- tion. This is correct but a bit too short; the German version of the page, 2 has a more de- tailed and clearer definition, taken from the “Duden Informatik A-Z” [CSB06, Eintrag Informatik, S. 305]. Informatik ist die Wissenschaft von der systematischen Darstellung, Spei- cherung, Verarbeitung und ¨Ubertragung von Informationen, besonders der automatischen Verarbeitung mit Hilfe von Digitalrechnern. 1https://en.wikipedia.org/wiki/Computer_science, accessed on August 2, 2024 2https://de.wikipedia.org/wiki/Informatik, accessed on August 2, 2024 5 In English, this reads as follows: Computer science is the science of the systematic representation, storage, processing and transfer of information, in particular of the automatic process- ing using computers. This means, computer science is about dealing with information. The German term Informatik transports this message much better than the English term Computer science. As an example, let’s consider addition as you (and children throughout many cen- turies) have learned it in primary school, for example 123 + 486 = 609 This is systematic processing of information (in this case numbers), represented in decimal place-value system. Hence, this is not only mathematics, but according to the above definition, it is also computer science! In modern terms, we would call schoolbook addition an algorithm. Computers can do such additions automatically and very fast (this is what the “in particular” part of the definition is about), but the algorithm itself was invented much earlier than the computers. While an algorithm is mostly associated with computer science, the theoretical foun- dations of many algorithms and other computer science inventions are inherently math- ematical. Schoolbook addition is a prime example. Here, the most important theoretical foundation is the place-value system. This is one of the great historical developments in mathematics, driven by the need to efficiently compute with numbers. Today, there are new needs, for example, efficiently training huge machine learning models, or securing computer systems against cyberattacks. This needs established as well as new mathematics. Mathematical research is often motivated by applications in computer science, and mathematicians work with computer science tools on a daily ba- sis. For these reasons, every computer science student needs mathematical foundations, and every mathematics student needs computer science foundations. An essential unity, indeed. 0.3 About linear algebra The origins of (linear) algebra can be traced back to the 9th century when the Persian polymath Al-Khwarizmi published The Compendious Book on Calculation by Completion and Balancing. The word algebra also goes back to this book, see the highlighted part on the title page shown in Figure 1. In modern Arabic letters and transcribed to Latin letters (right to left), this reads as follows: = = 6 Al Khwarizmi’s book teaches how to systematically solve linear and quadratic equa- tions in one variable. While this is basic highschool material today, the theory underlying it had to be developed at some point, and it was Al-Khwarizmi who did this; for exam- ple, the word balancing in the title of his work refers to the technique of moving a term to the other side of an equation, something you need to do when you want to solve for a variable. Figure 1: Title page of Al-Khwarizmi’s book; the highlighted text in the lines written from bottom to top is the Arabic word Al-jabr. This is a higher-resolution image of the one found on Wikipedia (https://en.wikipedia.org/wiki/Al-Jabr), provided to the author by Digital Bodleian, https://digital.bodleian.ox.ac.uk, CC-BY-NC 4.0. A translation of the title page can be found here The field of linear algebra has developed from two historical roots: analytic geometry and linear equations. Analytic geometry deals with the description of and calculation with geometric objects through coordinates and formulas. We could also call it “rigorous” 7 xy32(3, 2) Figure 2: A point in the two-dimensional plane, expressed in Cartesian coordinates geometry. Everybody knows what a circle is, but if you want to describe a particular circle, you need to say what the center and the radius are. The center is a point that you typically describe with Cartesian coordinates, as in Figure 2. Having such rigorous descriptions of geometric objects, you can start to answer ques- tions about them analytically by using mathematics; for example, do two given lines in three-dimensional space intersect or not? In answering this and many other questions, systems of linear equations in more than one variable come up, and this is the second root of linear algebra. Today, such systems are considered as easy to solve in many cases, but this is the result of developments that happened over centuries. And they still happen now: solving systems of linear equations is an active research topic, in particular since the traditional algorithms cannot handle the very large systems that we have in many applications today. Even small systems are not necessarily easy for a human and form a basis of many puzzles. Consider this one: Dominik is twice as old as Susanne and three years older than Claudia. Together, the children are 17 years old. How old are the three children? Even without having heard about linear equations, you will be able to figure this out, employing some guesswork and mental arithmetic. But this quickly reaches its limits when more children are involved in the puzzle. There is a reason why such puzzles that are made for entertainment rarely have more than three variables (in this case, the ages of the children). As a system of three linear equations, the puzzle reads as follows: D = 2S D = C + 3 D + S + C = 17 Here, D, S, and C are variables for the unknown ages of the children, and the three equa- tions encode the three pieces of information that the puzzle provides. The equations are linear because every variable occurs only in the first power. In contrast, x2 − 2x + 3 = 0 is a quadratic equation, because the variable x occurs in the second power as well. There are also cubic, quartic, quintic, etc. equations. Solving systems of those falls into the domain of (non-linear) algebra. Linear algebra deals with m linear equations in n variables, where both numbers m and n can be very large in practice. 8 In the above linear puzzle equations, you can convince yourself that there are unique numbers that you can plug in for D, S, and C such that all three equations are satisfied. Conveniently, these numbers are natural numbers; you don’t want a puzzle where a child is 2.7 years or −3 years old. But in principle, the solutions to a system of linear equations can be arbitrary numbers, and it is up to the application to decide whether these make sense or not. Interestingly, Al-Khwarizmi only provided formulas for positive solutions of equa- tions in his book, as he thought that only those make sense. Indeed, in a world where numbers count physical quantities, the idea of a negative number seems downright crazy. However, in Al-Khwarizmi’s computations (including “balancing”), negative numbers implicitly occur in order to arrive at the positive solutions. Starting from the roots of analytic geometry and systems of linear equations, linear algebra has grown many important branches, some of which appear in Figure 3 and also later in these notes. systems of linear equationsanalytic geometryvector spacescomplex number planematricesnumerical linear algebralinear transformationsdeterminantseigenvaluesvectors Figure 3: The tree of linear algebra: Roots and some important branches 9 Chapter 1 Vectors 1.1 Vectors and linear combinations Vectors in R m and their linear combinations are fundamental in linear algebra and at the same time easy to understand. In fact, you may know much of this material from highschool. This leaves room to also learn about some important elements of mathematical thinking and writing. Most notably, you will see a first proof. Vectors as you know them from highschool “live” in 2-dimensional or 3-dimensional space. More abstractly, they live in some m-dimensional space, where m ∈ N, the set {0, 1, 2, . . .} of natural numbers. 1 Starting from m = 4, these spaces are hard to visualize, but linear algebra can handle them as easily as R 2 and R 3. Mathematically, these spaces are sets that are called R 2, R 3, and R m, where R is the set of real numbers. R 2, the xy-plane, contains (has as elements) all pairs (v1, v2) of real numbers, for example (4, 1). R 3, the xyz-space, contains all triples (v1, v2, v3) of real num- bers, for example (−2, 2, 3). R m contains all tuples or sequences (v1, v2, . . . , vm) of m real numbers. Here, the numbers from 1 to m serve as indices indicating the position in the sequence: for example, v5 denotes the 5-th number in the sequence. When we think of the elements of R 2 or R 3 as vectors, we typically draw them as arrows in a Cartesian coordinate system, with the tail of the arrow at the origin, and the head at the respective coordinates. We use column vector notation, indicating that we now think of a pair (or triple, or tuple) as a vector; see Figure 1.1. Sometimes, we use row vector notation as in [ 4 1] . In referring to a vector in text or in a formula, we use bold lower case Latin letters such as v and w. You may have learned to write vectors as ⃗v, ⃗w, but v is as good (or bad) as ⃗v. The important thing is to be consistent. The zero vector corresponds to the origin and is written as 0, in every dimension. This is what mathematicians call an abuse of notation. The abuse here is that the meaning of 0 1For mathematicians, 1 is typically the first natural number; for computer scientists it’s 0. None of the two choices is better or worse than the other one. In these “linear algebra for computer scientists” notes, we start with 0. 10 xy [ 4 1 ][ −2 2 ] 0 = [ 0 0 ] 0xyz   −2 2 0     −2 2 3  0 =   0 0 0   03 v =      v1 v2 ... vm      , w =      w1 w2 ... wm      Figure 1.1: R 2, the xy-plane R 3, the xyz-space Column vectors in R m depends on the context and may, as in Figure 1.1, refer to the zero vector in R 2 or the zero vector in R 3. Such abuse of notation is common and also known from natural language. If you take “the bike”, it is clear that you mean your bike, while your friends, saying the same thing, refer to their bikes. The arrow drawing suggests an interpretation of a vector as a movement, for example “go 4 steps right and 1 step up!” Under this interpretation, the arrow can actually be placed anywhere and still visualizes the same vector. It can also be useful to visualize a vector as a point (at its coordinates); see Figure 1.2. xy [ 4 1 ] 0 [ 4 1 ] xy [ 4 1 ] 0 Figure 1.2: Vector: visualization as arrow (left), or point (right) As an example why this is useful, let’s try to visualize the set of vectors in R 2 whose coordinates sum up to 5, highlighting a specific such vector. Then it should be clear that Figure 1.3 (right), with the gray line showing the vectors in question as points at their arrowheads, is more suitable than Figure 1.3 (left) that is just cluttered with arrows. 1.1.1 Vector addition Adding two vectors combines their movements. Depending on which movement we do first, we can take two different routes to the same result; together, these routes form a parallelogram. Algebraically, vector addition works coordinate-wise, i.e. we add up corre- sponding coordinates of the two vectors; see Figure 1.4. 11 xy [ 4 1 ] 0xy [ 4 1 ] 0 Figure 1.3: Vectors v in R 2 with v1 + v2 = 5: visualization as arrows (left), or points at their arrowheads (right) xy [ 2 3 ][ 5 2 ][ 3 −1 ] [2 3 ] + [ 3 −1 ] = [ 5 2 ] Figure 1.4: The parallelogram of vector addition Definition 1.1 (Vector addition). Let v =      v1 v2 ... vm      , w =      w1 w2 ... wm      ∈ R m. The vector v + w :=      v1 + w1 v2 + w2 ... vm + wm      ∈ R m is the sum of v and w. While examples are very useful to understand a concept, a definition is the official ref- erence. The goal of a definition is to introduce the concept in full generality. Definition 1.1 tells you how to add any two vectors of any dimension. The symbol := is used to define what’s left of it by what’s right of it. In the same way, we can add more vectors. For example u + v + w is the sum of three vectors, and in this case, we get six possible routes to the result; see Figure 1.5. Unfortunately, Definition 1.1 does not define u + v + w. It only talks about adding two vectors at a time. So it defines (u + v) + w or u + (v + w). Since addition in R is associative, it doesn’t matter where we put the brackets, so it is customary to omit them and write u + v + w. Similarly, in talking about the two possible routes to arrive at v + w, we implicitly used that v + w = w + v which holds since addition in R is commutative. The six possible routes for three vectors come from the fact that we can add them up in six different orders. It is an element of mathematical thinking to clarify the meaning of u + v + w when this was not explicitly defined. Only after that, you really understand the meaning and can 12 xy [ 2 3 ][ 3 −1 ][ 5 4 ][ 10 6 ] [2 3 ] + [ 3 −1 ] + [5 4 ] = [10 6 ] Figure 1.5: Adding three vectors safely write u + v + w. As a Swiss luxury watch commercial puts it: to break the rules, you must first master them. Figure 1.6 illustrates some of the routes one can go in adding up 9 vectors. We haven’t drawn all possible routes, as this would lead to a very cluttered figure. Figure 1.6: Adding 9 vectors (solid colored arrows); the result is the black arrow, and the dashed arrows outline possible routes. The drawing was made using the programming language Processing, https://processing.org. Challenge 1.2. How many routes to the result are there in adding up 9 vectors? Or n vectors? Can you describe which of them have been selected for Figure 1.6? 13 1.1.2 Scalar multiplication This corresponds to moving λ times as far, for some real number λ (known as the scalar). For scalars, we typically use lower case Greek letters. We say that the resulting vector is a scalar multiple of the original one. The scalar can be negative, in which case we move into the opposite direction. Algebraically, scalar multiplication multiplies each coordinate of the vector with the scalar; see Figure1.7. xy [ 2 1 ][ 6 3 ][ −4 −2 ] 3 [2 1 ] = [ 6 3 ] (−2) [2 1 ] = [−4 −2 ] Figure 1.7: Scalar multiplication Definition 1.3 (Scalar multiplication). Let v =      v1 v2 ... vm      ∈ R m, λ ∈ R. The vector λv :=      λv1 λv2 ... λvm      ∈ R m is a scalar multiple of v. Note that the zero vector 0 is a scalar multiple of every vector, obtained by choosing scalar λ = 0. 1.1.3 Linear combinations This operation combines vector addition and scalar multiplication. Definition 1.4 (Linear combination). Let v, w ∈ Rm, λ, µ ∈ R. The vector λv + µw ∈ Rm is a linear combination of v and w. In general, if v1, v2, . . . , vn ∈ R m and λ1, λ2, . . . , λn ∈ R, then λ1v1 + λ2v2 + · · · + λnvn is a linear combination of v1, v2, . . . , vn. Table 1.1 gives three linear combinations of two specific vectors v and w. What are all the linear combinations of v and w that can we get in this way? Here is the answer: 14 v = [ 2 3 ] , w = [ 3 −1 ] : λ µ λv µw λv + µw −3 2 [−6 −9 ] [ 6 −2 ] [ 0 −11 ] 1 −1 [ 2 3 ] [−3 1 ] [−1 4 ] 3 0 [ 6 9 ] [ 0 0 ] [ 6 9 ] Table 1.1: Three linear combinations of two vectors v, w Fact 1.5. Every vector in R 2 is a linear combination of the two vectors v = [2 3 ] , w = [ 3 −1 ] . Proof. Let u = [u1 u2 ] be an arbitrary vector in R2. We need to show that we can find scalars λ, µ ∈ R such that λ [ 2 3 ] + µ [ 3 −1 ] = [ u1 u2 ] . Considering the rules of vector addition and scalar multiplication, this vector equation is actually made up of two “normal” equations, one for each coordinate: 2λ + 3µ = u1, 3λ − 1µ = u2. This is a system of two linear equations in two variables λ and µ, and the rest is therefore a highschool job. Well, almost: What you may find unusual here is that the system also contains the variables u1, u2. But while the variables λ and µ stand for the unknown numbers that we want to compute, u1 and u2 stand for known numbers, the entries of our target vector u. By solving this system of equations (the solution will depend on u1 and u2), we therefore solve it for all possible values of u1 and u2 at the same time. And this was exactly the point, since we want to make the argument for every possible vector u. This is what differentiates a proof from a calculation: a proof makes one argument for many (possibly infinitely many) situations, while a calculation just handles one situation. After this digression, let’s solve the system: Multiplying the second equation by 3 and adding it to the first one cancels the variable µ: 2λ + 3µ = u1 9λ − 3µ = 3u2 11λ = u1 + 3u2 15 This gives λ = u1 + 3u2 11 . To get µ, we isolate µ in one of the equations (let’s take the first one) and substitute the value of λ that we just got: 3µ = u1 − 2λ = u1 − 2u1 + 6u2 11 = 11u1 − (2u1 + 6u2) 11 = 9u1 − 6u2 11 . Dividing by 3 yields µ = 3u1 − 2u2 11 . So λ and µ have been found for all possible values of u1 and u2 which completes the proof. It’s always good to double check this on examples. Let’s look at the three linear com- binations that we have previously computed in Table 1.1 and see whether the formulas for λ and µ give us back the correct scalars. Table 1.2 shows that they do. v = [ 2 3 ] , w = [ 3 −1 ] : λ µ λv + µw = u u1 u2 u1+3u2 11 3u1−2u2 11 −3 2 [ 0 −11 ] 0 −11 −3 2 1 −1 [−1 4 ] −1 4 1 −1 3 0 [ 6 9 ] 6 9 3 0 Table 1.2: Checking the formulas from the proof of Fact 1.5 on three vectors We can also understand this proof geometrically. The two equations 2λ + 3µ = u1 and 3λ − 1µ = u2 can be drawn as lines in the λµ-plane, and their point of intersection is the desired solution to both equations. For u1 = −1, u2 = 4 (middle row of Table 1.2), the two lines are drawn in Figure 1.8 (left). Unsurprisingly, their intersection is at (λ, µ) = (1, −1). The crucial observation is that for other values of u1, u2, the lines are different but still parallel to the lines for u1 = −1, u2 = 4; see Figure 1.8 (right). So there is always an intersection, meaning that we find values λ, µ for every vector u. This was the “row picture” behind the proof. There is also a “column picture”, see Figure 1.9. The two vectors v and w define axes of a skewed coordinate system (solid lines). Given a target vector u, we make shifted copies of both axes such that they cross in u (dashed lines). Between the two pairs of axes, a parallelogram emerges, and this is exactly the one that expresses u as a sum of scaled versions of v and w, the gray arrows in Figure 1.9 (right). 16 λµλ = 1, µ = −13λ − 1µ = 42λ + 3µ = −1λµ3λ − 1µ = u22λ + 3µ = u1 Figure 1.8: “Row picture” behind the proof of Fact 1.5, based on the rows of the equation system. We find the target scalars by intersecting two lines in the λµ-plane. xy [ 2 3 ][ 3 −1 ][ u1 u2 ] 0xy [ 2 3 ][ 3 −1 ][ u1 u2 ] λ [ 2 3 ] µ [ 3 −1 ] 0 Figure 1.9: “Column picture” behind the proof of Fact 1.5, based on the columns of the equation system: We find the target scalars as coordinates in a skewed coordinate system. Fact 1.5 may be wrong for other vectors. As an example, consider v = [ 2 3 ] , w = [4 6 ] . Here, w is a scalar multiple of v, so every linear combination of the two is just another scalar multiple of v. Therefore, the linear combinations form a line through v, and a vector not on that line cannot be obtained as a linear combination. Challenge 1.6. Try to prove a version of Fact 1.5 where v = [ v1 v2 ] , w = [w1 w2 ] 17 are arbitrary vectors. Where does the proof fail if the two vectors are scalar multiples of each other? What goes wrong in the row and column pictures in this case? And why does the proof indeed succeed when the two vectors are not scalar multiples of each other? 1.1.4 Affine, conic, and convex combinations In many applications, we are not interested in all linear combinations of the given vectors. The following three types of special linear combinations are of particular importance. Definition 1.7 (Affine, conic, convex combination). A linear combination λ1v1 +λ2v2 +· · ·+ λnvn of vectors v1, v2, . . . , vn is called (i) an affine combination if λ1 + λ2 + · · · + λn = 1, (ii) a conic combination if λj ≥ 0 for j = 1, 2, . . . , n, and (iii) a convex combination if it is both an affine and a conic combination. We will illustrate these notions with linear combinations λv + µw of two vectors v, w ∈ R 2 that are not scalar multiples of each other. By Challenge 1.6, the set of lin- ear combinations of such vectors is the whole plane R 2; see Figure 1.10 (i). What is the set of affine combinations? If λ + µ = 1, we can write λv + µw = λv + (1 − λ) ︸ ︷︷ ︸ µ w = w + λ(v − w). Hence, the set of all affine combinations is the set {w + λ(v − w) : λ ∈ R}. This is the line through v and w (interpreted as points; see Figure 1.3). Plugging in λ = 0 gives w, and for λ = 1, we obtain v. Values of λ between 0 and 1 lead to points in between. For λ < 0 we end up left of w, and for λ > 1, we will be right of v. Generally, any point on the line is obtained by the rule “go to w, then make a step parallel to v − w!”. See Figure 1.10 (ii). In a conic combination, the parallelogram for adding λv and µw is always in the angle between v and w , and by varying λ and µ, we can get every vector in this angle, officially called the cone spanned by v and w; see Figure 1.10 (iii). Finally, the convex combinations are by definition all points on the line through v and w that are also in the cone spanned by v and w. This set is the line segment spanned by v and w, see Figure 1.10 (iv). Challenge 1.8. Understand the affine, conic and convex combinations of three vectors in R 2! 18 vwvwv − wvwλv + µwλvµwvw linear (i) affine (ii) conic (iii) convex combinations of two vectors Figure 1.10: Two vectors and their combinations 1.1.5 Defining the dots: sequences, sums, sets, and vectors The dots notations as used in v1, v2, . . . , vn and λ1v1 + λ2v2 + · · · + λnvn and      v1 v2 ... vm      are fine, but it is never too early to get to know the precise mathematical notations. This will also help in really understanding the dots notations. Formally, when we write v1, v2, . . . , vn, we mean the sequence (v1, v2, . . . , vn) of n vectors; we typically omit the surrounding brackets if we don’t need the sequence itself as a mathematical object. There is a more concise way of writing the sequence: (v1, v2, . . . , vn) = (vj) n j=1. Similarly, sums have a mathematical notation: λ1v1 + λ2v2 + · · · + λnvn = n∑ j=1 λjvj. Here, j is called the summation index. The benefit of this notation becomes apparent when we discuss some special cases. What if n = 2? It’s not entirely clear how to interpret v1, v2, . . . , vn in this case. Does v2 now appear twice? And if n = 1? Then there is not even a vector v2, so why is it mentioned? In starting with v1, v2, . . ., we want to indicate a pattern that continues until vn, and we therefore mean (v1, v2) if n = 2 and (v1) if n = 1. Still, it’s potentially confusing to always mention three vectors v1, v2 and vn, even if there are less. In contrast, the notation (vj) n j=1 19 has a clear definition: it’s the sequence of all vectors vj where j goes through the range from 1 to n in increasing order. This range contains the integers j that satisfy 1 ≤ j ≤ n. This naturally covers the cases n = 1 and n = 2. For the sum, it’s the same: n∑ j=1 λjvj is obtained by summing up all λjvj for which j goes through the range from 1 to n. This brings us to the last special case: what if n = 0, so there are no vectors? The mathematical notation handles this elegantly: as the range of integers from 1 to 0 is empty, we have (vj) 0 j=1 = (), the empty sequence. And 0∑ j=1 λjvj = 0, the zero vector. Indeed, if you add up vectors, you start from 0 and then add one vector at a time. But if there are no vectors to add, you remain stuck at 0. This is like a scale that shows a weight of 0 before you put anything on it. As a consequence, 0 is a linear combination of every sequence of vectors, even the empty one. Having made it clear that for n = 0, v1, v2, . . . , vn is the empty sequence and λ1v1 + λ2v2 + · · · + λnvn = 0, there is also no harm if we keep using the dot notations. For novices, they may be easier to read than (vj) n j=1 and ∑n j=1 λjvj. But as in particular the sum notation is standard throughout many sources, it is also good to get used to it early. When we reorder a sequence of vectors, we get the same linear combinations, because vector addition is commutative. Also, if a vector appears more than once in the sequence, we can omit its duplicates and still get the same linear combinations. This means, all that matters is the set of vectors involved in the sequence (vj) n j=1. We will write this set as {vj : j ∈ [n]}. Here, [n] is an unambiguous notation for the the range from 1 to n, considered as a set. Within a set, the order does not matter, so we prefer the notation [n] over {1, 2, . . . , n}. But the latter notation and also {v1, v2, . . . , vn} or {vj : j = 1, 2, . . . , n} for the set of vectors are perfectly fine as well. The important thing is that there is no order of vectors in the set, and every vector only appears once. We can write {v1, v1, v2}, but this is the same as {v1, v2} or {v2, v1}. Finally, the vertical dots: a vector      v1 v2 ... vm      20 can more concisely be written as [vi] m i=1. This is very similar to the sequence notation, and indeed, a vector as an element of R m is formally just a sequence of m real numbers. Using square brackets instead of normal brackets indicates that we mean a column vector. This notation for example allows us to write down the vector [i 2]5 i=1. What does this mean? The expression in brackets tells us what the i-th entry of the vector should be, and the range after the bracket tells us which range of i’s we are considering. Hence, [i 2]5 i=1 =       1 4 9 16 25       . Similarly, [0] 6 i=1 is the 6-dimensional zero vector. Using this notation, we could also define the sum of two vectors more “efficiently” than Definition 1.1 does it, namely as follows: [vi] m i=1 + [wi]m i=1 := [vi + wi]m i=1. We can still use the vertical dots notation as it is more readable, despite needing more space. But now that we know what it precisely means, we for example understand that for m = 1,      v1 v2 ... vm      = [v1] ∈ R 1. What about the case m = 0? Are there vectors with no entries? Formally, they should be elements of R 0. Actually, R0 contains exactly one element, namely the empty sequence () of real numbers. We can rightfully consider this as the 0-dimensional zero vector. But talking about this case in any further detail would get us into nerd territory. 1.2 Scalar products, lengths and angles The scalar product of two vectors is a number that lets us measure the length of a vector and the angle between two vectors. Scalar products naturally appear all over linear algebra and have many practical applications. Scalar products are also at the heart of two important inequalities, the Cauchy-Schwarz inequality, and the triangle inequality. Given how vector addition works (Section 1.1.1), you might expect vector multiplica- tion to work like this: [1 2 ] · [ 3 4 ] = [1 · 3 2 · 4 ] = [3 8 ] . 21 This is known as the Hadamard product, but in the range of linear algebra products, it only occupies a niche. Among the topsellers, we find the scalar product whose result is not a vector, but a number (or a scalar, in linear algebra jargon; this is where the name comes from). 1.2.1 Scalar product The scalar product of two vectors is obtained by multiplying corresponding coordinates, and adding up the products: [1 2 ] · [3 4 ] = 1 · 3 + 2 · 4 = 11. Definition 1.9 (Scalar product). Let v =      v1 v2 ... vm      , w =      w1 w2 ... wm      ∈ R m. The scalar product of v and w is the number v · w := v1w1 + v2w2 + · · · + vmwm = m∑ i=1 viwi. From this definition, we can directly derive some rules that are frequently needed in computing with scalar products. We summarize these in an observation. This is a state- ment whose proof is simple and straightforward enough to be omitted. Observation 1.10. Let u, v, w ∈ R m be vectors and λ ∈ R a scalar. Then (i) v · w = w · v; (symmetry) (ii) (λv) · w = λ(v · w) = v · (λw); (taking out scalars) (iii) u · (v + w) = u · v + u · w and (u + v) · w = u · w + v · w; (distributivity) (iv) v · v ≥ 0, with equality exactly if v = 0. (positive-definiteness) While (i) and (iv) are quite obvious from the definition of the scalar product, (ii) and (iii) need some straightforward calculations as a proof. To show what we mean by “straightforward”, we provide the calculations for the first part of (iii). These use dis- tributivity in R: u · (v + w) = m∑ i=1 ui(vi + wi) = m∑ i=1 (uivi + uiwi) = m∑ i=1 uivi + m∑ i=1 uiwi = u · v + u · w. Properties (ii) and (iii) together are known as linearity in both arguments. 22 1.2.2 Euclidean norm The Euclidean norm of a vector v is obtained by taking the square root of the scalar prod- uct with itself. We can do this, since this scalar product is nonnegative by Observa- tion 1.10 (iv). Definition 1.11 (Euclidean norm). Let v ∈ R m. The Euclidean norm of v is the number ∥v∥ := √ v · v. Some sources use |v| for the norm, but we reserve this notation for the absolute value of a number; for example, |3| = | − 3| = 3. You can think of the Euclidean norm as defining the length of a vector. You may argue that we don’t need to define this, since a vector already has a length (just measure how long the arrow is). But this is true only in R 2 and R 3 where we can draw vectors as arrows. In higher dimensions, it is not a priori clear how to measure the length of a vector v. But now we know: compute its Euclidean norm ∥v∥! In R 2 and R 3, the Euclidean norm indeed measures the length of the arrow; so we’re not really inventing a new concept of length here, we simply extend a familiar concept to higher dimensions. To see this, we first expand the scalar product to obtain the following formula for the Euclidean norm: ∥ ∥ ∥ ∥ ∥ ∥ ∥ ∥ ∥      v1 v2 ... vm      ∥ ∥ ∥ ∥ ∥ ∥ ∥ ∥ ∥ = √ v2 1 + v2 2 + · · · + v2 m = √ √ √ √ m∑ i=1 v2 i . For example, ∥ ∥ ∥ ∥ [ −4 2 ]∥ ∥ ∥ ∥ = √ (−4)2 + 22 = √ 20, ∥ ∥ ∥ ∥ ∥ ∥   −4 2 3   ∥ ∥ ∥ ∥ ∥ ∥ = √(−4)2 + 22 + 32 = √29. Let’s first look at the situation in R 2 where Figure 1.11 is the key. The vector v = [v1 v2 ] is the hypotenuse of a right-angled triangle whose legs have lengths |v1| and |v2|, see Figure 1.11. Hence, using the Pythagorean theorem, the squared length of v is |v1| 2 + |v2| 2 = v2 1 + v2 2 = ∥v∥2, and “length of v equals ∥v∥ is obtained by taking square roots. Building on this, Fig- ure 1.12 deals with the situation in R3. 23 v = [ v1 v2 ] 0|v2||v1|90 0 √ v2 1 + v2 2 Figure 1.11: The Euclidean norm measures the length of a vector in R 2.   v1 v2 0   0 √ v2 1 + v2 2|v3| v =   v1 v2 v3   90 0|v1||v2| √ v2 1 + v2 2 + v2 3 Figure 1.12: The Euclidean norm measures the length of a vector in R 3. In R3, the vector v =   v1 v2 v3   is the hypotenuse of a right-angled triangle whose legs have lengths √ v2 1 + v2 2 (as just computed) and |v3|; see Figure 1.12. Thus, Pythagoras tells us that the squared length of v is √ v2 1 + v2 2 2 + |v3| 2 = v2 1 + v2 2 + v2 3 = ∥v∥ 2. Unit vectors. A unit vector is a vector u such that ∥u∥ = 1. In R 2, the unit vectors lie on the unit circle with center 0 and radius 1; see Figure 1.13. Using Definition 1.11 of the Euclidean norm, and taking out scalars (Observation 1.10 (ii)), it is easy to see that for every vector v ̸= 0, the vector v ∥v∥ 24 0vu1 v ∥v∥ Figure 1.13: A unit vector u on the unit circle. For every nonzero vector v, the scaled vector v/∥v∥ is a unit vector. is a unit vector, where scalar division is just scalar multiplication with the reciprocal: v ∥v∥ := 1 ∥v∥v. In R m, there are m standard unit vectors. These are the ones that have one coordinate equal to 1 and all others equal to 0. We use the notation ei for the standard unit vector that has the 1 at the i-th coordinate: R 3 : e1 =  1 0 0   , e2 =   0 1 0   , e3 =   0 0 1   Rm : ei =        0 ... 1 ... 0        ← coordinate i In R2, the two standard unit vectors are the ones in the directions of x- and y-axis. In R 3, e3 goes along the z-axis; see Figure 1.14. e1 = [ 1 0 ] e2 = [ 0 1 ] xy e1 =   1 0 0  e2 =   0 1 0   xy e3 =   0 0 1   z Figure 1.14: The standard unit vectors in R 2 and R 3 Other norms. To stress the point that the length of a vector is nothing God-given, we remark that there are many other norms for vectors that make sense and are being used. 25 Here are two examples, the 1-norm and the ∞-norm: ∥v∥1 := m∑ i=1 |vi| (1-norm) and ∥v∥∞ := m max i=1 |vi| (∞-norm). In these norms, the “unit circles” look different, see Figure 1.15. The vectors in R 2 that have length 1 according to the 1-norm form a “diamond”; under the ∞-norm, we get a square. The standard unit vectors are also unit vectors under the 1-norm and the ∞-norm. 0u∥u∥ = 1∥u∥∞ = 10∥u∥1 = 1uu0 Figure 1.15: Unit vectors in the Euclidean norm (left), 1-norm (middle), ∞-norm (right) All three norms are special cases of p-norms, where p ≥ 1 can be any real number: ∥v∥p = p √ √ √ √ m∑ i=1 |vi|p. We see that the Euclidean norm is actually the 2-norm, but we still write it as ∥v∥, not ∥v∥2, since for us, it is the “standard” norm. The ∞-norm is an abuse of notation, since ∞ is not a real number. But as “p goes to infinity”, the largest coordinate of v in absolute value is all that matters. In formulas, lim p→∞ ∥v∥p = ∥v∥∞. 1.2.3 Cauchy-Schwarz inequality As innocent as it looks, the importance of this inequality cannot be overestimated. Lemma 1.12 (Cauchy-Schwarz inequality). For any two vectors v, w ∈ R m, |v · w| ≤ ∥v∥∥w∥. Moreover, equality holds exactly if one vector is a scalar multiple of the other. 26 Generally, a lemma is a helper statement that may not be very interesting on its own; but the more it actually helps, the more important it becomes. In this sense, a lemma is like a Swiss army knife, where the Cauchy-Schwarz inequality is a highly multifunctional one. Proof. We first consider the case where v and w are unit vectors, so ∥v∥ = ∥w∥ = 1. Using Observation 1.10 and Definition 1.11 of the Euclidean norm, we compute 0 ≤ (v − w) · (v − w) = v · v︸︷︷︸ + w · w︸ ︷︷ ︸ − 2v · w = 2 − 2v · w ⇒ v · w ≤ 1, ∥v∥ 2 ∥w∥ 2 0 ≤ (v + w) · (v + w) = ︷︸︸︷ v · v + ︷ ︸︸ ︷ w · w + 2v · w = 2 + 2v · w ⇒ v · w ≥ −1. Summarized as |v · w| ≤ 1, this proves the Cauchy-Schwarz inequality for unit vectors. Now suppose v and w are arbitrary vectors. If one of them is 0, the inequality holds (both sides are 0). If v ̸= 0 and w ̸= 0, we can apply the previous calculations after scaling v and w to unit length. This gives −1 ≤ v ∥v∥ · w ∥w∥ ≤ 1, Taking out scalars according to Observation 1.10 (ii) and multiplying all three terms with ∥v∥∥w∥ results in −1 ≤ v · w ∥v∥∥w∥ ≤ 1 and − ∥v∥∥w∥ ≤ v · w ≤ ∥v∥∥w∥. This can be summarized as |v·w| ≤ ∥v∥∥w∥ which proves the Cauchy-Schwarz inequality in general. For the “Moreover” part, we need to understand under which conditions equality holds. Going back to the calculations with the unit vectors, we see that the conditions are precisely 0 = (v − w)(v − w) or 0 = (v + w)(v + w). By positive-definiteness of the scalar product (Observation 1.10(iv)), this translates to v − w = 0 or v + w = 0. In other words, the two unit vectors are either the same or opposite vectors. For the unit vectors v/∥v∥ and w/∥w∥, it is easy to see that this is the case exactly if v and w are scalar multiples of each other. If equality is due to one of v and w being 0, it’s still true that one vector (namely 0) is a scalar multiple of the other one. Here is an application of the Cauchy-Schwarz inequality. Imagine that you have m squares with total area A, and you put them next to each other as in Figure 1.16. How much horizontal space do you need at most? To solve this, let v1, v2, . . . , vm be the side lengths of the squares. Then the horizontal space needed is m∑ i=1 vi. 27 ? Figure 1.16: Horizontal alignment of m squares with total area A Let v ∈ R m be the vector with coordinates v1, v2, . . . , vm. We know that ∥v∥ 2 = m∑ i=1 v2 i = A. Furthermore, let 1∈ R m be the vector with all coordinates equal to 1. We have ∥1∥ = √m. By the Cauchy-Schwarz inequality, m∑ i=1 vi = 1 · v ≤ ∥1∥∥v∥ = √ m√ A, so we need at most √mA horizontal space. If v is a scalar multiple of 1 (meaning that all squares have the same size), we need exactly √mA. Otherwise, we need less. Exercise 1.13. For the 1-norm and ∞-norm as defined on page 26, prove that the following in- equalities hold for every vector v ∈ R m. ∥v∥ ≤ ∥v∥1 ≤ √ m∥v∥ and ∥v∥∞ ≤ ∥v∥ ≤ √ m∥v∥∞. 1.2.4 Angles In R 2 and R3, the angle between two vectors is simply the angle between their arrows; see Figure 1.17. vwα Figure 1.17: The angle α between two vectors v and w As with the length, we are looking for a way to define the angle between two vectors also in higher dimensions, in such a way that nothing changes in R 2 and R 3. The Cauchy- Schwarz inequality is the key here. 28 Definition 1.14 (Angle). Let v, w ∈ Rm be two nonzero vectors. The angle between them is the unique α between 0 and π (180 degrees) such that cos(α) = v · w ∥v∥∥w∥ ∈ [−1, 1]. (Here, the interval [−1, 1] = {x ∈ R : −1 ≤ x ≤ 1} comes from the Cauchy Schwarz inequality, Lemma 1.12). In other words, α = arccos ( v · w ∥v∥∥w∥ ) . Let’s check that this coincides with the usual concept of angles in R 2. As the angle does not depend on how long the arrows are, and whether we simultaneously rotate them, we can look at the case where v = e1 and w is another unit vector, with an angle of α between the arrows; see Figure 1.18. v = e1wαcos(α)sin(α)1∥v − w∥1 − cos(α)v = e1wα− cos(α)sin(α)1∥v − w∥1 − cos(α)00 Figure 1.18: The angle α between two unit vectors in R 2. Left: α acute; right: α obtuse From highschool, we know that the legs of the gray triangle (whose hypotenuse is ∥w∥ = 1) are sin(α) and cos(α) (if α is an acute angle) or − cos(α) (if α is an obtuse angle). In both cases, the red triangle with hypotenuse ∥v − w∥ therefore has legs sin(α) and 1 − cos(α). By the Pythagorean theorem, ∥v − w∥2 = sin 2(α) + (1 − cos(α)) 2. Using sin 2(α) + cos 2(α) = 1, the right-hand side is 2 − 2 cos(α). By definition of the Eu- clidean norm, the left-hand side is (v − w) · (v − w), and we have already argued in the proof of Lemma 1.12 (Cauchy-Schwarz inequality) that this equals 2 − 2v · w. Hence, we have shown 2 − 2v · w = 2 − 2 cos(α) which simplifies to cos(α) = v · w. This indeed agrees with what Definition 1.14 says for unit vectors. 29 Definition 1.15 (Perpendicular vectors). Two vectors v, w ∈ R m are called perpendicular (a different term used is orthogonal) if v · w = 0; in other words, if the cosine of the angle between them is 0 and the angle itself is 90 degrees. Figure 1.19 gives an example. xy [ 4 2 ][ −1 2 ] 900 [4 2 ] · [ −1 2 ] = −4 · 1 + 2 · 2 = 0 Figure 1.19: Perpendicular vectors: the scalar product equals 0. 1.2.5 Triangle inequality Lemma 1.16. Let v, w ∈ R m. Then ∥v + w∥ ≤ ∥v∥ + ∥w∥. In R 2, Figure 1.20 shows that this is quite obvious from the parallelogram of vector addition (Figure 1.4). vwv + w∥w∥∥v∥∥v + w∥0∥w∥∥v∥ Figure 1.20: The triangle inequality: going from 0 directly to v + w is shorter than making a detour via v or w. In higher dimensions, the following proof shows that the triangle inequality is nothing but Cauchy-Schwarz in a different disguise. Proof. Since both sides of the inequality are nonnegative, we can instead prove the squared triangle inequality ∥v + w∥2 ≤ (∥v∥ + ∥w∥)2 30 and then take square roots on both sides to obtain the triangle inequality. To prove the squared version, we compute ∥v + w∥2 = (v + w) · (v + w) (Definition 1.11) of the Euclidean norm) = v · v + w · w + 2v · w (Observation 1.10 (iii) on scalar products) = ∥v∥2 + ∥w∥ 2 + 2v · w (Definition 1.11 of the Euclidean norm) ≤ ∥v∥2 + ∥w∥ 2 + 2|v · w| ≤ ∥v∥2 + ∥w∥ 2 + 2∥v∥∥w∥ (Cauchy Schwarz inequality, Lemma 1.12) = (∥v∥ + ∥w∥)2. Exercise 1.17. Turn this proof around and derive the Cauchy-Schwarz inequality from the (squared) triangle inequality! As a consequence, some sources say that the Cauchy-Schwarz inequality is equivalent to the triangle inequality; this is another abuse of notation, since any two statements that are both true are logically equivalent, even if they have otherwise nothing to do with each other. What is meant here is that the triangle inequality can easily be proved using the Cauchy-Schwarz inequality (as we did in the proof above), and vice versa, as we ask you to do in Exercise 1.17. 1.3 Linear independence Linear independence of vectors is probably the single most important concept of lin- ear algebra. A sequence of vectors is called linearly independent if none of the vectors is a linear combination of the others, and linearly dependent otherwise. We provide a number of alternative definitions of linear (in)dependence that illuminate the con- cept from different angles. We also introduce the span of a sequence of vectors, the set of all their linear combinations. Adding some linear combination as a new vector to the sequence does not change the span. 1.3.1 Definition and examples Definition 1.18 (Linear (in)dependence). Vectors v1, v2, . . . , vn are linearly dependent if at least one of them is a linear combination of the others, i.e. there exists an index k ∈ [n] and scalars λj such that vk = n∑ j=1 j̸=k λjvj. Otherwise, v1, v2, . . . , vn are linearly independent. 31 Here, the additional “j ̸= k” below the sum adds a condition to the j’s considered in the sum: take only the ones in the given range that satisfy the condition. Hence, the sum is over all j except k, so the equation indeed says that vk is a linear combination of the other vectors. Again, let’s do some examples. The two vectors [2 3 ] , [4 6 ] are linearly dependent, because [ 2 3 ] = 1 2 [4 6 ] or [ 4 6 ] = 2 [ 2 3 ] . We also call these two vectors collinear because they are are on the same line; see Fig- ure 1.21 (left). In contrast, [2 3 ] , [ 3 −1 ] are linearly independent, as none of them is a linear combination (scalar multiple) of the other one; see Figure 1.21 (right). xy [ 2 3 ][ 4 6 ] 0xy [ 2 3 ][ 3 −1 ] 0 Figure 1.21: Two collinear vectors (left): their linear combinations form a line; two linearly independent vectors (right) Let us now consider three vectors in R 2. These are always linearly dependent: If two of them are collinear, one of them is a linear combination of the other one and therefore of both other ones (pick scalar 0 for the second other one). Otherwise, each of the vectors is a linear combination of the other two by Challenge 1.6. What if we have just one vector v ∈ R m? Can v even be a linear combination of the other vectors when there are no other vectors? Yes, if v = 0, because 0 is a linear combination of the empty sequence of vectors (Section 1.1.5). But if v ̸= 0, v is linearly independent. Generally, when 0 is one of the vectors, they are automatically linearly dependent, because 0 is always a linear combination of the other ones, even if there are no other ones. 32 Similarly, when some vector appears twice, the vectors are linearly dependent, because one of the copies is already a linear combination of the other copy. Finally, what about the empty sequence of vectors? This in turn is linearly indepen- dent by Definition 1.18: because [n] = ∅ in this case (∅ is the symbol for the empty set), there is no index k ∈ [n], whatever we may require of it. Table 1.3 summarizes these examples. linearly independent linearly dependent[2 3 ] , [ 3 −1 ] [2 3 ] , [4 6 ] v1, v2, v3 ∈ R 2 v ̸= 0 v = 0 . . . , 0, . . . . . . , v, . . . , v, . . . empty sequence Table 1.3: Linear (in)depencence of some sequences of vectors 1.3.2 Alternative definitions There are two more important alternative definitions of linear dependence. The following lemma provides them. In some sources, (ii) is the “standard” definition of linear depen- dence. Lemma 1.19 (Alternative definitions of linear dependence). Let v1, v2 . . . , vn ∈ R m. The following statements are equivalent (meaning that they are either all true, or all false). (i) At least one of the vectors is a linear combination of the other ones. (This means, the vectors are linearly dependent according to Definition 1.18.) (ii) There are scalars λ1, λ2, . . . , λn besides 0, 0, . . . , 0 such that ∑n j=1 λjvj = 0. We also say that 0 is a nontrivial linear combination of the vectors. (iii) At least one of the vectors is a linear combination of the previous ones. For the proof, we apply the basic principles of logic. We first argue that (i) implies (ii), meaning that if (i) is true, then also (ii) is true. Logically, this is written as (i)⇒(ii). Next we prove (ii)⇒(iii) and (iii)⇒(i). 33 Having done this, we know that (i), (ii) and (ii) are equivalent: either all true or all false, written as (i)⇔(ii)⇔(iii). Indeed, because of the three (circular) implications, it can- not be that one of them is true and another one is false. In math prose, an equivalence such as (i)⇔(ii) is also written as “(i) if and only if (ii)”. This summarizes the two implications : “(i) if (ii)” writes out (ii)⇒(i), while “(i) only if (ii)” excludes the possibilty that (i) is true and (ii) is false. In other words, it writes out the implication (i)⇒(ii). Proof. (i)⇒(ii): If at least one of the vectors, vk say, is a linear combination of the other vectors, then vk = n∑ j=1 j̸=k λjvj. Defining λk = −1, we get 0 = n∑ j=1 λjvj. Hence, 0 is a nontrivial linear combination of the vectors (it’s nontrivial because λk ̸= 0). (ii)⇒(iii): If 0 is a nontrivial linear combination of the vectors, then we can write 0 in the form 0 = n∑ j=1 λjvj, where not all λj are zero. Let k be the largest index such that λk ̸= 0. Then we actually have 0 = k∑ j=1 λjvj, and this can be solved for vk, resulting in vk = k−1∑ j=1 (− λj λk ) vj. Hence, at least one vector, namely vk, is a linear combination of the previous ones. (iii)⇒(i): If at least one vector is a linear combination of the previous ones, the same vector is also a linear combination of the other ones (use scalar 0 for vectors after it). Here are the corresponding alternative definitions of linear independence. These are simply obtained by taking the opposites of (i)–(iii) in Lemma 1.19: If some statements are either all true or all false, the same holds for their opposites. We formulate the resulting definitions as a corollary which is a result that directly fol- lows from (is implied by) a previous one, without the need for a proof (or only a very simple proof such as “take the opposites of all statements!”). 34 Corollary 1.20 (Alternative definitions of linear independence). Let v1, v2 . . . , vn ∈ Rm. The following statements are equivalent (meaning that they are either all true, or all false). (i) None of the vectors is a linear combination of the other ones. (This means, the vectors are linearly independent according to Definition 1.18.) (ii) There are no scalars λ1, λ2, . . . , λn besides 0, 0, . . . , 0 such that ∑n j=1 λjvj = 0. We also say that 0 can only be written as a trivial linear combination of the vectors. (iii) None of the vectors is a linear combination of the previous ones. This has another important consequence: a linear combination of linearly independent vectors can be written as a linear combination in only one way. Lemma 1.21. Let v1, v2 . . . , vn ∈ R m be linearly independent, and let v = ∑n j=1 λjvj =∑n j=1 µjvj be two ways of writing v as a linear combination. Then λj = µj for all j ∈ [n]. Proof. Subtracting the two linear combinations, we get 0 = n∑ j=1 (λj − µj)vj. Since 0 can only be written as a trivial linear combination, we get λj − µj = 0 for all j. 1.3.3 Span of vectors The set of all linear combinations of some vectors is important enough to deserve a name. Definition 1.22 (Span). Let v1, v2, . . . , vn ∈ R m. Their span is the set of all linear combinations. In formulas, Span(v1, v2, . . . , vn) := { n∑ j=1 λjvj : λj ∈ R for all j ∈ [n] } . As an example, let us consider the span of three vectors v1, v2, v3 in R3. There are three cases; if you have looked into Challenge 1.6, this will not surprise you. Formally, the different cases result from analyzing systems of three linear equations in three variables (which we will not do now). The span can be a line through the origin (vectors are collinear), a plane through the origin (vectors are coplanar), or the whole space (vectors are linearly independent). Fig- ure 1.22 presents examples for all three cases. The attentive reader might have noticed that there is a fourth (but pretty boring) case: if v1 = v2 = v3 = 0, then 0 is the only linear combination, so the span is a point at the origin. Here are some more observations to illustrate the concept. 35 xyz   −2 2 2     −1 1 1     −3 3 3   xyz   −2 2 0     −1 1 3     −3 3 3   xyz   −2 2 0     −1 1 3     2 3 −1   Figure 1.22: The span of three vectors in R3: a line (left), a plane (middle), the whole space (right). We always have 0 ∈ Span(v1, v2, . . . , vn) (obtained by setting all λj to 0). This even holds if n = 0 and there are no vectors. As we have argued in Section 1.1.5, 0 is a linear combination of the empty sequence of vectors (and the only one), so Span() = {0}. In “span language,” Fact 1.5 can be rewritten as follows: Span ([ 2 3 ] , [ 3 −1 ]) = R 2. The span of two nonzero vectors that are scalar multiples of each other is always a line. For example, Span ([ 2 3 ] , [4 6 ]) = { λ [2 3 ] : λ ∈ R } . Figure 1.23 illustrates these two cases. Next we prove a useful statement that may seem obvious from the examples in R 3 but needs a proof: the span of vectors does not change when we add a linear combination of them as a new vector. Lemma 1.23. Let v1, v2 . . . , vn ∈ Rm, and let v ∈ Rm be a linear combination of v1, v2 . . . , vn. Then Span(v1, v2, . . . , vn) ︸ ︷︷ ︸ S = Span(v1, v2, . . . , vn, v) ︸ ︷︷ ︸ T . To prove that two sets S and T are equal, we can argue that each element of S is contained in T (then S is a subset of T , in formulas S ⊆ T ) and—vice versa—that each element of T is contained in S (then T ⊆ S). Having done this, there can’t be an element which is in one of the sets and not in the other one (same logic as with the implications in Lemma 1.19), so the two sets must be equal. 36 xy [ 2 3 ][ 4 6 ] 0xy [ 2 3 ][ 3 −1 ] 0 Figure 1.23: The span of two vectors in R 2: a line (left), or the whole space (right). Proof. S ⊆ T : Each element w ∈ S is a linear combination of v1, v2, . . . , vn and therefore also a linear combination of v1, v2, . . . , vn, v (add the scalar multiple 0v). So w ∈ T . T ⊆ S: each element w ∈ T is a linear combination of v1, v2, . . . , vn, v, w = n∑ j=1 λjvj + λv. But since v is a linear combination of v1, v2 . . . , vn, we also have v = n∑ j=1 µjvj. Plugging the second equation into the first one, we get w = n∑ j=1 λjvj + λv = n∑ j=1 λjvj + λ ( n∑ j=1 µjvj ) = n∑ j=1 (λj + λµj)vj. This means that w is a linear combination of v1, v2, . . . , vn and hence in S. 37 Chapter 2 Matrices 2.1 Matrices and linear combinations A matrix is a rectangular array of numbers. Upfront, this is just a notation for a sequence of column vectors (the columns of the matrix), or a sequence of row vectors (the rows of the matrix). But matrices turn out to be very useful in representing, arguing about or computing with sequences of vectors and their linear combinations. Central matrix concepts that we introduce here are matrix-vector multiplication, the column space, the row space, the transpose, and the rank. We often work with sequences of vectors. A matrix can be considered as a more com- pact notation for such a sequence. For example,   1 2 3 4 5 6   is a 3 × 2 matrix (3 rows, 2 columns) that represents the sequence     1 3 5   ,   2 4 6     of 2 vectors in R 3. Using the matrix, we save brackets and commas, but more importantly, it naturally represents a second sequence of 3 (row) vectors in R 2: ([ 1 2] , [3 4] , [5 6]). Definition 2.1 (Matrix). An m × n matrix is a rectangular array of real numbers with m rows and n columns. We use upper-case letters (A, B, . . .) to denote matrices, and write their entries with the corresponding lower case letters and two indices, as in A =      a11 a12 · · · a1n a21 a22 · · · a2n ... ... . . . ... am1 am2 · · · amn      . 38 Hence, aij is the entry in row i and column j of matrix A. The “dot-free” notation (see also Section 1.1.5) is A = [aij] m n i=1,j=1. The set of m × n matrices is denoted by R m×n. If we want to talk about the columns of A as column vectors or the rows of A as row vectors, we use column notation or row notation, A =   | | | v1 v2 · · · vn | | |   ︸ ︷︷ ︸ column notation , A =     |u1||u2|...|um|      ︸ ︷︷ ︸ row notation . While a vector needs one dot symbol in “dot notation”, a matrix needs seven. This is significant enough to use the other notations (dot-free, column, row) more frequently for matrices. A column vector v ∈ R m is an m × 1 matrix, and a row vector u ∈ R n is a 1 × n matrix. At this point, we have to admit an abuse of notation that we have started much earlier. While R m officially contains sequences (x1, x2 . . . , xm) of real numbers, we have silently also treated column vectors with m entries as elements of R m. In reality, they are matrices: elements of R m×1. But since an m × 1 matrix just contains a sequence of m real numbers, there is no harm in treating it as an element of R m as well. The same is true for row vectors: there is not really a difference between R 1×n and R n. While we are at it, there is also no problem in treating a 1 × 1 matrix as a real number. Just like vectors, two matrices of the same shape can be added and multiplied with a scalar, as in the following examples: [1 2 3 4 ] + [5 6 7 8 ] = [ 6 8 10 12 ] , 2 [ 1 2 3 4 ] = [2 4 6 8 ] . Definition 2.2 (Matrix addition, scalar multiplication, zero matrix, square matrix). Let A = [aij] m n i=1,j=1 and B = [bij]m n i=1,j=1 be m × n matrices, λ ∈ R a scalar. (i) The matrix A + B := [aij + bij] m n i=1,j=1 is the sum of A and B. (ii) The matrix λA := [λaij]m n i=1,j=1 is a scalar multiple of A. (iii) The matrix [0] m n i=1,j=1 is the m × n zero matrix, written as 0. (iv) If m = n (number of rows equals number of columns), then A is a square matrix. The non-square matrices come in two kinds of shapes, with somewhat unofficial but intuitive names. We have the tall and skinny matrices with more rows than columns, and the short and wide matrices with more columns than rows; see Figure 2.1. 39 mnnmmntall and skinnysquareshort and widem > nm = nm < n Figure 2.1: Matrix shapes Square matrices are particularly important, and they often have additional properties. Before we provide the general definitions, we give some 3 × 3 examples.   1 0 0 0 1 0 0 0 1    2 0 0 0 4 0 0 0 5     2 1 0 0 4 7 0 0 5     2 0 0 1 4 0 0 7 5     2 1 0 1 4 7 0 7 5   identity diagonal upper triangular lower triangular symmetric matrix matrix matrix matrix matrix Definition 2.3 (Square matrix classes). Let A = [aij]m m i=1,j=1 be an m × m square matrix. If j < i, j = i, j > i, then aij is said to be below, on, above the diagonal. j < ij > ij = i (i) If aii = 1 for all i and aij = 0 for all j ̸= i, then A is the identity matrix, denoted (in abuse of notation) by I in every dimension. A different way of defining I is as I := [δij] m m i=1,j=1. Here, δij is the Kronecker delta, defined as 1 if i = j and 0 otherwise. (ii) If aij = 0 for all j ̸= i (entries not on the diagonal are 0), then A is a diagonal matrix. (iii) If aij = 0 for all j < i (entries below the diagonal are 0), then A is an upper triangular matrix. (iv) If aij = 0 for all j > i (entries above the diagonal are 0), then A is a lower triangular matrix. (v) If aij = aji for all i, j, then A is a symmetric matrix. 40 Note that (ii)-(iv) each require that some entries are zero, but not that the other entries are nonzero. For example, the zero matrix is at the same time diagonal, upper triangular, and lower triangular, even though we do not see a “diagonal”, or a “triangle” in it. Also whenever we say things like “all i”, we mean all applicable i, in this case i ∈ [m]. 2.1.1 Matrix-vector multiplication Here is the efficient “matrix way” of writing down a linear combination as a matrix-vector multiplication: 7   1 3 5   + 8   2 4 6   ︸ ︷︷ ︸ linear combination =   1 2 3 4 5 6   [ 7 8 ] . ︸ ︷︷ ︸ matrix-vector product Definition 2.4 (Matrix-vector multiplication). Let A =   | | | v1 v2 · · · vn | | |   ∈ R m×n, x =      x1 x2 ... xn      ∈ R n. The vector Ax := n∑ j=1 xjvj ∈ R m is the product of A and x. It is important to also understand the product in the other matrix notations. Observation 2.5. Let A =      a11 a12 · · · a1n a21 a22 · · · a2n ... ... . . . ... am1 am2 · · · amn      = [aij]m n i=1,j=1 ∈ Rm×n, x =      x1 x2 ... xn      = [xj]n j=1 ∈ R n. Then Ax =      a11x1 + a12x2 + · · · + a1nxn a21x1 + a22x2 + · · · + a2nxn ... am1x1 + am2x2 + · · · + amnxn      = [ n∑ j=1 aijxj ]m i=1 ∈ R m. 41 This is in many sources the official definition of matrix-vector multiplication. It does not explicitly refer to the columns or rows of A but just looks at A as a two-dimensional array of numbers. This is a very useful definition if we actually want to compute the product, but it hides our motivation for defining the product as a short notation for a linear combination. But we easily see that both definitions say the same, by annotating the columns in Observation 2.5: A =      a11 a12 · · · a1n a21 a22 · · · a2n ... ... . . . ... am1 am2 · · · amn      v1 v2 · · · vn , Ax =      a11x1 + a12x2 + · · · + a1nxn a21x1 + a22x2 + · · · + a2nxn ... am1x1 + am2x2 + · · · + amnxn      x1v1 + x2v2 + · · · + xnvn . An immediate consequence is the following. Corollary 2.6. Let I be the m × m identity matrix (Definition 2.3). Then Ix = x for all x ∈ R m. Finally, we ce can also use row notation of A to define Ax in terms of scalar products. Observation 2.7. Let A =     |u1||u2|...|um|      ∈ Rm×n, x ∈ R n. Then Ax =      u1 · x u2 · x ... um · x      ︸ ︷︷ ︸ scalar products . This is seen to be correct by annotating the rows in Observation 2.5: A =      a11 a12 · · · a1n a21 a22 · · · a2n ... ... . . . ... am1 am2 · · · amn      u1 u2 ... um , Ax =      a11x1 + a12x2 + · · · + a1nxn a21x1 + a22x2 + · · · + a2nxn ... am1x1 + am2x2 + · · · + amnxn      u1 · x u2 · x ... um · x . Figure 2.2 provides another pictorial view, illustrating that if A is an m × n matrix, we can multiply it with an n-dimensional vector x and obtain an m-dimensional vector Ax as the result. 2.1.2 Column space and rank Definition 2.8 (Column space). Let A be an m × n matrix. The column space C(A) of A is the span (set of all linear combinations) of the columns, C(A) := {Ax : x ∈ R n} ⊆ R m. 42 AxAxnm=mn Figure 2.2: Matrix-vector multiplication, pictorially Here we use the definition of the matrix-vector product Ax as the linear combination of the columns with scalars from x; see Definition 2.4. When we consider all possible vectors x ∈ R n, we obtain all possible linear combinations (the span) of the columns. Note that we always have 0 ∈ C(A). Using the column space, the statement of Fact 1.5 can be written as C ([ 2 3 3 −1 ]) = Span ([ 2 3 ] , [ 3 −1 ]) = R 2. See Figure 1.23 (right) for an illustration. In general, when A is a 2 × 2 matrix with linearly independent columns, C(A) = R 2 holds; see Challenge 1.6. A crucial parameter of a matrix is its rank. The rank is defined as the number of independent columns. A column is called independent if it is not a linear combination of previous columns. Definition 2.9 ((In)dependent column and rank of a matrix). Let A =   | | | v1 v2 · · · vn | | |   be an m × n matrix with columns v1, v2, . . . , vn. Column vj is called independent if vj is not a linear combination of v1, v2, . . . , vj−1. Otherwise, vj is called dependent. The rank of A, written as rank(A), is the number of independent columns of A. This means, rank(A) is a number between 0 and n. We have rank(A) = n exactly if no column is a linear combination of the previous ones. According to Corollary 1.20, this is the same as saying that the columns are linearly independent. The case rank(A) = 0 happens exactly if A = 0, the zero matrix. Note that for A = 0, already the first column is a linear combination of the previous ones, because 0 is a linear combination of the empty sequence of vectors; see Section 1.1.5. Two more examples (see Figure 2.3) are rank ([ 2 4 3 1 ]) = 2, rank ([ 2 4 3 6 ]) = 1. 43 rank ([ 2 4 3 1 ]) = 2 xy [ 2 3 ][ 4 1 ] rank ([ 2 4 3 6 ]) = 1 xy [ 2 3 ][ 4 6 ] Figure 2.3: Ranks of two 2 × 2 matrices: 2 when both columns are independent (left), or 1 when only the first column is independent (right) If we reorder the columns of a matrix, we may get other independent columns. For example, the two matrices [2 4 3 6 ] and [4 2 6 3 ] have the same columns, but in different order. Each matrix has one independent column, namely its first one, and these are different. Still, both matrices have the same rank 1. This is not a coincidence. We will take this up again in the example immediately preceding Section 4.2.2; a consequence of the results in that section is that the rank of a matrix does not change when we reorder the columns. The independent columns of a matrix A are also of interest, because they already span the column space of A. Lemma 2.10. Let A be an m × n matrix with r independent columns, and let C be the m × r submatrix containing the independent columns. Then C(A) = C(C). By a submatrix of a matrix A, we mean a matrix obtained from A by deleting some rows and/or columns. Here, C is obtained from A by deleting the dependent columns. Proof. Let u1, u2, . . . , ur be the independent columns of A, and w1, w2, . . . , wn−r the de- pendent columns (in the same order as they appear in A). We will prove that Span(u1, u2, . . . , ur, w1, w2, . . . , wn−r) ︸ ︷︷ ︸ C(A) = Span(u1, u2, . . . , ur) ︸ ︷︷ ︸ C(C) . We first observe that wj is a linear combination of u1, u2, . . . , ur, w1, w2, . . . wj−1, for all j. Indeed, by Definition 2.9, a dependent column is a linear combination of the previous columns in A, and the sequence u1, u2, . . . , ur, w1, w2, . . . wj−1 contains all those (and pos- sibly a few extra independent ones). Hence, if we start from the sequence u1, u2, . . . , ur and then add w1, w2, . . . , wn−r one by one, Lemma 1.23 guarantees that the span of the sequence never changes. 44 R ([ 2 4 3 1 ]) = R 2 xy [ 2 4 ][ 3 1 ] R ([ 2 4 3 6 ]) = {c [ 2 4] : c ∈ R } xy [ 2 4 ][ 3 6 ] Figure 2.4: Row spaces of two 2 × 2 matrices: the whole plane (top) when the rows are linearly independent, or a line (botttom) when the rows are linearly dependent 2.1.3 Row space and transpose Recall that a matrix represents a second sequence of vectors, namely its rows. So we can also define the row space R(A) of a matrix: the span of its rows. Figure 2.4 illustrates the row spaces of the matrices from Figure 2.3. For both matrices, the number of independent columns equals the number of inde- pendent rows. Is this a coincidence? No! It turns out that this is true for every matrix. This is quite surprising, and we will prove it in Section 4.3.2. What we will do here is prepare the ground. We are still missing formal definitions of row space, independent rows, and potentially of a row rank based on counting inde- pendent rows. Conceptually, nothing new happens here, and we could easily provide “row versions” of Definitions 2.8 and 2.9 by essentially copying the “column versions”. But mathematicians do not like this kind of copy & paste. The more elegant way is to consider the rows as the columns of another matrix, and thus reduce everything to the column versions. This needs the concept of matrix transposition. The transpose A ⊤ of a matrix A is an- other matrix, obtained by “mirroring” A along the diagonal “ ⧹”, the line going through the diagonal entries a11, a22, . . . of A. Figure 2.5 shows a physical such mirror image and its mathematical abstraction where we don’t screw up the number and bracket symbols. The effect of this mirroring is that the rows of A become the columns of A⊤. Definition 2.11 (Transpose). Let A = [aij] m n i=1,j=1 be an m × n matrix. The transpose of A is 45 ↔    1 2 3 4 5 6       1 2 3 4 5 6    A = [1 2 3 4 5 6 ] ↔ A⊤ =  1 4 2 5 3 6   Figure 2.5: Mirroring a matrix along the diagonal, physically and mathematically the n × m matrix A ⊤ := [aji] n m i=1,j=1. This means, the entry of A ⊤ in row i and column j is aji, the entry of A in row j and column i. Transposing a matrix thus interchanges columns with rows. In particular, we can use transposition to turn column vectors into row vectors and vice versa, for example  1 3 5   ⊤ = [ 1 3 5 ] . In column and row notation, we therefore have A =   | | | v1 v2 · · · vn | | |   ⇔ A⊤ =     |v⊤ 1||v⊤ 2|...|v⊤ n|      , A =     |u1||u2|...|um|      ⇔ A ⊤ =   | | | u⊤ 1 u⊤ 2 · · · u⊤ m | | |   . It is easy to see that mirroring twice gives back the original. Also, the symmetric matrices are exactly the ones that are mirror images of themselves. Observation 2.12. Let A be an m × n matrix. Then (A ⊤) ⊤ = A. Moreover, a square matrix A is symmetric (Definition 2.3) if and only if A = A ⊤. Now we can define the row space of A simply as the column space of A⊤. Definition 2.13 (Row space). Let A be an m × n matrix. The row space R(A) of A is the column space of the transpose, R(A) := C(A ⊤). Similarly, we could define an independent row of A as an independent column of A⊤, and the row rank of A as the (column) rank of A⊤, but there is no need to do this anymore. Using the transpose, we can formulate everything in terms of columns. 46 2.1.4 Rank-1 matrices As a warmup, we will prove rank(A) = rank(A ⊤) for the case where A has rank 1. This will be a consequence of the following lemma that tells us how rank-1 matrices look like. Before that, let’s look at an example of a rank-1-matrix: A = [1 2 3 2 4 6 ] . This matrix has rank 1, since there is only one independent column (the first one), and the second and third are scalar multiples of it. In this example, there is also one independent row (the first one), and the second one is a scalar multiple of it. Lemma 2.14. Let A be an m × n matrix. The following two statements are equivalent. (i) rank(A) = 1. (ii) There are nonzero vectors v ∈ R m, w ∈ R n such that A = [viwj]m n i=1,j=1. In dot notation, the rank-1 matrices are therefore exactly the nonzero matrices of the form      v1w1 v1w2 · · · v1wn v2w1 v2w2 · · · v2wn ... ... . . . ... vmw1 vmw2 · · · vmwm      . Proof. (i)⇒(ii): If rank(A) = 1, there is exactly one independent column v ̸= 0 (Defi- nition 2.9). This means, all columns before v are 0, and all columns after v are scalar multiples of v. Thus, all columns are scalar multiples of v where at least one scalar (the one for column v itself) is nonzero. Let w ̸= 0 be the vector of scalars, meaning that the j-th column of A is wjv. Hence, aij (the entry in row i and column j of A) equals wjvi = viwj. In other words, A = [viwj]m n i=1,j=1. (ii)⇒(i): If A = [viwj] m n i=1,j=1 for nonzero vectors v, w, then the j-th column of A is wjv. The first column for which wj ̸= 0 is independent, since v ̸= 0 and all columns before it are 0; all columns after it are scalar multiples of this independent column (the scalar for column k is wk/wj). Hence, there is exactly one independent column, so rank(A) = 1. Corollary 2.15. Let A be an m × n matrix with rank(A) = 1. Then also rank(A ⊤) = 1. Proof. By Lemma 2.14, there are v, w ̸= 0 such that A = [viwj ︸︷︷︸ aij ] m n i=1,j=1. 47 By Definition 2.11 of the transpose, A ⊤ = [vjwi ︸︷︷︸ aji ]n m i=1,j=1 = [wivj] n m i=1,j=1, with w, v ̸= 0. Using Lemma 2.14 “in the other direction,” we have rank(A ⊤) = 1. 2.2 Matrix multiplication Matrix-vector multiplication is only a special case of a more powerful operation, namely matrix-matrix multiplication that we get to know in this section. This has many important applications, including matrix decompositions where we write a matrix as a product of other matrices with the goal of revealing some structural properties. We present the CR decomposition in this section; others will appear in subsequent chap- ters. The matrix-vector product Ax is a short notation for the linear combination of the columns of A with scalars from x (Definition 2.4). Often, we consider several linear combinations of the same vectors; as an example, see Table 1.1 that computes three different linear combinations of two fixed vectors. We would also like to express this in matrix notation. So let’s suppose we have some fixed vectors (the columns of A) and several vectors x1, x2, . . . , xb of scalars with the resulting linear combinations Ax1, Ax2, . . . , Axb. To com- pactly represent them, we introduce a matrix B with columns x1, x2, . . . , xb, and define the matrix-matrix product AB as the matrix with columns Ax1, Ax2, . . . , Axb. Definition 2.16 (Matrix multiplication). Let A be an a × n matrix and B =   | | | x1 x2 · · · xb | | |   an n × b matrix. The a × b matrix AB :=   | | | Ax1 Ax2 · · · Axb | | |   is the product of A and B. This allows us to summarize the computations of Table 1.1 in a single matrix multipli- cation; see Table 2.1. Some comments are in order here: for AB to be defined, the number of columns of A needs to match the number of rows of B; this is the number n in the definition. This 48 v = [2 3 ] , w = [ 3 −1 ] : λ µ λv + µw −3 2 [ 0 −11 ] 1 −1 [−1 4 ] 3 0 [ 6 9 ] [2 3 3 −1 ] ︸ ︷︷ ︸ A [−3 1 3 2 −1 0 ] ︸ ︷︷ ︸ B = [ 0 −1 6 −11 4 9 ] ︸ ︷︷ ︸ AB Table 2.1: Three linear combinations of two vectors v, w (left), summarized in one matrix multiplication (right) comes from the fact that the matrix-vector products Axj are only defined if xj ∈ R n; see Definition 2.4. This means, B needs to have n rows. By the same definition, Axj ∈ R a for all j, and this also explains why AB is an a × b matrix. One can define the product AB directly, without referring to matrix-vector multipli- cation: in order to compute the entry of AB in row i and column j, we take the scalar product of the i-th row of A and the j-th column of B. Observation 2.17. Let A =     |u1||u2|...|ua|      ∈ R a×n, B =   | | | x1 x2 · · · xb | | |   ∈ R n×b. Then AB =      u1 · x1 u1 · x2 · · · u1 · xb u2 · x1 u2 · x2 · · · u2 · xb ... ... . . . ... ua · x1 ua · x2 · · · ua · xb      ︸ ︷︷ ︸ ab scalar products = [ui · xj]a b i=1,j=1 ∈ R a×b. This is because the j-th column of AB is Axj = [ui · xj]a i=1; see Observation 2.7. Finally, here is the dot-free definition of AB. Observation 2.18. Let A = [aij] a n i=1,j=1, B = [bij]n b i=1,j=1. Then AB = [ n∑ ℓ=1 aiℓbℓj ]a b i=1,j=1 . This is just a different way of writing Observation 2.17, since ∑n ℓ=1 aiℓbℓj equals ui · xj (“i-th row of A times j-th column of B”). 49 Here are two examples. Let A = [1 2 3 4 ] , B = [0 1 1 0 ] . Then AB = [ 1 2 3 4 ] [ 0 1 1 0 ] = [1 · 0 + 2 · 1 1 · 1 + 2 · 0 3 · 0 + 4 · 1 3 · 1 + 4 · 0 ] = [2 1 4 3 ] (”column exchange in A”) BA = [0 1 1 0 ] [ 1 2 3 4 ] = [0 · 1 + 1 · 3 0 · 2 + 1 · 4 1 · 1 + 0 · 3 1 · 2 + 0 · 4 ] = [3 4 1 2 ] (”row exchange in A”) We see that matrix multiplication is not commutative, since we may have BA ̸= AB. If A and B are not square matrices, it may happen that AB is defined and BA is not. Or that both products are defined, but are of different shape. It’s not hard to understand the situation exactly. For AB to be defined, A must be a × n and B must be n × b, for some number n. For BA to be defined as well, we also need a = b. This means, A must be m × n and B must be n × m, for some number m. Then AB is m × m and BA is n × n, so both products are square matrices. If m = n, they have the same shape, otherwise, one is larger than the other; Figure 2.6 provides a pictorial view of matrix multiplication. AmnmnBmmAB=mnBmn=nnBAAAannBaAB=bb Figure 2.6: Matrix multiplication, pictorially An easy but important fact is how matrix multiplication interacts with transposition (Definition 2.11). Lemma 2.19. Let A be an a × n matrix and B an n × b matrix. Then (AB) ⊤ = B⊤A⊤. 50 Proof. Since B⊤ is a b × n matrix and A⊤ an n × a matrix, the product B⊤A ⊤ is a b × a matrix and therefore has the same shape as (AB) ⊤. Next, we compare the two matrices (AB) ⊤ = [cij]b a i=1,j=1 and B⊤A ⊤ = [dij]b a i=1,j=1 entry by entry. By Definition 2.11 of the transpose, cij is the entry in row j and column i of AB and therefore the scalar product of the j-th row of A and the i-th column of B; see Observation 2.17: cij = (j-th row of A) · (i-th column of B). By the same observation, dij = (i-th row of B⊤) · (j-th column of A ⊤). To conclude cij = dij, it remains to note that (j-th row of A) = (j-th column of A ⊤) and (i-th column of B) = (i-th row of B⊤). As seen before in Corollary 2.6 for matrix-vector multiplication, identity matrices also have no effect in matrix-matrix multiplications. We leave the proof of this as an (easy) exercise. Corollary 2.20. Let I be the m × m identity matrix. Then IA = A for all m × n matrices, and AI = A for all n × m matrices. 2.2.1 Everything is matrix multiplication You may already have realized that matrix-vector multiplication is a special case of matrix multiplication when we consider the vector as an m × 1 matrix: [1 2 3 4 ] ︸ ︷︷ ︸ 2×2 [1 1 ] ︸︷︷︸ 2×1 = [3 7 ] ︸︷︷︸ 2×1 . Similarly, we can now also talk about vector-matrix multiplication involving a row vector: [1 1] ︸ ︷︷ ︸ 1×2 [1 2 3 4 ] ︸ ︷︷ ︸ 2×2 = [4 6] ︸ ︷︷ ︸ 1×2 . The scalar product of two vectors is another special case when we use the convention that 1 × 1 matrices can be treated as numbers. Writing the first vector as a row vector and the second one as a column vector, the scalar product becomes a matrix-matrix product: [ 1 2] ︸ ︷︷ ︸ 1×2 [3 4 ] ︸︷︷︸ 2×1 = [ 11 ] ︸︷︷︸ 1×1 = 11. 51 This is the reason why the scalar product v · w is often written as v⊤w. Formally, this is a matrix multiplication where we treat the resulting 1 × 1 matrix as a number. There is another interesting variant here, the outer product that we get by multiplying a column vector with a row vector (they don’t have to be of the same dimensions). [ 3 4 ] ︸︷︷︸ 2×1 [ 1 2] ︸ ︷︷ ︸ 1×2 = [ 3 6 4 8 ] ︸ ︷︷ ︸ 2×2 . This provides another way of thinking about rank-1 matrices. Lemma 2.21. Let A be an m × n matrix. The following two statements are equivalent. (i) rank(A) = 1. (ii) There are nonzero vectors v ∈ R m, w ∈ R n such that A is their outer product, A = vw⊤. Proof. By definition of matrix multiplication, A = vw⊤ means that entry aij of A is “the i-th row of v times the j-th column of w⊤.” The i-th row of v consists of just one number vi, and similarly, the j-th column of w⊤ has one number wj. Hence, A = vw⊤ is equivalent to aij = viwj for all i and j, and this in turn is equivalent to the condition for rank 1 in Lemma 2.14. Definition 2.16 has introduced matrix multiplication via column notation and matrix- vector products. Using vector-matrix products, we can also write AB in row notation:     |u1B||u2B|...|umB|      ︸ ︷︷ ︸ AB, row notation =     |u1||u2|...|um|      ︸ ︷︷ ︸ A, row notation   | | | x1 x2 · · · xn | | |   ︸ ︷︷ ︸ B, column notation =   | | | Ax1 Ax2 · · · Axn | | |   ︸ ︷︷ ︸ AB, column notation 2.2.2 Distributivity and associativity While matrix multiplication is not commutative (we may have AB ̸= BA), two other important laws hold. Lemma 2.22. Let A, B, C be three matrices such that all sums and products in the following are defined. Then (i) A(B + C) = AB + AC and (B + C)D = BD + CD (distributivity); (ii) (AB)C = A(BC) (associativity). 52 Proof. Unfortunately, this requires some work but is at the same time boring. We are therefore lazy and omit the proof of distributivity (it’s also not that you couldn’t find this anywhere else). The proof of associativity is a bit more interesting, as it lets us exercise (and appreciate) the dot-free definition of matrix multiplication; see Observation 2.18. We define S = (AB)C and compute sij, the entry of S in row i and column j. Then we do the same with T = A(BC) and check that sij = tij for all i and j. In order for AB and BC to be defined, we need A ∈ R a,m, B ∈ R m,n, C ∈ Rn,c, for some integers a, m, n, c. Then AB ∈ Ra,n, BC ∈ R m,c and (AB)C, A(BC) ∈ Ra,c. So both A(BC) and A(BC) have the same shape. We know that sij is the i-th row of Q = AB times the j-th column of C, see Observa- tion 2.18: sij = n∑ ℓ=1 qiℓcℓj, where qiℓ = m∑ k=1 aikbkℓ (i-th row of A times ℓ-th column of B) by the same logic. Putting this together, we get sij = n∑ ℓ=1 ( m∑ k=1 aikbkℓ ) cℓj = n∑ ℓ=1 m∑ k=1 aikbkℓcℓj, using distributivity of the real numbers (“constant factors can be pulled into and out of sums”). For tij, the entry of A(BC) in row i and column j, we argue in the same way. With R = BC, we have tij = m∑ k=1 aikrkj, where rkj = n∑ ℓ=1 bkℓcℓj (k-th row of B times j-th column of C). Hence, tij = m∑ k=1 aik ( n∑ ℓ=1 bkℓcℓj ) = m∑ k=1 n∑ ℓ=1 aikbkℓcℓj. Now we see that sij and tij only differ in the order of summation, and as addition over the reals is commutative, we have sij = tij. We silently used an important technique here, exchange of summation order: m∑ k=1 n∑ ℓ=1 · · · = n∑ ℓ=1 m∑ k=1 · · · If you haven’t seen this before, it’s useful to look at this in more detail. Both double sums go through all pairs (k, ℓ) where k goes through the range from 1 to m and ℓ goes through the range from 1 to n. The only difference is the order. The left double sum goes through the pairs in the order (1, 1), (1, 2), . . . , (1, n), (2, 1), (2, 2), . . . , (2, n), . . . . 53 This is because the inner sum goes through all ℓ for k = 1, then for k = 2 and so on. In contrast, the right double sum goes through the pairs in the order (1, 1), (2, 1), . . . , (m, 1), (1, 2), (2, 2), . . . , (m, 2), . . . . As a computer science student, you can also think of this in terms of nested loops. Depending on whether you increase k or ℓ in the outer loop, you will print the pairs (k, ℓ) in different orders: / / ( 1 , 1 ) , ( 1 , 2 ) , . . . , ( 1 , n ) , ( 2 , 1 ) , . . . f o r ( i n t k = 1 ; k <= m; k++) { f o r ( i n t l = 1 ; l <= n ; l ++) { / / p r i n t ( k , l ) } } / / ( 1 , 1 ) , ( 2 , 1 ) , . . . , ( m, 1 ) , ( 1 , 2 ) , . . . f o r ( i n t l = 1 ; l <= n ; l ++) { f o r ( i n t k = 1 ; k <= m; k++) { / / p r i n t ( k , l ) } } Figure 2.7 illustrates this for m = 4, n = 3. kℓ1234123kℓ1234123 Figure 2.7: Different summation orders, same pairs: Outer sum / loop over k (left); outer sum / loop over ℓ (right) Knowing that matrix multiplication is associative allows us to write ABC for a prod- uct of three matrices. As it doesn’t matter whether we compute this as (AB)C or A(BC), we can as well omit the brackets. This also works for more matrices and is then called generalized associativity. For ex- ample, (AB)(CD) = A((BC)D) = · · · = ABCD, so we can again omit the brackets, since it doesn’t matter where we put them. We will not prove this here (but informally in Sec- tion 2.3.4) and only point out that it is not obvious. Associativity in Lemma 2.22 only works for three matrices, and one needs a separate proof for more matrices. There are several such proofs [War01], but generalized associativity does need a proof. 54 2.2.3 CR decomposition You know the prime factor decomposition of a natural number. For example, this writes the number 1001 as 1001 = 7 · 11 · 13. This decomposition tells you something about the “structure” of the number that you cannot easily tell from just staring at the number. In linear algebra, it’s mostly matrix decompositions that are of interest. This means, we want to write a matrix A as a product of other matrices, with the goal of learning more about A, and potentially speeding up computations involving A. In this section, we will see a first such decomposition. Theorem 2.23 (CR decomposition). Let A be an m × n matrix of rank r (Definition 2.9). Let C be the m × r submatrix of A containing the independent columns. Then there exists a unique r × n matrix R such that A = CR. Before we go to the proof, let us do an example. Consider the rank-1 matrix A = [2 4 6 3 6 9 ] with one independent column (the first one). Then the CR decomposition assumes the form of an outer product: A = [2 3 ] ︸︷︷︸ C,2×1 [1 2 3 ] ︸ ︷︷ ︸ R,1×3 . The columns of R contain the scalars that we need in order to write each column as a scalar multiple of the first one. In fact, we have already proved that each rank-1 matrix can be written as an outer product, see Lemma 2.21. Next we show the existence of the CR decomposition for every matrix A. Proof. We know that A and C have the same column space (Lemma 2.10), so every column of A can be written as a linear combination of the columns of C. Moreover, since the columns of C are linearly independent, the scalars in these linear combinations are unique (Lemma 1.21). If A has columns v1, v2, . . . , vn ∈ R m, we therefore have vj = Cxj for all j, where xj ∈ Rr is a unique vector of r scalars. This uses that matrix-vector multiplication is simply another notation for linear combinations; see Definition 2.4. But then we get A =   | | | v1 v2 · · · vn | | |   = C   | | | x1 x2 · · · xn | | |   ︸ ︷︷ ︸ R∈Rr×n = CR, using Definition 2.16 of matrix multiplication. 55 So we have a decomposition A = CR. The matrix C contains the independent columns of A and the matrix R contains the unique scalars that we need in order to write all columns as linear combinations of the independent columns. Below is an example. Each of the four columns vj, j = 1, 2, 3, 4, is either independent (and then vj = 1vj is the unique linear combination), or it is dependent and a unique linear combination of the previous independent columns. columns of A v1 v2 v3 v4==== A =   1 2 0 3 2 4 1 4 3 6 2 5     1 2 3     2 4 6     0 1 2    3 4 5  ====v1 1v1 2v1 3v1 v2 v3 1v3 −2v3 v4 independent? yes no yes no The resulting CR decomposition is   1 2 0 3 2 4 1 4 3 6 2 5   ︸ ︷︷ ︸ A, 3×4 =   1 0 2 1 3 2   ︸ ︷︷ ︸ C, 3×2 [1 2 0 3 0 0 1 −2 ] ︸ ︷︷ ︸ R, 2×4 . You can check that the decomposition is correct, but we have not explained how it was computed. In this example, only the last column of R requires some actual computations: we need to write a vector as a linear combination of two independent vectors in R 2. As in the proof of Fact 1.5, this boils down to solving a system of two linear equations in two variables. Easy enough, but if the matrix A is larger, we would need to solve larger systems in order to compute the CR decomposition. We don’t yet know how to do this. Also, it is not clear at this point what exactly we learn about A from the decomposition A = CR. We will come back to this in Section 3.5.5 where we will rediscover (and also efficiently compute) the CR decomposition in a different context (Gauss-Jordan elimination). Sec- tion 4.3 will interpret C and R in yet another context (C: basis of column space, R: basis of row space). Exercise 2.24. What are the matrices C and R in Theorem 2.23 if A is an m × m matrix of rank m (the columns are linearly independent)? What are the matrices C and R if A is the m × n zero matrix? (The second question also requires you to think about m × 0 and 0 × n matrices. While these are not extremely relevant in practice, it is good to understand how our definitions and proofs also work for them.) 56 2.3 Matrices and linear transformations When we multiply an m × n matrix A with a vector x in R n, we get a “transformed” vector Ax ∈ R m. Here, we look at the properties of and the theory behind this (linear) transformation. Linear transformations are for example used to draw 3-dimensional objects in 2-dimensional space, and they have many other applications, some of which we will encounter in subsequent chapters. We will also see that matrix multi- plication naturally appears when combining different linear transformations. 2.3.1 Matrices as functions An m × n matrix A can be thought of as “transforming” an input vector x ∈ R n into an output vector Ax ∈ R m. Formally, this transformation is a function from R n to R m. The “from” set is the domain of the function, and the “to” set is its range. Definition 2.25 (Matrix as function). Let A be an m × n matrix. TA : R n → R m is the function defined by TA( x︸︷︷︸ ∈Rn ) = Ax︸︷︷︸ ∈Rm . For example, if A = [ 0 1 1 0 ], then TA ([ x1 x2 ]) = [0 1 1 0 ] [ x1 x2 ] = [x2 x1 ] , the function that swaps the coordinates of the input vector in R 2. Observation 2.26. Let A be an m × n matrix, x, y ∈ R n and λ ∈ R. Then (i) TA(x + y) = TA(x) + TA(y) and (ii) TA(λx) = λTA(x). By combining (i) and (ii), we also get TA(λx + µy) = λTA(x) + µTA(y). Proof. After substituting the definition of TA, (i) and (ii) simply say that A(x+y) = Ax+Ay and A(λx) = λAx. Both equalities easily follow from the rules of vector addition (Defi- nition 1.1), scalar multiplication (Definition 1.3) and matrix-vector multiplication (Obser- vation 2.5). To understand what TA does for a given matrix A, it is useful to not only look at indi- vidual inputs, but at a whole set of inputs. For a set of input vectors X ⊆ Rn, we define TA(X) := {TA(x) : x ∈ X} as the set of transformed output vectors. In the following examples, we see how different TA’s transform the standard unit vectors e1, e2, and a set X ⊆ R2 (gray L-shaped polygon); see Figure 2.8 (middle). In all examples, the vector 57 TA(X)TA(e1)TA(e2) A = [1 0 0 3 4 ] ←− Xe1e2 A = [0 1 1 0 ] −→ TA(X)TA(e1)TA(e2) Figure 2.8: Right: Mirroring the input along the diagonal. Left: Stretching the input by a factor of 3 4 along the second coordinate. TA(e1) is the first column of A, while TA(e2) is the second column. The gray shape gets transformed accordingly. The polygon X has 6 corners and 6 line segments connecting the corners. It is an infinite set, but to compute TA(X), we only need to apply TA to the corners; the line segments will follow their two corners. To see this, consider a line segment s connecting two corners x and y. In Section 1.1.4, we have seen that s is the set of convex combinations of x and y, s = {λx + µy : λ + µ = 1, λ ≥ 0, µ ≥ 0}. Hence, TA(s) = {TA(λx + µy) : λ + µ = 1, λ ≥ 0, µ ≥ 0} = {λTA(x) + µTA(y) : λ + µ = 1, λ ≥ 0, µ ≥ 0} (by Observation 2.26). This means, the transformed line segment TA(s) is simply the line segment connecting the transformed corners TA(x) and TA(y). For our first example above where TA swaps the coordinates, the geometric effect of TA is shown in Figure 2.8 (right): Swapping the two coordinates corresponds to mirroring the input along the diagonal “ ⧸” of the coordinate system. Figure 2.8 (left) gives an example of stretching, which means to make the input longer or shorter along some or all of the coordinates. If the stretching factors are the same for all coordinates, we have a scaling, resulting in a larger or smaller copy of the input. TA(X)TA(e1)TA(e2) A = [ 1√2 − 1√2 1√2 1√2 ] ←− Xe1e2 A = [1 − 1 2 0 1 ] −→ TA(X)TA(e1)TA(e2) Figure 2.9: Right: Shearing the input parallel to the first coordinate. Left: Rotating the input by 45 degrees. Figure 2.9 shows an example of a shear, and of a rotation. If A = I, the identity matrix, then TA is the identity transformation, the function that simply outputs the input, without transforming it. 58 If A is a 2 × 3 matrix, then TA is an orthogonal projection from R 3 to R2. Such projections can be used to draw 3-dimensional objects in 2-dimensional space, for example the cube in the margin. You can recognize the cube, although what you really see is just a 2-dimensional im- age. Figure 2.10 shows how such an image is obtained through a transformation TA. Applying TA : R3 → R 2 with A = [2 1 0 0 1 2 ]:   0 0 0     1 0 0     0 0 1     1 1 1     1 1 0     0 1 1     0 1 0     1 0 1   −→ [ 0 0 ][ 2 0 ][ 0 2 ][ 3 3 ][ 3 1 ][ 1 3 ][ 1 1 ][ 2 2 ] xy Figure 2.10: Orthogonal projection of a 3-dimensional cube. The left figure shows the 8 corners of the 3-dimensional unit cube as vectors in R 3. Two corners are connected in the cube if they differ in exactly one coordinate. The right figure is a 2-dimensional drawing, resulting from applying TA to the cube corners (the connections follow the corners). With different matrices A, we get different drawings; see Figure 2.11 for another drawing. 2.3.2 Linear transformations After these examples, we are ready to define linear transformations in general. Definition 2.27 (Linear transformation). Let T : Rn → R m be a function from Rn to Rm. T is called a linear transformation if the following two statements hold for all x, y ∈ R n and all λ ∈ R. (i) T (x + y) = T (x) + T (y) and (ii) T (λx) = λT (x). By combining (i) and (ii), it then also holds that T (λx + µy) = λT (x) + µT (y). 59 Applying TA : R 3 → R 2 with A = [2 −1 −1 0 2 −1 ]: [ 0 0 ][ 2 0 ][ −1 2 ][ 1 2 ][ −1 −1 ][ −2 1 ][ 0 1 ][ 1 −1 ] Figure 2.11: Projecting the cube in Figure 2.10 (left) using a different matrix By Observation 2.26, all functions TA in Definition 2.25 are linear transformations. The statements (i) and (ii) in Definition 2.27 are called the axioms of a linear transformation. An axiom is a statement that serves as a starting point for further reasoning and argument. 1 What they say is the following: if we want to compute T (x + y), it doesn’t matter whether we first add the two vectors and then apply T to the result, or whether we first apply T to both vectors and then add up the results. And in computing T (λx), we can first scale x by λ and then apply T to the result, but we get the same output when we first apply T to x and then scale the result by λ. We can visualize this with commutative diagrams, see Table 2.2. In math jargon, these are also referred to as diagrams that commute. The arrows in a diagram correspond to certain operations, and if the diagram commutes, this means that we can follow any path through the diagram (apply the operations in any order), and the result is always the same. Let us look at three examples and two counterexamples of linear transformations. The function T : R n → R given by T (x) = n∑ i=1 xi is a linear transformation. We can directly check that the axioms hold, or observe that T is of the form TA for the 1 × n matrix A = [ 1 1 · · · 1 ] and then use Observation 2.26. Another (somewhat trivial but still instructive) example is T : R n → R m given by T (x) = 0. Here, 0 is the m-dimensional zero vector; the axioms of linear transformations obviously hold, and as before, we could also write T in the form TA for A the m × n zero matrix. 1https://en.wikipedia.org/wiki/Axiom, accessed September 7, 2024 60 T x, y −→ T (x), T (y) +  ↓  ↓ + x + y −→ T (x + y) = T (x) + T (y) T T x −→ T (x) ·λ  ↓  ↓ ·λ λx −→ T (λx) = λT (x) T Table 2.2: Commutative diagrams for Definition 2.27 (linear transformation): In both cases, there are two ways to go from the top left to the bottom right, but the result is the same. The identity T : Rn → R n is given by T (x) = x. This is also linear and of the form TA for A = I, the n × n identity matrix; see also Corollary 2.6. Slightly changing the first example to T (x) = n∑ i=1 |xi| = ∥x∥1 gives the 1-norm of x (see Section 1.2.2). This is no longer a linear transformation. To show this, we need to find a violation of the axioms. For example, if x ̸= 0 and λ < 0, then T (λx) > 0 but λT (x) < 0; so axiom (ii) does not always hold. Slightly changing the second example, we can produce another counterexample. Con- sider T (x) = u, where u ∈ Rm is some fixed nonzero vector. Here, axiom (i), T (x + y) = T (x) + T (y), always fails, since T (x + y) = u and T (x) + T (y) = 2u. As an easy consequence of the axioms, every linear transformation T satisfies T (λx + µy) = λT (x) + µT (y), a point that we have already made in Definition 2.27. This generalizes to arbitrary linear combinations. Lemma 2.28. Let T : R n → R m be a linear transformation, let x1, x2, . . . , xℓ ∈ R n and λ1, λ2, . . . , λℓ ∈ R. Then T ( ℓ∑ j=1 λjxj ) = ℓ∑ j=1 λjT (xj). In words, the function value of a linear combination is the linear combination of the function values. 61 Proof (with dots). We use the axioms (i) and (ii) of linear transformations in Definition 2.27 to obtain T ( ℓ∑ j=1 λjxj ) = T ( ℓ−1∑ j=1 λjxj + λℓxℓ ) (i) = T ( ℓ−1∑ j=1 λjxj ) +T (λℓxℓ) (ii) = T ( ℓ−1∑ j=1 λjxj ) +λℓT (xℓ) . Doing the same for ℓ − 1, we further get T ( ℓ∑ j=1 λjxj ) = T ( ℓ−2∑ j=1 λjxj ) + λℓ−1T (xℓ−1) ︸ ︷︷ ︸ T ( ∑ℓ−1 j=1 λj xj ) +λℓT (xℓ) . Repeating this for ℓ − 2, . . . , 1, we finally obtain T ( ℓ∑ j=1 λjxj ) = T ( 0∑ j=1 λjxj ) ︸ ︷︷ ︸ T (0) +λ1T (x1) + . . . + λℓ−1T (xℓ−1) + λℓT (xℓ) = ℓ∑ j=1 λjT (xj), because T (0) = T (0x) = 0T (x) = 0 for every x, using axiom (ii). This proof has the usual dots in λ1T (x1)+· · ·+λℓ−1T (xℓ−1)+λℓT (xℓ), but also dots of a different quality, namely the ones in ℓ − 2, . . . , 1. These dots indicate a repeating pattern in the proof itself. In Section 1.1.5, we have seen dot-free notations for sequences and sums and argued that they are more precise than the ones with the dots; is there also a dot-free notation for proofs such as the one above? Yes, and this notation is known as proof by induction, an important proof technique in mathematics. The concept of induction is the following: suppose we want to prove that some statement holds for all natural numbers n. For example, that n∑ j=1 j = n(n + 1) 2 . We first check the base case, meaning that the statement holds for the first natural number n = 0. Indeed, for n = 0, both sides are 0. For n > 0, we perform the induction step. This proves an implication: if the statement holds for the number n − 1 (this is the induction hypothesis), then it also holds for n (this concludes the induction step). In our example, if the statement holds for n − 1, we can compute n∑ j=1 j = (n−1∑ j=1 j ) + n = (n − 1)n 2 + n = n(n + 1) 2 . ↑ ↑ induction hypothesis easy calculation 62 So if the statement holds for n − 1, it indeed also holds for n. Once we have completed base case and induction step, we have proved the statement for all natural numbers. Why is that? We have proved it for the first natural number (base case), and the induction step lets us conclude that it also holds for the second natural number (if it holds for the first, then it also holds for the second). So we have proved it for the second natural number, and the induction step lets us conclude that it also holds for the third natural number. And so on (these are the dots, but now they don’t appear in the proof, but in its justification). Every natural number is eventually reached by this sequence of steps, so we have proved it for all natural numbers. Even though there are infinitely many natural numbers, every natural number itself is finite, so we eventually get to it. Let’s exercise induction for the statement of Lemma 2.28. Here, the natural number is not called n but ℓ. Proof (by induction). Base case: For ℓ = 0, the statement reads as T (0) = 0 which we already saw to be true in the proof with dots. For ℓ > 0, we perform the induction step: if the statement holds for ℓ − 1, we compute T ( ℓ∑ j=1 λjxj ) = T ( ℓ−1∑ j=1 λjxj + λℓxℓ ) (i) = T ( ℓ−1∑ j=1 λjxj ) + T (λℓxℓ) (ii) = T ( ℓ−1∑ j=1 λjxj ) + λℓT (xℓ) = ℓ−1∑ j=1 λjT (xj) + λℓT (xℓ) (induction hypothesis) = ℓ∑ j=1 λjT (xj). So if the statement holds for ℓ − 1, it indeed holds for ℓ. The proof is conceptually the same as the previous one, but replaces “repeating this for ℓ − 2, . . . , 1” by a single step. Whenever you see a proof using repetitions, or saying “and so on. . . ”, you can be pretty sure that this an informal proof by induction. There is nothing wrong with a proof using “and so on”, as long as you can turn it into a formal proof by induction, if needed. Looking back at the proof of Lemma 2.10, this was also an induction in disguise, where we have used “one by one” instead of “and so on” to indicate a repeating step in a proof. 2.3.3 The matrix of a linear transformation We have seen that every matrix A defines a linear transformation TA (Definition 2.25). Now we can prove that every linear transformation T : R n → Rm (Definition 2.27) is of the 63 form T = TA for a unique m × n matrix A. This means, linear transformations are in some sense “the same” as matrices, but provide a different (and very useful) interpretation of matrices as functions that “do something” (transform input vectors to output vectors). Theorem 2.29. Let T : R n → R m be a linear transformation. There exists a unique m × n matrix A such that T = TA. Proof. In order for T = TA to hold, we must have T (ej) = TA(ej) = Aej for all j ∈ [n]. Since Aej is the j-th column of A, the only candidate for A is A =   | | | T (e1) T (e2) · · · T (en) | | |   , the matrix whose columns are the m-dimensional output vectors that we get when we apply T to the n-dimensional standard unit vectors as inputs. With this matrix A, we indeed get TA(x) = Ax = n∑ j=1 xjT (ej) = T ( n∑ j=1 xjej ) = T (x). The first equality just applies Definition 2.25, in the second one we use Definition 2.4 of matrix-vector multiplication with our matrix A as defined above. In the third equality, we apply Lemma 2.28, and the last one uses that every vector x is a linear combination of the standard unit vectors, where the scalars are simply the entries of x, as in   x1 x2 x3   = x1   1 0 0   ︸︷︷︸ e1 +x2   0 1 0   ︸︷︷︸ e2 +x3   0 0 1   ︸︷︷︸ e3 . A consequence of the proof is the following: every linear transformation is completely determined by its behavior on the standard unit vectors. This also explains why we have paid special attention to this behavior in Figures 2.8 and 2.9. 2.3.4 Linear transformations and matrix multiplication If you have so far thought that the definition of matrix multiplication (Section 2.2) is a bit artificial, linear transformations may change your mind about this. Let’s say we have two linear transformations TA : R n → R a and TB : Rb → Rn. Then we can define a new function T : R b → R a via T ( x︸︷︷︸ ∈Rb ) = TA(TB(x) ︸ ︷︷ ︸ ∈Rn ) ︸ ︷︷ ︸ ∈Ra , 64 i.e. we first apply TB to x and then TA to the result. This operation is known as the composition of functions. It is easy to check that T is again a linear transformation, so we know from Theo- rem 2.29 that there is a matrix C such that T = TC, but what is this matrix? The answer is that C = AB, the product of A and B! Lemma 2.30. Let TA : R n → R a and TB : Rb → R n be two linear transformations. Then TA(TB(x)) = TAB(x) for all x ∈ R b. Proof. This is a one-liner: TA(TB(x)) = TA(Bx) = A(Bx) = (AB)x = TAB(x). Here, we have used Definition 2.25 for TA, TB, TAB as well as associativity of matrix mul- tiplication (Lemma 2.22), where we treat x as a b × 1 matrix. Wit this, we can now also understand generalized associativity of matrix multiplica- tion (Section 2.2.2). This is the fact that the placement of brackets doesn’t matter in the product of several matrices; let’s prove (AB)(CD) = A((BC)D) as an example. Using Lemma 2.30 three times, we get that T(AB)(CD)(x) = TAB(TCD(x)) = TAB(TC(TD(x))) = TA(TB(TC(TD(x)))). The initial brackets in (AB)(CD) have disappeared: to compute T(AB)(CD)(x), we simply apply TD, TC, TB and TA in this order. Let’s do the same for A((BC)D). We get TA((BC)D)(x) = TA(T(BC)D(x)) = TA(TBC(TD(x))) = TA(TB(TC(TD(x)))). In both cases, the result is the same for all x, so TA((BC)D) and TA((BC)D) are the same functions. With Theorem 2.29, we can conclude that also the matrices must be the same, hence (AB)(CD) = A((BC)D). We refrain from stating it as a formal theorem, but if we have a product P of matri- ces A1, A2, . . . , Ak, with brackets put in an arbitrary manner, then TP (x) is computed by applying TAk, TAk−1, . . . , TA1 in this order, so the result (and therefore also the product P ) does not depend on where the brackets are. 2.3.5 Kernel and Image For every linear transformation, there are two important sets of vectors. Definition 2.31 (Kernel and image). Let T : R n → R m be a linear transformation. The set Ker(T ) := {x ∈ R n : T (x) = 0} ⊆ R n is the kernel of T . The set Im(T ) := {T (x) : x ∈ R n} ⊆ Rm is the image of T . 65 T : R n → T (x) Ker(T ) Im(T ) R1 ∑n i=1 xi {x ∈ Rn : ∑n i=1 xi = 0} R Rm 0 R n {0} Rn x {0} R n Table 2.3: Kernel and image of some linear transformations The image of T is the set of all outputs that T can produce. This is actually a familiar concept. Observation 2.32. Let T : R n → R m be a linear transformation and A the m × n matrix such that T = TA (see Theorem 2.29). Then Im(T ) = C(A), the column space of A. This immediately follows from Definition 2.8 of C(A) and T (x) = Ax. In light of this, some sources also call the column space of A the image of A. Similarly, the kernel (the set of all inputs that produce output 0) can be expressed as Ker(T ) := {x ∈ R n : Ax = 0}. This is a set that we will later call the nullspace of A (Definition 4.31). Again, some sources also call this the kernel of A. Table 2.3 provides kernel and image of the linear transformations that we have con- sidered as examples on page 60. Exercise 2.33. Let v ∈ R m, w ∈ R n be nonzero vectors and consider the m × n matrix A = vw⊤ (this matrix has rank 1 by Lemma 2.21). Give formulas for Ker(TA) and Im(TA), depending on the vectors v and w! 66 Chapter 3 Solving Linear Equations Ax = b 3.1 Systems of linear equations How to solve systems of linear equations is one of the foundational problems of linear algebra. Such systems appear in many applications today and can be very large. One concrete application that we present is Google’s PageRank algorithm. We introduce systems of linear equations formally, using matrices and vectors, and we explain their “computer versions” that are used in computer programs for solving systems of linear equations. We have already seen a system of linear equations in Section 0.3: D = 2S D = C + 3 D + S + C = 17 With its three equations in three variables, this system encodes three pieces of information about the ages of three children (Dominik, Susanne and Claudia). Solving the system means to find values for the variables such that all the equations are satisfied. In general, systems of linear equations can have arbitrarily many equations and vari- ables; for a systematic treatment, we write such systems in a standard form, and we use vectors and matrices to argue about them. In standard form, the variables are called x1, x2, . . . and appear only left of “=” in the equations. In this form, the system from the children’s age puzzle is x1 − 2x2 = 0 x1 − x3 = 3 (3.1) x1 + x2 + x3 = 17 Here, x1 stands for D, x2 for S and x3 for C. 67 Definition 3.1 (System of linear equations). A system of linear equations in m equations and n variables x1, x2, . . . , xn is of the form a11x1 + a12x2 + · · · + a1nxn = b1 a21x1 + a22x2 + · · · + a2nxn = b2 ... am1x1 + am2x2 + · · · + amnxn = bm, where the aij and bi stand for known real numbers, and the xi stand for unknown real numbers that we want to compute such that they satisfy all the equations. In matrix-vector form, this can be written as Ax = b :      a11 a12 · · · a1n a21 a22 · · · a2n ... ... . . . ... am1 am2 · · · amn      ︸ ︷︷ ︸ A, m×n      x1 x2 ... xn      ︸ ︷︷ ︸ x∈Rn =      b1 b2 ... bm      ︸ ︷︷ ︸ b∈Rm . (Here we use matrix-vector multiplication in the form of Observation 2.5.)A is the coefficient matrix, b is the right-hand side, and x is the vector of variables. Solving the system means to compute a vector x ∈ R n such that Ax = b. In the form Ax = b, system (3.1) reads as  1 −2 0 1 0 −1 1 1 1   ︸ ︷︷ ︸ A   x1 x2 x3   ︸ ︷︷ ︸ x =   0 3 17   ︸ ︷︷ ︸ b . In this language, we can formulate linear independence of the columns of a matrix A in the following way. x Observation 3.2. Let A be an m × n matrix. The columns of A are linearly independent if and only if the system Ax = 0 has a unique solution, x = 0. Proof. By Definition 2.4 of matrix-vector multiplication, the solutions are precisely the vectors of scalars that express 0 as a linear combination of the columns. A unique solu- tion means that 0 can only be written as a trivial linear combination of the colums. By Lemma 1.19, this is equivalent to the columns of A being linearly independent. 3.1.1 The PageRank algorithm As an example where (large) systems of linear equations come up, we will discuss the PageRank algorithm. This algorithm was published by the Stanford students Sergey Brin and Lawrence Page in 1998 [BP98, pp. 109-110]. The first sentence of the abstract is the following: 68 In this paper, we present Google, a prototype of a large-scale search engine which makes heavy use of the structure present in hypertexts. The rest is history. To understand what PageRank does, let’s look at an example. Figure 3.1 shows a link graph where the circles represent web pages and the arrows indicate links between them. For the whole internet, you can think of a similar figure with a billion (10 9) circles. 213456 Figure 3.1: A link graph. The circles represent web pages, and an arrow from page p to page q indicates that p has a link to q. The PageRank algorithm sorts the pages by relevance. Which of the six pages shown in Figure 3.1 do you think is most relevant? It’s not clear what we mean by that, so let’s start by measuring relevance in terms of the number of links pointing to a page. These are also called citations of the page. In scientific literature, the number of citations that a paper has is indeed an established measure of relevance, so it seems natural to also apply it to web pages. This has been done long before PageRank, and according to this measure, page 2 (with 4 citations) is clearly the most relevant one; all other pages have at most 2 citations. The main insight behind PageRank is that not all citations are worth the same. Here are the two key points. 1. Citations from relevant pages should count more than citations from irrelevant ones. It is too easy to collect many citations from irrelevant pages created for that purpose. 2. If a page cites a large number of other pages, an individual citation on that page should count less. It is too easy to cite many random pages. In both points, web page citations fundamentally differ from scientific citations, where it is not so easy to manipulate citation counts in the described way (although it has be- come easier with predatory publishers that do not enforce quality standards). Point 2 is easy to incorporate into the relevance measurement by simply giving less weight to citations from pages that cite many other pages. How to address point 1 is less 69 clear. We want to define the relevance of a page depending on the relevances of the pages that cite it, but this definition goes in circles. In Figure 3.1, page 2 cites page 3 which cites page 5 which cites page 2. A system of linear equations comes to the rescue. Let’s focus on the situation in Fig- ure 3.1 and introduce variables x1, x2, . . . , x6 for the relevances of pages 1, 2, . . . , 6. We concretely define the relevance of a page as the sum of the weighted relevances of the pages that cite it, where the weighted relevance of a page is its relevance, divided by the number of citations on the page (this addresses point 2). Putting this in formulas for the relevance of page 2, we get the linear equation x2 = x1 2 + x4 + x5 + x6 4 . We have contributions from each of the four pages that cite page 2. For page 6, the weighted relevance is only x6/4, because page 6 cites 4 pages. Repeating this for all pages, we obtain a system of 6 equations in 6 variables. Unfortunately, setting all the xj to 0 is a solution from which we learn nothing. To avoid this, PageRank introduces a damping factor 0 < d < 1 and replaces the equation for x2 (and all others in the same way) by x2 = (1 − d) + d ( x1 2 + x4 + x5 + x6 4 ) . For d = 1, we get the previous system with the useless all-zero solution, but for d < 1, it can be proved that the system has a unique solution with all page relevances summing up to the number of pages. Brin and Page suggest to use d ≈ 0.85. Using d = 7/8, the relevances (which are then called page ranks) can be computed for example by pasting the following code into Wolfram Alpha. 1 solve( d=7/8, x1=(1-d)+d*(x6/4), x2=(1-d)+d*(x1/2+x4+x5+x6/4), x3=(1-d)+d*(x1/2+x2), x4=(1-d)+d*(x6/4), x5=(1-d)+d*(x3/2+x6/4), x6=(1-d)+d*(x3/2) ) The solution (rounded to five digits) is x1 = 0.31797, x2 = 1.6761, x3 = 1.7307, x4 = 0.31797, x5 = 1.0751, x6 = 0.88217. This means, according to PageRank, page 3 is actually the most relevant one, followed by pages 2, 5, 6. Pages 1 and 4 are the least relevant ones, with the same low page rank. How to solve systems of linear equations like that efficiently will be the focus of this chapter. 1https://www.wolframalpha.com/ 70 3.1.2 Computer vectors and matrices Systems of linear equations are solved by algorithms, stepwise procedures to solve a given problem. We explain the algorithms with examples and give mathematical proofs of cor- rectness, but in one case, we also present the full algorithm in computer code that can directly be used in Java and C++ programs, for example. For this, we first need to understand how matrices and vectors are represented in a computer program. While the details depend on the programming language being used, the concept is common to many languages. We introduce the concept here on a somewhat abstract level (but it directly works like that in Java and C++). A vector such as the right-hand side b is represented by an array. For a vector b ∈ Rm, we use an array named b with entries b[0], b[1], . . . , b[m − 1]. We can visualize b as b =      b[0] b[1] ... b[m − 1]      and call it a computer vector. Array indices start from 0, not from 1, this is the main thing one needs to get used to with arrays. The reason for this is that an array typically occupies a contiguous chunk of computer memory, starting at the memory location of its first entry. The index of an entry tells us how many locations “further to the right” the entry is. As the first entry is 0 locations further to the right, it consequently has index 0. A matrix such as the coefficient matrix A is represented by a two-dimensional array. This is an array of arrays. For a matrix A ∈ R m×n, we use an array named A with m entries A[0], A[1], . . . , A[m − 1] that are itself arrays with n entries each, representing the rows of A. This can be visualized as A =     |A[0]||A[1]|...|A[m − 1]|      and called a computer matrix in row notation. Sometimes, it makes sense to interpret the entries of A as the columns of A, but rows are better aligned with our understanding of a matrix entry aij as being in row i and column j. Indeed, if we visualize row A[i] of our computer matrix as A[i] = [ A[i][0] A[i][1] · · · A[i][n − 1]] , we get that A[i][j] is the entry of A in row i and column j (both counting from 0). In a full computer program, the entries of computer vectors and matrices also have types that encode what kind of numbers they represent. To represent real numbers on a computer, one typically uses floating-point numbers, and the corresponding types tend to be called float and double (standing for double precision). But floating-point numbers only approximate real numbers, and the problems with that are studied in numerical anal- ysis. Here, we abstract from this issue and do not talk about types. In writing down the 71 algorithms, we simply pretend that entries of computer vectors and matrices are also real numbers. 3.2 Gauss elimination We present Gauss elimination, the classical algorithm for solving systems of m equa- tions in the same number m of variables. Our version does not always work, but is particularly simple and provides important insights into the structure of the coef- ficient matrix A. As we show, the algorithm works exactly when the matrix A has linearly independent columns. We will also count the number of steps that the algo- rithm performs. For this section, we restrict to the case where the system Ax = b has a square coef- ficient matrix, i.e. A is assumed to be an m × m matrix. This is the important case of “m equations in m variables.” For example, the PageRank algorithm described in Sec- tion 3.1.1 needs to solve a system of this kind. The general case of arbitrary A will be treated in Section 3.5. 3.2.1 Back substitution We start with an easy case: if A is upper triangular (Definition 2.3 (iii)), the system Ax = b can be solved by back substitution. Let’s look at a 3 × 3 example:   2 3 4 0 5 6 0 0 7     x1 x2 x3   =   19 17 14   . Here, the blue entries are the ones that need to be zero in an upper triangular matrix. Due to this matrix shape, equation 3 has only variable x3: 7x3 = 14. We can solve this directly and get x3 = 2. Equation 2 has variables x2 and x3, but as we already know the value of x3, we can substitute x3 with this value and then directly solve for x2. In equation 1, we finally have all three variables, but if we substitute x2 and x3 with their already known values, we can directly solve for x1 and are done. Table 3.1 summarizes the steps. If A is an upper triangular m × m matrix, this works in the same way. We go through the equations in backwards order m, m − 1, . . . , 1. Equation i reads as m∑ j=i aijxj = bi. 72 equation before substitution after substitution solution 1 2x1 + 3x2 + 4x3 = 19 2x1 + 11 = 19 x1 = 4 2 5x2 + 6x3 = 17 5x2 + 12 = 17 x2 = 1 3 7x3 = 14 x3 = 2 ↑        Table 3.1: Back substitution in an upper triangular system of 3 equations in 3 variables We already know the values for xi+1, . . . , xm from the equations below. So we can substi- tute these variables with their values and then directly solve for xi, giving us xi = bi − ∑m j=i+1 aijxj aii . But this only works if the diagonal entry aii is nonzero, since we have to divide by it. This means, we need an upper triangular matrix where all diagonal entries are nonzero. Table 3.2 provides the algorithm in computer code. Note that the row and column indices are between 0 and m − 1, in agreement with the convention on computer vectors and matrices that we have discussed in Section 3.1.2. 1 f o r ( i = m− 1 ; i >= 0 ; i − −) { 2 sum = b [ i ] ; 3 f o r ( j = i + 1 ; j < m; j ++) 4 sum −= A[ i ] [ j ] * x [ j ] ; 5 x [ i ] = sum / A[ i ] [ i ] ; 6 } Table 3.2: The back substitution algorithm in computer code. The syntax is Java / C++. 3.2.2 Elimination If the input matrix A is not upper triangular, elimination is trying to transform the system Ax = b into an equivalent system U x = c where U is an upper triangular matrix. Equiv- alent means that both systems have the same solutions. If elimination succeeds, we can solve U x = c using back substitution and obtain a solution of Ax = b. Elimination transforms the system step by step. Again, we demonstrate this with a 3 × 3 example where A =   2 3 4 4 11 14 2 8 17   , b =   19 55 50   . (3.2) We would like to turn the three red nonzero entries into zeros so that the matrix becomes upper triangular. We do this column by column, from left to right. To get rid of the 4, we 73 subtract 2 · (equation 1) from equation 2 of the system: (equation 2) : 4x1 + 11x2 + 14x3 = 55 − 2 · (equation 1) : 4x1 + 6x2 + 8x3 = 38 (equation 2’) : 5x2 + 6x3 = 17 This eliminates variable x1 from the second equation, and we obtain an updated system A ′x = b′ with a new second equation, given by A ′ =  2 3 4 0 5 6 2 8 17   , b′ =   19 17 50   . In this step, x is just a distraction, all that matters are the entries of A and b. To get A ′ from A, we subtract 2 · (row 1) from (row 2). The same operation transforms b to b′ (here, a row is just a single number). Subtracting a multiple of some row from another row is a row subtraction. This can also be viewed as a linear transformation applied to all columns of A, and to b. We further know that each linear transformation comes from a matrix; see Section 2.3.3. In our example, the transformation is TE21     v1 v2 v3     =   v1 v2 − 2v1 v3   =   1 0 0 −2 1 0 0 0 1   ︸ ︷︷ ︸ E21   v1 v2 v3   . Hence, matrix and right-hand side of the transformed system A ′x = b′ can be computed as A ′ = E21A, b′ = E21b, E21: ”subtract 2·(row 1) from (row 2)”. E21 is called an elimination matrix. Generally, we use Eij to denote an elimination matrix that is supposed to create a zero entry in row i and column j. To argue that this transformation does not change the solutions, we need that it can be undone: A = E′ 21A′, b = E′ 21b′, E′ 21: ”add 2·(row 1) to (row 2)”. In the 3 × 3 case, the matrix for this row addition is E′ 21 =   1 0 0 2 1 0 0 0 1   . Now it easy to see that the two systems Ax = b and A ′x = b′ have the same solutions. First, if Ax = b, then A ′x = E21Ax = E21b = b′. 74 So every solution of Ax = b is also solution of A′x = b′. And if A′x = b′, then Ax = E′ 21A ′x = E′ 21b′ = b. So every solution of A ′x = b′ is also solution of Ax = b. Continuing from A ′, b′, there are two more steps to turn the remaining two red entries into zeros; in each step, the diagonal entry of the current column is used to eliminate the nonzeros below it. This entry is called the pivot. Table 3.3 summarizes all three steps. The result is precisely the system in upper triangular form that we have previously solved with back substitution in Section 3.2.1. As solutions never change during elimination, x =   4 1 2   also solves the original system (3.2) (check this!). fat number: the pivot A =   2 3 4 4 11 14 2 8 17   b =   19 55 50   subtract 2·(row 1) from (row 2): ↓ ↓ E21 =   1 0 0 −2 1 0 0 0 1   E21A =  2 3 4 0 5 6 2 8 17   E21b =   19 17 50   subtract 1·(row 1) from (row 3): ↓ ↓ E31 =   1 0 0 0 1 0 −1 0 1   E31E21A =  2 3 4 0 5 6 0 5 13   E31E21b =   19 17 31   subtract 1·(row 2) from (row 3): ↓ ↓ E32 =  1 0 0 0 1 0 0 −1 1   E32E31E21A ︸ ︷︷ ︸ U =  2 3 4 0 5 6 0 0 7   E32E31E21b ︸ ︷︷ ︸ c =   19 17 14   ↑ elimination matrices done! Table 3.3: Elimination reduces a system of linear equations to upper triangular form. The red entries are the ones that must become 0 (blue entries). Row exchanges. In a less nice case, it can happen that the pivot is 0 which we cannot use to eliminate nonzeros below it. But if there is some nonzero entry anywhere below the pivot, we can perform a row exchange to obtain a nonzero pivot. Table 3.4 gives an example for this situation. Here, we are immediately done after the row exchange, but in general, we would now use the new nonzero pivot to eliminate the nonzeros below. 75 A =   2 3 4 4 6 14 2 8 17   b = · · · elimination in first column: ↓ ↓ E31E21A =   2 3 4 0 0 6 0 5 13   E31E21b = · · · pivot 0: exchange (row 2) and (row 3): ↓ ↓ P23 =   1 0 0 0 0 1 0 1 0   P23E31E21A ︸ ︷︷ ︸ U =   2 3 4 0 5 13 0 0 6   P23E31E21b ︸ ︷︷ ︸ c = · · · ↑ permutation matrix done! Table 3.4: Elimination with row exchanges to ensure nonzero pivots A row exchange can also be interpreted as a linear transformation; its matrix is a spe- cial permutation matrix. In general, a permutation matrix corresponds to a permutation, a linear transformation that reorders the entries of its input vector. A row exchange is a special permutation that only exchanges two entries. As before with row subtractions, we can argue that a row exchange does not change the solution of the linear system of equations. Here, this is even more obvious, as a row exchange simply corresponds to exchanging the order of equations. Failure. Finally, there is an ugly case: the pivot is 0, and all entries below it are also 0, so that no row exchange helps and we are stuck. Table 3.5 shows an example. In this case, we give up for now. We also consider it ugly if this happens in the last column; see the right example in Table 3.5. A =   2 3 4 4 6 14 2 3 17   b = · · · elimination in first column: ↓ ↓ E31E21A =   2 3 4 0 0 6 0 0 13   E31E21b = · · · no row exchange helps, give up for now!   2 3 4 0 5 6 0 0 0   we also fail here! Table 3.5: Elimination fails: no row exchange can ensure a nonzero pivot. From a purely functional point of view, there would be no reason to give up. After all, the goal of the elimination steps is to remove the zeros below the diagonal; if in some 76 column, we start with only zeros below the diagonal, we could consider this column done and move on to the next one. Indeed, this works and transforms any square matrix A to an upper triangular matrix U , possibly with some 0’s on the diagonal. Back substitution becomes a bit more tricky, then (there might be no solutions, or many solutions of U x = c), but this can be handled. In many sources, Gauss elimination is presented like that for all matrices A, even for non-square ones. When we give up, this is not laziness. Distinguishing success and failure in Gauss elimination will allow us to exactly understand the “good” square matrices, the ones for which we succeed. This kind of understanding provides important insights, so we prefer to call it (mathematically) lazy to run a version of Gauss elimination that always succeeds. But once we have understood success and failure, we will make sure to also develop methods that always succeed; see Section 3.5. Gauss elimination in computer code. The code for Gauss elimination (with row sub- tractions and row exchanges) is given in Table 3.6. The code transforms A and b in place, meaning that the entries of the arrays A and b are being changed so that they in the end correspond to an equivalent upper triangular system. The code corresponds to the previous discussion, but there are two points to notice: in subtracting a multiple of the pivot row j from some row i below, we only update the entries in columns j, j +1, . . . , m of row i: in all columns further to the left, there is nothing to do, since row j has zero entries there, created in previous elimination steps. Also, the entry A[i][j] becomes zero by design, so there is no need to compute it. The second point is that elimination in the last column does not perform any actual work, but still checks whether the entry in the lower right corner of the matrix is nonzero. At this point, we catch the ugly corner case mentioned in Table 3.5 (right). 3.2.3 Success and failure In the previous section, we have seen that Gauss elimination may have to give up in solving a system of m linear equations in m variables. This seems unsatisfactory, and in reading other sources, you find variants of Gauss elimination that always succeed. Here, we do not want to fix the algorithm but understand why it is broken. The in- sights gained through this will also take us further along in the theory of linear algebra. So we rather like to think of giving up as a productive failure [Kap14]. We have argued (in the examples) that each step of Gauss elimination transforms the current system of linear equations into another one with the same solutions. This property is very important to guarantee that by solving the upper triangular system that we get in the end, we also solve the original system. Now, we want to make this argument in general, for systems of m equations in n variables. In this case, the elimination matrix corresponding to a row substraction is an 77 1 f o r ( j = 0 ; j < m; j ++) { 2 / / e l i m i n a t e i n column j 3 i f (A[ j ] [ j ] == 0 ) { 4 / / z e r o p i v o t , t r y row e x c h a n g e 5 k = j + 1 ; 6 while ( k < m && (A[ k ] [ j ] == 0 ) ) k ++; 7 i f ( k == m) 8 r e t u r n f a l s e ; / / no row e x c h a n g e i s p o s s i b l e , g i v e up 9 e l s e { 10 / / e x c h a n g e rows j and k . . . 11 row A = A[ j ] ; A[ j ] = A[ k ] ; A[ k ] = row A ; / / . . . o f A 12 row b = b [ j ] ; b [ j ] = b [ k ] ; b [ k ] = row b ; / / . . . o f b 13 } 14 } 15 / / c r e a t e z e r o s b e l o w A[ j ] [ j ] 16 f o r ( i = j + 1 ; i < m; i ++) { 17 / / s u b t r a c t c * row j f r o m row i . . . 18 c = A[ i ] [ j ] / A[ j ] [ j ] ; 19 A[ i ] [ j ] = 0 ; 20 f o r ( k = j + 1 ; k < m; k++) 21 A[ i ] [ k ] −= c * A[ j ] [ k ] ; / / . . . o f A 22 b [ i ] −= c * b [ j ] ; / / . . . o f b 23 } 24 } 25 r e t u r n t r u e ; Table 3.6: The Gauss elimination algorithm in computer code. The return value indicates whether the elimination was successful. m × m matrix Eij of the form Eij =       ⧹ 1 ⧹ −c 1 ⧹       ← j ← i ↑ ↑ j i Here ⧹ stands for a contiguous sequence of diagonal 1’s, and all omitted entries are 0. Hence, Eij is the identity matrix with an extra −c in row i and column j. You can convince yourself (using the rules of matrix-vector multiplication; see Section 2.1.1) that this is indeed the matrix of the linear transformation “subtract c·(row j) from (row i).” Applying it to all columns of A and to b, we get a new system A ′x = b′ with A ′ = EijA, b′ = Eijb. A permutation matrix as it appears in Gauss elimination for a row exchange has the 78 form Pjk =       ⧹ 0 1 ⧹ 1 0 ⧹       ← j ← k ↑ ↑ j k This is the matrix of the linear transformation “exchange (row j) and (row k),” and if we apply this row exchange, we get a new system A′x = b′ with A′ = PjkA, b′ = Pjkb. A row operation is either a row subtraction, or a row exchange, and the matrix of a row operation is called the row operation matrix. We can now prove the following result. Lemma 3.3. Let Ax = b be a system of m linear equations in n variables, and let M ∈ R m×m be a row operation matrix. Let A′ = M A and b′ = M b be the result of applying the row operation to both A and b. Then the two systems Ax = b and A ′x = b′ have the same solutions. Proof. This works as in the 3 × 3 examples: If Ax = b, then A′x = M Ax = M b = b′. For the other direction, let M ′ be the matrix of the row operation that undoes M , i.e. transforms A ′ and b′ back to A and b, by either adding back c · (row j) to (row i), or by exchanging row j and row k again. Now, if A ′x = b′, then Ax = M ′A ′x = M ′b′ = b. Hence, every solution of Ax = b is a solution of A′x = b′ and vice versa. Corollary 3.4. Let A be an m × n matrix, let M ∈ Rm×m be a row operation matrix, and let A ′ = M A be the result of applying the row operation to A. Then A has linearly independent columns if and only if A ′ has linearly independent columns. Proof. We apply Lemma 3.3 with b = 0 (and hence b′ = 0 as well). This gives us that Ax = 0 and A′x = 0 have the same solutions. If there is just one solution, namely the zero vector, we get linearly independent columns in both cases. If there is also another solution, we get linearly dependent columns in both cases; see Observation 3.2. Now we can understand exactly when Gauss elimination succeeds. Theorem 3.5. Let Ax = b be a system of m linear equations in m variables. The following two statements are eqivalent. (i) Gauss elimination as in Table 3.6 succeeds. (ii) The columns of A are linearly independent. 79 We will prove (i)⇒(ii) “normally”, but (ii)⇒(i) (“if (ii), then (i)”) is easier to prove by contraposition. This proves the logically equivalent implication “if not (i), then not (ii)”. You know this from natural language. Consider the sentence “If it rains, then the street is wet.” Another (logically equivalent) way of saying this is “If the street is not wet, then it does not rain”. The symbol for not (logical negation) is ¬, so the contraposition can be written as ¬(i)⇒ ¬(ii). Proof. (i) ⇒(ii): If Gauss elimination succeeds, it produces an upper triangular matrix U with all diagonal entries ujj nonzero. Such a matrix U has linearly independent columns: no column is a linear combination of the previous ones, due to the nonzero diagonal elements (and zeros to the left of them). Recall Corollary 1.20 (iii) for this alternative definition of linear independence. Hence, also A has linearly independent columns by Corollary 3.4 (applied throughout all elimination steps). ¬(i)⇒ ¬(ii): If Gauss elimination fails in column j, we have an intermediate matrix A ′ with zeros in rows j, j + 1, . . . , m of column j. But in all previous columns, elimination succeeded, and the diagonal entries are nonzero. Hence, A ′ looks like this: A ′ = [ U v · · · 0 0 · · · ] , U ∈ R (j−1)×(j−1) upper triangular, all uℓℓ ̸= 0, v ∈ R j−1. From this, we construct a nonzero solution x to A′x = 0, showing that A ′ (and hence also the original A, by repeated application of Corollary 3.4) has linearly dependent columns. We start by setting xj+1, xj+2, . . . , xm = 0. This already ensures that the vector A′x has zeros in rows j, j + 1, . . . , m, and to get A ′x = 0, we also need zeros in the first j − 1 rows, meaning that U      x1 x2 ... xj−1      ︸ ︷︷ ︸ y +xjv = 0. This can be achieved for example by setting xj = −1 and solving U y = v by back substi- tution; see Section 3.2.1. You may wonder whether this proof also works (or how it is to be interpreted) if Gauss elimination already fails in the first column (j = 1). In this case, the complete first column of the original matrix A is 0, and hence, the columns are linearly dependent for obvious reasons; see also Table 1.3 and the discussion before it. 80 3.2.4 Runtime Whenever computer scientists see an algorithm, they ask how efficient it is. For any given problem, there are different algorithms, and they may differ in their efficiency. Naturally, we would like to identify the most efficient one. But in order to do so, we must first define and measure the efficiency of an algorithm. The most important measures are runtime and memory consumption. Here, we focus on runtime. But how do we measure this? Suppose we are given a concrete system of linear equations, for example the one on page 70 that we previously fed into Wolfram Alpha to compute page ranks for the “toy internet” of Figure 3.1. Now we know how we can solve it ourselves, using Gauss elimination with back substitution as an algorithm, as realized for example by the lines of code in Tables 3.6 and 3.2. If we integrate these lines of code into a concrete computer program and run it on a concrete computer, we can measure the time it takes to solve our concrete system of linear equations. What we get is a number (in milliseconds, for example). On a different system of linear equations, and on a different computer, we will get a different number. So this kind of measurement is not very informative. What we do instead is the following: we inspect the algorithm, identify the crucial steps, and try to count how many of them the algorithm makes, depending on m. This results in a function g : N → N, where g(m) is the number of crucial steps needed to solve a system with m linear equations in m variables. If the “crucial steps” account for most of the steps in the algorithm, g(m) is also a good estimate for the algorithm’s efficiency in terms of the total number of steps. This measure is independent of the computer that we will actually use to run the algorithm. Here, the crucial steps are the arithmetic steps: how often does the algorithm add, sub- tract, multiply, or divide two numbers? On top of this, there are clearly other steps, but only rather simple ones, and not many more than arithmetic steps. For example, in sub- tracting a multiple of a row from another row (lines 20–22 in Table 3.6), the algorithm performs (m − j) multiplications and (m − j) subtractions. On top of this, there are extra steps: initialize a loop variable, check whether the loop is done, change a loop variable, change an array entry. But each of these extra steps also happens at most m − j times. Generally, we want to argue that for each arithmetic step, only a couple of extra steps are needed. This makes sense, since the main task is computation, and everything else should only be there to support the main task. To formalize this, we can imagine each step to cost CHF 1 each. The arithmetic steps have to pay not only for themelves, but also bear the cost of the extra steps. If we can charge the extra steps to the arithmetic steps in such a way that each arithmetic step pays at most CHF 10, say, then we know that the total number of steps is at most 10 times the number of arithmetic steps. Ideally, we want to charge each extra step to the arithmetic step that it directly sup- ports, and in this way (hopefully) never overcharge any single arithmetic step. For the extra steps in the aforementioned loop (lines 20-22), this is easy: we charge them directly 81 to the multiplication that happens in the corresponding iteration of the loop. The precise details of this kind of charging may be a little tricky, but it can be done in the same spirit for all extra steps, with one major exception: an unsuccessful search for a row exchange (lines 3-13) cannot easily be charged to any arithmetic step. For example, if A’s first column is 0, Gauss elimination does not do a single arithmetic step, but still needs extra steps before it gives up. In this case, we have no one to pay for them, but the cost of these steps is small. What we find in the end is the following: Let g(m) be the number of arithmetic steps necessary to solve a system of m equations with m variables using Gauss elimination with back substitution. Furthermore, let t(m) be the total number of steps needed for that in the worst case (we don’t even have to be super precise about what we count as a step). Then there is a constant c ∈ N such that t(m) ≤ c(g(m) + m) for all m ∈ N. (3.3) Here, the “+m” pays for the few steps in an unsuccessful search for a row exchange that cannot be charged to any arithmetic steps. You can also frame this as adding m “ghost” arithmetic steps, after which we can make sure that every arithmetic step will be charged at most CHF c. The previous inequality is our interpretation of the arithmetic steps accounting “for most” of the steps. If you want a less generous interpretation of “at most”, you also need to upgrade other steps to crucial status, but this usually entails a more complicated analysis. To summarize: for Gauss elimination with back substitution, we will find a function g such that g(m) counts the number of arithmetic steps necessary to solve a system of m equations with m variables. After adding m, the total number of steps is by some constant factor c larger. The precise value of c is difficult to determine. It depends on our charging scheme and on what we count as an “extra step”. Therefore, it is common practice not to talk about c at all. Instead, we will say that the runtime (in terms of total step count) for input size m is O(g(m) + m). This big-O-notation means “at most g(m) + m, multiplied by some constant factor” (that does not depend on m but could otherwise be anything; for example, 10, or 100). The good thing is that O(g(m) + m) is also a valid description of the algorithm’s actual runtime on any given computer. This is because the actual runtime (in whatever unit of time) is also just the total number of steps, multiplied by another constant that bounds the actual runtime of a single step. After having set the stage, we can now formulate and prove the main result about the efficiency of Gauss elimination. Our count for the number of arithmetic steps in Theo- rem 3.6 will be precise in the case where elimination succeeds; otherwise, it is an upper bound. Generally, it is rare that we can count the steps of an algorithm precisely. But up- per bounds “close to the truth” are usually all that is needed, in particular in an analysis involving a “big O.” 82 Theorem 3.6. Let Ax = b be a system of m linear equations in m variables, m ≥ 1. Gauss elimination with back substitution solves Ax = b (or gives up) with at most g(m) = 2 3m 3 + 3 2 m 2 − 7 6m arithmetic steps and therefore in time O(m 3). Here, O(m 3) is a shorthand for O(f (m)) where f (m) = m 3. Proof. Time O(m 3) follows from the value of g(m), since the first term 2 3m 3 is the dominat- ing one. We can for example say that that g(m) + m ≤ 2 3 m 3 + 3 2 m 3 = 13 6 m3. As argued before, the runtime is at most c(g(m) + m) for some constant c, hence at most c · 13 6 m 3 which is O(m 3). Let’s count the number of arithmetic steps. For elimination, this can be done by in- specting the code in Table 3.6. Arithmetic steps happen in line 18 (1 division), line 21 (1 multiplication, 1 subtraction) and line 22 (1 multiplication, 1 subtraction). These are done repeatedly through the surrounding loops starting in lines 20 (for (k...)), 16 (for (i...)) and 1 (for (j...)). Taking the loop ranges into account, we arrive at the following total number of arithmetic steps (if we give up on the way, we have less): e(m) = m−1∑ j=0 m−1∑ i=j+1       Line 18 ︷︸︸︷ 1 + m−1∑ k=j+1 Line 21 ︷︸︸︷ 2 ︸ ︷︷ ︸ Line 20 + Line 22 ︷︸︸︷ 2       ︸ ︷︷ ︸ Line 16 ︸ ︷︷ ︸ Line 1 . This triple sum looks a bit scary, but it’s not too bad. The number in the big bracket is easily computed, and the sum over i as well: e(m) = m−1∑ j=0 m−1∑ i=j+1 (2(m − j) + 1) = m−1∑ j=0 (m − j − 1)(2(m − j) + 1) = 2 m−1∑ j=0 (m − j − 1)(m − j) ︸ ︷︷ ︸ emul(m)=esub(m) + m−1∑ j=0 (m − j − 1) ︸ ︷︷ ︸ ediv(m) . Here, emul(m) counts the number of multiplications and esub(m) the number of sub- tractions (both numbers are the same); ediv(m) counts the number of divisions. After 83 substituting m − j − 1 with ℓ, these become standard summations that can be found in collections of formulas (or proved by induction): emul(m) = esub(m) = ∑m−1 ℓ=0 ℓ(ℓ + 1) = 1 3(m 3 − m), ediv(m) = ∑m−1 ℓ=0 ℓ = 1 2(m 2 − m). For back substitution as in Table 3.2, we can count in the same way and obtain that the number of arithmetic steps is b(m) = m−1∑ i=0       m−1∑ j=i+1 Line 4 ︷︸︸︷ 2 ︸ ︷︷ ︸ Line 3 + Line 5 ︷︸︸︷ 1       ︸ ︷︷ ︸ Line 1 = m−1∑ i=0 (2(m − i − 1) + 1) = 2 m−1∑ i=0 (m − i − 1) ︸ ︷︷ ︸ bmul(m)=bsub(m) + m−1∑ i=0 1 ︸ ︷︷ ︸ bdiv(m) . This gives bmul(m) = bsub(m) = ∑m−1 ℓ=0 ℓ = 1 2(m2 − m) bdiv(m) = ∑m−1 i=0 1 = m. In summary, we get e(m) = 2emul(m) + ediv(m) = 2 3(m 3 − m) + 1 2(m2 − m), b(m) = 2bmul(m) + bdiv(m) = (m 2 − m) + m, g(m) = e(m) + b(m) = 2 3(m 3 − m) + 3 2(m2 − m) + m. After collecting the linear terms (the ones with “m”), we get the claimed value. Let us look at some concrete values of g(m). For example, we get g(1) = 2 3 + 3 2 − 7 6 = 1. This corresponds to the fact that one linear equation ax = b in one variable x can be solved with one arithmetic step (a division), resulting in x = b/a. Here are some more values where we also list the dominating term 2 3m 3 (rounded to 84 integer) for comparison. m g(m) 2 3m3, rounded to integer 2 9 5 3 28 18 4 62 43 5 115 83 6 191 144 7 294 229 8 428 341 9 597 486 ... ... ... 100 681550 666667 ... ... ... 1000 668165500 666666667 This shows that for larger systems, the step count becomes quite high, and the dominating term actually accounts for most of it. So 2 3m 3 is a pretty good lower bound for the number of arithmetic steps. From this, we can al- ready conclude that Gauss elimination will be very slow for m = 106 (1 million) on a normal computer, even if that computer can perform an astonishing number of 10 12 (1 trillion) arithmetic steps per second. Indeed, for m = 10 6, Gauss elimination needs at least 2 3m 3 = 2 310 18 arithmetic steps, and these alone will take at least 2 310 6 seconds which is roughly a week. And if the system has m = 107 (10 million), the runtime goes up by factor of 1000, resulting in at least 20 years of computing. In practice, systems with m = 10 6 and even larger are common and can be solved much faster by algorithms exploiting that typical constraint matrices A are sparse, mean- ing that most of their entries are 0. In such cases, we can use algorithms that only deal with the few nonzero entries, resulting in significantly less (arithmetic) steps. 3.3 Inverse matrices Based on understanding success and failure of Gauss elimination, we look at the im- portant class of invertible matrices, the square matrices that lead to an “undoable” linear transformation x → Ax. It turns out that these are exactly the ones with lin- early independent columns, and therefore exactly the ones for which Gauss elimina- tion works and computes the unique solution of Ax = b. 85 In the proof of Lemma 3.3 (about the effect of a row operation during Gauss elimina- tion), we have used the “inverse” row operation with matrix M ′, undoing a previous row operation with matrix M : if M A = A′, then M ′A ′ = A. We can also write this as M ′M A = A, and as this works for every m × n matrix A, we can set A = I, the identity matrix, to get (using Corollary 2.20 in the first equality) that M ′M = M ′M I = I. Hence, if we premultiply the matrix of the row operation with the matrix of the inverse row operation, we get the identity matrix. For example,   1 0 0 2 1 0 0 0 1   ︸ ︷︷ ︸ add 2·(row 1) to (row 2)   1 0 0 −2 1 0 0 0 1   ︸ ︷︷ ︸ subtract 2·(row 1) from (row 2) =   1 0 0 0 1 0 0 0 1   ︸ ︷︷ ︸ I . This is not surprising if you think of the row subtraction and row addition as linear trans- formations TM and TM ′ whose effects cancel out (yield the identity transformation) when you apply them one after the other: TM ′(TM (x)) = TI(x) = x; see Section 2.3.4. Here, we want to investigate this for general matrices M , not only the ones corre- sponding to row operations. 3.3.1 Definition and basic properties Definition 3.7 (Invertible matrix). Let M be an m × m matrix. M is called invertible if there exists an m × m matrix M −1 (called the inverse of M ) such that M M −1 = M −1M = I. For a row operation matrix M , the matrix M ′ of the inverse row operation is indeed the inverse matrix according to this definition. We have already argued that M ′M = I, but M M ′ = I also holds, since the original row operation is the inverse of the inverse row operation. It is generally true for two m × m matrices A and B that AB = I implies BA = I, so the first equality in Definition 3.7 is actually superfluous. However, we will only prove this in Exercise 3.12 below, so for the time being, we work with the “fool- proof” Definition 3.7 of the inverse. Let us look at the situation for m = 1, 2. For m = 1, an m × m matrix is just a number (between square brackets), the identity matrix is the number 1, and the inverse is the reciprocal number: 86 Case 1 × 1. M = [a ] ⇒ M −1 = [ 1 a] (if a ̸= 0). Hence, the inverse exists unless a = 0. Case 2 × 2. M = [a b c d ] , ⇒ M −1 = 1 ad − bc [ d −b −c a ] (if ad − bc ̸= 0). To see that this formula for M −1 is correct, we simply check that it satisfies Definition 3.7. Again, we see that the inverse does not always exist, but the condition here (ad − bc ̸= 0) is less obvious than for m = 1. It is not hard to see (try to see this!) that this condition— expressed in words—reads as follows: The two columns of M are linearly independent. Could there also be a different inverse? No, if a matrix has an inverse, it is unique: Lemma 3.8. Let M be an m × m matrix with two inverses A and B. Then A = B. Proof. Using associativity of matrix multiplication (Lemma 2.22) as well as Corollary 2.20, we compute A = IA = (BM )A = B(M A) = BI = B. For m×m matrices with m > 2, there is also a formula for the inverse, but this requires the concept of determinants to which we will only get in the second part of the lecture. The next lemma lets us compute the inverse of a product of two invertible matrices. Lemma 3.9. Let A and B be invertible m × m matrices. Then AB is also invertible, and (AB) −1 = B−1A−1. Recall that AB is the matrix of the linear transformation TAB(x) = TA(TB(x)) (first apply TB, then TA); see Lemma 2.30. To undo this, we need to apply the inverse transfor- mations in reverse order: TB−1(TA−1(x)) = TB−1A−1(x) (first undo TA, then undo TB). You can already consider this a proof sketch of the lemma, but there is also a more direct proof not involving linear transformations. Proof. (AB)(B−1A −1) = A(BB−1)A −1 = AIA −1 = AA−1 = I and (B−1A −1)(AB) = B−1(A −1A)B = B−1IB = B−1B = I. This naturally extends to more matrices, for example (ABC) −1 = C −1B−1A −1, but we skip the formal statement and its proof. Inversion also commutes with transposition (Definition 2.11). 87 Lemma 3.10. Let A be an invertible m × m matrix. Then the transpose matrix A ⊤ is also invert- ible, and ( A⊤)−1 = (A −1)⊤ . Proof. We need to check that A⊤ ( A −1)⊤ ︸ ︷︷ ︸ (A−1A)⊤ = ( A−1)⊤ A ⊤ ︸ ︷︷ ︸ (AA−1)⊤ = I, and this is true since we can invoke Lemma 2.19 to pull the transpositions out (below the curly braces) and then use that A and A−1 are inverse to each other, alongside with I ⊤ = I. 3.3.2 The Inverse Theorem We have already made the point that a 2 × 2 matrix is invertible if and only if its columns are linearly independent. This actually holds for m × m matrices in general. Even for m = 1, this makes sense. Saying that the columns of a 1 × 1 matrix [ a] are linearly independent is a fancy way of saying that a ̸= 0. The following theorem can be thought of as characterizing “good” matrices (the ones for which Gauss elimination succeeds) in three different ways, connecting three impor- tant concepts. We already know from Theorem 3.5 that success of Gauss elimination is equivalent to statement (iii). Theorem 3.11 (Inverse Theorem). Let A be an m × m matrix. The following statements are equivalent. (i) A is invertible. (ii) For every b ∈ R m, Ax = b has a unique solution x. (iii) The columns of A are linearly independent. Proof. We establish equivalence through the following implications: (i) ⇒ (ii) ⇑ ⇓ (ii) ⇐ (iii) (i) ⇒ (ii): if A is invertible, the natural candidate for the unique solution of Ax = b is x = A−1b. This is the m-dimensional analog of solving ax = b via x = b/a. Indeed, this works. To show that A−1b solves Ax = b, we compute A(A −1b) = (AA−1)b = Ib = b. 88 To prove uniqueness, take any solution x satisfying Ax = b. Multiplying by A −1 from both sides gives x = A−1Ax = A −1b. So there is no solution other than A −1b. (ii) ⇒ (iii): if Ax = b has a unique solution for every b, this in particular holds for b = 0. For this choice of b, (ii) is saying that the columns of A are linearly independent, see Observation 3.2; so we have deduced (iii). (iii) ⇒ (ii): If the columns of A are linearly independent, Gauss elimination succeeds by Theorem 3.5, producing an equivalent system U x = c with the upper triangular matrix U having nonzero diagonal entries. Back substitution as in Section 3.2.1 shows that there is a unique solution of U x = c and hence also of Ax = b. (ii) ⇒ (i): If Ax = b has a unique solution for all b, we find vectors v1, v2, . . . , vm ∈ R m such that Av1 =      1 0 ... 0      ︸︷︷︸ e1 , Av2 =      0 1 ... 0      ︸︷︷︸ e2 , . . . , Avm =      0 0 ... 1      ︸︷︷︸ em ⇒ A   | | | v1 v2 · · · vm | | |   ︸ ︷︷ ︸ B =      1 0 · · · 0 0 1 · · · 0 ... ... . . . ... 0 0 · · · 1      ︸ ︷︷ ︸ I . So AB = I, and B is the obvious candidate for the inverse of A. We still need to show that BA = I to conclude that B = A−1 and that A is invertible according to Definition 3.7. For this, we first compute AI = IA = (AB)A = A(BA), so A(I − BA) = 0. Here, on top of associativity, we also use distributivity of matrix multiplication; see Lemma 2.22. Let the columns of I − BA be w1, w2, . . . , wm. Then A(I − BA) = 0 reads as Awj = 0 for all j. The columns of A are linearly independent by (ii) ⇒ (iii). Hence wj = 0 for all j, since 0 is the only solution of Ax = 0, see Observation 3.2. So all columns of I − BA are 0, meaning that BA = I. In the previous proof, we have seen that AB = I implies BA = I if the columns of A are linearly independent. The next exercise shows that we do not need to require this condition. As you may realize in solving this exercise, AB = I already implies that the columns of A are linearly independent. Exercise 3.12. Let A and B be two m×m matrices such that AB = I. Then we also have BA = I and therefore A−1 = B and B−1 = A by Definition 3.7. 89 3.4 LU and LUP decomposition Here, we introduce two important decompositions of a square matrix with linearly independent columns, or of some version of it with exchanged rows: A = LU if Gauss elimination succeeds without row exchanges, and P A = LU if row exchanges are needed. Here, L is a lower triangular matrix, U is an upper triangular matrix, and P is a permutation matrix. This section can also be considered as a formal correctness proof of Gauss elimination. Recall that Gauss elimination is trying to transform an m × m matrix A into an upper triangular matrix U , via row operations; see Section 3.2.2. If this succeeds without row exchanges, we will be able to says precisely how A and U relate to each other. This will lead us to the LU decomposition. But even if there are row exchanges (the less nice case), or Gauss elimination fails altogether (the ugly case), we can still get something a bit weaker, namely an LUP decomposition. Once we have an LU or LUP decomposition of A, we can solve Ax = b in time O(m2), for any given right-hand side b. This is much faster then Gauss elimination which we have seen needs O(m 3) time; see Section 3.2.4. We still need O(m 3) time to compute the LU or LUP decomposition, but if we need to solve Ax = b for many different b, this initial effort pays off quickly. 3.4.1 LU decomposition Let’s assume that Gauss elimination on Ax = b succeeds without row exchanges, trans- forming Ax = b into U x = c with the same solutions, where U is an upper triangular matrix. More concretely, we have obtained U as a product of some elimination matrices and A; in the example of Section 3.2.2, this product looks as follows:   1 0 0 0 1 0 0 −1 1   ︸ ︷︷ ︸ subtract 1·(row 2) from (row 3)   1 0 0 0 1 0 −1 0 1   ︸ ︷︷ ︸ subtract 1·(row 1) from (row 3)   1 0 0 −2 1 0 0 0 1   ︸ ︷︷ ︸ subtract 2·(row 1) from (row 2)   2 3 4 4 11 14 2 8 17   ︸ ︷︷ ︸ A =   2 3 4 0 5 6 0 0 7   ︸ ︷︷ ︸ U . For another 3×3 matrix A, the multiples that we subtract will be different, but the general pattern is the same:   1 0 0 0 1 0 0 −c32 1   ︸ ︷︷ ︸ subtract c32·(row 2) from (row 3)   1 0 0 0 1 0 −c31 0 1   ︸ ︷︷ ︸ subtract c31·(row 1) from (row 3)   1 0 0 −c21 1 0 0 0 1   ︸ ︷︷ ︸ subtract c21·(row 1) from (row 2) A = U. 90 Multiplying the three elimination matrices with each other yields   1 0 0 −c21 1 0 c32c21 − c31 −c32 1   A = U. (3.4) The “complicated” matrix on the left is the one resulting from applying the second and the third elimination step to the elimination matrix of the first step (and this is probably the easiest way to compute the matrix). But here comes the magic: The inverse of the complicated matrix is a very simple matrix:   1 0 0 −c21 1 0 c32c21 − c31 −c32 1   −1 =   1 0 0 c21 1 0 c31 c32 1   . To verify this, you can multiply the two matrices and check that the result is indeed the identity matrix. Hence, multiplying both sides of (3.4) with the simple inverse yields A =   1 0 0 c21 1 0 c31 c32 1   ︸ ︷︷ ︸ L U. This means, we have obtained a decomposition of A in the form A = LU . The matrix U is the upper triangular one resulting from Gauss elimination, and L is a lower triangular matrix with 1’s on the diagonal; below the diagonal, L records the multiples that we have used for the row subtractions throughout the elimination steps. The question is whether this always works; maybe it does for m = 3 but not in general. To show that it works in general, we need a theorem with a proof. If we are only interested in A, we can simply remove lines 12 and 22 (affecting b) from the elimination procedure in Table 3.6. This gives us a version of Gauss elimination that transforms only A instead of A and b, and this is the one we refer to in the next theorem. Theorem 3.13 (LU decomposition). Let A be an m × m matrix on which Gauss elimination as in Table 3.6 succeeds without row exchanges, resulting in an upper triangular matrix U . Let cij (computed in Line 18) be the multiple of row j that we subtract from row i > j when we eliminate in column j. Then A = LU where L =      1 c21 1 ... . . . cm1 · · · cm,m−1 1      . More formally, L = [ℓij] m m i=1,j=1 where ℓij =    0 if i < j 1 if i = j cij if i > j . 91 Proof. We look at a fixed row i. Whenever we change row i during Gauss elimination, we subtract cij · (row j) from it, for some previous row j. At this point, row j has already been “finalized”, meaning that it is the j-th row of our resulting matrix U . Pictorially, the situation is this, with ujj ̸= 0 being the current pivot: u11 · · · ← finalized (in U ) 0 u22 · · · ← finalized (in U ) 0 0 . . . ... row j 0 0 · · · ujj · · · ujm ← finalized (in U ) ... row i 0 0 · · · ⋆ij · · · ⋆im ← now subtract cij · (row j) So if we track what happens to row i (initially in A), we see multiples of finalized rows 1, 2, . . . , i − 1 being subtracted, after which we end up with row i in U : (row i) in A initially − ci1 · (row 1) in U step 1 − ci2 · (row 2) in U step 2 ... − ci,i−1 · (row i − 1) in U step i − 1 = (row i) in U finalized. If we solve this equation for the first term, (row i) in A, we see that this row is a linear combination of the first i rows of U . In matrix notation, the linear combination can be written as follows: (row i) of A = [ ci1 ci2 · · · ci,i−1 1 0 · · · 0 ] ︸ ︷︷ ︸ row vector U. Doing this for all rows of A and combining the m row vector equations into one matrix equation gives the desired result: A =      1 c21 1 ... . . . cm1 · · · cm,m−1 1      ︸ ︷︷ ︸ L U. In other words, Gauss elimination (without row exchanges) actually computes an LU decomposition of A while transforming A to U . This all happens in time O(m3), see The- orem 3.6. 92 Solving Ax = b from A = LU . Once we have an LU decomposition of A, we can use it to solve Ax = b for any given right-hand side b in O(m 2) time which is much faster than Gauss elimination. This is very useful if we need to solve many systems with the same matrix A. To see this, we write Ax = b as L U x︸︷︷︸ y = b. We first solve Ly = b for y. Since L is lower triangular, we can do this using forward substitution. This works exactly like back substitution (Section 3.2.1), except that it finds the solution in forward order y1, y2, . . ., because L is lower instead of upper triangular. Since L has 1’s on the diagonal, this succeeds, and we don’t even need divisions. Once we have y, we now solve U x = y using back substitution to obtain x. Forward and back substitution only require (m2) arithmetic steps each and therefore run in O(m 2) time; see Section 3.2.4. One way to think about solving Ly = b is that we simply replay the row subtractions that Gauss elimination previously did on A, except that we now do them on b as well (putting back lines 12 and 22), leading to y = c in the end. Beyond LU. What happens if Gauss elimination succeeds but needs some row exchanges on the way? The replay approach still works and solves Ax = b in time O(m 2) if we re- member all row subtractions and row exchanges that happened on the way. With A = LU , the matrix L was our “memory”, but if there are row exchanges, this no longer works. The proof of Theorem 3.13 breaks down, since it can only deal with elimination matri- ces, but not with permutation matrices in between them. There is in general no way to fix this: there are matrices for which Gauss elimination succeeds (with row exchanges), but there is no LU decomposition. As an example, consider a matrix of the the form A = [ 0 1 1 a ] , where a is an arbitrary number. Gauss elimination is done after one row exchange. Let’s try to write A in the form A = LU where L is lower triangular and U is upper triangular: [0 1 1 a ] ︸ ︷︷ ︸ A = [ℓ11 0 ℓ21 ℓ22 ] ︸ ︷︷ ︸ L [u11 u12 0 u22 ] ︸ ︷︷ ︸ U . These are 4 equations, one for each entry of A, and we consider three of them: 0 = ℓ11u11, 1 = ℓ11u12, 1 = ℓ21u11. The equation 0 = ℓ11u11 can only hold if at least one of ℓ11 and u11 is 0, but then at least one of the two other equations fails. So there is no way to get A = LU in this case. 93 But we can always get an LU decomposition after exchanging some rows of A first, and this will give us the LUP decomposition P A = LU, where P is a permutation matrix. This is the “memory” of all that happened when there were also row exchanges. The permutation matrix P will record the row exchanges, and L records the row subtractions. Before we can formally derive the LUP decomposition, we need to introduce permu- tation matrices. 3.4.2 Permutations and permutation matrices Previously, we have only used permutation matrices that exchange two rows, but now we need to deal with general permutation matrices. We start with permutations. Definition 3.14 (Permutation). A permutation of [m] = {1, 2, . . . , m} is a bijective function π : [m] → [m]. Bijective means that the function values cover all of [m]: no value is missing, and no value appears twice. Here is an example for m = 5. i 1 2 3 4 5 π(i) 2 3 5 4 1 (3.5) Hence, a permutation can be thought of as reordering the sequence 1, 2, . . . , m into π(1), π(2), . . . , π(m). In the example, the reordered sequence is 2, 3, 5, 4, 1. There are m! = 1 · 2 · · · m permutations of [m]. To see this, we can for example argue as follows: there are m ways of putting 1 into the sequence π(1), π(2), . . . , π(m). In the previous example, 1 was put last, but we could have put it at any of the 5 positions in the lower row of the table. For each of the m ways of putting 1, there are m − 1 ways of putting 2, so there are m(m − 1) ways of putting 1 and 2. For each of these ways, there are m − 2 ways of putting 3, and so on. In the end, the number of ways of putting all m numbers is m!. If π, π′ are two permutations of [m], then their composition π′ ◦ π is again a permutation. The composition is defined as (π′ ◦ π)(i) = π′(π(i)), the function that first applies π and then π′. To show that this is a permutation, we need to check the bijection: for every k, there is exactly one i with k = π′(π(i)). Indeed, there is exactly one j with k = π′(j), so we must have π(i) = j; again, there is exactly one i doing that. 94 Definition 3.15 (Permutation matrix). Let π : [m] → [m] be a permutation. The permutation matrix associated with π is the m × m matrix P = [pij] m m i=1,j=1 with pij = { 1 if j = π(i) 0 otherwise . Vice versa, we also say that π is associated with P . For example, P =       0 1 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 0 0       is the permutation matrix associated with π in (3.5). The row/column pairs (i, j) where we find 1’s are (1, 2), (2, 3), (3, 5), (4, 4), (5, 1)—exactly the pairs (i, j) where j = π(i). Since a permutation is bijective, a permutation matrix has a single 1 in each row and column. The 1 in row i is in column π(i), and the 1 in column j is in the unique row i such that π(i) = j. Vice versa, every matrix with a single 1 in each row and column is a permutation matrix. The associated permutation is given by the positions of the 1’s in each row. In this sense, a permutation matrix can be considered as the graph of its associated permutation, with inputs on the “row axis” and outputs on the “column axis”, se Fig- ure 3.2. iπ(i)1234512345        0 1 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 0 0        Figure 3.2: A permutation matrix can be interpreted as the graph of its associated permu- tation: The 1 in row i indicates the function value π(i), visualized with a black dot in the graph. The linear transformation TP of a permutation matrix P (Definition 2.25) reorders the 95 input vector according to the associated permutation. For example,       0 1 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 0 0       ︸ ︷︷ ︸ P       x1 x2 x3 x4 x5       ︸ ︷︷ ︸ x =       x2 x3 x5 x4 x1       ︸ ︷︷ ︸ TP (x) . It follows that if P is an m × m permutation matrix and A any m × n matrix, then P A is the matrix arising from A by reordering the rows according to the associated permutation. In summary, a permutation matrix is the “linear algebra way” of looking at a permu- tation, but it contains the same information. In particular, there are also m! permutation matrices of size m×m. The identity matrix I is a special permutation matrix; its associated permutation is the identity permutation with π(i) = i for all i. What about P −1, the inverse of the permutation matrix P associated with π? If it exists, then applying P −1 to P x (the input vector after reordering) must give P −1P x = Ix = x (the input vector before reordering), so P −1 is undoing the reordering. Such a matrix P −1 therefore indeed exists. It is again a permutation matrix, associated with π−1, the inverse of π: If π(i) = j, then π−1(j) = i. Table 3.7 shows how this looks like for the example (3.5): i 1 2 3 4 5 π(i) 2 3 5 4 1 j 1 2 3 4 5 π−1(j) 5 1 2 4 3 P =       0 1 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 0 0       P −1 =       0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 1 0 0       Table 3.7: The inverse permutation matrix corresponds to the inverse permutation. While π(5) = 1 (last column of π’s value table), we have π−1(1) = 5 (first column of π−1’s value table). In this example, P −1 is the transpose of P , and this is true in general. Lemma 3.16. Let P be a permutation matrix. Then P −1 = P ⊤. Proof. We check that P P ⊤ = I (P ⊤P = I follows by a similar argument, or directly from Exercise 3.12). What we need to check for this is (i-th row of P ) · (j-th column of P ⊤) = δij = { 1, if i = j 0, otherwise . 96 By Definition 2.11 of the transpose, the j-th column of P ⊤ is the j-th row of P , hence, we need to check that (i-th row of P ) · (j-th row of P ) = δij. This is now easy: row i of P is the standard unit vector eπ(i), where π is the associated permutation. Thus, eπ(i) · eπ(i) = 1 for all i. For i ̸= j, eπ(i) · eπ(j) = 0 since π(i) ̸= π(j) (π is a bijection). Whenever we have a square matrix M =   | | | v1 v2 · · · vm | | |   such that vi · vj = δij, we can argue in the same way that M −1 = M ⊤. Such matrices are called orthogonal matrices and will play an important role in the second part of the course. Here, we have seen that permutation matrices form a (simple) class of orthogonal matrices. We conclude the treatment of permutation matrices with another simple but important lemma. Lemma 3.17. Let P, P ′ be m × m permutation matrices with associated permutations π, π′. Then P P ′ is a permutation matrix as well, associated with the permutation π′ ◦ π. Before we prove this, let us clear a potential confusion: You may have expected the associated permutation to be π ◦ π′, and this would indeed be the result if we had de- cided to read a permutation matrix column-wise instead of row-wise, in order to obtain its associated permutation. But this decision would have had other counter-intuitive effects. At the bottom of this issue is the following: there are two natural ways of describing an ordering of [m] by a permutation π. Let’s look at the ordering 2, 3, 5, 4, 1 that we have considered before. We have chosen π such that the i-th element in the order- ing is π(i). This gives i 1 2 3 4 5 π(i) 2 3 5 4 1 . Alternatively, we could have chosen π such that element i occurs at position π(i) in the ordering. This gives i 1 2 3 4 5 π(i) 5 1 2 4 3 , exactly the inverse of the previous permutation. 97 The upshot is that if someone just says “the permutation 2, 3, 5, 4, 1”, then we should clarify what they mean. In any case, the important consequence of Lemma 3.17 is that the product of two per- mutation matrices is again a permutation matrix. Let’s prove it. Proof. We have P P ′      x1 x2 ... xm      = P      xπ′(1) xπ′(2) ... xπ′(m)      ︸ ︷︷ ︸ y = P      y1 y2 ... ym      =      yπ(1) yπ(2) ... yπ(m)      =      xπ′(π(1)) xπ′(π(2)) ... xπ′(π(m))      =      x(π′◦π)(1) x(π′◦π)(2) ... x(π′◦π)(m)      . In the second-to-last equality, we use the definition of y: yi = xπ′(i) for all i. 3.4.3 LUP decomposition Now we are prepared to establish the “memory” of Gauss elimination in the general case. Theorem 3.18 (LUP decomposition). Let A be an m × m matrix with linearly independent columns, m ≥ 1. There exist three m × m matrices P, L, U such that P A = LU, where P is a permutation matrix, L a lower triangular matrix with 1’s on the diagonal, and U an upper triangular matrix with nonzero diagonal entries. The idea is the following: running Gauss elimination yields U = Em−1Pm−1Em−2Pm−2 · · · E1P1A, where Pj is the permutation matrix for the row exchange in column j (Pj = I if there is no row exchange), and Ej is the product of all elimination matrices used to produce the zeros below the diagonal in column j. What if we move all row exchanges to the very beginning and then do Gauss elimi- nation on the matrix P A where P = Pm−1Pm−2 · · · P1, the permutation matrix that sum- marizes all the row exchanges? Our hope is that this now works without any further row exchanges, in which case Theorem 3.13 gives us an LU decomposition of P A. Now we rigorously show that our hope is reality. The following proof is from the text- book Introduction to Algorithms by Cormen, Leiserson, Rivest, and Stein [CLRS22, Section 28.1]. Their proof works for any square matrix A, resulting in a matrix U that may have some 0’s on the diagonal. To stay consistent with our chapter’s philosophy, we present the proof only for the case where A is invertible and Gauss elimination succeeds; this is the case covered by Theorem 3.18. 98 Proof. We proceed by induction on m. For the base case m = 1, we have A = [ a] for some number a ̸= 0, so we can choose P = L = [ 1 ] (the 1 × 1 identity matrix) and U = A to satisfy P A = LU as required. If m > 1, we start with a row exchange to transform A into P1A =     a|u|| v | B     , with a ̸= 0. P1 is the permutation matrix that does the row exchange (we have P1 = I if no row exchange is necessary). We know that such a row exchange is possible: Gauss elimination succeeds whenever A has linearly independent columns (Theorem 3.5). Next, we apply elimination in the first column to turn v into 0, via     1|0|| − 1 a v | I     ︸ ︷︷ ︸ E1 P1A =     a|u|| 0 | A ′     . (3.6) The matrix E1 is collecting all m − 1 elimination matrices we need for that in one matrix. The effect of premultiplication with E1 is that ( 1 avi) · (row 1) is subtracted from row i + 1, for i = 1, . . . , m − 1. As needed, this cancels all the vi’s below a. Since A ′ is an (m − 1) × (m − 1) matrix with linearly independent columns (this follows from Corollary 3.4; try to do the argument!), we can now use the induction hypothesis to find (m − 1) × (m − 1) matrices P ′, L ′, U ′ such that P ′A ′ = L ′U ′, where P ′ is a permutation matrix, L′ a lower triangular matrix with 1’s on the diagonal, and U ′ an upper triangular matrix with nonzero diagonal entries. Applying the extended permutation matrix P ′ + =     1|0|| 0 | P ′     to both sides of (3.6), we therefore get P ′ +E1P1A =     1|0|| 0 | P ′         a|u|| 0 | A ′     =     a|u|| 0 | P ′A ′     =     a|u|| 0 | L ′U ′     . (3.7) So far, nothing much has actually happened (but verify the first equality for yourself!). One way we can think about it is this: The induction hypothesis has already moved 99 the row exchanges in steps 2, . . . , m − 1 of Gauss elimination to the beginning of step 2. Since we simply assume the induction hypothesis, this has happened “for free” and transformed U = Em−1Pm−1Em−2Pm−2 · · · E1P1A into U = E′ m−1E′ m−2 · · · E′ 2 Pm−1Pm−2 · · · P2︸ ︷︷ ︸ P ′ + E1P1A. There is just one last step missing, but that one we have to do ourselves: get E1 past P ′ +. Claim: With w = − 1 a v and w′ = P ′(− 1 av) the result of applying P ′ to w, we have     1|0|| 0 | P ′     ︸ ︷︷ ︸ P ′ +     1|0|| −w | I     ︸ ︷︷ ︸ E1 =     1|0|| −w′ | I     ︸ ︷︷ ︸ E′ 1     1|0|| 0 | P ′     ︸ ︷︷ ︸ P ′ + . A claim is a “sub-lemma” within a proof that doesn’t deserve full lemma status, since it is needed only locally. A claim is also proved locally. To do this here, we compute both products. P ′ +E1 results from E1 by permuting the rows, leaving the first row in place and permuting the other ones using P ′. This yields P ′ +E1 =     1|0|| −w′ | P ′     . E′ 1P ′ +, on the other hand, results from P ′ + by subtracting w′ i · (row 1) from row i + 1, for i = 1, 2, . . . , m−1. As row 1 is [ 1|0|], this only affects the first column and replaces 0 by −w′ there, hence E′ 1P ′ + =     1|0|| −w′ | P ′     as well. Applying the claim to (3.7) gives E′ 1 P ′ +P1 ︸ ︷︷ ︸ P A =     a|u|| 0 | L′U ′.     . (3.8) 100 We have already found the desired permutation matrix P = P ′ +P1, using that a product of two permutation matrices is a permutation matrix (Lemma 3.17). It remains to find L and U . For this, we move E′ 1 to the other side, by multiplying both sides with (E′ 1) −1 =     1|0|| w′ | I     . This formula for the inverse is not surprising: to undo subtractions of multiples of the first row from the rows below, we simply add the same multiples back. We can also easily verify that the right-hand side of (3.8) can be decomposed as     a|u|| 0 | L′U ′.     =     1|0|| 0 | L′         a|u|| 0 | U ′     . After this, (3.8) becomes P A = (E′ 1) −1     1|0|| 0 | L′     ︸ ︷︷ ︸ computation as for E′ 1P ′ + in Claim     a|u|| 0 | U ′     =     1|0|| w′ | L′     ︸ ︷︷ ︸ L     a|u|| 0 | U ′     ︸ ︷︷ ︸ U . Hence we have also found L and U as desired. Admittedly, this proof was somewhat technical. But the alternative (believing it to be obvious that all row exchanges can be moved to the beginning) is not convincing. Solving Ax = b from P A = LU . This is very similar to what we did in Section 3.4.1 from A = LU , except that it now works for every matrix A on which Gauss elimination succeeds. We multiply with P −1 = P ⊤ (see Lemma 3.17 for this formula) to obtain A = P ⊤LU , hence we can write Ax = b as P ⊤ L U x︸︷︷︸ y ︸ ︷︷ ︸ z = b. We first solve P ⊤z = b for z. In fact, there’s nothing to solve: z = P b, so we permute b in the same way we have permuted the rows of A. After this, we proceed as before: we solve Ly = z for y using forward substitution and then solve U x = y for x, with back substitution. All this can be done in O(m 2) time, so again much faster than solving Ax = b from scratch using Gauss elimination. 101 3.5 Gauss-Jordan elimination In this section, we present an algorithm for solving any system Ax = b (or detecting that there is no solution). The algorithm is very similar in spirit to Gauss elimina- tion. The most important theoretical contribution of Gauss-Jordan elimination is a reduction of A to a unique standard form from which we can easily read off many properties of A that are difficult to see directly. On top of providing an efficient way of solving Ax = b, this standard form also yields the CR decomposition of A. So far, we have seen how to solve systems Ax = b where A is a square matrix with linearly independent columns. This is a very nice case in the sense that there is always a unique solution x that can be found with Gauss elimination; see Theorem 3.5 and the Inverse Theorem 3.11. We will now see an algorithm for the general case (A may be non-square and/or may have linearly dependent columns). This is a simple extension of Gauss elimination. 3.5.1 (Reduced) row echelon form In Gauss elimination, we are trying to transform a square matrix A into an upper trian- gular matrix U with nonzero diagonal entries, using row operations repeatedly. This fails if A has linearly dependent columns. Now, we want to define a “standard form” into which we can transform every matrix, even if it has linearly dependent columns, or is not a square matrix. This standard form is the row echelon form (REF). Figure 3.3 gives an example of a matrix in REF. This will make it easier to understand the subsequent definition. 1111000000j1j2j3j4123456 Figure 3.3: A 6 × 10 matrix in row echelon form REF(2, 3, 6, 8). White entries are 0, unla- beled gray entries can be any numbers. Removing the last two rows yields a 4 × 10 matrix in RREF(2, 3, 6, 8). Definition 3.19 (Row echelon and reduced row echelon form). Let R = [rij] m n i=1,j=1 be an m × n matrix. R is in row echelon form (REF) if the following holds: There exist r ≤ m column indices 1 ≤ j1 < j2 < · · · < jr ≤ n such that the following two statements hold: (i) For i = 1, 2, . . . , r, we have riji = 1. 102 (ii) For all i, j, we have rij = 0 whenever i > r or j < ji or j = jk for some k > i. If r = m, R is in reduced row echelon form (RREF). If we want to describe the shape of R precisely, we say that R is in REF(j1, j2, . . . , jr) or RREF(j1, j2, . . . , jm). It’s easiest to understand REF by looking at the matrix row by row. Row i only has 0s if i > r. This case happens for rows 5 and 6 in Figure 3.3. Otherwise, row i starts with ji − 1 0s. In column ji, we then have a 1 (first gray entry in the row). After this, we can have any entries, unless we are in some column jk: there we again need to have a 0. You can check that the matrix in Figure 3.3 has this behavior for all rows. As a consequence, column ji equals ei, the i-th standard unit vector. The shape of a matrix in REF resembles that of an upper triangular matrix in the sense that there are prescribed 0s to the lower left (the white area). Removing rows 5 and 6 in Figure 3.3 yields a matrix in RREF. Generally, RREF does not allow any zero rows. The m × m identity matrix I is in RREF(1, 2, . . . , m), while the m × n zero matrix is in REF(), meaning that r = 0. Matrices in REF and RREF are very nice in the sense that we can read off many prop- erties easily. Here is a first example. Observation 3.20. A matrix R in REF(j1, j2, . . . , jr) has rank r. Proof. Recall that the rank of a matrix is the number of independent columns, the ones that are not linear combinations of previous columns (Definition 2.9). In R, the indepen- dent columns are precisely the ones with indices j1, j2, . . . , jr. Indeed, in each of these columns, R makes a “downward step” (has a nonzero entry where all previous columns have zeros). Such a downward step column is therefore not a linear combination of the previous columns. But any other column is immediately seen to be a linear combination of columns j1, j2, . . . , jr, since these are simply the first r standard unit vectors; their linear combinations are all vectors in Rm with 0-entries at coordinates r + 1, r + 2, . . . , m. 3.5.2 Direct solution It is easy to solve Ax = b whenever A is in REF. Much easier even than back substitution for an upper triangular matrix. Suppose that A is an m × n matrix in REF(j1, j2, . . . , jr) There are two cases: if bi ̸= 0 for some i > r, there is no solution. This is because the i-th row of A is zero for all i > r, so the i-th entry of Ax is zero for i > r, no matter what x is. If bi = 0 for all i > r, there is a solution: we define x by xj = { bi, if j = ji 0, otherwise. See Figure 3.4 for an illustration. 103 1111000000j1j2j3j4123456b1b2b3b400000000b1b2b3b4=Axb←←if ̸= 0 here, no solution Figure 3.4: Direct solution of Ax = b when A is in RREF . This works: since the ji-th column of A is ei, this yields Ax = r∑ i=1 biei = m∑ i=1 biei = b. For the middle equality, we use that br+1, br+2, . . . , bm = 0. The vector x just constructed may not be the only solution, but it is the canonical one. 3.5.3 Elimination Here, we proceed similar to Gauss elimination and use row operations to transform any system Ax = b into an equivalent system R0x = c where R0 is in REF. After this, we can use direct solution (see previous section) to either report that the system is unsolvable, or to compute the canonical solution. The resulting matrix is called R0, since it may have zero rows at the end. We reserve the name R for the matrix obtained after removing the zero row. For Gauss elimination, we have first shown examples and then computer code. Fi- nally, Theorem 3.18 can be considered as the “official” correctness proof of Gauss elim- ination. Here, we again start with an example but skip the computer code. It is very similar to the one for Gauss elimination but more lengthy. After the example, we there- fore proceed directly to the correctness proof. The good thing is that this one is simpler than the one for Gauss elimination and at the same time closer to the actual algorithm. We only show how elimination transforms A into R0. To transform b into c, we apply the same row operations. One elegant way of doing this is to directly work with the extended matrix [A|b] that has b as an extra column. But even after transforming only A (and computing the product of all row operation matrices on the way), we will be able to efficiently solve Ax = b for every b; see Theorem 3.23 below. This is similar to the case of Gauss elimination where the resulting LU or LUP decomposition of A provides a way of solving Ax = b for every b; see Sections 3.4.1 and 3.4.3. 104 As in Gauss elimination, we proceed column by column. The nonzero pivots that we find will determine the “downward step” columns j1, j2, . . . of the resulting matrix in REF. We maintain a number r, initially equal to 0, counting how many downwards steps we have already made. The next downward step will then be in some upcoming column of row r + 1. Our example matrix is the 3 × 5 matrix A =   2 4 2 2 −2 6 12 6 7 1 4 8 2 2 6   . (3.9) In column 1, we find a nonzero pivot in row 1 that we will use for elimination in this column. But in contrast to Gauss elimination, we first apply a row division in order to make the pivot equal to 1, and only after that eliminate the nonzero entries in column 1 (which then requires no further divisions): A =  2 4 2 2 −2 6 12 6 7 1 4 8 2 2 6   (r = 0) divide (row 1) by 2: ↓ 1 2 1 1 −1 6 12 6 7 1 4 8 2 2 6   subtract 6·(row 1) from (row 2): ↓ 1 2 1 1 −1 0 0 0 1 7 4 8 2 2 6   subtract 4·(row 1) from (row 3): ↓  1 2 1 1 −1 0 0 0 1 7 0 0 −2 −2 10   As we have made a downward step, we increase r and move on to column 2:   1 2 1 1 −1 0 0 0 1 7 0 0 −2 −2 10   (r = 1) In Gauss elimination we would now be in the ugly case where no row exchange can bring a nonzero entry into the pivot position. However, this is a good case now. There is simply no downward step in column 2, and we directly move on to column 3. In this column, it is possible to make a row exchange to get a nonzero pivot: 105   1 2 1 1 −1 0 0 0 1 7 0 0 −2 −2 10   (r = 1) exchange (row 2) and (row 3): ↓  1 2 1 1 −1 0 0 −2 −2 10 0 0 0 1 7   divide (row 2) by −2: ↓  1 2 1 1 −1 0 0 1 1 −5 0 0 0 1 7   Now we eliminate in column 3, but unlike in Gauss elimination, we now also eliminate above the pivot, since REF requires that the pivot columns become standard unit vectors.   1 2 1 1 −1 0 0 1 1 −5 0 0 0 1 7   subtract 1·(row 2) from (row 1): ↓  1 2 0 0 4 0 0 1 1 −5 0 0 0 1 7   No elimination below the pivot is necessary here, so our downward step is done, we increase r and move to column 4. We already have a pivot of 1 in the desired position, so we need no row exchange and no row division. All that is left to do in this downward step is one row subtraction:   1 2 0 0 4 0 0 1 1 −5 0 0 0 1 7   (r = 2) subtract 1·(row 3) from (row 2): ↓ R0 =   1 2 0 0 4 0 0 1 0 −12 0 0 0 1 7   After increasing r, we now have r = 3 (number of rows) already; further downward steps are neither possible nor necessary, so we can stop even before having looked at the last column. Indeed, the matrix R0 that we have now is in REF(1, 3, 4), and even in RREF(1, 3, 4) as there are no zero rows in the end. But in general, the result of Gauss- Jordan elimination can be a matrix R0 with some zero rows in the end. As a simple example, think of the same starting matrix A as above, with a zero row appended in the end. Here is the algorithm in general. 106 Theorem 3.21 (Gauss-Jordan elimination). Let A be an m×n matrix. There exists an invertible m × m matrix M such that R0 = M A is in REF. The matrix M turns out to be the product of all row operation matrices that we use during elimination in order to transform the initial matrix A into the final matrix R0. We can also compute M itself on the way, by transforming a second initial m × m identity matrix I via the same row operations. This will result in M as a second final matrix. In our example above, we get M =   − 1 2 0 1 2 4 −1 − 1 2 −3 1 0   . You can check that indeed, R0 = M A for A in (3.9) and R0 the result of elimination as obtained on the previous page. Proof. We proceed by induction on n where the base case is n = 0. If you have not looked into Exercise 2.24, you may wonder what an m × 0 matrix looks like, a matrix with no columns. But whatever it looks like, it is in REF() with r = 0 according to Definition 3.19, so M = I works. Indeed, the definition in this case requires something “for i = 1, 2, . . . , r” in (i) and “for all i, j” in (ii). This means that there are in fact no requirements, since there are no applicable i in (i) and no applicable pairs (i, j) in (ii); see also the remark after Definition 2.3. If you don’t like the base case n = 0, you can also convince yourself that any m × 1 matrix can be transformed to REF, for example by repeating the induction step below to handle this case. Now we consider n > 0 and assume that the statement already holds for all m×(n−1) matrices. Then we can write A as A =   A ′ ︸︷︷︸ m×(n−1) | v |   and use the induction hypothesis to get R′ 0 = M ′A ′ where R′ 0 is in REF and M ′ is invert- ible. Under the hood, the induction hypothesis has already performed elimination in all but the last column of A: We have M ′A =   R′ 0︸︷︷︸ m×(n−1) | w |   , where w = M ′v. Let R′ 0 be in REF(j1, j2, . . . , jr). Then M ′A looks like in Figure 3.5 (left). There are two cases: if wi = 0 for all i > r, we are done; see Figure 3.5 (middle). In this case, M ′A is also in REF(j1, j2, . . . , jr), so we can set M = M ′ to conclude that R0 = M A is in REF. This case also happens if r = m in which case there are no applicable i > r. In the other case, there is some i > r such that wi ̸= 0, indicated by ⋆ in Figure 3.5 (right). In this case, we do some more row operations to transform M ′A into REF. We 107 w1 w2 ... wr−1 wr wr+1 ... wm 0 0 ... 0 1 w1 w2 ... wr−1 wr 0 ... 0 0 0 ... 0 1 w1 w2 ... wr−1 wr ... ⋆ ... 0 0 ... 0 1 Figure 3.5: Matrix M ′A is in REF, except possibly the last column (left). There are two cases: wi = 0 for all i > r (middle), or there is some i > r such that wi ̸= 0 (right). first perform a row exchange (if necessary) to get ⋆ into row r + 1; see Figure 3.6 (left). This does not affect the previous columns since they have zero entries in rows i > r. Let M1 be the corresponding (invertible) row operation matrix. w1 w2 ... wr−1 wr ⋆ ... ... 0 0 ... 0 1 w1 w2 ... wr−1 wr 1 ... ... 0 0 ... 0 1 0 0 ... 0 1 0 0 ... 0 0 1 Figure 3.6: Performing row operations to transform M ′A into REF: a row exchange (left), a row division (middle), and eliminations in the last column (right) Next, we perform a row division which divides a row by a nonzero scalar. Here, we divide row r + 1 by ⋆ so that ⋆ turns into 1; see Figure 3.6 (middle). Again, this does not change the previous columns, since ⋆ is the only nonzero entry in this row. Like all other row operations, a row division is undoable (multiply the row by ⋆!) and is therefore realized by an invertible row operation matrix M2. Concretely, M2 results from the m × m identity matrix by replacing the 1 in row r + 1 and column r + 1 with 1/⋆. Finally, we perform eliminations: subtract suitable multiples of row r+1 from all other rows, also the ones above row r + 1, so that the last column is transformed into er+1; see Figure 3.6 (right). As before, this leaves the previous columns untouched. Let M3 be the product of all elimination matrices needed for the eliminations. The matrix R0 resulting after all the row operations is now in REF(j1, j2, . . . , jr, jr+1) 108 with jr+1 = n and satisfies R0 = M3M2M1M ′ ︸ ︷︷ ︸ M A = M A, where M —as a product of invertible matrices—is invertible; see Lemma 3.9. From R0 as obtained from A through Gauss-Jordan elimination, we can directly read off the independent columns of A. Lemma 3.22. Let A be an m × n matrix, M an invertible m × m matrix, and R0 = M A in REF(j1, j2, . . . , jr). Then A has independent columns j1, j2, . . . , jr. Proof. We will argue that A and R0 = M A have their independent columns at the same positions. Then the statement follows since R0 has independent columns j1, j2, . . . , jr, as shown in the proof of Observation 3.20. The argument is a refinement of what we did in Corollary 3.4 to prove that multipli- cation with a row operation matrix does not affect linear independence of the columns. Column j of A is independent if and only if there exists a vector x ∈ R n such that Ax = 0, xj = −1, xk = 0 for k > j. Indeed, after adding the j-th column of A to both sides of Ax = 0, this expresses column j as a linear combination of the previous columns. Similarly, column j of R0 is independent if and only if there is x ∈ R n such that R0x = 0, xj = −1, xk = 0 for k > j. Now, if such a vector exists for A, the same vector also works for R0 = M A, and vice versa: since M is invertible, Ax = 0 ⇔ M Ax = 0 (this is Lemma 3.3 applied with b = 0). Hence, A and R0 = M A have their independent columns at the same positions. It is worthwhile to understand what Gauss-Jordan elimination does on an invertible m × m matrix A. In this case, all columns are independent, so by the previous Lemma, the resulting m × m matrix R0 is in REF(1, 2, . . . , m), and hence equal to the identity matrix I. So we have R0 = I = M A, and this means that the matrix M in Theorem 3.21 is actually the inverse of A. 3.5.4 Runtime We have shown that Gauss elimination on an invertible m×m matrix A needs time O(m3), see Theorem 3.6. For Gauss-Jordan elimination, we have two more parameters: n, the number of columns of A, and r, the rank of A. The following result gives the runtime depending on these three parameters. If n = r = m, we again get O(m 3). Hence, using Gauss-Jordan elimination on an invertible m × m is only by a constant factor slower than Gauss elimination. It will be slower, since it does more work in this case: reduce A to the identity matrix I instead of an upper triangular matrix U . 109 Theorem 3.23. Let A be an m × n matrix of rank r, and let b ∈ R m. (i) Using Gauss-Jordan elimination, A can be transformed into R0 = M A in REF as given by Theorem 3.21 in time O(rmn + mn). (ii) By simultaneously transforming the m × m identity matrix using the same row operations, M = M I can be computed in additional time O(rm2 + m 2). (iii) Given M , the system Ax = b can be solved in time O(m 2). Proof. The analysis for (i) and (ii) is similar as for Gauss elimination (Section 3.2.4); except that we count the steps much less exactly—the big-O lets us get away with this. In each of the r downward steps, we update some matrix entries. As there are mn entries in A, the bound in (i) follows, where the extra mn account for handling the columns in which we do not make downward steps. The bound in (ii) is simply (i) applied with n = m. For (iii), we compute the matrix-vector product c = M b which can be done in O(m 2) time; this is easy to see using the definitions of matrix-vector multiplication in Section 2.1.1. After this, we directly solve R0x = c, see Section 3.5.2. Knowing the independent column indices j1, j2, . . . , jr of R0 from its shape, this can actually be done in time O(m) which is dominated by O(m 2). Now we also have a solution of Ax = b, or we know that there is no solution. The two systems are equivalent, since Ax = b and R0x = c are obtainable from each other by premultiplication of both sides with M and M −1, respectively. If m ≤ n, then (ii) does not lead to much extra work. If m > n (A is tall and skinny), however, this step may cost more time than (i). If r and n are both small, the extra work for computing the m × m matrix M is significant. In this case, we can skip (ii) and solve Ax = b via a replay approach: remembering the row operations we did in the r downward steps in (i) and applying them again to b, we only need O(rm) time to transform b into c = M b and O(m) time to directly solve R0x = c. 3.5.5 Computing the CR decomposition In Theorem 2.23, we have introduced the CR decomposition of an m × n matrix A. This writes A as A = C︸︷︷︸ m×r R︸︷︷︸ r×n , where r is the rank of A, the matrix C contains the independent columns of A, and col- umn j of R tells us how column j of A can be expressed as a linear combination of the independent columns. We have also seen that there is a unique such matrix R. What we left open is how to systematically compute it. Here is the answer. Theorem 3.24. Let A be an m × n matrix and let A = CR as in Theorem 2.23. Let R0 = M A in REF(j1, j2, . . . , jr) be the result of Gauss-Jordan elimination on A, see Theorem 3.21. Then R results from R0 by removing the zero rows at the end (if there are any); in particular, R is in RREF(j1, j2, . . . , jr), and C is the submatrix of A with columns j1, j2, . . . , jr. 110 This may be a bit surprising. Upfront, it is not clear what CR decomposition has to do with Gauss-Jordan elimination. The theorem also implies that the result R0 of Gauss- Jordan elimination is unique, despite the fact that there are some choices in the algorithm that could potentially influence the result. These choices concern the row exchanges. If in some step, there is more than one nonzero entry ⋆ below a zero pivot, then there is more than one way to bring a nonzero entry up. But apparently, it doesn’t matter which entry we bring up. This is also easy to see directly if we review the proof of Theorem 3.21 with this aspect in mind. Before we prove the theorem, let’s check it on an example. In Section 2.2.3, we have considered A =   1 2 0 3 2 4 1 4 3 6 2 5   and manually computed A =   1 0 2 1 3 2   ︸ ︷︷ ︸ C [ 1 2 0 3 0 0 1 −2 ] ︸ ︷︷ ︸ R . To get R0, we apply Gauss-Jordan elimination on A. This is a particularly simple case, since no row exchanges and row divisions are necessary, and also no eliminations above the pivot: A =   1 2 0 3 2 4 1 4 3 6 2 5   elimination in column 1: ↓  1 2 0 3 0 0 1 −2 0 0 2 −4   elimination in column 3: ↓ R0 =   1 2 0 3 0 0 1 −2 0 0 0 0   After removing the zero row in the end, we indeed have R in this case. Proof of Theorem 3.24. Let R0 = M A be in REF(j1, j2, . . . , jr). Plugging in A = CR gives R0 = M CR, where C is the submatrix of A containing columns j1, j2, . . . , jr: these are the independent ones by Lemma 3.22. Hence, M C is the submatrix of M A = R0 containing columns j1, j2, . . . , jr. By Definition 3.19 of REF, these columns are the unit vectors e1, e2, . . . , er. 111 Therefore, R0 = M CR =     I︸︷︷︸ r×r 0︸︷︷︸ (m−r)×r     ︸ ︷︷ ︸ M C R =     R︸︷︷︸ r×n 0︸︷︷︸ (m−r)×n     ︸ ︷︷ ︸ R0 Since R0 has exactly m − r zero rows at the end (one for each row i > r), R is indeed R0 without the zero rows in the end. There was another question we asked in Section 2.2.3: what is the CR decomposition good for? We will shed some light on this in Section 4.3). 112 Chapter 4 The Four Fundamental Subspaces 4.1 Vector spaces In this section, we finally reveal the truth about vectors: thinking of them as arrows in some space Rm provides an incomplete picture. We introduce the abstract concept of a vector space and see that the R m’s form just one species in a whole fauna of vector spaces (an important species, though). The abstraction also allows to view subspaces such as the column space of a matrix as vector spaces themselves. So far, we have said that vectors are elements of some space R m, and for m = 2, 3, we have drawn them as arrows in the 2-dimensional plane or in 3-dimensional space. But this is by far not the full picture. So what is a vector, really? To approach this, we resort to an analogy: Asking someone what a mammal is might bring up the answer “a cat, for example.” A more elaborate answer may also include other examples such as dogs, whales, lions, and humans. But this kind of answer doesn’t define a mammal. Let’s ask Wikipedia what a mammal is. Here is the answer: 1 A mammal [. . . ] is a vertebrate animal of the class Mammalia. Mammals are characterized by the presence of milk-producing mammary glands for feeding their young, [. . . ] This is not a definition of an individual mammal, but of the class of mammals: what is it that makes a mammal a mammal? This is what we need to know in order to really understand mammals. The species of cats is just one of many in the class of mammals. Coming back to vectors: Saying that a vector is an element of some R m is actually like saying that a mammal is a cat. Both statements are false, but in case of vectors, we can blame it on ignorance: so far, we simply haven’t seen any “species” of vectors beyond the R m’s. But such species exist in other corners of the world of mathematics (we will discover one of them soon). Knowing this, the more fundamental question arises: what 1https://en.wikipedia.org/wiki/Mammal, accessed August 27, 2024 113 do they have in common? What is it that that makes a vector a vector? Here is the high- level answer, in the same spirit as the Wikipedia definition of a mammal: A vector is an element of a vector space. Vector spaces are characterized by the presence of two operations on their elements: vector addition and scalar multiplication. Table 4.1 summarizes the situation. vectors mammals individual xy [ 4 1 ] 0 breed R 2 Bombay cat species all R m’s cats class vector spaces Mammalia characterization of the class vector addition, scalar multiplication milk-producing mammary glands Table 4.1: Vectors vs. mammals 4.1.1 Definition and examples After this high-level definition, here comes the actual definition of a real vector space. The word real doesn’t stand for true or proper, but indicates that this is a vector space where the scalars are real numbers. Each R m is a real vector space, but immediately after the definition, we will see a new “species.” There are also vector spaces where the scalars are other kinds of numbers (complex numbers are an important case, and so are bits, the elements of the set {0, 1}), but we will not discuss them here. So we omit the word real and simply say vector space. Don’t be scared by the length of the definition; in particular, you will not be asked to learn the axioms by heart. You can look them up whenever needed. Definition 4.1 (Vector space). A vector space is a triple (V, +, ·) where V is a set (the vectors), and + : V × V → V is a function (vector addition), · : R × V → V is a function (scalar multiplication), satisfying the following axioms of a vector space for all u, v, w ∈ V and all λ, µ ∈ R. 114 1. v + w = w + v commutativity 2. u + (v + w) = (u + v) + w associativity 3. There is a vector 0 such that v + 0 = v for all v zero vector 4. There is a vector −v such that v + (−v) = 0 negative vector 5. 1 · v = v identity element 6. (λ·µ)v = λ · (µ · v) compatibility of · and · in R 7. λ(v + w) = λv + λw distributivity over + 8. (λ+µ)v = λv + µv distributivity over + in R Here, we use red color to indicate that there are two different “+”, and also two differ- ent “·”. The red ones stand for the normal addition and multiplication of real numbers. The black ones are for vector addition and scalar multiplication. We will omit the color- ing (and even the “·”) in the following, but there is (hopefully) only limited potential for confusion. After all, if you want to know which “+” or “·” is meant in a given context, you simply need to check what is on the left side and the right side. We have done this kind of overloading before when we used the normal multiplication symbol “·” also for the scalar product of two vectors, as in v · w. Now for the actual axioms: most of them seem obvious. Well, in R m, they are indeed (more or less) obvious, given how we have defined vector addition and scalar multiplica- tion. Observation 4.2. (R m, +, ·), with “+” as in Definition 1.1 and “·” as in Definition 1.3, is a vector space. Previously, we have called this vector space R m which—in hindsight—is an abuse of notation. But this is acceptable, since our “+” and “·” are what mathematicians call the canonical (accepted standard) choices for vector addition and scalar multiplication in Rm, so there is no strict need to mention them. But in general, V could be any set, with + and · defined in non-canonical ways, so we explicitly need to include these functions and also make sure that they behave as expected, where the expectations come from what happens in R m. This is what the axioms are for. To illustrate the concept, we exhibit a new breed, the vector space of real polynomials in one variable. This belongs to the species of polynomials in several variables, but here we restrict to one variable. As for vector spaces, we omit the word real, since we do not consider any other polynomials in this course. Definition 4.3 (Polynomial). A polynomial p is a sum of the form p = m∑ i=0 pixi, for some m ∈ N. Here x is a variable, and the numbers p0, p1, . . . , pm ∈ R are the coefficients of p. The largest i such that pi ̸= 0 is the degree of p. If all pi are 0, we have the zero polynomial 0 = 0 whose degree we define to be −1. 115 We note that m doesn’t have to be the same for all polynomials, but for every polyno- mial, we have some m. For example, p = 2x2 + x + 1 is a polynomial of degree 2, and q = 5x − 2 is a polynomial of degree 1. To turn the set of polynomials into a vector space, we need to define addition of two polynomials, and multiplication of a polynomial with a scalar. There are canonical ways of doing this. In the example, we would define p + q = 2x2 + 6x − 1, i.e. we add corresponding powers of x. And scalar multiplication simply scales all coeffi- cients, as in 5p = 10x2 + 5x + 5. Theorem 4.4. Let R[x] be the set of polynomials in one variable x. Given polynomials p =∑m i=0 pixi and q = ∑n i=0 qixi, we define p + q to be the polynomial p + q = max(m,n)∑ i=0 (pi + qi)xi, where we set pi = 0 for i > m and qi = 0 for i > n. For a scalar λ ∈ R, we further define λp as the polynomial λp = m∑ i=0 (λpi)xi. Then (R[x], +, ·) is a vector space. We omit the proof, since it is quite boring; it boils down to checking the obvious. Here is a second example: the vector space of m × n matrices. Again, we omit the easy but boring proof. Theorem 4.5. Let R m×n be the set of m × n matrices, with addition A + B and scalar multipli- cation λA defined in the usual way, see Definition 2.2. Then (R m×n, +, ·) is a vector space. Proving the obvious. As boring as this may be, it is still surprising that we only need to check 8 “obvious” axioms to guarantee proper behavior of a vector space. Indeed, there are many other things that we expect from knowing how things work in Rm. For example, we expect that there is only one zero vector in a vector space (V, +, ·), but this doesn’t appear among the axioms. So we need to prove that it follows from the axioms. Fact 4.6. Let (V, +, ·) be a vector space. V contains exactly one zero vector (a vector satisfying axiom 3 of Definition 4.1: v + 0 = v for all v). Proof. Take two zero vectors 0 and 0′. Then 0′ = 0 ′ + 0 (by axiom 3, since 0 is a zero vector) = 0 + 0′ (by axiom 1, commutativity) = 0 (by axiom 3, since 0 ′ is a zero vector). So 0 and 0′ are equal. 116 Doing this may feel a little bit like learning to walk again after a serious leg injury: hard work for something that we have previously taken for granted. Here is another such “relearning step.” Fact 4.7. Let (V, +, ·) be a vector space. For every v ∈ V , there is exactly one negative vector −v (a vector satisfying axiom 4 of Definition 4.1: v + (−v) = 0). Proof. First of all, we need to realize that there is no way of simply computing −v as we do it in R m, by negating all entries. A vector v might not have any such “entries”, and there is no “−” operator in (V, +, ·) that we could apply. In the proof, we can only use the 8 axioms (and everything we have already derived from them). We could attempt to compute −v as (−1)v using scalar multiplication; this can be shown to produce a negative vector but does not rule out the existence of another negative vector. Here is how we go about that: Take two negative vectors u and u′ of v. Then u′ = u′ + 0 (by axiom 3, zero vector) = u′ + (v + u) (by axiom 4, since u is a negative of v) = (u′ + v) + u (by axiom 2, associativity) = (v + u′) + u (by axiom 1, commutativity) = 0 + u (by axiom 4, since u′ is a negative of v)) = u + 0 (by axiom 1, commutativity) = u (by axiom 3, zero vector). So u and u′ are equal. Having understood why a vector space is a triple (V, +, ·) and not simply a set V of vectors, we will continue our (now more educated) abuse of notation and still write V for the vector space, with the understanding that vector addition and scalar multiplication are clear from the context. We also still write 0 for the zero vector when V is clear from the context. 4.1.2 Subspaces Definition 4.8 (Subspace). Let V be a vector space. A nonempty subset U ⊆ V is called a subspace of V if the following two axioms of a subspace are true for all v, w ∈ U and all λ ∈ R. (i) v + w ∈ U ; (ii) λv ∈ U . These axioms guarantee that vector addition and scalar multiplication cannot take us out of the subspace. As a consequence, a subspace of V always contains at least the zero vector. 117 xyzxyzxyz Figure 4.1: Subspaces of R 3: a line through 0 (all scalar multiples of one vector); a plane through 0 (all linear combinations of two linearly independent vectors); the right subset is not a subspace, since it misses 0. Lemma 4.9. Let U ⊆ V be a subspace of a vector space V . Then 0 ∈ V . Proof. Take any u ∈ U (U is nonempty), By subspace axiom (ii), 0u = 0 ∈ U . This proof seems quite obvious, but it is actually incomplete. While the equation 0u = 0 certainly holds in any R m, this does not automatically mean that it holds in all vector spaces. We need to prove it—another case of learning to walk again. We promise, it’s the last one! In fact, the axioms of vector spaces have been carefully designed by mathematicians before us, with the goal of ensuring that everything that seems obvious is actually true. So we will relax and rely on this in the future. Fact 4.10. Let V be a vector space, v ∈ V . Then 0v = 0. Proof. 0v = 0v + 0 (by axiom 3 of Definition 4.1, zero vector) = 0v + (0v + (−0v)) (by axiom 4, negative vector) = (0v + 0v) + (−0v) (by axiom 2, associativity) = (0+0)v + (−0v) (by axiom 8, distributivity over + in R) = 0v + (−0v) (by the rules of R) = 0 (by axiom 4, negative vector) Figure 4.1 gives two examples and one counterexample of subspaces of R3. Here is a subspace of R m that we have already encountered, without thinking about it as a sub- space: the column space of a matrix. Lemma 4.11. Let A be an m × n matrix. Then C(A) = {Ax : x ∈ R n} is a subspace of R m. 118 Proof. Let v, w be in C(A). Then there exist vectors x, y ∈ R n such that v = Ax, w = Ay. Hence, A(x + y ︸ ︷︷ ︸ ∈Rn ) = Ax + Ay = v + w ⇒ v + w ∈ C(A). This was subspace axiom (i). For axiom (ii), let λ ∈ R. Then A( λx︸︷︷︸ ∈Rn ) = λAx = λv ⇒ λv ∈ C(A). In both chain of equalities, the first equality comes from the interpretation of matrix- vector multiplication as a linear transformation of the vector; see Observation 2.26. Knowing that a subspace always contains 0, we can actually say more. Lemma 4.12. Let V be a vector space and U a subspace. Then U is also a vector space (with the same “+” and “·” as V ). Proof. Formally, to turn “+” and “·” into functions that work for U , we have to restrict their domains to U × U and R × U , respectively. The subspace axioms (i) and (ii) then also restrict their ranges to U . Next, we need to check the 8 axioms. All but axiom 4 are true for all vectors in V , since V is a vector space; in particular, they hold for all vectors in U , so there is nothing to check. In Case of axiom 3, we are also using that 0 ∈ U (Lemma 4.9). What remains is axiom 4: we need to make sure that for all u ∈ U , −u is actually in U ; so far we only know that it is in V . But this holds, since (−1)u ∈ U by subspace axiom (ii), and “obviously” (−1)u = −u. If you are up for it, you can prove the obvious, otherwise, you can safely believe it. Subspaces of R[x]. Let’s look at some subspaces of R[x], the vector space of polynomials (Theorem 4.4). A polynomial without constant term is a polynomial of the form p = m∑ i=0 pixi where p0 = 0. An example is x2 + 3x. It is clear that these polynomials form a subspace of R[x], since the sum of two polynomials without constant term is again a polynomial without constant term, and so is each scalar multiple of a polynomial without constant term. A quadratic polynomial is a polynomial p of the form p = p0 + p1x + p2x2. We do not require p2 ̸= 0, so 3x is also a quadratic polynomial. Again, it is easy to see that the quadratic polynomials form a subspace of R[x]. In fact, this subspace looks a lot like 119 R 3: each quadratic polynomial p is determined by three real numbers p0, p1, p2, so we can also describe it by a vector vp =   p0 p1 p2   ∈ R 3. Moreover, “+” and “·” on quadratic polynomials translate to “+” and “·” on the corre- sponding vectors: vp+q = vp + vq, vλp = λvp. Therefore, the subspace of quadratic polynomials is just R 3 in disguise. The mathematical term is that the two spaces are isomorphic. This allows us an interesting view of R[x] as the union of all R m, m = 0, 1, . . .. In this union, we can also add vectors of different dimensions, for example   1 1 2   ︸︷︷︸ 2x2+x2+1 + [ −2 5 ] ︸ ︷︷ ︸ 5x−2 =   −1 6 2   ︸ ︷︷ ︸ 2x2+6x−1 . In this sense, polynomials form a “super breed” containing R m for all m. Subspaces of R m×n. Let’s turn to Rm×n, the vector space of matrices (Theorem 4.5). Here, we first observe that this is not really a new breed of vectors spaces, as R m×n is iso- morphic to R mn: an m×n matrix is one way of grouping mn numbers, an mn-dimensional vector is another way. In both cases, vector addition and scalar-multiplication are defined entry-wise, so it doesn’t really matter how we group the numbers. The difference is really in how we think about vectors and matrices. A vector in R mn is a “flat” 1-dimensional array, while a matrix in R m×n is a 2-dimensional array of rows and columns; see also Section 3.1.2 on computer vectors and matrices. The 2-dimensional view leads to subspaces that would not make intuitive sense in a 1-dimensional array. For the examples, we consider R 2×2. Our first subspace is the set of symmetric matrices, the ones of the form [a b b d ] . To see that this is a subspace, it suffices to observe that the sum of two symmetric matrices is symmetric, and that scaling a symmetric matrix keeps it symmetric. The second and slightly more creative example are the matrices of trace 0, the ones of the form [ a b c d ] , where a + d = 0. Generally, the trace of a square matrix is the sum of the diagonal elements. Again, it is easy to see that the subspace axioms are satisfied. However, matrices of trace 1 do not 120 form a subspace, and there are many other non-subspaces, for example the invertible matrices, the ones of rank 1, etc. Try to find violations of the subspace axioms for all of them! 4.2 Bases and dimension The dimension of a vector space is an important measure of its complexity. So far, we only have an intuitive understanding of dimension, according to which R m has dimension m. In this section, we first define bases of vector spaces and prove via the Steinitz exchange lemma that all bases have the same size. This allows us to define the dimension of a vector space as the size of any basis of it. Moroever, a basis provides a compact description of the vector space; computing a vector space means to compute a basis of it. From working with the vector spaces R m, we have an intuitive understanding of di- mension. According to this, R m has dimension m. But if V is some other vector space, we may not have such an intuition, so we need to define the dimension of a vector space. We expect this definition to tell us that R m indeed has dimension m. But for example, what is the dimension of the vector space of polynomials introduced in Section 4.1.1)? As it “contains” Rm for all m, we expect the dimension to be infinite. 4.2.1 Bases The crucial concept here is that of a basis. A basis of a vector space V consists of linearly independent vectors whose span is V (see Section 1.3.3 for the definition of span). Previ- ously, we have defined linear independence and span for a sequence of vectors, and only in R m. This was important to handle for example the sequence of columns of a matrix that may contain duplicates. But here, sets of vectors turn out to be more practical. So we start by recalling the definitions of linear combination, linear independence and span, adapted for a set of vectors in a general vector space. At the same time, we extend them such that we can also handle infinite sets. Definition 4.13 (Linear combination of a set of vectors). Let V be a vector space, G ⊆ V a (possibly infinite) subset of vectors. A linear combination of G is a sum of the form ∑ v∈F λvv, where F ⊆ G is a finite subset of G and λv ∈ R for all v ∈ F . Summing over all elements of a set as in ∑ v∈F λvv is in general not well-defined, since a set does not have an order of its elements. But if the result of the summation is the same for every possible order, this notation can be used. Here, this is the case, since vector addition is commutative (axiom 1 in Definition 4.1). 121 A linear combination can also be written in the usual way, namely as ∑ v∈G λvv, and requiring that only finitely many λv are nonzero. Here is an important fact. It is obvious in any R m, but for a general vector space, we have to prove it. Lemma 4.14. Let V be a vector space, G ⊆ V . Every linear combination of G is again in V . Proof. Let ∑ v∈F λvv be a linear combination, |F | = n. Enumerating the elements of F in arbitrary order v1, v2, . . . , vn, we can write the combination as ∑n j=1 λjvj. Since V is a vector space, we have wj := λjvj ∈ V for all j, by definition of the scalar multiplication (function · : R × V → V ). By definition of vector addition (function + : V ×V → V ), we also have w1 +w2 ∈ V . Applying this again yields (w1 +w2)+w3 ∈ V , and so on, until we get the desired conclusion w1 + w2 + · · · + wn ∈ V . (Under the hood, this is a proof by induction, and it uses the “obvious” fact that brackets can be omitted in writing down a sum of vectors). You may wonder why we do not allow infinite linear combinations. Infinite sums in themselves can be defined, you may for example know the formula ∞∑ i=0 xi = 1 1 − x for x < 1. The problem is that Lemma 4.14 may fail for infinite linear combinations, and we don’t want this. Consider the vector space of polynomials R[x], and let G be the infinite subset of unit monomials: 1, x 2, x 3, . . .. With λp = 1 for all p ∈ G, we get the infinite “linear combination” ∑ p∈G λpp = ∞∑ i=0 xi. This is not a polynomial. The vector space axioms only imply that finite linear combina- tions of vectors are again vectors, but infinite ones may be undefined or take us out of the vector space, as we have just seen. With linear combinations as in Definition 4.13, we can now define span and linear independence in the usual way. Definition 4.15 (Span and linear independence of sets of vectors). Let V be a vector space, G ⊆ V a (possibly infinite) subset of vectors. The span of G, written as Span(G), is the set of all linear combinations of G. The set G is called linearly independent if no vector v ∈ G is a linear combination of G \\ {v}. Now we can formally define a basis. Definition 4.16 (Basis). Let V be a vector space. A subset B ⊆ V of vectors is called a basis of V if B is linearly independent and Span(B) = V . 122 Examples. The set {e1, e2, . . . , em} of standard unit vectors (Section 1.2.2) is a basis of R m. For example, if m = 2, then e1 = [1 0 ] , e2 = [ 0 1 ] . These two vectors are linearly independent and span R 2: for every vector v = [ v1 v2 ] ∈ R 2, we have v = v1e1 + v2e2. For general m, the standard unit vectors are seen to be linearly independent by the private nonzero argument: every standard unit vector has a nonzero entry (a 1-entry, actually) at a coordinate where all other standard unit vectors have 0- entries. We call such an entry a private nonzero. A vector with a private nonzero cannot be a linear combination of the other ones, and if every vector has a private nonzero, the vectors are linearly independent. Lemma 4.17. Let A be an m × n matrix. The set of independent columns of A (Definition 2.9) is a basis of the column space C(A). Proof. C(A) is a subspace by Lemma 4.11. The independent columns are in the column space and linearly independent: by definition, no independent column is a linear combi- nation of the previous columns, and this means that the independent columns are in fact linearly independent; see Corollary 1.20. Furthermore, the independent columns span the column space, as we have shown in Lemma 2.10. For the subspace of symmetric 2 × 2 matrices [a b b d ] , the following set of three symmetric matrices is a basis. {[ 1 0 0 0 ] , [0 1 1 0 ] , [ 0 0 0 1 ]} . They are linearly independent: every matrix has at least one private nonzero, a 1-entry where all other matrices have 0-entries. It remains to observe that every symmetric matrix is a linear combination of these three matrices: [a b b d ] = a [1 0 0 0 ] + b [0 1 1 0 ] + d [0 0 0 1 ] . For the trace-0 matrices [ a b c d ] , a + d = 0, 123 a basis is {[ 1 0 0 −1 ] , [0 1 0 0 ] , [0 0 1 0 ]} . Linear independence is again easy due to private nonzeros, and as d = −a in a trace-0 matrix, we can obtain every trace-0 matrix as a linear combination: [ a b c −a ] = a [ 1 0 0 −1 ] + b [0 1 0 0 ] + c [0 0 1 0 ] . For the vector space R[x] of polynomials, the infinite set of unit monomials {xi : i = 0, 1, . . .} is a basis. By Definition 4.3, every polynomial is a linear combination of unit monomials (observe that x0 = 1). It remains to argue that the unit monomials are linearly indepen- dent. Indeed, every unit monomial xi has its private nonzero, the i-th power of x and can therefore not be obtained as a linear combination of other monomials. The subspace of polynomials without a constant term has {xi : i = 1, 2, . . .} as a basis, and for the subspace of quadratic polynomials, a basis is {1, x, x2}. Finally, what is the basis of {0}, the smallest possible subspace of a given vector space? It is the empty set . Indeed, this is linearly independent by Definition 4.15: in the empty set, no vector is a linear combination of the others. And Span(∅) = {0}, since an empty sum yields 0; see the discussion in Section 1.1.5. There are typically many bases. The above examples should not trick us into believing that there is always only one basis of a vector space. For example, {e1, e2, . . . , em} is the canonical basis of Rm, but there are many other choices. Observation 4.18. Every set B = {v1, v2, . . . , vm} of m linearly independent vectors is a basis of R m. Proof. B is linearly independent, so we only need to show that Span(B) = R m, meaning that every vector v ∈ R m is a linear combination of B. For this, let A be the m × m matrix with (linearly independent) columns v1, v2, . . . , vm. By Theorem 3.11, Ax = v has a unique solution x, and v = m∑ j=1 xjvj ︸ ︷︷ ︸ Ax is the desired linear combination of B. 124 As a second example, let us consider the column space C(A) of a matrix A. In Sec- tion 2.2.3, we have computed the independent columns of A =   1 2 0 3 2 4 1 4 3 6 2 5   . These are the first and the third column, so these two columns form a basis of C(A). But we could also have defined the “backwards independent” columns, by going through the columns of A from right to left, and selecting a column if it is not a linear combination of the ones succeeding it. This also results in a basis of C(A), by the same arguments as for the “forward independent” columns. If we do this in our example, we end up with the fourth and the third column as a basis. More generally, we could go through the columns in any order and pick up the ones that are not linear combinations of the ones previously considered. This potentially gives us many different bases of C(A). However, what we find in both R m and C(A) is that the alternative bases still have the same number of vectors: m in case of R m and 2 in case of the column space example. In the next section, we prove that this is not a coincidence but true for every vector space. 4.2.2 The Steinitz exchange lemma Let V be a vector space, and suppose that F ⊆ V is a finite set of linearly independent vectors, and G ⊆ V a finite set of vectors with Span(G) = V . The Steinitz exchange lemma makes two statements. The first one is that |F | ≤ |G|. This makes sense: In R2, for example, we can have at most 2 linearly independent vectors, and it takes at least 2 vectors to span R 2. The second stament sounds a bit technical: we can enlarge F by elements from G such that the enlarged set has at most the size of G and also spans V . The name of the lemma comes from the fact that we can think of this as “exchanging elements between G and F ”. Lemma 4.19 (Steinitz exchange lemma). Let V be a vector space, F ⊆ V a finite set of linearly independent vectors, and G ⊆ V a finite set of vectors with Span(G) = V . Then the following two statements hold. (i) |F | ≤ |G|. (ii) There exists a subset E ⊆ G of size |G| − |F | such that Span(F ∪ E) = V . Note that the set E in (ii) is allowed to contain elements of F . In this case, |F ∪E| < |G|. There is a stronger version of the lemma in which we require E ⊆ G \\ F , and this always leads to |F ∪ E| = |G|. But we will not need this. 125 Proof. We adapt the proof from Wikipedia.2 The proof is by induction on f = |F |. The base case is f = 0 in which case F is the empty set. Then (i) is clear and for (ii), we chose E = G. If f > 0, pick an arbitrary vector u ∈ F , set F ′ = F \\ {u}, and let g = |G|. Note that F ′ is also linearly independent. By the induction hypothesis, we have (i) g ≥ f − 1. (ii) There exists a subset E′ ⊆ G of size g − (f − 1) with Span(F ′ ∪ E′) = V . Since u ∈ V = Span(F ′ ∪ E′), there are scalars λv, v ∈ F ′ ∪ E′ such that u = ∑ v∈F ′∪E′ λvv. (4.1) There must be some w ∈ E′ with λw ̸= 0. Indeed, since F is linearly independent, u is not a linear combination of F ′ = F \\ {u} (Definition 4.15). This shows that |E′| = g − (f − 1) ≥ 1, so we get g ≥ f which already proves (i) for size f . To show (ii) for size f , we remove w from E′ to obtain E. This set has the required size g − f , and it remains to prove that Span(F ∪ E) = V . Towards this, we solve (4.1) for w: w = 1 µw ( u − ∑ v∈F ′∪E λvv ) . Thus, w is a linear combination of {u} ∪ F ′ ∪ E = F ∪ E, and by Lemma 1.23, this means that Span(F ∪ E) = Span(F ∪ E ∪ {w} ︸ ︷︷ ︸ F ∪E′ ). (4.2) Formally, we would need a version of Lemma 1.23 for general vector spaces, and for finite sets instead of finite sequences. But we skip this (the proof would be the same as before). Recall from (4.1) that u is a linear combination of F ′ ∪ E′, so Lemma 1.23 also gives Span(F ′ ∪ E′) = Span(F ′ ∪ E′ ∪ {u} ︸ ︷︷ ︸ F ∪E′ ). (4.3) Putting together (4.2) and (4.3), and using the inductive hypothesis, we get Span(F ∪ E) = Span(F ′ ∪ E′) = V. The Steinitz exchange lemma has an important corollary. Because of its importance, we also call it a theorem. It confirms what we have observed in examples before: even if a vector space has different bases, all of them have the same number of vectors. 2https://en.wikipedia.org/wiki/Steinitz_exchange_lemma, accessed August 5, 2024 126 Theorem 4.20. Let V be a vector space and B, B′ ⊆ V two finite bases of V . Then |B| = |B′|. Proof. By Definition 4.16 of a basis, B and B′ are linearly independent, and Span(B) = Span(B′) = V . Then, statement (i) of the Steinitz exchange lemma with F = B, G = B′ yields |B| ≤ |B′|; with F = B′, G = B, we get |B′| ≤ |B|. There are vector spaces that do not have finite bases, such as the vector space R[x] of polynomials defined in Section 4.1.1. While the theorem as presented here does not apply to such vector spaces, it can be generalized to the infinite case where |B| = |B′| then means “the same kind of infinity.” A more fundamental question is this: does every vector space even have a basis? The answer is yes, even in the infinite case. Proving this involves some machinery that is standard but beyond the scope of these notes. The Wikipedia article about bases of vector spaces is a good entry point for further reading.3 Here we will present a proof for the finite case. This case is defined as follows. Definition 4.21 (Finitely generated vector space). A vector space V is called finitely gener- ated if there exists a finite subset G ⊆ V with Span(G) = V . For example, Rm is finitely generated (by G = {e1, e2, . . . , em}) but R[x], the vector space of polynomials, is not. Theorem 4.22. Let V be a finitely generated vector space, and let G ⊆ V be a finite subset with Span(G) = V . Then V has a basis B ⊆ G. Proof. This is what we call an “algorithmic proof’.” It constructs B by an algorithm. Here is how it goes. If G is linearly independent, B = G is a basis by Definition 4.16. Otherwise, there is a “line 1” vector v ∈ G that is a linear combination of the other vectors, so we have Span(G\\{v}) = Span(G) = V via Lemma 1.23. Then we replace G with G \\ {v} (which still spans V ) and go back to line 1. As G gets smaller in every step, this must eventually stop and produce a basis. A formal correctness proof would go via a precise formulation of the algorithm, either as a loop (correctness proof with loop invariants), or a recursive algorithm (correctness proof by induction). The latter approach is very close to directly proving the theorem by induc- tion on g = |G| (this is a good exercise). Our proof above, while hopefully being clear and easy to understand, is of a somewhat informal “and-so-on” nature, but knowing how we could make it formal if necessary, this is acceptable. 4.2.3 Dimension Now we can define the dimension of a vector space, at least if it is finitely generated. 3https://en.wikipedia.org/wiki/Basis_(linear_algebra), accessed August 5, 2024 127 Definition 4.23 (Dimension). Let V be a finitely generated vector space. Then dim(V ), the dimension of V , is the size of any basis B of V . This definition uses that every finitely generated vector space has a basis to begin with (Theorem 4.22), and that all bases have the same size (Theorem 4.20). From the examples of bases in Section 4.2.1, you can therefore immediately deduce the dimensions of the corresponding vector spaces. As expected, dim(R m) = m. Figure 4.2 shows three subspaces of R 3, of dimensions 0 (point), 1 (line), and 2 (plane). xyzxyzxyz Figure 4.2: Three subspaces of R3: The unique subspace of dimension 0—the point at the origin with an empty basis (left); a subspace of dimension 1—a line through the origin with a basis of size 1 (middle); a subspace of dimension 2—a plane through the origin with a basis of size 2 (right) We also have the following result that simplifies basis checks: if dim(V ) many vectors are linearly independent, we already know that their span is V , and the other way around. In both cases, the set can therefore be identified as a basis, from only one of the basis axioms in Definition 4.16. Lemma 4.24. Let V be a vector space with dim(V ) = d. (i) Let F ⊆ V be a set of d linearly independent vectors. Then F is a basis of V . (ii) Let G ⊆ V be a set of d vectors with Span(G) = V . Then G is a basis of V . Proof. For (i), let G be a basis of V . Since Span(G) = V , the Steinitz exchange Lemma 4.19 applies with F and G, but since |F | = |G| = d, the set E in part (ii) can only be the empty set. Hence, Span(F ) = Span(F ∪ E) = V , and F is a basis according to Definition 4.16. For (ii), we use Theorem 4.22 which guarantees a basis B ⊆ G of size d. But since |B| = |G| = d, we must have B = G, so G itself is a basis. 128 4.3 Computing the fundamental subspaces An m×n matrix A defines four fundamental subspaces. Column space and row space are two of them, and here we introduce the other two: nullspace and left nullspace. We show how they can be computed by which we mean to find bases for them. Once we have such bases, we also know the dimensions of the fundamental subspaces, and how they relate to each other: If r = rank(A), both row and column space have dimension r, while the nullspace has dimension n − r, and the left nullspace has dimension m − r. We finally apply these results to understand the space of all solutions of a system of linear equations Ax = b. A vector space V typically contains infinitely many vectors. But if V is finitely gener- ated, we can compute a basis of it. Such a basis is a finite representation of V in the sense that it allows us to describe all elements of V : these are simply all linear combinations of the basis. Moreover, for every v ∈ V , there is a unique such linear combination by (the vector space version of) Lemma 1.21. By computing a vector space, we mean to compute a basis of it. Figure 4.2 visualizes this representation by a basis: knowing the basis vectors, the space in question is uniquely determined. For a given m × n matrix A, we now want to compute the four fundamental subspaces C(A) (column space), R(A) (row space), N(A) (nullspace) and LN(A) (left nullspace). If we have bases for them, we also know the dimensions of these spaces, by Definition 4.23. Column and row space have been introduced before (Definitions 2.8 and 2.13), but here the focus is on looking at them at subspaces of R m and R n, respectively. The key to understanding these subspaces is Gauss-Jordan elimination. In Section 3.5.3, we have shown how this algorithm can transform every matrix A into a matrix R0 in row echelon form (REF). As our running example, we use the one considered in Sections 2.2.3 and 3.5.5 before: A =  1 2 0 3 2 4 1 4 3 6 2 5   → R0 =   1 2 0 3 0 0 1 −2 0 0 0 0   . (4.4) From this, we have already been able to read off the CR decomposition of A that we have previously computed manually (Section 2.2.3). Now we will see how to read off (bases of) the fundamental subspaces. 4.3.1 Column space We recall Definition 2.8. The column space of an m × n matrix A is the set of all linear combinations of the columns of A, C(A) = {Ax : x ∈ R n} ⊆ R m. 129 The column space is sometimes also denoted by Im(A) and called the image of A. This is because C(A) is the image of the linear transformation T (x) = Ax; see Definition 2.31. We know from Lemma 4.11 that C(A) is a subspace. For computing it, we have already done all the work. We summarize the situation in the following theorem. Theorem 4.25. Let A be an m × n matrix, and let R0 in REF(j1, j2, . . . , jr) be the result of Gauss-Jordan elimination on A, according to Theorem 3.21. Then A has independent columns j1, j2, . . . , jr, and these form a basis of the column space C(A). Hence dim(C(A)) = r = rank(A). Proof. The independent columns of A form a basis of C(A) by Lemma 4.17, and using Gauss-Jordan elimination, we can compute their positions; see Lemma 3.22, telling us that A has independent columns j1, j2, . . . , jr. In the running example (4.4), R0 is in REF(1, 3), so the basis B of C(A) consists of the two independent columns 1 and 3, B =      1 2 3   ,   0 1 2      . 4.3.2 Row space The row space of an m × n matrix A is the set of all linear combinations of the rows of A; to avoid concept repetition, we have officially defined the row space as the column space of the transpose, see Definition 2.13: R(A) = C(A⊤) ⊆ R n. From Lemma 4.11 applied to A ⊤, we immediately obtain Corollary 4.26. Let A be an m × n matrix. Then R(A) is a subspace of R n. We could compute R(A) = C(A⊤) via Gauss-Jordan elimination on A⊤, according to Theorem 4.25. But the nice thing is that our previous Gauss-Jordan elimination on A already provides us with all we need. The following lemma is the key. Lemma 4.27. Let A be an m × n matrix and M an invertible m × m matrix. Then R(A) = R(M A). Proof. We need to prove that C(A ⊤) = C((M A)⊤). Since (M A) ⊤ = A ⊤M ⊤ (Lemma 2.19) and N := M ⊤ is also invertible (Lemma 3.10), we set B := AT and prove C(B) = C(BN ) 130 as follows: v ∈ C(B) ⇕ v = Bx for some x ∈ R m ⇑ ⇓ ← y := N −1x ⇔ x := N y v = BN y for some y ∈ R m ⇕ v ∈ C(BN ). Using this, a basis of R(A) can be found as follows. Theorem 4.28. Let A be an m × n matrix, and let R0 in REF(j1, j2, . . . , jr) be the result of Gauss-Jordan elimination on A, according to Theorem 3.21. Then the first r rows of R0 form a basis of the row space R(A), and dim(R(A)) = r = rank(A). Proof. Gauss-Jordan elimination according to Theorem 3.21 provides us with a matrix R0 = M A in REF and the same row space as A, by Lemma 4.27. From R0, a basis of the row space can be read off immediately: R0 starts with r nonzero rows and ends with m − r zero rows. Hence, the r nonzero rows already span the row space, and they are also linearly independent, as each of them has a private nonzero (where the “downward step” occurs). Finally, r = rank(A) holds by Lemma 3.22. In our running example (4.4), the basis B is B = {[ 1 2 0 3 ] , [0 0 1 −2 ]} . This theorem has a very interesting consequence that also deserves to be called a the- orem, although it is now simply a corollary of Theorems 4.25 and 4.28. These two the- orems together imply that dim(C(A)) = dim(R(A)). Since R(A) = C(A ⊤), we also get dim(C(A)) = dim(C(A ⊤)), meaning that both A and A⊤ have the same number of inde- pendent columns, hence the same rank. This is the following result that is sometimes summarized as row rank equals columns rank. Theorem 4.29. Let A be an m × n matrix. Then rank(A) = rank(A ⊤). We have previously proved this in Section 2.1.4 for matrices of rank 1, but now we know that it holds for all matrices. We also obtain a particularly beautiful interpretation of the CR decomposition. Corollary 4.30. Let A = CR be the CR decomposition of A (Section 2.2.3). By Theorem 4.25, the columns of C form a basis of the column space C(A). By Theorem 4.28 together with Theorem 3.24, the rows of R form a basis of the row space R(A). Both spaces have the same dimension r = rank(A) = rank(A ⊤). 131 4.3.3 Nullspace We now get to the third fundamental subspace defined by a matrix A. Definition 4.31 (Nullspace). Let A be an m × n matrix. The nullspace of A is the set of all solutions of Ax = 0, N(A) = {x ∈ R n : Ax = 0} ⊆ R n. The nullspace is sometimes also denoted by Ker(A) and called the kernel of A. The rea- son is that N(A) is the kernel of the linear transformation T (x) = Ax; see Definition 2.31. Lemma 4.32. Let A be an m × n matrix. Then N(A) is a subspace of R n. Proof. Let v, w ∈ N(A), λ ∈ R. As in the proof of Lemma 4.11 we use that A realizes a linear transformation and directly get the subspace axioms in Definition 4.8. A(v + w) = Av︸︷︷︸ 0 + Aw︸︷︷︸ 0 = 0 ⇒ v + w ∈ N(A) and A(λv) = λ Av︸︷︷︸ 0 = 0 ⇒ λv ∈ N(A). Gauss-Jordan elimination also reveals the nullspace of A, with the help of the follow- ing lemma. This is actually Lemma 3.3 “reloaded”, with b = 0; back then, we did not know inverse matrices yet and therefore had to argue with undoing a row operation. Lemma 4.33. Let A be an m × n matrix and M an invertible m × m matrix. Then A and M A have the same nullspace N(A) = N(M A). Proof. For x ∈ R n, we have x ∈ N(A) ⇔ Ax = 0 ⇒ M Ax = M 0 ⇑ ⇓ M −1M A = M −10 ⇐ M Ax = 0 ⇔ x ∈ N(M A). To show how to extract a basis of N(A) = N(R0) from the matrix R0 = M A that we obtain from Gauss-Jordan elimination, we look at the running example (4.4) first. Here, R0 =   1 2 0 3 0 0 1 −2 0 0 0 0   . We observe that N(R0) = N(R) where R = [ 1 2 0 3 0 0 1 −2 ] 132 is obtained from R0 by removing the zero row at the end. Indeed, the system Rx = 0 is equivalent to R0x = 0, since the extra equations in R0x = 0 are of the form “0 = 0” and therefore always hold. Recall from the previous Section 4.3.2 that R is the matrix in the CR decomposition A = CR, and that the rows of R form a basis of the row space R(A). In the running example, the matrix R is in RREF(1, 3), hence we can write Rx = 0 as [1 0 0 1 ] ︸ ︷︷ ︸ I [ x1 x3 ] ︸ ︷︷ ︸ x(I) + [2 3 0 −2 ] ︸ ︷︷ ︸ Q [x2 x4 ] ︸ ︷︷ ︸ x(Q) = 0. The submatrix of R with the independent columns 1, 3 is the identity matrix, and we let Q be the submatrix of R with the dependent columns 2, 4. Furthermore, x(I) and x(Q) denote the subvectors of x “belonging” to I and Q. We therefore can further write Rx = 0 as x(I) = −Qx(Q). Solving this system is very easy: the free variables x(Q) can be substituted with arbi- trary real numbers. The values of the basic variables x(I) are then determined via x(I) = −Qx(Q). We have already encountered this “direct solution” approach in Section 3.5.2 for solving Ax = b when A is in REF. Here, the goal is different: we want to compute a basis of N(R) = N(A). It turns out that there is a basis of special solutions, the ones where x(Q) is set to one of the standard unit vectors; see Table 4.2. special solutions free variables [x2 x4 ] ︸ ︷︷ ︸ x(Q) [1 0 ] ︸︷︷︸ v1(Q) [0 1 ] ︸︷︷︸ v2(Q) basic variables [−2 −3 0 2 ] ︸ ︷︷ ︸ −Q [x2 x4 ] ︸ ︷︷ ︸ x(Q) = [x1 x3 ] ︸ ︷︷ ︸ x(I) [−2 0 ] ︸ ︷︷ ︸ v1(I) [−3 2 ] ︸ ︷︷ ︸ v2(I) nullspace equation 0 = [1 2 0 3 0 0 1 −2 ] ︸ ︷︷ ︸ R     x1 x2 x3 x4     ︸ ︷︷ ︸ x     −2 1 0 0     ︸ ︷︷ ︸ v1     −3 0 2 1     ︸ ︷︷ ︸ v2 Table 4.2: The special solutions v1 and v2 form a basis of the nullspace N(R) = N(A). Let’s double check on the running example (4.4) that v1, v2 as computed in Table 4.2 133 are indeed in N(A). For this, we verify that   1 2 0 3 2 4 1 4 3 6 2 5   ︸ ︷︷ ︸ A     −2 1 0 0     ︸ ︷︷ ︸ v1 = 0, and   1 2 0 3 2 4 1 4 3 6 2 5   ︸ ︷︷ ︸ A     −3 0 2 1     ︸ ︷︷ ︸ v2 = 0. But why is {v1, v2} a basis of N(R) = N(A) according to Definition 4.16? This follows from the general argument that we make next. The notation with the double indices might appear a bit frightening, but from the example before, it should be quite clear what happens. Lemma 4.34. Let R be an r × n matrix in RREF(j1, j2, . . . , .jr), meaning that R has inde- pendent columns j1 < j2 < · · · < jr that are equal to the standard unit vectors e1, e2, . . . , er (Definition 3.19). We let jr+1 < jr+2 < · · · < jn denote the indices of the dependent columns. The r × r submatrix of R formed by the independent columns is the identity matrix I. We let Q denote the r × (n − r) submatrix of R formed by the dependent columns. For a vector x ∈ R n, we let x(I) ∈ Rr and x(Q) ∈ Rn−r denote the subvectors x(I) =      xj1 xj2 ... xjr      ∈ R r and x(Q) =      xjr+1 xjr+2 ... xjn      ∈ R n−r of basic and free entries. Let v1, v2, . . . , vn−r ∈ R n be the n − r vectors defined via vi(Q) = ei and vi(I) = −Qvi(Q), i = 1, 2, . . . , n − r. (4.5) Then {v1, v2, . . . , vr} is a basis of N(R). Proof. By construction, all vectors satisfy the nullspace equation Rx = 0 in its equivalent version x(I) = −Qx(Q), so they are actually in the nullspace. We further need to check that the vectors are linearly independent which follows from the usual private nonzero entry argument. More concretely, already the subvectors vi(Q) are linearly independent, as they are different standard unit vectors. The full vectors are then linearly independent as well. Finally, we must show that every vector in N(R) is a linear combination of the vi’s. For this, take any vector x ∈ N(R). We claim that x = n−r∑ i=1 xjr+ivi (4.6) 134 is the desired linear combination. For the subvector of free entries, this reads as x(Q) = n−r∑ i=1 xjr+i vi(Q) ︸ ︷︷ ︸ =ei by (4.5) (4.7) and is therefore obvious by definition of the subvector and the vi’s. For the subvector of basic variables, we compute x(I) Rx=0 = −Qx(Q) (4.7) = −Q (n−r∑ i=1 xjr+ivi(Q) ) = n−r∑ i=1 xjr+i    −Qvi(Q) ︸ ︷︷ ︸ =vi(I) by (4.5)    = n−r∑ i=1 xjr+ivi(I). Together with (4.7), this shows the claim (4.6). In the third equality, we were able to “pull Q in” due to “matrix-vector multiplication = linear transformation”; this needs the generalization of Observation 2.26 to more vectors, as provided by Lemma 2.28. In summary, we obtain the following result. Theorem 4.35. Let A be an m × n matrix, and let R0 in REF(j1, j2, . . . , jr) be the result of Gauss-Jordan elimination on A, according to Theorem 3.21. Let R in RREF(j1, j2, . . . , jr) be the submatrix of R0 consisting of the first r rows. The vectors v1, v2, . . . .vn−r as constructed in Lemma 4.34 form a basis of N(A) = N(R0) = N(R), and therefore, dim(N(A)) = n − r = n − rank(A). Proof. By Definition 3.19 of REF and RREF, R is in RREF(j1, j2, . . . , jr), so Lemma 4.34 applies and yields a basis of N(R) with n − r vectors. As R0 differs from R only by extra zero rows, we have N(R) = N(R0). Finally, N(R0) = N(A) follows from Lemma 4.33, us- ing that R0 = M A with M invertible, according to Theorem 3.21. As before, r = rank(A) holds by Lemma 3.22. 4.3.4 Left nullspace Just like the row space of a matrix A is defined as the column space of the transpose A⊤, the left nullspace of A is defined as the nullspace of A⊤. Definition 4.36 (Left nullspace). Let A be an m × n matrix. The left nullspace of A is the set of all (row vector) solutions of yA = 0⊤, or equivalently, the (column vector) solutions of A ⊤y = 0, hence the nullspace of A ⊤: LN(A) := N(A ⊤) ⊆ R m. This is called left nullspace, since it contains the solutions of a system of equations yA = 0 where the (row) vector of variables is multiplied with A from the left. Lemma 4.32 applied to A ⊤ immediately yields 135 Lemma 4.37. Let A be an m × n matrix. Then LN(A) is a subspace of R m. Computing the left nullspace via Gauss-Jordan elimination on A ⊤ as described in the previous Section 4.3.3 is a natural way to proceed, but as for the row space, there is a way to derive a basis directly from Gauss-Jordan elimination on A. This involves the matrix M , the “memory” of the row operations being performed. Theorem 4.38. Let A be an m × n matrix, and let R0 = M A in REF(j1, j2, . . . , jr) be the result of Gauss-Jordan elimination on A according to Theorem 3.21. Then the last m − r rows wr+1, wr+2, . . . , wm of the m × m matrix M form a basis of LN(A), and therefore, dim(LN(A)) = m − r = m − rank(A). Proof. Since R0 ends with m − r zero rows, the last m − r rows of the matrix equation R0 = M A read as 0 =     |wr+1||wr+2|...|wm|      A. This shows that wr+1, wr+2, . . . , wm ∈ LN(A). Moreover, these vectors are linearly inde- pendent: Since M is invertible, its columns are linearly independent (Theorem 3.11), so rank(M ) = m; therefore also rank(M ⊤) = m (Theorem 4.29), so the rows of M are also linearly independent. It remains to show that wr+1, wr+2, . . . , wm is a basis. By applying Theorem 4.35 to A⊤, we already know dim(LN(A)) = dim(N(A⊤)) = m − rank(A ⊤) = m − rank(A) = m − r. Here we have used Theorem 4.29 together with the fact that A has rank r due to R0 in REF(j1, j2, . . . , jr). Hence, we have found m − r linearly independent vectors in the subspace LN(A) of dimension m − r. So these vectors must be a basis by Lemma 4.24 (i). Let’s check this in our running example (4.4) which provides the initial matrix A and the final matrix R0 in REF(1, 3). We still need the matrix M . You can verify that it is M =   0 2 −1 0 −3 2 1 −2 1   . From R0, we know that rank(A) = 2, so dim(LN(A)) = 3 − 2 = 1, and the last row of M forms a basis of LN(A): [1 −2 1]   1 2 0 3 2 4 1 4 3 6 2 5   = [ 0 0 0 ] . 136 4.3.5 The solution space of Ax = b Having introduced and computed the four fundamental subspaces of a matrix A, we finally come back to solving systems of linear equations. Using Gauss elimination and back substitution (Sections 3.2.2 and 3.2.1), we have seen how to compute the unique solution x of Ax = b when A is an invertible square matrix. In the general case, we have applied Gauss-Jordan elimination and direct solution (Sections 3.5.3 and 3.5.2) to either find some solution, or to conclude that there is no solu- tion. Here, we want to understand and compute the space of all solutions. Definition 4.39 (Solution space). Let A be an m × n matrix and b ∈ Rm. The set Sol(A, b) := {x ∈ R n : Ax = b} ⊆ R n is the solution space of Ax = b. If b ̸= 0, Sol(A, b) is not a subspace of R n, simply because it doesn’t contain the zero vector (see xLemma 4.9). Let’s look at the situation where A is a 2 × 1 matrix, so we have a system in one equation and two variables. As a concrete example, consider the system 2x + 3y = 6. The solutions of this, all pairs (x, y) satisfying 2x + 3y = 6, form a line in R 2, not contain- ing the origin; see Figure 4.3. Replacing the right-hand side 6 by 0 results in another line which contains the origin and is a subspace (the nullspace of the matrix [ 2 3] ; see Sec- tion 4.3.3). In this example, we see that Sol( [2 3] , [6]) and Sol([ 2 3] , [0]) = N( [2 3]) are parallel lines. xy2x + 3y = 62x + 3y = 0 Figure 4.3: Sol(A, b) for A = [2 3], b = [6] (upper line) and b = [0] (lower line). Here is the general picture: For b ̸= 0, and if there is a solution at all, Sol(A, b) is obtained by “shifting” the nullspace of A “away from the origin.” Theorem 4.40. Let A be an m × n matrix, b ∈ R m. Let s be some solution of Ax = b. Then Sol(A, b) = {s + x : x ∈ N(A)}. 137 Hence, we can also compute Sol(A, b), despite the fact that it is not a subspace. To describe all solutions, we just need some solution (see Section 3.5.2) and a basis of N(A) (see Section 4.3.3). Proof. We first show that every solution y ∈ Sol(A, b) is of the form s + x with x ∈ N(A). Indeed, we can write y as y = s + (y − s) ︸ ︷︷ ︸ x , and due to Ax = Ay − As = b − b = 0, we have x ∈ N(A). For the other direction, we show that every vector y of the form y = s + x with x ∈ N(A) is in Sol(A, b). For this, we compute Ay = As + Ax = b + 0 = b. The number of solutions. A system Ax = b of linear equations has two characteristic numbers: m, the number of equations, and n, the number of variables. Then A is an m × n matrix. But if we want to understand the solution space, there is a third important characteristic number r, the rank of A. This is revealed only by Gauss-Jordan elimination on A: r is the number of “downward steps” in the row echelon form of the resulting matrix R0; see Theorem 4.25. If there is a solution at all, the solution space is obtained by shifting the nullspace N(A) away from the origin (Theorem 4.40). We can then declare Sol(A, b) to be of the same dimension as the nullspace, and this dimension is n − r by Theorem 4.35. Sol(A, b) can be a point, a line, a plane,. . . , see Figure 4.4. xyzSol(A, b)N(A)xxyzxSol(A, b)N(A)vxyzxSol(A, b)N(A)vw Figure 4.4: A solution space of dimension 0—a point (left); a solution space of dimension 1—a line (middle); a solution space of dimension 2—a plane (right) Next, we want to understand in more detail in which cases there is a solution. As we will see, this typically depends on whether r = m or not. The case r = m corresponds to the situation where the matrix R0 resulting from Gauss- Jordan elimination (Theorem 3.21) is in reduced row echelon form RREF, with no zero 138 rows at the end. We can arrive at this case if A is a square matrix, or a short and wide ma- trix (n > m), see Figure 2.1. In the latter case, we call the system Ax = b underdetermined. If A is tall and skinny (n < m), the system is overdetermined and must have r < m. The reason is that r, the number of independent columns, cannot exceed n, the total number of columns. Lemma 4.41. Let A be an m × n matrix of rank r = m. Then Ax = b has a solution for every b ∈ R m. Proof. From Theorem 4.25, we know that dim(C(A)) = r = m, so C(A) is a subspace of R m of the same dimension m. Then we must have C(A) = R m. This seems obvious, but actually needs an argument. The argument is that every basis B of C(A) is a set of m independent vectors in R m and therefore also basis of Rm by Lemma 4.24 (i). Hence, Span(B) = C(A) = R m by Definition 4.16 of a basis. It follows that every b ∈ Rm is in C(A), and this is the same as saying that Ax = b has a solution. In contrast, if r < m, the column space C(A) has dimension smaller than m. Then, “almost all” vectors b ∈ R m are not in C(A) and the system Ax = b is unsolvable. For an illustration, consider Figure 2.3 (right) where the column space is a line in R 2. If we pick a “typical” b from R 2 (whatever that precisely means), we expect b to land outside of that line. This can properly be formalized, and under this formalization, a subspace of R m that has dimension smaller than m is a set of measure 0. Hence, if we define a typical b to be one that is not in a given set of measure 0, then typical systems Ax = b are unsolvable if r < m. For example, overdetermined systems (n < m) are typically unsolvable. Underdetermined systems (n > m) are solvable (and then have infinitely many solutions) if we have a “typical” A, one of full rank r = m. To argue that short and wide matrices with r < m are untypical and form a set of measure 0, we need determinants that will be introduced in the second part of the course. Similarly, square systems (n = m) are (uniquely) solvable for typical A. Affine subspaces. Generally, a shifted copy of a subspace in a vector space is called an affine subspace. In the context of affine subspaces, a “normal” subspace is also sometimes called linear subspace. There is a full theory of affine spaces which are essentially vector spaces without a distinguished origin.4 The elements of an affine space are typically called points, and they are simply locations in space whose positions are “absolute” and not defined relative to a special origin 0. 4.3.6 Summary Here we summarize the computations of the four fundamental subspaces of an m × n matrix A, and of the solution space Sol(A, b). Gauss-Jordan elimination (Theorem 3.21) 4https://en.wikipedia.org/wiki/Affine_space, accessed September 7, 2024 139 yields M A = R0, where R0 is in row echelon form REF(j1, j2, . . . , jr) for some 1 ≤ j1 < j2 < · · · < jr ≤ n, and M is an invertible m × m matrix, recording all the row operations that happened during elimination. M and R0 can be computed by performing Gauss-Jordan elimination on the first n columns of the extended m × (n + m) matrix [A|I]. This transforms A into M A = R0 and I into M I = M . From M and R0, bases of the four fundamental subspaces can be read off easily, see Figure 4.5 for our running example. LN(A) C(A) N(A) R(A)[ 1 0 ] ︸︷︷︸ v1(Q) [ 0 1 ] ︸︷︷︸ v2(Q)   1 2 3    0 1 2   [ −2 0 ] ︸ ︷︷ ︸ v1(I) [−3 2 ] ︸ ︷︷ ︸ v2(I) ↑ ↑ ↑ ↑ [ 1 −2 1] ←   0 2 −1 0 −3 2 1 −2 1   ︸ ︷︷ ︸ M   1 2 0 3 2 4 1 4 3 6 2 5   ︸ ︷︷ ︸ A =   1 2 0 3 0 0 1 −2 0 0 0 0   ︸ ︷︷ ︸ R0 in REF(1,3) → [ 1 2 0 3 ] → [ 0 0 1 −2 ] ︸ ︷︷ ︸ R in RREF(1,3) Figure 4.5: Computation of (bases of) the four fundamental subspaces of A, based on Gauss-Jordan elimination (M A = R0) Column space. Columns j1, j2, . . . , jr of A (the independent columns) form a basis of the column space C(A); see Theorem 4.25. Row space. The first r rows of R0 (the nonzero rows) define a matrix R in reduced row echelon form RREF(j1, j2, . . . , jr). This matrix coincides with the matrix R in the CR decomposition of A; see Theorem 3.24. Moreover, the rows of R are a basis of the row space R(A); see Theorem 4.28. Nullspace. For i = 1, 2, . . . , n − r, the i-th basis vector vi is obtained by setting the subvector vi(Q) of the n − r free variables (the ones corresponding to the dependent columns) to the i-th standard unit vector ei. The subvector vi(I) of basic variables is then computed via vi(I) = −Qvi(Q) = −Qei where Q is the submatrix of R containing the n − r dependent column; see Theorem 4.35. Note that −Qei is simply the negative of the i-th column of Q. 140 Left nullspace. The last m − r rows of M form a basis of the left nullspace LN(A); see Theorem 4.38. Table 4.3 summarizes the resulting dimensions (number of basis vectors) of the four subspaces. subspace name symbol dimension column space C(A) r row space R(A) r nullspace N(A) n − r left nullspace LN(A) m − r Table 4.3: Dimensions of the four fundamental subspaces of an m × n matrix A of rank(A) = r. Solution space of Ax = b. The solution space Sol(A, b) is a “shifted copy” of the nullspace (Theorem 4.40) and therefore also of dimension n − r, if there is a solution. For r = m, this is always the case (Lemma 4.41) but for r < m, it is typically not the case (see the discussion after Lemma 4.41). 141 Bibliography [BP98] Sergey Brin and Lawrence Page. The anatomy of a large-scale hypertex- tual web search engine. Computer Networks and ISDN Systems, 30(1):107– 117, 1998. https://www.sciencedirect.com/science/article/pii/ S016975529800110X. [CLRS22] Cormen, Thomas H., Leiserson, Charles E., Rivest, Ronald L., and Stein, Clifford. Introduction to Algorithms, Fourth Edition. The MIT Press, 2022. https://mitpress.mit.edu/9780262046305/ introduction-to-algorithms/. [CSB06] V. Claus, A. Schwill, and R. B ¨oving. Duden Informatik A - Z: Fachlexikon f ¨ur Studium, Ausbildung und Beruf, 4. Auflage. Dudenverlag, 2006. [Fel93] Michael R. Fellows. Computer science and mathematics in the elementary schools. In Harvey B. Keynes Naomi Fisher and Philip D. Wagreich, edi- tors, Mathematicians and Education Reform 1990–1991, volume 3 of CBMS Is- sues in Mathematics Education. American Mathematical Society, 1993. https: //ianparberry.com/research/cseducation/fellows1991.pdf. [Kap14] Manu Kapur. Productive failure in learning math. Cognitive Science, 38(5):1008– 1022, 2014. https://doi.org/10.1111/cogs.12107. [Str23] Gilbert Strang. Introduction to Linear Algebra (Sixth Edition). Wellesley- Cambridge Press, 2023. [War01] William P. Wardlaw. A generalized general associative law. Mathematics Mag- azine, 74(3):230–233, 2001. https://doi.org/10.1080/0025570X.2001. 11953069. 142 Index ∅, 33 ∞-norm, 26 0, 10 1, 28 1-norm, 26 absolute value, 23 abuse of notation, 10 affine combination, 18 affine space, 139 affine subspace, 139 algorithms, 71 applicable, 41 array, 71 associative, 12 associativity, 52 axiom, 60 axioms of a linear transformation, 60 axioms of a subspace, 117 axioms of a vector space, 114 back substitution, 72 base case, 62 basic variables, 133 basis, 121, 122 big-O-notation, 82 bits, 114 Bombay cat, 114 breed, 114 canonical, 104, 115 canonical basis, 124 Cartesian coordinate system, 10 citations, 69 claim, 100 class, 113 coefficient matrix, 68 coefficients, 115 collinear, 32, 35 column notation, 39 column space, 42 column vector, 10 commutative, 12 commutative diagrams, 60 complex numbers, 114 composition, 65, 94 computer matrix, 71 computer vector, 71 computing a vector space, 129 condition, 32 cone, 18 conic combination, 18 contraposition, 80 convex combination, 18 coordinate-wise, 11 coplanar, 35 corollary, 34 damping factor, 70 definition, 12 degree, 115 dependent, 43 determinants, 87, 139 diagonal, 40, 45 distributivity, 22, 52 domain, 57 double, 71 elimination, 73 143 elimination matrix, 74 empty set, 33 equivalent, 31, 33 Euclidean norm, 23 finitely generated, 127 float, 71 floating-point numbers, 71 follows, 34 forward substitution, 93 free variables, 133 fundamental subspaces, 129 generalized associativity, 54 Hadamard product, 22 identity, 40, 61 if, 34 if and only if, 34 image, 65, 130 implies, 33 in place, 77 independent, 43 indices, 10 induction hypothesis, 62 induction step, 62 interval, 29 inverse, 86 invertible, 86 isomorphic, 120 kernel, 65, 132 Kronecker delta, 40 left nullspace, 135 lemma, 27 line segment, 18 linear combination, 14, 121 linear subspace, 139 linear transformation, 59 linearity in both arguments, 22 linearly dependent, 31 linearly independent, 31, 122 link graph, 69 loop invariants, 127 lower triangular, 40 mammal, 113 matrix, 38 matrix multiplication, 48 matrix transposition, 45 matrix-matrix multiplication, 48 matrix-matrix product, 48 matrix-vector multiplication, 41 natural numbers, 10 negation, 80 nontrivial linear combination, 33 nullspace, 66, 132 numerical analysis, 71 observation, 22 only if, 34 orthogonal, 30 orthogonal matrices, 97 orthogonal projection, 59 outer product, 52 overdetermined, 139 overloading, 115 parallelogram, 11 permutation, 76 permutation matrix, 76, 95 perpendicular, 30 pivot, 75 points, 139 polynomials, 115 positive-definiteness, 22 private nonzero, 123, 124, 131, 134 Processing, 13 product, 41, 48 productive failure, 77 proof by induction, 62 Pythagorean theorem, 23 quadratic polynomial, 119 range, 57 rank, 43 144 real numbers, 10 real polynomials, 115 real vector space, 114 recursive algorithm, 127 reduced row echelon form, 103 replay, 93, 110 right-hand side, 68 rotation, 58 row addition, 74 row division, 105, 108 row echelon form, 102 row exchange, 75 row notation, 39 row operation, 79 row operation matrix, 79 row rank equals columns rank, 131 row space, 46 row subtraction, 74 row vector, 10 scalar, 14 scalar division, 25 scalar multiple, 14, 39 scalar multiplication, 14 scalar product, 22 scaling, 58 sequences, 10 set, 20 set of measure 0, 139 shear, 58 short and wide, 39 solution space, 137 span, 35, 122 sparse, 85 special solutions, 133 species, 113 square matrix, 39 standard unit vectors, 25 stretching, 58 stretching factors, 58 submatrix, 44 subset, 36 subspace, 117 sum, 12, 39 summation index, 19 symmetric, 40 symmetry, 22 system of linear equations, 68 taking out scalars, 22 tall and skinny, 39 trace, 120 transpose, 45 trivial linear combination, 35 tuples, 10 two-dimensional array, 71 types, 71 underdetermined, 139 union, 120 unit circle, 24 unit cube, 59 unit monomials, 122 unit vector, 24 upper triangular, 40, 72 vector addition, 11 vector of variables, 68 vector space, 114 vector-matrix multiplication, 51 weighted, 70 without constant term, 119 zero matrix, 39 zero polynomial, 115 zero vector, 10 145","libVersion":"0.5.0","langs":""}