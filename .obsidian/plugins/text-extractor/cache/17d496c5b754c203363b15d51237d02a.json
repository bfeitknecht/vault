{"path":"sem4/CN/PV/sumamries/CN-summary-bfunk.pdf","text":"The Internet and Its Layers bfunk December 15, 2024 Disclaimer & Information To differentiate the content covered a bit more we use colors as follows: Paradigms This color is mostly reserved for paradigms. They solely encapsulate an idea or approach for a specific problem and are often used in protocols. Protocols This color is reserved for protocols used, such as TCP or UDP. Algorithms This color is reserved for algorithms, such as Bloom-Filter or Dijkstra. Security & Problems This color is reserved for problems, which can be exploited and thus have an impact on cyber security. While I endeavored to provide as precise information as possible, I cannot guarantee the correctness nor completeness of this overview. The Internet The internet communication can be decomposed in 5 independent layers. 1. Application Layer 2. Transport Layer 3. Network Layer 4. Link Layer 5. Physical Layer Each layer uses the services of the layer below and offers services to the layer above. 1 Application Layer This layer provides network services to devices. It does that by 1. Defining domain names and translation to IP-addresses Domain Naming System (DNS) The internet has two global systems for addressing hosts (IP) and naming hosts (domain names). DNS is used to translate between the two. It is a many-to-many relationship This is analogous to a telephone book, where the IP-addressed correspond the phone numbers and the name corresponds to the domain names. The naming system is structured hierarchically from right to left. That is, the top-level domain (TPD) is the most right identifier, followed by the second-level domain (2LD). example www .... ethz. ch . domain ... second-level domain top-level domain root managed by ethz ict service SWITCH IANA The dot ”.” represents the root which is managed by Internet Assigned Number Authority (IANA). DNS now maps a domain name to IP address by requesting the IP address by the authority responsible for the domain. This can be done in two ways: • Iteratively : Here the local DNS server does all the work and first asks the TLD for an IP-address and then 2LD... • Recursively : With this query each domain recursively asks the sub-domain for the IP-address and returns the whole IP-address. Root servers only respond to iterative queries. It is common that the root server, as well as the TPD are replicated to provide reliability. This is done using BGP anycast. DNS Server also may use caching to reduce resolution times. Addresses in the cache can directly be resolved. Authoritive name servers can influence how long an entry is kept in the cache by associating an time-to-live (TTL) to each records. 2. Defining Syntax of data-transfers and how it is transmitted Hypertext Transfer Protocol (HTTP) HTTP is a text-based, bidirectional and stateless protocol used for transmitting hypertext requests and information between clients and web servers. It defines the syntax for requests as follows: [Method] [URL] [HTTP Version] [Header1: Value1] [Header2: Value2] ... [Blank Line] Where: • Method : The HTTP method to be applied to the resource. (GET , POST, PUT, ... ) • URL : The path to the resource being requested, which can also include query parameters. • HTTP Version: The version of the HTTP protocol being used (HTTP/1.1, ...) • Header : Key-value pairs that provide additional information about the request, such as content type, user- agent, and host. • Blank Line : An empty line that separates the headers from the body of the request (if there is a body). This line indicates the end of the header section. Similarly, the response is defined as: [HTTP Version] [Status Code] [Reason Phrase] [Header1: Value1] [Header2: Value2] ... [Blank Line] [Optional Body] Where: • Status Code : A three-digit numeric code that represents the result of the server’s attempt to process the request. 1XX - information 2XX - success (i.e. 200 OK) 2 3XX - redirection (i.e. 301 - moved permamently, 303 moved temporarily ) 4XX - client error (i.e. 404 Not Found) 5XX - server error (i.e. 505 Not Found) • Reason Phrase : A human-readable phrase providing a brief explanation of the status code. • Header : Provide additional information about the server’s response. Headers are key-value pairs separated by a colon • Optional Body : The body of an HTTP response contains the actual data or content being sent by the server to the client. Since HTTP is stateless no information of users can be stored on server-side. Thus cookies are introduced to store information client-side, which are sent with the request, if a connection has been established before. HTTP and DNS can be combined in URL and thus provides the two requirements needed by the application layer. Uniform Resource Locator (URL) URL is strictly speaking not a protocol. It standardises syntax and incorporates DNS as part of its structure to specify how to access the resource. It further allows to choose HTTP as a method to access said data. It follows the syntax: protocol :// domain_name:port/path?query#fragment Where: • protocol : Method used to access the resource. (HTTP, HTTPS, FTP, ...) • domain_name : The address of the server where the resource is hosted. (DNS name or IP address) • port : 80 (standard for HTTP), 443 (standard for HTTPS) • path : Specifies the specific resource or directory on the server. (/articles , ...) • query : A string, which contains data to be sent to the server, typically used for search parameters or input data in web forms. (?author=alice, ...) Packet- vs Circuit Switching An application may want to choose two different modes of connections: Packet switching or circuit switching. Both are not implemented by the application layer itself. • Packet switching : Data is fragmented into packets and each packet is sent to the destination separately. Each packet may choose different paths. When the data stream is unpredictable (i.e. sending e-mails, chatting, ...) this method is preferred • Circuit switching : An path is established and teh data flows continuously without interruptions or sharing the network with other users. When we have predictable connections (i.e. video streaming) this method is preferred. 3 Loading Webpages A web page consists of many different resources. For each of them the client has to request the specified resource and it may include requests for further requests. Since dependencies cannot be generally known the client has to be conservative and wait for answer of specific requests. An directed acyclic graph (DAC) is induced. Loading all resources may take a very long time and predicting loading time is hard, since the graph may change with each interaction of the user. Topological Sort There are several approaches to make webpages faster on the level of application: • Restructure web page : Compress or inline resources • Tag asynchronous resources : This allows to simplify the DAC • Increasing bandwidth : This approach only helps when the age accessed is very resource-intensive. If that is not given, there will be no decrease in time. • Simplify network protocols : Since every resource sent by a packets needs a handshake we need 2n RTT for n packets. We can improve this by – Enabling M concurrent packets : 2n M – Keeping connection open for all requests : n + 1 – Pipelining : 2 Loading Videos (ABR) We discussed the following approach on how to handle video streaming. Adaptive Bitrate Streaming (ABR) is a technique used in video streaming that allows the quality of a video stream to adjust dynamically based on the user’s network conditions. The goal is to provide the best possible viewing experience without buffering or interruptions. This is done by optimising the following: • Maximise the downloaded video quality. • Minimise the rebuffering. • Minimise the startup delay. • Maximise the stream stability. • Minimize the bandwidth wastage/power drain of the device. The high-level idea is to break down the whole video into chunks of a few seconds. Each chunk may have a different resolution and thus has a different size. The size is then indirectly adjusted by changing the resolution of each chunk. How and when the size is changed is up to the decision of the algorithm. We covered two main types: • Buffer-based algorithm : The resolution is decided upon considering the status of the buffer. • Latency-based algorithm : The resolution is decided upon considering the latency of chunks. DASH (Dynamic Adaptive Streaming over HTTP) is a specific implementation of ABR that is an open standard developed by the Moving Picture Experts Group (MPEG). It allows streaming of media content over HTTP. The DASH client (video player) uses an MPD file (Media Presentation Description; an XML file describing the structure of the media) to determine which segment to download next, based on the current network conditions, and can switch between different bitrates as needed. DASH is widely supported Transport Layer The layer provides: 1. Reliability : We have defined that reliability consists of 4 different aspects. Those are: • correctness : A packet is always resent if the previous packet was lost or corrupted • timeliness : Minimising time for data transfer • efficiency : Minimise bandwidth used • fairness : Balancing the bandwidth among concurrent transfers. 4 In class, we mainly focused on correctness. One way on achieving this is using ARQ, which simply re-sends lost packages. Automatic Repeat reQuest (ARQ) ARQ is the general concept of providing reliability by using re-transmissions. A packet is re-send when the timer runs out for the corresponding acknowledgment (ACK) packet. Depending on the context we have to use different Acks. We have seen three types in class: • Cumulative ACK: Acknowledges all packets up to the highest received in-order packet. • Individual ACK: Acknowledges specific packets that have been received, useful for indicating out-of-order packets. • Full ACK: Combination of cumulative and selective ACKs. It provides the reliability in the transport layer and is used in the Sliding window paradigm as well as in TCP, QUIC. (UDP does not use ARQ) A very simple instantiation of ARQ is the Stop-and-Wait Protocol Stop-and-Wait Protocol In this version the sender transmits a single packet and waits for an acknowledgment (ACK) before sending the next packet. If an ACK is not received within a certain timeout period, the packet is retransmitted. This is simple but ineﬀicient for high-latency networks. We have also seen two other versions of ARQ: Go-Back-N Here, the sender can send multiple packets within a window size (N ) without waiting for an ACK for each one. However, if a packet is lost or an error is detected, all subsequent packets are retransmitted. Thus, this version relies on cumulative ACKs. Selective Repeat The sender also sends multiple packets within a window size. Unlike Go-Back-N, only the specific lost or erroneous packets are retransmitted based on individual ACKs and timers, making it more eﬀicient. This version normally uses only individual ACKs. 2. Flow control : With this aspect we want to ensure to not overwhelm the receiver with too many packets. Receiver Window Size (rwnd) The basic idea is that the receiver communicates an upper limit, the so-called Receiver Window Size. The sender only limits it’s output given by that number. This basic idea provides a mechanism to prevent overwhelming the receiver. 3. Congestion control : With this aspect we want to ensure to not overwhelm the network. Congestion Window Size (cwnd) The idea is to adjusts the rate of packet transmission based on network conditions to prevent congestion. This is then incorporated by the Congestion Window Size, which gives an upper limit for the amount of packets. This limit changes dynamically and relies on time-outs and ACKs. 5 Sliding Window The Sliding Window Paradigm now uses ARQ, Receiver- and Congestion Window Size to ensure all presented conditions are met. As defined, we have: • sender window size (swnd) : Defines how many packets are sent concurrently • receiver window size (rwnd) : Defines how many packets are accepted by the receiver • congestion window size (cwnd) : Is given by the capacity of the network. Then the main idea is to make the sender output dependant on the two metrics we have defined before: min(cwnd, rwnd) A problem, which occurs when a protocol uses the sliding window approach is Head of Line Blocking. This describes the effect that packets have to wait for a previous packet to be acknowledged in order to be received. This can be partially prevented by introducing buffers but not fully as buffer sizes are limited (and thus finite). We have seen the following examples of protocols, which provide the mentioned requirements Transmission Control Protocol (TCP) TCP uses • 3-Way Handshake to establish connection • ARQ : Selective Repeat with cumulative ACKs and random Initial Sequence Number (ISN) • Fast Retransmit on 3 duplicate ACKs • Adaptive Congestion Control with slow start. The first step in a transmission is to establish a connection. In the case of TCP this happens with the 3-Way Handshake. 3-Way Handshake The TCP handshake is a three-step process that establishes a reliable connection between a client and a server. 1. SYN : The process begins with the client sending a TCP segment with the SYN flag set to the server. This segment includes initial sequence number (ISN) that identifies the starting sequence number for data exchange. 2. SYN-ACK : Upon receiving the SYN segment, the server responds with a TCP segment where both the SYN and ACK flags are set. The server acknowledges the receipt of the client’s SYN segment and sends its own initial sequence number (ISN) in the ACK segment. 3. ACK : Finally, the client responds with an ACK segment to acknowledge the receipt of the server’s SYN- ACK segment. The ACK segment includes the acknowledgment number, which is set to one more than the received sequence number (i.e., the ISN of the server). This handshake needs 1 RTT. However, this connection is not encrypted. If that is required one also need the TLS handshake, which needs an additional 2 RTTs. Thus, in total we have at least 3 RTTs. 6 Congestion Control Algorithm (CCA) The CCA employed by TCP is loss-based and thus cannot achieve the Kleinrock Optimum (Latency-based algorithm can). Furthermore, it also is inherently Round-Trip Time (RTT) unfair. This happens since transmissions with smaller RTTs increase the swnd faster. It is classified as an Additive-Increade-Multiplicative-Decrease (AIMD) CCA. It works as follows: The algorithm initialises the variables cwnd = 1, ssthresh = infinite Then it uses slow-start, which lets the window size exponentially increase until it reaches the slow start threshold (ssthresh). The increase is exponentially since the cwnd is incremented for every received ACK. Which means with a swnd if x we also (ideally) get x ACKs and thus increment swnd to 2x. if (cwnd < ssthresh): cwnd = cwnd + 1 When we are not in slow-start we are in congestion avoidance mode and increase the cwnd only linearly. else: cwnd = cwnd + 1/ cwnd dup_ack = 0 Multiplicative Decrease defines the rate for which we indirectly regulate cwnd. It uses timeouts as an indicator for strong congestion. As a consequence it halves the ssthresh, which lowers the phase of exponentially-fast increase. On Timeout: ssthresh = cwnd /2 cwnd = 1 Fast Recovery uses duplicate ACKs for an indication of a weak congestion. Duplicate ACKs received: dup_ack ++; if (dup_ack >= 3): ssthresh = cwnd /2 cwnd = ssthresh This results in a behaviour, which might look as follows: Here the blue parts are defined by the slow-start, which happens in the beginning or after a timeout (red). After the slow-start reaches its threshold it will increase the window size linearly, which is indicated in green. This also occurs after a receiving 3 duplicate ACKs (orange). A behaviour which is not reflected is the change of ssthresh Each data segment contains a header and the whole package is encapsulated by the IP-Layer. This means the whole package is part of the IP Data segment. Another alternative to TCP is QUIC. 7 Quick UDP Internet Connections (QUIC) QUIC builds up on the UDP, while the actual QUIC header and data is stored in the UDP data segment. This makes an UDP package indistinguishable from an QUIC Package at first glance. This is done on purpose since otherwise Middleboxes just would throw them away otherwise. It has the following advantages over TCP: • Faster connection establishment : QUIC has an 1-RTT Handshake and 0-RTT for all subsequent connections 1-RTT Handshake The Handshake involves only one request and one response, which is enough to fully establish an encrypted connection. 0-RTT Handshake The client sends a Client Hello packet to the server. This packet includes the initial data and requests a connection to the server. If supported and allowed by the server, the client can send encrypted data (early data) along with the Client Hello. This allows the client to start sending application data immediately without waiting for the handshake to complete. • Better privacy and mobility : CIDS are independant of the IP address, which allows better mobility and security, as the address can be hidden. For that UDP sockts are used. • Prevents Head-of-Line Blocking by introducing logical independant streams The aforementioned UDP is fast and lightweight, but does not offer any of the requirements defined in this course. User Datagram Protocol (UDP) UDP is a transport layer protocol that operates on top of IP (or other network layer protocols). It is often used for applications where low latency and quick data transmission are more critical than guaranteed delivery and error recovery. It is a simple and lightweight protocol, lacking the built-in error checking and retransmission mechanisms found in other transport layer protocols like TCP . The only two properties it has are: • Demultiplexing • Checksums (optional) To bridge the OS and the network sockets are used. Sockets Sockets provide an abstraction layer that facilitates communication between applications and the underlying network protocols (UDP or TCP), managed by the operating system. They enable applications to establish communication sessions, send and receive data, and manage network connections. Here ports are numerical identifiers (16-bit unsigned integers) associated with each socket on a host. They help in multiplexing multiple network connections on a single host, distinguishing between different applications or services running concurrently. We have covered two types of sockets 1. UDP Sockets (SOCK_DGRAM): UDP sockets operate using the Datagram Model, as discussed in the Network Layer. This model is connectionless, meaning each datagram is independently routed and delivered to its destination without establishing a persistent connection. They provide a lightweight, low-latency communication method suitable for applications where occasional packet loss is acceptable, such as real-time audio/video streaming and DNS (Domain Name System) queries. 2. TCP Sockets (SOCK_STREAM): TCP sockets operate using the Virtual Circuit Model, which provides a reli- able, connection-oriented communication channel between two endpoints. They establish a virtual circuit through a handshake process before data exchange begins, ensuring reliable and ordered delivery of data. The sockets are used in applications requiring guaranteed delivery, in-order delivery of data, and congestion control. Examples include web browsing (HTTP), file transfer (FTP), and email (SMTP). Each application communicates via a socket assigned to a specific port. The operating system maintains a mapping between applications and their respective sockets, ensuring data from the network is delivered to the correct application. 8 Network Layer We covered two different models for this layer. Those are: 1. Datagram Model : In this model each segment stores the destination address and switches use forwarding tables to send the packet to the desired address. 2. Virtual Circuit Model : After establishing a circuit between sender and receiver, all packets are sent along the same path and in-order. Regardless of the model used store-and-forward packet switching is implemented. Which means that routers receive a complete packet, storing it temporarily if necessary before forwarding it onwards. The buffer is typically a FIFO queue. In case of congestion, which means that the buffer is full, packets are discarded. Deciding where to send packets is an expensive operation and called control plane of the network or routing. Sending packets in their respective directions is a cheap, local operation called data plane of the network or forwarding. With that in mind, we can describe what the layer provides: 1. Logical Addressing Internet Protocol (IP) IP provides logical addressing through IP addresses. An IP address is a unique identifier assigned to each device on a network, enabling it to be located and communicated with. IP enables the routing of packets by providing the necessary information for routing protocols and devices to make decisions about packet forwarding. This is done by encapsulating data into packets with an IP header. The header contains information necessary for the routing algorithms. This includes the source and destination address, as well as additional fields such as time to live (TTL) or protocol type. The assignment of the IP-Address of a device is done by DHCP. Dynamic Host Configuration Protocol (DHCP) DHCP uses the following procedure to allocate IP-Addresses to devices. 1. Discover : When a device (client) connects to the network, it sends a broadcast message called a DHCP Discover message to locate DHCP servers. 2. Offer : DHCP servers on the network respond with a DHCP Offer message that includes an available IP address and other configuration details. 3. Request : The client selects one of the offered IP addresses and sends a DHCP Request message back to the chosen server, requesting to lease the offered IP address. 4. Acknowledge : The DHCP server sends a DHCP Acknowledge message to the client, confirming the lease of the IP address and providing the full set of network configuration details. The allocation can happen both static and dynamic. • Dynamic Allocation : Devices on a network request an IP address from a DHCP server, which assigns an available IP address from a predefined pool. This assignment is typically temporary and valid for a specific period (known as the lease time). • Static Allocation (Reserved IPs) : DHCP can also reserve specific IP addresses for particular devices based on their MAC addresses, ensuring that they receive the same IP address each time they connect to the network. 2. Routing : Routing is a complex construct and we mainly discussed two big topics. The delivery model as well as according protocols. Different delivery models determine how packets are sent and received across networks. 9 Routing Models The modes, which were discussed in class, are: • Unicast : A one-to-one communication. Each packet has a single and unique destination address. • Broadcast : A one-to-all communication. The sender does not specify any particular destination address but instead sends the packet to a special broadcast address. Routers or switches in the network propagate the broadcast packet to all connected devices. • Multicast : A one-to-many communication. The sender specifies a multicast group address as the destination, and routers in the network forward packets only to those nodes that have joined the multicast group. • Anycast : A one-to-nearest communication. Multiple nodes share the same anycast address, but the routing infrastructure forwards the packet to the nearest instance of the anycast destination based on routing metrics such as shortest path or lowest latency. Various routing rrotocols are then used to enable exchange of packets. Routing Protocols We want to the following properties for routing algorithms: 1. Correctness : Finds paths that work. 2. Efficient paths : Given path is minimal for some metric. 3. Fair paths : Doesn’t starve any nodes. 4. Fast convergence : Recovers after changes eventually. 5. Scalability : Works well as network grows large. Those conditions have to be satisfied in a decentralised and distributed setting. Concretely that means that there is no controller and nodes work concurrently. Information is shared by message passing but there is the possibility of link failures. Routing protocols can broadly be classified into two different sets. Protocols that deal with Inter-Domain Routing are used to distribute data between different Autonomous Systems (ASes). These protocols are called Exterior Gateway Protocols (EGPs). Intra-Domain Routing, on the other hand, deals with routing within a single AS. These protocols are referred to as Interior Gateway Protocols (IGPs). What is an AS? An Autonomous System (AS) is a network or a collection of networks operated by a single organisation or administrative entity. This entity determines the internal routing protocols and controls the routing information shared with neighboring ASes. Each AS is assigned a unique identifier called an Autonomous System Number (ASN). The ASN is used for BGP. An example is the AS3303, which is the AS managed by Swisscom. Examples of policy decisions can be found in Business Relationship 10 Interior Gateway Protocol (IGP) Internal Border Gateway Protocol (iBGP) iBGP is a form of BGP used within a single AS. Its primary function is to distribute routing information learned from eBGP peers to other routers within the same AS. It does not make routing decisions or perform actual routing; instead, it helps propagate routing information. Link-State Protocols Intermediate System to Intermediate System (IS-IS) Open Shortest Path First (OSPF) Distance-Vector Protocols Routing Information Protocol (RIP) Equal-Cost Multi-Path Routing (ECMP) Flooding External Gateway Protocol (EGP) EGP is a term used to describe protocols that are used to exchange data between different autonomous systems (ASes). Historically, EGP was also the name of an early protocol used for this purpose, but it has largely been replaced by BGP. Path-Vector Protocols A path-vector protocol maintains information about the entire path that a route has taken across the network. This information is then needed for making routing decisions and preventing loops. Border Gateway Protocol (BGP) BGP is responsible for selecting the best routes based on the information received from eBGP and iBGP. The selection process involves evaluating various attributes and making routing decisions based on policy and preference. External Border Gateway Protocol (eBGP) eBGP is a specific use of BGP for routing between different autonomous systems. It is the instance of BGP used to exchange routing information between separate ASes. It does not make routing decisions or perform actual routing; instead, it helps propagate routing information. 11 Business Relationship Each Autonomous System (AS) customizes BGP parameters according to its own policies and objectives. The primary goal of these adjustments is often to maximize monetary gain, which means that BGP configurations are optimized to make decisions that either reduce costs or increase revenue for the AS. As a result, BGP can be seen as a ”follow-the-money protocol.” For example, consider a device connected within AS1 that wants to access a server in AS2. The entity managing AS1 provides connectivity to any IP address on the Internet, regardless of whether it is within the same AS or not. However, depending on the business relationship between AS1 and AS2, AS1 might adjust BGP parameters to influence routing decisions in a way that aligns with its financial interests. This could involve prioritising routes that are more cost-effective or generating additional revenue. ASes use the 95th percentile billing method. This means that an AS pays for traﬀic based on mea- surements taken over a specified period, excluding the top 5% of traﬀic volumes to mitigate the effects of high-traﬀic peaks. This approach encourages ASes to balance and optimize their traﬀic patterns to manage costs effectively. Consider the following constellation. AS1 AS2 AS3 AS4 AS5 AS6 AS7 Peer Provider Customer Peer An AS prefers to send packages in the following order 1. Customers : The managing entity earn money. I.e. AS2 earns money if it sends packets to AS5, as AS5 pays for it. 2. Peers : The managing entity avoids paying money. I.e. AS2 can avoid paying AS1, when AS5 request a connection with AS6 by using the peer-to-peer link with AS3. 3. Providers : The managing entity has to pay in order to send the package. I.e. AS2 has to pay AS1 if a packet goes through AS1. Since the managing entity can decide what Addresses are actually advertised, it can also ”hide” a connection to a specific AS from other ASes. I.e. it makes sense for AS2 to ”hide” the connection to AS1 from AS3. Otherwise AS3 would send packets to AS1 for free to AS2 and then AS2 pays for sending (foreign) packets through AS1. However, it makes sense that AS5 knows that a connection to AS1 through AS2 exists, since AS5 has to pay for it anyways. One can construct a table. It is read as follows: If a path is advertised by X (column) do you propagate it to Y (row)? Customer Peer Provider Customer yes yes yes Peer yes no no Provider yes no no 3. Packet Forwarding 12 Longest Matching Prefix In a network, IP addresses belonging to the same network share a common prefix. Routers decide the next hop with the following strategy: 1. For each incoming packet, routers identify all matching prefixes in the routing table - those that contain the destination address. 2. Among these matching prefixes, routers select the longest (most specific) prefix. 3. The packet is then forwarded to the next-hop router associated with this longest matching prefix. What does most specific prefix mean? Consider the address A.B.C.D/E with the subnet mask E. Consider the first E bits from the left and choose the option that has the most fitting bits. For example: A B C D E 1 192 168 1 0 - 11000000. 10101000. 00000001. 10000010 2 192 168 0 0 16 11000000. 10101000. 00000000. 00000000 3 192 168 1 0 24 11000000. 10101000. 00000001. 00000000 4 192 168 1 128 25 11000000. 10101000. 00000001. 10000000 5 192 164 1 128 16 11000000. 11000010. 00000001. 10000000 Consider that we want to find the next hop for address 1 in the table. Notice that we don’t have a subnet mask (i.e. E). Then we compare the addresses in the routing table with our address. Above matching bits are high- lighted. The first E bits have to match, as indicated with the underlining. With that we already can exclude option 5. Otherwise an address is not in the respective range. Notice that in 2 there are more bits matching than allowed by the subnet mask. This is because the mask indicates how many bits are actually used for the network itself. (The rest is used for the host) We pick then the address which has the longest matching prefix. Sub-prefix hijacking Sub-prefix hijacking is a specific form of prefix hijacking where an attacker announces a more specific subnet of an existing IP prefix Since announcement of addresses are not validated in BGP, any AS can announce an arbitrarily specific address. This exploits the longest matching prefix and makes it possible to redirect the traﬀic to a desired network. This can be used for: • Blackhole : Data traﬀic is discarded • Snooping : Data traﬀic is inspected, then redirected 4. Fragmentation & Reassembly Fragmentation Since different paths have different MTU (Maximum Transmission Unit) sizes a packet might have to be fragmented into multiple subparts to adhere to the limits. This is mostly done by the router. IPv4 does implement fragmentation, whereas IPv6 only will send an ”Packet Too Big” error-message if the MTU is exceeded. To make fragmentation possible the header with different fields are used. Only at the final destination host the fragmented packets are reassembled. (a) Identification Field : This field uniquely identifies each datagram. If one packet gets fragmented, all the subpart will share the same identification field. (b) Fragment Offset Field : This field indicates the position of the current fragment within the original datagram. (c) Fragment Control Bits : These bits include flags such as the ”More Fragments” (MF) flag and the ”Don’t Fragment” (DF) flag. The MF flag indicates whether more fragments of the original datagram are to follow, while the (DF) flag instructs routers not to fragment the datagram if it exceeds the MTU of the outgoing interface. The following Protocols satisfy the requirements and were presented in class: 13 Internet Protocol version 4 (IPv4) Uses both DHCP and ARD 1. Logical Addressing : IPv4 uses 32-bit addresses and thus can host around 4.3 billion addresses. This is not suﬀicient. This is expanded by IPv6 or Network Address Translation (NAT). Network Address Translation (NAT) NAT allows multiple devices on a local network to share a single public IP address when accessing external networks. Routers may include NAT functionality as part of their firmware or software. These routers manage the translation of IP addresses between a private internal network and the public external network. Sending a packet works as follows: 1. A device creates an IP-Packet and sends it to the router. 2. The router checks its NAT table to see if there is an existing mapping for the internal IP address and port. If not, it creates a new entry. 3. The router changes the following information and sends the packet with the new information. • Change Source IP Address: The router replaces the private IP address (192.168.1.2) with its own public IP address (e.g., 203.0.113.1). • Change Source Port Number: The router replaces the private port number with a new port number that is unique in the NAT table (e.g., 54321). The router can now identify the specific datagram by the port since that is unique for every connection. With NAT, given that the router is powerful enough, it could host 49,152 connections. (One for each port available.) 2. Routing 3. Packet Forwarding : IPv4 uses longest matching prefix. 4. Fragmentation and Reassembly : The procedure is done as described in fragmentation. IPv4 packets have a minimum fragment size of 8 bytes (because the fragment offset field in the IPv4 header is measured in 8-byte units). This sets a practical lower bound on how small a fragment can be. This also means a packet cannot be fragmented arbitrarily many times. Internet Protocol version 6 (IPv6) Only uses DHCP optionally and uses Neighbor Discovery Protocol (NDP) instead of ARD. However, this was not covered in class. 1. Logical Addressing : IPv6 uses 128-bit addresses, allowing for a virtually unlimited number of unique IP addresses - approximately 340 undecillion (3.4 · 1038) addresses. 2. Routing 3. Packet Forwarding : IPv6 uses longest matching prefix. 4. Fragmentation and Reassembly : IPv6 does not use fragmentation. An Path MTU Discovery (PMTUD) is used to determine the maximum MTU. If this is exceeded an error message is sent to the original host (sender) by the router who detects it. It is also possible for devices to use both IPv4 and IPv6. This is called Dual-Stack 14 Dual-Stack A Dual-Stack device has the following advantages: • Reachability: Ensures devices can communicate with both IPv4-only and IPv6-only systems. • Reliability: Provides an additional layer of reliability by allowing fallback to IPv4 if IPv6 is unavailable or vice versa. • Future-Proof : Prepares networks and devices for the eventual widespread adoption of IPv6, ensuring long-term viability. • Optimized Performance : Allows the use of IPv6’s enhanced features (such as larger address space and improved routing) when available, potentially improving network eﬀiciency and performance. • Security Improvements : Enables the use of IPv6’s built-in security features, like IPsec, while still supporting existing IPv4 security mechanisms. A new protocol is SCION, which was developed at ETH and introduces some fundamental changes. SCION SCION divides the internet in so-called Isolation Domains (ISD). Each domain is managed by the ISD Core, which at the same time is the only connection for any device within the ISD to other ISDs. The core defines the Trust Root Configuration (TRC), which contains the root cryptographic keys to verify ISD operations. ISD Core AS AS Border Router Beacon Router Path Server ISD Core AS AS Beacon Server The Beacon Routers initiate periodically a Path-Segment Construction Beacon (PCB), which gets forwarded by the Border Routers . When a Border Router receives such an PCB, it checks the authenticity. This is done by • Verifying the cryptographic signatures associated with the path segments in the PCB. Each signature is checked using the public key of the AS or router that signed that segment. • Checking that the AS identifiers in the PCB match known, valid and trusted ASes. • Checking timestamps to ensure that the PCB is recent and has not expired. If authentication is completed the down-stream path is stored in the Path Server and stored Beacon Server . . The Beacon Server periodically initiates PCB to neighbouring ASes through Border Routers. Constructing a Path : When a device in a specific AS wants to connect to an IP it asks the Path Server for possible paths. If the path is known, i.e. the according PCB is stored in the Path Server it returns possible paths, from which the device can choose from and optimize arbitrary metrics. If that is not the case the Path Server requests the missing path segments by the Path Server in the Core ISD. 15 Dynamic Host Configuration Protocol (DHCP) DHCP dynamically assigns IP addresses to devices on a network, simplifying network administration and resource alloca- tion. 16 Link Layer The link layer is responsible for transferring data between adjacent network nodes within the same local area network (LAN) or between connected devices. Its primary functions include • Framing : Framing in refers to the process of dividing the continuous stream of data from the network layer into manageable units called frames. These frames are used to encapsulate data with the necessary control information to ensure reliable communication. Framing is crucial for error detection, synchronisation, addressing, and flow control. We covered three different versions on how to address this problem: 1. Byte Count : Each frame has as length field. 2. Flags : Uses flag pattern ‘01111110‘ at beginning and end. 3. Stuffy Bits : Use escape pattern to escape flags appearing in the payload. • Addressing : Addressing is primarily achieved through the use of Media Access Control (MAC) addresses, which uniquely identify each device on a local network. Protocols like ARP assist in mapping higher-layer addresses to MAC addresses. Address Resolution Protocol (ARP) This protocol resolves the hardware (MAC) addresses of devices within the same network. ARP resolves this address by a two-step procedure. 1. REQUEST : After checking the ARP-cache and the wanted MAC address is not present, the sender broadcasts (flooding) an ARP request onto the local network segment. 2. REPLY : Upon receiving the ARP request, the target device (the host with the specified IP address) responds with an ARP reply. However, if a device wants to send something on another network, ARP won’t work. Then the packet gets encapsu- lated by the IP Address and is sent to the default gateway. Via the network layer the packet gets send to the local router of the wanted device. Here ARP can be used by the local router to determine the MAC address if necessary. • Error correction : The strategies we have seen in class are ARQ (which already was explained earlier) and Error Correcting Code. Error Correction The idea is that we send check bits R, which are a function of the data bits D, i.e. R = f (D). In this context we define the Hamming Distance, which is the minimum distance between any pair of strings. To determine the distance between two binary strings one can just count the digites where both differ. Binary String 1 Binary String 2 Hamming Distance 101010 100110 2 1111 0000 4 11001 11011 1 1011101 1001001 3 From here on, we assume that binary strings (code words) represent certain symbols. That can be numbers but does not have to be. Error Detection : When all code-words of a mapping have the minimum hamming distance d + 1, we can detect up to d errors. Error Correction : For a code of Hamming distance 2d + 1, up to d errors can always be corrected by mapping to the closest code-word A possible mapping could be 0 ↦→ 000 and 1 ↦→ 111. Notice that all other binary strings of length 3 have no meaning in this mapping and thus one can conclude that they are corrupted if they occur. We can detect an error as long as the hamming distance is greater than the amount of errors occurred. Assume one wants to send a 0, which gets mapped to 000. The following can happen. Code-word 000 001 010 100 011 101 110 111 Amount of Errors 0 1 1 1 2 2 2 3 Value certainly 0 likely 0 likely 0 likely 0 likely 1 likely 1 likely 1 certainly 1 Correctable - yes yes yes no no no no Notice that we can detect up to 2 errors. Having 3 bit flips would not be recognised in this mapping. As long as we have more 0s than 1s we can also correct the binary string. We covered several different methods to conduct error detection. 17 Parity Bit The idea is simply adding a parity bit at the end of the data, which is 1 iff the amount of bits set in D is odd and 0 otherwise. The Hamming Distance is 1. Checksum In the 1s complement checksum method the data to be transmitted is divided into blocks of a fixed size, typically 16 bits. The checksum is then computed by treating each block as an unsigned integer in 1s comple- ment and just adding them together. The overflow is ignored. The result is then inverted and appended to the actual data D. The inversion happens by simply mapping 0 ↦→ 1 and 1 ↦→ 0. (Remember that inverting a number n in 1s complement we get −n) The Hamming Distance is 1 but it can detect multiple errors if they occur in different ”columns”. To detect error one can just add up all blocks (including the additional one added by the checksum) and then the result should yield −0, which is 1111 . . . in 1s complement. Consider the example 0110 0110 0110 0110 0110 01100110 0110 0000 1111 0000 1111 We form the blocks and perform the addition. 0 1 1 0 0 1 1 0 0 1 1 0 0 1 1 0 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 0 0 0 1 1 1 1 0 0 0 0 1 1 1 1 C = 1 1 0 0 1 0 1 0 1 1 0 0 1 0 1 0 The checksum is now the inverted value from the table C. Which is ¯C = 0 0 1 1 0 1 0 1 0 0 1 1 0 1 0 1 Cyclic Redundancy Check (CRC) 18 Hamming Code Hamming code is an error-detection and error-correction code that can detect up to two simultaneous bit errors and correct single-bit errors. Thus the Hamming Distance is 3. The idea is to embed parity bits at fixed positions in the message, which are responsible for specific bits. For a message of length n we need k parity bits given by n ≤ 2 k − k − 1. The parity bits Pi are in position 2 i. (counted from the right). Further let Ri be the bit at the i-th position from the right. So, for a message of length 4. This looks as follows: 1 0 1 P4 1 P2 P1 We compute Pi by considering the positions which are a summand of i. 2 2 2 1 20 Parity Bit involved 1 0 0 1 P1 2 0 1 0 P2 3 0 1 1 P2, P1 4 1 0 0 P4 5 1 0 1 P4, P1 6 1 1 0 P4, P2 7 1 1 1 P4, P2, P1 Then Pi is the parity bit for the positions as indicated in the table. That is, one has to compute the XOR ( ⊗) of aforementioned positions. Thus, with our initial example we get: P1 = R3 ⊗ R5 ⊗ R7 = 0 ⊗ 0 ⊗ 1 = 0 P2 = R3 ⊗ R6 ⊗ R7 = 0 ⊗ 0 ⊗ 1 = 1 P4 = R5 ⊗ R6 ⊗ R7 = 1 ⊗ 0 ⊗ 1 = 0 The overall message becomes 1 0 1 0 1 1 0 The syndrome is now defined as the concatenated P2k . . . P4P2P1. When the message is error-free the syndrome is 0 . . . 0. Otherwise the syndrome indicates the position of the flipped bit. Consider a bitflip at the 6th position. 1 1 1 0 1 1 0 We compute the syndrome: P1 = R1 ⊗ R3 ⊗ R5 ⊗ R7 = 0 ⊗ 0 ⊗ 1 ⊗ 1 = 0 P2 = R2 ⊗ R3 ⊗ R6 ⊗ R7 = 0 ⊗ 1 ⊗ 1 ⊗ 1 = 1 P4 = R4 ⊗ R5 ⊗ R6 ⊗ R7 = 0 ⊗ 1 ⊗ 1 ⊗ 1 = 1 Which indicates a bitflip at the 6th position. • Flow control : 19 Multiple Access (MA) Schemes Multiple access schemes multiplex users based on their demands, leveraging statistical multiplexing to eﬀiciently utilise network resources. This approach optimises resource allocation by dynamically assigning bandwidth according to the varying needs of users, resulting in improved eﬀiciency and overall performance. Although the principles introduced are shared for wireless and wired networks, this part mainly focuses on the wired case. The wireless case will be discussed later. We’ll examine two types of multiple access protocols covered in class: Randomized Multiple Access In an Randomized Multiple Access Protocol, nodes randomly initiate their access attempts to the shared resource. This method works well in low-load scenarios, where the probability of collisions (simultaneous access attempts by multiple nodes) is low due to the sparse activity. One of the earliest protocol was the ALOHA Protocol. ALOHA Protocol The Aloha protocol is a simple method that allows nodes to share a single communication channel without central coordination. It follows the following three steps: 1. Sending Data : In the Aloha protocol, nodes simply send their data whenever they have traﬀic to transmit. There is no need to wait for permission or listen to the channel before transmission. 2. Collision Detection : After sending data, a node waits for an acknowledgment (ACK) from the receiver. If it does not receive an ACK within a certain time frame, it assumes that its transmission collided with another node’s transmission. 3. Randomized Resend: Upon detecting a collision (no ACK received), the node waits for a random amount of time before attempting to resend the data. This randomization helps reduce the likelihood of collisions reoccurring and improves the chances of successful transmission upon reattempt. ALOHA was historically used in early satellite and radio networks and influenced the development of modern multiple access protocols but is not commonly used in current networks. Carrier Sense Multiple Access with Collision Detection (CSMA/CD) This version of CSMA is used in wired networks. (Whereas CSMA/CA is used for the wireless case.) CSMA is an improvement over ALOHA that involves nodes listening for activity on the network before attempting to transmit data. This approach enhances eﬀiciency by reducing the likelihood of collisions. – Collision Detection : When a node is sending, it also tries to detect foreign or corrupted signals. If found the node concludes a collision happened. As a consequence it immediately stops trans- mitting and jams the channel to notify other nodes of the collision and initiates an Exponential Backoff Algorithm – Collision Window : Nodes listen for the jamming signal (implyig a collision) within a specific time window of 2D seconds after starting transmission. – Minimum Frame Size : A minimum frame size of 64 bytes is enforced to ensure collisions are detected within the 2D-second window, preventing premature frame completion. Contention-free Multiple Access In a Contention-free Multiple Access Protocol, nodes orderly schedule their access attempts to the shared resource, ensuring a systematic and collision-free approach. This protocol is suitable for high-load situations or when guaranteed quality of service is required, as it minimizes the likelihood of collisions and ensures eﬀicient resource allocation. 20 Wireless Multiple Access Schemes As the name implies Wireless Multiple Access Protocols deal with MA but for wireless networks. As before we have the distinction of randomised and contention-free MA. Wireless Multiple Access introduces additional complexities compared to wired connections: – Variability in Coverage : Nodes in a wireless network may have different areas of coverage due to factors such as distance from the access point, interference, and obstacles. This variability makes *traditional carrier sense mechanisms less effective*, as nodes may not reliably detect ongoing transmissions in distant areas or in the presence of interference. Here we have the following terminology: ∗ Hidden Terminals : Hidden Terminals is a set of terminals, which cannot directly communicate with each other due to range of the signal. However, both cover a third terminal, which both of them can potentially send data to. A problem arises since neither of them can detect each other sending, but still corrupt data sent to the third terminal. ∗ Exposed Terminals : The opposite case are called exposed terminals. in this scenario two terminals are in its others range and send data to other nodes, which are out of the range of the other terminals. Then both can detect each other and abort sending, although they would not interfere at their respective destination. – Inability to Hear While Sending : Unlike in wired networks where nodes can listen for activity on the medium while transmitting data, nodes in wireless networks typically cannot receive signals while they are transmitting. This limitation renders collision detection challenging, as nodes cannot detect collisions while they are in the process of sending data. Randomized Multiple Access Multiple Access with Collision Avoidance (MACA) MACA is a protocol that addresses the issue of hidden terminals and aims to reduce collisions in wireless networks. Unlike traditional Carrier Sense Multiple Access (CSMA) protocols, MACA utilizes a short handshake mechanism to coordinate transmissions. It works as follows: 1. RTS (Request-To-Send) : When a sender node wishes to transmit data, it first sends a RTS frame to the intended receiver node, indicating its intention to send and specifying the length of the upcoming frame. 2. CTS (Clear-To-Send) : Upon receiving the RTS frame, the receiver replies with a CTS frame, granting permission for the sender to transmit. The CTS frame also includes the length of the expected frame. 3. Transmission : After receiving the CTS frame, the sender transmits the data frame. Meanwhile, other nodes that overhear the CTS frame refrain from transmitting to avoid collisions. While collisions on the RTS/CTS frames are still possible, they are less likely compared to traditional CSMA protocols. Contention-free Multiple Access Carrier Sense Multiple Access with Collision Avoidance (CSMA/CA) IEEE 802.11 CSMA/CA is a protocol used in wireless networks, particularly Wi-Fi networks, to manage multiple devices accessing the shared transmission medium (air). Switching 21 Multiplexing Multiplexing in networking refers to the method of eﬀiciently sharing a single resource among multiple users. In the classic scenario of sharing a link among different users, two common techniques are employed: 1. Time Division Multiplexing (TDM): In TDM, the available time on the resource (such as a communication link) is divided into fixed intervals, and each *user* is allocated a specific time slot during which they can transmit data. This ensures that each user gets exclusive access to the resource during their assigned time slot, thus enabling multiple users to share the resource without interference. This approach is characterised by periodic bursts of high-rate transmission followed by idle periods. 2. Frequency Division Multiplexing (FDM): In FDM, the available frequency spectrum of the resource is divided into distinct frequency bands, and each user is assigned a separate frequency band for their commu- nication. By allocating different frequency bands to each user, FDM allows multiple users to simultaneously transmit data over the same resource without causing interference between their signals. This approach is characterised by continuous sending of data at a low rate. It is well-suited for scenarios with continuous traﬀic and a predetermined number of users. The ethernet defines framing, addressing, and error detection Ethernet Point-to-Point Protocol (PPP) This protocol used to establish a direct connection between two networking nodes. It can provide connection authentication, transmission encryption, and compression. In particular it provides: • Framing • Error Detection • Authentication 22 Physical Layer The physical layers provides an abstraction and model of the real-world implementation of the transfers. We use this abstraction to express desired properties. We have defined • Rate [ bits second ] • Delay [seconds] : We can further distinguish between two types of delay: 1. Transmission Delay : T-delay [seconds] = M [bits] Rate [ bits second ] = M R 2. Propagation Delay : P-delay [seconds] = Length Speedofsignals = Length 2 3 C = D The total Latency is then given by L = M R + D With that definition we can compute the amount of data, which is ”travelling” in the network. It is defined by the bandwidth-delay Product BD [bits] = R · D Generally, we have to deal with a large span of numbers and thus we use prefixes. We use powers of 10 for rates Prefix Exp. Prefix Exp. K (Kilo) 103 m (Milli) 10−3 M (Mega) 106 µ (Micro) 10−6 G (Giga) 109 n (Nano) 10−9 and powers of 2 for storage/data size: Prefix Exp. (bits) Prefix Exp. (bits) K (Kilo) 210 m (Milli) 2 −10 M (Mega) 220 µ (Micro) 2 −20 G (Giga) 230 n (Nano) 2 −30 Many different media can be used to transfer bits. The two most prominent ones are wires ( such as Twisted Pair-, Coaxial Cable, or Fibres) and the air (Wlan) 23 Modulation Schemes The process of representing bits through signals is achieved through modulation. Modulation involves encoding digital information onto a carrier signal, altering one or more properties of the carrier signal such as its amplitude, frequency, or phase to convey the binary data. This modulation process allows for the transmission of digital information over various communication channels, enabling eﬀicient and reliable communication. When the medium is a wire we call it baseband modulation, otherwise it is called passband modulation. Baseband Modulations Using a high voltage (+V) to represent a 1 and a low voltage (-V) to represent a 0 characterizes Non-Return to Zero (NRZ) encoding. In NRZ encoding, the voltage level is maintained throughout each bit duration, without returning to zero between successive bits. (i.e. two set bits represents to two time units of high voltage, instead of having a low voltage inbetween) The problem with that scheme is, that we don’t necessarily know how many zeros or ones we have seen, since we need the clock-frequency for that. We covered two ways how we can deal with that. Normally, only one of them will be used on the same signal. Non-Return to Zero Inverted (NRZI) with Scrambling In NRZI encoding, a binary 1 is represented by a transition (change in voltage level), and a binary 0 is represented by no transition (no change in voltage level). To prevent long sequences with no changes we use scrambling. Scrambling is a process that pseudo-randomly modifies the bit sequence before it is encoded with NRZI. The purpose is to ensure that the data stream has enough transitions to maintain clock synchronization and to spread the signal spectrum more evenly. Manchester Coding Manchester coding maps every bit into two bits, which correspond to a transition. So: • logical 1 : High-to-Low Transition (10) • logical 0 : Low-to-High Transition (01) For example: • 0000 ↦→ 10 10 10 10 • 1011 ↦→ 10 01 10 10 Passband Modulations This modulation leverages properties of oscillating waveform in media. It involves altering one or more character- istics of this carrier signal - amplitude, frequency, or phase - to encode information for transmission. By modifying these aspects of the carrier signal, we can embed digital data onto it, enabling communication over various channels. The rate at which we can transmit information over a link is constrained by theoretical limits such as the Nyquist limit (circa 1924) and Shannon capacity (1948). Practical systems are designed to approach these limits. • The Nyquist limit states that the maximum symbol rate is twice the bandwidth (2B), allowing for a maximum bit rate of R = 2B log2(V ) bits sec , where (V ) represents the number of distinguishable signal levels. • Shannon Capacity determines the maximum information-carrying rate of the channel based on the Signal-to-Noise Ratio (SNR), denoted as C = B log2(1 + S N ) bits sec . SNR is typically measured in decibels (dB), where SN RdB = 10 log10( S N ). 24","libVersion":"0.5.0","langs":""}