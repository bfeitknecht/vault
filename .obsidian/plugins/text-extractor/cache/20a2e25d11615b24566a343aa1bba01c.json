{"path":"sem4/W&S/UE/s/W&S-s-u08.pdf","text":"Probability and Statistics (D-INFK) Lecturer: Prof. Dr. Dylan Possamaï Coordinator: Daniel Kršek Probability and Statistics Exercise sheet 8 - Solutions MC 8.1. Let X and Y be two independent and identically distributed random variables taking values in {1, 2}, such that P[X = i] = P[Y = i] = 1 2 , i ∈ {1, 2}. Define the random variable Z := X + Y. (Exactly one answer is correct for each question.) 1. What is the cumulative distribution function of Z? (a) FZ(a) =    0, a < 2, 1 4 , 2 ≤ a < 3, 3 4 , 3 ≤ a < 4, 1, 4 ≤ a. (b) FZ(a) =    0, a < 2, 1 4 , 2 ≤ a < 3, 1 2 , 3 ≤ a < 4, 1, 4 ≤ a. (c) FZ(a) =    0, a < 2, 1 3 , 2 ≤ a < 3, 3 4 , 3 ≤ a < 4, 1, 4 ≤ a. (d) FZ(a) =    0, a < 2, 1 4 , 2 ≤ a < 3, 5 6 , 3 ≤ a < 4, 1, 4 ≤ a. 2. What is the value of Cov(X, Z)? (a) Cov(X, Z) = 1 4 . (b) Cov(X, Z) = 0. (c) Cov(X, Z) = 1 2 . (d) Cov(X, Z) = 19 4 . 1 Probability and Statistics (D-INFK) Lecturer: Prof. Dr. Dylan Possamaï Coordinator: Daniel Kršek Solution: 1. (a). We compute: P[Z = 2] = P[X + Y = 2] = P[X = 1, Y = 1] = P[X = 1] × P[Y = 1] = 1 4 , P[Z = 3] = P[X + Y = 3] = P[X = 1, Y = 2] + P[X = 2, Y = 1] = P[X = 1] × P[Y = 2] + P[X = 2] × P[Y = 1] = 1 4 + 1 4 = 1 2 , P[Z = 4] = P[X + Y = 4] = P[X = 2, Y = 2] = P[X = 2] × P[Y = 2] = 1 4 . Therefore, FZ(a) = P[Z ≤ a] =    0, a < 2, 1 4 , 2 ≤ a < 3, 3 4 , 3 ≤ a < 4, 1, 4 ≤ a. 2. (a). We compute: E[XZ] = E[X(X + Y )] = E[X 2] + E[XY ] = E[X 2] + E[X] × E[Y ], where we used independence of X and Y . Now, compute each term: E[X 2] = E[Y 2] = 12 × 1 2 + 22 × 1 2 = 5 2 , E[X] = E[Y ] = 1 × 1 2 + 2 × 1 2 = 3 2 , E[Z] = 2 × 1 4 + 3 × 1 2 + 4 × 1 4 = 3, (Alternatively, E[Z] = E[X] + E[Y ] = 2 × 3 2 = 3). Therefore: E[XZ] = 5 2 + ( 3 2 )2 = 5 2 + 9 4 = 19 4 and Cov(X, Z) = E[XZ] − E[X] × E[Z] = 19 4 − 3 2 × 3 = 19 4 − 9 2 = 1 4 . Alternative solution: Note that: Cov(X, Z) = Cov(X, X + Y ) = Cov(X, X) + Cov(X, Y ) Since X and Y are independent, Cov(X, Y ) = 0, so: Cov(X, Z) = Var(X) 2 Probability and Statistics (D-INFK) Lecturer: Prof. Dr. Dylan Possamaï Coordinator: Daniel Kršek From earlier calculations: Var(X) = E[X 2] − (E[X]) 2 = 5 2 − ( 3 2 )2 = 5 2 − 9 4 = 1 4 Therefore, Cov(X, Z) = 1 4 . MC 8.2. Let X and Y be random variable with joint density fX,Y (x, y) = { 1 9 , 1 ≤ x ≤ 4 and 1 ≤ y ≤ 4, 0, otherwise. (Exactly one answer is correct for each question.) 1. Are X and Y identically distributed, i.e., do X and Y have the same distribution? (a) Yes. (b) No. 2. Are X and Y independent? (a) Yes. (b) No. 3. Are X and Y i.i.d.? (a) Yes. (b) No. 4. Which of the following functions is the density function fX of X? (a) x ↦→ 1 for x ∈ R. (b) x ↦→ 1 9 for x ∈ R. (c) x ↦→ 1 3 for x ∈ R. (d) x ↦→    x 9 , if x ∈ [1, 4], 1, if x > 4, 0, otherwise. (e) x ↦→    x−1 3 , if x ∈ [1, 4], 1, if x > 4, 0, otherwise. (f) x ↦→ { 1 9 , if x ∈ [1, 4], 0, otherwise. (g) x ↦→ { 1 3 , if x ∈ [1, 4], 0, otherwise. 3 Probability and Statistics (D-INFK) Lecturer: Prof. Dr. Dylan Possamaï Coordinator: Daniel Kršek (h) x ↦→ { x 9 , if x ∈ [1, 4], 0, otherwise. 5. Which of the functions from Question 4 is the distribution function FX of X? Solution: (1) (a). Yes, X and Y are identically distributed because FX = FY (see the solutions to 4 and 5). (2) (a). Yes, X and Y are independent. This follows from the fact that fX,Y (x, y) = fX (x)fY (y) for all x, y ∈ R (see the solution to 4 for fX and fY ). (3) (a). Yes, X and Y are i.i.d. This follows directly from (1) and (2), since i.i.d. means independent and identically distributed. (4) (g). The marginal density of X is computed as: fX (x) = ∫ R fX,Y (x, y)dy = ∫ 4 1 1 9 · 1[1,4](x)dy = 3 9 · 1[1,4](x) = { 1 3 , if x ∈ [1, 4], 0, otherwise. The marginal density fY of Y is computed analogously and, due to symmetry, gives the same result. (5) (e). The cumulative distribution function FX of X is computed as: FX (a) = ∫ a −∞ fX (x)dx = ∫ a −∞ 1 3 1[1,4](x)dx =    0, if a < 1, ∫ a 1 1 3 dx = a−1 3 , if a ∈ [1, 4], ∫ 4 1 1 3 dx = 1, if a > 4. The cumulative distribution function FY of Y is identical due to symmetry. MC 8.3. Let X and Y be two random variables with E[X 2] < ∞ and E[Y 2] < ∞. Which of the following statements is generally true? (Exactly one answer is correct.) (a) Var[X + Y ] = Var[X] + Var[Y ]. (b) If X and Y are independent, then Var[X − Y ] = Var[X] − Var[Y ]. (c) Var[X] = Var[−X]. (d) The equality Var[X + Y ] = Var[X] + Var[Y ] is only true if X and Y are independent. Solution: (a) is not generally true. It holds in specific cases, e.g., when X and Y are independent (or uncor- related). (b) is almost never true. Note that Var[X − Y ] ≥ 0, but Var[X] − Var[Y ] might be negative. 4 Probability and Statistics (D-INFK) Lecturer: Prof. Dr. Dylan Possamaï Coordinator: Daniel Kršek (c) is true, since Var[−X] = E[(−X − E[−X]) 2] = E [ (−(X − E[X])) 2] = E[(X − E[X]) 2] = Var[X]. (d) is not true. It is also true for instance when X and Y are uncorrelated, see Exercise 8.7. MC 8.4. Let X be a random variable with E[X 2] < ∞. Which of the following statements are true? (The number of correct answers is between 0 and 4.) (a) E[X 2] = (E[X]) 2. (b) E[X 2] ≥ (E[X]) 2. (c) If X is centered (i.e., E[X] = 0), then Var[X] = E[X 2]. (d) Var[X] > 0. (e) The random variable Y := X − E[X] has the same variance as X. Solution: (a) is almost never true. It is true if and only if there exists a constant c ∈ R such that P[X = c] = 1. (b) is true. The map x ↦→ x2 is convex, and so the result follows from Jensen’s inequality. Alterna- tively, the inequality (X − E[X]) 2 ≥ 0 implies 0 ≤ E[(X − E[X]) 2] = Var[X] = E[X 2] − (E[X]) 2, which gives the result. (c) is true, since Var[X] = E[X 2] − (E[X]) 2 = E[X 2] − 0 2 = E[X 2]. (d) is not generally true. If there exists a constant c ∈ R such that P[X = c] = 1, then Var[X] = 0. In fact, this is the only case where (d) does not hold. (e) is true. We have that E[Y ] = E[X − E[X]] = E[X] − E[X] = 0. It follows Var[Y ] = E[(Y − E[Y ]) 2] = E[(Y )2] = E[(X − E[X]) 2] = Var[X]. More generally, it can be seen that adding or subtracting constants from a random variable never changes its variance. MC 8.5. Let X be a random variable that takes values in the set {0, 1, 3} with E[X] = 2. Which of the following statements are true? (The number of correct answers is between 0 and 4.) (a) P[X = 0] ≥ 1 3 . (b) P[X = 1] ≥ 1 2 . (c) P[X = 0] ≤ 1 6 . (d) P[X = 3] ≥ 1 2 . 5 Probability and Statistics (D-INFK) Lecturer: Prof. Dr. Dylan Possamaï Coordinator: Daniel Kršek Solution: (a) is not true, for example if P[X = 1] = P[X = 3] = 1 2 . (b) is not true, for example if P[X = 0] = 1 3 and P[X = 3] = 2 3 . (c) is not true, for example if P[X = 0] = 1 3 and P[X = 3] = 2 3 . (d) is true. For any choice of P, let pi := P[X = i] for i = 0, 1, 3, then 2 = E[X] = p1 + 3p3 = (1 − p0 − p3) + 3p3 = 1 − p0 + 2p3. Thus, 2p3 = 1 + p0 ≥ 1, and therefore P[X = 3] = p3 ≥ 1 2 Exercise 8.6. Based on many years of research, it is known that the lead concentration X in a soil sample is approximately normally distributed. It is also known that the expected value is 32 ppb (parts per billion) and that the standard deviation is 6 ppb. (The standard deviation is defined as the square root of the variance, i.e. sd(X) := √ Var[X].) (a) Sketch the density of X and indicate in the sketch the probability that a soil sample contains between 26 and 38 ppb of lead. (b) What is the probability that a soil sample contains at most 40 ppb of lead? Hint: Standardize the random variable and use the table of the standard normal distribution below. (c) What is the probability that a soil sample contains at most 27 ppb of lead? (d) What lead concentration is not exceeded with 97.5% probability? That is, find the value c such that the probability that the lead concentration is less than or equal to c is exactly 97.5%. (e) What lead concentration is not exceeded with 10% probability? (f) What is the value of the probability indicated in part (a)? Solution: (a) We have . 6 Probability and Statistics (D-INFK) Lecturer: Prof. Dr. Dylan Possamaï Coordinator: Daniel Kršek (b) From the information above, we have that X ∼ N (µ, σ2), with µ = 32, and σ2 = 6 2. Without a computer, for practical reasons, one usually standardizes to the random variable Z = (X − µ)/σ. Then Z ∼ N (0, 1) and: P[X ≤ 40] = P [ Z ≤ 40 − 32 6 ] ≈ P[Z ≤ 1.33] = Φ(1.33) ≈ 0.9082. (c) As before, we have P[X ≤ 27] ≈ P[Z ≤ −0.83] = Φ(−0.83) = 1 − Φ(0.83) ≈ 0.2033. (d) Generally, we have P[X ≤ c] = P [Z ≤ c − 32 6 ] = Φ ( c − 32 6 ) . Using the table, we find Φ(1.96) ≈ 0.975 (That means that 1.96 is the 97.5% quantile of the standard normal distribution). So: P[X ≤ c] ≈ 0.975 ⇐⇒ c − 32 6 ≈ 1.96 ⇐⇒ c ≈ 32 + 1.96 × 6 = 43.76. In words: the lead concentration does not exceed 43.76 with a probability of 97.5%. (e) From the table we have Φ(1.28) ≈ 0.9.The standard normal distribution is symmetric around 0, and so P[Z ≤ c] = 1 − P[Z ≥ −c]. It follows that Φ(−1.28) = 1 − Φ(1.28) ≈ 1 − 0.9 = 0.1. Thus, arguing as in (d) gives: c ≈ 32 − 1.28 × 6 = 24.32. In words: the lead concentration does not exceed 24.32 with a probability of 10%. (f) We have P[26 ≤ X ≤ 38] = P [ 26 − 32 6 ≤ Z ≤ 38 − 32 6 ] = P[−1 ≤ Z ≤ 1] = Φ(1) − Φ(−1) = Φ(1) − (1 − Φ(1)) = 2Φ(1) − 1 ≈ 2 × 0.8413 − 1 = 0.6826. 7 Probability and Statistics (D-INFK) Lecturer: Prof. Dr. Dylan Possamaï Coordinator: Daniel Kršek Exercise 8.7. The joint density f (x, y) of two continuous random variables X and Y is constant on the square Q (see sketch) and zero outside of Q. Q 1-1 1 -1 y x . (a) Determine the joint density of (X, Y ). (b) Determine the marginal densities fX and fY of the random variables X and Y . (c) Are X and Y independent? (d) What is the answer to (c) if the square Q is rotated by 45 degrees? Solution: (a) The area of Q is 4 × 1×1 2 = 2. Therefore, the joint density f is given by f (x, y) = { 1 2 , if (x, y) ∈ Q, 0, otherwise. (b) For the marginal density fX , we distinguish two cases: For −1 ≤ x ≤ 0: fX (x) = ∫ ∞ −∞ f (x, y)dy = ∫ 1+x −1−x 1 2 dy = 1 2 (1 + x + 1 + x) = 1 + x. For 0 ≤ x ≤ 1: fX (x) = ∫ ∞ −∞ f (x, y)dy = ∫ 1−x −1+x 1 2 dy = 1 2 (1 − x + 1 − x) = 1 − x. Thus, fX (x) =    1 + x, if − 1 ≤ x ≤ 0, 1 − x, if 0 ≤ x ≤ 1, 0, otherwise. 8 Probability and Statistics (D-INFK) Lecturer: Prof. Dr. Dylan Possamaï Coordinator: Daniel Kršek By symmetry, fY = fX , so fY (y) =    1 + y, if − 1 ≤ y ≤ 0, 1 − y, if 0 ≤ y ≤ 1, 0, otherwise. (c) We see that fX (x)fY (y) ̸≡ 1 2 = f (x, y) for every (x, y) ∈ Q, and so X and Y are not independent. (d) Due to symmetry, the marginal density of X will be the same as that of Y , but now they are uniform on the interval [ − 1√2 , 1√2 ]: fX (x) = fY (x) =    1 √2 , if − 1 √2 ≤ x ≤ 1 √2 , 0, otherwise. Since fX (x)fY (y) = f (x, y) for every x, y ∈ R, we conclude that X and Y are independent in this case. Exercise 8.8. For two independent random variables X and Y , it is known from the lecture that Cov(X, Y ) = 0, i.e., the random variables are uncorrelated. In this problem, we show that the converse is not true in general. (a) Let X ∼ U([−π, π]). Show that Y := cos(X) and Z := sin(X) are uncorrelated, i.e., Cov(Y, Z) = 0. (b) Show that Y and Z are not independent. Hint: If Y and Z were independent, then Y 2 and Z 2 would also be independent. Disprove the latter by considering P[Y 2 ≤ 1/2, Z 2 ≤ 1/2]. Solution: (a) First, we show that Y and Z are uncorrelated. By the definition of covariance, Cov(Y, Z) = E[Y Z] − E[Y ]E[Z] = E[cos(X) sin(X)] − E[cos(X)]E[sin(X)] = 1 2π ∫ π −π cos(x) sin(x)dx − ( 1 2π ∫ π −π cos(x)dx) ( 1 2π ∫ π −π sin(x)dx) = 1 2π ∫ π −π cos(x) sin(x)dx. Integration by parts gives ∫ π −π cos(x) sin(x)dx = sin2(x) ∣ ∣ ∣π x=−π − ∫ π −π sin(x) cos(x)dx = − ∫ π −π sin(x) cos(x)dx. 9 Probability and Statistics (D-INFK) Lecturer: Prof. Dr. Dylan Possamaï Coordinator: Daniel Kršek Thus, 1 2π ∫ π −π cos(x) sin(x)dx = 0, and so Cov(Y, Z) = 0. Alternative: Using sin(x) cos(x) = 1 2 sin(2x), we get ∫ π −π cos(x) sin(x)dx = 1 2 ∫ π −π sin(2x)dx = − 1 4 cos(2x) ∣ ∣ ∣π x=−π = 0, since cos(2π) = cos(−2π) = 1. (b) Now we show that Y and Z are not independent. Since Y 2 + Z 2 = cos 2(X) + sin2(X) = 1 P–a.s., and we have {Y 2 < 1/2, Z 2 < 1/2} ⊆ {Y 2 + Z 2 < 1}, it holds 0 ≤ P[Y 2 < 1/2, Z 2 < 1/2] ≤ P[Y 2 + Z 2 < 1] = 0. However, P[Y 2 < 1/2] = 1 2π ∫ π −π 1{cos2(x)<1/2} dx = 1 2π ∫ π −π 1{−1/ √2<cos(x)<1/√2} dx = 1 2π ∫ π −π 1{π/4<x<3π/4}∪{−3π/4<x<−π/4} dx = 1 2π ( 3π 4 − π 4 + 3π 4 − π 4 ) = 1 2 . By symmetry, we also get P[Z 2 < 1/2] = 1 2 . Assume that Y and Z are independent. Then Y 2 and Z 2 are also independent, so P[Y 2 < 1/2, Z 2 < 1/2] = P[Y 2 < 1/2] × P[Z 2 < 1/2] = 1 2 × 1 2 = 1 4 . But earlier we saw this probability is 0. This is a contradiction. Hence, Y and Z are not independent. Exercise 8.9. Let X and Y be two random variables that can only take the values 0 and 1. The joint distribution of (X, Y ) satisfies: P[X = 0] = 1 2 , P[Y = 0] = 1 3 , and P[X = 0, Y = 0] = p. 10 Probability and Statistics (D-INFK) Lecturer: Prof. Dr. Dylan Possamaï Coordinator: Daniel Kršek (a) What values can p take, and for which values of p are X and Y independent? (b) Compute the expectations E[X], E[Y ], and E[XY ], as well as the variances Var[X] and Var[Y ], as functions of p. When does E[XY ] = E[X]E[Y ] hold? (c) Give an example of random variables U and V such that E[U V ] = E[U ]E[V ], but U and V are not independent. Solution: (a) The joint distribution of (X, Y ) is: P[X = 0, Y = 0] = p, P[X = 0, Y = 1] = P[X = 0] − P[X = 0, Y = 0] = 1 2 − p, P[X = 1, Y = 0] = P[Y = 0] − P[X = 0, Y = 0] = 1 3 − p, P[X = 1, Y = 1] = 1 − P[X = 0, Y = 0] − P[X = 0, Y = 1] − P[X = 1, Y = 1] = 1 − ( 1 2 − p + 1 3 − p + p) = 1 6 + p. Since all of these are probabilities, they must lie in [0, 1]. Therefore, p must satisfy: 0 ≤ p ≤ 1 3 . Conversely, it is easy to check that if p ∈ [0, 1/3], then all probabilities are valid (non-negative and sum to 1). X and Y are independent if and only if for all i, j ∈ {0, 1}, P[X = i, Y = j] = P[X = i]P[Y = j]. We have P[X = 0, Y = 1] = 1 2 − p and P[X = 0]P[Y = 1] = 1 2 × 2 3 , P[X = 1, Y = 0] = 1 3 − p and P[X = 1]P[Y = 0] = 1 2 × 1 3 , P[X = 1, Y = 1] = 1 6 + p and P[X = 1]P[Y = 1] = 1 2 × 2 3 . We thus have to solve 1 2 − p = 1 2 × 2 3 , 1 3 − p = 1 2 × 1 3 , 1 6 + p = 1 2 × 2 3 . This system is satisfied exactly when p = 1 6 . 11 Probability and Statistics (D-INFK) Lecturer: Prof. Dr. Dylan Possamaï Coordinator: Daniel Kršek (b) We compute E[X] = 0 × P[X = 0] + 1 × P[X = 1] = 1 − 1 2 = 1 2 , E[Y ] = 0 × P[Y = 0] + 1 × P[Y = 1] = 1 − 1 3 = 2 3 , E[XY ] = 0 × (P[X = 0, Y = 0] + P[X = 1, Y = 0] + P[X = 0, Y = 1]) + 1 × P[X = 1, Y = 1] = 1 6 + p. Since X and Y take values in {0, 1}, we have X 2 = X and Y 2 = Y P–a.s., and in particular, E[X 2] = E[X] and E[Y 2] = E[Y ]. It follows Var[X] = E[X 2] − (E[X]) 2 = 1 2 − ( 1 2 )2 = 1 4 , Var[Y ] = E[Y 2] − (E[Y ]) 2 = 2 3 − ( 2 3 )2 = 2 9 . We have E[XY ] = E[X]E[Y ] if and only if 1 6 + p = 1 2 × 2 3 = 1 3 , which is equivalent to p = 1 6 . (c) Let U be a discrete random variable with values in {−1, 0, 1}, each with probability 1/3. Define V := U 2. Clearly U and V are not independent. For example: P[U = 1, V = 1] = P[U = 1] = 1 3 ̸= P[U = 1] × P[V = 1]. However, we have: E[U V ] = E[U 3] = E[U ] = 0 = E[U ] × E[V ]. 12 Probability and Statistics (D-INFK) Lecturer: Prof. Dr. Dylan Possamaï Coordinator: Daniel Kršek Quantile table for the standard normal distribution 0.5 0.75 0.9 0.95 0.975 0.99 0.995 0.999 0 0.6745 1.2816 1.6449 1.9600 2.3263 2.5758 3.0902 . For instance, Φ−1(0.9) = 1.2816, where Φ is the distribution function of N (0, 1). Table of standard normal distribution 0.00 0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.0 0.5000 0.5040 0.5080 0.5120 0.5160 0.5199 0.5239 0.5279 0.5319 0.5359 0.1 0.5398 0.5438 0.5478 0.5517 0.5557 0.5596 0.5636 0.5675 0.5714 0.5753 0.2 0.5793 0.5832 0.5871 0.5910 0.5948 0.5987 0.6026 0.6064 0.6103 0.6141 0.3 0.6179 0.6217 0.6255 0.6293 0.6331 0.6368 0.6406 0.6443 0.6480 0.6517 0.4 0.6554 0.6591 0.6628 0.6664 0.6700 0.6736 0.6772 0.6808 0.6844 0.6879 0.5 0.6915 0.6950 0.6985 0.7019 0.7054 0.7088 0.7123 0.7157 0.7190 0.7224 0.6 0.7257 0.7291 0.7324 0.7357 0.7389 0.7422 0.7454 0.7486 0.7517 0.7549 0.7 0.7580 0.7611 0.7642 0.7673 0.7704 0.7734 0.7764 0.7794 0.7823 0.7852 0.8 0.7881 0.7910 0.7939 0.7967 0.7995 0.8023 0.8051 0.8078 0.8106 0.8133 0.9 0.8159 0.8186 0.8212 0.8238 0.8264 0.8289 0.8315 0.8340 0.8365 0.8389 1.0 0.8413 0.8438 0.8461 0.8485 0.8508 0.8531 0.8554 0.8577 0.8599 0.8621 1.1 0.8643 0.8665 0.8686 0.8708 0.8729 0.8749 0.8770 0.8790 0.8810 0.8830 1.2 0.8849 0.8869 0.8888 0.8907 0.8925 0.8944 0.8962 0.8980 0.8997 0.9015 1.3 0.9032 0.9049 0.9066 0.9082 0.9099 0.9115 0.9131 0.9147 0.9162 0.9177 1.4 0.9192 0.9207 0.9222 0.9236 0.9251 0.9265 0.9279 0.9292 0.9306 0.9319 1.5 0.9332 0.9345 0.9357 0.9370 0.9382 0.9394 0.9406 0.9418 0.9429 0.9441 1.6 0.9452 0.9463 0.9474 0.9484 0.9495 0.9505 0.9515 0.9525 0.9535 0.9545 1.7 0.9554 0.9564 0.9573 0.9582 0.9591 0.9599 0.9608 0.9616 0.9625 0.9633 1.8 0.9641 0.9649 0.9656 0.9664 0.9671 0.9678 0.9686 0.9693 0.9699 0.9706 1.9 0.9713 0.9719 0.9726 0.9732 0.9738 0.9744 0.9750 0.9756 0.9761 0.9767 2.0 0.9772 0.9778 0.9783 0.9788 0.9793 0.9798 0.9803 0.9808 0.9812 0.9817 2.1 0.9821 0.9826 0.9830 0.9834 0.9838 0.9842 0.9846 0.9850 0.9854 0.9857 2.2 0.9861 0.9864 0.9868 0.9871 0.9875 0.9878 0.9881 0.9884 0.9887 0.9890 2.3 0.9893 0.9896 0.9898 0.9901 0.9904 0.9906 0.9909 0.9911 0.9913 0.9916 2.4 0.9918 0.9920 0.9922 0.9925 0.9927 0.9929 0.9931 0.9932 0.9934 0.9936 2.5 0.9938 0.9940 0.9941 0.9943 0.9945 0.9946 0.9948 0.9949 0.9951 0.9952 2.6 0.9953 0.9955 0.9956 0.9957 0.9959 0.9960 0.9961 0.9962 0.9963 0.9964 2.7 0.9965 0.9966 0.9967 0.9968 0.9969 0.9970 0.9971 0.9972 0.9973 0.9974 2.8 0.9974 0.9975 0.9976 0.9977 0.9977 0.9978 0.9979 0.9979 0.9980 0.9981 2.9 0.9981 0.9982 0.9982 0.9983 0.9984 0.9984 0.9985 0.9985 0.9986 0.9986 3.0 0.9987 0.9987 0.9987 0.9988 0.9988 0.9989 0.9989 0.9989 0.9990 0.9990 For instance, P[Z ≤ 1.96] = 0.975. 13","libVersion":"0.5.0","langs":""}