{"path":"sem2/DDCA/VRL/extra/slides/DDCA-L25a-multicore-caches.pdf","text":"Digital Design & Computer Arch. Lecture 25a: Caches in Multi-Core Systems Frank K. Gürkaynak Rahul Bera Prof. Onur Mutlu ETH Zürich Spring 2024 30 May 2024 Brief Self Introduction 2 n Rahul Bera q Ph.D. Student in SAFARI Research Group q Previously worked with Intel Labs and AMD q https://sites.google.com/view/rahulbera/home q write2bera@gmail.com (best way to reach me) n Research interests: Microarchitecture and memory system design q High-performance memory system design n Prefetchers, Cache Replacement Polcies, ... q Microarchitectural techniques to reduce/tolerate memory latency q Data-driven microarchitectures n Machine-learning-assisted microarchitectural predictors and policies Agenda for Today n Part 1: Multi-core issues in Caching q Cache sharing/partitioning n Pros and Cons q Brief intro to cache coherence n Part 2: Prefetching q What, When, Where, How q Different types of prefetchers q Brief intro to ML-inspired prefetchers 3 The Memory Hierarchy Memory Hierarchy n Fundamental tradeoff q Fast memory: small q Large memory: slow n Idea: Memory hierarchy n Latency, cost, size, bandwidth 5 CPU Main Memory (DRAM)RF Cache Hard Disk Memory Hierarchy Example 6 Kim & Mutlu, “Memory Systems,” Computing Handbook, 2014 https://people.inf.ethz.ch/omutlu/pub/memory-systems-introduction_computing-handbook14.pdf Multi-Core Issues in Caching Caches in a Multi-Core System 8 CORE 1L2 CACHE 0SHARED L3 CACHEDRAM INTERFACE CORE 0 CORE 2 CORE 3L2 CACHE 1L2 CACHE 2L2 CACHE 3DRAM BANKS DRAM MEMORY CONTROLLER Caches in a Multi-Core System 9Source: https://www.anandtech.com/show/16252/mac-mini-apple-m1-tested Apple M1, 2021 Caches in a Multi-Core System 10Source: https://twitter.com/Locuza_/status/1454152714930331652 Intel Alder Lake, 2021 Caches in a Multi-Core System 11https://wccftech.com/amd-ryzen-5000-zen-3-vermeer-undressed-high-res-die-shots-close-ups-pictured-detailed/ AMD Ryzen 5000, 2020 Core Count: 8 cores/16 threads L1 Caches: 32 KB per core L2 Caches: 512 KB per core L3 Cache: 32 MB shared Caches in a Multi-Core System 12https://youtu.be/gqAYMx34euU https://www.tech-critter.com/amd-keynote-computex-2021/ https://community.microcenter.com/discussion/5 134/comparing-zen-3-to-zen-2 Additional 64 MB L3 cache die stacked on top of the processor die - Connected using Through Silicon Vias (TSVs) - Total of 96 MB L3 cache AMD increases the L3 size of their 8-core Zen 3 processors from 32 MB to 96 MB 3D Stacking Technology: Example 13https://www.pcgameshardware.de/Ryzen-7-5800X3D-CPU-278064/Specials/3D-V-Cache-Release-1393125/ Caches in a Multi-Core System 14https://www.it-techblog.de/ibm-power10-prozessor-mehr-speicher-mehr-tempo-mehr-sicherheit/09/2020/ IBM POWER10, 2020 Cores: 15-16 cores, 8 threads/core L2 Caches: 2 MB per core L3 Cache: 120 MB shared Caches in a Multi-Core System 15https://www.tomshardware.com/news/infrared-photographer-photos-nvidia-ga102-ampere-silicon Nvidia Ampere, 2020 Cores: 128 Streaming Multiprocessors L1 Cache or Scratchpad: 192KB per SM Can be used as L1 Cache and/or Scratchpad L2 Cache: 40 MB shared Caches in a Multi-Core System 16https://developer.nvidia.com/blog/nvidia-hopper-architecture-in-depth/ Nvidia Hopper, 2022 L1 Cache or Scratchpad: 256KB per SM Can be used as L1 Cache and/or Scratchpad Cores: 144 Streaming Multiprocessors L2 Cache: 60 MB shared Caches in Multi-Core Systems n Cache efficiency becomes even more important in a multi- core/multi-threaded system q Memory bandwidth is at premium q Cache space is a limited resource across cores/threads n How do we design the caches in a multi-core system? n Many decisions and questions q Shared vs. private caches q How to maximize performance of the entire system? q How to provide QoS & predictable perf. to different threads in a shared cache? q Should cache management algorithms be aware of threads? q How should space be allocated to threads in a shared cache? q Should we store data in compressed format in some caches? q How do we do better reuse prediction & management in caches? 17 Private vs. Shared Caches n Private cache: Cache belongs to one core (a shared block can be in multiple caches) n Shared cache: Cache is shared by multiple cores 18 CORE 0 CORE 1 CORE 2 CORE 3 L2 CACHE L2 CACHE L2 CACHE DRAM MEMORY CONTROLLER L2 CACHE CORE 0 CORE 1 CORE 2 CORE 3 DRAM MEMORY CONTROLLER L2 CACHE Resource Sharing Concept and Advantages n Idea: Instead of dedicating a hardware resource to a hardware context, allow multiple contexts to use it q Example resources: functional units, pipeline, caches, buses, memory, interconnects, storage n Why? + Resource sharing improves utilization/efficiency à throughput q When a resource is left idle by one thread, another thread can use it; no need to replicate shared data + Reduces communication latency q For example, data shared between multiple threads can be kept in the same cache in multithreaded processors + Compatible with the shared memory programming model 19 Resource Sharing Disadvantages n Resource sharing results in contention for resources q When the resource is not idle, another thread cannot use it q If space is occupied by one thread, another thread needs to re- occupy it - Sometimes reduces each or some thread’s performance - Thread performance can be worse than when it is run alone - Eliminates performance isolation à inconsistent performance across runs - Thread performance depends on co-executing threads - Uncontrolled (free-for-all) sharing degrades quality of service - Causes unfairness, starvation Need to efficiently and fairly utilize shared resources 20 Private vs. Shared Caches n Private cache: Cache belongs to one core (a shared block can be in multiple caches) n Shared cache: Cache is shared by multiple cores 21 CORE 0 CORE 1 CORE 2 CORE 3 L2 CACHE L2 CACHE L2 CACHE DRAM MEMORY CONTROLLER L2 CACHE CORE 0 CORE 1 CORE 2 CORE 3 DRAM MEMORY CONTROLLER L2 CACHE Shared Caches Between Cores n Advantages: q High effective capacity q Dynamic partitioning of available cache space n No fragmentation due to static partitioning n If one core does not utilize some space, another core can q Easier to maintain coherence (a cache block is in a single location) n Disadvantages q Slower access (cache not tightly coupled with the core) q Cores incur conflict misses due to other cores’ accesses n Misses due to inter-core interference n Some cores can destroy the hit rate of other cores q Guaranteeing a minimum level of service (or fairness) to each core is harder (how much space, how much bandwidth?) 22 Real World Example: Intel Skylake 23 https://www.makeuseof.com/tag/what-is-cpu-cache/ Real World Example: Apple M1 24 https://chipsandcheese.com/2022/05/21/igpu-cache-setups-compared-including-m1/ Lectures on Multi-Core Cache Management 25https://www.youtube.com/watch?v=7_Tqlw8gxOU&list=PL5Q2soXY2Zi9OhoVQBXYFIZywZXCPl4M_&index=17 Lectures on Multi-Core Cache Management 26https://www.youtube.com/watch?v=c9FhGRB3HoA&list=PL5Q2soXY2Zi9JXe3ywQMhylk_d5dI-TM7&index=29 Lectures on Multi-Core Cache Management 27https://www.youtube.com/watch?v=Siz86__PD4w&list=PL5Q2soXY2Zi9JXe3ywQMhylk_d5dI-TM7&index=30 Lectures on Multi-Core Cache Management n Computer Architecture, Fall 2018, Lecture 18b q Multi-Core Cache Management (ETH, Fall 2018) q https://www.youtube.com/watch?v=c9FhGRB3HoA&list=PL5Q2soXY2Zi9JXe3ywQM hylk_d5dI-TM7&index=29 n Computer Architecture, Fall 2018, Lecture 19a q Multi-Core Cache Management II (ETH, Fall 2018) q https://www.youtube.com/watch?v=Siz86__PD4w&list=PL5Q2soXY2Zi9JXe3ywQM hylk_d5dI-TM7&index=30 n Computer Architecture, Fall 2017, Lecture 15 q Multi-Core Cache Management (ETH, Fall 2017) q https://www.youtube.com/watch?v=7_Tqlw8gxOU&list=PL5Q2soXY2Zi9OhoVQBXY FIZywZXCPl4M_&index=17 28https://www.youtube.com/onurmutlulectures Lectures on Memory Resource Management 29https://www.youtube.com/watch?v=0nnI807nCkc&list=PL5Q2soXY2Zi9xidyIgBxUz7xRPS-wisBN&index=21 Lectures on Memory Resource Management n Computer Architecture, Fall 2020, Lecture 11a q Memory Controllers (ETH, Fall 2020) q https://www.youtube.com/watch?v=TeG773OgiMQ&list=PL5Q2soXY2Zi9xidyIgBxUz 7xRPS-wisBN&index=20 n Computer Architecture, Fall 2020, Lecture 11b q Memory Interference and QoS (ETH, Fall 2020) q https://www.youtube.com/watch?v=0nnI807nCkc&list=PL5Q2soXY2Zi9xidyIgBxUz7 xRPS-wisBN&index=21 n Computer Architecture, Fall 2020, Lecture 13 q Memory Interference and QoS II (ETH, Fall 2020) q https://www.youtube.com/watch?v=Axye9VqQT7w&list=PL5Q2soXY2Zi9xidyIgBxU z7xRPS-wisBN&index=26 n Computer Architecture, Fall 2020, Lecture 2a q Memory Performance Attacks (ETH, Fall 2020) q https://www.youtube.com/watch?v=VJzZbwgBfy8&list=PL5Q2soXY2Zi9xidyIgBxUz7 xRPS-wisBN&index=2 30https://www.youtube.com/onurmutlulectures Cache Coherence Cache Coherence n Basic question: If multiple processors cache the same block, how do they ensure they all see a consistent state? 32 P1 P2 x Interconnection Network Main Memory 1000 The Cache Coherence Problem 33 P1 P2 x Interconnection Network Main Memory ld r2, x 1000 1000 The Cache Coherence Problem 34 P1 P2 x Interconnection Network Main Memory ld r2, x ld r2, x 1000 1000 1000 The Cache Coherence Problem 35 P1 P2 x Interconnection Network Main Memory ld r2, x add r1, r2, r4 st x, r1 ld r2, x 1000 10002000 The Cache Coherence Problem 36 P1 P2 x Interconnection Network Main Memory ld r2, x add r1, r2, r4 st x, r1 ld r2, x 1000 10002000 ld r5, x Should NOT load 1000 Hardware Cache Coherence n Basic idea: q A processor/cache broadcasts its write/update to a memory location to all other processors q Another processor/cache that has the location either updates or invalidates its local copy 37 A Very Simple Coherence Scheme n Idea: All caches “snoop” (observe) each other’s write/read operations. If a processor writes to a block, all others invalidate the block. n A simple protocol: 38 n Write-through, no- write-allocate cache n Actions of the local processor on the cache block: PrRd, PrWr, n Actions that are broadcast on the bus for the block: BusRd, BusWr PrWr / BusWr Valid BusWr Invalid PrWr / BusWr PrRd / BusRd PrRd/-- Lecture on Cache Coherence 39https://www.youtube.com/watch?v=T9WlyezeaII&list=PL5Q2soXY2Zi9xidyIgBxUz7xRPS-wisBN&index=38 Lecture on Memory Ordering & Consistency 40https://www.youtube.com/watch?v=Suy09mzTbiQ&list=PL5Q2soXY2Zi9xidyIgBxUz7xRPS-wisBN&index=37 Lecture on Cache Coherence & Consistency n Computer Architecture, Fall 2020, Lecture 21 q Cache Coherence (ETH, Fall 2020) q https://www.youtube.com/watch?v=T9WlyezeaII&list=PL5Q2soXY2Zi9xidyIgBxUz7 xRPS-wisBN&index=38 n Computer Architecture, Fall 2020, Lecture 20 q Memory Ordering & Consistency (ETH, Fall 2020) q https://www.youtube.com/watch?v=Suy09mzTbiQ&list=PL5Q2soXY2Zi9xidyIgBxUz 7xRPS-wisBN&index=37 n Computer Architecture, Spring 2015, Lecture 28 q Memory Consistency & Cache Coherence (CMU, Spring 2015) q https://www.youtube.com/watch?v=JfjT1a0vi4E&list=PL5PHm2jkkXmi5CxxI7b3JCL 1TWybTDtKq&index=32 n Computer Architecture, Spring 2015, Lecture 29 q Cache Coherence (CMU, Spring 2015) q https://www.youtube.com/watch?v=X6DZchnMYcw&list=PL5PHm2jkkXmi5CxxI7b3 JCL1TWybTDtKq&index=33 41https://www.youtube.com/onurmutlulectures Digital Design & Computer Arch. Lecture 25A: Caches in Multi-Core Systems Frank K. Gürkaynak Rahul Bera Prof. Onur Mutlu ETH Zürich Spring 2024 24 May 2024 Additional Slides: Cache Coherence 43 Two Cache Coherence Methods q How do we ensure that the proper caches are updated? q Snoopy Bus [Goodman ISCA 1983, Papamarcos+ ISCA 1984] n Bus-based, single point of serialization for all memory requests n Processors observe other processors’ actions q E.g.: P1 makes “read-exclusive” request for A on bus, P0 sees this and invalidates its own copy of A q Directory [Censier and Feautrier, IEEE ToC 1978] n Single point of serialization per block, distributed among nodes n Processors make explicit requests for blocks n Directory tracks which caches have each block n Directory coordinates invalidations and updates q E.g.: P1 asks directory for exclusive copy, directory asks P0 to invalidate, waits for ACK, then responds to P1 44 Directory Based Coherence n Idea: A logically-central directory keeps track of where the copies of each cache block reside. Caches consult this directory to ensure coherence. n An example mechanism: q For each cache block in memory, store P+1 bits in directory n One bit for each cache, indicating whether the block is in cache n Exclusive bit: indicates that a cache has the only copy of the block and can update it without notifying others q On a read: set the cache’s bit and arrange the supply of data q On a write: invalidate all caches that have the block and reset their bits q Have an “exclusive bit” associated with each block in each cache (so that the cache can update the exclusive block silently) 45 Directory Based Coherence Example (I) 46 Directory Based Coherence Example (I) 47 Maintaining Coherence n Need to guarantee that all processors see a consistent value (i.e., consistent updates) for the same memory location n Writes to location A by P0 should be seen by P1 (eventually), and all writes to A should appear in some order n Coherence needs to provide: q Write propagation: guarantee that updates will propagate q Write serialization: provide a consistent order seen by all processors for the same memory location n Need a global point of serialization for this write ordering 48 Coherence: Update vs. Invalidate n How can we safely update replicated data? q Option 1 (Update protocol): push an update to all copies q Option 2 (Invalidate protocol): ensure there is only one copy (local), update it n On a Read: q If local copy is Invalid, put out request q (If another node has a copy, it returns it, otherwise memory does) 49 Coherence: Update vs. Invalidate (II) n On a Write: q Read block into cache as before Update Protocol: q Write to block, and simultaneously broadcast written data and address to sharers q (Other nodes update the data in their caches if block is present) Invalidate Protocol: q Write to block, and simultaneously broadcast invalidation of address to sharers q (Other nodes invalidate block in their caches if block is present) 50 Update vs. Invalidate Tradeoffs n Which one is better? Update or invalidate? q Write frequency and sharing behavior are critical n Update + If sharer set is constant and updates are infrequent, avoids the cost of invalidate-reacquire (broadcast update pattern) - If data is rewritten without intervening reads by other cores, updates would be useless - Write-through cache policy è bus can become a bottleneck n Invalidate + After invalidation, core has exclusive access rights + Only cores that keep reading after each write retain a copy - If write contention is high, leads to ping-ponging (rapid invalidation-reacquire traffic from different processors) 51 Additional Slides: Memory Interference 52 Inter-Thread/Application Interference n Problem: Threads share the memory system, but memory system does not distinguish between threads’ requests n Existing memory systems q Free-for-all, shared based on demand q Control algorithms thread-unaware and thread-unfair q Aggressive threads can deny service to others q Do not try to reduce or control inter-thread interference 53 Unfair Slowdowns due to Interference (Core 0) (Core 1) Moscibroda and Mutlu, “Memory performance attacks: Denial of memory service in multi-core systems,” USENIX Security 2007. matlab (Core 1) gcc (Core 2) 54 55 Uncontrolled Interference: An Example CORE 1 CORE 2 L2 CACHE L2 CACHE DRAM MEMORY CONTROLLER DRAM Bank 0 DRAM Bank 1 DRAM Bank 2 Shared DRAM Memory System Multi-Core Chip unfairness INTERCONNECT stream random DRAM Bank 3 // initialize large arrays A, B for (j=0; j<N; j++) { index = rand(); A[index] = B[index]; … } 56 A Memory Performance Hog STREAM - Sequential memory access - Very high row buffer locality (96% hit rate) - Memory intensive RANDOM - Random memory access - Very low row buffer locality (3% hit rate) - Similarly memory intensive // initialize large arrays A, B for (j=0; j<N; j++) { index = j*linesize; A[index] = B[index]; … } streaming random Moscibroda and Mutlu, “Memory Performance Attacks,” USENIX Security 2007. 57 What Does the Memory Hog Do? Row BufferRow decoder Column mux Data Row 0 T0: Row 0 Row 0 T1: Row 16 T0: Row 0T1: Row 111 T0: Row 0T0: Row 0T1: Row 5 T0: Row 0T0: Row 0T0: Row 0T0: Row 0T0: Row 0 Memory Request Buffer T0: STREAM T1: RANDOM Row size: 8KB, cache block size: 64B 128 (8KB/64B) requests of T0 serviced before T1 Moscibroda and Mutlu, “Memory Performance Attacks,” USENIX Security 2007. 58 DRAM Controllers n A row-conflict memory access takes significantly longer than a row-hit access n Current controllers take advantage of the row buffer n Commonly used scheduling policy (FR-FCFS) [Rixner 2000]* (1) Row-hit first: Service row-hit memory accesses first (2) Oldest-first: Then service older accesses first n This scheduling policy aims to maximize DRAM throughput n But, it is unfair when multiple threads share the DRAM system *Rixner et al., “Memory Access Scheduling,” ISCA 2000. *Zuravleff and Robinson, “Controller for a synchronous DRAM …,” US Patent 5,630,096, May 1997. Effect of the Memory Performance Hog 0 0.5 1 1.5 2 2.5 3 STREAM RANDOM 59 1.18X slowdown 2.82X slowdown Results on Intel Pentium D running Windows XP (Similar results for Intel Core Duo and AMD Turion, and on Fedora Linux) Slowdown 0 0.5 1 1.5 2 2.5 3 STREAM gcc 0 0.5 1 1.5 2 2.5 3 STREAM Virtual PC Moscibroda and Mutlu, “Memory Performance Attacks,” USENIX Security 2007. Greater Problem with More Cores n Vulnerable to denial of service (DoS) n Unable to enforce priorities or SLAs n Low system performance Uncontrollable, unpredictable system 60 Greater Problem with More Cores n Vulnerable to denial of service (DoS) n Unable to enforce priorities or SLAs n Low system performance Uncontrollable, unpredictable system 61 Distributed DoS in Networked Multi-Core Systems 62 Attackers (Cores 1-8) Stock option pricing application (Cores 9-64) Cores connected via packet-switched routers on chip ~5000X latency increase Grot, Hestness, Keckler, Mutlu, “Preemptive virtual clock: A Flexible, Efficient, and Cost-effective QOS Scheme for Networks-on-Chip,“ MICRO 2009. More on Memory Performance Attacks n Thomas Moscibroda and Onur Mutlu, \"Memory Performance Attacks: Denial of Memory Service in Multi-Core Systems\" Proceedings of the 16th USENIX Security Symposium (USENIX SECURITY), pages 257-274, Boston, MA, August 2007. Slides (ppt) 63 http://www.youtube.com/watch?v=VJzZbwgBfy8 More on Interconnect Based Starvation n Boris Grot, Stephen W. Keckler, and Onur Mutlu, \"Preemptive Virtual Clock: A Flexible, Efficient, and Cost- effective QOS Scheme for Networks-on-Chip\" Proceedings of the 42nd International Symposium on Microarchitecture (MICRO), pages 268-279, New York, NY, December 2009. Slides (pdf) 64 Energy Comparison of Memory Technologies The Problem: Energy n Faster is more energy-efficient q SRAM, ~5 pJ q DRAM, ~40-140 pJ q PCM-DIMM (Intel Optane DC DIMM), ~80-540 pJ q PCM-SSD, ~120 µJ q Flash memory, ~250 µJ q Hard Disk, ~60 mJ n Other technologies have their place as well q MRAM, RRAM, STT-MRAM, memristors, … (not mature yet) 66 The Problem (Table View): Energy 67 Memory Device Capacity Latency Cost per Megabyte Energy per access Energy per byte access SRAM < 1 KByte sub-nanosec ~5 pJ ~1.25 pJ SRAM KByte~MB yte ~nanosec < 0.3$ DRAM Gigabyte ~50 nanosec < 0.006$ ~40-140 pJ ~10-35 pJ PCM-DIMM (Intel Optane DC DIMM) Gigabyte ~300 nanosec < 0.004$ ~80-540 pJ ~20-135 pJ PCM-SSD (Intel Optane SSD) Gigabyte ~Terabyte ~6-10 µs < 0.002$ ~120 µJ ~30 nJ Flash memory Gigabyte ~Terabyte ~50-100 µs < 0.00008$ ~250 µJ ~61 nJ Hard Disk Terabyte ~10 millisec < 0.00003$ ~60 mJ ~15 µJ These sample values (circa ~2022) scale with time Bigger is slower Faster is more expensive ($$$ and chip area) Faster is more energy-efficient Basic Cache Examples: For You to Study Cache Terminology n Capacity (C): q the number of data bytes a cache stores n Block size (b): q bytes of data brought into cache at once n Number of blocks (B = C/b): q number of blocks in cache: B = C/b n Degree of associativity (N): q number of blocks in a set n Number of sets (S = B/N): q each memory address maps to exactly one cache set 69 How is data found? n Cache organized into S sets n Each memory address maps to exactly one set n Caches categorized by number of blocks in a set: q Direct mapped: 1 block per set q N-way set associative: N blocks per set q Fully associative: all cache blocks are in a single set n Examine each organization for a cache with: q Capacity (C = 8 words) q Block size (b = 1 word) q So, number of blocks (B = 8) 70 Direct Mapped Cache 7 (111) 00...00010000 230 Word Main Memory mem[0x00...00] mem[0x00...04] mem[0x00...08] mem[0x00...0C] mem[0x00...10] mem[0x00...14] mem[0x00...18] mem[0x00..1C] mem[0x00..20] mem[0x00...24] mem[0xFF...E0] mem[0xFF...E4] mem[0xFF...E8] mem[0xFF...EC] mem[0xFF...F0] mem[0xFF...F4] mem[0xFF...F8] mem[0xFF...FC] 23 Word Cache Set Number Address 00...00000000 00...00000100 00...00001000 00...00001100 00...00010100 00...00011000 00...00011100 00...00100000 00...00100100 11...11110000 11...11100000 11...11100100 11...11101000 11...11101100 11...11110100 11...11111000 11...11111100 6 (110) 5 (101) 4 (100) 3 (011) 2 (010) 1 (001) 0 (000) 71 Direct Mapped Cache Hardware DataTag 00 Tag Set Byte OffsetMemory Address DataHit V = 27 3 27 32 8-entry x (1+27+32)-bit SRAM 72 Direct Mapped Cache Performance # MIPS assembly code addi $t0, $0, 5 loop: beq $t0, $0, done lw $t1, 0x4($0) lw $t2, 0xC($0) lw $t3, 0x8($0) addi $t0, $t0, -1 j loop done: DataTagV 00...001 mem[0x00...04] 0 0 0 0 0 00 Tag Set Byte OffsetMemory Address V 3 00100...00 1 00...00 00...00 1 mem[0x00...0C] mem[0x00...08] Set 7 (111) Set 6 (110) Set 5 (101) Set 4 (100) Set 3 (011) Set 2 (010) Set 1 (001) Set 0 (000) Miss Rate = 73 Direct Mapped Cache Performance # MIPS assembly code addi $t0, $0, 5 loop: beq $t0, $0, done lw $t1, 0x4($0) lw $t2, 0xC($0) lw $t3, 0x8($0) addi $t0, $t0, -1 j loop done: DataTagV 00...001 mem[0x00...04] 0 0 0 0 0 00 Tag Set Byte OffsetMemory Address V 3 00100...00 1 00...00 00...00 1 mem[0x00...0C] mem[0x00...08] Set 7 (111) Set 6 (110) Set 5 (101) Set 4 (100) Set 3 (011) Set 2 (010) Set 1 (001) Set 0 (000) Miss Rate = 3/15 = 20% Temporal Locality Compulsory Misses 74 Direct Mapped Cache: Conflict # MIPS assembly code addi $t0, $0, 5 loop: beq $t0, $0, done lw $t1, 0x4($0) lw $t2, 0x24($0) addi $t0, $t0, -1 j loop done: DataTagV 00...001 mem[0x00...04] 0 0 0 0 0 00 Tag Set Byte OffsetMemory Address V 3 00100...01 0 0 Set 7 (111) Set 6 (110) Set 5 (101) Set 4 (100) Set 3 (011) Set 2 (010) Set 1 (001) Set 0 (000) mem[0x00...24] Miss Rate = 75 Direct Mapped Cache: Conflict # MIPS assembly code addi $t0, $0, 5 loop: beq $t0, $0, done lw $t1, 0x4($0) lw $t2, 0x24($0) addi $t0, $t0, -1 j loop done: DataTagV 00...001 mem[0x00...04] 0 0 0 0 0 00 Tag Set Byte OffsetMemory Address V 3 00100...01 0 0 Set 7 (111) Set 6 (110) Set 5 (101) Set 4 (100) Set 3 (011) Set 2 (010) Set 1 (001) Set 0 (000) mem[0x00...24] Miss Rate = 10/10 = 100% Conflict Misses 76 N-Way Set Associative Cache DataTag Tag Set Byte OffsetMemory Address Data Hit1 V =01 00 32 32 32 DataTagV = Hit1Hit0 Hit 28 2 28 28 Way 1 Way 0 77 N-way Set Associative Performance # MIPS assembly code addi $t0, $0, 5 loop: beq $t0, $0, done lw $t1, 0x4($0) lw $t2, 0x24($0) addi $t0, $t0, -1 j loop done: DataTagV DataTagV 00...001 mem[0x00...04]00...10 1mem[0x00...24] 0 0 0 0 0 0 Way 1 Way 0 Set 3 Set 2 Set 1 Set 0 Miss Rate = 78 N-way Set Associative Performance # MIPS assembly code addi $t0, $0, 5 loop: beq $t0, $0, done lw $t1, 0x4($0) lw $t2, 0x24($0) addi $t0, $t0, -1 j loop done: Miss Rate = 2/10 = 20% Associativity reduces conflict misses DataTagV DataTagV 00...001 mem[0x00...04]00...10 1mem[0x00...24] 0 0 0 0 0 0 Way 1 Way 0 Set 3 Set 2 Set 1 Set 0 79 Fully Associative Cache n No conflict misses n Expensive to build DataTagV DataTagV DataTagV DataTagV DataTagV DataTagV DataTagV DataTagV 80 Spatial Locality? n Increase block size: q Block size, b = 4 words q C = 8 words q Direct mapped (1 block per set) q Number of blocks, B = C/b = 8/4 = 2 DataTag 00 Tag Byte OffsetMemory Address Data V00011011 Block Offset 32 32 32 32 32 Hit = Set 27 27 2 Set 1 Set 0 81 Direct Mapped Cache Performance addi $t0, $0, 5 loop: beq $t0, $0, done lw $t1, 0x4($0) lw $t2, 0xC($0) lw $t3, 0x8($0) addi $t0, $t0, -1 j loop done: 00...00 0 11 DataTag 00 Tag Byte OffsetMemory Address Data V00011011 Block Offset 32 32 32 32 32 Hit = Set 27 27 2 Set 1 Set 000...001 mem[0x00...0C] 0 mem[0x00...08] mem[0x00...04] mem[0x00...00] Miss Rate = 82 Direct Mapped Cache Performance addi $t0, $0, 5 loop: beq $t0, $0, done lw $t1, 0x4($0) lw $t2, 0xC($0) lw $t3, 0x8($0) addi $t0, $t0, -1 j loop done: Miss Rate = 1/15 = 6.67% Larger blocks reduce compulsory misses through spatial locality 00...00 0 11 DataTag 00 Tag Byte OffsetMemory Address Data V00011011 Block Offset 32 32 32 32 32 Hit = Set 27 27 2 Set 1 Set 000...001 mem[0x00...0C] 0 mem[0x00...08] mem[0x00...04] mem[0x00...00] 83 Cache Organization Recap n Main Parameters q Capacity: C q Block size: b q Number of blocks in cache: B = C/b q Number of blocks in a set: N q Number of Sets: S = B/N Organization Number of Ways (N) Number of Sets (S = B/N) Direct Mapped 1 B N-Way Set Associative 1 < N < B B / N Fully Associative B 1 84 Capacity Misses n Cache is too small to hold all data of interest at one time q If the cache is full and program tries to access data X that is not in cache, cache must evict data Y to make room for X q Capacity miss occurs if program then tries to access Y again q X will be placed in a particular set based on its address n In a direct mapped cache, there is only one place to put X n In an associative cache, there are multiple ways where X could go in the set. n How to choose Y to minimize chance of needing it again? q Least recently used (LRU) replacement: the least recently used block in a set is evicted when the cache is full. 85 Types of Misses n Compulsory: first time data is accessed n Capacity: cache too small to hold all data of interest n Conflict: data of interest maps to same location in cache n Miss penalty: time it takes to retrieve a block from lower level of hierarchy 86 LRU Replacement # MIPS assembly lw $t0, 0x04($0) lw $t1, 0x24($0) lw $t2, 0x54($0) DataTagV DataTagVU DataTagV DataTagVU (a) (b) Set Number 3 (11) 2 (10) 1 (01) 0 (00) Set Number 3 (11) 2 (10) 1 (01) 0 (00) 87 LRU Replacement # MIPS assembly lw $t0, 0x04($0) lw $t1, 0x24($0) lw $t2, 0x54($0) DataTagV 0 DataTagV 0 0 0 0 0 U mem[0x00...04]1 00...000mem[0x00...24] 100...010 0 0 0 0 DataTagV 0 DataTagV 0 0 0 0 0 U mem[0x00...54]1 00...101mem[0x00...24] 100...010 0 0 0 1 (a) (b) Way 1 Way 0 Way 1 Way 0 Set 3 (11) Set 2 (10) Set 1 (01) Set 0 (00) Set 3 (11) Set 2 (10) Set 1 (01) Set 0 (00) 88 Slides for Future Lectures 89 Issues in Set-Associative Caches n Think of each block in a set having a “priority” q Indicating how important it is to keep the block in the cache n Key issue: How do you determine/adjust block priorities? n There are three key decisions in a set: q Insertion, promotion, eviction (replacement) n Insertion: What happens to priorities on a cache fill? q Where to insert the incoming block, whether or not to insert the block n Promotion: What happens to priorities on a cache hit? q Whether and how to change block priority n Eviction/replacement: What happens to priorities on a cache miss? q Which block to evict and how to adjust priorities 90 Eviction/Replacement Policy n Which block in the set to replace on a cache miss? q Any invalid block first q If all are valid, consult the replacement policy n Random n FIFO n Least recently used (how to implement?) n Not most recently used n Least frequently used? n Least costly to re-fetch? q Why would memory accesses have different cost? n Hybrid replacement policies n Optimal replacement policy? 91 Implementing LRU n Idea: Evict the least recently accessed block n Problem: Need to keep track of access ordering of blocks n Question: 2-way set associative cache: q What do you minimally need to implement LRU perfectly? n Question: 4-way set associative cache: q What do you minimally need to implement LRU perfectly? q How many different orderings possible for the 4 blocks in the set? q How many bits needed to encode the LRU order of a block? q What is the logic needed to determine the LRU victim? n Repeat for N-way set associative cache 92 Approximations of LRU n Most modern processors do not implement “true LRU” (also called “perfect LRU”) in highly-associative caches n Why? q True LRU is complex q LRU is an approximation to predict locality anyway (i.e., not the best possible cache management policy) n Examples: q Not MRU (not most recently used) q Hierarchical LRU: divide the N-way set into M “groups”, track the MRU group and the MRU way in each group q Victim-NextVictim Replacement: Only keep track of the victim and the next victim 93 Cache Replacement Policy: LRU or Random n LRU vs. Random: Which one is better? q Example: 4-way cache, cyclic references to A, B, C, D, E n 0% hit rate with LRU policy n Set thrashing: When the “program working set” in a set is larger than set associativity q Random replacement policy is better when thrashing occurs n In practice: q Performance of replacement policy depends on workload q Average hit rate of LRU and Random are similar n Best of both Worlds: Hybrid of LRU and Random q How to choose between the two? Set sampling n See Qureshi et al., ”A Case for MLP-Aware Cache Replacement,” ISCA 2006. 94 What Is the Optimal Replacement Policy? n Belady’s OPT q Replace the block that is going to be referenced furthest in the future by the program q Belady, “A study of replacement algorithms for a virtual-storage computer,” IBM Systems Journal, 1966. q How do we implement this? Simulate? n Is this optimal for minimizing miss rate? n Is this optimal for minimizing execution time? q No. Cache miss latency/cost varies from block to block! q Two reasons: Where miss is serviced from and miss overlapping q Qureshi et al. “A Case for MLP-Aware Cache Replacement,\" ISCA 2006. 95 Recommended Reading n Key observation: Some misses more costly than others as their latency is exposed as stall time. Reducing miss rate is not always good for performance. Cache replacement should take into account cost of misses. n Moinuddin K. Qureshi, Daniel N. Lynch, Onur Mutlu, and Yale N. Patt, \"A Case for MLP-Aware Cache Replacement\" Proceedings of the 33rd International Symposium on Computer Architecture (ISCA), pages 167-177, Boston, MA, June 2006. Slides (ppt) 96 What’s In A Tag Store Entry? n Valid bit n Tag n Replacement policy bits n Dirty bit? q Write back vs. write through caches 97 Handling Writes (I) n When do we write the modified data in a cache to the next level? n Write through: At the time the write happens n Write back: When the block is evicted q Write-back + Can combine multiple writes to the same block before eviction q Potentially saves bandwidth between cache levels + saves energy -- Need a bit in the tag store indicating the block is “dirty/modified” q Write-through + Simpler design + All levels are up to date & consistent à Simpler cache coherence: no need to check close-to-processor caches’ tag stores for presence -- More bandwidth intensive; no combining of writes 98 Handling Writes (II) n Do we allocate a cache block on a write miss? q Allocate on write miss: Yes q No-allocate on write miss: No n Allocate on write miss + Can combine writes instead of writing each individually to next level + Simpler because write misses can be treated the same way as read misses -- Requires transfer of the whole cache block n No-allocate + Conserves cache space if locality of written blocks is low (potentially better cache hit rate) 99 Handling Writes (III) n What if the processor writes to an entire block over a small amount of time? n Is there any need to bring the block into the cache from memory in the first place? n Why do we not simply write to only a portion of the block, i.e., subblock q E.g., 4 bytes out of 64 bytes q Problem: Valid and dirty bits are associated with the entire 64 bytes, not with each individual 4 bytes 100 Subblocked (Sectored) Caches n Idea: Divide a block into subblocks (or sectors) q Have separate valid and dirty bits for each subblock (sector) q Allocate only a subblock (or a subset of subblocks) on a request ++ No need to transfer the entire cache block into the cache (A write simply validates and updates a subblock) ++ More freedom in transferring subblocks into the cache (a cache block does not need to be in the cache fully) (How many subblocks do you transfer on a read?) -- More complex design -- May not exploit spatial locality fully 101 tagsubblockvsubblockv subblockvd d d Instruction vs. Data Caches n Separate or Unified? n Pros and Cons of Unified: + Dynamic sharing of cache space: no overprovisioning that might happen with static partitioning (i.e., separate I and D caches) -- Instructions and data can evict/thrash each other (i.e., no guaranteed space for either) -- I and D are accessed in different places in the pipeline. Where do we place the unified cache for fast access? n First level caches are almost always split q Mainly for the last reason above – pipeline constraints n Outer level caches are almost always unified 102 Multi-level Caching in a Pipelined Design n First-level caches (instruction and data) q Decisions very much affected by cycle time & pipeline structure q Small, lower associativity; latency is critical q Tag store and data store usually accessed in parallel n Second- and third-level caches q Decisions need to balance hit rate and access latency q Usually large and highly associative; latency not as important q Tag store and data store can be accessed serially n Serial vs. Parallel access of levels q Serial: Second level cache accessed only if first-level misses q Second level does not see the same accesses as the first n First level acts as a filter (filters some temporal and spatial locality) n Management policies are therefore different 103 Deeper and Larger Cache Hierarchies 104Source: https://www.anandtech.com/show/16252/mac-mini-apple-m1-tested Apple M1, 2021 Deeper and Larger Cache Hierarchies 105Source: https://twitter.com/Locuza_/status/1454152714930331652 Intel Alder Lake, 2021 Deeper and Larger Cache Hierarchies 106https://wccftech.com/amd-ryzen-5000-zen-3-vermeer-undressed-high-res-die-shots-close-ups-pictured-detailed/ AMD Ryzen 5000, 2020 Core Count: 8 cores/16 threads L1 Caches: 32 KB per core L2 Caches: 512 KB per core L3 Cache: 32 MB shared AMD’s 3D Last Level Cache (2021) 107https://youtu.be/gqAYMx34euU https://www.tech-critter.com/amd-keynote-computex-2021/ https://community.microcenter.com/discussion/5 134/comparing-zen-3-to-zen-2 Additional 64 MB L3 cache die stacked on top of the processor die - Connected using Through Silicon Vias (TSVs) - Total of 96 MB L3 cache AMD increases the L3 size of their 8-core Zen 3 processors from 32 MB to 96 MB Deeper and Larger Cache Hierarchies 108https://www.it-techblog.de/ibm-power10-prozessor-mehr-speicher-mehr-tempo-mehr-sicherheit/09/2020/ IBM POWER10, 2020 Cores: 15-16 cores, 8 threads/core L2 Caches: 2 MB per core L3 Cache: 120 MB shared Deeper and Larger Cache Hierarchies 109https://www.tomshardware.com/news/infrared-photographer-photos-nvidia-ga102-ampere-silicon Nvidia Ampere, 2020 Cores: 128 Streaming Multiprocessors L1 Cache or Scratchpad: 192KB per SM Can be used as L1 Cache and/or Scratchpad L2 Cache: 40 MB shared Deeper and Larger Cache Hierarchies 110https://wccftech.com/nvidia-hopper-gpus-featuring-mcm-technology-tape-out-soon-rumor/ Nvidia Hopper, 2022 L1 Cache or Scratchpad: 256KB per SM Can be used as L1 Cache and/or Scratchpad Cores: 144 Streaming Multiprocessors L2 Cache: 60 MB shared Deeper and Larger Cache Hierarchies 111https://developer.nvidia.com/blog/nvidia-hopper-architecture-in-depth/ Nvidia Hopper, 2022 L1 Cache or Scratchpad: 256KB per SM Can be used as L1 Cache and/or Scratchpad Cores: 144 Streaming Multiprocessors L2 Cache: 60 MB shared n Example of data movement between GPU global memory (DRAM) and GPU cores. NVIDIA V100 & A100 Memory Hierarchy A100 feature: Direct copy from L2 to scratchpad, bypassing L1 and register file. 112https://images.nvidia.com/aem-dam/en-zz/Solutions/data-center/nvidia-ampere-architecture-whitepaper.pdf NVIDIA A100 Tensor Core GPU Architecture In-Depth 40 NVIDIA A100 Tensor Core GPU Architecture A100 improves SM bandwidth efficiency with a new load-global-store-shared asynchronous copy instruction that bypasses L1 cache and register file (RF). Additionally, A100’s more efficient Tensor Cores reduce shared memory (SMEM) loads. Figure 15. A100 SM Data Movement Efficiency New asynchronous barriers work together with the asynchronous copy instruction to enable efficient data fetch pipelines, and A100 increases maximum SMEM allocation per SM 1.7x to 164 KB (vs 96 KB on V100). With these improvements A100 SMs continuously data stream data to keep the L2 cache constantly utilized. L2 Cache and DRAM Bandwidth improvements - The NVIDIA A100 GPU’s increased number of SMs and more powerful Tensor Cores in turn increase the required data fetch rates from DRAM and L2 cache. To feed the Tensor Cores, A100 implements a 5-site HBM2 memory subsystem with bandwidth of 1555 GB/sec, over 1.7x faster than V100. A100 further provides 2.3x the L2 cache read bandwidth of V100. Alongside the raw data bandwidth improvements, A100 improves data fetch efficiency and reduces DRAM bandwidth demand with a 40 MB L2 cache that is almost 7x larger than that of Tesla V100. To fully exploit the L2 capacity A100 includes improved cache management controls. Optimized for neural network training and inferencing as well as general compute workloads, the new controls ensure that data in the cache is used more efficiently by minimizing writebacks to memory and keeping reused data in L2 to reduce redundant DRAM traffic. Memory in the NVIDIA H100 GPU 113 … SM Core Control Core Core Core Core Core Core Core SM Core Control Core Core Core Core Core Core Core SM Core Control Core Core Core Core Core Core Core L2 Cache Global Memory Registers Shared Memory L1 Cache Constant Cache Registers Shared Memory L1 Cache Constant Cache Registers Shared Memory L1 Cache Constant Cache ≈1 cycle ≈5 cycles ≈5 cycles ≈500 cycles Slide credit: Izzat El Hajj 60 MB 80 GB Direct copy SM-to-SM 3 TB/s Multi-Level Cache Design Decisions n Which level(s) to place a block into (from memory)? n Which level(s) to evict a block to (from an inner level)? n Bypassing vs. non-bypassing levels n Inclusive, exclusive, non-inclusive hierarchies q Inclusive: a block in an inner level is always included also in an outer level à simplifies cache coherence q Exclusive: a block in an inner level does not exist in an outer level à better utilizes space in the entire hierarchy q Non-inclusive: a block in an inner level may or may not be included in an outer level à relaxes design decisions 114 Cache Performance Cache Parameters vs. Miss/Hit Rate n Cache size n Block size n Associativity n Replacement policy n Insertion/Placement policy n Promotion Policy 116 Cache Size n Cache size: total data (not including tag) capacity q bigger can exploit temporal locality better n Too large a cache adversely affects hit and miss latency q bigger is slower n Too small a cache q does not exploit temporal locality well q useful data replaced often n Working set: entire set of data the executing application references q Within a time interval 117 hit rate cache size “working set” size Benefit of Larger Caches Widely Varies n Benefits of cache size widely varies across applications 118 Low Utility Application High Utility Application Saturating Utility Application Num ways from 16-way 1MB L2Misses per 1000 instructions Qureshi and Patt, “Utility-Based Cache Partitioning,” MICRO 2006. Block Size n Block size is the data that is associated with an address tag q not necessarily the unit of transfer between hierarchies n Sub-blocking: A block divided into multiple pieces (each w/ V/D bits) n Too small blocks q do not exploit spatial locality well q have larger tag overhead n Too large blocks q too few total blocks à exploit temporal locality not well q waste cache space and bandwidth/energy if spatial locality is not high 119 hit rate block size Large Blocks: Critical-Word and Subblocking n Large cache blocks can take a long time to fill into the cache q Idea: Fill cache block critical-word first q Supply the critical data to the processor immediately n Large cache blocks can waste bus bandwidth q Idea: Divide a block into subblocks q Associate separate valid and dirty bits for each subblock q Recall: When is this useful? 120 tagsubblockvsubblockv subblockvd d d Associativity n How many blocks can be present in the same index (i.e., set)? n Larger associativity q lower miss rate (reduced conflicts) q higher hit latency and area cost n Smaller associativity q lower cost q lower hit latency n Especially important for L1 caches n Is power of 2 associativity required? 121 associativity hit rate Recall: Higher Associativity (4-way) n 4-way 122 Tag store Data store =?=? =?=? MUX MUX byte in block Logic Hit? Address tag index byte in block 3 bits1 b4 bits Higher Associativity (3-way) n 3-way 123 Tag store Data store =?=? =? MUX MUX byte in block Logic Hit? Address tag index byte in block 3 bits1 b4 bits Recall: 8-way Fully Associative Cache 124 Tag store Data store =? =? =? =? =? =? =? =? MUX MUX byte in block Logic Hit? Address tag byte in block 3 bits5 bits 7-way Fully Associative Cache 125 Tag store Data store =? =? =? =? =? =? =? MUX MUX byte in block Logic Hit? Address tag byte in block 3 bits5 bits Classification of Cache Misses n Compulsory miss q first reference to an address (block) always results in a miss q subsequent references should hit unless the cache block is displaced for the reasons below n Capacity miss q cache is too small to hold all needed data q defined as the misses that would occur even in a fully- associative cache (with optimal replacement) of the same capacity n Conflict miss q defined as any miss that is neither a compulsory nor a capacity miss 126 How to Reduce Each Miss Type n Compulsory q Caching (only accessed data) cannot help; larger blocks can q Prefetching helps: Anticipate which blocks will be needed soon n Conflict q More associativity q Other ways to get more associativity without making the cache associative n Victim cache n Better, randomized indexing into the cache n Software hints for eviction/replacement/promotion n Capacity q Utilize cache space better: keep blocks that will be referenced q Software management: divide working set and computation such that each “computation phase” fits in cache 127 How to Improve Cache Performance n Three fundamental goals n Reducing miss rate q Caveat: reducing miss rate can reduce performance if more costly-to-refetch blocks are evicted n Reducing miss latency or miss cost n Reducing hit latency or hit cost n The above three together affect performance 128 Improving Basic Cache Performance n Reducing miss rate q More associativity q Alternatives/enhancements to associativity n Victim caches, hashing, pseudo-associativity, skewed associativity q Better replacement/insertion policies q Software approaches n Reducing miss latency/cost q Multi-level caches q Critical word first q Subblocking/sectoring q Better replacement/insertion policies q Non-blocking caches (multiple cache misses in parallel) q Multiple accesses per cycle q Software approaches 129 Software Approaches for Higher Hit Rate n Restructuring data access patterns n Restructuring data layout n Loop interchange n Data structure separation/merging n Blocking n … 130 Restructuring Data Access Patterns (I) n Idea: Restructure data layout or data access patterns n Example: If column-major q x[i+1,j] follows x[i,j] in memory q x[i,j+1] is far away from x[i,j] n This is called loop interchange n Other optimizations can also increase hit rate q Loop fusion, array merging, … 131 Poor code for i = 1, rows for j = 1, columns sum = sum + x[i,j] Better code for j = 1, columns for i = 1, rows sum = sum + x[i,j] Restructuring Data Access Patterns (II) n Blocking q Divide loops operating on arrays into computation chunks so that each chunk can hold its data in the cache q Avoids cache conflicts between different chunks of computation q Essentially: Divide the working set so that each piece fits in the cache n Also called Tiling 132 Data Reuse: An Example from GPU Computing n Same memory locations accessed by neighboring threads for (int i = 0; i < 3; i++){ for (int j = 0; j < 3; j++){ sum += gauss[i][j] * Image[(i+row-1)*width + (j+col-1)]; } } 133 Gaussian filter applied on every pixel of an image Lecture 22: GPU Programming (Spring 2018) https://www.youtube.com/watch?v=y40-tY5WJ8A Data Reuse: Tiling in GPU Computing n To take advantage of data reuse, we divide the input into tiles that can be loaded into shared memory (scratchpad memory) __shared__ int l_data[(L_SIZE+2)*(L_SIZE+2)]; … Load tile into shared memory __syncthreads(); for (int i = 0; i < 3; i++){ for (int j = 0; j < 3; j++){ sum += gauss[i][j] * l_data[(i+l_row-1)*(L_SIZE+2)+j+l_col-1]; } } 134Lecture 22: GPU Programming (Spring 2018) https://www.youtube.com/watch?v=y40-tY5WJ8A Naïve Matrix Multiplication (I) n Matrix multiplication: C = A x B n Consider two input matrices A and B in row-major layout q A size is M x P q B size is P x N q C size is M x N 135 A B C P M P N i jk k Naïve Matrix Multiplication (II) n Naïve implementation of matrix multiplication has poor cache locality 136 #define A(i,j) matrix_A[i * P + j] #define B(i,j) matrix_B[i * N + j] #define C(i,j) matrix_C[i * N + j] for (i = 0; i < M; i++){ // i = row index for (j = 0; j < N; j++){ // j = column index C(i, j) = 0; // Set to zero for (k = 0; k < P; k++) // Row x Col C(i, j) += A(i, k) * B(k, j); } } A B C P M P N i jk k Consecutive accesses to B are far from each other, in different cache lines. Every access to B is likely to cause a cache miss Tiled Matrix Multiplication (I) n We can achieve better cache locality by computing on smaller tiles or blocks that fit in the cache q Or in the scratchpad memory and register file if we compute on a GPU 137 A B C P M P N k k tile_dimtile_dimi j Lam+, \"The cache performance and optimizations of blocked algorithms,\" ASPLOS 1991. https://doi.org/10.1145/106972.106981 Bansal+, \"Chapter 15 - Fast Matrix Computations on Heterogeneous Streams,\" in \"High Performance Parallelism Pearls\", 2015. https://doi.org/10.1016/B978-0-12-803819-2.00011-2 Kirk & Hwu, \"Chapter 5 - Performance considerations,\" in \"Programming Massively Parallel Processors (Third Edition)\", 2017. https://doi.org/10.1016/B978-0-12-811986-0.00005-4 Tiled Matrix Multiplication (II) n Tiled implementation operates on submatrices (tiles or blocks) that fit fast memories (cache, scratchpad, RF) 138 #define A(i,j) matrix_A[i * P + j] #define B(i,j) matrix_B[i * N + j] #define C(i,j) matrix_C[i * N + j] for (I = 0; I < M; I += tile_dim){ for (J = 0; J < N; J += tile_dim){ Set_to_zero(&C(I, J)); // Set to zero for (K = 0; K < P; K += tile_dim) Multiply_tiles(&C(I, J), &A(I, K), &B(K, J)); } } Multiply small submatrices (tiles or blocks) of size tile_dim x tile_dim A B C P M P N k k tile_dimtile_dimi j Lam+, \"The cache performance and optimizations of blocked algorithms,\" ASPLOS 1991. https://doi.org/10.1145/106972.106981 Bansal+, \"Chapter 15 - Fast Matrix Computations on Heterogeneous Streams,\" in \"High Performance Parallelism Pearls\", 2015. https://doi.org/10.1016/B978-0-12-803819-2.00011-2 Kirk & Hwu, \"Chapter 5 - Performance considerations,\" in \"Programming Massively Parallel Processors (Third Edition)\", 2017. https://doi.org/10.1016/B978-0-12-811986-0.00005-4 Tiled Matrix Multiplication on GPUs 139Computer Architecture - Lecture 9: GPUs and GPGPU Programming (Fall 2017) https://youtu.be/mgtlbEqn2dA?t=8157 Restructuring Data Layout (I) n Pointer based traversal (e.g., of a linked list) n Assume a huge linked list (1B nodes) and unique keys n Why does the code on the left have poor cache hit rate? q “Other fields” occupy most of the cache line even though they are rarely accessed! 140 struct Node { struct Node* next; int key; char [256] name; char [256] school; } while (node) { if (nodeàkey == input-key) { // access other fields of node } node = nodeànext; } Rarely accessed Frequently accessed Frequently accessed Rarely accessed Restructuring Data Layout (II) n Idea: separate rarely- accessed fields of a data structure and pack them into a separate data structure n Who should do this? q Programmer q Compiler n Profiling vs. dynamic q Hardware? q Who can determine what is frequently accessed? 141 struct Node { struct Node* next; int key; struct Node-data* node-data; } struct Node-data { char [256] name; char [256] school; } while (node) { if (nodeàkey == input-key) { // access nodeànode-data } node = nodeànext; } Improving Basic Cache Performance n Reducing miss rate q More associativity q Alternatives/enhancements to associativity n Victim caches, hashing, pseudo-associativity, skewed associativity q Better replacement/insertion policies q Software approaches n Reducing miss latency/cost q Multi-level caches q Critical word first q Subblocking/sectoring q Better replacement/insertion policies q Non-blocking caches (multiple cache misses in parallel) q Multiple accesses per cycle q Software approaches 142 Miss Latency/Cost n What is miss latency or miss cost affected by? q Where does the miss get serviced from? n What level of cache in the hierarchy? n Row hit versus row conflict in DRAM (bank/rank/channel conflict) n Queueing delays in the memory controller and the interconnect n Local vs. remote memory (chip, node, rack, remote server, …) n … q How much does the miss stall the processor? n Is it overlapped with other latencies? n Is the data immediately needed by the processor? n Is the incoming block going to evict a longer-to-refetch block? n … 143 Memory Level Parallelism (MLP) q Memory Level Parallelism (MLP) means generating and servicing multiple memory accesses in parallel [Glew’98] q Several techniques to improve MLP (e.g., out-of-order execution) q MLP varies. Some misses are isolated and some parallel How does this affect cache replacement? time A B C isolated miss parallel miss Traditional Cache Replacement Policies q Traditional cache replacement policies try to reduce miss count q Implicit assumption: Reducing miss count reduces memory- related stall time q Misses with varying cost/MLP breaks this assumption! q Eliminating an isolated miss helps performance more than eliminating a parallel miss q Eliminating a higher-latency miss could help performance more than eliminating a lower-latency miss 145 Misses to blocks P1, P2, P3, P4 can be parallel Misses to blocks S1, S2, and S3 are isolated Two replacement algorithms: 1. Minimizes miss count (Belady’s OPT) 2. Reduces isolated miss (MLP-Aware) For a fully associative cache containing 4 blocks S1P4 P3 P2 P1 P1 P2 P3 P4 S2 S3 An ExampleFewest Misses = Best Performance P3 P2 P1 P4 H H H H M H H H MHit/Miss Misses=4 Stalls=4 S1P4 P3 P2 P1 P1 P2 P3 P4 S2 S3 Time stall Belady’s OPT replacement M M MLP-Aware replacement Hit/Miss P3 P2 S1 P4 P3 P2 P1 P4 P3 P2 S2P4 P3 P2 S3P4 S1 S2 S3P1 P3 P2 S3P4 S1 S2 S3P4 H H H S1 S2 S3P4 H M M M H M M M Time stall Misses=6 Stalls=2 Saved cycles Cache Recommended: MLP-Aware Cache Replacement n How do we incorporate MLP/cost into replacement decisions? n How do we design a hybrid cache replacement policy? n Qureshi et al., “A Case for MLP-Aware Cache Replacement,” ISCA 2006. 148 Improving Basic Cache Performance n Reducing miss rate q More associativity q Alternatives/enhancements to associativity n Victim caches, hashing, pseudo-associativity, skewed associativity q Better replacement/insertion policies q Software approaches q … n Reducing miss latency/cost q Multi-level caches q Critical word first q Subblocking/sectoring q Better replacement/insertion policies q Non-blocking caches (multiple cache misses in parallel) q Multiple accesses per cycle q Software approaches q … 149 Lectures on Cache Optimizations (I) 150https://www.youtube.com/watch?v=OyomXCHNJDA&list=PL5Q2soXY2Zi9OhoVQBXYFIZywZXCPl4M_&index=3 Lectures on Cache Optimizations (II) 151https://www.youtube.com/watch?v=55oYBm9cifI&list=PL5Q2soXY2Zi9JXe3ywQMhylk_d5dI-TM7&index=6 Lectures on Cache Optimizations (III) 152https://www.youtube.com/watch?v=jDHx2K9HxlM&list=PL5PHm2jkkXmi5CxxI7b3JCL1TWybTDtKq&index=21 Lectures on Cache Optimizations n Computer Architecture, Fall 2017, Lecture 3 q Cache Management & Memory Parallelism (ETH, Fall 2017) q https://www.youtube.com/watch?v=OyomXCHNJDA&list=PL5Q2soXY2Zi9OhoVQBX YFIZywZXCPl4M_&index=3 n Computer Architecture, Fall 2018, Lecture 4a q Cache Design (ETH, Fall 2018) q https://www.youtube.com/watch?v=55oYBm9cifI&list=PL5Q2soXY2Zi9JXe3ywQMh ylk_d5dI-TM7&index=6 n Computer Architecture, Spring 2015, Lecture 19 q High Performance Caches (CMU, Spring 2015) q https://www.youtube.com/watch?v=jDHx2K9HxlM&list=PL5PHm2jkkXmi5CxxI7b3J CL1TWybTDtKq&index=21 153https://www.youtube.com/onurmutlulectures Multi-Core Issues in Caching Caches in a Multi-Core System 155 CORE 1L2 CACHE 0SHARED L3 CACHEDRAM INTERFACE CORE 0 CORE 2 CORE 3L2 CACHE 1L2 CACHE 2L2 CACHE 3DRAM BANKS DRAM MEMORY CONTROLLER Caches in a Multi-Core System 156Source: https://www.anandtech.com/show/16252/mac-mini-apple-m1-tested Apple M1, 2021 Caches in a Multi-Core System 157Source: https://twitter.com/Locuza_/status/1454152714930331652 Intel Alder Lake, 2021 Caches in a Multi-Core System 158https://wccftech.com/amd-ryzen-5000-zen-3-vermeer-undressed-high-res-die-shots-close-ups-pictured-detailed/ AMD Ryzen 5000, 2020 Core Count: 8 cores/16 threads L1 Caches: 32 KB per core L2 Caches: 512 KB per core L3 Cache: 32 MB shared Caches in a Multi-Core System 159https://youtu.be/gqAYMx34euU https://www.tech-critter.com/amd-keynote-computex-2021/ https://community.microcenter.com/discussion/5 134/comparing-zen-3-to-zen-2 Additional 64 MB L3 cache die stacked on top of the processor die - Connected using Through Silicon Vias (TSVs) - Total of 96 MB L3 cache AMD increases the L3 size of their 8-core Zen 3 processors from 32 MB to 96 MB 3D Stacking Technology: Example 160https://www.pcgameshardware.de/Ryzen-7-5800X3D-CPU-278064/Specials/3D-V-Cache-Release-1393125/ Caches in a Multi-Core System 161https://www.it-techblog.de/ibm-power10-prozessor-mehr-speicher-mehr-tempo-mehr-sicherheit/09/2020/ IBM POWER10, 2020 Cores: 15-16 cores, 8 threads/core L2 Caches: 2 MB per core L3 Cache: 120 MB shared Caches in a Multi-Core System 162https://www.tomshardware.com/news/infrared-photographer-photos-nvidia-ga102-ampere-silicon Nvidia Ampere, 2020 Cores: 128 Streaming Multiprocessors L1 Cache or Scratchpad: 192KB per SM Can be used as L1 Cache and/or Scratchpad L2 Cache: 40 MB shared Caches in a Multi-Core System 163https://developer.nvidia.com/blog/nvidia-hopper-architecture-in-depth/ Nvidia Hopper, 2022 L1 Cache or Scratchpad: 256KB per SM Can be used as L1 Cache and/or Scratchpad Cores: 144 Streaming Multiprocessors L2 Cache: 60 MB shared Caches in Multi-Core Systems n Cache efficiency becomes even more important in a multi- core/multi-threaded system q Memory bandwidth is at premium q Cache space is a limited resource across cores/threads n How do we design the caches in a multi-core system? n Many decisions and questions q Shared vs. private caches q How to maximize performance of the entire system? q How to provide QoS & predictable perf. to different threads in a shared cache? q Should cache management algorithms be aware of threads? q How should space be allocated to threads in a shared cache? q Should we store data in compressed format in some caches? q How do we do better reuse prediction & management in caches? 164 Private vs. Shared Caches n Private cache: Cache belongs to one core (a shared block can be in multiple caches) n Shared cache: Cache is shared by multiple cores 165 CORE 0 CORE 1 CORE 2 CORE 3 L2 CACHE L2 CACHE L2 CACHE DRAM MEMORY CONTROLLER L2 CACHE CORE 0 CORE 1 CORE 2 CORE 3 DRAM MEMORY CONTROLLER L2 CACHE Resource Sharing Concept and Advantages n Idea: Instead of dedicating a hardware resource to a hardware context, allow multiple contexts to use it q Example resources: functional units, pipeline, caches, buses, memory n Why? + Resource sharing improves utilization/efficiency à throughput q When a resource is left idle by one thread, another thread can use it; no need to replicate shared data + Reduces communication latency q For example, data shared between multiple threads can be kept in the same cache in multithreaded processors + Compatible with the shared memory programming model 166 Resource Sharing Disadvantages n Resource sharing results in contention for resources q When the resource is not idle, another thread cannot use it q If space is occupied by one thread, another thread needs to re- occupy it - Sometimes reduces each or some thread’s performance - Thread performance can be worse than when it is run alone - Eliminates performance isolation à inconsistent performance across runs - Thread performance depends on co-executing threads - Uncontrolled (free-for-all) sharing degrades QoS - Causes unfairness, starvation Need to efficiently and fairly utilize shared resources 167 Private vs. Shared Caches n Private cache: Cache belongs to one core (a shared block can be in multiple caches) n Shared cache: Cache is shared by multiple cores 168 CORE 0 CORE 1 CORE 2 CORE 3 L2 CACHE L2 CACHE L2 CACHE DRAM MEMORY CONTROLLER L2 CACHE CORE 0 CORE 1 CORE 2 CORE 3 DRAM MEMORY CONTROLLER L2 CACHE Shared Caches Between Cores n Advantages: q High effective capacity q Dynamic partitioning of available cache space n No fragmentation due to static partitioning n If one core does not utilize some space, another core can q Easier to maintain coherence (a cache block is in a single location) n Disadvantages q Slower access (cache not tightly coupled with the core) q Cores incur conflict misses due to other cores’ accesses n Misses due to inter-core interference n Some cores can destroy the hit rate of other cores q Guaranteeing a minimum level of service (or fairness) to each core is harder (how much space, how much bandwidth?) 169 Lectures on Multi-Core Cache Management 170https://www.youtube.com/watch?v=7_Tqlw8gxOU&list=PL5Q2soXY2Zi9OhoVQBXYFIZywZXCPl4M_&index=17 Lectures on Multi-Core Cache Management 171https://www.youtube.com/watch?v=c9FhGRB3HoA&list=PL5Q2soXY2Zi9JXe3ywQMhylk_d5dI-TM7&index=29 Lectures on Multi-Core Cache Management 172https://www.youtube.com/watch?v=Siz86__PD4w&list=PL5Q2soXY2Zi9JXe3ywQMhylk_d5dI-TM7&index=30 Lectures on Multi-Core Cache Management n Computer Architecture, Fall 2018, Lecture 18b q Multi-Core Cache Management (ETH, Fall 2018) q https://www.youtube.com/watch?v=c9FhGRB3HoA&list=PL5Q2soXY2Zi9JXe3ywQM hylk_d5dI-TM7&index=29 n Computer Architecture, Fall 2018, Lecture 19a q Multi-Core Cache Management II (ETH, Fall 2018) q https://www.youtube.com/watch?v=Siz86__PD4w&list=PL5Q2soXY2Zi9JXe3ywQM hylk_d5dI-TM7&index=30 n Computer Architecture, Fall 2017, Lecture 15 q Multi-Core Cache Management (ETH, Fall 2017) q https://www.youtube.com/watch?v=7_Tqlw8gxOU&list=PL5Q2soXY2Zi9OhoVQBXY FIZywZXCPl4M_&index=17 173https://www.youtube.com/onurmutlulectures Lectures on Memory Resource Management 174https://www.youtube.com/watch?v=0nnI807nCkc&list=PL5Q2soXY2Zi9xidyIgBxUz7xRPS-wisBN&index=21 Lectures on Memory Resource Management n Computer Architecture, Fall 2020, Lecture 11a q Memory Controllers (ETH, Fall 2020) q https://www.youtube.com/watch?v=TeG773OgiMQ&list=PL5Q2soXY2Zi9xidyIgBxUz 7xRPS-wisBN&index=20 n Computer Architecture, Fall 2020, Lecture 11b q Memory Interference and QoS (ETH, Fall 2020) q https://www.youtube.com/watch?v=0nnI807nCkc&list=PL5Q2soXY2Zi9xidyIgBxUz7 xRPS-wisBN&index=21 n Computer Architecture, Fall 2020, Lecture 13 q Memory Interference and QoS II (ETH, Fall 2020) q https://www.youtube.com/watch?v=Axye9VqQT7w&list=PL5Q2soXY2Zi9xidyIgBxU z7xRPS-wisBN&index=26 n Computer Architecture, Fall 2020, Lecture 2a q Memory Performance Attacks (ETH, Fall 2020) q https://www.youtube.com/watch?v=VJzZbwgBfy8&list=PL5Q2soXY2Zi9xidyIgBxUz7 xRPS-wisBN&index=2 175https://www.youtube.com/onurmutlulectures Cache Coherence Cache Coherence n Basic question: If multiple processors cache the same block, how do they ensure they all see a consistent state? P1 P2 x Interconnection Network Main Memory 1000 The Cache Coherence Problem P1 P2 x Interconnection Network Main Memory ld r2, x 1000 1000 The Cache Coherence Problem P1 P2 x Interconnection Network Main Memory ld r2, x ld r2, x 1000 1000 1000 The Cache Coherence Problem P1 P2 x Interconnection Network Main Memory ld r2, x add r1, r2, r4 st x, r1 ld r2, x 1000 10002000 The Cache Coherence Problem P1 P2 x Interconnection Network Main Memory ld r2, x add r1, r2, r4 st x, r1 ld r2, x 1000 10002000 ld r5, x Should NOT load 1000 A Very Simple Coherence Scheme (VI) n Idea: All caches “snoop” (observe) each other’s write/read operations. If a processor writes to a block, all others invalidate the block. n A simple protocol: 182 n Write-through, no- write-allocate cache n Actions of the local processor on the cache block: PrRd, PrWr, n Actions that are broadcast on the bus for the block: BusRd, BusWr PrWr / BusWr Valid BusWr Invalid PrWr / BusWr PrRd / BusRd PrRd/-- Lecture on Cache Coherence 183https://www.youtube.com/watch?v=T9WlyezeaII&list=PL5Q2soXY2Zi9xidyIgBxUz7xRPS-wisBN&index=38 Lecture on Memory Ordering & Consistency 184https://www.youtube.com/watch?v=Suy09mzTbiQ&list=PL5Q2soXY2Zi9xidyIgBxUz7xRPS-wisBN&index=37 Lecture on Cache Coherence & Consistency n Computer Architecture, Fall 2020, Lecture 21 q Cache Coherence (ETH, Fall 2020) q https://www.youtube.com/watch?v=T9WlyezeaII&list=PL5Q2soXY2Zi9xidyIgBxUz7 xRPS-wisBN&index=38 n Computer Architecture, Fall 2020, Lecture 20 q Memory Ordering & Consistency (ETH, Fall 2020) q https://www.youtube.com/watch?v=Suy09mzTbiQ&list=PL5Q2soXY2Zi9xidyIgBxUz 7xRPS-wisBN&index=37 n Computer Architecture, Spring 2015, Lecture 28 q Memory Consistency & Cache Coherence (CMU, Spring 2015) q https://www.youtube.com/watch?v=JfjT1a0vi4E&list=PL5PHm2jkkXmi5CxxI7b3JCL 1TWybTDtKq&index=32 n Computer Architecture, Spring 2015, Lecture 29 q Cache Coherence (CMU, Spring 2015) q https://www.youtube.com/watch?v=X6DZchnMYcw&list=PL5PHm2jkkXmi5CxxI7b3 JCL1TWybTDtKq&index=33 185https://www.youtube.com/onurmutlulectures","libVersion":"0.3.2","langs":""}