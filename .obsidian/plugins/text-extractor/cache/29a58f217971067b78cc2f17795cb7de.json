{"path":"sem2/DDCA/VRL/slides/DDCA-L22a-GPU-architectures-2.pdf","text":"Digital Design & Computer Arch. Lecture 22a: GPU Architectures II Frank K. Gürkaynak Mohammad Sadrosadati Prof. Onur Mutlu ETH Zürich Spring 2024 17 May 2024 Readings for this Week ◼ Required ◼ Lindholm et al., \"NVIDIA Tesla: A Unified Graphics and Computing Architecture,\" IEEE Micro 2008. ◼ Recommended ❑ Peleg and Weiser, “MMX Technology Extension to the Intel Architecture,” IEEE Micro 1996. 2 Recall: Flynn’s Taxonomy of Computers ◼ Mike Flynn, “Very High-Speed Computing Systems,” Proc. of IEEE, 1966 ◼ SISD: Single instruction operates on single data element ◼ SIMD: Single instruction operates on multiple data elements ❑ Array processor ❑ Vector processor ◼ MISD: Multiple instructions operate on single data element ❑ Closest form: systolic array processor, streaming processor ◼ MIMD: Multiple instructions operate on multiple data elements (multiple instruction streams) ❑ Multiprocessor ❑ Multithreaded processor 3 Recall: Array vs. Vector Processor 4 ARRAY PROCESSOR VECTOR PROCESSOR LD VR  A[3:0] ADD VR  VR, 1 MUL VR  VR, 2 ST A[3:0]  VR Instruction Stream Time LD0 LD1 LD2 LD3 AD0 AD1 AD2 AD3 MU0 MU1 MU2 MU3 ST0 ST1 ST2 ST3 LD0 LD1 AD0 LD2 AD1 MU0 LD3 AD2 MU1 ST0 AD3 MU2 ST1 MU3 ST2 ST3 Space Space Same op @ same time Different ops @ same space Different ops @ time Same op @ space Recall: Array vs. Vector Processors ◼ Array vs. vector processor distinction is a “purist’s” distinction ◼ Most “modern” SIMD processors are a combination of both ❑ They exploit data parallelism in both time and space ❑ GPUs are a prime example we will cover in more detail 5 GPUs (Graphics Processing Units) Recall: GPUs are SIMD Engines Underneath ◼ The instruction pipeline operates like a SIMD pipeline (e.g., an array processor) ◼ However, the programming is done using threads, NOT SIMD instructions ◼ To understand this, let’s go back to our parallelizable code example ◼ But, before that, let’s distinguish between ❑ Programming Model (Software) vs. ❑ Execution Model (Hardware) 7 Recall: A GPU is a SIMD (SIMT) Machine ◼ Except it is not programmed using SIMD instructions ◼ It is programmed using threads (SPMD programming model) ❑ Each thread executes the same code but operates a different piece of data ❑ Each thread has its own context (i.e., can be treated/restarted/executed independently) ◼ A set of threads executing the same instruction are dynamically grouped into a warp (wavefront) by the hardware ❑ A warp is essentially a SIMD operation formed by hardware! 8 Recall: SIMD vs. SIMT Execution Model ◼ SIMD: A single sequential instruction stream of SIMD instructions → each instruction specifies multiple data inputs ❑ [VLD, VLD, VADD, VST], VLEN ◼ SIMT: Multiple instruction streams of scalar instructions → threads grouped dynamically into warps ❑ [LD, LD, ADD, ST], NumThreads ◼ Two Major SIMT Advantages: ❑ Can treat each thread separately → i.e., can execute each thread independently (on any type of scalar pipeline) → MIMD processing ❑ Can group threads into warps flexibly → i.e., can group threads that are supposed to truly execute the same instruction → dynamically obtain and maximize benefits of SIMD processing 9 Warps and Warp-Level FGMT ◼ Warp: A set of threads that execute the same instruction (on different data elements) → SIMT (Nvidia-speak) ◼ All threads run the same code ◼ Warp: The threads that run lengthwise in a woven fabric … Thread Warp 3 Thread Warp 8 Thread Warp 7 Thread Warp Scalar Thread W Scalar Thread X Scalar Thread Y Scalar Thread Z Common PC SIMD Pipeline Lindholm et al., \"NVIDIA Tesla: A Unified Graphics and Computing Architecture,\" IEEE Micro 2008. High-Level View of a GPU Lindholm et al., \"NVIDIA Tesla: A Unified Graphics and Computing Architecture,\" IEEE Micro 2008. Latency Hiding via Warp-Level FGMT ◼ Warp: A set of threads that execute the same instruction (on different data elements) ◼ Fine-grained multithreading ❑ One instruction per thread in pipeline at a time (No interlocking) ❑ Interleave warp execution to hide latencies ◼ Register values of all threads stay in register file ◼ FGMT enables simple pipeline & long latency tolerance ❑ Millions of threads operating on the same large image/video 12 DecodeRFRFRFALUALUALU D-Cache Thread Warp 6 Thread Warp 1 Thread Warp 2DataAll Hit? Miss? Warps accessing memory hierarchy Thread Warp 3 Thread Warp 8 Writeback Warps available for scheduling Thread Warp 7 I-Fetch SIMD Pipeline Slide credit: Tor Aamodt ◼ Same instruction in different threads uses thread id to index and access different data elements SIMT Memory Access (Loads and Stores) Let’s assume N=16, 4 threads per warp → 4 warps 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15+ + + + + Slide credit: Hyesoon Kim Threads Data elements Warp 0 Warp 1 Warp 2 Warp 3 13 For maximum performance, memory should provide enough bandwidth (i.e., elements per cycle throughput to match computation unit throughput) ◼ CPU threads and GPU kernels ❑ Sequential or modestly parallel sections on CPU ❑ Massively parallel sections on GPU: Blocks of threads Serial Code (host) . . . . . . Parallel Kernel (device) KernelA<<<nBlk, nThr>>>(args); Serial Code (host) Parallel Kernel (device) KernelB<<<nBlk, nThr>>>(args); Warps not Exposed to GPU Programmers 14 Slide credit: Hwu & Kirk Sample GPU SIMT Code (Simplified) for (ii = 0; ii < 100000; ++ii) { C[ii] = A[ii] + B[ii]; } // there are 100000 threads __global__ void KernelFunction(…) { int tid = blockDim.x * blockIdx.x + threadIdx.x; int varA = aa[tid]; int varB = bb[tid]; C[tid] = varA + varB; } CPU code CUDA code Slide credit: Hyesoon Kim 15 Sample GPU Program (Less Simplified) 16Slide credit: Hyesoon Kim Lecture on GPU Programming 17https://youtu.be/AkYnuqVpCug Heterogeneous Systems Course (Spring 2022) https://safari.ethz.ch/projects_and_seminars/spring2022/doku.php ?id=heterogeneous_systems https://youtube.com/playlist?list=PL5Q2soXY2Zi9XrgXR38IM_FTjmY6h7Gzm ◼ Short weekly lectures ◼ Hands-on projects 18 Heterogeneous Systems Course (Spring 2023) https://safari.ethz.ch/projects_and_seminars/spring2023/doku.php ?id=heterogeneous_systems https://www.youtube.com/watch?v=8JGo2zylE80&list=PL5Q2soXY2Zi- qSKahS4ofaEwYl7_qp9mw ◼ Short weekly lectures ◼ Hands-on projects From Blocks to Warps ◼ GPU core: A SIMD pipeline ❑ Streaming Processor (SP) ❑ Many such SIMD Processors ◼ Streaming Multiprocessor (SM) ◼ Blocks are divided into warps ❑ SIMD/SIMT unit (32 threads) … t0 t1 t2 … t31 … … t0 t1 t2 … t31 … Block 0’s warps Block 1’s warps … t0 t1 t2 … t31 … Block 2’s warps 20 NVIDIA Fermi architecture SIMD vs. SIMT Execution Model ◼ SIMD: A single sequential instruction stream of SIMD instructions → each instruction specifies multiple data inputs ❑ [VLD, VLD, VADD, VST], VLEN ◼ SIMT: Multiple instruction streams of scalar instructions → threads grouped dynamically into warps ❑ [LD, LD, ADD, ST], NumThreads ◼ Two Major SIMT Advantages: ❑ Can treat each thread separately → i.e., can execute each thread independently on any type of scalar pipeline → MIMD processing ❑ Can group threads into warps flexibly → i.e., can group threads that are supposed to truly execute the same instruction → dynamically obtain and maximize benefits of SIMD processing 21 Threads Can Take Different Paths in Warp-based SIMD ◼ Each thread can have conditional control flow instructions ◼ Threads can execute different control flow paths 22 Thread Warp Common PC Thread 2 Thread 3 Thread 4 Thread 1 B C D E F A G Slide credit: Tor Aamodt Control Flow Problem in GPUs/SIMT ◼ A GPU uses a SIMD pipeline to save area on control logic ❑ Groups scalar threads into warps ◼ Branch divergence occurs when threads inside warps branch to different execution paths 23 Branch Path A Path B Branch Path A Path B Slide credit: Tor Aamodt This is the same as conditional/predicated/masked execution. Recall the Vector Mask and Masked Vector Operations Remember: Each Thread Is Independent ◼ Two Major SIMT Advantages: ❑ Can treat each thread separately → i.e., can execute each thread independently on any type of scalar pipeline → MIMD processing ❑ Can group threads into warps flexibly → i.e., can group threads that are supposed to truly execute the same instruction → dynamically obtain and maximize benefits of SIMD processing ◼ If we have many threads ◼ We can find individual threads that are at the same PC ◼ And, group them together into a single warp dynamically ◼ This reduces “divergence” → improves SIMD utilization ❑ SIMD utilization: fraction of SIMD lanes executing a useful operation (i.e., executing an active thread) 24 Dynamic Warp Formation/Merging ◼ Idea: Dynamically merge threads executing the same instruction, i.e., at the same PC (after branch divergence) ◼ Form new warps from warps that are waiting ❑ Enough threads branching to each path enables the creation of full new warps 25 Warp X Warp Y Warp Z Dynamic Warp Formation/Merging ◼ Idea: Dynamically merge threads executing the same instruction, i.e., at the same PC (after branch divergence) ◼ Fung et al., “Dynamic Warp Formation and Scheduling for Efficient GPU Control Flow,” MICRO 2007. 26 Branch Path A Path B Branch Path A Dynamic Warp Formation Example 27 A A B B G G A AC C D D E E F F Time A A B B G G A AC D E E F Time A x/1111 y/1111 B x/1110 y/0011 C x/1000 y/0010 D x/0110 y/0001 F x/0001 y/1100 E x/1110 y/0011 G x/1111 y/1111 A new warp created from scalar threads of both Warp x and y executing at Basic Block D D Execution of Warp x at Basic Block A Execution of Warp y at Basic Block A Legend AA Baseline Dynamic Warp Formation Slide credit: Tor Aamodt An Example GPU NVIDIA GeForce GTX 285 ◼ NVIDIA-speak: ❑ 240 stream processors ❑ “SIMT execution” ◼ Generic speak: ❑ 30 cores ❑ 8 SIMD functional units per core ◼ NVIDIA, “NVIDIA GeForce GTX 200 GPU. Architectural Overview. White Paper,” 2008. Slide credit: Kayvon Fatahalian 29 NVIDIA GeForce GTX 285 Core … = instruction stream decode= SIMD functional unit, control shared across 8 units = execution context storage = multiply-add = multiply 64 KB of storage for thread contexts (registers) Slide credit: Kayvon Fatahalian 30 NVIDIA GeForce GTX 285 Core … 64 KB of storage for thread contexts (registers) ◼ Groups of 32 threads share instruction stream (each group is a Warp): they execute the same instruction on different data ◼ Up to 32 warps are interleaved in an FGMT manner ◼ Up to 1024 thread contexts can be stored Slide credit: Kayvon Fatahalian NVIDIA GeForce GTX 285 Tex Tex Tex Tex Tex Tex Tex Tex Tex Tex … … … ……… ……… ……… ……… ……… ……… ……… ……… ……… 30 cores on the GTX 285: 30,720 threads Slide credit: Kayvon Fatahalian 32 0.0 5000.0 10000.0 15000.0 20000.0 25000.0 0 1000 2000 3000 4000 5000 6000 7000 8000 GTX 285 (2009) GTX 480 (2010) GTX 780 (2013) GTX 980 (2014) P100 (2016) V100 (2017) A100 (2020)GFLOPS#Functional Units Functional units (stream processors) GFLOPS Evolution of NVIDIA GPUs 33 NVIDIA V100 ◼ NVIDIA-speak: ❑ 5120 stream processors ❑ “SIMT execution” ◼ Generic speak: ❑ 80 cores ❑ 64 SIMD functional units per core ❑ Tensor cores for Machine Learning ◼ NVIDIA, “NVIDIA Tesla V100 GPU Architecture. White Paper,” 2017. 34 NVIDIA V100 Block Diagram 80 cores on the V100 https://devblogs.nvidia.com/inside-volta/ 35 NVIDIA V100 Core 15.7 TFLOPS Single Precision 7.8 TFLOPS Double Precision 125 TFLOPS for Deep Learning (Tensor cores) 36 https://devblogs.nvidia.com/inside-volta/ Edge TPU: Baseline Accelerator DRAM ML Model PE ArrayBuffer Dataflow 64x64 array 2TFLOP/s 4MB on-chip buffer Output ActivationParameter Input Activation =* 37 Introduction TPU and Model Characterization Mensa Framework Mensa-G Evaluation Conclusion ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● Research Lecture on Edge TPU 38https://youtu.be/KPPfRRPENgQ?t=2999 Lecture 18b: Systolic Array Architectures https://www.youtube.com/watch?v=Ayo8uVPvjyw&list=PL5Q2soXY2Zi-EImKxYYY1SZuGiOAOBKaf&index=22 NVIDIA A100 ◼ NVIDIA-speak: ❑ 6912 stream processors ❑ “SIMT execution” ◼ Generic speak: ❑ 108 cores ❑ 64 SIMD functional units per core ❑ Tensor cores for Machine Learning ◼ Support for sparsity ◼ New floating point data type (TF32) ◼ https://developer.nvidia.com/blog/nvidia-ampere-architecture-in-depth/ 40 NVIDIA A100 Block Diagram 108 cores on the A100 (Up to 128 cores in the full-blown chip) 40MB L2 cache https://developer.nvidia.com/blog/nvidia-ampere-architecture-in-depth/ 41 NVIDIA A100 Core 19.5 TFLOPS Single Precision 9.7 TFLOPS Double Precision 312 TFLOPS for Deep Learning (Tensor cores) 42 https://developer.nvidia.com/blog/nvidia-ampere-architecture-in-depth/ 0 10000 20000 30000 40000 50000 60000 0 2000 4000 6000 8000 10000 12000 14000 16000 GTX 285 (2009) GTX 480 (2010) GTX 780 (2013) GTX 980 (2014) P100 (2016) V100 (2017) A100 (2020) H100 (2022)GFLOPS#Functional Units Functional Units (Stream Processors) GFLOPS Evolution of NVIDIA GPUs (Updated) 43 NVIDIA H100 Block Diagram 144 cores on the full GH100 60MB L2 cache https://developer.nvidia.com/blog/nvidia-hopper-architecture-in-depth/ 44 NVIDIA H100 Core 48 TFLOPS Single Precision* 24 TFLOPS Double Precision* 800 TFLOPS (FP16, Tensor Cores)* 45 https://developer.nvidia.com/blog/nvidia-hopper-architecture-in-depth/ * Preliminary performance estimates Digital Design & Computer Arch. Lecture 22a: GPU Architectures Frank K. Gürkaynak Mohammad Sadrosadati Prof. Onur Mutlu ETH Zürich Spring 2024 17 May 2024 Backup SlidesGPU vs. Array/Vector Processor Recall: Vector Instruction Execution 49 VADD A,B → C C[1] C[2] C[0] A[3] B[3] A[4] B[4] A[5] B[5] A[6] B[6] Execution using one pipelined functional unit C[4] C[8] C[0] A[12] B[12] A[16] B[16] A[20] B[20] A[24] B[24] C[5] C[9] C[1] A[13] B[13] A[17] B[17] A[21] B[21] A[25] B[25] C[6] C[10] C[2] A[14] B[14] A[18] B[18] A[22] B[22] A[26] B[26] C[7] C[11] C[3] A[15] B[15] A[19] B[19] A[23] B[23] A[27] B[27] Execution using four pipelined functional units Slide credit: Krste Asanovic Time Space Time Warp Execution (Recall the Previous Slide) 50 32-thread warp executing ADD A[tid],B[tid] → C[tid] C[1] C[2] C[0] A[3] B[3] A[4] B[4] A[5] B[5] A[6] B[6] Execution using one pipelined functional unit C[4] C[8] C[0] A[12] B[12] A[16] B[16] A[20] B[20] A[24] B[24] C[5] C[9] C[1] A[13] B[13] A[17] B[17] A[21] B[21] A[25] B[25] C[6] C[10] C[2] A[14] B[14] A[18] B[18] A[22] B[22] A[26] B[26] C[7] C[11] C[3] A[15] B[15] A[19] B[19] A[23] B[23] A[27] B[27] Execution using four pipelined functional units Slide credit: Krste Asanovic Time Space Time Recall: Vector Unit Structure 51 Lane Functional Unit Partitioned Vector Registers Memory Subsystem Elements 0, 4, 8, … Elements 1, 5, 9, … Elements 2, 6, 10, … Elements 3, 7, 11, … Slide credit: Krste Asanovic 52 Lane Functional Unit Registers for each Thread Memory Subsystem Registers for thread IDs 0, 4, 8, … Registers for thread IDs 1, 5, 9, … Registers for thread IDs 2, 6, 10, … Registers for thread IDs 3, 7, 11, … Slide credit: Krste Asanovic GPU SIMD Execution Unit StructureRecall: Vector Instruction Level Parallelism Can overlap execution of multiple vector instructions ❑ Example machine has 32 elements per vector register and 8 lanes ❑ Example with 24 operations/cycle (steady state) while issuing 1 vector instruction/cycle 53 load load mul mul add add Load Unit Multiply Unit Add Unit time Instruction issue Slide credit: Krste Asanovic Warp Instruction Level Parallelism Can overlap execution of multiple instructions ❑ Example machine has 32 threads per warp and 8 lanes ❑ Completes 24 operations/cycle (steady state) while issuing 1 warp/cycle 54 W3 W0 W1 W4 W2 W5 Load Unit Multiply Unit Add Unit time Warp issue Slide credit: Krste Asanovic Warp-based SIMD vs. Traditional SIMD ◼ Traditional SIMD contains a single thread ❑ Sequential instruction execution; lock-step operations in a SIMD instruction ❑ Programming model is SIMD (no extra threads) → SW needs to know vector length ❑ ISA contains vector/SIMD instructions ◼ Warp-based SIMD consists of multiple scalar threads executing in a SIMD manner (i.e., same instruction executed by all threads) ❑ Does not have to be lock step ❑ Each thread can be treated individually (i.e., placed in a different warp) → programming model not SIMD ◼ SW does not need to know vector length ◼ Enables multithreading and flexible dynamic grouping of threads ❑ ISA is scalar → SIMD operations can be formed dynamically ❑ Essentially, it is SPMD programming model implemented on SIMD hardware 55 SPMD ◼ Single procedure/program, multiple data ❑ This is a programming model rather than computer organization ◼ Each processing element executes the same procedure, except on different data elements ❑ Procedures can synchronize at certain points in program, e.g. barriers ◼ Essentially, multiple instruction streams execute the same program ❑ Each program/procedure 1) works on different data, 2) can execute a different control-flow path, at run-time ❑ Many scientific applications are programmed this way and run on MIMD hardware (multiprocessors) ❑ Modern GPUs programmed in a similar way on a SIMD hardware 56 Warp Scheduling Large Warps and Two-Level Warp Scheduling (II) ◼ Two main reasons for GPU resources be underutilized ❑ Branch divergence ❑ Long latency operations 58 Narasiman et al., “Improving GPU Performance via Large Warps and Two-Level Warp Scheduling,” MICRO 2011. Large Warps and Two-Level Warp Scheduling ◼ Two main reasons for GPU resources be underutilized ❑ Branch divergence ❑ Long latency operations 59 time Core Memory System All Warps Compute Req Warp 0 All Warps Compute Req Warp 1 Req Warp 15 Round Robin Scheduling, 16 total warps Narasiman et al., “Improving GPU Performance via Large Warps and Two-Level Warp Scheduling,” MICRO 2011. Long-latency load Two-Level Scheduling of Warps ◼ Scheduling smaller warp groups reduces stalls due to long latency operations Narasiman et al., “Improving GPU Performance via Large Warps and Two-Level Warp Scheduling,” MICRO 2011. time Core Memory System All Warps Compute Req Warp 0 All Warps Compute Req Warp 1 Req Warp 15 Round Robin Scheduling, 16 total warps time Core Memory System Compute Req Warp 0 Req Warp 1 Req Warp 7 Two Level Round Robin Scheduling, 2 fetch groups, 8 warps each Group 0 Compute Group 1 Req Warp 8 Req Warp 9 Req Warp 15 Compute Group 0 Compute Group 1 Saved Cycles Large Warp Microarchitecture Example Decode Stage 1 0 0 1 0 1 0 0 0 0 1 1 1 0 0 0 0 0 1 0 0 1 0 0 1 0 0 1 0 1 0 0 0 0 0 0 1 1 1 1 0 0 0 0 1 1 1 1 0 0 0 1 1 1 11 1 0 1 Sub-warp 0 mask Sub-warp 0 maskSub-warp 1 mask Sub-warp 0 maskSub-warp 1 maskSub-warp 2 mask 1 1 1 1 1 1 1 1 ◼ Idea: Reduce branch divergence by having large warps ◼ Dynamically break down a large warp into sub-warps Narasiman et al., “Improving GPU Performance via Large Warps and Two-Level Warp Scheduling,” MICRO 2011. Dynamic Warp Formation Example Dynamic Warp Formation Example 63 A A B B G G A AC C D D E E F F Time A A B B G G A AC D E E F Time A x/1111 y/1111 B x/1110 y/0011 C x/1000 y/0010 D x/0110 y/0001 F x/0001 y/1100 E x/1110 y/0011 G x/1111 y/1111 A new warp created from scalar threads of both Warp x and y executing at Basic Block D D Execution of Warp x at Basic Block A Execution of Warp y at Basic Block A Legend AA Baseline Dynamic Warp Formation Slide credit: Tor Aamodt Hardware Constraints Limit Flexibility of Warp Grouping 64 Lane Functional Unit Registers for each Thread Memory Subsystem Registers for thread IDs 0, 4, 8, … Registers for thread IDs 1, 5, 9, … Registers for thread IDs 2, 6, 10, … Registers for thread IDs 3, 7, 11, … Slide credit: Krste Asanovic Can you move any thread flexibly to any lane?","libVersion":"0.3.2","langs":""}