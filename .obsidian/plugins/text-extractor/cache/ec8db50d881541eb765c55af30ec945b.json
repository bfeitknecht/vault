{"path":"sem3/A&D/VRL/extra/rzhang/A&D-rzhang-w04.pdf","text":"Algorithms and Datastructures (HS2024) Week 4 Rui Zhang October 14, 2024 Contents 1 Revision Theory 2 1.1 Searching . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2 1.1.1 Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . 2 1.1.2 Realistic Example . . . . . . . . . . . . . . . . . . . . . . 2 1.1.3 Linear Search . . . . . . . . . . . . . . . . . . . . . . . . . 2 1.1.4 Binary Search . . . . . . . . . . . . . . . . . . . . . . . . . 2 1.1.5 Limits of Comparison Based Searching . . . . . . . . . . . 3 1.2 Sorting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 1.2.1 Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . 4 1.2.2 Formal Problem . . . . . . . . . . . . . . . . . . . . . . . 4 1.2.3 IsSorted . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 1.2.4 Bubblesort . . . . . . . . . . . . . . . . . . . . . . . . . . 4 1.2.5 Selectionsort . . . . . . . . . . . . . . . . . . . . . . . . . 5 1.2.6 Insertionsort . . . . . . . . . . . . . . . . . . . . . . . . . 6 1.2.7 Mergesort . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 2 Exercises 7 2.1 Exercise Sheet 4 - Priority List . . . . . . . . . . . . . . . . . . . 7 2.2 Exercise Sheet 2 - Feedback . . . . . . . . . . . . . . . . . . . . . 7 3 Supplementary Exercises 8 3.1 Induction, Binary Search . . . . . . . . . . . . . . . . . . . . . . . 8 3.2 3.2 Supplement . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8 1 1 Revision Theory 1.1 Searching 1.1.1 Motivation In large computer programs, we often deal with large amounts of data. The last few weeks we have covered writing algorithms for large input data sets. In the real world however, programs often need persistent state. Consider the famous hit game Minecraft: In Minecraft, players get to create worlds and build and destroy blocks. Let us say you have created a architectural beauty consisting of many placed blocks. You would surely want that work of art to persist across sessions of play, no? After storing that data, we need to read the data everytime the player wants to play the game again. However, depending on the way we have stored the progress of the player, we would need to search for the relevant parts of data. 1.1.2 Realistic Example Most of us, so I would assume, are not quite at the level of dealing with searching scenarios described such as above. So let us simplify everything: Assume we are given an array A of length n of numbers and an element k. We are asked to ”find” the element k by returning the index (position of that element) of that element in the array of numbers. For example: [2, 3, 4, 1, 5, 8, 2] with n = 7 and we are searching for some number k. Let that be in this example k = 8. If you were a computer what would be the most simple and naive way of achieving this? 1.1.3 Linear Search The answer is to just go through the entire array element by element until we find what we are searching for. This algorithm has a runtime of O(n), because for an input of size n, we would need at most n checks, as we are checking element by element. Now, we realize that we cannot do any better in the general case. If we know nothing else about the input array, no matter what kind of strategy we come up with to find the index of k, in the worst case scenario, k could still be the last element we check with this strategy, leaving us at a worst case scenario of O(n). 1.1.4 Binary Search We could not do any better, if we knew nothing else. But what if we knew something about the input array? Specifically, assume we know that the input 2 array A is sorted with the elements in ascending order. Formally, ∀ 1 ≤ i, j ≤ n if i < j then A[i] ≤ A[j] (1) In this case, we could perform so called binary search. The idea is to start at in middle of the array. If the element in the middle is equal to k, then we return the middle index and we are done. If the element in the middle is greather than k, then we cut the left half of the array. This is because due to (1) we know for sure that all elements in the left half will be less, and thus not equal to k. Then we repeat the algorithm. If the element in the middle is less than k, then we cut the right half of the array. The reasoning is equivalent here. If at some point, we have cut the array so much that only one element is left, and that element is still not k, then we return that there is no such element k in A. The runtime for this algorithm is O(log n). This is left as an exercise. 1.1.5 Limits of Comparison Based Searching We cannot do any better than O(log n)., assuming that we are using a comparison based searching algorithm. To prove this, let us assume we have an arbitrary search algorithm. Since this algorithm depends on comparisons, and for each comparison it makes, that comparison can either return true or f alse. Thus, the algorithm makes one binary decision for each comparison. To simplify this idea, we may visualize a so called ”decision-tree”, where the right path means ”true” and the left path means ”false”: b < A[1] A[1] > A[2] . . . . . . return 2 . . . return 5 return 7 b == A[1]? return 1 ... return ”not found” true false Note that for this algorithm to be correct, the decision tree must have all possible outputs (”return x”) as end-decisions. Since we have an array of length n and we also have to be able to output ”not found”. This tree must have at least n + 1 so called ”leaves”, i.e. n + 1 elements in the most bottom layer. Now consider the number for elements per layer of this tree, also called ”nodes” In the first layer, we only have 1 node. In the second layer, since we are making binary decisions, we can have at most 2 nodes. In the third at most 4 nodes... Notice that we have at most 2h nodes in layer h. Now, let T be the total number of layers, the so called ”depth” of the tree. As stated above, we have at least n + 1 leaves, so 2 T ≥ number of leaves ≥ n + 1 ≥ 3 n. As such, T ≥ log2(n). Since each layer contains at least one elementary operation (a comparison), we will have at least T ≥ log2(n) many elementary operaions. Thus resulting in a worst-case-runtime of at least Ω(log n). 1.2 Sorting 1.2.1 Motivation Binary Search is pretty good then. But it only works if the input array is sorted. But what if we are not lucky? In this section, We will be looking at the ideas behind the sorting algorithms you should know for this course. We will not be presenting the (pseudo-)code for these algorithms, as these should be in the lecture notes. However, I would recommend trying to implement these algorithms in Java to improve your pro- gramming skills. 1.2.2 Formal Problem Given an input array A of length n, output a permutation of A, A′, such that A′ is sorted with its elements in ascending order, so: A[1] ≤ A[2] ≤ A[3] ≤ · · · ≤ A[n] (2) In our algorithm, we will consider comparisons and swaps between two elements as elementary operations. 1.2.3 IsSorted Before actually sorting, let us first find an idea for an algorithm that can check if an array is sorted. Without this algorithm, we would not even be able to test any of our later sorting algorithms for correctness. To do this, we notice that we only need to compare every two adjacent elements in the array. If any two adjacent elements are not sorted ascendingly, then the array A is most definitely not sorted by definition (2). If every pair of two adjacent elements are sorted ascendingly, then by transitivity of the ”less than or equal relation” (I hope you have learned this in Discrete Mathematics already), we have (2), which precisely implies sortedness. 1.2.4 Bubblesort The proof at the end of last section gives the idea for our first algorithm. To sort an array, we just have to assure that every pair of two adjacent elements are sorted. So, the idea behind bubble sort is to compare all pairs of two adjacent elements and swap them if needed until the entire array is sorted. The best way to go through every pair of two adjacent elements is to go through the array from left to right and check each pair. (3) This will take Θ(n) time. However, doing this once will not be sufficient. You could, as an exercise, think of an example where this is not enough. 4 For the full algorithm, we will have to repeat this step until the array is sorted. But when is that the case? After looking at multiple examples, we will see that every time we repeat (3), the next largest element will ”bubble up” to the top (end) of the array. So every time we repeat (3), we will have sorted at least one element so that it ends up at the correct spot. This gives us our answer: We will need at most O(n) repetitions (also called iterations) of (3) to sort the entire array. This leaves us with a total worst- case runtime of O(n) ∗ O(n) = O(n2). Actually, if we consider comparisons as elementary operations in this sorting algorithm (which we do in the lecture), then we would have Θ(n 2). 1.2.5 Selectionsort Another way to sort an array is to select the largest element in an array first and then put that element into the next largest ”spot” in the array. This is probably quite intuitive. In the first iteration, we go through the entire array and find the largest element by keeping track of the current largest element amax that we have found and its index imax. Whenever we get to another element aj we compare amax with aj. If amax is smaller than aj then we set amax to be the value of aj and set the index imax to be j. After going through the entire array, we must have found the largest element in the array then and swap this element with the last element in the array. In the second iteration, we go through the entire array, excluding the last element and proceed the same way. This obviously makes sense because the last element is the one we have just inserted into the correct ”largest spot”. The largest element we find in this iteration will be the second largest element in the array and we will place it just before the last position (so the ”second largest position”), since that one is already occupied by the largest element. We continue equivalently for all further iterations, excluding one more ele- ment each iteration until we only have one element left and all other ”largest elements” have been inserted. Now, all elements are sorted. Now, what is the runtime? For the number of comparisons, we will go through n elements in the first iteration, n − 1 elements in the second and so on and so fourth, resulting in n∑ i=1 i = n(n + 1) 2 comparisons just to find the maximum for each iteration. That is in Θ(n2). If we are lucky, then the array is sorted and we do not need to swap any two elements at all, since the next largest element is always at its correct spot already. However, in the worst case scenario, we would have to swap each element once (but notice, not more). That results in O(n) many swaps, which is already an improvement over bubblesort. 5 1.2.6 Insertionsort This sort is maybe what you do when you sort cards when playing a card game, from the left side, you have a group of cards that are sorted and then you take the next card and insert it into this group of sorted cards at the correct position (at least this is what people on the internet always say but I don’t know about you but when I sort cards I use the classic ”improvise, adapt, overcome” strategy). Translated into computer science speak, we will take an array and start from the second element. Then, we insert this element into the sorted ”part” (left subarray) that is sorted, which in the beginning, consists of exactly one element (so either we insert the second element in front of the first element if the second element is smaller or leave it where it is if it is larger). After this iteration, we have a sorted subarray consisting of two elements. Now, we continue with the third element and insert it into the correct spot in this new, bigger, better and improved and revolutionized and better and bigger sorted subarray. Notice, ”finding” this correct spot is the same problem we have solved in section 1.1. We could use linear search. But since the subarray we are searching in is sorted, we may also use binary search, which is faster. We just repeat this process n − 1 times until all elements have been put in the respective places and the sorted subarray is just the entire array. Notice that every time we insert some element. We have to shift all elements on the left by one towards the left. Determining the runtime properties (number of total comparisons and swaps) is left as an exercise. (this is definitely not because I am too lazy to write this down because it has already been written in the lecture notes). 1.2.7 Mergesort ”To create good algorithms is to divide-and-conquer.” - Sun Tzu. Here, let Sun Tzu be defined as a really good computer scientist. Like Johannes Lengler. Sorting the entire array is too slow. Let us split it up into two halves and sort them recursively, but splitting those two halves into four fourths. These Four fourths we will sort recursively by splitting them up into 8 eigh- okay you get it. Once we reach an array-partition-size of just one element, we can reconstruct the array by merging two subarrays into one. Resulting in sorted subarrays of size 2. Then we can merge these subarrays of size 2 into subarrays of size 4, until we have merged all subarrays into one. Okay this is cool and all but the first time I was told that mergesort is cool I did not understand how this merging worked. So how do we merge two smaller, sorted subarrays A1 and A2 of equal size into one large sorted subarray double the size? We do this by creating a larger array double the size with empty slots (no values). We start off at the first element of the large array and determine which element to put here. We only have two options actually. Either we take the first element of A1 or the first element of A2 (and we just take the smaller element). 6 We cannot do any other element, since A1 and A2 are sorted. Every time we decide to take one element from one of the two smaller arrays, let us say, A1, then the next time we insert something we will look at the element after that in A1, otherwise we may insert one element twice. Let the number of elements in the larger array be 2n and the number of element in the smaller arrays be n respectively. Since we insert exactly 2n times and compare n times, for each recursion we have a runtime of O(n). How many recursions do we have? Well, we have as many recursions as we divided the array in half. And as we have seen from previous examples, such as Maximum Subarray Sum, those are O(log(n)) levels. Which is why we have a runtime of O(n log(n)). 2 Exercises 2.1 Exercise Sheet 4 - Priority List Again, top is most important, bottom is least important. 1. 4.5), 4.3) Good exercises that could be exam relevant 2. 4.4) builds skills in algorithm development 3. 4.2) obviously important since it’s an old exam exercise, but not bonus 4. 4.1) 2.2 Exercise Sheet 2 - Feedback 1. 2.2d) was not an easy one. The way many of you got to the solution was very informal. If you take the limit of n”some expression in n”, then you have to be really careful. In this case, we had ∞ something negative. This is indeed 0, but very informally speaking. Instead, take a look at the solution, there, they managed to get the n in the base into the exponent by applying the eln(... ) trick we used in the first exercise as well. Refer to this trick in the future if you find yourself in such a situation. 2. You guys really love L’Hopital. This is not bad but think of other ways to compute limits too. 3. I wanted to emphasize that the point of exercise 2.3) was to show that ”backwards proofs” are often susceptible to mistakes! What you are trying to do is to derive a statement S′ from the statement S that you wanted to prove that is obviously true. However, it is important then that the direction of the implications are all valid, especially the implications going from the bottom (from S′) to the top (S). This now becomes an even less 7 intuitive taks because the direction in which you are deriving statements is the exact opposite of the direction the proof goes. In essence, try your best to do a ”forwards proof” first (the one I called ”from left to right” / ”from right to left”). 3 Supplementary Exercises 3.1 Induction, Binary Search Let us define a function T : N → R such that T (n) is the runtime of binary search for an input array of length n. We have that T (1) = c for some constant c, as the runtime for an array consisting of just one element is constant by the above definition. 1. Formulate a recursive formula for T (n), such that it depends on T (n/2). 2. Prove via induction that T (n) ≤ O(log/n) Note: log n is a shorthand way of writing the binary logarithm. 3.2 3.2 Supplement In the exercise sheet that you did last week, 3.2) dealt with substring counting. Which algorithm from the lecture does this exercise remind you of? (If you did not do the exercise you may look at the solutions) 8","libVersion":"0.5.0","langs":""}