{"path":"sem4/W&S/UE/s/W&S-s-u11.pdf","text":"Probability and Statistics (D-INFK) Lecturer: Prof. Dr. Dylan Possamaï Coordinator: Daniel Kršek Probability and Statistics Exercise sheet 11 - Solutions MC 11.1. Let n ∈ N and let X1, . . . , Xn be i.i.d. Bernoulli(p) random variables for some p ∈ (0, 1). (Exactly one answer is correct in each question.) (i) What is the maximum likelihood estimator of p? (a) TM L = X n. (b) TM L = 1/X n. (c) TM L = max{Xi : i ∈ {1, . . . , n}}. (d) TM L = Xn. (ii) Is the maximum likelihood estimator unbiased? (a) Yes. (b) No. (iii) What is the distribution of the random variable n × TM L? (a) n × TM L ∼ Binom(n, p). (b) n × TM L ∼ Geom(p). (c) n × TM L ∼ Binom(n, p/n). (d) n × TM L ∼ Bernoulli(np). Solution: (i) (a). We have the likelihood function L(x1, . . . , xn; p) = n∏ i=1 Pp[Xi = xi] = n∏ i=1 [ (1 − p)1{xi=0} + p1{xi=1}] = n∏ i=1(1 − p) 1−xip xi = (1 − p) ∑n i=1(1−xi)p ∑n i=1 xi. Thus, log L(x1, . . . , xn; p) = log(1 − p) n∑ i=1(1 − xi) + log(p) n∑ i=1 xi, and ∂ ∂p log L(x1, . . . , xn; p) = −1 1 − p n∑ i=1(1 − xi) + 1 p n∑ i=1 xi. This is equal to zero if and only if p = 1 n n∑ i=1 xi, 1 Probability and Statistics (D-INFK) Lecturer: Prof. Dr. Dylan Possamaï Coordinator: Daniel Kršek which gives (a). We can also verify that this point is indeed the maximum: ∂2 ∂p2 log L(x1, . . . , xn; p) = −1 (1 − p)2 n∑ i=1(1 − xi) + −1 p2 n∑ i=1 xi ≤ 0. (ii) (a). We have Ep[X n] = 1 n n∑ i=1 Ep[Xi] = 1 n n∑ i=1 p = p, ∀p ∈ (0, 1). (iii) (a). We have n × TM L = ∑n i=1 Xi. As (Xi)n i=1 are i.i.d. Bernoulli(p) variables, the result follows immediately. MC 11.2. Let X1, . . . , Xn, for n ∈ N be i.i.d. random variables with Xi ∼ U([−θ, θ]) for some unknown θ ∈ (0, ∞). What is the maximum likelihood estimator for θ? (Exactly one answer is correct.) (a) TM L = 2 n ∑n i=1 Xi1{Xi>0}. (b) TM L = 1 n ∑n i=1 2Xi. (c) TM L = max{|Xi| : i ∈ {1, . . . , n}}. (d) The maximum likelihood estimator does not exist. Solution: (c). We have f (xi; θ) = 1 2θ 1{xi∈[−θ,θ]}. Thus, the likelihood function is L(x1, . . . , xn; θ) = n∏ i=1 1 2θ 1{xi∈[−θ,θ]} = ( 1 2θ )n1{x1∈[−θ,θ],...,xn∈[−θ,θ]} = ( 1 2θ )n1{|x1|∈[0,θ],...,|xn|∈[0,θ]}. We see that the function θ ↦→ ( 1 2θ )n is strictly decreasing and positive on (0, ∞). Moreover, the indicator 1{|x1|∈[0,θ],...,|xn|∈[0,θ]} is equal to 0 whenever there is an observation xi such that θ < |xi|. It follows that the likelihood function is maximized at the lowest θ such that θ ≥ |xi|, i ∈ {1, . . . , n} is satisfied, and so ̂θ(x1, . . . , xn) = max{|xi| : i ∈ {1, . . . , n}} is the maximizer. The maximum likelihood estimator is thus TM L = max{|Xi| : i ∈ {1, . . . , n}}. MC 11.3. Let X be a random variable with well-defined positive variance. Find a ∈ R and b > 0 such that for Z := (X − a)/b, it holds that E[Z] = 0 and Var[Z] = 1. (Exactly one answer is correct.) (a) a = E[X], b = Var[X]. 2 Probability and Statistics (D-INFK) Lecturer: Prof. Dr. Dylan Possamaï Coordinator: Daniel Kršek (b) a = E[X], b = √Var[X]. (c) a = E[X 2], b = Var[X]. (d) a = (E[X]) 2, b = Var[X]. Solution: (b) is correct. We compute E[Z] = 1 b (E[X] − a) and Var[Z] = 1 b2 Var[X − a] = 1 b2 Var[X]. We have 1 b2 Var[X] = 1 if and only if b = ±√ Var[X]. Since we require b > 0, we get b = √Var[X]. Furthermore, E[X]−a b = 0 if and only if a = E[X]. Altogether, we obtain Z = X − E[X] √Var[X] . Exercise 11.4. Let X1, . . . , Xn be independent random variables with Xi ∼ N (θαi, 1), where αi ̸= 0 are known parameters, but θ ∈ R is unknown. Find the maximum likelihood estimator for θ. Solution: The marginal densities are given by fXi(xi; θ) = 1√2π exp (− 1 2 (xi − θαi) 2). Due to inde- pendence, the likelihood function is L(x1, . . . , xn; θ) = n∏ i=1 fXi(xi; θ) = n∏ i=1 1 √2π exp ( − 1 2 (xi − θαi) 2) . We compute log L(x1, . . . , xn; θ) = n∑ i=1 ( − 1 2 log(2π) − 1 2 (xi − θαi) 2) . We want to maximize this with respect to θ, so we compute ∂ ∂θ log L(x1, . . . , xn; θ) = − n∑ i=1(xi − θαi)(−αi) = n∑ i=1(xi − θαi)αi. Since ∑n i=1 α2 i > 0, we have n∑ i=1(xi − θαi)αi = 0 ⇐⇒ n∑ i=1 xiαi − θ n∑ i=1 α2 i = 0 ⇐⇒ θ = ∑n i=1 xiαi∑n i=1 α2 i . Furthermore, ∂2 ∂θ2 log L(x1, . . . , xn; θ) = − n∑ i=1 α2 i ≤ 0. Hence, the function θ ↦→ log L(x1, . . . , xn; θ) is concave, and ̂θ(x1, . . . , xn) = ∑n i=1 xiαi∑n i=1 α2 i 3 Probability and Statistics (D-INFK) Lecturer: Prof. Dr. Dylan Possamaï Coordinator: Daniel Kršek is the maximizer. The maximum likelihood estimator for θ is thus TM L = ∑n i=1 Xiαi∑n i=1 α2 i . Exercise 11.5. In a sawmill, the sawn timber of a certain sorting class undergoes quality control. Each production day, a sample of ten boards is taken, and each board is tested for its stiffness. Based on experience, it can be assumed that the stiffness of a board is normally distributed with a known standard deviation σ = 1430 MPa1. (a) Derive the formula for the 95% confidence interval for µ after 15 production days. You can assume that the samples are mutually independent. (b) Compute the realized confidence interval from (a) for a sample mean of x = 11,000 MPa (after 15 production days). (c) How many samples would be required to ensure that the width of the confidence interval is less than 200 MPa? Solution: (a) Since the sum of independent and normally distributed random variables is normally distributed, we know that X n := 1 n n∑ i=1 Xi ∼ N (µ, σ2 n ) . Therefore, P  −z1−α/2 < X n − µ √ σ2 n < z1−α/2   = 1 − α, where z1−α/2 = Φ −1(1 − α 2 ) is the (1 − α 2 )-quantile of the standard normal distribution. Rear- ranging gives P [ X n − z1−α/2 σ √n < µ < X n + z1−α/2 σ √n ] = 1 − α. Thus, the confidence interval has the form (X n − z1−α/2 σ √n , X n + z1−α/2 σ √n ) . The confidence interval after 15 production days (with samples of 10 boards each) is thus for α = 0.05 equal to (X n − 1.96 σ √150 , X n + 1.96 σ √150 ) , where we found in the table that Φ−1(1 − α 2 ) = Φ −1(0.975) = 1.96. 1The pascal is a derived SI unit of pressure and mechanical stress. It is named after Blaise Pascal and defined as follows: 1 Pa = 1 kg×m−1×s−2 = 1 N×m−2. One pascal is the pressure exerted by a force of one newton on an area of one square meter. 1 MPa = 106 Pa = 10 bar. 4 Probability and Statistics (D-INFK) Lecturer: Prof. Dr. Dylan Possamaï Coordinator: Daniel Kršek (b) From (a), we know that the confidence interval has the form (X n − 1.96 σ √n , X n + 1.96 σ √n ) . Substituting the values gives us (10771.15, 11228.85). (c) The width of the confidence interval is given by 2z1−α/2 σ√n . Hence, we require: 2z1−α/2 σ √n ≤ 200, z1−α/2σ 100 ≤ √n, z2 1−α/2σ2 1002 ≤ n. Plugging in the values gives z2 1−α/2σ2 1002 = 785.57. This means n ≥ 786 must hold. Therefore, either samples of 10 boards must be taken on at least 79 production days, or at least 53 boards must be tested per day over 15 production days. Exercise 11.6. Let Θ = [1/2, 1]. We consider the model family (Pθ)θ∈Θ, where X1, . . . , Xn are independent and identically distributed under Pθ with X1 ∼ Geom(θ). The maximum likelihood estimator for θ is given by (check!) TM L = n ∑n i=1 Xi . Find an approximate 95% confidence interval for θ. Hint: For all θ ∈ [1/2, 1], we have √1−θ θ ≤ √2. You can use this to simplify the expression. Note that enlarging the interval only increases the probability of coverage. However, for θ close to 1, the interval might be unnecessarily wide. Hint 2: Recall that we know that Eθ[X1] = 1 θ and Varθ[X1] = 1 − θ θ2 . Remark: In practice, if the variance is unknown, we typically replace it with an estimate. Either we use the generic estimator ˆσ2 n = 1 n − 1 n∑ i=1(Xi − X n) 2, or, since in this case Var[X1] = 1−θ θ2 and we have an estimator for θ, we can also use ˜σ2 n = 1 − TM L T 2 M L . As both estimators are consistent in this case, we still have by Slutsky’s theorem that ∑n i=1 Xi − n θ√ nˆσ2 n ≈ N (0, 1) and ∑n i=1 Xi − n θ√n˜σ2 n ≈ N (0, 1). 5 Probability and Statistics (D-INFK) Lecturer: Prof. Dr. Dylan Possamaï Coordinator: Daniel Kršek Solution: Since X1 ∼ Geom(θ) has expectation 1/θ and variance (1 − θ)/θ2, we have that ∑n i=1 Xi − n θ√ n 1−θ θ2 is approximately standard normal for n → ∞, by the CLT. By the hint, √1 − θ/θ ≤ √2 for all θ ∈ [1/2, 1], so it follows that Pθ [ −1.96 ≤ ∑n i=1 Xi − n/θ √2n ≤ 1.96] ≥ Pθ [ −1.96 ≤ ∑n i=1 Xi − n/θ √n(1 − θ)/θ2 ≤ 1.96 ] ≈ 0.95. We have −1.96 ≤ ∑n i=1 Xi − n θ√2n ≤ 1.96 −1.96 n ≤ 1 n ∑n i=1 Xi − 1 θ√2n ≤ 1.96 n −1.96 n ≤ T −1 M L − 1 θ√2n ≤ 1.96 n −T −1 M L + −1.96√2 √n ≤ − 1 θ ≤ −T −1 M L − −1.96√2 √n T −1 M L + −1.96√2 √n ≤ 1 θ ≤ T −1 M L + 1.96√2 √n 1 T −1 M L + 1.96√2√n ≤ θ ≤ 1 T −1 M L − 1.96√2√n . Note that since we are seeking an approximate confidence interval and assume that n is large, we may also assume √n > 1.96√2 and thus ∑n i=1 Xi ≥ n > 1.96 √2n, because Xi takes values in N. Thus, all the terms in the second-to-last line are positive, so we can take the inverse. We conclude that an approximate 95% confidence interval for θ is   1 T −1 M L + 1.96√2√n , 1 T −1 M L − 1.96√2√n   . Exercise 11.7. To estimate the number N of trout in a lake, the following procedure is used (capture- recapture method): In a first step, 500 trout are caught, marked, and released back into the lake. In a second step, 200 trout are caught again, and the number X of marked trout is recorded. (a) X is modeled as a binomial random variable, X ∼ Binom(n, θ), where θ denotes the probability that a fish caught in the second step is marked. What is the value of n? What is the value of the parameter θ if the total number of trout in the lake is N = 2000 or N = 5000? (b) The actual observed value for X is 40. Give a reasonable estimate for the parameter θ, and derive from it an estimate for the total number N of trout in the lake. 6 Probability and Statistics (D-INFK) Lecturer: Prof. Dr. Dylan Possamaï Coordinator: Daniel Kršek (c) Determine an approximate 95% confidence interval for θ, and from that, an approximate 95% confidence interval for N . Solution: (a) Since 200 fish are caught in the second step, n = 200. The probability of catching a marked fish in this step is θ = Number of marked fish Total number of fish in the lake = 500 N . (1) For N = 2000, we get θ = 1/4, and for N = 5000, we get θ = 1/10. (b) We know that if Y1, . . . , Yn are i.i.d. Bernoulli(p) random variables, then the maximim likelihood estimator of p is 1 n ∑n i=1 Yi, where ∑n i=1 Yi ∼ Binom(n, p). Thus, in our example, it makes sense to estimate θ by T = X/n. The observed estimate is T (θ)(ω) = X(ω)/n = 40/200 = 1/5. Solving equation (1) for N gives the estimator T (N ) = 500 T (θ) = 500n X , with observed value T (N )(ω) = 500 T (θ)(ω) = 2500. (c) Since X ∼ Binom(n, θ) has expectation nθ and variance nθ(1 − θ), the statistic X − nθ √nθ(1 − θ) is approximately standard normal distributed for large n, by the central limit theorem. For all θ ∈ (0, 1), we then have Pθ [ −1.96 ≤ X − nθ √ n/4 ≤ 1.96 ] ≥ Pθ [ −1.96 ≤ X − nθ √nθ(1 − θ) ≤ 1.96 ] ≈ 0.95, where we used the bound θ(1 − θ) ≤ 1/4 for all θ ∈ [0, 1]. Thus, an approximate 95% confidence interval for θ is [ T (θ) − 1.96 2 √n , T (θ) + 1.96 2√n ] . With the observed value T (θ)(ω) = 1/5, this gives the interval [0.13, 0.27]. Using equation (1), we convert this to an approximate 95% confidence interval for N as: [ 500 0.27 , 500 0.13 ] = [1851, 3846]. 7 Probability and Statistics (D-INFK) Lecturer: Prof. Dr. Dylan Possamaï Coordinator: Daniel Kršek Quantile table for the standard normal distribution 0.5 0.75 0.9 0.95 0.975 0.99 0.995 0.999 0 0.6745 1.2816 1.6449 1.9600 2.3263 2.5758 3.0902 . For instance, Φ−1(0.9) = 1.2816, where Φ is the distribution function of N (0, 1). Table of standard normal distribution 0.00 0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.0 0.5000 0.5040 0.5080 0.5120 0.5160 0.5199 0.5239 0.5279 0.5319 0.5359 0.1 0.5398 0.5438 0.5478 0.5517 0.5557 0.5596 0.5636 0.5675 0.5714 0.5753 0.2 0.5793 0.5832 0.5871 0.5910 0.5948 0.5987 0.6026 0.6064 0.6103 0.6141 0.3 0.6179 0.6217 0.6255 0.6293 0.6331 0.6368 0.6406 0.6443 0.6480 0.6517 0.4 0.6554 0.6591 0.6628 0.6664 0.6700 0.6736 0.6772 0.6808 0.6844 0.6879 0.5 0.6915 0.6950 0.6985 0.7019 0.7054 0.7088 0.7123 0.7157 0.7190 0.7224 0.6 0.7257 0.7291 0.7324 0.7357 0.7389 0.7422 0.7454 0.7486 0.7517 0.7549 0.7 0.7580 0.7611 0.7642 0.7673 0.7704 0.7734 0.7764 0.7794 0.7823 0.7852 0.8 0.7881 0.7910 0.7939 0.7967 0.7995 0.8023 0.8051 0.8078 0.8106 0.8133 0.9 0.8159 0.8186 0.8212 0.8238 0.8264 0.8289 0.8315 0.8340 0.8365 0.8389 1.0 0.8413 0.8438 0.8461 0.8485 0.8508 0.8531 0.8554 0.8577 0.8599 0.8621 1.1 0.8643 0.8665 0.8686 0.8708 0.8729 0.8749 0.8770 0.8790 0.8810 0.8830 1.2 0.8849 0.8869 0.8888 0.8907 0.8925 0.8944 0.8962 0.8980 0.8997 0.9015 1.3 0.9032 0.9049 0.9066 0.9082 0.9099 0.9115 0.9131 0.9147 0.9162 0.9177 1.4 0.9192 0.9207 0.9222 0.9236 0.9251 0.9265 0.9279 0.9292 0.9306 0.9319 1.5 0.9332 0.9345 0.9357 0.9370 0.9382 0.9394 0.9406 0.9418 0.9429 0.9441 1.6 0.9452 0.9463 0.9474 0.9484 0.9495 0.9505 0.9515 0.9525 0.9535 0.9545 1.7 0.9554 0.9564 0.9573 0.9582 0.9591 0.9599 0.9608 0.9616 0.9625 0.9633 1.8 0.9641 0.9649 0.9656 0.9664 0.9671 0.9678 0.9686 0.9693 0.9699 0.9706 1.9 0.9713 0.9719 0.9726 0.9732 0.9738 0.9744 0.9750 0.9756 0.9761 0.9767 2.0 0.9772 0.9778 0.9783 0.9788 0.9793 0.9798 0.9803 0.9808 0.9812 0.9817 2.1 0.9821 0.9826 0.9830 0.9834 0.9838 0.9842 0.9846 0.9850 0.9854 0.9857 2.2 0.9861 0.9864 0.9868 0.9871 0.9875 0.9878 0.9881 0.9884 0.9887 0.9890 2.3 0.9893 0.9896 0.9898 0.9901 0.9904 0.9906 0.9909 0.9911 0.9913 0.9916 2.4 0.9918 0.9920 0.9922 0.9925 0.9927 0.9929 0.9931 0.9932 0.9934 0.9936 2.5 0.9938 0.9940 0.9941 0.9943 0.9945 0.9946 0.9948 0.9949 0.9951 0.9952 2.6 0.9953 0.9955 0.9956 0.9957 0.9959 0.9960 0.9961 0.9962 0.9963 0.9964 2.7 0.9965 0.9966 0.9967 0.9968 0.9969 0.9970 0.9971 0.9972 0.9973 0.9974 2.8 0.9974 0.9975 0.9976 0.9977 0.9977 0.9978 0.9979 0.9979 0.9980 0.9981 2.9 0.9981 0.9982 0.9982 0.9983 0.9984 0.9984 0.9985 0.9985 0.9986 0.9986 3.0 0.9987 0.9987 0.9987 0.9988 0.9988 0.9989 0.9989 0.9989 0.9990 0.9990 For instance, P[Z ≤ 1.96] = 0.975. 8","libVersion":"0.5.0","langs":""}