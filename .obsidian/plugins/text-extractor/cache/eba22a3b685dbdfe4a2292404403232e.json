{"path":"sem3/LinAlg/VRL/extra/plans/LinAlg-plan-w06.pdf","text":"Lecture plan Linear Algebra (401-0131-00L, HS24), ETH Z ¨urich Numbering of Sections, Definitions, Figures, etc. as in the Lecture Notes Week 6 Vector spaces (Section 4.1) Question Answer (by example) What is a vector? An element of some R m What is a mammal? A cat Answers (by definition): A mammal [. . . ] is a vertebrate animal of the class Mammalia. Mammals are characterized by the presence of milk-producing mammary glands [. . . ] 1 A vector is an element of a vector space. Vector spaces are characterized by the presence of two operations on their elements: vector addition and scalar multiplication. Definition 4.1: A vector space is a triple (V, +, ·) where V is a set (the vectors), and + : V × V → V is a function (vector addition), · : R × V → V is a function (scalar multiplication), satisfying the following axioms (rules) for all u, v, w ∈ V and all λ, µ ∈ R. 1. v + w = w + v commutativity 2. u + (v + w) = (u + v) + w associativity 3. There is a vector 0 such that v + 0 = v for all v zero vector 4. There is a vector −v such that v + (−v) = 0 negative vector 5. 1 · v = v identity element 6. (λ·µ)v = λ · (µ · v) compatibility of · and · in R 7. λ(v + w) = λv + λw distributivity over + 8. (λ+µ)v = λv + µv distributivity over + in R Observation 4.2: (Rm, +, ·), with “+” as in Definition 1.1 and “·” as in Definition 1.3, is a vector space. Definition 4.3: A polynomial p is a sum of the form p = m∑ i=0 pixi, for some m ∈ N. x: a variable; p0, p1, . . . , pm ∈ R the coefficients. Largest i such that pi ̸= 0: degree of p. If all pi are 0: zero polynomial (degree −1). 1https://en.wikipedia.org/wiki/Mammal 1 Examples: p = 2x2 + x + 1 : degree 2 q = 5x − 2 : degree 1 p + q = 2x2 + 6x − 1 : “+” 5p = 10x2 + 5x + 5 : “·” Lemma 4.4: Let R[x] be the set of polynomials in one variable x. Then (R[x], +, ·) is a vector space. Proof. Check the obvious! Lemma 4.5: Let Rm×n be the set of m × n matrices, with A + B and λA defined in the usual way (Definition 2.2). Then (R m×n, +, ·) is a vector space. Proving the obvious: vector spaces behave as expected (from Rm). Example: Fact 4.6: Let (V, +, ·) be a vector space. V contains exactly one zero vector (a vector satis- fying axiom 3). Proof. Take two zero vectors 0 and 0′. Then 0′ = 0 ′ + 0 (axiom 3: 0 is a zero vector) = 0 + 0′ (axiom 1: commutativity) = 0 (axiom 3: 0′ is a zero vector) Abuse of notation: (V, +, ·) → V Subspaces: Definition 4.8: Let V be a vector space. A nonempty subset U ⊆ V is a subspace of V if the following two axioms hold for all v, w ∈ U and all λ ∈ R. (i) v + w ∈ U ; (ii) λv ∈ U . We always have 0 ∈ U : take any u ∈ U , then 0u = 0 ∈ U by (ii). Needs “obvious” Fact 4.10. xyzxyzxyz subspaces of R3: a line a plane not a subspace (misses 0) 2 Lemma 4.11: Let A be an m × n matrix. Then C(A) is a subspace of R m. Proof. Let v, w ∈ C(A): there exist x, y ∈ R n such that v = Ax, w = Ay. A(x + y ︸ ︷︷ ︸ ∈Rn ) = Ax + Ay = v + w ⇒ v + w ∈ C(A) ⇒ subspace axiom (i). For λ ∈ R, A( λx︸︷︷︸ ∈Rn ) = λAx = λv ⇒ λv ∈ C(A) ⇒ subspace axiom (ii). Lemma 4.12: Let V be a vector space and U a subspace. Then U is also a vector space (with the same “+” and “·” as V ). Proof. Check the (almost) obvious! Subspaces of. . . . . . R[x]: The polynomials without constant term: p = m∑ i=0 pixi where p0 = 0 The quadratic polynomials: lookalike of (isomorphic to) R 3 p = p0 + p1x + p2x2 R[x] “contains” R 1, R 2, R3, R 4, . . . (constant, linear, quadratic, cubic,. . . polynomials)! . . . R 2×2: isomorphic to R4 The symmetric matrices: [a b b d ] The matrices of trace 0: [a b c d ] , where a + d = 0 Bases and dimension (Section 4.2) Basis of V : linearly independent vectors whose span is V . Formal definition uses set of vectors, not sequence (more practical; handles infinite case). Definition 4.13: Let V be a vector space, G ⊆ V a (possibly infinite) subset of vectors. A linear combination of G is a sum of the form ∑ v∈F λvv, where F ⊆ G is a finite subset of G and λv ∈ R for all v ∈ F . 3 Lemma 4.14: Let V be a vector space, G ⊆ V . Every linear combination of G ⊆ V is again in V . Proof. Linear combination (of finite F ⊆ G, in some order): ∑n j=1 λjvj. • wj := λjvj ∈ V for all j (function · : R × V → V ) • w1 + w2 ∈ V (function + : V × V → V ) • Repeat: (w1 + w2) ︸ ︷︷ ︸ ∈V +w3 ∈ V , and so on, until w1 + w2 + · · · + wn ∈ V Why not infinite linear combinations? Previous lemma may fail (example: polynomials)! G: the unit monomials: 1, x 2, x 3, . . .; ∑ p∈G 1p = ∑∞ i=0 xi is not a polynomial. Definition 4.15: Let V be a vector space, G ⊆ V a subset of vectors. Span(G): set of all linear combinations of G. G is linearly independent if no vector v ∈ G is a linear combination of G \\ {v}. Definition 4.16: Let V be a vector space. B ⊆ V is a basis of V if B is linearly independent and Span(B) = V . Examples: (For linear independence, use private nonzero argument!) vector space V basis B R m {e1, e2, . . . , em} C(A) (subspace of R m) independent columns of A 2 × 2 symmetric matrices (subspace of R2×2) {[ 1 0 0 0 ] , [0 1 1 0 ] , [0 0 0 1 ]} R[x] (polynomials) {xi : i = 0, 1, . . .} (infinite set) {0} (smallest vector space) (empty set) There can be many bases: Observation 4.18: Every set B = {v1, v2, . . . , vm} of m linearly independent vectors is a basis of R m. Proof. Still need Span(B) = Rm (every v ∈ Rm is a linear combination of B). A: m×m matrix with columns v1, v2, . . . , vm. Theorem 3.11: Ax = v has a unique solution ⇒ v = m∑ j=1 xjvj ︸ ︷︷ ︸ Ax 4 Steinitz exchange lemma: Lemma 4.19: Let V be a vector space, F ⊆ V finite and linearly independent, G ⊆ V finite with Span(G) = V . Then (i) |F | ≤ |G|. (ii) There exists a subset E ⊆ G of size |G| − |F | such that Span(F ∪ E) = V . Remark: |F ∪ E| ≤ |G| (E is allowed to contain elements of F ). Proof. Induction on f = |F |. f = 0 (F = ∅): (i) clear, for (ii), take E = G. f > 0: choose u ∈ F , F ′ = F \\ {u}, g = |G|. F ′ is also linearly independent. Induction hypothesis: (i) g ≥ f − 1. (ii) There exists a subset E′ ⊆ G of size g − (f − 1) with Span(F ′ ∪ E′) = V . u ∈ V = Span(F ′ ∪ E′), u /∈ Span(F ′) (F linearly independent!) ⇓ ⇓ u = ∑ v∈F ′∪E′ λvv, λw ̸= 0 for some w ∈ E′ (⋆) ⇒ |E′| = g − (f − 1) ≥ 1 ⇔ g ≥ f ⇒ (i) for size f . (ii) for size f : E = E′ \\ {w}; solve (⋆) for w: w = 1 µw ( u − ∑ v∈F ′∪E λvv ) Lemma 1.23: ⇒ w is linear combination of F ∪E ︷ ︸︸ ︷ {u} ∪ F ′ ∪ E : Span(F ∪ E) = Span( F ∪E′ ︷ ︸︸ ︷ F ∪ E ∪ {w})==(⋆) : u is linear combination of F ′ ∪ E′ : Span(F ′ ∪ E′) ︸ ︷︷ ︸ V = Span(F ′ ∪ E′ ∪ {u} ︸ ︷︷ ︸ F ∪E′ ) Theorem 4.20: Let V be a vector space; B, B′ ⊆ V two finite bases of V . Then |B| = |B′|. Proof. As bases, B and B′ are linearly independent, and Span(B) = Span(B′) = V . Apply Steinitz exchange lemma (i): • F = B, G = B′ ⇒ |B| ≤ |B′| 5 • F = B′, G = B ⇒ |B′| ≤ |B|. Also works without “finite” (case of polynomials). For infinite sets, |B| = |B′| means “the same kind of infinity”. Does every vector space have a basis? Yes! Here: the “finite” case. Definition 4.21: A vector space V is called finitely generated if there exists a finite subset G ⊆ V with Span(G) = V . R m: finitely generated (G = {e1, e2, . . . , em}) R[x]: not finitely generated Theorem 4.22: Let V be a finitely generated vector space, G ⊆ V a finite subset with Span(G) = V . Then V has a basis B ⊆ G. Proof. If G is linearly independent, B = G is a basis by Definition 4.16. “line 1” Otherwise, some v ∈ G is a linear combination of the other vectors ⇒ Span(G \\ {v}) = Span(G) = V (Lemma 1.23). Replace G with G \\ {v} (still spans V ) and go to line 1. G gets smaller in every step: this finally stops with B = G. Dimension: Definition 4.23: Let V be a finitely generated vector space. Then dim(V ), the dimension of V , is the size of any basis B of V . dim(R m) = m (no surprise) xyzxyzxyz dim = 0 dim = 1 dim = 2 Simplified basis criterion: Lemma 4.24: Let V be a vector space with dim(V ) = d. (i) Let F ⊆ V be a set of d linearly independent vectors. Then F is a basis of V . (ii) Let G ⊆ V be a set of d vectors with Span(G) = V . Then G is a basis of V . 6 Proof. (i): Let G be a basis of V . Steinitz exchange Lemma 4.19 (ii) applies with F and G. |F | = |G| = d ⇒ E = ∅. Span(F ) = Span(F ∪ E) = V ⇒ F is a basis. (ii) We find a basis B ⊆ G of size d (Theorem 4.22). |B| = |G| ⇒ B = G ⇒ G is a basis. 7","libVersion":"0.5.0","langs":""}