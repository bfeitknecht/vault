{"path":"sem4/DMDB/VRL/extra/slides/DMDB-FS25-Distributed-Commit.pdf","text":"Data Modeling and Databases Spring Semester 2025 2 Phase Commit 2 Phase Commit Gustavo Alonso Institute of Computing Platforms Department of Computer Science ETH Zürich 1 Atomic Commitment 2 Phase Commit 2 The Agreement Problem 2 Phase Commit 3 Phase Commit Applications The Consensus Problem • Distributed consensus is the problem of reaching an agreement among all working processes on the value of a variable • Consensus is not a difficult problem if the system is reliable (no site failures, no communication failures) • Asynchronous = no timing assumptions can be made about the speed of processes or the network delay (it is not possible to distinguish between a failure and a slow system) The impossibility result implies that there is always a chance to remain uncertain (unable to make a decision), hence: • If failures may occur, then all entirely asynchronous commit protocols may block. • No commit protocol can guarantee independent recovery (if a site fails when being uncertain, upon recovery it will have to find out from others what the decision was). 2 Phase Commit 3 In an asynchronous environment where failures can occur there is no protocol that can guarantee consensus is reached Generals problem • To succeed the generals must attack at the same time • The generals can only communicate through messages • The system is asynchronous: messages can be lost or delayed indefinitely • The impossibility in the generals problem arises from the need to have complete knowledge: I need to know my state, the other’s state, that the other knows my state, that the other knows that I know her state, that the other knows that I know that she knows my state … • If the system is entirely asynchronous, this problem cannot be solved by simply exchanging messages • There are many forms of this problem and atomic commitment is one of them: • all sites must decide on whether to commit or abort a transaction and all must make the same decision 2 Phase Commit 4 Under these circumstances, the generals will never be able to agree on a simultaneous attack, that is, they can never reach consensus Atomic Commitment Properties to enforce: • AC1 = All processors that reach a decision reach the same one (agreement, consensus). • AC2 = A processor cannot reverse its decision. • AC3 = Commit can only be decided if all processors vote YES (no imposed decisions). • AC4 = If there are no failures and all processors voted YES, the decision will be to commit (non triviality). • AC5 = Consider an execution with normal failures. If all failures are repaired and no more failures occur for sufficiently long, then all processors will eventually reach a decision (liveness). 2 Phase Commit 5 2PC Protocol and correctness PROTOCOL: • Coordinator sends VOTE-REQ to all participants. • Upon receiving a VOTE-REQ, a participant sends a message with YES or NO (if the vote is NO, the participant aborts the transaction and stops). • Coordinator collects all votes: • All YES = Commit and send COMMIT to all others. • Some NO = Abort and send ABORT to all which voted YES. • A participant receiving COMMIT or ABORT messages from the coordinator decides accordingly and stops. CORRECTNESS: The protocol meets the 5 AC conditions (I - V): • AC1 = every processor decides what the coordinator decides (if one decides to abort, the coordinator will decide to abort). • AC2 = any processor arriving at a decision “stops”. • AC3 = the coordinator will decide to commit if all decide to commit (all vote YES). • AC4 = if there are no failures and everybody votes YES, the decision will be to commit. • AC5 = the protocol needs to be extended in case of failures (in case of timeout, a site may need to “ask around”). 2 Phase Commit 6 Timeout Possibilities 2 Phase Commit 7 COORDINATOR send VOTE-REQ wait for votes send COMMIT send ABORT COMMIT ABORT all vote YES some vote NO Timeout Possibilities 2 Phase Commit 8 PARTICIPANT wait for VOTE-REQ wait for decision ABORT COMMITvote YES vote NO ABORT received COMMIT received Timeout and termination In those three waiting periods: • If the coordinator times-out waiting for votes: it can decide to abort (nobody has decided anything yet, or if they have, it has been to abort) • If a participant times-out waiting for VOTE-REQ: it can decide to abort (nobody has decided anything yet, or if they have, it has been to abort) • If a participant times-out waiting for a decision: it cannot decide anything unilaterally, it must ask (run a Cooperative Termination Protocol). If everybody is in the same situation no decision can be made: all processors will block. This state is called uncertainty period When in doubt, ask. If anybody has decided, they will tell us what the decision was: • There is always at least one processor that has decided or is able to decide (the coordinator has no uncertainty period). Thus, if all failures are repaired, all processors will eventually reach a decision • If the coordinator fails after receiving all YES votes but before sending any COMMIT message: all participants are uncertain and will not be able to decide anything until the coordinator recovers. This is the blocking behavior of 2PC (compare with the impossibility result discussed previously) 2 Phase Commit 9 Recovery and persistence Processors must know their state to be able to tell others whether they have reached a decision. This state must be persistent: • Persistence is achieved by writing a log record. This requires flushing the log buffer to disk (I/O). • This is done for every state change in the protocol. • This is done for every distributed transaction. • This is expensive! • When sending VOTE-REQ, the coordinator writes a START-2PC log record (to know the coordinator). • If a participant votes YES, it writes a YES record in the log BEFORE it send its vote. If it votes NO, then it writes a NO record. • If the coordinator decides to commit or abort, it writes a COMMIT or ABORT record before sending any message. • After receiving the coordinator’s decision, a participant writes its own decision in the log. 2 Phase Commit 10 Linear 2PC • Linear 2PC commit exploits a particular network configuration to minimize the number of messages: 2 Phase Commit 11 YES ... YES YES COM Linear 2PC • The total number of messages is 2n instead of 3n, but the number of rounds is 2n instead of 3 2 Phase Commit 12 YES YES NO NO NO NO 3 Phase Commit Protocol 2PC may block if the coordinator fails after having sent a VOTE-REQ to all processes and all processes vote YES. It is possible to reduce the window of vulnerability even further by using a slightly more complex protocol (3PC). In practice 3PC is not used. It is too expensive (more than 2PC) and the probability of blocking is considered to be small enough to allow using 2PC instead. But 3PC is a good way to understand better the subtleties of atomic commitment We will consider two versions of 3PC: • One capable of tolerating only site failures (no communication failures). Blocking occurs only when there is a total failure (every process is down). This version is useful if all participants reside in the same site. • One capable of tolerating both site and communication failures (based on quorums). But blocking is still possible if no quorum can be formed. 2 Phase Commit 13 Blocking in 2PC Why does a process block in 2PC? • If a process fails and everybody else is uncertain, there is no way to know whether this process has committed or aborted (NOTE: the coordinator has no uncertainty period. To block the coordinator must fail). • Note, however, that the fact that everybody is uncertain implies everybody voted YES! • Why, then, uncertain processes cannot reach a decision among themselves? The reason why uncertain process cannot make a decision is that being uncertain does not mean all other processes are uncertain. Processes may have decided and then failed. To avoid this situation, 3PC enforces the following rule: • NB rule: No operational process can decide to commit if there are operational processes that are uncertain. How does the NB rule prevent blocking? 2 Phase Commit 14 Avoiding Blocking in 3PC The NB rule guarantees that if anybody is uncertain, nobody can have decided to commit. Thus, when running the cooperative termination protocol, if a process finds out that everybody else is uncertain, they can all safely decide to abort. • The consequence of the NB rule is that the coordinator cannot make a decision by itself as in 2PC. Before making a decision, it must be sure that everybody is out of the uncertainty area. Therefore, the coordinator, must first tell all processes what is going to happen: (request votes, prepare to commit, commit). This implies yet another round of messages! 2 Phase Commit 15 3PC Coordinator 2 Phase Commit 16 bcast vote-req wait for votes ABORT COMMIT bcast commit bcast abort bcast pre-commit wait for ACKs Possible time-out actions all vote YES some vote NO all ACKs received 3PC Participant 2 Phase Commit 17 wait for vote-req ABORT COMMIT wait for pre-commit send ACK wait for commit Possible time-out actions vote YES abort received vote NO pre-commit received commit received 3PC and Knowledge (using the NB rule) 3PC is interesting in that the processes know what will happen before it happens: • Once the coordinator reaches the “bcast pre- commit”, it knows the decision will be to commit. • Once a participant receives the pre-commit message from the coordinator, it knows that the decision will be to commit. Why is the extra-round of messages useful? • The extra round of messages is used to spread knowledge across the system. They provide information about what is going on at other processes (NB rule). The NB rule is used when time-outs occur (remember, however, that there are no communication failures): • If coordinator times out waiting for votes = ABORT. • If participant times out waiting for vote-req = ABORT. • If coordinator times out waiting for ACKs = ignore those who did not sent the ACK! (at this stage everybody has agreed to commit). • If participant times out waiting for pre-commit = still in the uncertainty period, ask around. • If participant times out waiting for commit message = not uncertain any more but needs to ask around! 2 Phase Commit 18 Persistence and recovery in 3PC Similarly to 2PC, a process has to remember its previous actions to be able to participate in any decision. This is accomplished by recording every step in the log: • Coordinator writes “start-3PC” record before doing anything. It writes an “abort” or “commit” record before sending any abort or commit message. • Participant writes its YES vote to the log before sending it to the coordinator. If it votes NO, it writes it to the log after sending it to the coordinator. When reaching a decision, it writes it in the log (abort or commit). Processes in 3PC cannot independently recover unless they had already reached a decision or they have not participated at all: • If the coordinator recovers and finds a “start 3PC” record in its log but no decision record, it needs to ask around to find out what the decision was. If it does not find a “start 3PC”, it will find no records of the transaction, then it can decide to abort. • If a participant has a YES vote in its log but no decision record, it must ask around. If it has not voted, it can decide to abort. 2 Phase Commit 19 Termination Protocol • Elect a new coordinator. • New coordinator sends a “state req” to all processes. Participants send their state (aborted, committed, uncertain, committable). • TR1 = If some “aborted” received, then abort. • TR2 = If some “committed” received, then commit. • TR3 = If all uncertain, then abort. • TR4 = If some “committable” but no “committed” received, then send “PRE- COMMIT” to all, wait for ACKs and send commit message. TR4 is similar to 3PC, have we actually solved the problem? • Yes, failures of the participants on the termination protocol can be ignored. At this stage, the coordinator knows that everybody is uncertain, those who have not sent an ACK have failed and cannot have made a decision. Therefore, all remaining can safely decide to commit after going over the pre-commit and commit phases. • The problem is when the new coordinator fails after asking for the state but before sending any pre-commit message. In this case, we have to start all over again. 2 Phase Commit 20 Partition and total failures This protocol does not tolerate communication failures: • A site decides to vote NO, but its message is lost. • All vote YES and then a partition occurs. Assume the sides of the partition are A and B and all processes in A are uncertain and all processes in B are committable. When they run the termination protocol, those in A will decide to abort and those in B will decide to commit. • This can be avoided is quorums are used, that is, no decision can be made without having a quorum of processes who agree (this reintroduces the possibility of blocking, all processes in A will block). Total failures require special treatment, if after the total failure every process is still uncertain, it is necessary to find out which process was the last on to fail. If the last one to fail is found and is still uncertain, then all can decide to abort. • Why? Because of partitions. Everybody votes YES, then all processes in A fail. Processes in B will decide to commit once the coordinator times out waiting for ACKs. Then all processes in B fail. Processes in A recover. They run the termination protocol and they are all uncertain. Following the termination protocol will lead them to abort. 2 Phase Commit 21 CAP Theorem • Eric Brewer’s CAP Theorem: • In a distributed system, you can have 2 of these three properties: consistency, Availability, or tolerance for network partitions • Consistency: you see the same result everywhere (all copies are identical) • Availability: one can continue working even if part of the system fails • Network partitions: one can continue working even if the system split into two or more parts that cannot communicate with each other 2 Phase Commit 22 Defining availability 2 Phase Commit 23 Uptime Downtime in one year 99% (two 9’s) 87.6 hours 99.9% (three 9’s) 8.76 hours 99.99% (four 9’s) 53 min 99.999% (five 9’s) 5 min 99.9999% (six9’s) 32 sec 99.99999% (seven 9’s) 3 sec About the CAP theorem • It is really not a theorem … • … and a lot has been said about it (see next slide) • But it captures the essence of the problem: • Consistency is the desired goal • Availability is a property but not obvious that a system that is available but where the data is inconsistent is truly working (or is useful) • Network partitions are an operational problem affecting availability and may make consistency impossible • A, C, and P are not symmetric and they imply systems that make no sense • At the end of the day, it is a complex trade-off involving performance and cost. 2 Phase Commit 24 Unavailable or slow? • In practice, availability has many aspects: • A slow system might be available but it could be useless 2 Phase Commit 25 https://www.akamai.com/us/en/multimedia/documents/repor t/akamai-state-of-online-retail-performance-spring-2017.pdf References for those interested • Eric Brewer, \"CAP twelve years later: How the 'rules' have changed\", Computer, Volume 45, Issue 2 (2012), pg. 23–29. • Daniel Abadi, Consistency Tradeoffs in Modern Distributed Database System Design: CAP is Only Part of the Story. Computer 45(2): 37-42 (2012) 2 Phase Commit 26 Distributed databases 272 Phase Commit Distributed DB ▪ There are different ways that a distributed database can be constructed: ➢ Shared Memory ➢ Shared Disk ➢ Shared Nothing ▪ We focus on Shared Nothing architecture. ➢ Master receives queries ➢ Each worker is a single-machine DB that uses what we covered so far to answer queries. ➢ Master coordinate: optimization, planning, concurrency control, logging and recovery. … Master Worker 1 Worker 2 Worker N Network 282 Phase Commit Distributed DB ▪ Why do we want a distributed DB? ➢ Data is too large to fit into a single machine. ➢ Computation intensive queries that require computation power beyond a single machine to be fast. ➢ How fast can you scan data when you have N machines? Aggregated I/O bandwidths ▪ Basic idea: ➢ Partition the database to each worker, and each worker only deals with its own partition. ▪ Goal: Data Transparency ➢ Users should not be required to know where data is physically located; or how tables are partitioned. ➢ In the ideal case, users should not know they are using a distributed DB. … Master Worker 1 Worker 2 Worker N Network 292 Phase Commit Naïve Table Partitioning a b c R(a, b, c) a d S(a, d) Worker 1 Worker 2 a b c R(a, b, c) a d S(a, d) SELECT * FROM R, S WHERE R.b = 1 AND S.d = 2 AND R.a = S.a; SELECT * FROM R WHERE R.b = 1; SELECT * FROM S WHERE S.d = 2; a b c R’(a, b, c) a d S’(a, d) SELECT * FROM R', S' WHERE R’.a = S’.a; Benefits: •Worker 1 and Worker 2 can process two relations concurrently. •If R'(a,b,c) or S'(a, d) is small, the communication is small. 302 Phase Commit Naïve Table Partitioning a b c R(a, b, c) a d S(a, d) Worker 1 Worker 2 a b c R(a, b, c) a d S(a, d) SELECT * FROM R, S WHERE R.b = 1 AND S.d = 2 AND R.a = S.a; SELECT * FROM R WHERE R.b = 1; SELECT * FROM S WHERE S.d = 2; a b c R’(a, b, c) a d S’(a, d) SELECT * FROM R', S' WHERE R’.a = S’.a; Problems: •What if one table does not fit into a single machine? •Queries that can be parallelized is actually quite limited under this model. 312 Phase Commit Horizontal Partitioning a b c 1 2 3 4 R(a, b, c) a d 1 2 3 4 S(a, d) Worker 1 Worker 2 a b c 1 2 R1(a, b, c) a d 1 2 S1(a, d) SELECT * FROM R, S WHERE R.a = S.a; SELECT * FROM R1, S1 WHERE R1.a = S1.a; SELECT * FROM R2, S2 WHERE R2.a = S2.a; a b c d T1 SELECT * FROM T1 UNION SELECT * FROM T2 Benefits: •When the result is small, can be fast since each machine is doing their job independently and concurrently. a b c 3 4 R2(a, b, c) a d 3 4 S2(a, d) range/hash a b c d T2 322 Phase Commit Distributed QO a b c 1 2 3 4 R(a, b, c) a d 1 2 3 4 S(a, d) Worker 1 Worker 2 a b c 1 2 R1(a, b, c) a d 1 2 3 4 S(a, d) a b c 3 4 R2(a, b, c) a d 1 2 3 4 S(a, d) range/hash ▪ Example 1: One table is replicated at every node. each node joins its local data and then sends their results to a coordinating node. ▪ ▪ Worker 1: 𝑇1 = 𝑅1 ⋈ 𝑆 ▪ Worker 2: 𝑇2 = 𝑅2 ⋈ 𝑆 ▪ Communicate: 𝑇1 ∪ 𝑇2 SELECT * FROM R, S WHERE R.a = S.a; 332 Phase Commit Distributed QO a b c 1 2 3 4 R(a, b, c) a d 1 2 3 4 S(a, d) Worker 1 Worker 2 a b c 1 2 R1(a, b, c) a d 1 2 S1(a, d) a b c 3 4 R2(a, b, c) a d 3 4 S2(a, d) range/hash ▪ Example 2: Tables are partitioned on the join attribute. Each node performs the join on local data and then sends to the same node. ▪ ▪ Worker 1: 𝑇1 = 𝑅1 ⋈ 𝑆1 ▪ Worker 2: 𝑇2 = 𝑅2 ⋈ 𝑆2 ▪ Communicate: 𝑇1 ∪ 𝑇2 SELECT * FROM R, S WHERE R.a = S.a; 342 Phase Commit Distributed QO a b c 1 2 3 4 R(a, b, c) a d 1 2 3 4 S(a, d) Worker 1 Worker 2 a b c 1 2 R1(a, b, c) a d 1 3 S1(a, d) a b c 3 4 R2(a, b, c) a d 2 4 S2(a, d) range/hash ▪ Example 3: Both tables are partitioned on different keys. If one of the tables is small, then the DBMS broadcasts the small table to all nodes. ▪ ▪ Worker 1: 𝑇1 = 𝑅1 ⋈ (𝑆1 ∪ 𝑆2) ▪ Worker 2: 𝑇2 = 𝑅2 ⋈ (𝑆1 ∪ 𝑆2) ▪ Communicate: 𝑇1 ∪ 𝑇2 SELECT * FROM R, S WHERE R.a = S.a; 352 Phase Commit","libVersion":"0.5.0","langs":""}