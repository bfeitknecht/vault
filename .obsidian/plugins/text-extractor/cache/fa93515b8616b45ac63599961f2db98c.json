{"path":"sem2/PProg/VRL/extra/slides/PProg-L06-architecture.pdf","text":"Parallel Programming Parallel Architectures: Parallelism on the Hardware Level Big Picture (Part I) 2 CPU OS JVM (Process A) Core Core Core Core OS thread OS thread OS thread OS thread OS scheduler JVM scheduler JVM thread Process B Memory Space A Memory Space B Physical Memory JVM thread JVM thread JVM threadL03-05 L06 L07 L08-09 L10-L11 L13 ‚Ä¶ ‚Ä¶ Stack Registers PC Stack Registers PC Stack Registers PC Stack Registers PC Parallel performance & algorithms L12Virtual threads Parallel vs. Concurrent (Recap) In practice, these terms are often used interchangeably Key concerns: Parallelism: Use extra resources to solve a problem faster Concurrency: Correctly and efficiently manage access to shared resources Parallel and Concurrent vs. Distributed (Preview) Common assumption for parallel and concurrent: ‚Ä¢ one ‚Äúsystem‚Äù Distributed computing: ‚Ä¢ Physical separation, administrative separation, different domains, multiple systems Reliability / Fault tolerance is a big issue in distributed computing ‚Ä¢ Also for parallel computing ‚Ä¢ Some of the approaches developed for distributed systems may find their way into parallel systems. Motivation for material to come Get some high-level intuition about: ‚Ä¢ Architectural challenges & choices ‚Ä¢ Why architects have turned to multicores Useful for parallel programming ‚Ä¢ Due to performance implications (caches, locality) ‚Ä¢ Some challenges & choices transfer to software Today's computers: different appearances ‚Ä¶‚Ä¶ similar from the inside CPU Memory Basic principles of today's computers Based on the Von Neumann architecture (or Princeton arch.): program data and program instructions in the same memory Wasn't always like this: see ENIAC, the first general purpose (1945, Turing-complete) computer; used Harvard arch. Von Neumann arch. is simpler: one address space (data and code), one bus John von Neumann (1903-1957). Von Neumann architecture ‚Äúmatches‚Äù imperative programming languages (PL) such as Java: statement executes, then another statement, then a 3rd statement, etc‚Ä¶ Have imperative languages been designed in some sense to ‚Äò‚Äômatch‚Äô‚Äô hardware rather than human thinking? 10 John Backus (IBM Research), Turing award winner 1977 co-inventor of Fortran, 1st high level imperative programming language co-inventor BNF (Backus-Naur Form), used to define formal languages CPU Caches CPUs grew faster Memories grew bigger Accessing memory became slower than accessing CPU registers Locality: ‚Ä¢ Data locality/locality of reference: related storage locations (spatial) are often accessed shortly after each other (temporal) ‚Ä¢ (Modularity/Encapsulation: reason locally, e.g. one thread at a time) CPUs and Memory Hierarchies CPU reads/writes values from/to main memory, to compute with them ‚Ä¶ ‚Ä¶ with a hierarchy of memory caches in between Faster memory is more expensive, hence smaller: L1 is 5x faster than L2, which is 30x faster than main memory, which is 350x faster than disk 12 Main Memory (32GB) CPU L2 cache (32MB) L1 cache (32KB) Memory size and speed are approximated but realistic numbers CPUs and Memory Hierarchies Multi-core CPUs have caches per core ‚Üí more complicated hierarchies 13 Main Memory (32GB) L3 cache L1 CPU Core Core CoreCore L2 L1 L2 L1 L2 L1 L2 18 Code example: 01_cache_effects How can we make computations faster (on hardware level)? Parallel Execution I.e., additional execution units (that are actually used) 3 approaches to apply parallelism to improve sequential processor performance ‚Ä¢ Vectorization: Exposed to developers ‚Ä¢ Instruction Level Parallelism (ILP): Inside CPU ‚Ä¢ Pipelining: Also internal, but transfers to software Vectorization X Z X Y OP X0 X1 X2 X3 Y0 Y1 Y2 Y3 Z0 Z1 Z2 Z3 OP OP OP OP Single Instruction (OP), applied to Multiple Data = SIMD Example: adding vectors X and Y Step 1: load (mem->registers) Step 2: Operation Step 3: store (registers->mem) Standard way: 1-at-a-time Vectorized way: N-at-a-time X X0 X1 X2 X3 Y Yo Y1 Y2 Y3 24 Code example: 02_gcc_vectorize 3 approaches to apply parallelism to improve sequential processor performance ‚Ä¢ Vectorization ‚Ä¢ Instruction Level Parallelism (ILP) ‚Ä¢ Pipelining Modern CPUs exploit Instruction Level Parallelism (ILP)ILP: a very simple example Consider the following program: 1: e = a + b // this and the one below are independent 2: f = c + d // can execute these 2 instructions in parallel 3: m = e * f // this one depends on results above, so has to wait Independent, if ‚Ä¢ different register names ‚Ä¢ different memory addresses 2831 Code example: 03_reordering Instruction Level Parallelism (ILP) ‚Ä¢ Enable ILP: Superscalar CPUs ‚Ä¢ Multiple instructions per cycle / multiple functional units ‚Ä¢ Increase opportunities for ILP: ‚Ä¢ Speculative execution ‚Ä¢ Predict results to continue execution ‚Ä¢ Out-of-Order (OoO) execution ‚Ä¢ Potentially change execution order of instructions ‚Ä¢ As long as the programmer observes the sequential program order ‚Ä¢ Pipelining √† next 3 approaches to apply parallelism to improve sequential processor performance ‚Ä¢ Vectorization ‚Ä¢ Instruction Level Parallelism (ILP) ‚Ä¢ Pipelining Washing clothesWashing clothes ‚Äì Pipeline ‚Ä¢ Additional functional units ‚Ä¢ Multiple ‚Äúinputs‚Äù (stream) I0 I0 I0 I1 I1 I1 I1 1st batch (Input 0) 2nd batch (Input 1) T0 T T2T1 T3 I0 Balanced Pipeline balanced = all steps require same timePipeline Characteristics/Metrics ‚Ä¢ Throughput = amount of work that can be done by a system in a given period of time ‚Ä¢ Latency = time needed to perform a given computation (e.g., a CPU instruction) More exists, e.g. bandwidth (amount of work done in parallel) 38 Throughput ‚Ä¢ Throughput = amount of work that can be done by a system in a given period of time ‚Ä¢ In CPUs: # of instructions completed per second ‚Ä¢ Larger is better Throughput bound = ! \"#$ %&'()*+*,&-*,'. /*+0./ (ignoring lead-in and lead-out time in pipeline with large number of states; cannot do better than this) 39 Latency ‚Ä¢ Latency = time to perform a computation (e.g., a CPU instruction) ‚Ä¢ In CPU: time required to execute a single instructions in the pipeline ‚Ä¢ Lower is better Latency bound = #stages\t¬û max ùëêùëúùëöùëùùë¢ùë°ùëéùë°ùëñùëúùëõùë°ùëñùëöùëí ùë†ùë°ùëéùëîùëíùë† ‚Ä¢ Pipeline latency only constant over time if pipeline balanced: sum of execution times of each stage 41 Washing clothes ‚Äì Unbalanced Pipeline Takes 5 seconds. We use ‚Äúw‚Äù for Washer next. Takes 10 seconds. We use ‚Äúd‚Äù for Dryer next. Takes 5 seconds. We use ‚Äúf‚Äù for Folding next. Takes 10 seconds. We use ‚Äúc‚Äù for Closet next. Designing a pipeline: 1st Attempt (lets consider 5 washing loads) Time (s) Load # 0 5 10 15 20 25 30 35 40 45 50 55 60 65 70 Load 1 w d d f c c Load 2 w _ d d f c c Load 3 w _ _ d d f c c Load 4 w _ _ _ d d f c c Load 5 w _ _ _ _ d d f c c The total time for all 5 loads is 70 seconds. This pipeline can work, however it cannot bound the latency of a Load as it keeps growing. If we want to bound this latency, one approach is to make each stage take as much time as the longest one, thus balancing it. In our example, the longest time is 10 seconds, so we can do the following: Make Pipeline balanced by increasing time for each stage to match longest stage Now takes 10 seconds. Takes 10 seconds, as before. Now takes 10 seconds. Takes 10 seconds, as before. Designing a pipeline: 2nd Attempt Time (s) Load # 0 10 20 30 40 50 60 70 80 90 100 110 60 65 70 75 80 85 90 Load 1 w d f c Load 2 w d f c Load 3 w d f c Load 4 w d f c Load 5 w d f c This pipeline is a bit wasteful, but the latency is bound at 40 seconds for each Load. Throughput here is about 1 load / 10 seconds, so about 6 loads / minute. So now we have the total time for all 5 loads at 80 seconds, higher than before. Can we somehow get a bound on latency while improving the time/throughput? Step 1: make the pipeline from 1st attempt a bit more fine-grained: Like in the 1st attempt, this takes 5 seconds. Lets have 2 dryers working in a row. The first dryer is referred to as d1 and takes 4 seconds, the second as d2 and takes 6 sec. Like in the 1st attempt, it takes 5 seconds. Lets have 2 closets working in a row. The first closet is referred to as c1 and takes 4 seconds, the second as c2 and takes 6 sec. Step 2: and also, like in the 2nd pipeline, make each stage take as much time as the longest stage does from Step 1 [this is 6 seconds due to d2 and c2] It now takes 6 seconds. Each of d1 and d2 dryers take 6 seconds. Now takes 6 seconds. Each of c1 and c2 closets now take 6 seconds. Designing a pipeline: 3rd Attempt (lets consider 5 washing loads) Time (s) Load # 0 6 12 18 24 30 36 42 48 54 60 110 60 65 70 75 80 85 90 Load 1 w d1 d2 f c1 c2 Load 2 w d1 d2 f c1 c2 Load 3 w d1 d2 f c1 c2 Load 4 w d1 d2 f c1 c2 Load 5 w d1 d2 f c1 c2 The bound on latency for each load is now: 6 * 6 = 36 seconds. The throughput is approximately: 1 load / 6 seconds = ~ 10 loads / minute. The total time for all 5 loads is 60 seconds. Throughput vs. Latency Throughput optimization may increase the latency: in our 3rd pipeline attempt, we split the dryers into 2, but it could be that the split of ‚Äòd‚Äô into d1 and d2 leads to higher times for d1 and d2 than 4 and 6. Pipelining typically adds constant time overhead between individual stages (synchronization, communication) √û Infinitely small pipeline steps not practical √û Time it takes to get one complete task through the pipeline may take longer than with a serial implementation 51 CPU Pipeline (Classical RISC) Multiple stages (CPU functional units) ‚Ä¢ Each instruction takes 5 time units (cycles) ‚Ä¢ 1 instr. / cycle (not always possible, though) Parallelism (multiple hardware functional units) Leads to faster execution of sequential programs Actual pipelines potentially much more complicated Instr. Fetch Instr. Decode Execution Mem. access Writeback (remaining slides not exam relevant)CPU Pipeline (Classical RISC) ‚Ä¢ Fetches next instruction from memory (CPU‚Äôs instruction pointer) ‚Ä¢ May prefetch additional instructions (ILP, speculative execution) Instr. Fetch Instr. Decode Execution Mem. access Writeback CPU Pipeline (Classical RISC) Prepares instructions for execution, e.g. ‚Ä¢ Decodes bit sequence 01101‚Ä¶ into ADD A1 A2 ‚Ä¢ ‚ÄúUnderstands‚Äù registers denoted by A1/A2 (RISC instructions either transfer data between memory and CPU registers, or compute on registers) Instr. Decode Execution Mem. access WritebackInstr. Fetch CPU Pipeline (Classical RISC) Executes decoded instruction (in CPU, no data transfer yet) Execution Mem. access WritebackInstr. Fetch Instr. Decode CPU Pipeline (Classical RISC) Exchange data between CPU and memory (if needed, depending on executed instruction) Mem. access WritebackInstr. Fetch Instr. Decode Execution CPU Pipeline (Classical RISC) Updates registers (if needed, depending on executed instruction; also special registers such as flags) WritebackInstr. Fetch Instr. Decode Execution Mem. access For a long time... ‚Ä¢ CPU architects improved sequential execution by exploiting Moore's law and ILP ‚Ä¢ more transistors ‚Üí used for add. CPU pipeline stages ‚Üí more performance ‚Ä¢ sequential programs were becoming exponentially faster with each new CPUs ‚Ä¢ Most programmers did not worry about performance ‚Ä¢ They waited for the next CPU generation But architects hit walls ‚Ä¢ power (dissipation) wall ‚Ä¢ faster CPU ‚Üí consumes more energy ‚Üí expensive to cool ‚Ä¢ memory wall ‚Ä¢ CPUs faster than memory access ‚Ä¢ ILP wall ‚Ä¢ Limits in inherent program's ILP, complexity no longer affordable to increase sequential CPU performance Multicore processors ‚Ä¢ Use transistors to add cores: ‚Äúexternalize‚Äù parallelization to developers ‚Ä¶ ‚Ä¢ ‚Ä¶ instead of improving sequential performance: ‚Äúinternalize‚Äù parallelization to hardware designers ‚Ä¢ Expose parallelism to software ‚Ä¢ Implication: programmers need to write parallel programs to take advantage of new hardware ‚Ä¢ Past: parallel programming was performed by select few ‚Ä¢ Now (since 2008): ETH teaches parallel programming in the first year √† programmers need to worry about (parallel) performance Parallel Architectures Shared / Distributed memory architecturesShared memory architectures ‚Ä¢ Simultaneous Multithreading (Hyper-Threading) ‚Ä¢ Multicores ‚Ä¢ Symmetric Multiprocessor System ‚Ä¢ Non-Uniform Memory Access less resource sharing at hardware level Simultaneous Multithreading ‚Ä¢ Single physical core, but multiple logical/virtual cores (to OS) ‚Ä¢ Certain pipelines steps (e.g. decode) duplicated ‚Ä¢ Multiple instruction streams (called threads) ‚Ä¢ Between ILP and multicore ‚Ä¢ ILP: multiple units for one instr. stream ‚Ä¢ SMT: multiple units for multiple instr. streams ‚Ä¢ Multicore: completely duplicated cores ‚Ä¢ Limited parallel performance, but can increase pipeline utilization if CPU would otherwise have to wait for memory (CPU stalling) Intel‚Äôs Hyper-threading ‚Ä¢ First mainstream attempt to expose parallelism to developers ‚Ä¢ Push users to structure their software in parallel units ‚Ä¢ Motivation for developers: potentially gain performance (not ‚Äúonly‚Äù reactivity) Multicores ‚Ä¢ Single chip, multiple cores ‚Ä¢ Dual-, quad-, octa-... ‚Ä¢ Each core has its own hardware units ‚Ä¢ Computations in parallel perform well ‚Ä¢ Might share part of the cache hierarchy AMD Bulldozer (2011) Between multicore and simultaneous multithreading ‚Üí hybrid design ‚Ä¢ 2x cores integer ‚Ä¢ 1x core floating point (image taken from wikipedia) ‚Ä¢ Multiple chips (CPUs) on the same system ‚Ä¢ CPUs share main memory (same cost to access memory for each CPU) ‚Ä¢ Communication through main memory ‚Ä¢ CPUs still have caches, could have multiple cores Symmetric Multi ProcessingNon-Uniform Memory Access ‚Ä¢ Main memory is distributed ‚Ä¢ Accessing local/remote memory is faster/slower ‚Ä¢ Shared memory interface Distributed Memory ‚Ä¢ Clusters, Data Warehouses ‚Ä¢ Large-scale machines ‚Ä¢ See top500 ‚Ä¢ Message Passing ‚Ä¢ MPI ‚Ä¢ ‚Ä¶ Shared vs Distributed Memory ‚Ä¢ The categorization is about the native programming/ communication interface the system provides ‚Ä¢ Shared: directly access memory locations (x.f) [implicit communication] ‚Ä¢ Distributed: send messages (e.g. over network) to access/exchange data [explicit communcation] ‚Ä¢ Shared memory systems still need to exchange messages between processors (synchronize caches) ‚Ä¢ It is possible to program (via suitable abstractions) ‚Ä¢ shared memory systems as distributed memory systems ‚Ä¢ distributed memory systems as shared memory systems Summary ‚Ä¢ Parallelism is used to improve performance (at all levels) ‚Ä¢ Architects cannot improve sequential CPU performance anymore ‚Ä¢ ‚ÄúMulticore era‚Äù ‚Üí programmers need to write parallel programs ‚Ä¢ Shared vs distributed memory architectures Further reading material (for the interested)","libVersion":"0.3.2","langs":""}