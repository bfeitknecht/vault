{"path":"sem5/VLSI1/VRL/extra/Top-Down-Digital-VLSI-Design/Chapter-3---From-Algorithms-to-Architecture_2015_Top-Down-Digital-VLSI-Desig.pdf","text":"CHAPTER 3 FROM ALGORITHMS TO ARCHITECTURES 3.1 THE GOALS OF ARCHITECTURE DESIGN VLSI architecture design is concerned with deciding on the necessary hardware resources for carrying out computations from data and/or signal processing and with organizing their interplay such as to meet target specifications defined by marketing. The foremost concern is to get the desired functionality right. The second priority is to meet some given performance target often expressed in terms of data throughput or operation rate. A third objective, of economic nature this time, is to minimize production costs. Assuming a given fabrication process, this implies minimizing circuit size and maximizing fabrication yield such as to obtain as many functioning parts per processed wafer as possible. Another general concern in VLSI design is energy efficiency. Battery-operated equipment, such as hand-held phones, tablet and laptop computers, digital hearing aids, etc. obviously impose stringent limits on the acceptable power consumption. It is perhaps less evident that energy efficiency is also of interest when power gets supplied from the mains. One reason for this is the cost of removing the heat generated by high-performance high-density ICs. The capability to change from one mode of operation to another in very little time, the flexibility to accommodate evolving needs and/or to upgrade to future standards are other highly desirable qualities and subsumed here under the term agility. Last but not least, two distinct architectures are likely to differ in terms of the overall engineering effort required to work them out in full detail and, hence also, in their respective time to market. 3.1.1 AGENDA Driven by dissimilar applications and priorities, hardware engineers have, over the years, devised a multitude of diverse architectural concepts that we will put into perspective in this chapter. Section 3.2, opposes program-controlled and hardwired hardware concepts before showing how their respective strengths can be combined. After the necessary groundwork for architectural analysis has been laid in section 3.3, we then discuss how to select, arrange, and improve the necessary hardware resources in an efficient way with a focus on dedicated architectures. Section 3.4 is concerned with organizing computations of combinational nature, section 3.6 extends our analysis to non-recursive sequential Top-Down Digital VLSI Design © 2015 Elsevier Inc. All rights reserved. 63 64 CHAPTER 3 FROM ALGORITHMS TO ARCHITECTURES computations before timewise recursive computations are addressed in section 3.7. Finally, section 3.8 generalizes our findings to other than word-level computations on real numbers. Inserted in between is section 3.5 that discusses the options available for temporarily storing data and their implications on architectural decisions. How to make sure functionality is implemented correctly will be the subject of chapter 5. 3.2 THE ARCHITECTURAL SOLUTION SPACE 65 3.2 THE ARCHITECTURAL SOLUTION SPACE 3.2.1 THE ANTIPODES Basically, any given computation algorithm can be implemented either as a software program that gets executed an instruction-set computer — such as a microprocessor or a digital signal processor (DSP) — or, alternatively, as a hardwired electronic circuit that carries out the necessary computation steps (Figure 3.1 and Table 3.1). The systems engineer, therefore, has to decide between two fundamentally different options early on in each project. a) Select a processor-type general-purpose architecture and write program code for it, or b) Tailor a dedicated hardware architecture for the specific computational needs. application-specific hardware structure general-purpose hardware structure general-purpose hardware with application-specific software content dedicated to specialized unit subtask B dedicated to specialized unit subtask A dedicated to specialized unit subtask D dedicated to specialized unit subtask C output data input data program storage controller data memory program-controlled processor purpose datapath general- output data input data program- controlled processor input data output data= GP SP general-purpose special-purpose (b) (a) FIGURE 3.1 Program-controlled general-purpose processor (a) and dedicated (special-purpose) hardware structure (b) as architectural antipodes. Table 3.1 gives further details. 66 CHAPTER 3 FROM ALGORITHMS TO ARCHITECTURES Table 3.1 The architectural antipodes compared. Hardware architecture General purpose Special purpose Algorithm any, not known a priori ﬁxed, must be known Architecture instruction set processor, dedicated design, von Neumann or Harvard style no single established pattern Execution model fetch-load-execute-store cycle process data item and pass on “instruction-oriented” “dataﬂow-oriented” Datapath universal operations, speciﬁc operations only, ALU(s) plus memory customized design Controller with program microcode typically hardwired Performance instructions per second, data throughput, indicator run time of various can be anticipated benchmark programs analytically Paradigm from craftsmaninhis machineshop division of labor in a factory manufacturing working according to set up for smooth production different plans every day of a few closely related goods Possible hardware standard mC|DSP components ASIC of dedicated architecture implementations or ASIC with on-chip mC|DSP or FPL (FPGA|CPLD) Engineering effort mostly software design mostly hardware design Strengths highly ﬂexible, room for max. performance, immediately available, highly energy-efficient, routine design ﬂow, lean circuitry low up-front costs This is a major decision that has to be made before embarking in the design of a complex circuit. A great advantage of commercial microprocessors is that developers can focus on higher-level issues such as functionality and system-level architecture right away. There is no need for them to address all those exacting chores that burden semi- and — even more so — full-custom design.1 In addition, there is no need for custom fabrication masks. Observation 3.1. Opting for commercial instruction-set processors and/or FPL sidesteps many tech- nical issues that absorb much attention when a custom IC is to be designed instead. Conversely, it is precisely the focus on the payload computations, the absence of programming and configuration overhead together with the full control over every aspect of architecture, circuit, and layout design that make it possible to optimize performance and energy efficiency. Circuit examples where dedicated architectures outperform instruction set computers follow. 1 Such as power distribution, clock preparation and distribution, input/output design, physical design and verification, signal integrity, electrical overstress protection, wafer testing, and package selection, all to be discussed in forthcoming chapters. Setting up a working CAE/CAD design flow typically also is a major stumbling block, to say nothing of estimating sales volume, hitting a narrow window of opportunity, finding the right partners, and providing the necessary resources, in-house expertise, and investments. Also note that field-programmable logic (FPL) dispenses developers from dealing with many of these issues too. 3.2 THE ARCHITECTURAL SOLUTION SPACE 67 Example Table 3.2 Comparison of architectural alternatives for a Viterbi decoder (code rate 1 2 ,constraint length 7, soft decision decoding, Euclidean distance metric). DSPs are at their best for sustained multiply-accumulate operations and offer word widths of 32 bit or so. However, as the Viterbi algorithm can be arranged to make no use of multiplication and to do with word widths of 6 bit or less, DSPs cannot take advantage of these resources. A pipeline of tailored-made stages optimized for branch metric computation, path metric update, and survivor path traceback operations, in contrast, makes it possible to exploit the parallelism inherent in the Viterbi algorithm. Diverse throughput requirements can be accommodated by trading the number of computational units in each stage for throughput. Sophisticated DSPs, such as the C6455, include an extra coprocessor to accelerate path metric update and survivor traceback. Architecture General purpose Special purpose Key component DSP ASIC TI TMS320C6455 sem03w6 sem05w1 without with ETH ETH Viterbi coprocessor VCP2 Number of chips 1 1 1 1 CMOS process 90 nm 90 nm 250 nm 250 nm Program code 187 Kibyte 242 Kibyte none none Circuit size n.a. n.a. 73 kGE 46 kGE Max. throughput 45 kbit/s 9 Mbit/s 310 Mbit/s 54 Mbit/s @clock 1GHz 1 GHz 310 MHz 54 MHz Power dissipation 2.1 W 2.1 W 1.9 W 50 mW Year 2005 2005 2004 2006 \u0002 Example Table 3.3 Comparison of architectural alternatives for a secret-key block encryption/decryption algorithm (IDEA cipher as shown in fig.3.20, block size 64 bit, key length 128 bit). The clear edge of the VINCI ASIC is due to a high degree of parallelism in its datapath and, more particularly, to the presence of four pipelined computational units for multiplication modulo (216 + 1) designed in full-custom layout that operate concurrently and continuously. The more recent IDEA kernel combines a deep submicron fabrication process with four highly optimized arithmetic units. Full-custom layout was no longer needed to achieve superior performance. Architecture General purpose Special purpose Key component DSP RISC Workst. ASSP ASSP Motorola 56001 Sun Ultra 10 VINCI [20] IDEA Kernel Number of chips 1 + memory motherboard 1 1 CMOS process n.a. n.a. 1.2 mm 250 nm Max. throughput 1.25 Mbit/s 13.1 Mbit/s 177 Mbit/s 700 Mbit/s @clock 40 MHz 333 MHz 25 MHz 100 MHz Year 1995 1998 1992 1998 \u0002 Example Table 3.4 Comparison of architectural alternatives for lossless data compression with the Lempel-Ziv-77 algorithm that heavily relies on string matching operations [21]. The dedicated hardware architecture is implemented on a reconfigurable coprocessor board built around four field-programmable gate-array components. 512 special-purpose processing elements are made to carry out string comparison subfunctions in parallel. The content-addressed symbol memory is essentially organized as a shift register thereby giving simultaneous access to all entries. Of course, the two software implementations obtained from compiling C source code cannot nearly provide a similar degree of concurrency. Architecture General purpose Special purpose Key component RISC Workst. CISC Workst. FPGA Xilinx Sun Ultra II Intel Xeon XC4036XLA Number of chips motherboard motherboard 4+ conﬁg. CMOS process n.a. n.a. n.a. Max. throughput 3.8 Mbit/s 5.2 Mbit/s 128 Mbit/s @clock 300 MHz 450 MHz 16 MHz Year 1997 1999 1999 \u0002 Example Table 3.5 Comparison of architectural alternatives for a secret-key block encryption/decryption algorithm (AES cipher, block size 128 bit, key length 128 bit). The Rijndael algorithm makes extensive use of a so-called S-Box function and its inverse; the three hardware implementations include multiple lookup tables (LUT) for implementing that function. Also, (de)ciphering and sub- key preparation are carried out concurrently by separate hardware units. On that background, the throughput of the assembly language program running on a Pentium III is indeed impressive. This largely is because the Rijndael algorithm has been designed with the Pentium architecture in mind (MMX instructions, LUTs that fit into cache memory, etc.). Power dissipation remains daunting, though. Architecture General purpose Special purpose Key component RISC Proc. CISC Proc. FPGA ASIC ASIC Embedded Pentium III Virtex-II CryptoFun core only Sparc Amphion ETH UCLA [22] Number of chips motherbo. motherbo. 1 + conﬁg. 1 1 Programming C Assembler none none none Circuit size n.a. n.a. n.a. 76 kGE 173 kGE CMOS process n.a. n.a. 150 nm 180 nm 180 nm Max. throughp. 133 kbit/s 648 Mbit/s 1.32 Gbit/s 2.00 Gbit/s 1.6 Gbit/s @clock 120 MHz 1.13 GHz n.a. 172 MHz 125 MHz Power dissipat. 120 mW 41.4 W 490 mW n.a. 56 mWa @ supply 1.5 V 1.8 V 1.8 V Year n.a. 2000 ≈ 2002 2007 2002 a Most likely speciﬁed for core logic alone, that is without I/O circuitry. \u0002 3.2 THE ARCHITECTURAL SOLUTION SPACE 69 Upon closer inspection, one finds that dedicated architectures fare much better in terms of performance and/or dissipated energy than even the best commercially available general-purpose processors in some situations, whereas they prove a dreadful waste of both hardware and engineering resources in others. Algorithms that are very irregular, highly data-dependent, and memory-hungry are unsuitable for dedicated architectures. Situations of this kind are found in electronic data processing such as databank applications, accounting, and in reactive systems2 like industrial control,3 user interfaces, and others. In search of optimal architectures for such applications, one will invariably arrive at hardware structures patterned after instruction set processors. Writing code for a standard microcomputer — either bought as a physical part or incorporated into an ASIC as a megacell or as a virtual component — is more efficient and more economic in this case. Situations where data streams are to be processed in fairly regular ways offer far more room for coming up with dedicated architectures. Impressive gains in performance and energy efficiency over solutions based on general-purpose parts can so be obtained, see tables 3.2, 3.3, 3.4 and 3.5 among other examples. Generally speaking, situations that favor dedicated architectures are often found in real-time applica- tions from digital signal processing and telecommunications such as • Source coding (i.e. data, audio and video (de)compression), • (De)ciphering (primarily for secret key ciphers), • Channel coding (i.e. error correction), • Digital (de)modulation (for modems, wireless communication, and disk drives), • Adaptive channel equalization (after transmission over copper lines and optical fibers), • Filtering (for noise cancellation, preprocessing, spectral shaping, etc.), • Multipath combiners in broadband wireless access networks (RAKE, MIMO), • Digital beamforming with phased-array antennas (Radar), • Computer graphics and video rendering, • Multimedia (e.g. MPEG, HDTV), • Packet switching (e.g. ATM, IP), • Transcoding (e.g. between various multimedia formats), • Medical signal processing, • Pattern recognition, and more. 2 A systemis saidtobe reactive if it interacts continuously with an environment, at a speed imposed by that environment. The system deals with events and the mathematical formalisms for describing them aim at capturing the complex ordering and causality relations between events that may occur at the inputs and the corresponding reactions — events themselves — at the outputs. Examples: elevators, protocol handlers, antilock brakes, process controllers, graphical user interfaces, operating systems. As opposed to this, a transformatorial system accepts new input values — often at regular intervals —, uses them to compute output values, and then rests until the subsequent data items arrive. The system is essentially con- cerned with arithmetic/logic processing of data values. Formalisms for describing transformatorial systems capture the numerical dependencies between the various data items involved. Examples: filtering, data compression, ciphering, pat- tern recognition and other applications colloquially referred to as number crunching but also compilers and payroll programs. 3 Control in the sense of the German “programmierte Steuerungen” not “Regelungstechnik”. 70 CHAPTER 3 FROM ALGORITHMS TO ARCHITECTURES Observation 3.2. Processing algorithms and hardware architectures are intimately related. While dedicated architectures outperform program-controlled processors by orders of magnitude in many applications of predominantly transformatorial nature, they cannot rival the agility and economy of processor-type designs in others of more reactive nature. More precise criteria for finding out whether a dedicated architecture can be an option or not from a purely technical point of view follow in section 3.2.2 while fig.3.2 puts various applications from signal and data processing into perspective. data rate [item/s] 1Gop/ s 1Mop/ s 1kop/ s Intel 80386 (1985) Sun SPARC (1987) 1Eop/ scomputation rate[operation/s] computational effort per data item aka intensity 1Pop/ s NEC Earth Simulator (supercomputer 2003) 1Top/ s IBM Roadrunner (supercomputer 2008) [op/ item] technology push WLAN wireless ATM MPEG-2 encoder HDTV encoder Turbo for UMTS decoder1k 10k 100k 1M 10M 100M 1G 10 100 1 Cray Titan (supercomputer 2012) Atmel 8051 (2012) Intel i7-3900 (2013) low-cost microprocessors GSM baseband simple modem multiview synthesis dedicated high-performance architectures multi-antenna (MIMO) wireless receiver (4G) satellite communication multiprocessor systems Dolby AC-3 5.1 decoder TI C66x (DSP core, 2012) Nvidia GeForce GTX780 (graphics card, 2013)UMTS: 3.84 Mchip/sCD: 44.1 ksample/sPAL: 11 Mpixel/sHDTV: 62 Mpixel/sQFHD: 249 Mpixel/sUHD: 995 Mpixel/sPCM: 8 ksample/s Turbo for LTE-A decoder signal and graphics processors high-performance microprocessors (multicore) 10 100 10k1k 100k 10M1M 100M 10G1G 100G1 1T towards high data rates low-cost hardware towards applications towards sophisticated high audio & video quality, (strong compression, secure communication, robust transmission, etc.) energy-saving algorithms towards FIGURE 3.2 Computational needs and capabilities (approximate, exact meaning of operation and data item left unspeciﬁed, 16 bit-by-16 bit multiply-accumulate (MAC) operations on 16 bit samples are often considered typical in a context of digital signal processing). 3.2 THE ARCHITECTURAL SOLUTION SPACE 71 3.2.2 WHAT MAKES AN ALGORITHM SUITABLE FOR A DEDICATED VLSI ARCHITECTURE? Costs are not the same in hardware as in software. As an example, permutations of bits within a data word are time-consuming operations in software as they must be carried out sequentially. In hardware, they reduce to simple wires that cross while running from one subcircuit to the next. Lookup tables (LUT) of almost arbitrary size, on the other hand, have become an abundant and cheap resource in any microcomputer while large on-chip RAMs and ROMs tend to eat substantial proportions of the timing and area budgets of ASIC designs. In an attempt to provide some guidance, we have collected ten criteria that an information processing algorithm should ideally meet in order to justify the design of a special-purpose VLSI architecture and to take full advantage of the technology. Of course, very few real-world algorithms satisfy all of the requirements listed. It is nevertheless safe to say that designing a dedicated architecture capable of outperforming a general-purpose processor on the grounds of performance and costs will prove difficult when too many of these criteria are violated. The list below begins with the most desirable characteristics and then follows their relative significance. 1. Loose coupling between major processing tasks. The overall data processing lends itself to being decomposed into tasks that interact in a simple and unmutable way. Whether those tasks are to be carried out consecutively or concurrently is of secondary importance at this point, what counts is to come up with a well-defined functional specification for each task and with manageable interactions between them. Architecture design, functional verification, optimization, and reuse otherwise become real nightmares. 2. Simple control flow. The computation’s control flow is simple. This key property can be tracked down to two more basic considerations: a) The course of operation does not depend too much on the data being processed; for each loop the number of iterations is a priori known and constant.4 b) The application does not ask for computations to be carried out with overly many varieties, modes of operations, data formats, distinct parameter settings, etc. The benefit of a simple control flow is twofold. For one thing, it is possible to anticipate the datapath resources required to meet a given performance goal and to design the chip’s architecture accordingly. There is no need for statistical methods in estimating the computational burden nor in sizing data memories and the like. For another thing, datapath control can be handled by counters and by simple finite state machines (FSM) that are small, fast, energy-efficient and — most important — easy to verify. An overly complicated course of operations, on the other hand, that involves much data- dependent branching, multitasking, and the like, favors a processor-type architecture that operates under control of stored microcode. Most control operations will then translate into a sequence of machine instructions that take several clock cycles to execute. 4 Put in different terms, the target algorithm is virtually free of branchings and loops such as if...then[...else], while...do,and repeat...until that include data items in their condition clauses. 72 CHAPTER 3 FROM ALGORITHMS TO ARCHITECTURES 3. Regular data flow. The flow of data is regular and their processing is based on a recurrence of a fairly small number of identical operations, there are no computationally expensive operations that are called only occasionally. Regularity opens a door for sharing hardware resources in an efficient way by applying techniques such as iterative decomposition and time sharing, see subsections 3.4.2 and 3.4.5 respectively. Conversely, multiple data streams that are to be processed in a uniform way lend themselves for concurrent processing by parallel functional units. A regular data flow further helps to reduce communications overhead both in terms of area and interconnect delay as the various functional units can be made to exchange data over fixed local links. Last but not least, regularity facilitates reuse and reduces design and verification effort. As opposed to this, operations that are used infrequently will either have to be decomposed into a series of substeps to be executed one after the other on a general-purpose datapath, which is slow, or will necessitate dedicated functional units bound to sit idle for most of the time, which inflates chip size. Irregular data flow asks for long and flexible communication busses which are at the expense of layout density, operating speed and energy efficiency. 4. Reasonable storage requirements. Overall storage requirements are modest and have a fixed upper bound which precludes the use of dynamic data structures. Memories that occupy an inordinate amount of chip area cannot be incorporated into ASICs in an economic way and must, therefore, be implemented off-chip from standard parts, see subsection 3.5. Massive storage requirements in conjunction with moderate computational burdens tend to place dedicated architectures at a disadvantage. 5. Compatible with finite precision arithmetics. The algorithm is insensitive to effects from finite precision arithmetics. That is, there is no need for floating point arithmetics; fairly small word widths of, say, 16 bit or less, suffice for the individual computation steps. Standard microprocessors and DSPs come with datapaths of fixed and often generous width (24, 32, 64 bit, or even floating point) at a given price. No extra costs arise unless the programmer has to resort to multiple precision arithmetics. As opposed to this, ASICs and FPL offer an opportunity to tune the word widths of datapaths and on-chip memories to the local needs of computation. This is important because circuit size, logic delay, interconnect length, parasitic capacitances, and energy dissipation of addition, multiplication, and other operations all tend to grow with word width, combining into a burden that multiplies at an overproportional rate.5 5 Processor datapaths tend to be fast and area efficient because they are typically hand-optimized at the transistor level (e.g. dynamic logic) and implemented in tiled layout rather than built from standard cells. These are only rarely options for ASIC designers. 3.2 THE ARCHITECTURAL SOLUTION SPACE 73 0 0.2 0.4 0.6 0.8 1.0 1.4 1.6 1.2 1.8 2.0 mm2 computation delay area occupation Nonrestoring Radix-2 SRT Radix-4 SRT Commercial 10 50 10020 30 40 60 70 80 90 ns0 16 bit 24 40 32 52 64 bit Pareto-optimal solutions suboptimal solutions FIGURE 3.3 Comparison of hardware divider architectures in terms of area and delay estimated from pre-layout netlists for a 180 nm CMOS process under worst-case PTV conditions [23]. Note the impact of quotient width on both circuit size and performance for all four architectures. The concept implemented in the commercial component is not disclosed by the vendor. SRT stands for Sweeney, Robertson and Tocher. 6. Non-recursive linear time-invariant computation. The processing algorithm describes a non- recursive linear time-invariant system over some algebraic field.6 Each of these properties opens a door to reorganizing the data processing in one way or another, see sections 3.4 through 3.9 for details and table 3.11 for an overview. High throughputs, in particular, are much easier to obtain from non-recursive computations as will become clear in section 3.7. 7. No transcendental functions. The algorithm does not make use of roots, logarithmic, exponential, or trigonometric functions, arbitrary coordinate conversions, translations between incompatible number systems, and other transcendental functions as these must either be stored in large lookup 6 Recursiveness is to be defined in section 3.7. Linear is meant to imply the principle of superposition: f (x(t) + y(t)) ≡ f (x(t)) + f (y(t)) and f (cx(t)) ≡ cf (x(t)). Time-invariant means that the sole effect of shifting the input in time is a corresponding time shift of the output: if z(t) = f (x(t)) is the response to x(t) then z(t − T) is the response to x(t − T). Fields and other algebraic structures are compared in section 3.11. 74 CHAPTER 3 FROM ALGORITHMS TO ARCHITECTURES tables (LUT) or get calculated on-line in lengthy and often irregular computation sequences. Such functions can be implemented more economically provided modest accuracy requirements allow for approximation by way of lookups from tables of reasonable size possibly followed by interpolation steps.7 8. Extensive usage of data operations unavailable from standard instruction sets. Of course, there exist many processing algorithms that cannot do without costly arithmetic/logic operations. It is often possible to outperform traditional program-controlled processors in cases where such operations need to be assembled from multiple instructions. Dedicated hardware can then be designed to do the same computation in a more efficient way. Examples include finite field arithmetics, add-compare-select operations, many ciphering operations, and, again, CORDIC.7 It also helps when part of the arguments are constants because this makes it possible to apply some form of preprocessing. Multiplication by a variable is more onerous than by a constant, for instance.8 9. No divisions and multiplications on very wide data words, no matrix inversions. The algorithm does not make extensive use of multiplications and even less so of divisions as their VLSI implementation is much more expensive than that of addition/subtraction when the data words involved grow wide. Also, depending on the arguments, multiplication and division may vastly expand the numerical range of the results. This gives rise to scaling issues, particularly in conjunction with a fixed-point number system. Matrix inversion is a particularly nasty case in point as it involves division operations and often brings about numerical instability. 10. Throughput rather than latency is what matters. Tight latency requirements rule out pipelin- ing, one of the most effective ways to boost the throughput of a datapath. Stringent latency requirements are not in favor of microprocessors either, however, as program-controlled operation cannot guarantee brief and fixed response times, even less so when a complex operating system is involved. 3.2.3 THERE IS PLENTY OF LAND BETWEEN THE ANTIPODES Most markets ask for performance, agility, low power, and a modest design effort at a time. This requires that the throughput and the energy efficiency of a dedicated VLSI architecture for demanding but highly repetitive computations be combined with the convenience and flexibility of an instruction set processor for more control-oriented tasks. The question is “How to blend the best of both worlds into a suitable architecture design?” 7 The most popular such technique is known as CORDIC and used to calculate coordinate rotations, trigonometric and hyperbolic functions, see section 3.3.1. Advice on how to approximate other transcendental functions can be found in [24] (sine functions), [25] [26] (logarithms), and table 3.8 (magnitude function) among many others. 8 Dropping unit factors and/or zero sum terms (both at word and bit levels), substituting integer powers of 2 as arguments in multiplications and divisions, omitting insignificant contributions, special number representation schemes, taking advantage of symmetries, precomputed lookup tables, and distributed arithmetic, see subsection 3.8.3, are just a few popular measures that may help to lower the computational burden in situations where part of the arguments are known ahead of time. 3.2 THE ARCHITECTURAL SOLUTION SPACE 75 Six approaches for doing so are going to be presented in sections 3.2.4 through 3.2.9 with diagrammatic illustrations in figs.3.4 to 3.9. 3.2.4 ASSEMBLIES OF GENERAL-PURPOSE AND DEDICATED PROCESSING UNITS The observation below forms the starting point for the conceptually simplest approach. Observation 3.3. It is often possible to segregate the needs for computational efficiency from those for flexibility. This is because those parts of a system that ask for maximum computation rate are not normally those that are subject to change very often, and vice versa. Examples abound, see table 3.6. The finding immediately suggests a setup where a software-controlled microcomputer cooperates with one or more dedicated hardware units. Separating the quest for computational efficiency from that for agility makes it possible to fully dedicate the various functional units to their respective tasks and to optimize them accordingly. Numerous configurations are possible and the role of the instruction set microcomputer varies accordingly. dedicated to specialized unit subtask B dedicated to specialized unit subtask A dedicated to specialized unit subtask D parametrization bus (optional) handles subtask C program- controlled processor output data input data dedicated to specialized unit subtask B dedicated to specialized unit subtask A dedicated to specialized unit subtask D input data output data program- controlled processor handles subtask C plus dispatching of data data exchange and control busses (a) (b) FIGURE 3.4 General-purpose processor and dedicated satellite units working in a chain (a), a host computer with specialized ﬁxed-function blocks or coprocessors (b). In fig.3.4a, three dedicated and one program-controlled processing units are arranged in a chain. Each unit does its data processing job and passes the result to the downstream unit. While offering ample room for optimizing performance, this structure cannot accommodate much variation if everything is hardwired and tailor-made. Making the specialized hardware units support a limited degree of 76 CHAPTER 3 FROM ALGORITHMS TO ARCHITECTURES parametrization (e.g. wrt data word width, filter order, code rate, data exchange protocol, and the like) renders the overall architecture more versatile while, at the same time, keeping the overhead in terms of circuit complexity and energy dissipation fairly low. The term weakly programmable satellites has been coined to reflect the idea. An optional parametrization bus suggests this extension of the original concept in fig.3.4a. Example Table 3.6 Some digital systems and the computing requirements of major subfunctions thereof. Subfunctions primarily characterized by irregular control ﬂow and/or need repetitive control ﬂow and need Application for ﬂexibility for computing efficiency Blu-ray player user interface, track seeking, 16-to-8 bit demodulation, tray and spindle control, error correction, processing of non-video data MPEG-2 decompression (DCT), (directory, title, author, deciphering (AACS AES-128), subtitles, region codes) video signal processing Smartphone user interface, SMS/MMS, intermediate frequency ﬁltering, directory management, (de)modul., channel (de)coding, battery monitoring, error correction (de)coding, communication protocol, (de)ciphering, speech and channel allocation, video (de)compression, roaming, accounting display graphics Pattern recognition pattern classiﬁcation, image stabilization, (e.g. as part of a objects tracking, redundancy reduction, defensive missile) target acquisition, image segmentation, triggering of actions feature extraction \u0002 3.2.5 HOST COMPUTER WITH HELPER ENGINES Fig.3.4b is based on segregation too but differs in how the various components interact. All satellites now operate under command of a software-programmable host. A bidirectional bus gives the necessary liberty for transferring data and control words back and forth. Each unit is optimized for a few specific subtasks such as filtering, video and/or audio (de)coding, (de)ciphering, modem operations, display graphics, and the like. These helper engines, as they are fittingly called, may either be hardwired fixed- function blocks or be themselves instruction-set programmable, in which case they are to be viewed as true coprocessors. A helper engine sits idle until it receives a set of input data along with a starting command. As an alternative, the data may be kept in the host’s own memory all the time but get accessed by the coprocessor via direct memory access (DMA). Once local computation has come to an end, the helper sets a status flag and/or sends an interrupt signal to the host computer. The host then accepts the processed data and takes care of further action. 3.2 THE ARCHITECTURAL SOLUTION SPACE 77 As evident from fig. 3.5, industrial examples are more complex but the concept of co-operating instruction-set processors and optimized fixed-function blocks remains. FIGURE 3.5 A system on a chip for smartphones (source Texas Instruments, reprinted with permission). 3.2.6 APPLICATION-SPECIFIC INSTRUCTION SET PROCESSORS Patterning the overall architecture after a program-controlled processor affords much more flexibility. Application-specific features are largely confined to the data processing circuitry itself. That is, one or more datapaths are designed and hardwired such as to support specific data manipulations while operating under control of a common microprogram. The number of ALUs, their instruction sets, the data formats supported, the capacity of local storage, etc. are tailored to the computational problems to be solved. What’s more, the various datapaths can be made to operate simultaneously on different pieces of data thereby providing a limited degree of concurrency. The resulting architecture, sketched in fig.3.6, is that of an application-specific instruction set processor (ASIP). 78 CHAPTER 3 FROM ALGORITHMS TO ARCHITECTURES program storage data memory program-controlled processor controller handles subtasks A, B, C and D with the aid of multiple specialized datapaths datapath type 1 specialized datapath type 2 specialized datapath type 3 specialized output data input data application-specific hardware structure general-purpose hardware structure general-purpose hardware with application-specific software content FIGURE 3.6 Application-speciﬁc instruction set processor (ASIP). Example Table 3.7 An ASIP implementation of the AES (Rijndael) algorithm, compare with table. 3.5 Architecture ASIP Key component Cryptoprocessor core UCLA [27] Number of chips 1 Programming Assembler Circuit size 73.2 kGE CMOS process 180 nm 4Al2Cu Throughput 3.43 Gbit/s @clock 295 MHz Power dissipation 86 mWa @ supply 1.8 V Year 2004 a Estimate for core logic alone, that is without I/O circuitry, not a measurement. \u0002 The hardware organization of an ASIP bears much resemblance to architectural concepts from general- purpose computing. As more and more concurrent datapath units are being added, what results essentially is a very-long instruction word (VLIW) architecture. An open choice is that between a multiple-instruction multiple-data (MIMD) machine, where an individual field in the overall instruction word is set apart for each datapath unit, and a single-instruction multiple-data (SIMD) model, where a bunch of identical datapaths work under control of a single instruction word. Several data items can so be made to undergo the same operation at a time.9 9 In an effort to better serve high-throughput video and graphics applications, many vendors have enhanced their micro- processor families in the late 1990’s by adding special instructions that provide some degree of concurrency. During each such instruction, the processor’s datapath gets split up into several smaller subunits. A datapath of 64 bit can so be made to process four 16 bit data words at a time, for instance, provided the operation is the same for all of them. The technique is best described as sub-word parallelism, but is better known under various trademarks such as multimedia extensions (MMX), streaming SIMD extensions (SSE) (Pentium family), Velocity Engine, AltiVec, and VMX (PowerPC family). 3.2 THE ARCHITECTURAL SOLUTION SPACE 79 Be cautioned that defining a proprietary instruction set makes it impossible to take advantage of existing compilers, debugging aids, assembly language libraries, experienced programmers, and other resources that are routinely available for commercial processors. Industry provides us with such a vast selection of micro- and signal processors that only very particular requirements justify the design of a proprietary CPU. Rather than anywhere else, opportunities can be found in mobile signal processing devices that ask for a combination of specific processing needs, agility, performance, and energy efficiency. [28] describes an interesting framework for ASIP development whereby assembler, linker, simulator, and RTL synthesis code are generated automatically by system-level software tools. Using LISA 2.0 (Language for Instruction Set Architectures), hardware designers essentially begin by defining the most adequate instruction set for a target application. Transiting through a cycle-accurate model, they can cast their ideas into an RTL-type model which means they remain in full control over the processor’s architecture if they wish to do so. With some restrictions, that model then gets translated into VHDL synthesis code. All this happens within the same EDA environment and using the same language, so that the process of evaluating competing architectural options and of working out all the details is greatly expedited. A number of predefined processor templates further helps with the initial modeling steps.10 Example While generally acknowledged to produce more realistic renderings of 3D scenes than industry-standard raster graphics processors, ray tracing algorithms have long been out of reach for real-time applications due to the myriad floating point computations and the immense memory bandwidth they require. Hardwired custom architectures do not qualify either as they cannot be programmed and as ray tracing necessitates many data-dependent recursions and decisions. Ray tracing may finally find more general adoption in multi-ASIP architectures that combine multiple ray processing units (RPU) into one powerful rendering engine. Working under control of its own program thread, each RPU operates as a SIMD processor that follows a subset of all rays in a scene. The independence of light rays allows for a welcome degree of scalability where frame rate can be traded against circuit complexity. The authors of [29] have further paid attention to define an instruction set for their RPUs that is largely compatible with pre-existing industrial graphics processors. \u0002 3.2.7 RECONFIGURABLE COMPUTING Another crossbreed between dedicated and general-purpose architectures has not become viable until the late 1990’s but is now being promoted by FPL manufacturers and researchers [30] [31]. The IEEE 1532 standard has also been created in this context. The idea is to reuse the same hardware for implementing subfunctions that are mutually exclusive in time by reconfiguring FPL devices on the fly. 10 A commercial product that has emanated from this research is “Processor Designer” by Synopsys. 80 CHAPTER 3 FROM ALGORITHMS TO ARCHITECTURES input data output data repository for coprocessor configurations [re]configuration request handles subtask C plus overhead A, B, and D handles subtasks one at a time in-system reconfigurable coprocessor data exchange bus program- controlled processor configuration data application-specific hardware structure general-purpose hardware structure general-purpose hardware with application-specific software content FIGURE 3.7 General-purpose processor with juxtaposed reconﬁgurable coprocessor. As shown in fig.3.7, the general hardware arrangement bears some resemblance to the coprocessor approach of fig.3.4b, yet in-system configurable (ISC) devices are being used instead of hardwired logic. As a consequence, the course of operations is more sophisticated and requires special action from the hardware architects. For each major subtask, the architects must ask themselves whether the computations involved • Qualify for being delegated to in-system configurable logic, • Never occur at the same time — or can wait until the FPL device becomes free —, and • Whether the time for having the FPL reconfigured in between is acceptable or not. Typically this would be the case for repetitive computations that make use of sustained, highly parallel, and deeply pipelined bit-level operations. When designers have identified some suitable subfunction, they devise a hardware architecture that solves the particular computational problem with the resources available in the target FPGA or CPLD, prepare a configuration file, and have that stored in a configuration memory. In some sense, they create a large hardware procedure instead of programming a software routine in the customary way. Whenever the host computer encounters a call to such a hardware procedure, it configures the FPL accordingly by downloading the pertaining configuration file. From now on, all the host has to do is to feed the “new” coprocessor with input data and to wait until the computation completes. The host then fetches the results before proceeding with the next subtask.11 It so becomes possible to support an assortment of data processing algorithms each with its optimum architecture — or almost so — from a single hardware platform. What often penalizes this approach in practice are the dead times incurred whenever a new configuration is being loaded. Another price to pay is the extra memory capacity for storing the configuration bits for all operation modes. Probably 11 As an extension to the general procedure described here, an extra optimization step can get inserted before the coprocessor is being configured [32]. During this stage, the host would adapt a predefined generic configuration to take advantage of particular conditions of the specific situation at hand. Consider pattern recognition, for instance, where the template remains unchanged for a prolonged lapse of time, or secret-key (de)ciphering where the same holds true for the key. As stated in subsection 3.2.2 item 3.2.2, it is often possible to simplify arithmetic and logic hardware a lot provided that part of the operands do have a fixed value. 3.2 THE ARCHITECTURAL SOLUTION SPACE 81 the most valuable benefit, however, is the possibility to upgrade information processing hardware to new standards and/or modes of operation even after the system has been fielded. Examples Transcoding video streams in real time is a good candidate for reconfigurable computing because of the many formats in existence such as DV, AVI, MPEG-2, DivX and H.264. For each conversion scheme, a configuration file is prepared and stored in local memory from where it is being transferred into the reconfigurable coprocessor on demand. And should a video format or variation emerge that was unknown or unpopular at the time when the system was being developed, extra configuration files can be made available in a remote repository from where they can be fetched much like software plug-ins get downloaded via the Internet. The results from a comparison between Lempel-Ziv data compression with a reconfigurable copro- cessor and with software execution on a processor [21] have been summarized in table 3.4. A related application was to circumvent the comparatively slow PCI bus in a PC [33]. \u0002 3.2.8 EXTENDABLE INSTRUCTION SET PROCESSORS This offbeat approach pioneered by Stretch Inc. combines ideas from ASIP design and reconfigurable computing. As indicated in fig. 3.8, the general architecture includes both a program-controlled processor and electrically reconfigurable logic. program storage data memory controller handles subtasks A, B, C, and D with a combination of fixed and configurable datapaths output data input data datapath general purpose datapath logic configurable on-chip FIGURE 3.8 Combining performance and agility with an Extendable instruction set processor (simpliﬁed). The key innovation is a suite of proprietary EDA tools that allows system developers to focus on writing their application program in C or C++ as if for a regular general-purpose processor. Those tools begin by profiling the software code in order to identify sequences of instructions that are executed many times over. For each such sequence, reconfigurable logic is then synthesized into a dedicated and massively parallel computation network that completes within one clock cycle — ideally at least. Finally, each occurrence of the original instruction sequence in the machine code gets by replaced by a simple function call that activates the custom-made datapath logic. 82 CHAPTER 3 FROM ALGORITHMS TO ARCHITECTURES In essence, the base processor gets unburdened from lengthy code sequences by augmenting its instruction set with a few essential additions that fit the application and that get tailor-made almost on the fly. Yet, the existence of reconfigurable logic and the business of coming up with a suitable hardware architecture are hidden from the system developer. The fact that overall programm execution remains strictly sequential should further simplify the design process. A related concept is discussed in [34]. 3.2.9 PLATFORM ICS (DSPP) The exploding costs of engineering and mask sets are pushing the minimum sales quantity required to economically justify custom silicon to ever higher values, creating a need for malleable platforms that can cover a range of applications with a single design. This calls for a new concept that borrows from all other approaches discussed so far in an attempt to reconcile performance, energy efficiency, agility, and fast turnaround. As illustrated in fig.3.9, a typical platform IC12 has a heterogeneous architecture that generously marries hardware resources from • Instruction-set-programmable processors (CPUs, GPUs, ASIPs), • Hardwired circuits (fixed-function blocks useful for one domain of application), • Electrically configurable fabrics (FPL), and • Various types of on-chip memories (SRAM, DRAM, flash). dedicated to specialized unit subtask A input data output data program- controlled processor handles subtask C plus dispatching of data controller datapath for subtask B specialized data memory handles subtask B program storage repository for configuration handles subtask D in-system configurable logic configuration data m u l t i p l e t h r e a d s o f e x e c u t i o n configurable bus system dedicated to specialized unit subtask E dedicated to specialized unit subtask F not used in this application not used in this application FIGURE 3.9 Domain-speciﬁc programmable platform (simpliﬁed). 12 There is no established name for the concept yet. While Domain-Specific Programmable Platform (DSPP), Domain- Specific Heterogeneous Standard Platform (DSHSP), and Targeted Design Platform capture the spirit, the term Application- Specific Programmable Platform (ASPP) is slightly off the point. We usually prefer the name platform IC for conciseness. 3.2 THE ARCHITECTURAL SOLUTION SPACE 83 Most of the programming occurs in a domain-specific high-level language.13 For each subtask, de- veloper tools determine the most adequate execution unit (or units) such as to meet the performance targets with the minimum dissipated energy. The configurable logic is used to extend the datapaths and/or the instruction sets where beneficial in terms of throughput, energy efficiency, updates, etc., but time-consuming on-the-fly reconfigurations are minimized. The degree of concurrency is limited by the circuit resources available. To reduce static power, all subcircuits are turned off when inactive. While the platform circuitry is enormously complex and includes subcircuits that may never be used in a given application or product, the on-going technological progress tends to make such concerns less and less relevant. What’s more, the concept benefits from numerous trends to be explained in forthcoming chapters. Fig.3.10 shows an industrial example that can be viewed as a forerunner. Paraphrasing EE Times, the author believes CPU and GPU cores are the new gates, and platform ICs are the new gate arrays. 3.2.10 DIGEST Program execution on a general-purpose processor and hardwired circuitry optimized for one specific flow of computation are two architectural antipodes. Luckily, many useful compromises exist in between as reflected in figs.3.11 and 3.13. A general advice is this: Observation 3.4. Rely on dedicated hardware only for those subfunctions that are called many times and are unlikely to change, keep the rest programmable either via software, or via reconfiguration, or both. 13 Such languages are currently being developed by the supercomputing community to ensure code portability and optimum performance in spite of unlike and more and more heterogeneous computers without having to rewrite the code. Example: Liszt for mesh-based solvers of partial differential equations. 84 CHAPTER 3 FROM ALGORITHMS TO ARCHITECTURES FIGURE 3.10 Zynq All Programmable SoC Architecture (source Xilinx, reprinted with permission). Fig. 3.13 gives rise to an interesting observation. While there are many ways to trade agility for computational efficiency and vice versa, the two seem to be mutually exclusive as we know of no architecture that would meet both goals at a time. 3.2 THE ARCHITECTURAL SOLUTION SPACE 85 There is plenty of land between the antipodes general-purpose architectures special-purpose architectures everything program-controlled everything hardwired design productivity towards agility and (above all wrt energy) towards computational efficiency GP SP FIGURE 3.11 The architectural solution space viewed as a globe. FIGURE 3.12 Coexistence of general-purpose CPUs, ASIPs, and hardwired helper engines in the Tegra 2 SoC (source Nvidia, reprinted with permission). 86 CHAPTER 3 FROM ALGORITHMS TO ARCHITECTURES computational efficiency high hand layout hardwired architecture low accomodate unstable specs, immature standards, and/or frequent product enhancements limited design effort, short turnaround times poor agility productivity good ideal computing platform towards the hardware towards universal per-case basis towards hardware optimized on a high throughput small circuit low power transformatorial fixed, repetitive, principle of segregation: select optimum architecture for each task plan for both hardwired and programmable subsystems, computation nature of reactive, irregular, subject to change ASIP instruction set processor general-purpose reconfigurable computing DSPP FIGURE 3.13 The basic options of architecture design (simpliﬁed). 3.3 DEDICATED VLSI ARCHITECTURES AND HOW TO DESIGN THEM 87 3.3 DEDICATED VLSI ARCHITECTURES AND HOW TO DESIGN THEM Let us now turn our attention to the main topic of this chapter: “How to decide on the necessary hardware resources for solving a given computational problem and how to best organize them?” Their conceptual differences notwithstanding, many techniques for obtaining high performance at low cost are the same for general- and special-purpose architectures. As a consequence, much of the material presented in this chapter applies to both of them. Yet, the emphasis is on dedicated architectures as the a priori knowledge of a computational problem offers room for a number of ideas that do not apply to instruction set processors.14 Observation 3.5. Most data and signal processing algorithms would lead to grossly inefficient or even infeasible solutions if they were implemented in hardware as they are. Adapting processing algorithms to the technical and economical conditions of large scale integration is one of the intellectual challenges in VLSI design. Basically, there is room for remodeling in two distinct domains, namely in the algorithmic domain and in the architectural domain. 3.3.1 THERE IS ROOM FOR REMODELING IN THE ALGORITHMIC DOMAIN ... In the algorithmic domain, the focus is on minimizing the number of computational operations weighted by the estimated costs of such operations. A given processing algorithm thus gets replaced by a different one better suited to hardware realization in VLSI. Data structures and number representation schemes are also subject to optimizations such as subsampling and/or changing from floating point to fixed point arithmetics. All this implies that alternative solutions are likely to slightly differ in their functionality as expressed by their input-to-output relations. Six examples When designing a digital filter, one is often prepared to tolerate a somewhat lower stopband suppression or a larger passband ripple in exchange for a reduced computational burden obtained, for instance, from substituting a lower order filter and/or from filling in zeros for the smaller coefficients. Conversely, a filter structure that necessitates a higher number of computations may sometimes prove acceptable in exchange for less stringent precision requirements imposed on the individual arithmetic operations and, hence, for narrower data words. In a decoder for digital error-correction, one may be willing to sacrifice 0.1 dB or so of coding gain for the benefit of doing computations in a more economic way. Typical simplifications to the ideal 14 There exists a comprehensive literature on general-purpose architectures including [35] [36]. The historical evolution of the microprocessor is summarized in [37] [38] along with economic facts and trends. [39] [40] [41] emphasize the impact of deep-submicron technology on high-performance microprocessor architectures. 88 CHAPTER 3 FROM ALGORITHMS TO ARCHITECTURES Viterbi algorithm include using an approximation formula for branch metric computation, truncating the dynamic range of path metrics, rescaling them when necessary, and restricting traceback operations to some finite depth. The autocorrelation function (ACF) has many applications in signal processing, yet it is not always needed in the form mathematically defined. ACFxx(k) = rxx(k) = ∞∑ n=−∞ x(n) · x(n + k) (3.1) Many applications offer an opportunity to relax the effort for multiplications because one is interested in just a small fragment of the entire ACF, because one can take advantage of symmetry, or because modest precision requirements allow for a rather coarse quantization of data values. It is sometimes even possible to substitute the average magnitude difference function (AMDF) that does away with costly multiplication altogether. AMDFxx(k) = r′ xx(k) = N−1∑ n=0 |x(n) − x(n + k)| (3.2) Code-excited linear predictive (CELP) coding is a powerful technique for compressing speech signals, yet it has long been left aside in favor of regular pulse excitation because of its prohibitive computational burden. CELP requires that hundreds of candidate excitation sequences be passed through a cascade of two or three filters and be evaluated in order to pick the one that fits best. In addition, the process must be repeated every few milliseconds. Yet, experiments have revealed that the usage of sparse (up to 95% of samples replaced with zeros), of ternary (+1,0,−1), or of overlapping excitation sequences has little negative impact on auditory perception while greatly simplifying computations and reducing memory requirements [42]. When having to compute trigonometric functions, lookup tables (LUT) are likely to prove impractical because of size overruns. Executing a lengthy algorithm, on the other hand, may be just too slow, so a tradeoff between circuit size, speed, and precision must be found. The CORDIC (coordinate rotation digital computer) family of algorithms is one such compromise that has been put to service in scientific pocket calculators in the 1960s and that continues to find applications in DSP [43] [44] [45]. Today, CORDIC units are available as virtual components. Note that CORDIC can be made to compute hyperbolic and other transcendental functions too. Computing the magnitude function m = √ a2 + b2 is a rather costly proposition in terms of circuit hardware. Luckily, there exist at least two fairly precise approximations based on add, shift, and compare operations exclusively, see table 3.8 and problem 1. Better still, the performance of many optimization algorithms used in the context of demodulation, error correction, and related applications does not suffer much when the computationally expensive ℓ2-norm gets replaced by the much simpler ℓ1-or ℓ∞-norm. See [46] for an example. \u0002 The common theme is that the most obvious formulation of a processing algorithm is not normally the best starting point for VLSI design. Departures from some mathematically ideal algorithm are almost always necessary to arrive at a solution that offers the throughput and energy efficiency requested at 3.3 DEDICATED VLSI ARCHITECTURES AND HOW TO DESIGN THEM 89 economically feasible costs. Most algorithmic modifications alter the input-to-output mapping and so imply an implementation loss, that is a minor cut-back in signal-to-noise ratio, coding gain, bit-error- rate, mean time between errors, stopband suppression, passband ripple, phase response, false-positive and false-negative rates, data compression factor, fidelity of reproduction, total harmonic distortion, image and color definition, intelligibility of speech, or whatever figures of merit are most important for the application. Table 3.8 Approximations for computing magnitudes. Name aka Formula lesser -norm l =min( |a|, |b|) sum 1-norm s = |a| + |b| magnitude (reference) 2-norm m = √a2 + b2 greater -norm g =max( |a|, |b|) Approximation 1 m ≈ m1 = 3 8 s + 5 8 g Approximation 2 [47] m ≈ m2 =max( g, 7 8 g + 1 2 l) Experience tells us that enormous improvements in terms of throughput, energy efficiency, circuit size, design effort, and agility can be obtained by adapting an algorithm to the peculiarities and cost factors of hardware. Optimizations in the algorithmic domain are thus concerned with “How to tailor an algorithm such as to cut the computational burden, to trim down memory require- ments, and/or to speed up calculations without incurring unacceptable implementation losses?” What the trade-offs are and to what extent departures from the initial functionality are acceptable depends very much on the application. It is, therefore, crucial to have a good command of the theory and practice of the computational problems to be solved. Observation 3.6. Digital signal processing programs often come with floating point arithmetics. Reimplementing them in fixed point arithmetics, with limited computing resources, and with minimum memory results in an implementation loss. The effort for finding a good compromise between numerical accuracy and hardware efficiency is often underestimated. The necessity to validate trimmed-down implementations for all numerical conditions that may occur further adds to the effort. It is not uncommon to spend as much time on issues of numerical precision as on all subsequent VLSI design phases together. 3.3.2 ... AND THERE IS ROOM IN THE ARCHITECTURAL DOMAIN In the architectural domain, the focus is on meeting given performance targets for a specific data processing algorithm with a minimum of hardware resources. The key concern is “How to organize datapaths, memories, controllers, and other hardware resources for implementing some given computation flow such as to optimize throughput, energy efficiency, circuit size, design effort, agility, overall costs, and similar figures of merit while leaving the original input-to-output relationship unchanged except, possibly, for latency?” 90 CHAPTER 3 FROM ALGORITHMS TO ARCHITECTURES As computations are just reorganized, not altered, there is no implementation loss at this point. Given some data or signal processing algorithm, there exists a profusion of alternative architectures although the number of fundamental options available for reformulating it is rather limited. This is because each such option can be applied at various levels of detail and can be combined with others in many different ways. Our approach is based on reformulating algorithms with the aid of equivalence transforms. The remainder of this chapter gives a systematic view on all such transforms and shows how they can be applied to optimize VLSI architectures for distinct size, throughput, and energy targets. 3.3.3 SYSTEMS ENGINEERS AND VLSI DESIGNERS MUST COLLABORATE Systems theorists tend to think in mathematical categories, so an algorithm from data or signal processing is not much more than a sequence of equations to them. To meet pressing deadlines — or simply for reasons of convenience — they tend to model such algorithms in floating point arithmetics, even when a fairly limited numeric range would amply suffice for the application. This is typically unacceptable in VLSI architecture design, and establishing a lean bit-true software model is a first step towards a cost-effective circuit. Generally speaking, it is always necessary to balance many contradicting requirements to arrive at a working and marketable embodiment of the mathematical or otherwise abstracted initial model of a system. A compromise will have to be found between the theoretically desirable and the economically feasible. So, there is more to VLSI design than accepting a given algorithm and turning that into gates with the aid of some HDL synthesis tool. Algorithm design is typically carried out by systems engineers whereas VLSI architecture is more the domain of hardware designers. The strong mutual interaction between algorithms and architectures mandates a close and early collaboration between the two groups, see fig.3.14. 3.3 DEDICATED VLSI ARCHITECTURES AND HOW TO DESIGN THEM 91 design architecture technology- specific implementation algorithm design IC fabrication dataproduct idea evaluation of functional needs and specification design architecture technology- specific implementation algorithm design IC fabrication data evaluation of functional needs and specification product idea competence of systems engineers competence of systems engineers competence of VLSI designers competence of VLSI designers (a) (b) FIGURE 3.14 Models of collaboration between systems engineers and hardware designers. Sequential thinking doomed to failure (a) versus a networked team more likely to come up with satisfactory results (b). Observation 3.7. Finding a good trade-off between the key characteristics of the final circuit and implementation losses asks for an on-going collaboration between systems engineers and VLSI experts during the phases of specification, algorithm development, and architecture design. The fact that algorithm design is not covered in this text does not imply that it is of less importance to VLSI than architecture design. As illustrated by the example below, the opposite is true. A comprehensive textbook that covers the joint development of algorithms and architectures is [48], anecdotal observations can be found in [49]. Example Sequence estimation (SE) is an important subfunction in every EDGE15 receiver and actually part of the channel estimator - channel equalizer function. Design targets include a soft (i.e. multi valued) output, a processing time of less than 577 μs per burst, a small circuit, low power dissipation, and a minimal block error rate at any given signal-to-noise ratio. The key properties of three algorithmic alternatives are summarized below and in fig.3.15. 15 Enhanced Data Rates for GSM Evolution, a standard from wireless telecommunications. 92 CHAPTER 3 FROM ALGORITHMS TO ARCHITECTURES Algorithm Delayed Max-log-MAP Soft Output Decision Feedback Viterbi Equalizer Soft output no yes yes Forward recursion yes yes yes Backward recursion no yes no Backtracking step yes no no Memory requirements 1x 50x 0.13x The DDFSE algorithm does not qualify because of its inferior estimation performance that is due to the hard (i.e. binary) output. While the max-log-MAP and SOVE algorithms perform almost equally well, the backward iteration step of the former requires so many branch metrics to be kept in memory that no architecture-level optimization can compensate the handicap. This explains why the SOVE algorithm was chosen as a starting point in the project from which the above data were taken [50]. 10 SNR [dB] 15 205Coded BLER DDFSE Log-MAP SOVE Max-Log-MAP 10−3 10−2 10−1 100 FIGURE 3.15 Block error rate vs. SNR for various sequence estimation algorithms. \u0002 3.3.4 A GRAPH-BASED FORMALISM FOR DESCRIBING PROCESSING ALGORITHMS We will often find it useful to capture a data processing algorithm in a data dependency graph (DDG) as this visual formalism is suggestive of possible hardware structures. A DDG is a directed graph where vertices and edges have non-negative weights, see fig.3.16. A vertex stands for a memoryless operation and its weight indicates the amount of time necessary to carry out that operation. The precedence of one operation over another is represented as a directed edge. The weight of an edge indicates by how many computation cycles or sampling periods execution of the first operation must precede that of 3.3 DEDICATED VLSI ARCHITECTURES AND HOW TO DESIGN THEM 93 the second.16 Edge weight zero implies the two operations are scheduled to happen within the same computation or sampling period — one after the other, though —. An edge may also be viewed as expressing the transport of data from one operation to another and its weight as indicating the number of registers included in that transport path. edge transport weight indicates latency in computation cycles Definitions vertex operation memoryless fan out expressed as \"no operation\" vertex illegal! 0 Danger of race conditions 0 0 00 0 circular paths of edge weight zero are not admitted! x(k) time-varying data source variable input expressed as c constant data source constant input expressed as y(k) data sink output expressed as Shorthand notations introduced for convenience 0 = 1 = 2 = FIGURE 3.16 Data dependency graph (DDG) notation. To warrant consistent outcomes from computation, circular paths of total edge weight zero are disallowed in DDGs.17 Put differently, any feedback loop shall include one or more latency registers. 16 The term “computation cycle” is to be explained shortly in section 3.3.7. 17 A circular path is a closed walk in which no vertex, except the initial and final one, appears more than once and that respects the orientation of all edges traversed. As the more customary terms “circuit” and “cycle” have other meanings in the context of hardware design, we do prefer “circular path” in spite of its clumsiness. For the same reason, let us use “vertex” when referring to graphs and “node” when referring to electrical networks. A zero-weight circular path in a DDG implies immediate feedback and expresses a self-referencing combinational function. Such zero-latency feedback loops are known to expose the pertaining electronic circuits to unpredictable behavior and are, therefore, highly undesirable, see section 6.4.3 for details. 94 CHAPTER 3 FROM ALGORITHMS TO ARCHITECTURES 3.3.5 THE ISOMORPHIC ARCHITECTURE Example (a) y(k) = n=0 N=3 bn x(k-n) (b) b20b 1b x(k) y(k) b3 y(k) =n=0 3 bn(k) x(k-n) (c) b20b 1b x(k) y(k) b3 z-1 z-1 z-1 (d) b21b x(k) b3 multiplier parallel * * ** y(k) adder+++ 0b 1:1 Σ FIGURE 3.17 Third order (N = 3) transversal ﬁlter expressed as a mathematical function (a), drawn as data dependency graph (DDG) (b), and implemented with the isomorphic hardware architecture (d). Signal ﬂow graph shown for comparison (c). \u0002 No matter how one has arrived at some initial proposal, it always makes sense to search for a better hardware arrangement. Inspired VLSI architects let guide themselves by intuition and experience to come up with a few tentative designs before looking for beneficial reorganizations. Yet, for the subsequent discussion and evaluation of the various equivalence transforms available, we need something to compare with. A natural candidate is the isomorphic architecture, see fig.3.17dfor an example, where • Each combinational operation in the DDG is carried out by a hardware unit of its own, • Each hardware register stands for a latency of one in the DDG, • There is no need for control because DDG and block diagram are isomorphic,18 and • Clock rate and data input/output rate are the same. 18 Two directed graphs are said to be isomorphic if there exists a one-to-one correspondence between their vertices and between their edges such that all incidence relations and all edge orientations are preserved. More informally, two isomorphic graphs become indistinguishable when the labels and weights are removed from their vertices and edges. Remember that how a graph is drawn is of no importance for the theory of graphs. 3.3 DEDICATED VLSI ARCHITECTURES AND HOW TO DESIGN THEM 95 An architecture design as naive as this obviously cannot be expected to utilize hardware efficiently, but it will serve as a reference for discussing both the welcome and the unfavorable effects of various architectural reorganizations. You may also think of the isomorphic architecture as a hypothetical starting point from which any more sophisticated architecture can be obtained by applying a sequence of equivalence transforms.19 3.3.6 RELATIVE MERITS OF ARCHITECTURAL ALTERNATIVES Throughout our analysis, we will focus on the subsequent figures of merit. Cycles per data item \u0003 denotes the number of computation cycles that separates the releasing of two consecutive data items, or — which is normally the same — the number of computation cycles between accepting two subsequent data items. Longest path delay tlp indicates the lapse of time required for data to propagate along the longest combinational path through a given digital network. Path lengths are typically indicated in ns. What makes the maximum path length so important is that it limits the operating speed of a given architecture. For a circuit to function correctly, it must always be allowed to settle to a — typically new — steady state within a single computation period Tcp.20 We thus obtain the requirement tlp ≤ Tcp where the exact meaning of computation period is to be defined shortly in section 3.3.7. Time per data item T indicates the lapse of time between releasing two subsequent data items. Depending on the application, T might be stated in μs/sample, ms/frame, s/computation, or time per unit of what you want to have done in the end. T = \u0003 · Tcp ≥ \u0003 · tlp holds with equality if the circuit gets clocked at the fastest possible rate. Data throughput \u0004 = 1 T is the most meaningful measure of computational performance. Throughput gets expressed in terms of data items or operations processed per time unit e.g. in pixel/s, sample/s, frame/s, data record/s, FFT/s, matrix inversion/s, and the like. It is given by \u0004 = fcp \u0003 = 1 \u0003 · Tcp ≤ 1 \u0003 · tlp (3.3) for a circuit operated at computation rate fcp or, which is the same, with a computation period Tcp.21 Again, we are most interested in the maximum throughput where Tcp = tlp. Latency L indicates the number of computation cycles from a data item being entered into a circuit until the pertaining result becomes available at the output. Latency is zero when the result appears within the very clock cycle during which the input datum was fed in. 19 See problem 6 for a more thorough exposure. Also observe that our transform approach to architecture design bears some resemblance to the theory of evolution. 20 We neither consider multi-cycle paths, nor wave-pipelined operation, nor asynchronous circuits at this point. 21 It is sometimes more adequate to express data throughput in terms of bits per time unit. (3.3) must then be restated as \u0004 = w fcp \u0003 where w indicates how many bits make up one data item. 96 CHAPTER 3 FROM ALGORITHMS TO ARCHITECTURES Circuit size A. Depending on how actual hardware costs are best expressed, the designer is free to interpret size as area occupation (in mm2 or lithographic squares F2 for ASICs)oras circuit complexity (in terms of GE for ASICs and FPL). Size-time product AT combines circuit size and computation time to indicate the hardware resources spent to obtain a given throughput. This is simply because AT = A \u0004 . The lower the AT-product, the more hardware-efficient a circuit. Energy per data item E is meant to quantify the amount of energy dissipated in carrying out some given computation on a data item. As examples, consider indications in pJ/MAC, nJ/sample, μJ/datablock or mWs/videoframe. This quantity can also be understood as power-per-throughput ratio E = P \u0004 measured in units like mW Mbit/s or W Gop/s because energy data item = energy per second data item per second = power throughput. When it comes to evaluating microprocessors, the inverse figure of merit expressed in terms of Mop/s mW or Gop/s W is more popular, however. Energy per data item is closely related to the power-delay product (PDP) pdp = P · tlp ,a quantity often used for comparing standard cells and other transistor-level circuits. The difference is that our definition accounts for slack and for multi-cycle operations because E = PT = P · \u0003 · Tcp ≥ P · \u0003 · tlp = \u0003 · pdp. Energy-time product ET indicates how much energy gets spent for achieving a given throughput since ET = E \u0004 = P \u00042 which also explains the synonym “energy-per-throughput ratio”. ET may be expressed in μJ datablock/s or mWs videoframe/s , for instance, and is a useful yardstick whenever a better performance must be bought at the expense of energy efficiency. The energy-delay product (EDP) is to ET what the PDP is to E. Example In the occurrence of the architecture shown in fig.3.17d, one easily finds the quantities below A = 3Areg + 4A∗ + 3A+ (3.4) \u0003 = 1 (3.5) tlp = treg + t∗ + 3t+ (3.6) AT = (3Areg + 4A∗ + 3A+)(treg + t∗ + 3t+) (3.7) L = 0 (3.8) E = 3Ereg + 4E∗ + 3E+ (3.9) where indices ∗, + and reg refer to a multiplier, an adder, and a data register respectively. \u0002 3.3 DEDICATED VLSI ARCHITECTURES AND HOW TO DESIGN THEM 97 A word of caution is due here. Our goal in using formulae to approximate architectural figures of merit is not so much to obtain numerical values for them but to explain how the various equiv- alence transforms available to VLSI architects tend to affect them.22 For illustration purposes, we will repeatedly use a graphical representation that suggests hardware organization, circuit size, longest path length, data throughput, and latency in a symbolic way, see figs 3.18b and c for a first example. 3.3.7 COMPUTATION CYCLE VERSUS CLOCK PERIOD So far, we have been using the term computation period without defining it. In synchronous digital circuits, a calculation is broken down into a series of shorter computation cycles the rhythm of which gets imposed by a periodic clock signal. During each computation cycle, fresh data emanate from a register, propagate through combinational circuitry where they undergo various arithmetic, logic, and/or routing operations before the result gets stored in the next analogous register (same clock, same active edge). Deﬁnition 1. A computation period Tcp is the time span that separates two consecutive computation cycles. For the moment being, it is safe to assume that computation cycle, computation period, clock cycle, and clock period are all the same, Tcp = Tclk, which is indeed the case for all those circuits that adhere to single-edge-triggered one-phase clocking.23 The inverse, that is the number of computation cycles per second, is referred to as computation rate fcp = 1 Tcp . 22 As an example, calculation of the long path delay tlp is grossly simplified in (3.6). For one thing, interconnect delays are neglected which is an overly optimistic assumption. For another thing, the propagation delays of the arithmetic operations are simply summed up which sometimes is a pessimistic assumption, particularly in cascades of multiple ripple-carry adders (RCA) where all operands arrive simultaneously. Synthesis followed by place and route often is the only way to determine overall path delays with sufficient accuracy. 23 As an exception, consider dual-edge-triggering where each clock period comprises two consecutive computation periods so that Tcp = 1 2 Tclk. Details are to follow in section 7.2.3. 98 CHAPTER 3 FROM ALGORITHMS TO ARCHITECTURES 3.4 EQUIVALENCE TRANSFORMS FOR COMBINATIONAL COMPUTATIONS A computation that depends on the present arguments exclusively is termed combinational. A sufficient condition for combinational behavior is a DDG which is free of circular paths and where all edge weights equal zero. Consider some fixed but otherwise arbitrary combinational function y(k) = f (x(k)). As suggested by the dashed edges in fig.3.18a, both input x(k) and output y(k) can include several subvectors. No assumptions are made about the complexity of f which, in principle, can range from a two-bit addition, over an algebraic division, to the Fast Fourier Transform (FFT) operation on a data block, see fig.3.27, and beyond. In practice, architecture designers primarily focus on those operations that heavily impact chip size, performance, power dissipation, etc. The isomorphic architecture simply amounts to a hardware unit that does nothing but evaluate function f , a rather expensive proposition if f is complex such as in the FFT example. Three options for reorganizing and improving this unsophisticated arrangement exist.24 1. Decomposing function f into a sequence of subfunctions that get executed one after the other in order to reuse the same hardware as much as possible. 2. Pipelining of the functional unit for f to improve computation rate by cutting down combinational depth and by working on multiple consecutive data items simultaneously. 3. Replicating the functional unit for f and having all units work concurrently. It is intuitively clear that replication and pipelining both trade circuit size for performance while iterative decomposition does the opposite. This gives rise to questions such as “Does it make sense to combine pipelining with iterative decomposition in spite of their contrarian effects?” “How do replication and pipelining compare?” “Are there situations where one should be preferred over the other?” which we will try to answer in the following subsections. 24 Of course, many circuit alternatives for implementing a given arithmetic or logic function also exist at the gate level. However, within the general context of architecture design, we do not address the problem of developing and evaluating such options as this involves lower-level considerations that strongly depend on the specific operations and on the target library. The reader is referred to the specialized literature on computer arithmetics and on logic design. 3.4 EQUIVALENCE TRANSFORMS FOR COMBINATIONAL COMPUTATIONS 99 3.4.1 COMMON ASSUMPTIONS f (a) x(k) y(k) (c)size throughputlatencylongest path (b) register input stream output stream datapath section combinational logic no control section FIGURE 3.18 DDG for some combinational function f (a). A symbolic representation of the reference hardware conﬁguration (b) with its key characteristics highlighted (c). The architectural arrangement that will serve as a reference for comparing various alternative designs is essentially identical to the isomorphic configuration of fig.3.18a with a register added at the output to allow for the cascading of architectural chunks without their longest path delays piling up. The characteristics of the reference architecture then are A(0) = Af + Areg (3.10) \u0003(0) = 1 (3.11) tlp(0) = tf + treg (3.12) AT(0) = (Af + Areg)(tf + treg) (3.13) L(0) = 1 (3.14) E(0) = Ef + Ereg (3.15) where subscript f stands for the datapath hardware that computes some given combinational function f and where subscript reg denotes a data register. For the sake of simplicity, the word width w in the datapath is assumed to be constant throughout. The quotient Af /Areg relates the size of the datapath hardware to that of a register, and tf /treg does the same for their respective time requirements.25 Their product Af Areg tf treg thus reflects the computational complexity of function f in some sense. (3.16) holds whenever logic function f is a fairly substantial computational operation. We will consider this the typical, although not the only possible case. Aregtreg ≪ Af tf (3.16) Many of the architectural configurations to be discussed require extra circuitry for controlling datapath operation and for routing data items. Two additive terms Actl and Ectl are introduced to account for this where necessary. As it is very difficult to estimate the extra hardware without detailed knowledge of the specific situation at hand, the only thing that can be said for sure is that Actl is on the order of 25 Typical size A and delay figures t for a number of logic and arithmetic operations are given as illustrative material in appendix 3.12. 100 CHAPTER 3 FROM ALGORITHMS TO ARCHITECTURES Areg or larger for most architectural transforms. Control overhead may in fact become significant or even dominant when complex control schemes are brought to bear as a result from combining multiple transforms. As for energy, we will focus on the dynamic contribution that gets dissipated in charging and discharing electrical circuit nodes as a consequence of fresh data propagating through gate-level networks. Any dissipation due to static currents or due to idle switching is ignored. Throughout our architectural comparisons, we further assume all electrical and technological conditions to remain the same.26 A comparison of architectural alternatives on equal grounds is otherwise not possible as a shorter path delay or a lower energy figure would not necessarily point to a more efficient design alternative. 3.4.2 ITERATIVE DECOMPOSITION The idea behind iterative decomposition — or decomposition, for short — is nothing else than resource sharing through step-by-step execution. The computation of function f is broken up into a sequence of d subtasks which are carried out one after the other. From a dataflow point of view, intermediate results are recycled until the final result becomes available at the output d computation cycles later, thereby making it possible to reuse a single hardware unit several times over. A configuration that reuses a multifunctional datapath in a time-multiplex fashion to carry out f in d = 3 subsequent steps is symbolically shown in fig.3.19. Note the addition of a control section that pilots the datapath on a per-cycle basis over a number of control lines. f (b) datapath section f13f2f control section decomposition iterative (a) f1 3f 2ff FIGURE 3.19 Iterative decomposition. DDG (a) and hardware conﬁguration for d = 3(b). Performance and cost analysis Assumptions: 1. The total size requirement for implementing the various subfunctions into which f is decomposed ranges between Af d and Af . 2. The decomposition is lossless and balanced, i.e. it is always possible to break up f into d subfunctions the computations of which require a uniform amount of time tf d . 26 This includes supply voltage, cell library, transistor sizes, threshold voltages, fabrication process, and the gate-level structure of arithmetic units. 3.4 EQUIVALENCE TRANSFORMS FOR COMBINATIONAL COMPUTATIONS 101 As a first-order approximation, iterative decomposition leads to the following figures of merit: Af d + Areg + Actl ≤ A(d) ≤ Af + Areg + Actl (3.17) \u0003(d) = d (3.18) tlp(d) ≈ tf d + treg (3.19) d(Areg + Actl)treg + (Areg + Actl)tf + Af treg + 1 d Af tf ≤ AT(d) ≤ d(Af + Areg + Actl)treg + (Af + Areg + Actl)tf (3.20) L(d) = d (3.21) E(d) ≷ Ef + Ereg (3.22) Let us confine our analysis to situations where the control overhead can be kept small so that Areg ≈ Actl ≪ Af and Ereg ≈ Ectl ≪ Ef . A key issue in interpreting the above results is whether size A(d) tends more towards its lower or more towards its upper bound in (3.17). While iterative decomposition can, due to (3.16), significantly lower the AT-product in the former case, it does not help in the latter. The lower bounds hold in (3.17)and (3.20) when the chunk’s function f makes repetitive use of a single subfunction because the necessary datapath is then essentially obtained from cutting the one that computes f into d identical pieces only one of which is implemented in hardware. A monofunctional processing unit suffices in this case. At the opposite end are situations where computing f asks for very disparate subfunctions that cannot be made to share much hardware resources in an efficient way. Iterative decomposition is not an attractive option in this case, especially if register delay, control overhead, and the difficulty of meeting assumption 2 are taken into consideration. Multiplication, for instance, can be broken into repeated shift & add operations. Chains of additions and subtractions in either fixed point or floating point arithmetics also lend themselves well for being combined into a common computational unit. Similarly, angle rotations, trigonometric, and hyperbolic functions can all be computed with essentially the same CORDIC datapath. As for energy efficiency, there are two mechanisms that counteract each other. On the one hand, iterative decomposition entails register activity not present in the original circuit. The extra control and data recycling logic necessary to implement step-by-step execution further inflate dissipation. On the other hand, we will later find that long register-to-register signal propagation paths tend to foster transient node activities, aka glitches. Cutting such propagation paths often helps to mitigate glitching activities and the associated energy losses. Such second order effects are not accounted for in the simplistic unit-wise additive model introduced in (3.15), however, making it difficult to apprehend the impact of iterative decomposition on energy before specific circuit details become available. Example A secret-key block cipher operated in electronic code book (ECB) mode is a highly expensive combinational function. ECB implies a memoryless mapping y(k) = c(x(k), u(k)) where x(k) denotes the plaintext, y(k) the ciphertext, u(k) the key, and k the block number or time index. What most block ciphers, such as the Data Encryption Standard (DES), the International Data Encryption Algorithm (IDEA), and the Advanced Encryption Standard (AES) Rijndael have in common is a cascade of several 102 CHAPTER 3 FROM ALGORITHMS TO ARCHITECTURES rounds, see fig.3.20 for the IDEA algorithm [51]. The only difference between the otherwise identical rounds is in the values of the subkeys used that get derived from u(k). What is referred to as output transform is nothing else than a subfunction of the previous rounds. first round seven more identical rounds output transformation x(k) y(k) sub- key sub- key sub- key sub- key sub- key sub- key sub- key sub- key sub- key sub- key = modulo 2 addition bitwise = modulo 2 addition 16 = multiplication modulo 2 +1 16 FIGURE 3.20 DDG of the block cipher IDEA. If we opt for iterative decomposition, a natural choice consists in designing a datapath for one round and in recycling the data with changing subkeys until all rounds have been processed. As control is very simple, the circuit’s overall size is likely to stay close to the lower bound in (3.17) after this first step of decomposition. When continuing in the same direction, however, benefits will diminish because the operations involved (bitwise addition modulo 2, addition modulo 216 and multiplication modulo (216 + 1) ) are very disparate. In addition, the impact of control on the overall circuit size would be felt. \u0002 A more radical approach is to decompose arbitrary functions into sequences of arithmetic and/or logic operations from a small but extremely versatile set and to provide a single ALU instead. The datapath of any microprocessor is just a piece of universal hardware that arose from the general idea of step-by- step computation, and the reduced instruction set computer (RISC) can be viewed as yet another step in the same direction. While iterative decomposition together with programmability and time sharing, see 3.4.5, explains the outstanding Flexibility and hardware economy of this paradigm, it also accounts for its modest performance and poor energy efficiency when compared to more focussed architecture designs. 3.4 EQUIVALENCE TRANSFORMS FOR COMBINATIONAL COMPUTATIONS 103 Examples Examples of ASICs the throughputs of which exceeded that of contemporary high-end general-purpose processors by orders of magnitude are given in sections 3.2 and 3.7.3. \u0002 3.4.3 PIPELINING Pipelining aims at increasing throughput by cutting combinational depth into several separate stages of approximately uniform computational delays by inserting registers in between.27 The combinational logic between two subsequent pipeline registers is designed and optimized to compute one specific subfunction. As an ensemble, the various stages cooperate like specialist workers on an assembly line. Fig.3.21 sketches a functional unit for f subdivided into p = 3 pipeline stages by p − 1 extra registers. Note the absence of any control hardware. (a) f1 3f 2ff datapath section f1 3f 2f no control section pipelining (b) FIGURE 3.21 Pipelining. DDG (a) and hardware conﬁguration for p = 3(b). Performance and cost analysis Assumptions: 1. The combinational logic for implementing function f is not affected by the number of pipeline stages introduced. Its overall size Af , therefore, remains constant. 2. The pipeline is lossless and balanced, i.e. similarly to decomposition it is always possible to partition the logic into p stages such that all have identical delays tf p . 3. The size penalty of pipelining can be expressed by an additive term Areg for each register accounting for the silicon area occupied by storage elements. 4. At each pipeline stage a performance penalty results from introducing a register delay treg which includes the delay caused by the storage element. Pipelining changes performance and cost figures as follows: A(p) = Af + pAreg (3.23) \u0003(p) = 1 (3.24) tlp(p) ≈ tf p + treg (3.25) 27 For a more formal discussion see subsection 3.6.1. 104 CHAPTER 3 FROM ALGORITHMS TO ARCHITECTURES AT(p) ≈ pAregtreg + (Aregtf + Af treg) + 1 p Af tf (3.26) L(p) = p (3.27) E(p) fine grain ≷ coarse grain Ef + Ereg (3.28) Both performance and size grow monotonically with pipeline depth. The same holds true for latency. What is more interesting, a modest number of pipeline stages each of which has a substantial depth dramatically lowers the AT-product due to (3.16). This regime is referred to as coarse grain pipelining. Example Equation (3.25) relates combinational delay to register delay. Another popular way to quantify the degree of pipelining is to express the delay on the longest path as a multiple of fanout-of-4 (FO4) inverter delays.28 year clock freq. FO4 inverter delays CPU [MHz] per pipeline stage Intel 80386 1989 33 ≈ 80 Intel Pentium 4 2003 3200 12...16 Core 2 Duo 2007 2167 ≈ 40 Core i7 980X 2011 3333...3600 42...46 IBM POWER5 2004 1650...1900 22 IBM POWER6 2007 3500...5000 13 IBM Cell Processor 2006 3200 11 \u0002 Continuing along this line, one may want to insert more and more pipeline registers. However, (3.25) reveals that the benefit fades when the combinational delay per stage tf p approaches the register delay treg. For large values of p the area-delay product gets dominated by the register delay rather than by the payload function. A natural question for this type of deep or fine grain pipelining is to ask “What is the maximum computation rate for which a pipeline can be built?” The fastest logic gates that can do any data processing are 2-input nand and nor gates.29 Even if we are prepared to profoundly redesign a pipeline’s logic in an attempt to minimize the longest path tlp,we must leave room for at least one such gate between two consecutive registers. Tcp ≥ min(tlp) = min(tgate) + treg = min(tnand, tnor) + tsu ff + tpd ff (3.29) thus represents a lower bound for (3.25). Practical applications that come close to this theoretical mini- mum are limited to tiny subcircuits, however, mainly because of the disproportionate number of registers required, but also because meeting assumptions 1 and 2 is difficult with fine grained pipelines. Even in 28 Comparing circuit alternatives in terms of FO4 inverters makes sense because fanout-of-4 inverters commonplace in buffer trees driving large loads and because the delays of other static CMOS gates have been found to track well with those of FO4 inverters. 29 This is because binary nand and nor operations (a) form a complete gate set each and (b) are efficiently implemented from MOSFETs, see section A.2.10. 3.4 EQUIVALENCE TRANSFORMS FOR COMBINATIONAL COMPUTATIONS 105 high-performance datapaths, energy efficiency and economic reasons preclude pipelining below 12 or so FO4 inverter delays per stage. As the above numbers illustrate, Intel has in fact reversed the historical trend towards ever deeper pipelines when transiting from the Pentium 4 to the Core architecture to reclaim energy efficiency [39]. Fig.3.22 confirms that this is indeed a general industry trend. Equation (3.29) further indicates that register delay is critical in high speed design. In fact, a typical relation is treg ≈ 3...5 min(tgate). As a consequence, it takes twenty or so levels of logic between subsequent registers before flip-flop delays are relegated to insignificant proportions. A high-speed cell library must, therefore, not only include fast combinational functions but also provide bistables with minimum insertion delays.30 Example Plugging into (3.29) typical numbers for a 2-input nor gate and a D-type flip-flop with no reset from a 130 nm CMOS standard cell library, one obtains Tcp ≥ tNOR2D1 + tDFFPB1 = 18 ps + 249 ps ≈ 267 ps which corresponds to a maximum computation rate of about 3.7 GHz. \u0002 1985 0 40 80 120 160 1990 1995 2000 Relative Length of a Pipe Stage YearFO4 per Cycle 2005 2010 2015 NexGen HAL Cyrix Sun SGI MIPS Motorola Intel IBM HP Hitachi Fujitsu DEC Cypress AMD 1/2 FIGURE 3.22 Evolution of pipeline depth over the years (source Stanford CPU database). 30 Function latches where bistables and combinational logic get merged into a single library cell in search of better performance are to be discussed in section 7.2.6. 106 CHAPTER 3 FROM ALGORITHMS TO ARCHITECTURES “How many stages yield optimum pipeline efficiency?” Optimum hardware efficiency means minimum size-time product AT(p) = min (3.30) which is obtained for p0 = √ Af tf Aregtreg (3.31) Beyond this point, adding more pipeline registers causes the size-time product to deteriorate even though performance is still pushed further. It also becomes evident from (3.31) that, in search of an economic solution, the more complex a function, the more pipelining it supports. In practice, efficiency is likely to degrade before p0 is reached because our initial assumptions 1 and 2 cannot be entirely satisfied. Also, shallow logic just a few gates deep is more exposed to on-chip variations (OCV). “How does pipelining affect energy efficiency?” The additional registers suggest that any pipelined datapath dissipates more energy than the reference architecture does. This is certainly true for fine grain pipelines where the energy wasted by the switching of all those extra subcircuits becomes the dominant contribution. For coarse grain designs, the situation is more fortunate. Experience shows that pipeline registers tend to reduce the unproductive switching activity associated with glitching in deep combinational networks, a beneficial side effect neglected in a simple additive model. Interestingly, our finding that throughput is greatly increased makes it possible to take advantage of coarse grain pipelining for improving energy efficiency, albeit indirectly. Recall that the improved throughput is a result from cutting the longest path while preserving a processing rate of one data item per computation cycle. The throughput of the isomorphic architecture is thus readily matched by a pipelined datapath implemented in a slower yet more energy-efficient technology, e.g. by operating CMOS logic from a lower supply voltage or by using mostly minimum size transistors. Our model cannot reflect this opportunity because we have decided to establish energy figures under the assumption of identical operating conditions and cell libraries. Another highly welcome property of pipelining is the absence of energy-dissipating control logic. Pipelining in the presence of multiple feedforward paths Although pipelining can be applied to arbitrary feedforward computations, there is a reservation of economic nature when a DDG includes many parallel paths. In order to preserve overall functionality, any latency introduced into one of the signal propagation paths must be balanced by inserting an extra register into each of its parallel paths. Unless those shimming registers help cut combinational depth there, they bring about substantial size and energy penalties, especially for deep pipelines where p is large. 3.4 EQUIVALENCE TRANSFORMS FOR COMBINATIONAL COMPUTATIONS 107 Example c r’(k) u(k) l’(k) c r(k) w(k) u(k) l(k) v(k) g(l,r,u) g(v,w,u’) enciphering deciphering function function (a) r(k) w(k) c1 2c ..... dc .....shimming registers pipeline registers u(k,1) l(k) v(k) ..... (b) u(k,2) u(k,3) = modulo 2 addition bitwise =c memoryless mapping arbitrary pipelining FIGURE 3.23 Involutory cipher algorithm. DDG before (a) and after pipelining (b). With simplifications, fig.3.23a reproduces the block cipher IDEA. Variable k stands for the block index, l(k) and r(k) each denote half of a 64 bit plaintext block while v(k) and w(k) do thesamefor a64 bit ciphertext block. u(k) and u′(k) stand for the keys used during enciphering and deciphering operations respectively. Provided the two keys are the same, i.e. u′(k) = u(k), the net result is l′(k) = l(k) and r′(k) = r(k), which implies that the plaintext is recovered after calling g twice. Note that this involution property31 is totally independent of function c which, therefore, can be designed such as to maximize cryptographic security. Extensive pipelining seems a natural way to reconcile the computational complexity of c with ambitious performance goals. Yet, as a consequence of the two paths bypassing c, every pipeline register entails two shimming registers, effectively tripling the costs of pipelining, see fig.3.23b. This is the reason why pipeline depth had to be limited to eight stages per round in a VLSI implementation of the IDEA cipher in spite of stringent throughput requirements [52]. \u0002 31 A function g is said to be involutory iff g(g(x)) ≡ x, ∀ x. As trivial examples, consider multiplication by −1 in classic algebra where we have −(−x) ≡ x, the complement function in Boolean algebra where x ≡ x, or a mirroring operation from geometry. Involution is a welcome property in cryptography since it makes it possible to use exactly the same equipment for both enciphering and deciphering. 108 CHAPTER 3 FROM ALGORITHMS TO ARCHITECTURES 3.4.4 REPLICATION Replication is a brute force approach to performance: If one functional unit does not suffice, allow for several of them. Concurrency is obtained from providing q instances of identical functional units for f and from having each of them process one out of q data items in a cyclic manner. To that end, two synchronous q-way switches distribute and recollect data at the chunk’s input and output respectively. An arrangement where q = 3 is shown in fig.3.24.32 Overall organization and operation is reminiscent of a multi-piston pump. f (a) datapath section (b) distributor recollector control section replication FIGURE 3.24 Replication. DDG (a) and hardware conﬁguration for q = 3(b). Performance and cost analysis Assumptions: 1. Any size penalties associated with distributing data to replicated functional units and with recollecting them are neglected. 2. Any energy dissipated in data distribution and recollection is ignored. The above assumption hold fairly well provided the circuitry for computing f is much larger than the one for data distribution and recollection. The key characteristics of replication then become: A(q) = q(Af + Areg) + Actl (3.32) \u0003(q) = 1 q (3.33) tlp(q) ≈ tf + treg (3.34) AT(q) ≈ (Af + Areg + 1 q Actl)(tf + treg) ≈ (Af + Areg)(tf + treg) (3.35) L(q) = 1 (3.36) E(q) ≈ Ef + Ereg + Ectl (3.37) 32 Multiple processing units that work in parallel are also found in situations where the application naturally provides data in parallel streams, each of which is to undergo essentially the same processing. In spite of the apparent similarity, this must not be considered as the result of replication, however, because DDG and architecture are isomorphic. This gets reflected by the fact that no data distribution and recollection mechanism is required in this case. Please refer to section 3.4.5 for the processing of multiple data streams. 3.4 EQUIVALENCE TRANSFORMS FOR COMBINATIONAL COMPUTATIONS 109 As everyone would expect, replication essentially trades area for speed. Except for the control overhead, the AT-product remains the same. Pipelining, therefore, is clearly more attractive than replication as long as circuit size and performance do not get dominated by the pipeline registers, see fig.3.25 for a comparison. Energywise, replication is indifferent except for the contributions for datapath control and data distribution/recollection. Also note, by the way, that replication does not shorten the computation period which contrasts with iterative decomposition and pipelining.33 A more accurate evaluation of replication versus pipelining would certainly require revision of some of the assumptions made here and does depend to a large extent on the actual DDG and on implementation details. Nevertheless, it is safe to conclude that neither fine grain pipelining nor replication are as cost- effective as coarse grain pipelining. Its penalizing impact on circuit size confines replication to rather exceptional situations in ASIC design. A megacell available in layout form exclusively represents such a need because adding pipeline registers to a finished layout would ask for a disproportionate effort. Replication is limited to high performance circuits and always combined with generous pipelining. Superscalar and multicore microprocessors are two related ideas from computer architecture.34 Several factors have pushed the computer industry towards replication: CMOS technology offered more room for increasing circuit complexity than for pushing clock frequencies higher. The faster the clock, the smaller the region on a semiconductor die that can be reached within a single clock period.35 Fine grain pipelines dissipate a lot of energy for relatively little computation. Reusing a well-tried subsystem benefits design productivity and lowers risks. A multicore processor can still be of commercial value even if one of its CPUs is found to be defective. Example Consider a simple network processor that handles a stream of incoming data packets and does some address calculations before releasing the packets with a modified header. Let that processor be characterized by the subsequent cost figures: Af = 60w GE, Areg = 6w GE, where w is an integer relating to datapath width, tf = 12 ns, treg = 1.2 ns and min(tgate) = 0.3 ns. 33 Observe that the entirety of functional units must be fed with q data items per computation cycle and that processed data items emanate at the same rate. Only the data distribution and recollection subcircuits must be made to operate at arate q times higher than the computational instances themselves. High data rates are obtained from configuring data distribution/recollection networks as heavily pipelined binary trees. Maximum speed is, again, determined by (3.29). Yet, circumstances permitting, it may possible to implement data distribution and recollection using a faster technology than the one being used in the body of the processing chunk (“superfast distribution and recollection”). Also see [53] for further information on fast data distribution/recollection circuitry. 34 A superscalar processor combines multiple execution units, such as integer ALUs, FPUs, load/store units, and the like, into one CPU. The architecture enables the processor to concurrently fetch and process multiple instructions from a thread of execution. Multicore architectures go one step further in that they replicate entire CPUs on a single chip and so enable a processor to work on two or more threads of execution at a time. 35 For a rationale, refer to section 7.3 that discusses delay in interconnect lines without and with repeaters. 110 CHAPTER 3 FROM ALGORITHMS TO ARCHITECTURES A Areg A f pipelining break-even point treg ft gatetmin( ) minimum size optimum efficiency coarse-grain regime T 6 5 30 8 10 15 20 12 60 100 grain fine- regime p =1234 isomorphic configuration replication 3 4 5 6 7 8 10 9 12 11 q =1 2 40 80 120 25 50 maximum throughput 2468 10 12 [ns] [GE] 800w 700w 600w 500w 400w 300w 200w 100w pipelining possible deepest infeasible distribution recollection circuitry super-fast and except with towards throughput hardware towards efficiency hardware towards economy FIGURE 3.25 AT -characteristics of pipelining and replication compared. Simpliﬁed by assuming perfect balancing in the occurrence of pipelining and by abstracting from control, data distribution and recollection associated with replication. \u0002 3.4.5 TIME SHARING So far we have been concerned with the processing of a single data stream as depicted in fig.3.18. Now consider a situation where a number of parallel data streams undergo processing as illustrated in fig.3.26, for instance. Note that the processing functions f , g and h may or may not be the same. The isomorphic architecture calls for a separate functional unit for each of the three operations in this case. This may be an option in applications such as image processing where a great number of dedicated but comparatively simple processing units are repeated along one or two dimensions, where data exchange is mainly local, and where performance requirements are very high. More often, however, the costs of fully parallel processing are unaffordable and one seeks to cut overall circuit size. A natural idea is to pool hardware by having a single functional unit process the parallel data streams one after the other in a cyclic manner. Analogously to replication, a synchronous s-way switch at the input of that unit collects the data streams while a second one redistributes the processed data at the output. While the approach is known as time sharing in computing, it is more often referred 3.4 EQUIVALENCE TRANSFORMS FOR COMBINATIONAL COMPUTATIONS 111 to as multiplexing or as resource sharing in the context of circuit design.36 What it requires is that the circuitries for computing the various functions involved all be combined into a single datapath of possibly multifunctional nature. A student sharing his time between various subjects might serve as an analogy from every-day live. datapath section collector redistributor output streams input streams f g h control section (a) hfg (b) hfg sharing time FIGURE 3.26 Time sharing. DDG with parallel data streams (a) and hardware conﬁguration for s = 3(b). Performance and cost analysis Assumptions: 1. The size of a circuit capable of implementing functions f , g and h with a single computational unit ranges between max f ,g,h (A) = max(Af , Ag, Ah) and ∑ f ,g,h A = (Af + Ag + Ah). 2. The time for the combined computational unit to evaluate any of the functions f , g and h has a fixed value max f ,g,h (t) = max(tf , tg, th). 3. As for replication, any size and energy penalties associated with collecting and redistributing data are neglected. 4. The energy spent for carrying out functions f , g and h (all together) with one shared unit is closer to s max f ,g,h (E) = s max(Ef , Eg, Eh) than to ∑ f ,g,h E = Ef + Eg + Eh. Time sharing yields the following circuit characteristics: max f ,g,h (A) + Areg + Actl ≤ A(s) ≤ ∑ f ,g,h A + Areg + Actl (3.38) \u0003(s) = s (3.39) tlp(s) ≈ max f ,g,h (t) + treg (3.40) s(max f ,g,h (A) + Areg + Actl)(max f ,g,h (t) + treg) ≤ AT(s) ≤ s(∑ f ,g,h A + Areg + Actl)(max f ,g,h (t) + treg) (3.41) L(s) = s (3.42) E(s) ≈ s max f ,g,h (E) + Ereg + Ectl (3.43) 36 This is our second resource sharing technique after iterative decomposition introduced in section 3.4.2. 112 CHAPTER 3 FROM ALGORITHMS TO ARCHITECTURES Similarly to what was found for iterative decomposition, the question whether size A(s) tends more towards its lower or more towards its upper bound in (3.38) greatly depends on how similar or dissimilar the individual processing tasks are. The most favorable situation occurs when one monofunctional datapath proves sufficient because all streams are to be processed in exactly the same way. In our example we then have f ≡ g ≡ h from which max f ,g,h (A) = Af = Ag = Ah and max f ,g,h (t) = tf = tg = th follow immediately. Apart from the overhead for control and data routing, AT(s) equals the lower bound s(Af + Areg)(tf + treg) which is identical to the isomorphic architecture with its s separate computational units. It is in this best case exclusively that time sharing leaves the size-time product unchanged and may, therefore, be viewed as antithetic to replication. The contrary condition occurs when f , g and h are very dissimilar so that little or no savings can be obtained from concentrating their processing into one multifunctional datapath. Time sharing will then just lower throughput by a factor of s thereby rendering it an unattractive option. Provided speed requirements are sufficiently low, a radical solution is to combine time sharing with iterative decomposition and to adopt a processor-style architecture as already mentioned in subsection 3.4.2. The energy situation is very similar. If the processing functions are all alike and if the computation rate is kept the same, then the energy spent for processing actual data also remains much the same.37 Extra energy is then spent only for controlling the datapath and for collecting and redistributing data items. More energy is going to get dissipated in a combined datapath when the functions markedly differ from each other. As time sharing has no beneficial impact on glitching activity either, we conclude that such an architecture necessarily dissipates more energy than a comparable non-time-shared one. By processing s data streams with a single computational unit, time sharing deliberately refrains from taking advantage of the parallelism inherent in the original problem. This is of little importance as long as performance goals are met with a given technology. When in search of more performance, however, a time-shared datapath will have to run at a much higher speed to rival the s concurrent units of the isomorphic architecture which implies that data propagation along the longest path must be substantially accelerated. Most measures suitable to do so, such as higher supply voltage, generous transistor sizing, usage of high-speed cells and devices, adoption of faster but also more complex adder and multiplier schemes, etc., tend to augment the amount of energy spent for the processing of one data item even further. Example The Fast Fourier Transform (FFT) is a rather expensive combinational function, see fig.3.27. Luckily, due to its regularity, the FFT lends itself extremely well to various reorganizations that help reduce hardware size. A first iterative decomposition step cuts the FFT into log2(n) rounds where n stands for the number of points. When an in-place computation scheme is adopted, those rounds become identical 37 Consider (3.43) and note that the equation simplifies to sEf + Ereg + Ectl when f , g and h are the same. The partial sum sEf + Ereg then becomes almost identical to s(Ef + Ereg) of the reference architecture. The apparent saving of (s − 1)Ereg obtained from doing with a single register does not materialize in practice because of the necessity to store data items from all streams. 3.4 EQUIVALENCE TRANSFORMS FOR COMBINATIONAL COMPUTATIONS 113 except for their coefficient values.38 Each round so obtained consists of n 2 parallel computations referred to as butterflies because of their structure, see fig.3.27b. The idea of sharing one or two computational units between the butterfly operations of the same round is very obvious at this point.39 0x (k) 1x (k) 2x (k) 3x (k) 4x (k) 5x (k) 7x (k) 6x (k) 0y (k) 1y (k) 2y (k) 3y (k) 4y (k) 5y (k) 7y (k) 6y (k) (a) 1st round 2nd round 3rd round (b)= butterfly FIGURE 3.27 DDG of 8-point FFT (a) and of butterﬂy operator (b) (reduced for simplicity). DDGs as regular as this offer ample room for devising a range of architectures that represent diverse compromises between a single-ALU microprocessor and a hardwired data pipeline maximized for throughput. Providing a limited degree of scalability to accommodate FFTs of various sizes, is not overly difficult either. Favorable conditions similar to these are found in many more applications including, among others, transversal filters (repetitive multiply-accumulate operations), correlators (idem), lattice filters (identical stages), and block ciphers (cascaded rounds). \u0002 38 For a number of computational problems, it is a logical choice to have two memories that work in a ping-pong fashion. At any moment of time, one memory provides the datapath with input data while the other accommodates the partial results presently being computed. After the evaluation of one round is completed, their roles are swapped. As simple as it is, this approach unfortunately requires twice as much memory than needed to store one set of intermediate data. A more efficient technique is in-place computation where part of the input data are immediately overwritten by the freshly computed values. In-place computation may cause data items to get scrambled in memory, though, which necessitates corrective action. Problems amenable to in-place computation combined with memory unscrambling include the FFT and the Viterbi algorithm. 39 Alternative butterfly circuits and architectures have been evaluated in [54] with emphasis on energy efficiency. As opposed to the above, [55] discusses systolic radix-4 high-performance FFT architectures. Also, in fig.3.27, we have assumed input samples to be available as eight concurrent data streams. FFT processors often have to interface to one continuous word-serial stream, however. Architectures that optimize hardware utilization for this situation are being discussed in [56]. 114 CHAPTER 3 FROM ALGORITHMS TO ARCHITECTURES So far, we have come up with four equivalence transforms, namely • Iterative decomposition, • Pipelining, • Replication, and • Time sharing. Fig.3.28 puts them into perspective in a grossly simplified but also very telling way. More comments will follow in section 3.4.8. towards throughput hardware towards efficiency hardware towards economy pipelining replication time sharing decom- position iterative constant AT ideal effect actual effect 27 3 9 1 1/3 1/9 311/31/9 isomorphic configuration assumed t =lp A= L= cpd=1 1/9 27 1 t =lp A= L= cpd=3 1/3 9 1/3 t =lp A= L= cpd=1 1/3 9 1 t =lp A= L= cpd=9 1 3 1/9 t =lp A= L= cpd=3 1 3 1/3 t =lp A= L= cpd=1 1 3 1 t =lp A= L= cpd=9 3 1 1/9 t =lp A= L= cpd=3 3 1 1/3 t =lp A= L= cpd=3 3 1 1 t =lp A= L= cpd=9 9 1/3 1/3t =lp A= L= cpd=27 9 1/3 1/9 t =lp A= L= cpd=27 27 1/9 1/9 time-shared datapath single fine-grain pipelined datapath highly iterative and time-shared T = tlp time per data item size A t =lp A= L= cpd=81 27 1/9 1/27 iterative and mid-grain pipelined time-shared datapath multiple mid-grained pipelines concurrent processing units multiple replicated multiple coarse-grain pipelines with each stage iteratively decomposed Γ FIGURE 3.28 A roadmap illustrating the four universal transforms for tailoring combinational hardware. Only a subset of all possible architectural conﬁgurations is shown, see problem 6. Greatly simpliﬁed by • abstracting from register overhead (Areg = 0, treg = 0), which also implies • not making any difference between RAMs and ﬂip-ﬂops (ARAM = Aff · #bits, tRAM = tff ), • assuming ideal iterative decomposition and ideal time sharing (lower bounds in (3.17)and (3.38)), and by • ignoring any overhead associated with control and/or data distribution and collection. 116 CHAPTER 3 FROM ALGORITHMS TO ARCHITECTURES 3.4.6 ASSOCIATIVITY TRANSFORM All four architectural transforms discussed so far have one thing in common. Whether and how to apply them for maximum benefit can be decided from a DDG’s connectivity and weights alone, no matter what operations the vertices stand for. In the sequel, we will call any architectural reorganization that exhibits this property a universal transform. The practical consequence is that any computational flow qualifies for reorganization by way of universal transforms. This also implies that any two computations the DDGs of which are isomorphic can be solved by the same architecture just with the vertices interpreted differently. More on the negative side, universal transforms have a limited impact on the flow of computation because the number and precedence of operations are left unchanged.40 As many computational problems ask for more specific and more profound forms of reorganization in order to take full advantage of the situation at hand, one cannot expect to get optimum results from universal transforms alone. Rather, one needs to bring in knowledge on the particular functions involved and on their algebraic properties. Architectural reorganizations that do so are referred to as operation-specific or algebraic transforms. Probably the most valuable algebraic property from an architectural point of view is the associative law. Associativity can be capitalized on to ◦ Convert a chain structure into a tree or vice versa, see example below, ◦ Reorder operations such as to accommodate input data that arrive later than others do, ◦ Reverse the order of execution in a chain as demonstrated in section 3.6,or to ◦ Relocate operations from within a recursive loop to outside the loop, see section 3.7. This explains why the associativity transform is also known as operator reordering and as chain/tree conversion. Example Consider the problem of finding the minimum among I input values y(k) = min(xi(k)) where i = 0, 1, ..., (I − 1) (3.44) Assuming the availability of 2-way minimum operators, this immediately suggests a chain structure such as the one depicted in fig. 3.29afor I = 8. The delay along the longest path is (I − 1)tmin and increases linearly with the number of terms. As the 2-way minimum function is associative, the DDG lends itself to being rearranged into a balanced tree as shown in fig.3.29b. The longest path is thereby shortened from I − 1to ⌈log2I¬ operations which makes the tree a much better choice, especially for large values of I. The number of operations and the circuit’s size remain the same. \u0002 40 While it is true that the number of DDG vertices may change, this is merely a consequence of viewing the original operations at a different level of detail. 3.4 EQUIVALENCE TRANSFORMS FOR COMBINATIONAL COMPUTATIONS 117 The conversion of a chain of operations into a tree, as in the above example, is specifically referred to as tree-height minimization. As a side effect, this architectural transform often has a welcome impact on energy efficiency. This is because glitches die out more rapidly and are more likely to neutralize when all data propagation paths are of similar lengths. In addition, we observe the same indirect benefit as with pipelining, in that a shortened longest path makes it possible to use a slower yet more energy-efficient circuit style or a reduced supply voltage if circumstances permit. (a) 0th term x(k) y(k) minmin min min minminmin I th term (b) x(k) y(k) min min min min minmin min 0th termI th term chain/tree conversion FIGURE 3.29 8-way minimum function. Chain-type DDG (a), tree-type DDG (b). 3.4.7 OTHER ALGEBRAIC TRANSFORMS It goes without saying that many more algebraic laws can be put to use for improving dedicated architectures. Distributivity helps to replace the computation of (a2 − 2ax + x2) by the more economic form of (a − x)2, for instance, and is instrumental in exploiting known symmetries in coefficient sets of correlators, (matched) filters, and the like. Together with commutativity, distributivity is also at the heart of distributed arithmetic to be introduced in subsection 3.8.3. Horner’s scheme serves to evaluate polynomials with a minimum number of multiplications while the method of finite differences can calculate any number of equidistant values with no multiplication at all. Karatsuba multiplication reduces the effort for very wide data words, the principle of superposition holds in linear systems, De Morgan’s theorem helps in optimizing Boolean networks, and so on. See problem 5 for yet another operation-specific alteration. As a rule, always ask yourself what situation-specific properties might be capitalized on. The transforms discussed in this text just represent the more common ones and are by no means exhaustive. 118 CHAPTER 3 FROM ALGORITHMS TO ARCHITECTURES 3.4.8 DIGEST • Iterative decomposition, pipelining, replication, and time sharing are based on the DDG as a graph and make no assumptions on the nature of computations carried out in its vertices, which is why they are qualified as universal. Associativity transform, in contrast, is said to be an algebraic transform because it depends on the operations involved being identical and associative. • Iterative decomposition, pipelining, replication, and a variety of algebraic transforms make it possible to tailor combinational computations on a single data stream to given size and performance constraints without affecting functionality. Time sharing is another option in the presence of inherently parallel data streams and operations. • For iterative decomposition to be effective, complex functions must be amenable to being broken into similar subfunctions such as to make it possible to reuse the same circuitry. Much the same reasoning also holds for time sharing in that parallel functions must not be too diverse to share a single functional unit in a fairly efficient way. • Pipelining is generally more efficient than replication, see fig.3.25. While coarse grain pipelining improves throughput dramatically, benefits decline as more and more stages are included. When in search of utmost performance, begin by designing a pipeline the depth of which yields close-to-optimum efficiency. Only then — if throughput is still insufficient — consider replicating the pipelined functional unit a few times, see problem 2 for an example. This approach also makes sense in view of design productivity because duplicating a moderately pipelined unit may be easier and quicker than pushing pipelining to the extreme. • A theoretical upper bound on throughput, expressed as data items per time unit, that holds for any circuit technology and architecture is \u0004 ≤ 1 min(tgate)+treg .41 • Pipelining and iterative decomposition are complementary in that they both can contribute to lowering the size-time product AT. While the former acts to improve performance, the latter cuts circuit size by sharing resources. Combining them indeed makes sense, within certain bounds, in order to obtain a high throughput from a small circuit. • Starting from the isomorphic configuration, a great variety of architectures is obtained from applying equivalence transforms in different order. Combining several of them is typical for VLSI architecture design. Figure 3.28 gives an idealized overview of the design space spanned up by the four universal transforms.42 Which configuration is best in practice cannot be decided without fairly detailed knowledge of the application at hand, of the performance requirements, and of the target cell library and technology. 41 Further improvements are possible only by processing larger data chunks at a time i.e. by packing more bits, pixels, samples, characters, or whatever into one data item. Note this is tantamount to opting for a larger w in the sense of footnote 21. 42 As a more philosophical remark, observe from fig.3.28 that there exists no single transform that leads towards optimum hardware efficiency. To move in that direction, designers always have to combine two or more transforms much as a yachtsman must tack back and forth to progress windward with his sailboat. 3.4 EQUIVALENCE TRANSFORMS FOR COMBINATIONAL COMPUTATIONS 119 • Program-controlled microprocessors follow an architectural concept that pushes iterative decomposition and time sharing to the extreme and that combines them with pipelining, and often with replication too. Developers of general-purpose hardware cannot take advantage of algebraic transforms as their application requires detailed knowledge about the data processing algorithm. • It can be observed from fig.3.28 that lowering the size-time product AT always implies cutting down the longest path tlp in the circuit. This comes with no surprise as better hardware efficiency can only be obtained from keeping most hardware busy for most of the time by means of a higher computation rate fcp. • Important power savings are obtained from operating CMOS logic with a supply voltage below its nominal value. Clever architecture design must compensate for the loss of speed that is due to the extended gate delays. Suggestions are not only given throughout this chapter, but also in the forthcoming material on energy efficiency. 120 CHAPTER 3 FROM ALGORITHMS TO ARCHITECTURES 3.5 OPTIONS FOR TEMPORARY STORAGE OF DATA Except for trivial SSI/MSI circuits, any IC includes some form of memory. If the original data processing algorithm is of sequential nature and, therefore, mandates the temporary storage of data, we speak of functional memory. If storage gets introduced into a circuit as a consequence from architectural transformations, the memory is sometimes said to be of nonfunctional nature. The major options for temporary storage of data are as follows: ◦ On-chip registers built from individual bistables (flip-flops or latches), ◦ On-chip memory (SRAM macrocell or — possibly — embedded DRAM), or ◦ Off-chip memory (SRAM or DRAM catalog part).43 There are important differences from an implementation point of view that matter from an architectural perspective and that impact high-level design decisions. 3.5.1 DATA ACCESS PATTERNS Standard single-port RAMs provide access to data words one after the other.44 This is fine in sequential architectures as obtained from iterative decomposition and time sharing that process data in a step-by- step fashion. Program-controlled microprocessors with their “fetch, load, execute, store” processing paradigm are a perfect match for RAMs. High-throughput architectures obtained from pipelining, retiming and loop unfolding,45 in contrast, keep data moving in every computation cycle which mandates the usage of registers as only those allow for simultaneous access to all of the data words stored. Incidentally, also keep in mind that the contents of DRAMs needs to be periodically refreshed which dissipates electrical power even when no data accesses take place. 3.5.2 AVAILABLE MEMORY CONFIGURATIONS AND AREA OCCUPATION Next compare how much die area gets occupied by registers and by on-chip RAMs. While register files allow for any conceivable memory configuration in increments of one data word of depth and one data bit of width, their area efficiency is rather poor. In the occurrence of fig.3.30, registers occupy an order of magnitude more area than a single-port SRAM for capacities in excess of 5000 bits. This is because registers get assembled from individual flip-flops or latches with no sharing of hardware resources. Due to their simple and extremely compact bit cells, RAMs make much better use of area in spite of the shared auxiliary subcircuits they must accommodate. In a typical commodity DRAM, for instance, roughly 60% of the die is occupied by storage cells, the rest by address decoders, switches, precharge 43 Please refer to section A.4 if totally unfamiliar with semiconductor memories. 44 Dual-port RAMs can access two data words at a time, multi-port RAMs are rather uncommon. 45 Retiming and loop unfolding are to be explained in sections 3.6 and 3.7 respectively. 3.5 OPTIONS FOR TEMPORARY STORAGE OF DATA 121 circuitry, sense amplifiers, internal sequencers, I/O buffers, and padframe. Yet, such circuit overheads tend to make RAMs less attractive for storing smaller data quantities which is also evident from fig.3.30. A more serious limitation is that macrocells are available in a very limited number of configurations only. Adding or dropping an address bit alters memory capacity by a factor of two, and fractional cores are not always supported. Always keep in mind that such effects have been ignored in the cost and performance analyses carried out in sections 3.4.2 through 3.4.5 where Areg had been assumed to be fixed with no distinction between registers and RAMs. More specifically, this also applies to fig.3.28. 1000 10000 100000 1000000 10000000 100 1000 10000 100000 1000000 storage capacity [bit]area occupation [um^2]register file, 8bit wordwidth register file, 16bit wordwidth register file, 32bit wordwidth register file, 64bit wordwidth on-chip SRAM, 8bit wordwidth on-chip SRAM, 16bit wordwidth on-chip SRAM, 32bit wordwidth on-chip SRAM, 64bit wordwidth FIGURE 3.30 Area occupation of registers and on-chip RAMs for a 130 nm CMOS technology. 3.5.3 STORAGE CAPACITIES Embedded memories cannot rival the copious storage capacities offered by commodity RAMs. The densest memory chips available are DRAMs built from one-transistor cells whereas the macrocells intended for being embedded within ASICs typically get assembled from six-transistor SRAM storage cells, see table 3.9.46 46 DRAMs further take advantage of three-dimensional trench capacitors and other area-saving structures made possible by dedicated fabrication steps and process options unavailable to macrocells that are to be compatible with a baseline CMOS process. Also, processes for commodity memories are often ahead of ASIC processes in terms of feature size. Finally, the layout of memory chips is highly optimized by hand. 122 CHAPTER 3 FROM ALGORITHMS TO ARCHITECTURES The economic disparity between on-chip memories and cheap commodity DRAMs, that are cost- optimized, fabricated in large quantities, and — all too often — subject to ruinous competition has long discouraged the on-chip storage of very large amounts of data within ASICs. The design of true “systems on a chip” requires this gap being closed. In fact, some integrated VLSI manufacturers are capable of embedding high-density DRAMs within their designs, but the approach is slow to catch on with the ASIC community and, more particularly, with the providers of ASIC design kits. Examples Embedded DRAMs occupy a large part of the market for 3D graphics accelerator chips for laptops because higher performance and lower power dissipation are key value propositions [57]. The so-called “Emotion Engine” superscalar multimedia processor chip designed and produced by Toshiba for Sony’s PlayStation 2 is another popular example. \u0002 3.5.4 WIRING AND THE COSTS OF GOING OFF-CHIP On the negative side, off-chip memories add to pin count, package count, and board space. Communi- cating with them involves a profusion of parasitic capacitances and delays that cause major bottlenecks in view of operating speed, performance, and energy efficiency. In addition, most commodity RAMs feature bidirectional data pins in an effort to keep costs and pin counts as low as possible. They so impose the adoption of a bidirectional bus on any IC that is going to exchange data with them. Yet, note that bidirectional on-chip busses and even more so bidirectional pads require special attention during circuit design and test. • Not only stationary but even transient drive conflicts must be avoided because of the strong drivers and important currents involved. • Automated test equipment (ATE) must be made to alternate between read and write modes with no physical access to any control signal within the chip. • Test patterns must be prepared for verifying bidirectional operation and high-impedance states during circuit simulation and testing. • Electrical and timing measurements become more complicated. 3.5.5 LATENCY AND TIMING RAM-type memories further differ from registers in terms of latency, paging, and timing. Firstly, some RAMs have latency while others do not. In a read operation, we speak of latency zero if the content of a memory location becomes available at the RAM’s data output in the very clock cycle during which its address has been applied to the RAM’s address port. This is also the behavior of a register bank. As opposed to this, we have a latency of one if the data word does not appear before an active clock edge has been applied. Latency is even longer for memories that operate in a pipelined manner internally. Latency may have a serious impact on architecture design and certainly affects HDL coding.47 47 A trick that may help to conceal latency is to operate the RAM’s memory address register on the opposite clock edge than the rest of the circuit. Note that this introduces extra timing conditions with respect to the supposedly inactive clock edge that do not exist in a single-edge triggered circuit, however. 3.5 OPTIONS FOR TEMPORARY STORAGE OF DATA 123 Secondly, commodity DRAMs have their row and column addresses multiplexed over the same pins to cut down package size and board-level wiring. Latency then depends on whether a memory location shares the row address with the one accessed before, in which case the two are said to sit on the same page, or not. Paged memories obviously affect architecture design. Thirdly, address decoding, precharging, the driving of long word and bit lines, and other internal suboperations inflate the access times of both SRAMs and DRAMs. RAMs thus impose a comparatively slow clock that encompasses many gate delays per computation period whereas registers are compatible with much higher clock frequencies.48 3.5.6 DIGEST Most on-chip RAMs available for ASIC design ± are of static nature (SRAMs), + have their views obtained automatically using a macrocell generator, − offer a limited choice of configurations (in terms of #words and wdata), + occupy much less area than flip-flops, − but do so only above some minimum storage capacity, + greatly improve speed and energy efficiency over off-chip RAMs, − but cannot compete in terms of capacity, − restrict data access to one read or write per cycle, − impose rigid constraints on timing, minimum clock period, and latency. 48 It is sometimes possible to multiply maximum RAM access rate by recurring to memory interleaving, a technique whereby a small number of memories operate in a circular fashion such as to emulate a single storage array of shorter cycle time. 124 CHAPTER 3 FROM ALGORITHMS TO ARCHITECTURES Table 3.9 Key characteristics of register-based and RAM-based data storage compared. architectural option on- chi p off-chip bistables embedded commodity ﬂip-ﬂop latch SRAM DRAM DRAM fabrication process compatible with logic optimized devices in each cell 20...30T 12...16T 6T 1T1C 1T1C cell area per bit [ F 2]a 1700...2800 1100...1800 135...170 18...30b 6...8 extra circuit overhead none 1.3 ≤ factor ≤ 2 off-chip memory refresh cycles none ye s extra package pins none none addr. & data bus nature of wiring multitude of local lines on-chip busses package & board bidirectional busses none optional mandatory access to data words all at a time one at a time available conﬁgurations any restricted energy efficiency goodc fair poor very poor latency and paging none no ﬁxed rules yes impact on clock period minor substantial severe a Area of one bit cell in multiples of F 2 where F 2 stands for the area of one lithographic square. bAs low as 6...8 for processes that accomodate 3D capacitors at the price of 4 to 6 extra masks [58]. c Depending on the register access scheme, conditional clocking may or may not be an option. Examples Cu-11 is the name of an ASIC technology by IBM that has a drawn gate length — and hence also a minimum feature size — of 110 nm and that operates at 1.2 V. The process combines copper interconnect with low-k interlevel dielectric materials. As part of the Cu-11 design library, IBM offers an SRAM macrocell generator for memories ranging from 128 bit to 1 Mibit as well as embedded DRAM megacells of trench capacitor type with up to 16 Mibit. A 1 Mibit eDRAM has a cycle time of 15 ns which is equivalent to 555 times the nominal delay of a 2-input nand gate. eDRAM bit cell area is 0.31 μm2 which corresponds to 25.6F2. A 1 Mibit eDRAM occupies an area of 2.09 mm2 (with an overhead factor 1.84) and its 16 Mibit counterpart 14.1 mm2 (overhead factor 1.63). Actel’s ProASICPLUS flash-based FPGA family makes embedded SRAMs available in chunks of 256x9 bit. The APA1000 part includes 88 such blocks which corresponds to 198 Kibit of embedded RAM if fully used. \u0002 Flash memories have not been addressed here as they do not qualify for short-time random-access storage. This is primarily because data must be erased in larger chunks before it becomes possible to rewrite individual words. The comparatively low speed and limited endurance are other limitations that make flash more suitable for longer-term storage applications such as retaining FPL configurations as explained in section 2.2. 3.5 OPTIONS FOR TEMPORARY STORAGE OF DATA 125 Just for comparison, the physical bit cell area of flash technology is a mere 4 to 12F2 and, hence, comparable to DRAM rather than SRAM. What’s more, by using four voltage levels instead of two, two bits can be stored per flash cell bringing down the minimum area to just 2F2 per bit. Endurance is on the order of 100 000 write&erase cycles for flash cells that hold one bit (two states) and of 10 000 cycles for those that hold two bits (four states). Still higher numbers are made possible by wear-leveling schemes implemented in special memory controllers. Observation 3.8. Only registers allow for simultaneous access to all data but occupy a lot of die area per bit. SRAMs serve to temporarily hold more significant quantities of data with access times that are slower than registers but faster than DRAMs. DRAM and Flash memories are cost-efficient for large data quantities. Flash is used for permanent storage, speed is much lower than with RAMs, though. Off-chip commodity memories offer virtually unlimited capacities at low costs, but are is associated with speed, energy and other penalties. As further details of the various memory technologies are of little importance here, the reader is referred to the literature [59] [60] [61] [62]. An excellent introduction to flash memory technology is given in [63], [64] elaborates on improvements towards high-density storage and high-speed programming, while [65] and [66] focus on the role of nand flash for mass storage and on the system and packaging aspects of removable media respectively. 126 CHAPTER 3 FROM ALGORITHMS TO ARCHITECTURES 3.6 EQUIVALENCE TRANSFORMS FOR NON-RECURSIVE COMPUTATIONS Unlike combinational computations, the outcome of sequential computations depends not only on present but also on past values of its arguments. Architectures for sequential computations must therefore include memory. In the DDG this gets reflected by the presence of edges with weights greater than zero. However, as non-recursiveness implies the absence of feedback, the DDG remains free of circular paths. The storage capacity required by the isomorphic architecture is referred to as memory bound because no other configuration exists that could do with less.49 Table 3.9 allows for approximate translation from memory bits to chip area. 3.6.1 RETIMING The presence of registers in a circuit suggests a new type of reorganization known as retiming or as register balancing, whereby registers get relocated such as to allow for a higher computation rate without affecting functionality [67] [68]. The goal is to equalize computational delays between any two registers, thereby shortening the longest path that bounds the computation period from below. Referring to a DDG one must therefore know “In what way is it possible to modify edge weights without altering the original functionality?” (a) g wf wg wh f h i wf wg wh gf h i +l + − l l lag l datapath section no control section retiming nonuniform computational delays uniform(b) FIGURE 3.31 Retiming. DDG (a) and a hardware conﬁguration for l = 1(b). Let us follow an intuitive approach to find an answer.50 Consider a DDG and pick a vertex, say h in fig.3.31, for instance. Now suppose the operation of vertex h is made to lag behind those of all others by adding latency to every edge pointing towards that vertex, and by removing the same amount of latency from every edge pointing away from that vertex. Conversely, any vertex could be made to lead the others by transferring latency from its incoming to its outgoing edges. Since any modifications made to the incoming edges are compensated for at the outgoing edges, nothing changes when the DDG is viewed from outside. The retimed DDG is, therefore, functionally equivalent to the initial one. As it is always possible (a) to think of an entire subgraph as one supervertex, and (b) to repeat the operation with changing vertices and supervertices, the idea paves the way for significant hardware reorganizations. 49 For combinational computations, the memory bound is obviously zero. 50 A more formal treatise is given in [69]. 3.6 EQUIVALENCE TRANSFORMS FOR NON-RECURSIVE COMPUTATIONS 127 Not all DDGs obtained in this way are legal, though, because the general requirements for DDGs stated in subsection 3.3.4 impose a number of restrictions on edge weights or — which is the same — on latency registers. Any legal retiming must observe the rules below. 1. Neither data sinks (i.e. outputs) nor sources of time-varying data (i.e. variable inputs) may be part of a supervertex that is to be retimed. Sources of constant data (i.e. fixed inputs) do not change in any way when subject to retiming. 2. When a vertex or a supervertex is assigned a lag (lead) by l computation cycles, the weights of all its incoming edges are incremented (decremented) by l and the weights of all its outgoing edges are decremented (incremented) by l. 3. No edge weight may be changed to assume a negative value. 4. Any circular path must always include at least one edge of strictly positive weight.51 Interestingly, rule 4 does not need to be taken into consideration explicitly — provided it was satisfied by the initial DDG — because the weight along a circular path will never change when rule 2 is observed. The proof is trivial. As a direct consequence of rule 1, retiming does not affect latency. Retiming necessitates neither control logic nor extra data storage facilities but may alter the total number of registers in a circuit depending on the structure of the DDG being subject to the transformation.52 Energywise, it is difficult to anticipate the overall impact of retiming as switching activities, fanouts, and node capacitances all get altered. Yet, much as for pipelining, the reduced long path delay either allows for compensating a lower supply voltage or can be invested in doing with a slower but more energy-efficient circuit style or technology. The fact that retiming does not normally burden a circuit with much overhead renders it particularly attractive. 3.6.2 PIPELINING REVISITED Recall from section 3.4.3 that pipelining introduces extra registers into a circuit and necessarily increases its latency, which contrasts with what we have found for retiming. Pipelining can in fact be interpreted as a transformation of edge-weights that differs from retiming in that rule 1 is turned into its opposite, namely 1. Any supervertex to be assigned a lag (lead) must include all data sinks (all time-varying data sources). What retiming and pipelining have in common is that they allow one to operate a circuit at a higher computation rate. Most high-speed architectures therefore combine the two. 51 Although irrelevant here due to the absence of circular paths, this stipulation does apply in the context of recursive computations. 52 For many applications it is important that a sequential circuit assumes its operation from a well-defined start state. As a rule, initial state does matter for controllers but is often irrelevant for datapath circuitry. If so, a mechanism must be provided for forcing the circuit into that state. Finding the start state for a retimed circuit is not always obvious. The problem of systematically computing the start state for retimed circuits is addressed in [70]. 128 CHAPTER 3 FROM ALGORITHMS TO ARCHITECTURES Example Consider the problem of computing y(k) = N∑ n=0 h(cn, x(k − n)) (3.45) i.e. a time-invariant N-th order correlation where h(c, x) stands for some unspecified — possibly nonlinear — function. Think of h(c, x) as some distance metric between two DNA fragments c and x,for instance, in which case y(k) stands for the overall dissimilarity between the DNA strings (c0, c1, ..., cN) and (x(k), x(k − 1), ..., x(k − N)) of length N. x(k) y(k) h 0c 1c 2c 3c h h h lead 1 lead 2 lead 3 y(k-1)x(k) h 0c 1c 2c 3c h h h lag 1 x(k) y(k) h 0c 1c 2c 3c h h h x(k) y(k) h 0c 1c 2c 3c h h h chain reversal pipelining retiming (c) (d) (a) (b) FIGURE 3.32 Nonlinear time-invariant third order correlator. Original DDG (a), with adder chain reversed by associativity transform (b), after retiming (c), with pipelining added on top such as to obtain a systolic architecture (d). 3.6 EQUIVALENCE TRANSFORMS FOR NON-RECURSIVE COMPUTATIONS 129 The DDG of a third order correlator is shown in fig.3.32 together with its stepwise reorganization. For the sake of concreteness, let us assume that a register delay is treg = 0.5ns, that computing one distance metric takes th = 3ns, and that adding up two items costs t+ = 2ns. (a) Original DDG as obtained from straight interpretation of (3.45). The delay along the longest path is stated in the table below, note that is grows with correlation order N. There is no retiming that would relocate the existing registers in a useful way. Although the configuration is amenable to pipelining, reformulating it first will eventually pay off. (b) Same as (a) with the adder chain reversed by an associativity transform. Maximum delay and register count remain unchanged, but the computation has now become suitable for retiming. Also refer to problem 3. (c) Functional registers transferred into the adder chain by retiming of (b). The three vertices and supervertices enclosed by dashed lines have been assigned leads of 1, 2, and 3 computation cycles respectively. Long path delay is substantially reduced with no registers added. Also notice that the maximum operating speed is no longer a function of correlation order N. (d) Retiming complemented by pipelining. The supervertex enclosed by a dashed line, which includes the data sink, has been assigned a lag of 1. The longest path is further shortened at the price of extra registers and of an increased latency. Observe that it is not possible to improve performance any further unless one is willing to intervene into the adders on a lower level of circuit detail, also see section 3.8.1. Architectural variant Key characteristics (a) (b) (c) (d) arithmetic units (N +1) Ah + NA+ idem idem idem functional registers NAreg idem idem idem nonfunctional registers 0 idem idem (N +1) Areg cycles per data item 1 idem idem idem longest path delay tlp treg + th + Nt+ idem treg + th + t+ treg +max( th ,t+ ) for N =3 [ns] 9.5 idem 5.5 3.5 for N =30 [ns] 63.5 idem 5.5 3.5 latency L 0 idem idem 1 Γ \u0002 Make sure you understand there is a fundamental difference between the architectural transforms used in the above example. While retiming and pipelining are universal transforms that do not depend on the operations involved, changing the order of execution in the algebraic transform that leads from (a) to (b) insists in the operations concerned being identical and associative. The practical significance is that the sequence of reorganizations that has served to optimize the nonlinear correlator example also applies to transversal filters which are of linear nature, for instance, but not to DDGs of similar structure where addition is replaced by some non-associative operation. 130 CHAPTER 3 FROM ALGORITHMS TO ARCHITECTURES 3.6.3 SYSTOLIC CONVERSION Both pipelining and retiming aim at increasing computation rate by reducing and by equalizing register- to-register delays. For a given granularity, maximum speed is obtained when there is no more than one combinational operation between any two registers. This is the basic idea behind systolic computation. A DDG is termed systolic if the edge weight between any two vertices is one or more. The architecture depicted in fig.3.32d is in fact systolic, and the ultimate pipeline in the sense of (3.29)is now also recognized as a circuit that is systolic at the gate level. Please refer to [71] for a more comprehensive discussion of systolic computation and to [72] for details on systolic conversion, that is on how to turn an arbitrary non-recursive computation into a systolic one. 3.6.4 ITERATIVE DECOMPOSITION AND TIME SHARING REVISITED Applying the ideas of decomposition and time sharing to sequential computations is straightforward. Clearly, only combinational circuitry can be multiplexed whereas functional memory requirements remain the same as in the isomorphic architecture. Example In the isomorphic architecture of a transversal filter, see fig.3.17d, each filter tap is being processed by its own multiplier. All calculations associated with one sample are thus carried out in parallel and completed within a single computation cycle. Nevertheless, the architecture is slow because the longest path traverses the entire adder chain thereby mandating a long computation period. Also, hardware costs are immoderate, at least for higher filter orders N. A more economic alternative that handles one filter tap after the other follows naturally, see fig.3.33. This architecture manages with a single multiplier that gets time-shared between taps. A single adder iteratively sums up the partial products until all taps that belong to one sample have been processed after N + 1 computation cycles. An accumulator stores the intermediate sums. Coefficients may either be kept in a hardwired lookup table (LUT), in a ROM, or in some sort of writable memory. Datapath control is fairly simple. An index register that counts n = N, N − 1, ..., 0 addresses one coefficient at any time. The very same register also selects the samples from the input shift register, either by way of a multiplexer, of a three-state bus, or by arranging the shift register in circular fashion and by having data there make a full cycle between any two subsequent samples. An output register maintains the end result while computation proceeds with the next sample. For filters of higher order, one would want to substitute a RAM for the input shift register. While this requires some extra circuitry for addressing, it does not fundamentally change the overall architecture. 3.6 EQUIVALENCE TRANSFORMS FOR NON-RECURSIVE COMPUTATIONS 131 b21b x(k) b3 * * ** y(k)+++ 0b multiplier parallel LUT adder accumulator * + x(k) y(k) nb n x(k-1) x(k-2) x(k-3) datapath section control section multiplexer to 1N +1 coefficient storage wordsN +1 +1 mod N +1 long shift register N =3 output register index register datapath section no control section iterative decomposition & time sharing parallel multipliersN +1 addersN long shift register N =3 hard-wired coefficients (a) (b) FIGURE 3.33 Third order transversal ﬁlter. Isomorphic architecture (a) and a more economic alternative obtained by combining time sharing with iterative decomposition (b) (simpliﬁed). \u0002 3.6.5 REPLICATION REVISITED The concept of replication introduced in section 3.4.4 cannot normally be applied to sequential computations as the processing of one data item is dependent on previous data items. A notable exception exists in the form of so-called multipath filters (aka N-path filters) designed to implement sequential computations of linear time-invariant nature. With H1(z) denoting the transfer function of a single path, all of which are identical, and with q as replication factor, a composite transfer function H(z) = H1(z q) (3.46) is obtained [73], which implies that the elemental transfer function H1(z) is compressed and repeated along the frequency axis by a scaling factor of q. Due to the resulting extra passbands the usefulness of this approach is very limited. An extended structure capable of implementing general FIR and IIR53 transfer functions is proposed in [73] under the name of delayed multipath structures. Regrettably, the number of necessary operations is found to grow with q2. 53 FIR stands for finite impulse response, IIR for infinite impulse response. 132 CHAPTER 3 FROM ALGORITHMS TO ARCHITECTURES 3.6.6 DIGEST • The throughput of arbitrary feedforward computations can be improved by way of retiming, by way of pipelining, and by combining the two. Replication is, in general, not a viable alternative. • The associative law is often helpful in rearranging a DDG prior to iterative decomposition, pipelining, and especially retiming in order to take full advantage of these transforms. • Much as for combinational computations, iterative decomposition and time sharing are the two options available for reducing circuit size for feedforward computations. • Highly time-multiplexed architectures are found to dissipate energy on a multitude of ancillary activities that do not directly contribute to data computation, however. 3.7 EQUIVALENCE TRANSFORMS FOR RECURSIVE COMPUTATIONS 133 3.7 EQUIVALENCE TRANSFORMS FOR RECURSIVE COMPUTATIONS A computation is said to be timewise recursive — or recursive for short — if it somehow depends on an earlier outcome from that very computation itself, a circumstance that gets reflected in the DDG by the presence of a feedback loop. Yet, recall that circular paths of weight zero have been disallowed to exclude the risk of race conditions. Put differently, circular paths do exist but each of them includes at least one latency register. This section examines equivalence transforms that apply specifically to recursive computations. We will find that such transforms are not universal. This is why we address linear time-invariant, linear time-variant, and nonlinear computations separately. 3.7.1 THE FEEDBACK BOTTLENECK Consider a linear time-invariant first-order recursive function y(k) = ay(k − 1) + x(k) (3.47) which, in the z domain, corresponds to transfer function H(z) = Y(z) X(z) = 1 1 − az−1 (3.48) The corresponding DDG is shown in fig.3.34a. Many examples for this and similar types of computa- tions are found in IIR filters, DPCM54 data encoders, servo loops, etc. a x(k) y(k) y(k-1) (a)*+ adder multiplier parallela x(k) y(k) (b) FIGURE 3.34 Linear time-invariant ﬁrst-order feedback loop. DDG (a) and isomorphic architecture with longest path highlighted (b). Recursion demands that the result from the actual computation cycle be available no later than the next input sample. The longest path defined by all computations that are part of the loop must, therefore, not exceed one sampling interval. In the occurrence ∑ loop t = treg + t∗ + t+ = tlp ≤ Tcp (3.49) 54 DPCM is an acronym for differential pulse code modulation. 134 CHAPTER 3 FROM ALGORITHMS TO ARCHITECTURES As long as this iteration bound is satisfied by the isomorphic architecture of fig.3.34b implemented using some available and affordable technology, there is no out-of-the-ordinary design problem. As an example, we could easily provide a sustained computation rate of 100 MHz if the three delay figures for the actual word width were of 0.5 ns, 5 ns, and 2 ns respectively. When in search of some higher throughput, say 200 MHz, recursiveness becomes a real bottleneck since there is no obvious way to make use of replication or to insert pipeline registers without altering the overall transfer function and input-to-output mapping. Retiming does not help either as the weight along a circular path is always preserved. So the problem is “How to allow more time for those computations that are part of the recursion loop?” 3.7.2 UNFOLDING OF FIRST-ORDER LOOPS The key idea is to relax the timing constraint by inserting additional latency registers into the feedback loop while preserving the original transfer function. In other words, a tentative solution for a first-order loop must look like H(z) = Y(z) X(z) = N(z) 1 − apz−p (3.50) where an unknown expression N(z) is here to compensate for the changes that are due to the new denominator 1 − apz−p. Recalling the sum of geometric series we easily establish N(z) as N(z) = 1 − apz−p 1 − az−1 = p−1∑ n=0 a nz −n (3.51) The new transfer function can then be completed to become H(z) = ∑p−1 n=0 anz−n 1 − apz−p (3.52) and the new recursion in the time domain follows as y(k) = a py(k − p) + p−1∑ n=0 a nx(k − n) (3.53) The modified equations correspond to a cascade of two sections. A first section, represented by the numerator of (3.52), is a DDG that includes feedforward branches only. This section is amenable to pipelining as discussed in section 3.4.3. The denominator stands for the second section, a simple feedback loop which has been widened to include p unit delays rather than one as in (3.47). Using retiming, the corresponding latency registers can be redistributed into the combinational logic for computing the loop operations such as to serve as pipeline registers there. Neglecting, for a moment, the limitations to pipelining found in section 3.4.3, throughput can in fact be multiplied by an arbitrary positive integer p through this unfolding technique, several variations of which are discussed in [74]. 3.7 EQUIVALENCE TRANSFORMS FOR RECURSIVE COMPUTATIONS 135 Unless p is prime, it is further possible to simplify the DDG — and hence the implementing circuit — by factoring the numerator into a product of simpler terms. Particularly elegant and efficient solutions exist when p is an integer power of 2 because of the lemma p−1∑ n=0 a nz −n = log2 p −1∏ m=0 (a 2m z −2m + 1) p = 2, 4, 8, 16, ... (3.54) The feedforward section can then be realized by cascading log2 p subsections each of which consists of just one multiplication and one addition. Example The linear time-invariant first-order recursive function (3.47) takes on the following form after unfolding by a factor of p = 4 y(k) = a 4y(k − 4) + a 3x(k − 3) + a 2x(k − 2) + ax(k − 1) + x(k) (3.55) which corresponds to transfer function H(z) = 1 + az−1 + a2z−2 + a3z−3 1 − a4z−4 (3.56) Making use of (3.54) the numerator can be factorized into such as to obtain H(z) = (1 + az−1)(1 + a2z−2) 1 − a4z−4 (3.57) The DDG corresponding to this equation is shown in figure 3.35a. Note that the configuration is not only simple but also highly regular. Further improvements are obtained from pipelining in conjunction with retiming and shimming where necessary. The final architecture, shown in fig.3.35b, is equivalent except for latency. Incidentally, also note that threefold instantiation of one pipelined multiply-add building block favors further optimizations at lower levels of detail. 136 CHAPTER 3 FROM ALGORITHMS TO ARCHITECTURES a y(k)x(k) a 2 a 4 numerator denominator(a) x(k) a a 2 building block pipelined multiply-add*+ adder multiplier parallel y(k-6) a 4 (b)*+*+ FIGURE 3.35 Linear time-invariant ﬁrst-order feedback loop. DDG after unfolding by a factor of p = 4 (a) and high-performance architecture with pipelining and retiming on top (b). \u0002 Performance and cost analysis In the occurrence of optimally efficient configurations, where p is an integer power of 2, a lower bound for total circuit size can be given as follows A(p) ≥ (log2 p + 1)Af + p(log2 p + 1)Areg (3.58) In the above example, we count three times the original arithmetic logic plus 14 extra (nonfunctional) registers. In return for an almost fourfold throughput, this is finally not too bad. Analogously to what was found for pipelining in section 3.4.3, the speedup of loop unfolding tends to diminish while the difficulties of balancing delays within the combinational logic tend to grow when unfolding is pushed too far, p ≫ 1. A hidden cost factor associated with loop unfolding is due to finite precision arithmetics. For the sake of economy, datapaths are designed to do with minimum acceptable word widths, which implies that output and intermediate results get rounded or truncated somewhere in the process. In the above example, for instance, addition would typically handle only part of the bits that emanate from multiplication. Now, the larger number of roundoff operations that participate in the unfolded loop with respect to the initial configuration leads to more quantization errors, a handicap which must be offset by using somewhat wider data words [75]. Loop unfolding greatly inflates the amount of energy dissipated in the processing of one data item because of the extra feedforward computations and the many latency registers added to the unfolded 3.7 EQUIVALENCE TRANSFORMS FOR RECURSIVE COMPUTATIONS 137 circuitry. More on the positive side, the shortened longest path may bring many recursive computations into the reach of a relatively slow but power-efficient technology or may allow for a lower supply voltage. The idea of loop unfolding demonstrated on a linear time-invariant first-order recursion can be extended into various directions, and this is the subject of the forthcoming three subsections. 3.7.3 HIGHER-ORDER LOOPS a x(k) y(k) y(k-1) y(k-2) b (a)+**+ a b x(k) y(k) (b) FIGURE 3.36 Linear time-invariant second-order feedback loop. DDG (a) and isomorphic architecture with longest paths highlighted (b). Instead of unfolding loops of arbitrary order directly, we make use of a common technique from digital filter design that consists in factoring a higher-order transfer function into a product of second- and first-order terms. The resulting DDG takes the form of cascaded second- and first-order sections. High- speed IIR filters of arbitrary order can be constructed by pipelining the cascade so obtained. As an added benefit, cascade structures are known to be less sensitive to quantization of coefficients and signals than direct forms. We will, therefore, limit the discussion to second-order recursive functions here y(k) = ay(k − 1) + by(k − 2) + x(k) (3.59) which correspond to the DDG depicted in fig.3.36. The equivalent in the z domain is H(z) = Y(z) X(z) = 1 1 − az−1 − bz−2 (3.60) After multiplying numerator and denominator by a factor of (1 + az−1 − bz−2) the transfer function becomes H(z) = 1 + az−1 − bz−2 1 − (a2 + 2b)z−2 + b2z−4 (3.61) which matches the requirements for loop unfolding by a factor of p = 2. Analogously we obtain for p = 4 H(z) = (1 + az−1 − bz−2)(1 + (a2 + 2b)z−2 + b2z−4) 1 − ((a2 + 2b)2 − 2b2)z−4 + b4z−8 (3.62) 138 CHAPTER 3 FROM ALGORITHMS TO ARCHITECTURES Example b 2 +2ba 2+*+* b 2 - b 4 a - b y(k)x(k) +2ba 222+2b)(a -2b2 numerator denominator (a) a - b x(k)+*+* (b) building block pipelined multiply-add - b 4 22+2b)(a -2b2 y(k-6)+*+* FIGURE 3.37 Linear time-invariant second-order feedback loop. DDG after unfolding by a factor of p = 4 (a) and high- performance architecture with pipelining and retiming on top (b). Figure 3.37 shows a DDG and a block diagram that implement the second order recursion (3.62). Except for finite precision effects, the transfer function remains exactly the same as in (3.59), but the arithmetic operations inside the loop can now be carried out in four rather than one computation period. The same pipelined hardware block is instantiated three times. A high-speed fourth-order ARMA55 filter chip that includes two sections similar to fig.3.37bhas been reported back in 1992 [76]. Pipelined multiply-add units have been designed as combinations of consecutive carry-save and carry-ripple adders. The circuit, fabricated in a 0.9 μm CMOS technology, has been measured to run at a clock frequency of 85 MHz and spits out one sample per clock cycle, so \u0003 = 1. Overall computation rate roughly is 1.5 Gop/s.56 The authors write that one to two extra data bits had to be added in the unfolded datapath to maintain similar roundoff and quantization characteristics as in the initial configuration. Circuit size is approximately 20 kGE. At full speed the chip dissipates 55 ARMA is an acronym for “auto recursive moving average” used to characterize IIR filters that comprise both recursive (AR) and non-recursive computations (MA). 56 Multiply-add operations, in the occurrence, taking into account all of the filter’s AR and MA sections. 3.7 EQUIVALENCE TRANSFORMS FOR RECURSIVE COMPUTATIONS 139 2.2 W from a 5 V supply. While performance and power data are obsolete today, loop unfolding allows to push out the need for fast but costly fabrication technologies such as GaAs, then and now. \u0002 Performance and cost analysis When compared to the first-order case, the number of pipeline registers per subsection is doubled while the other figures remain unchanged. Hence, size estimation yields A(p) ≥ (log2 p + 1)Af + (2p(log2 p + 1))Areg (3.63) 3.7.4 TIME-VARIANT LOOPS Here, the feedback coefficient a is no longer constant but varies as a function of time a(k) y(k) = a(k)y(k − 1) + x(k) (3.64) The unfolded recursions are derived in the time domain. Substituting y(k − 1) in (3.64) yields y(k) = a(k)a(k − 1)y(k − 2) + a(k)x(k − 1) + x(k) (3.65) which computes y(k) from y(k − 2) directly, so the unfolding factor is p = 2. Repeating this operation leads to a configuration with p = 3where y(k) = a(k)a(k − 1)a(k − 2)y(k − 3) + a(k)a(k − 1)x(k − 2) + a(k)x(k − 1) + x(k) (3.66) and once more to p = 4 y(k) = a(k)a(k − 1)a(k − 2)a(k − 3)y(k − 4)+ + a(k)a(k − 1)a(k − 2)x(k − 3) + a(k)a(k − 1)x(k − 2) + a(k)x(k − 1) + x(k) (3.67) As for the time-invariant case, the process of unfolding can be continued to widen the recursive loop by an arbitrary positive integer p as expressed by y(k) = ( p−1∏ n=0 a(k − n)) · y(k − p) + ⎛ ⎝p−1∑ n=1( n−1∏ m=0 a(k − m)) · x(k − n) ⎞ ⎠ + x(k) (3.68) However, because precomputation is not applicable here, all necessary coefficient terms must be calculated on-line, which requires extra hardware. Depending on how the terms of (3.68) are combined, various DDGs can be obtained. One of them derived from (3.67) is depicted in fig.3.38. 140 CHAPTER 3 FROM ALGORITHMS TO ARCHITECTURES x(k) y(k) a(k) coefficient calculation output computation FIGURE 3.38 Linear time-variant ﬁrst-order feedback loop. DDG after unfolding by a factor of p = 4. Performance and cost analysis The count of adders and multipliers is proportional to the number of subsections p. Each subsection requires approximately 2p pipeline registers as both multipliers must be pipelined. Together with shimming registers, many of which are needed in this configuration due to the numerous parallel branches, roughly 4p2 registers are needed. 3.7.5 NONLINEAR OR GENERAL LOOPS A nonlinear difference equation implies that the principle of superposition does not hold. The most general case of a first-order recursion is described by y(k) = f (y(k − 1), x(k)) (3.69) and can be unfolded an arbitrary number of times. For simplicity we will limit our discussion to a single unfolding step, i.e. to p = 2where y(k) = f (f (y(k − 2), x(k − 1)), x(k)) (3.70) The associated DDG of fig.3.39c shows that loop unfolding per se does not relax the original timing constraint, the only difference is that one can afford two cycles for two operations f instead of one cycle for one operation. As confirmed by fig.3.39d, there is no room for any meaningful retiming in this case. Yet, the unfolded recursion can serve as a starting point for more useful reorganizations. Assume function f is known to be associative. Following an associativity transform the DDG is redrawn as shown in fig.3.39e. The computation so becomes amenable to pipelining and retiming, see fig.3.39f, which cuts the longest path in half when compared to the original architecture of fig.3.39b. Even more speedup can be obtained from higher unfolding degrees, the price to pay is multiplied circuit size and extra latency, though. In summary, architecture, performance, and cost figures resemble those found for linear computations. 3.7 EQUIVALENCE TRANSFORMS FOR RECURSIVE COMPUTATIONS 141 y(k-1) (a) f y(k)x(k) (b) f y(k) x(k) y(k-4) f fx(k) y(k) (c) f fx(k) y(k) (d) (e) x(k) f y(k)f (f) x(k) x(k) y(k) (g) f \" y(k-2) x(k) (h) 12 f is associative provided retiming operator reordering loop unfolding aggregation f \" feedforward feedback f f f \" FIGURE 3.39 Architectural alternatives for nonlinear time-variant ﬁrst-order feedback loops. Original DDG (a) and isomorphic architecture (b), DDG after unfolding by a factor of p = 2 (c), same DDG with retiming added on top (d). DDG reorganized for an associative function f (e), pertaining architecture after pipelining and retiming (f), DDG with the two functional blocks for f combined into f \" (g), pertaining architecture after pipelining and retiming (h). 142 CHAPTER 3 FROM ALGORITHMS TO ARCHITECTURES The situation is definitely more difficult when f is not associative. Still, it is occasionally possible to relax the loop constraint to some extent by playing a trick. Reconsider fig.3.39c and think of the two occurrences of f being combined into an aggregate computation y(k) = f \"(y(k − 2), x(k − 1), x(k)) (3.71) as sketched in fig.3.39g. If that aggregate computation can be made to require less than twice as much time as the original computation, then the bottleneck gets somewhat alleviated. This is because it should then be possible to insert a pipeline register into the datapath unit for f \" so that the maximum path length in either of the two stages becomes shorter than the longest delay in a datapath that computes f alone. tlp f \" = max(tlp f \"1 , tlp f \"2 )< tlp f (3.72) More methods for speeding up general time-variant first-order feedback loops are examined in [77]. One technique, referred to as expansion or look-ahead, is closely related to aggregate computation. The idea is to process two or more samples in each recursive step so that an integer multiple of the sampling interval becomes available for carrying out the necessary computations. In other terms, the recursive computation is carried out at a lower pace but on wider data words. This approach should be considered when the combinational logic is not amenable to pipelining, for example because it is implemented as table lookup in a ROM. The limiting factor is that the size of the lookup table (LUT) tends to increase dramatically. Yet another approach, termed concurrent block technique, groups the incoming data stream into blocks of several samples and makes the processing of these blocks independent from each other. While data processing within the blocks remains sequential, it so becomes possible to process the different blocks concurrently. The unified algebraic transformation approach promoted in [78] combines both universal and algebraic transforms to make the longest path independent of problem size in computations such as recursive filtering, recursive least squares algorithm, and singular value decomposition. Any of the various architectural transforms that helps to successfully introduce a higher degree of parallel processing into recursive computations takes advantage of algorithmic properties such as linearity, associativity, fixed coefficients, limited word width, or of a small set of register states. If none of these applies, we can’t help but agree with the authors of [77]. Observation 3.9. When the state size is large and the recurrence is not a closed-form function of specific classes, our methods for generating a high degree of concurrency cannot be applied. Example Cryptology provides us with a vivid example for the implications of nonlinear nonanalytical feedback loops. Consider a block cipher that works in electronic code book (ECB) mode as depicted in fig.3.41a. The algorithm implements a combinational function y(k) = c(x(k), u(k)) where u(k) denotes the key and k the block number or time index. However complex function c, there is no fundamental obstacle to pipelining or to replication in the datapath. Unfortunately, ECB is cryptologically weak as two identical blocks of plaintext result in two identical blocks of ciphertext because y(k) = y(m) if x(k) = x(m) and u(k) = u(m). If a plaintext to be encrypted contains sufficient repetition, the ciphertext necessarily carries and betrays patterns from the original plaintext. Fig.3.40 nicely illustrates this phenomenon. 3.7 EQUIVALENCE TRANSFORMS FOR RECURSIVE COMPUTATIONS 143 (a) (b) (c) FIGURE 3.40 A computer graphics image in clear text, ciphered in ECB mode, and ciphered in CBC-1 mode (from left to right, Tux by Larry Ewing). To prevent this from happening, block ciphers are typically used with feedback. In cipher block chaining (CBC) mode, the ciphertext gets added to the plaintext before encryption takes place, see fig.3.41b. The improved cipher algorithm so becomes y(k) = c( x(k) ⊕ y(k − 1), u(k)) and is sometimes referred to as CBC-1 mode because y(k − 1) is being used for feedback. From an architectural point of view, however, this first-order recursion is awkward because it offers little room for reorganizing the computation. This is particularly true in ciphering applications where the nonlinear functions involved are chosen to be complex, labyrinthine, and certainly not analytical. The fact that word width (block size) is on the order of 64 or 128 bit makes everything worse. Inserting pipeline registers into the computational unit for c does not help since this would alter algorithm and ciphertext. Throughput in CBC mode is thus limited to a fraction of what is obtained in ECB mode.57 10 =c en/deciphering memoryless mapping = modulo 2 addition bitwise cx(k) y(k) u(k) (a) cryptographic improvement (b) x(k) y(k)c u(k) = (c) c )k(y)k(x u(k) 10 FIGURE 3.41 DDGs for three block ciphering modes. Combinational operation in ECB mode (a) vs. time-variant nonlinear feedback loop in CBC mode (b), and CBC-8 operation b (c). \u0002 57 Higher data rates must then be bought by measures on lower levels of abstraction, that is by optimizing the circuitry at the arithmetic/logic level, by recurring to transistor-level design in conjunction with full-custom layout, and/or by adopting a faster target technology, all of which measures ask for extra effort and come at extra cost. Only later have cryptologists come up with a better option known as counter mode (CTR) that manages without feedback and still avoids the leakage of plaintext into ciphertext that plagues ECB. 144 CHAPTER 3 FROM ALGORITHMS TO ARCHITECTURES 3.7.6 PIPELINE INTERLEAVING, NOT QUITE AN EQUIVALENCE TRANSFORM It has repeatedly been noted in this section that any attempt to insert an extra register into a feedback loop with the idea of pipelining the datapath destroys the equivalence between original and pipelined computations unless its effect is somehow compensated. After all, circuits c and b of fig.3.41 behave differently. Although this may appear a futile question, let us ask “What happens if we do just that to a first-order recursion?” Adding an extra latency register to (3.69) results in the DDG of fig.3.42a and yields y(k) = f (y(k − 2), x(k)) (3.73) Observe that all indices are even in this equation. As k increments with time k = 0, 1, 2, 3, ... indices do in fact alternate between even and odd values. It thus becomes possible to restate the ensuing input- to-output mapping as two separate recursions with no interaction between “even” data items x(k = 0, 2, 4, ..., 2n, ...) and “odd” items x(k = 1, 3, 5, ..., 2n + 1, ...). y(2n) = f (y(2n − 2), x(2n)) (3.74) y(2n + 1) = f (y(2n − 1), x(2n + 1)) (3.75) (c) f y(k=2n-2) y(k=2n)x(k=2n) f y(k=2n-1) y(k=2n+1)x(k=2n+1) & f y(k-2) y(k)x(k) (a)= (d) f x(0) x(2) x(...) x(4) x(1) x(3) x(...) x(5) y(0) y(2) y(...) y(4) y(1) y(3) y(...) y(5) f y(k-2) x(k) (b) pipeline interleaving FIGURE 3.42 Pipeline interleaving. DDG of nonlinear time-variant ﬁrst-order feedback loop with one extra register inserted (a) and isomorphic architecture (b). Interpretation as two interleaved data streams each of which gets processed exactly as speciﬁed by the original nonlinear ﬁrst-order recursion of ﬁg.3.39a(c,d). This pair of equations says that the original data processing recursion of (3.69) now gets applied to two separate data streams as depicted in fig.3.42c. From a more general perspective, it is indeed possible to cut the combinational delay in any first-order feedback loop down to 1 p by inserting p − 1 pipelining 3.7 EQUIVALENCE TRANSFORMS FOR RECURSIVE COMPUTATIONS 145 registers, yet the computation then falls apart into the processing of p interleaved but otherwise independent data streams. More often than not this is undesirable. However, practical applications exist where it is possible to take advantage of this effect. Examples Cipher block chaining (CBC) implements the recursion y(k) = c( x(k) ⊕ y(k − 1), u(k)). What counts from a cryptographic point of view is that patterns from the plaintext do not show up in the ciphertext. Whether this is obtained from feeding back the immediately preceding block of ciphertext y(k − 1) (CBC-1 mode) or some prior block y(k − p) where 2 ≤ p ∈ N (CBC-p mode) is of minor importance. Some cryptochips, therefore, provide a fast but nonstandard CBC-8 mode in addition to the regular CBC-1 mode, see fig.3.41c. In the occurrence of the IDEA chip described in [79], maximum throughout is 176 Mbit/s both in pipelined ECB mode and in pipeline-interleaved CBC-8 mode as compared to just 22 Mbit/s in nonpipelined CBC-1. Fig.3.43 shows a high-level block diagram of a sphere decoder, a key subfunction in a MIMO OFDM (orthogonal frequency division multiplex) receiver. Sphere decoding is essentially a sophisticated tree- traversal algorithm that achieves close-to-minimum error rate performance at a lower average search complexity than an exhaustive search. Pipelining the computation in search of throughput is not an option because of the (nonlinear) first-order recursion. Instead, the facts that (a) OFDM operates on many subcarriers at a time (typically 48 to 108) and that (b) each such subcarrier poses an independent tree-search problem, make sphere decoding an ideal candidate for pipeline interleaving. This and many other refinements to sphere decoding have been reported in [80] from which fig. 3.44 has been taken.levelselect radius update register bank (functional) unit metric computation sphere constraint check unit metric enumeration shimming registers pipeline registers storage for intermediate search results triplicated register bank FIGURE 3.43 Sphere decoder. The dashed arrows point to the extra circuitry required to handle three individual subcarriers in an interleaved fashion (simpliﬁed). 146 CHAPTER 3 FROM ALGORITHMS TO ARCHITECTURES FIGURE 3.44 The beneﬁcial impact of pipeline interleaving on area and throughput of a sphere decoder circuit (diagram courtesy of Dr. Markus Wenk). For a much simpler example, consider some image processing algorithm where rows of pixels are dealt with independently from each other. Rather than scanning the image row by row, pixels from p successive rows are entered one by one in a cyclic manner before the process is repeated with the next column, and so on. All processing can so be carried out using a single pipelined datapath of p stages [81]. \u0002 Pipeline interleaving does obviously not qualify as an equivalence transform. Still, it yields useful architectures for any recursive computation — including nonlinear ones — provided that data items arrive as separate time-multiplexed streams that are to be processed independently from each other, or can be arranged to do so. From this perspective, pipeline interleaving is easily recognized as a clever and efficient combination of time sharing with pipelining. 3.7.7 DIGEST • When in search of high performance for recursive computations, reformulating a high-order system as a cascade of smaller-order sections in order to make the system amenable to coarse grain pipelining should be considered first. As a by-product, the reduced orders of the individual recursion loops offer additional speedup potential. • Throughput of low-order recursive computations can be significantly improved by loop unfolding in combination with fine grain pipelining. This may bring computations into the reach of static CMOS technology that would otherwise ask for faster but also more expensive alternatives such as SiGe, BiCMOS, or GaAs. 3.7 EQUIVALENCE TRANSFORMS FOR RECURSIVE COMPUTATIONS 147 • Whether the inflated latency is acceptable or not depends on the application. Also, the rapid growth of overall circuit size tends to limit economically practical unfolding degrees to fairly low values, say p = 2...8, especially when the system is a time-variant one. • The larger number of roundoff operations resulting from loop unfolding must be compensated for by using longer word widths, which increases the cost of loop unfolding beyond the sole proliferation of computational units and intermediate registers. • Nonlinear feedback loops are, in general, not amenable to throughput multiplication by applying unfolding techniques. A notable exception exists when the loop function is associative. • Pipeline interleaving is highly efficient for accelerating recursive computations because it does not depend on any specific properties of the operations involved. Yet, as it modifies the input-to-output mapping, it is not an option unless the application admits that multiple data streams undergo the same processing independently from each other. 148 CHAPTER 3 FROM ALGORITHMS TO ARCHITECTURES 3.8 GENERALIZATIONS OF THE TRANSFORM APPROACH 3.8.1 GENERALIZATION TO OTHER LEVELS OF DETAIL As stated in section 3.3.4, DDGs are not concerned with the granularity of operations and data. Recall, for instance, figs.3.20 and 3.41a that describe the same block cipher at different levels of detail. As a consequence, the techniques of iterative decomposition, pipelining, replication, time sharing, algebraic transform, retiming, loop unfolding, and pipeline interleaving are not limited to any specific level of abstraction although most examples so far have dealt with operations and data at the word level, see table 3.10. Table 3.10 An excerpt from the VLSI abstraction hierarchy. Level Granularity Relevant items of abstraction Operations Data Architecture subtasks, processes time series, pictures, blocks Word arithmetic/logic operations words, samples, pixels Bit gate-level operations individual bits Architecture level Things are pretty obvious at this higher level where granularity is coarse. As an example, fig.3.45 gives a schematic overview of a visual pattern recognition algorithm. Four subtasks cooperate in a cascade with no feedback, namely preprocessing, image segmentation, feature extraction, and object classification. (b) preprocessing segmentation feature extraction classification (a) preprocessing segmentation feature extraction classification pipelining (c) 2. segmentation 3. feature extraction 4. classification 1. preprocessing overall control versatile datapath iterative decomposition all studied at architecture level FIGURE 3.45 Overall architectural alternatives for a pattern recognition system. Isomorphic architecture (a), pipelined operation (b), and iteratively decomposed computation ﬂow (c). 3.8 GENERALIZATIONS OF THE TRANSFORM APPROACH 149 In a real-time application, one would definitely begin by introducing pipelining because four processing units are so made to work concurrently at negligible cost. In addition, each unit is so dedicated to a specific subtask and can be optimized accordingly. The option of replicating the entire datapath would most likely get discarded at this point because it cannot compete with pipelining in terms of hardware efficiency. Replication of selected functional units could become an interesting alternative in later transforms, however, when the actual performance bottlenecks are known. Iterative decomposition would only be considered if the desired throughput were so modest that it could be obtained from a single processing unit. Bit level Equivalence transformations can also be beneficial at low levels of abstraction. Take addition, for instance, which is an atomic operation when considered at the word level, see fig.3.46a. When viewed at the gate level, however, the same function appears as a composite operation that can be implemented from bit-level operations in many ways, the most simple of which is a ripple-carry adder shown in fig.3.46b. This detailed perspective clearly exposes the longest path and opens new opportunities for reorganizing the DDG that remain hidden from a word or higher level of abstraction. studied at word level studied at bit level (c) p=W shimming registers (a) W WW iterative decomposition pipelining (d) s=W (b) BSLBSM 4W= control section nonfunctional feedback loop shimming registers FIGURE 3.46 4-bit addition at the register transfer level (a), broken down into a ripple-carry adder (b) before being pipelined (c) or iteratively decomposed (d). A gate-level pipelined version makes it possible to operate the circuit at a computation rate many times higher than with word-level pipelining alone, see fig.3.46c. As this amounts to fine-grain pipelining, the price to pay in terms of circuit size is likely to be excessive, however. In the above example, better solutions are obtained from more sophisticated arithmetic schemes such as carry-save, carry-skip, 150 CHAPTER 3 FROM ALGORITHMS TO ARCHITECTURES carry-select, or carry-lookahead adders, possibly in combination with moderate pipelining. Incidentally, note that modern synthesis tools support automatic retiming of gate-level networks. Conversely, the structure shown in fig.3.46d follows when the W-bit addition is decomposed into one- bit suboperations. Computation starts with the LSB. A flip-flop withholds the carry-out bit for the next computation period where it serves as carry-in to the next more significant bit. Obviously, the flip-flop must be properly initialized in order to process the LSB and any potential carry input in a correct way. Although this entails some extra control overhead, substantial hardware savings may nevertheless result when the words being processed are sufficiently wide. 3.8.2 BIT-SERIAL ARCHITECTURES The idea underlying fig.3.46d gives rise to an entirely different family of architectures known as bit-serial computation [82]. While most datapaths work in a bit-parallel fashion in that word-level operations are executed one after the other with all bits of a data word being processed simultaneously, organizing computations the other way round is also possible. Here, the overall structure remains isomorphic with the DDG, but the various word-level operations are decomposed into steps that are carried out one bit after the other. Example A bit-serial implementation of a third order transversal filter is shown in fig.3.47,also see fig.3.17c for the DDG. The w-bit wide input samples x(k) enter serially with the LSB first whereas coefficients bn(k) must be applied in a parallel format. The circuit is operated at a computation rate s times higher than the sampling frequency. Evaluation proceeds from the LSB to the MSB with computation periods numbered w = s − 1, ..., 0. The first computation period ingests the LSB of the actual input sample x(k), evaluates the LSBs of all samples x(k), ..., x(k − 3), and sees the LSB of the result y(k) emerge at the output. The second period then handles the next significant bit and so on.58 Shifts and carry-overs from one bit to the next higher position are obtained from using latency registers. As these registers may contain carries from previous additions after all W bits of the input have been processed, extra computation periods are required to bring out the MSB of y(k),sothat s > W. Note that iterative decomposition has led to nonfunctional feedback loops in the architecture of a transversal filter, although the DDG is free of circular paths by definition. As this kind of feedback is confined to within the multiply and add units, the filter as a whole remains amenable to pipelining provided computations inside the loops are not affected. 58 DSP applications frequently deal with input data scaled such that |xk| < 1 and coded with a total of W bits in 2’s-complement format, see (3.77). In this particular case, computation periods w = s − 1, ..., 1 process the input bits of weight 2−w respectively, while the last computation period with w = 0 is in charge of the sign bit. 3.8 GENERALIZATIONS OF THE TRANSFORM APPROACH 151 scalar (single wire) vector (parallel bus) adder full multiplier 1-by- -bitw -bit adderw y(k,w) b21b b3 s x(k,w) s s b0 studied at bit level + LSB + LSB + LSB + LSB c s + c s + c s + broken down to bit level & iterative decomposition FIGURE 3.47 Bit-serial implementation of a third order transversal ﬁlter functionally equivalent to the bit-parallel architecture of ﬁg.3.33 (simpliﬁed). \u0002 Closer examination of bit-serial architectures reveals the following properties: + Control overhead is small when compared to bit-parallel alternatives because the isomorphism of DDG and architecture is maintained. − As the DDG is hardwired into the datapath with no explicit control instance, changes to the processing algorithm, switching between different modes of operation, and exception handling are awkward to deal with. + The shallow combinational depth in conjunction with a high computation rates helps to keep all computational units busy for most of the time. + All non-local data communication is via serial links that operate close to the maximum rate supported by the target technology which cuts down on-chip wiring requirements. + Much of the data circulation is local which contrasts favorably with the data items travelling back and forth between datapath and storage in bit-parallel architectures. + As FPL devices provide only limited wiring resources, the two previous assets tend to favor bit-serial architectures when designing for FPGAs or CPLDs. − The word width must be the same throughout a serial architecture. Parts of the computation that do not make use of the full precision typically cause the hardware to idle for a number of computation periods. + Conversely, arbitrary precision can be accommodated with the same hardware when execution time is allowed to grow with word length. 152 CHAPTER 3 FROM ALGORITHMS TO ARCHITECTURES − Bit-serial computation is incompatible with the storing of data in word-oriented RAMs and ROMs. Extra format conversion circuitry is required whenever such memories are preferred for their high storage densities. − Division, data-dependent decisions, and many other functions are ill-suited to bitwise iterative decomposition and pipelining. While it is possible to incorporate bit-parallel functions, provided their interfaces are serialized and that they exhibit a fixed latency, the resulting hybrid structures often prove unsatisfactory and inefficient. − Algorithms based on successive approximation naturally operate with the MSB first and the same applies to picking the maximum or minimum value from a set of numbers. It is sometimes possible to reconcile LSB-first and MSB-first operations at the price of recurring to redundant number representation schemes. In summary, bit-serial architectures are at their best for unvaried real-time computations that involve fixed and elementary operations such as addition and multiplication by a constant. The reader is referred to the specialized literature [82] [83] for case studies and for further information on bit-serial design techniques. 3.8.3 DISTRIBUTED ARITHMETIC Bit-serial architectures have been obtained from breaking down costly word-level operations into bit- level manipulations followed by universal transforms such as iterative decomposition and pipelining. Another family of serial architectures results from making use of algebraic transforms at the bit level too. Consider the calculation of the following inner product y = K−1∑ k=0 ckxk (3.76) where each ck is a fixed coefficient and where each xk stands for an input variable. Fig.3.48a shows the architecture that follows from routinely applying decomposition at the word level. Computation works by way of repeated multiply-accumulate operations, takes K computation periods per inner product, and essentially requires a hardware multiplier plus a look up-table for the coefficients. Now assume that the inputs are scaled such that |xk| < 1 and coded with a total of W bits in 2’s- complement format.59 We then have xk =−xk,0 + W−1∑ w=1 xk,w 2 −w (3.77) 59 This is by no means a necessity. We simply assume |xk| < 1 for sake of convenience and 2’s-complement format because it is the most common representation scheme for signed numbers in digital signal processing. 3.8 GENERALIZATIONS OF THE TRANSFORM APPROACH 153 with xk,0 denoting the sign bit and with xk,w standing for the bit of weight 2−w in the input word xk.By combining (3.76)and (3.77) the desired output y can be expressed as y = K−1∑ k=0 ck(−xk,0 + W−1∑ w=1 xk,w 2 −w) (3.78) With the aid of the distributive law and the commutative law of addition, the computation now gets reorganized into the equivalent form below where the order of summation is reversed. y = K−1∑ k=0 ck(−xk,0) + W−1∑ w=1( K−1∑ k=0 ckxk,w) 2 −w (3.79) The pivotal observation refers to the term in parentheses K−1∑ k=0 ckxk,w = p(w) (3.80) For any given bit position w, calculating the sum of products takes one bit from each of the K data words xk which implies that the result p(w) can take on no more than 2K distinct values. Now, as coefficients ck have been assumed to be constants, all those values can be precomputed and kept in a lookup table (LUT) instead of being calculated from scratch whenever a new data set xk arrives at the input. A ROM is typically used to store the table. It must be programmed such as to return the partial product p(w) when presented with the address w, i.e. with a vector obtained from concatenating all bits of weight 2−w across all input variables xk. Playing this trick, and noting that ∑K−1 k=0 ck(−xk,0) is nothing else than p(0) with the sign reversed, (3.79) takes on the utterly simple form y =−p(0) + W−1∑ w=1 p(w) 2 −w (3.81) While the isomorphic architecture calls for W LUTs with identical contents, a much smaller architecture can be obtained from decomposing the evaluation of (3.81) into a series of W consecutive steps. The new architecture manages with a single lookup table but requires a nonfunctional register for accumulating the partial products, see fig.3.48b. Calculation proceeds one bit position at a time, starting with the LSB in computation period w = W − 1 and processing the sign bit in the final cycle where w = 0. A minor complication comes from the fact that the term −p(0) has a sign opposite to all other contributions to y. A simple solution consists of using an adder-subtractor working under control of a “sign-bit cycle” signal from a modulo W counter that acts as a controller. The same counter is also in charge of releasing the fully completed result and of clearing the accumulator at the end of the last computation period (two details not shown in fig.3.48b). In addition, it guides the selection of the appropriate input bits unless the xks can be made to arrive in a bit-serial format LSB first. 154 CHAPTER 3 FROM ALGORITHMS TO ARCHITECTURES multiplier parallel +1 mod K ROM ... adder accumulator y kx * + multiplexer to 1K K coefficient storage data words W motto: \"all bits from one word at a time\" index register output register ⎡log 2K⎤ (a) studied at word level ... kx ROM accumulator y +/-adder- subtractor -1 mod W bit position register multiplexer to 1W K partial product storage data words2 K w = 0 1/2 motto: \"one bit from each word at a time\" output register ⎡log 2W⎤ (b) studied at bit level broken down to bit level & algebraic transforms ≈ W ≈ W FIGURE 3.48 Architectures for computing a sum of products by way of repeated multiply-accumulate operations (a) and with distributed arithmetic (b) (simpliﬁed). The most striking difference between the two architectural alternatives of fig.3.48 is the absence of any multiplier in the second design. Rather than being concentrated in a single hardware unit, multiplication is spread over the circuit which is why such architectures were given the name distributed arithmetic. A limitation of distributed arithmetic is that memory size is proportional to 2K where K is the order of the inner product to be computed. Although a more sophisticated coding scheme makes it possible to introduce a symmetry into the lookup table which can then be exploited to halve its size [84], the required storage capacity continues to grow exponentially with K. More impressive memory savings are obtained from reformulating (3.80) in the following way K−1∑ k=0 ckxk,w = H−1∑ k=0 ckxk,w + K−1∑ k=H ckxk,w (3.82) where 0 < H < K. Instead of having all K bits address a single ROM, they are split into two subsets of H and K − H bits respectively, each of which drives its own LUT. The total storage requirement is so reduced from 2K data words to 2H + 2K−H which amounts to 2 K 2 +1 when input bits are split such that H = 1 2 K. The price to pay is an extra adder for combining the outputs from the two tables. Clearly, the idea can be extended to more than two tables. Memory requirements may sometimes be slashed further by taking advantage of properties of the coefficient values at hand such as symmetries, repetitions, and relations between their binary codes, also see problem 8. Memory size and circuit structure thereby become highly dependent on the particular coefficient values, though, which makes it difficult to accommodate modifications. In conclusion, distributed arithmetic should be considered when coefficients are fixed, when the number of distinct coefficient values is fairly small, and when lookup tables (LUT) are available at little cost 3.8 GENERALIZATIONS OF THE TRANSFORM APPROACH 155 compared to bit-parallel multipliers. This explains why this approach has recently regained popularity in the context of DSP applications with LUT-based FPGAs [85] [86]. Please refer to the literature for tutorials [84] and further VLSI circuit examples [87] if distributed arithmetic appears to be an option for your filtering problem. 3.8.4 GENERALIZATION TO OTHER ALGEBRAIC STRUCTURES So far we have mostly been dealing with the infinite field (R, +, · ) formed by the set of all real numbers together with addition and multiplication.60 Accordingly, most examples have been taken from digital signal processing where this type of computation is commonplace. Now, as all algebraic fields share a common set of axioms, any algebraic transform that is valid for some computation in (R, +, · )must necessarily hold for any other field.61 Finite ﬁelds Galois fields such as GF(2), GF(p), and GF(pn) have numerous applications in data compression (source coding), error correction (channel coding), and information security (ciphering). Thus, when designing high-speed telecommunications or computer equipment, it sometimes proves useful to know that the loop unfolding techniques discussed for linear systems in (R, +, · ) directly apply to any linear computation in any Galois or other finite field too. Semirings The analysis of recursive computations in section 3.7 has revealed that almost all successful and efficient loop unfolding techniques are tied to linear systems over a field. That computation be performed in a field is a sufficient but not a necessary condition, however, as will become clear shortly. Recall how loop unfolding was derived for the time-variant linear case in (3.64) through (3.68). Substituting the generic operator symbols ⊞ for + and ⊡ for · we can write y(k) = a(k) ⊡ y(k − 1) ⊞ x(k) (3.83) After the first unfolding step, i.e. for p = 2, one has y(k) = a(k) ⊡ a(k − 1) ⊡ y(k − 2) ⊞ a(k) ⊡ x(k − 1) ⊞ x(k) (3.84) and for arbitrary integer values of p ≥ 2 y(k) = ( p−1∏ n=0 a(k − n)) ⊡ y(k − p) ⊞ p−1∑ n=1( n−1∏ m=0 a(k − m)) ⊡ x(k − n) ⊞ x(k) (3.85) where ∑ and ∏ refer to operators ⊞ and ⊡ respectively. The algebraic axioms necessary for that derivation were closure under both operators, associativity of both operators, and the distributive law of ⊡ over ⊞. The existence of identity or inverse elements is not required. Also, we have never made use of commutativity of operator ⊡ which implies (a) that the result also holds for other than commutative 60 See appendix 3.11 for a summary on algebraic structures. 61 Universal transforms remain valid anyway as they do not depend on algebraic properties. 156 CHAPTER 3 FROM ALGORITHMS TO ARCHITECTURES operators ⊡, in which case (b) the above order of “multiplication” is indeed mandatory. The algebraic structure defined by these axioms is referred to as a semiring. The practical benefit is that recursive computations of seemingly nonlinear nature when formulated in the field (R, +, · ) — or in some other field — become amenable to loop unfolding, provided it is possible to restate them as linear computations in a ring or semiring [88]. A number of problems related to finding specific paths through graphs are amenable to reformulation in this way. Suitable algebraic systems that satisfy the axioms of a semiring are listed in [89] under the name of path algebras and in section 3.11. Example Convolutional codes find applications in telecommunication systems for error recovery when data get transmitted over noisy channels. While a convolutional (en)coder is simple, the computational effort for decoding at the receiver end is more substantial. The most popular decoding method is the Viterbi algorithm [90] [91], a particular case of dynamic programming62 for finding the shortest path through a trellis graph. The subsequent discussion is about the central step of fig.3.49. Figure 3.50 shows a couple of architectural options for an 8-state convolutional code. The trellis graph of fig.3.50a can be understood as an abstracted DDG where each node stands for an add-compare-select (ACS) operation. ACS operations occur in pairs of two and each pair is said to form a butterfly.63 Figure 3.50d specifically suggests a datapath that includes the necessary hardware to compute one set of path metrics from the previous set in a single clock cycle and to store the result in a bank of registers. This obvious solution represents a dramatic improvement over the highly inefficient and, hence, impractical isomorphic architecture of fig.3.50a. Yet, there is room for more beneficial reorganizations depending on application requirements. 62 Dynamic programming is an odd name — chosen by Richard Bellman for promotional reasons — for a broad class of optimization algorithms that decompose the search for a global optimum into a sequence of simpler decision problems at a local level. All decisions must obey Bellman’s principle of optimality which states that the globally optimum solution includes no suboptimal local decision. This is a very welcome property because it allows to prune inferior candidates early during the search process. Dynamic programming finds applications in fields as diverse as telecommunications, speech recognition, video coding, watermark detection, flight trajectory planning, and genome sequencing. For an anecdotal introduction, think of the following situation. During the darkness of night, a group of four has to cross a fragile suspension bridge that can carry no more than two persons at a time. The four persons take 5, 10, 20 and 25 minutes respectively for traversing the bridge. A torch must be carried while on the bridge, the torch available will last for exactly one hour. The problem is how to organize the operation. Draw a graph where each state of affair gets represented by a vertex and each traversal of the bridge by an edge. By solving this quiz in a systematic way, you are bound to discover the ideas behind dynamic programming yourself. The solution is available on the book’s companion website. 63 This is in fact not the only commonality with the FFT. Much as the computations there, the path metric update step in the Viterbi algorithm is amenable to in-place computation [92]. 3.8 GENERALIZATIONS OF THE TRANSFORM APPROACH 157 path metric update branch metric computation survivor path trace back path metric memory (functional) FIGURE 3.49 The three major steps of the Viterbi algorithm. Smaller circuit. Apply time sharing and then again iterative decomposition as suggested earlier in the context of the FFT (in section 3.4.5). This route essentially trades throughput for economy and ultimately leads to a processor-type datapath built around an ALU as portrayed in fig.3.50e. Reduced clock. The longest path in fig.3.50d includes no more than two adders, a multiplexer, a register, and the pertaining wiring. As the remainder of the circuit is likely to impose a much longer clock period, there will be a mismatch in many situations. Besides, the maximum clock rate may be undesirably high. The architecture suggested by fig.3.50c may then prove more adequate as it yields roughly the same throughput with half the clock. The more complex datapath — combinational logic gets approximately doubled — together with the inflated AT-product are the downsides of this modification. Still higher throughput. As a consequence from iterative decomposition, each butterfly participates in a recursive computation illustrated in fig.3.51a that goes y1(k) = min(a11(k) + y1(k − 1), a12(k) + y2(k − 1)) (3.86) y2(k) = min(a21(k) + y1(k − 1), a22(k) + y2(k − 1)) (3.87) As all other computations in the Viterbi algorithm are of feedforward nature, the maximum achievable throughput of the decoding process gets indeed limited by this nonlinear recursion. Now consider a semiring where • Set of elements: S = R ∪{∞}, • Algebraic addition: ⊞ = min, and • Algebraic multiplication: ⊡ =+. The new and linear — in the semiring — formulation of the ACS operation goes y1(k) = a11(k) ⊡ y1(k − 1) ⊞ a12(k) ⊡ y2(k − 1) (3.88) y2(k) = a21(k) ⊡ y1(k − 1) ⊞ a22(k) ⊡ y2(k − 1) (3.89) which, making use of vector and matrix notation, can be rewritten as ⃗y(k) = A(k) ⊡ ⃗y(k − 1) (3.90) By replacing ⃗y(k − 1) in (3.90) one gets the unfolded recursion for p = 2 ⃗y(k) = A(k) ⊡ A(k − 1) ⊡ ⃗y(k − 2) (3.91) 158 CHAPTER 3 FROM ALGORITHMS TO ARCHITECTURES butterfly== ACS ACS min min (b) (a) ...... ...... ...... state transitions with branch metricsstates with path metrics time slotk543210 k-1 iterative decomposition (c) (f) desirable location for extra registers (d) loop unfolding time sharing iterative decomposition inverse transform ALU iterative decomposition (e) FIGURE 3.50 Architectural options for the Viterbi algorithm. Abstracted trellis-type DDG for path metric computation (a) with details for one butterﬂy (b). Datapath architectures obtained from different degrees of iterative decomposition (c,d,e). Doomed attempt to boost throughput by inserting extra latency registers into the nonlinear ﬁrst-order feedback loop (f) (simpliﬁed). 3.8 GENERALIZATIONS OF THE TRANSFORM APPROACH 159 To take advantage of this unfolded form, the product B(k) = A(k) ⊡ A(k − 1) must be computed outside the loop. Resubstituting the original operators and scalar variables we finally obtain the recursion y1(k) = min(b11(k) + y1(k − 2), b12(k) + y2(k − 2)) (3.92) y2(k) = min(b21(k) + y1(k − 2), b22(k) + y2(k − 2)) (3.93) which includes the same number and types of operations as the original formulation but allows for twice as much time. The price to pay is the extra hardware required to perform the non-recursive computations below in a heavily pipelined way. b11(k) = min(a11(k) + a11(k − 1), a12(k) + a21(k − 1)) (3.94) b12(k) = min(a11(k) + a12(k − 1), a12(k) + a22(k − 1)) (3.95) b21(k) = min(a21(k) + a11(k − 1), a22(k) + a21(k − 1)) (3.96) b22(k) = min(a21(k) + a12(k − 1), a22(k) + a22(k − 1)) (3.97) \u0002 min 1y (k) 12a (k) a 11(k) (k-1)y 1 min (k)y 2 21a (k) 22a (k) (k-1)y 2 1y (k) 12b (k+1) b 11(k+1) (k-1)y 1 (k)y 2 21b (k+1) 22b (k+1) (k-1)y 2 min min 22b (k)+ (k-2)-y 2 21b (k)+ (k-2)y 1 12b (k)+ (k-2)y 2 11b (k)+ (k-2)y 1 1y (k) 12a (k) a 11(k) (k-1)y 1 (k)y 2 21a (k) 22a (k) (k-1)y 2 loop unfolding two nonlinear time-invariant first-order feedback loops two linear time-invariant first-order feedback loops reformulated over semiring loop unfolding (a) (c)(b) extra registers placed in loops FIGURE 3.51 The ﬁrst-order recursion of the Viterbi algorithm before (a) and after being reformulated over a semiring (b), and with loop unfolding added on top (c) (simpliﬁed, only one butterﬂy shown). The remarkable hardware structure so obtained demonstrates that taking advantage of specific proper- ties of an algorithm and of algebraic transforms has more potential to offer than universal transforms alone. Some computations can be accelerated by creating concurrencies that did not exist in the original formulation which opens a door to solutions that would otherwise have remained off-limits. 160 CHAPTER 3 FROM ALGORITHMS TO ARCHITECTURES 3.8.5 DIGEST • The transform approach to architecture design promoted in this text has been found to yield useful solutions at any level of granularity. Some of the resulting architectures are truly surprising. • Both bit-serial architectures and distributed arithmetic follow quite naturally when arithmetic/logic operations are dissected into bit-level manipulations before the various equivalence transforms are being applied. They are worthwhile to be considered when fixed and data-independent computations are to be carried out with limited hardware resources and moderate performance. After having sunk into oblivion for many years, the two techniques have had a comeback for filtering and other DSP applications with LUT-based FPGAs. • All universal and algebraic transforms that apply to computations on the field of reals also apply to computations on Galois fields, of course. • While loop unfolding is applicable to any linear computation in the field of reals, this is not a necessary condition. In case a recursion forms a bottleneck when in pursuit of higher performance, check whether it is possible to restate or modify the computations within the feedback loop such as to make them linear over a semiring. 3.9 CONCLUSIONS 3.9.1 SUMMARY We have begun this chapter by comparing instruction set processors with dedicated architectures. It was found that general-purpose computing asks for a high degree of flexibility that only program-controlled processors can provide. However, the ability to execute an arbitrary sequence of instructions on an unknown range of data types brings about numerous inefficiencies and largely precludes architectural optimizations. For well-defined computational tasks, much better performance and energy efficiency can be obtained from hardwired architectures with resources tailored to the specific computational needs of the target application. Segregation, weakly-programmable satellites, ASIPs, and configurable computing have been found to form useful compromises. Next, we have investigated a number of options for organizing datapath hardware. Our approach was based on reformulating a given data processing algorithm such as to preserve its input-to- output relationship except, possibly, for latency, while improving on performance, circuit size, energy efficiency, and the like. Findings on how to best rearrange combinational, non-recursive, and recursive computations were given in sections 3.4.8, 3.6.6,and 3.7.7 respectively. The approach was then generalized in terms of granularity and algebraic structure with the results summarized in section 3.8.5. The essences of these insights is collected in tables 3.11 and 3.12. 3.9 CONCLUSIONS 161 Table 3.11 Options available for reorganizing datapath architectures. Upper-case letters denote transforms generally available whereas lower-case letters indicate some preconditions must be satisfied by the application and/or type of computation to make this a viable option. Type of computation combinational sequential (aka memorizing) (aka memoryless) non-recursive recursive Data ﬂow feedforward feedforward feedback Memory no yes yes Data DAGa with DAG with Directed cyclic graph dependency all edge some or all edge with no circular path graph weights zero weights non-zero of weight zero Response length M =1 1 <M < M = Nature linear time-invariant D,P,Q,S,a D,P,q,S,a,R D,S,a,R,i,U of linear time-variant D,P,Q,S,a D,P,S,a,R D,S,a,R,i,U system nonlinear D,P,Q,S,a D,P,S,a,R D,S,a,R,i,u Discussed in section 3.4 3.6 3.7 D : Iterative decomposition P : Pipelining Q : Replication q : Multipath ﬁltering as special case of replication provided the resulting repetitive transfer function is acceptable S: Time sharing a : Associativity transform provided operations are identical and associative R : Retiming i : Pipeline interleaving, i.e. pipelining in conjunction with time sharing, provided a number of data streams can be processed separately from each other U : Loop unfolding u : Loop unfolding provided computation is linear over a semiring a DAG is an acronym for directed acyclic graph, i.e. for a directed graph with no circular path. 162 CHAPTER 3 FROM ALGORITHMS TO ARCHITECTURES Table 3.12 Summary of the most important architectural transforms and their characteristics. Architectural Decom- Pipe- Repli- Time Associa- Retiming Loop transform position lining cation sharing tivity unfolding Kind universal universal universal universal algebraic universal algebraic Applicable to combinational computations sequential computations Impact on nonrecurs. recursive A − ... = = ...+ + − ... = = = + Γ + = − + = = = tlp − − =, mux − = − ...+ − − T = Γ • tlp = − − + − ...+ − − AT − ... = − ... = = = ...+ − ...+ − + L + + =, mux + + = = + E − ...+ − ...+ = = ...+ − ...+ = + Extra recy. distrib., collect., extra hardware and none recoll., redist., none none word overhead cntl. and cntl. and cntl. width Helpful no coarse possibly no yes yes possibly for indirect grain yes yes energy saving yes Compatible any register register any any register register storage type Discussed in 3.4.2 3.4.3 3.4.4 3.4.5 3.4.6 3.6.1 3.7.2 subsection(s) 3.6.4 3.6.2 3.6.4 3.7.2 A : circuit size Γ : cycles per data item tlp : longest path length T : time per data item L : latency in computation periods E : energy per data item = : approximately constant + : tends to increase − : tends to decrease ... : in less favorable situations auxiliary circuitry for recy.: data recycling cntl.: datapath control dist.: data distribution coll.: data collection As energy efficiency depends on so many parameters, the pertaining entries of table 3.12 deserve further clarification. Assuming fixed energy costs per operation and ignoring any static currents, most architectural transforms discussed inflate the energy dissipated on a given calculation as conveyed by table entries E and “Extra hardware overhead”. Put in other words, cutting circuit size and boosting throughput typically are at the expense of energy efficiency. 3.9 CONCLUSIONS 163 The picture is more favorable when there is room for cutting the energy spent per computational operation by playing with voltages, transistor sizes, circuit style, fabrication process, and the like. The most effective way to do so in CMOS is to lower the supply voltage since the energy dissipated per operation augments quadratically with voltage whereas a circuit’s operating speed does not. The long paths through a circuit are likely to become unacceptably slow, though. A suitable architecture transform may then serve to trim these paths such as to compensate for the loss of speed. This is precisely what the attribute “Helpful for indirect energy saving” in table 3.12 refers to. Retiming, chain/tree conversion, and coarse grain pipelining are the most promising candidates as they entail no or very little overhead. Note that the net gain of any such technique must be examined in detail on a per case basis, however. 3.9.2 THE GRAND ARCHITECTURAL ALTERNATIVES FROM AN ENERGY POINT OF VIEW Over the first decade of the 21th century, power efficiency has become as important as die size, so let us re-examine the fundamental architectural alternatives from an energy point of view. Program-controlled processors heavily rely on subcircuits and activities such as • General-purpose multi-operation ALUs, • Generic register files of generous capacity, • Multi-driver busses, bus switches, multiplexers, and the like, • Program and data memories along with address generation, • Controllers or program sequencers, • Instruction fetching and decoding, • Stack operations and interrupt handling, • Dynamic reordering of operations, • Branch prediction and speculative execution, • Data shuffling between main memory and multiple levels of cache, and • Various mechanisms for maintaining cache and data consistency. From a purely functional point of view, all of this is a tremendous waste of energy because none of it contributes to payload data processing. The welcome agility of instruction set processors is thus paid for with an overhead in terms of control operations and a formidable inflation of switching activity. The isomorphic architecture, in contrast, does not carry out any computations or data transfers unless mandated by the original data processing algorithm itself. There are no instruction to fetch and decode. There is no addressing and no accessing of memories either as all data are kept in registers. Data transfers are local, there is no data shuffling between registers, cache, and main memory. There are no busses with their important load capacitances to drive. “Does this imply the isomorphic architecture is the most energy-efficient option then?” Somewhat surprisingly, this is not so because of glitching and leakage. Glitching is a phenomenon observed in digital circuits that causes redundant signal transients to occur on top of those stipulated by the computation per se. Glitch-induced switching is particularly intense when data recombine in 164 CHAPTER 3 FROM ALGORITHMS TO ARCHITECTURES combinational logic after having travelled along propagation paths of largely disparate lengths because node voltages are then likely to rock up and down several times before settling.64 By cutting overlong propagation paths, moderate pipelining and iterative decomposition tend to abate glitching and so help to improve overall energy efficiency. Leakage refers to static transistor currents that flow irrespective of whether they sit idle or are busy carrying out computations. Everything else being equal, a smaller circuit, as obtained from iterative decomposition and time sharing, tends to have fewer leakage paths. Back to the main point, it is not unusual to find that a program-controlled processor dissipates two or three orders of magnitude as much energy as an application-specific architecture does for the same computation.65 A quote from Steve Jobs (2010) underlines the point. “To achieve long battery life when playing video, mobile devices must decode the video in hardware (on the GPU); decoding it in software (on the CPU) uses too much power. ... The difference is striking: on an iPhone 4, for example, H.264 videos play for up to 10 h, while videos decoded in software play forless than5hbefore the battery is fully drained.” General-purpose processors operate with data words of uniform and often oversized width throughout an entire algorithm.66 As opposed to this, dedicated architectures make it possible to fine-tune the number of bits in every register and logic block to individual requirements as there is no compelling need to combine subfunctions with largely different precision requirements into a single datapath sized for worst-case requirements. The overriding concern is to avoid switching activities that are not relevant to the final result. Last but not least, the impressive throughputs of general-purpose processors have been bought at the price of operating CMOS circuits under conditions that are far from optimal from an energy efficiency perspective (fine-grain pipelining, extremely fast clock, large overdrive factors i.e. comparatively high supply voltage Udd combined with low MOSFET threshold voltages Uth and, hence, significant leakage). Observation 3.10. Increasing performance in power-constrained applications (almost all today), requires that the amount of energy spent per payload operation be lowered because power dissipation is nothing else than the product of operations per second and energy per operation. A key challenge of architecture design is to • minimize redundant switching activities, • provide as just as much flexibility as required, • keep the effort for design and verification within reasonable bounds, 64 You may want to refer to appendix A.5 to learn more about the causes of glitching. 65 [93] estimates the gap to be up to four orders of magnitude over direct-mapped architectures and growing. 66 The so-called multimedia instructions can provide programmers with an opportunity to process fewer bits per data item. Yet, not all instruction sets include them and not all algorithms lend themselves to taking advantage of sub-word parallelism. 3.9 CONCLUSIONS 165 and all this at a time. In most cases, this will require a clever combination of hardwired units with program-controlled processors. Energy considerations thus tend to give architecture design new momentum over the unimaginative usage of general-purpose microcomputers. 3.9.3 A GUIDE TO EVALUATING ARCHITECTURAL ALTERNATIVES In spite of our efforts to present a systematic overview on dedicated datapath architectures, we must admit that architecture design is more art than science. Many practical constraints and technical idiosyncrasies make it impossible to obtain a close-to-optimum solution by analytical means alone. The common procedure, therefore, is to come up with a variety of alternative ideas, to devise the corresponding architectures to a reasonable level of detail, and to evaluate their respective merits and drawbacks before decisions are taken. This approach — which is typical for many engineering activities — asks for creativity, methodology, and endeavor. It is nevertheless hoped that the material in this chapter gives some insight into the options available and some directions on when to prefer what option for tailoring VLSI architectures to specific technical requirements. What follows are some practical guidelines. 1. Begin by analyzing the algorithm. Section by section, identify the data flow and the nature of the essential computations. Estimate the necessary datapath resources by giving quantitative indications for • the word widths (check [94] for references), • the data rates between all major building blocks, • the memory bounds, access rates and access patterns in each building block, and • the computation rates for all major arithmetic operations.67 2. Make sure the potential for simplifications and optimizations in the algorithmic domain has been fully tapped, see section 3.3.1 for suggestions. 3. Identify the controllers that are required to govern the flow of computation along with its interplay with the external world. Examine the control flow in terms of data dependencies, overall complexity, and flexibility requirements. Find out where to go for a hard-wired dedicated architecture, where for a program-controlled processor, and where to look for a compromise. 4. Rather than starting from a hypothetical isomorphic architecture, let your intuition come up with a number of preliminary architectural concepts. Establish a rough block diagram for each of them. Make the boundaries between major subfunctions coincide with registers as you would otherwise have to trace path delays across circuit blocks during timing verification and optimization. 67 Watch out if you are given source code from some prior implementation, such as C code for a 32 bit DSP, for instance. You are likely to find items solely mandated by the resources available there or by software engineering considerations. Typical examples include operations related to (un)packing and (re)scaling, usage of computationally expensive data types, arithmetic operations substituted for bit-level manipulations, multitudes of nicely named variables that unnecessarily occupy distinct memory locations, and more. 166 CHAPTER 3 FROM ALGORITHMS TO ARCHITECTURES 5. It is always a good idea to prepare a comprehensive and, hence, fairly large table that opposes the different architectures under serious consideration. The rows serve to describe the hardware resources. Each major subfunction occupies a row of its own. Each such subfunction is then hierarchically decomposed into ever smaller subcircuits on a number of subsequent rows until it becomes possible to give numerical estimates for A, tlp, and, possibly, for E as well. Once a subcircuit has been broken down to the RTL level, one can take advantage of HDL synthesis to obtain those figures with a good degree of precision. Finite state machines, in particular, are difficult to estimate otherwise. For each architectural variant, a few adjacent columns are reserved to capture A, tlp, E and \u0003. An extra column is set aside for n, a natural number that indicates how many times the hardware resource is meant to be instantiated for the architecture being considered. \u0003 stands for the number of cycles required to obtain one processed data item with the hardware resources available. Of course, this quantity tends to diminish with n but it is not possible to state the exact dependency in general terms. 6. Estimating the overall circuit size, cycles per data item, latency, and dissipated energy for each architecture now essentially becomes a matter of bookkeeping that can be carried out with the aid of spreadsheet software. Path delays are more tricky to deal with as logic and interconnect delays are subject to vary significantly as a function of lower-level details.68 It is thus quite common to code, synthesize, place, and route the most time-critical portions of a few competing architectures merely for the purpose of evaluating max(tlp) and of extrapolating clock frequency, overall computation rate, and overall throughput. 7. Analysis of the figures so obtained will identify performance bottlenecks and inacceptably burdensome subfunctions in need of more efficient implementations. This is the point where the equivalence transforms discussed in this chapter come into play. 8. Compare the competing architectural concepts against the requirements. Narrow down your choice before proceeding to more detailed analyses and implementations. Example The table below shows results from exploring the design space for AES encryption with a key length of 128 bit [95]. The available options for trading datapath resources for computation time are evident. The narrower datapaths require extra circuitry for storing and routing intermediate results which inflates complexity and adds to path delays. What all variants have in common is that the ten cipher rounds are carried out by a single datapath as a result from iterative decomposition of the AES algorithm. Also, none of the architectures makes use of pipelining which results in latency and cycles per data item being the same. SubBytes refers to the cipher’s most costly operation from a hardware point of view. While the figures include control logic and have been obtained from actual synthesis, simplifications have been made to obtain reasonably accurate estimates for the key figures of merit without having to establish the HDL code for each architectural alternative in full detail. 68 Please refer to footnote 22 for a comment on the limitations of anticipating path delays. 3.9 CONCLUSIONS 167 Datapath width [bit] 8 16 32 64 128 Parallel SubBytes units 1 2 4 8 16 Circuit complexity [GE] 5052 6281 7155 11 628 20 410 Area A (normalized) 1 1.27 1.47 2.43 4.27 Cycles per data item 160 80 40 20 10 Longest path delay tlp (normalized) 1.35 1.34 1.21 1.13 1 Time per data item T (normalized) 21.6 10.7 4.83 2.23 1 Size-time product AT 21.6 13.6 7.10 5.42 4.27 Γ \u0002 Designers of large VLSI chips running at elevated clock rates, such as high-performance uniprocessors, inevitably run into interconnect delay as another limiting factor. This is because it is no longer possible to transmit data from one corner of a chip to the opposite one within a single clock cycle. While this important aspect has been left aside in this chapter, more will be said in fig.7.18. As a concluding remark, we would like to recall once more that good solutions call for analysis and reor- ganization of data processing algorithms at all levels of details, including architecture (process/block), register transfer (arithmetic/word), and logic (gate/bit) levels. It has been shown on numerous occasions that viewing a problem from a totally different angle can pave the way to unexpected architectural solutions that feature uncommon characteristics. Also, the possibilities for replacing a given algorithm by a truly different suite of computations that is equivalent for any practical purpose of the application at hand, but better suited to VLSI, should always be investigated first. 168 CHAPTER 3 FROM ALGORITHMS TO ARCHITECTURES 3.10 PROBLEMS 1. ∗∗∗ Computationally efficient approximations for the magnitude function √a2 + b2 have been presented in table 3.8. (a) Show that approximation 2 remains within ±3% of the correct result for any values of a and b. (b) Give three alternative architectures that implement the algorithm and compare them in terms of datapath resources, cycles per data item, longest path, and control overhead. Assume input data remain valid as long as you need them, but plan for a registered output. Begin by drawing the DDG. 2. ∗∗ Discuss the idea of combining replication with pipelining. Using fig.3.25 and the numbers that come along with it as a reference, take a pipelined datapath before duplicating it. Sketch the result in the AT-plane for various pipeline depths, e.g. for p = 2, 3, 4, 5, 6, 8, 10. Compare the results with those of competing architectures that achieve similar performance figures (a) by replicating the isomorphic configuration and (b) by extending the pipeline approach beyond the most efficient depth. How realistic are the various throughput figures when data distribution/recollection is to be implemented using the same technology and cell library as the datapath? 3. ∗∗ Reconsider the third order correlator of fig.3.32a. (a) To boost performance, try to retime and pipeline the isomorphic architecture without prior reversal of the adder chain. How does the circuit so obtained compare with fig.3.32d. Give estimates for datapath resources, cycles per data item, longest path, latency, and control overhead. (b) Next assume your prime concern is area occupation. What architectures qualify? 4. ∗∗ Fig.3.33 shows a viable architecture for a transversal filter. Before this architecture can be coded using an HDL, one must work out the missing details about clocking, register clear, register enable, and multiplexed control signals. Establish a schedule that lists clock cycle by clock cycle what data items the various computational units are supposed to work on, what data items or states that the various registers are supposed to hold, and what logic values the various control signals must assume to marshal the interplay of all those hardware items. Samples are to be processed as specified by fig.3.17a. 5. ∗∗ Arithmetic mean x and standard deviation σ are defined as x = 1 N N∑ n=1 xn σ 2 = 1 N − 1 N∑ n=1(xn − x)2 (3.98) Assume samples xn arrive sequentially one at a time. More specifically, each clock cycle sees anew w-bit data item appear. Find a dedicated architecture that computes x and σ 2 after N clock cycles and where N is some integer power of two, say 32. Definitions in (3.98) suggest one needs to store up to N − 1 past values of x. Can you do with less? What mathematical properties do you call on? What is the impact on datapath word width? This is actually an old problem the solution of which has been made popular by early scientific pocket calculators such as the HP-45, for instance. Yet, it nicely shows the difference between a crude and a more elaborate way of organizing a computation. 3.10 PROBLEMS 169 6. ∗ Most locations in the map of fig.3.28 can be reached from the isomorphic configuration on more than one route. Consider the location where A = 1/3and T = 1, for instance. Possible routes include ◦ (time share → decompose → pipeline) as shown on the map, ◦ (time share → pipeline → decompose), ◦ (pipeline → decompose → time share), ◦ (pipeline → time share → decompose), and ◦ (decompose → decompose). Architectures obtained when following distinct routes typically differ. Fig.3.28 indicates only one possible outcome per location and is, therefore, incomplete. Adding the missing routes and datapath configurations is left as a pastime to the reader. Purely out of academic interest, you may want to find out which transforms form commutative pairs. 7. ∗ Fig.3.28 shows a kind of compass that expresses the respective impact of iterative decomposition, pipelining, replication, and time sharing. Include the impact of the associativity transform in a similar way. 8. ∗∗∗ Calculating the convolution of a two-dimensional array with a fixed two-dimensional operator is a frequent problem from image processing. The operator cx,y is moved over the entire original image p(x, y) and centered over one pixel after the other. For each position X, Y the pertaining pixel of the convoluted image q(x, y) is obtained from evaluating the inner product q(X, Y) = +w∑ y=−w +w∑ x=−w cx,y p(X + x, Y + y) (3.99) Consider an application where w = 2. All pixels that contribute to (3.99) are then confined to a 5-by-5 square with the current location in its center. An uninspired implementation with distributed arithmetics would thus call for a lookup table with 225 entries which is exorbitant. A case study by an FPGA manufacturer explains how it is possible to cut this requirement down to one lookup table of a mere 16 words. Clearly, this remarkable achievement requires a couple of extra adders and flip-flops and is dependent on the particular set of coefficients given below. It combines putting together multiple occurrences of identical weights, splitting of the lookup table, taking advantage of nonoverlapping 1s across two coefficients, and clever usage of the carry input for the handling unit weights. Try to reconstruct the architecture. How close can you come to the manufacturer’s result published in [85]? cx,y x − 2 − 1 0 1 2 2 − 16 − 7 − 13 − 7 − 16 1 − 7 − 1 12 − 1 − 7 y 0 − 13 12 160 12 − 13 − 1 − 7 − 1 12 − 1 − 7 − 2 − 16 − 7 − 13 − 7 − 16 9. ∗ It has been claimed in section 3.9.2 that dedicated architectures can carry out computations with a small fraction of the energy that a general purpose microprocessor would dissipate. However, in the iPhone example given there, Steve Jobs found the impact on battery run time to be just a factor of 2. Are these two statements in contradiction? 170 CHAPTER 3 FROM ALGORITHMS TO ARCHITECTURES 3.11 APPENDIX I: A BRIEF GLOSSARY OF ALGEBRAIC STRUCTURES Any algebraic structure is defined by a set of elements S and by one or more operations. The nature of the operations involved determines which of the axioms below are satisfied. Consider a first binary operation ⊞ 1. Closure wrt ⊞:if a and b are in S then a ⊞ b is also in S. 2. Associative law wrt ⊞: (a ⊞ b) ⊞ c = a ⊞ (b ⊞ c). 3. Identity element wrt ⊞: There is a unique element e such that a ⊞ e = e ⊞ a = a for any a, (e is often referred to as “zero” element). 4. Inverse element wrt ⊞: For every a in S there is an inverse −a such that a ⊞ −a =−a ⊞ a = e. 5. Commutative law wrt ⊞: a ⊞ b = b ⊞ a. Consider a second binary operation ⊡ that always takes precedence over operation ⊞ 6. Closure wrt ⊡:if a and b are in S then a ⊡ b is also in S. 7. Associative law wrt ⊡: (a ⊡ b) ⊡ c = a ⊡ (b ⊡ c). 8. Identity element wrt ⊡: There is a unique element i such that a ⊡ i = i ⊡ a = a for any a, (i is often referred to as “one” or “unity” element). 9. Inverse element wrt ⊡: For every a in S there is an inverse a−1 such that a ⊡ a−1 = a−1 ⊡ a = i, the only exception is e for which no inverse exists. 10. Commutative law wrt ⊡: a ⊡ b = b ⊡ a. 11. Distributive law of ⊡ over ⊞: a ⊡ (b ⊞ c) = a ⊡ b ⊞ a ⊡ c and (a ⊞ b) ⊡ c = a ⊡ c ⊞ b ⊡ c. 12. Distributive law of ⊞ over ⊡: a ⊞ b ⊡ c = (a ⊞ b) ⊡ (a ⊞ c) and a ⊡ b ⊞ c = (a ⊞ c) ⊡ (b ⊞ c). 13. Complement: For every a in S there is a complement a such that a ⊞ a = i and a ⊡ a = e. Name of Opera- Axioms satisﬁed algebraic structure tions 123 45 678910 11 12 13 Set Semigroup 12 Monoid 123 Group 123 4 Abelian or commutative group 123 45 Abelian semigroup 12 5 Abelian monoid 123 5 Ring 123 45 67 11 Ring with unity 123 45 678 11 Division ring aka skew ﬁeld 123 45 6789 11 Field 123 45 678910 11 Commutative ring 123 45 67 10 11 Commutative ring with unity 123 45 678 10 11 Semiring 12 5 67 11 Commutative semiring 12 5 67 10 11 Boolean algebra 123 5 678 10 11 12 13 3.11 APPENDIX I: A BRIEF GLOSSARY OF ALGEBRAIC STRUCTURES 171 Examples with one operation Let SDNA stand for the set of all possible DNA sequences of non-zero length with characters taken from the alphabet {A,T,C,G}. This set together with the binary operation of string concatenation denoted as ⌣69 forms an infinite semigroup (SDNA, ⌣ ). A monoid (SDNA ∪{ϵ}, ⌣ ) is obtained iff the empty sequence ϵ is also admitted. All possible permutations of a given number of elements make up a group when combined with binary composition of functions70 as sole operation. For a practical example, consider all six distinct rearrangements of three elements, shown below, and let us refer to them as set S3. 1. 2. 3. 1. 2. 3. 1. 2. 3. 1. 2. 3. 1. 2. 3. 1. 2. 3. ↓↓↓ ↓↓ ↓ ↓ ↓↓ ↓↓↓ ↓↓↓ ↓↓↓ 1. 2. 3. 2. 3. 1. 3. 1. 2. 3. 2. 1. 1. 3. 2. 2. 1. 3. This set of permutations along with binary composition ◦ forms a finite group (S3, ◦). The set of all positive integers N+ ={1, 2, 3, ...} together with addition as sole operation constitutes an infinite Abelian semigroup (N+, +). When supplemented with 0, the above structure turns into an Abelian monoid (N, +). The set of all integers Z together with addition forms an infinite Abelian group (Z, +). Examples with two operations A commutative ring with unity (Z, +, · ) results when multiplication is added as a second operation to the aforementioned Abelian group. The number of elements is obviously infinite. (N, +, · ), in contrast, is merely a commutative semiring because natural numbers have no additive inverses. Yet, in addition to the necessary requirements for a semiring, identity elements with respect to both operations also happen to exist in this example. The set of all rational numbers Q together with addition as a first and multiplication as a second operation forms the field71 (Q, +, · ). Other popular fields with infinitely many elements are (R, +, ·) over the real numbers, and (C, +, · ) over the complex numbers. The set of all quotients P(x) Q(x) of two polynomials P(x) and Q(x) with real-valued coefficients together with addition and multiplication makes up for yet another infinite field. For any square matrix Mnxn with coefficients taken from a field, there exist identity elements with respect to both addition and multiplication. Any such matrix has an additive inverse, but a multiplicative only if the determinant ||Mnxn|| ̸= 0. Also, matrix multiplication is not commutative. The algebraic structure 69 Alternative symbols for the concatenation operator include . (mathematics) and & (computer science). 70 Binary composition of functions means that two functions are invoked one after the other (...) ◦ f ◦ g ≡ g(f (...)). Also keep in mind that a permutation is just a particular kind of function. 71 The German term for a field is “(Zahlen)körper”, the French “corps”, and the Italian “campo”. 172 CHAPTER 3 FROM ALGORITHMS TO ARCHITECTURES formed by the set of all square matrices together with matrix addition and matrix multiplication thus is an infinite ring with unity.72 Any subset of integers S ={0, 1, ..., p − 1} forms a field together with addition modulo p and multipli- cation modulo p iff p is a prime number. Any such finite field is called a Galois field GF(p). For p = 5, for instance, we obtain the GF(5) ({0,1,2,3,4}, + mod 5 , · mod 5 ) with the addition and multiplication tables given below. =+ mod 5 0123 4 0 0123 4 1 1234 0 2 2340 1 3 3401 2 4 4012 3 = mod 5 01234 0 00000 1 01234 2 02413 3 03142 4 04321 The best-known finite field is the GF(2) ({0,1}, ⊕, ∧). Observe that the additive inverse in GF(2) of a −a is a itself and that the multiplicative inverse of 1 1−1 is 1 while 0 has no multiplicative inverse. As opposed to this, algebraic structures of type ({0,1,...,m − 1}, + mod m , · mod m )where m is not prime merely form finite commutative rings with unity as 0 is not the only element that lacks a multiplicative inverse. In the occurrence of ({0,1,2,3,4,5,6,7,8}, + mod 9 , · mod 9 ), this also applies to elements 3 and 6. Cardinalities of finite fields are not confined to prime numbers. A so-called extension field GF(pn) can be defined for any power pn provided 2 ≤ n ∈ N+. All polynomials P(x) of degree 0, 1, ..., n − 1 with coefficients from GF(p) constitute the set of elements. The first operation is addition modulo M(x) and the second one multiplication modulo M(x) where M(x) an irreducible polynomial73 of degree n with coefficients from GF(p). GF(32), for instance, consists of the nine elements {0, 1, 2, x, x + 1, x + 2, 2x,2x + 1, 2x + 2}, the operations are + mod (x2 + 1) and · mod (x2 + 1) with M(x) = x2 + 1 being an irreducible polynomial. The factors of 30 together with operations least common multiple (lcm) and greatest common divisor (gcd) constitute a Boolean algebra of eight elements ({1,2,3,5,6,10,15,30}, lcm, gcd). It necessarily follows that taking the complement a is tantamount to computing 30 a for any a. (S, ∪, ∩) is a Boolean algebra with union and intersection as binary operations iff S is a power set P. Consider a set of three elements \b = {a,b,c}, for instance. The set of all sets that can be composed from these three elements, that is { ∅, {a}, {b}, {c}, {a,b}, {a,c}, {b,c}, {a,b,c} }, is called the power set of \b and denoted as P(\b).(P(\b), ∪, ∩) then forms a Boolean algebra. Two particular elements of P(\b), namely the empty set ∅ and the universal set \b, act as identity elements e and i for the first and the second operation respectively. Each structure element x ∈ P(\b) has a complement x = \b − x. 72 Incidentally note that all concepts of linear algebra (matrices, inverses, determinants, etc.) apply to matrices with coefficients from any field. 73 An irreducible polynomial cannot be expressed as a product of non-trivial polynomials of lower degree. 3.11 APPENDIX I: A BRIEF GLOSSARY OF ALGEBRAIC STRUCTURES 173 The above structure is readily extended to an infinite Boolean algebra when elements and sets are chosen such that |P(\b)|=∞ which, in turn, is obtained from making |\b|=∞. As an example, assume \b is made up of all DNA sequences of arbitrary length. The well-known switching algebra ({0,1}, ∨, ∧) is a Boolean algebra with just two elements. The complement of an element is its logic inverse and denoted with the negation operator ¬ . With no more than six axioms, the class of semirings is very broad. It encompasses but is not limited to the embodiments tabulated below: Constituent S the commutative semiring of natural numbers + • the commutative ring with unity of integers + • the “ordinary” ﬁelds + • + • + • all Galois ﬁelds, e.g. {0,1} ⊕ ∧ all other ﬁelds, e.g. P (x) Q(x) + • the switching algebra {0,1} ∨ ∧ other ﬁnite Boolean algebras, e.g. {0,1} ∧ ∨ or {1,2,3,4,6,12} lcm gcd all other Boolean algebras, e.g. the path algebras {0,1} max min min + max + max • max min the matrix algebras for every n n xn + • 174 CHAPTER 3 FROM ALGORITHMS TO ARCHITECTURES 3.12 APPENDIX II: AREA AND DELAY FIGURES OF VLSI SUBFUNCTIONS with contributions by Beat Muheim This appendix gives real-world numbers for common subfunctions such as logic gates, bistables, adders, and multipliers. All data refer to commercial high-performance cell libraries in static CMOS technology under typical operating conditions.74 They extend over multiple process generations and nicely document the benefits of geometric downscaling and other improvements.75 Process generations and figures of merit compared Process M1 min. half-pitch Lithographic square Number Supply generation F [nm] F 2 [nm2] of metals [V] 250 nm 320 102 400 5 2.5 180 nm 240 57 600 6 1.8 130 nm 160 25 600 up to 8 1.2 90 nm 120 14 400 up to 9 1.2 65 nm 90 8100 up to 12 1.2 45 nm 70 4900 up to 11 1.1 28 nm 50 2500 up to 11 1.0 Quantity A states the area occupied by the circuitry required to implement the target functionality. Except for individual standard cells, the intercell wiring has been completed and the associated overhead included. Parameter tid denotes the insertion delay.76 74 Numbers for individual library cells relate to a variant with simple output strength (1x drive) loaded with four standard inverters (FO4). In the occurrence of bistables, delay figures refer to the non-inverting output. 75 Do not jump to conclusions from the numbers alone, discontinuities exist. Firstly, changes in the industry have made it impossible to compile the tables from data sets of a single source. A horizontal line indicates where the foundry or the cell library vendor alters. Secondly, the lack of a universal standard for library characterization is a major source of inconsistencies. Thirdly, transistor-level circuits, transistor geometries, and threshold voltages for any given function may shift from one library generation to the next. 76 Insertion delay reflects the lapse of time that a subcircuit takes to pass on a data item from its input to the output and is defined in section A.6. As a reminder, tc = tid c = max(tpd c) for combinational functions, tff = tid ff = tsu ff + tpd ff for flip-flops, and tlc = tid lc = tsu lc + tpd lc for latches. 3.12 APPENDIX II: AREA AND DELAY FIGURES OF VLSI SUBFUNCTIONS 175 Bistable storage functions Table 3.13 Selected flip-flops and latches (D flip-flops with no reset are found in pipelines). [transparent] Latch A tid with async. reset [mm2] [F 2] [ps] 250 nm 63.4 619 n.a. 180 nm 43.7 759 343 130 nm 23.0 898 200 90 nm 11.8 819 138 65 nm 5.8 716 195 45 nm 2.9 592 194 28 nm n.a. a Figures refer to a scanable cell here as the 28 nm library includes no non-scan ﬂip-ﬂops. Dﬂip-ﬂop A tid Dscan ﬂip-ﬂop A tid with no reset [mm2] [F 2] [ps] with async. reset [mm2] [F 2] [ps] 250 nm 97.9 956 n.a. 250 nm 121.0 1181 n.a. 180 nm 50.0 868 321 180 nm 65.6 1139 648 130 nm 25.6 1000 209 130 nm 35.8 1400 435 90 nm 13.3 924 154 90 nm 18.8 1306 286 65 nm 6.8 840 129 65 nm 10.1 1247 246 45 nm 4.1 837 180 45 nm 5.7 1163 273 28 nma 3.8 1520 118 28 nm 4.4 1760 174 176 CHAPTER 3 FROM ALGORITHMS TO ARCHITECTURES Elementary logic functions Table 3.14 Selected logic gates. Inverter A tid Cinp Full adder A tid [mm2] [F 2] [ps] [fF] [mm2] [F 2] [ps] 250 nm 11.5 113 n.a. 5.6 250 nm 144.0 1410 n.a. 180 nm 6.2 108 71 3.8 180 nm 78.1 1356 341 130 nm 3.8 148 55 2.0 130 nm 41.0 1600 173 90 nm 2.4 167 29 2.0 90 nm 25.9 1800 133 65 nm 1.1 136 26 1.0 65 nm 7.6 938 128 45 nm 0.5 102 29 0.6 45 nm 4.3 878 137 28 nm 0.3 120 11 0.6 28 nm 2.6 1040 46 2-input NAND A tid 2-input NOR A tid [mm2] [F 2] [ps] [mm2] [F 2] [ps] 250 nm 17.3 169 n.a. 250 nm 17.3 169 n.a. 180 nm 9.4 162 75 180 nm 9.4 162 129 130 nm 5.1 200 65 130 nm 5.1 200 92 90 nm 3.1 215 31 90 nm 3.1 215 52 65 nm 1.4 173 39 65 nm 1.4 173 61 45 nm 0.7 143 36 45 nm 0.7 143 51 28 nm 0.5 200 14 28 nm 0.5 200 21 2-input XOR A tid 2-to-1 MUX A tid [mm2] [F 2] [ps] [mm2] [F 2] [ps] 250 nm 51.8 506 n.a. 250 nm 46.1 450 n.a. 180 nm 28.1 488 179 180 nm 21.9 379 164 130 nm 14.1 550 146 130 nm 11.5 450 121 90 nm 7.8 542 68 90 nm 7.1 493 79 65 nm 3.6 444 93 65 nm 3.6 444 76 45 nm 1.8 367 70 45 nm 2.0 408 71 28 nm 1.1 440 21 28 nm 1.5 600 25 3.12 APPENDIX II: AREA AND DELAY FIGURES OF VLSI SUBFUNCTIONS 177 Arithmetic functions Tables 3.15 and 3.16 refer to unpipelined adders and multipliers respectively. They include the approximate area for intercell wiring as estimated by Synopsys DesignCompiler. Synthesis results have been obtained by instantiating the appropriate DesignWare component followed by optimization with no timing constraint. Table 3.15 2’s complement adders with carry-in and carry-out. Note that delay data do not grow smoothly throughout but exhibit steps. ripple-carry adder A tid carry-lookahead A tid DW01 add [µm2] [F 2] [ps] DW01 addsub [µm2] [F 2] [ps] 90 nm 8 bit 207 14 400 529 90 nm 8 bit 267 18 600 578 90 nm 16 bit 414 28 800 1016 90 nm 16 bit 627 43 600 629 90 nm 24 bit 621 43 100 1512 90 nm 24 bit 880 61 100 933 90 nm 32 bit 828 57 500 2008 90 nm 32 bit 1231 85 500 933 ripple-carry adder A tid carry-lookahead A tid DW01 add [µm2] [F 2] [ps] DW01 addsub [µm2] [F 2] [ps] 28 nm 8 bit 20.9 8360 296 28 nm 8 bit 40.1 16 100 226 28 nm 16 bit 41.8 16 700 580 28 nm 16 bit 93.2 37 300 270 28 nm 24 bit 62.7 25 100 865 28 nm 24 bit 131.2 52 500 388 28 nm 32 bit 83.6 33 400 1150 28 nm 32 bit 185.2 74 100 377 Table 3.16 2’s complement multipliers. carry-save multiplier A tid DW02 mult [mm2] [F 2] [ps] 90 nm 8 bit x 8 bit 1 780 123 400 1315 90 nm 16 bit x 16 bit 7 180 498 300 2480 90 nm 24 bit x 24 bit 16 290 1131100 3750 90 nm 32 bit x 32 bit 29 230 2029600 4680 carry-save multiplier A tid DW02 mult [mm2] [F 2] [ps] 28 nm 8 bit x 8 bit 214 85 800 665 28 nm 16 bit x 16 bit 817 326 600 1200 28 nm 24 bit x 24 bit 1834 733 600 1740 28 nm 32 bit x 32 bit 3252 1 301 000 2080","libVersion":"0.5.0","langs":""}