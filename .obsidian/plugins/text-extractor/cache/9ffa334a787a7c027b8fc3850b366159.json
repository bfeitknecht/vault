{"path":"sem4/W&S/UE/s/W&S-s-u10.pdf","text":"Probability and Statistics (D-INFK) Lecturer: Prof. Dr. Dylan Possamaï Coordinator: Daniel Kršek Probability and Statistics Exercise sheet 10 - Solutions MC 10.1. Let n ∈ N and let X1, . . . , Xn be i.i.d. and standard normally distributed, i.e., Xi ∼ N (0, 1). Define Y := n∑ i=1 X 2 i . In particular, Y is a χ 2 n-distributed random variable. (Exactly one answer is correct in each question.) 1. What is the value of E[Y ]? (a) E[Y ] = 0. (b) E[Y ] = n2. (c) E[Y ] = n. (d) E[Y ] = √n. 2. What is the value of Var[Y ]? (a) Var[Y ] = n2. (b) Var[Y ] = 2n. (c) Var[Y ] = n. (d) Var[Y ] = 2n2. 3. Let now n = 12. What is the approximation of the probability P [ ∣ ∣ Y n − 1 ∣ ∣ ≤ 0.75] using the CLT? (a) P [ ∣ ∣ Y n − 1 ∣ ∣ ≤ 0.75] ≈ 2Φ ( 3 4 √6 ) − 1. (b) P [ ∣ ∣ Y n − 1 ∣ ∣ ≤ 0.75] ≈ 2Φ ( 7 4 √6 ). (c) P [ ∣ ∣ Y n − 1 ∣ ∣ ≤ 0.75] ≈ Φ (√ 7 4 ). (d) P [ ∣ ∣ Y n − 1 ∣ ∣ ≤ 0.75] ≈ 1 − 2Φ (√6). Solution: (i) (c). From the definition of Y and linearity of expectation we have E[Y ] = n∑ i=1 E[X 2 i ] = n∑ i=1 Var[Xi] = n∑ i=1 1 = n. (ii) (b). Using that (Xn)n∈N are i.i.d., we have E[X 2 i X 2 j ] = E[X 2 i ]E[X 2 j ] = (E[X 2 1 ]) 2 for any i ̸= j. It follows E[Y 2] = E[(X 2 1 + X 2 2 + · · · + X 2 n) 2] = E [ n∑ i=1 n∑ j=1 X 2 i X 2 j ] = nE[X 4 1 ] + n(n − 1)(E[X 2 1 ]) 2. 1 Probability and Statistics (D-INFK) Lecturer: Prof. Dr. Dylan Possamaï Coordinator: Daniel Kršek Using again E[X 2 1 ] = 1 and E[X 4 1 ] = ∫ ∞ −∞ x 4φ(x)dx = 1 √2π ∫ ∞ −∞ x4e−x 2/2dx = 1 √2π ( − x3e−x2/2)∣ ∣ ∣∞ x=−∞ + 3 √2π ∫ ∞ −∞ x 2e−x2/2dx = 0 + 3 × E[X 2 1 ] = 3, where we have used integration by parts, we obtain E[Y 2] = 3n + n(n − 1) = n2 + 2n, and thus Var[Y ] = E[Y 2] − (E[Y ]) 2 = (n2 + 2n) − n2 = 2n. Alternative solution: Since X1, . . . , Xn are independent, so are X 2 1 , . . . , X 2 n, and so we have Var[Y ] = n∑ i=1 Var[X 2 i ] = n × Var[X 2 1 ]. As above, Var[X 2 1 ] = E[X 4 1 ] − (E[X 2 1 ]) 2 = 3 − 1 = 2, and so Var[Y ] = 2n. (iii) (a). Let Zi := X 2 i for i ∈ {1, . . . , n}. Then Y = ∑n i=1 Zi with (Zn)n∈N i.i.d. and Zi ∼ χ2 1, and so E[Zi] = 1 and Var[Zi] = 2. By the central limit theorem: P [∣ ∣ ∣ ∣ Y n − 1 ∣ ∣ ∣ ∣ ≤ 0.75] = P [∣ ∣ ∣ ∣ Y − n n ∣ ∣ ∣ ∣ ≤ 3 4 ] = P [∣ ∣ ∣ ∣ Y − n √2n ∣ ∣ ∣ ∣ ≤ 3 4 √ n 2 ] = P [ − 3 4 √ n 2 ≤ Y − n √2n ≤ 3 4 √ n 2 ] ≈ Φ ( 3 4 √ n 2 ) − Φ ( − 3 4 √ n 2 ) = 2Φ ( 3 4 √ n 2 ) − 1. For n = 12, we thus have P [∣ ∣ ∣ ∣ Y n − 1 ∣ ∣ ∣ ∣ ≤ 0.75] ≈ 2Φ ( 3 4 √6) − 1. 2 Probability and Statistics (D-INFK) Lecturer: Prof. Dr. Dylan Possamaï Coordinator: Daniel Kršek Exercise 10.2. Let U1, U2, U3 be i.i.d. random variables uniformly distributed on [0, 1]. We consider the random variables L := min{U1, U2, U3} and M := max{U1, U2, U3}. (a) Show that M and L have densities and find them. (b) Show that for ϕ, ψ : R → R piecewise continuous and bounded, the following holds: E[ϕ(M )ψ(L)] = ∫ ∞ −∞ ∫ ∞ −∞ ϕ(m) · ψ(ℓ) · 6(m − ℓ)1{0≤ℓ≤m≤1}dℓdm. (c) Use (b) to determine the joint distribution function and the joint density of (M, L). Solution: (a) We first note that P[M ∈ [0, 1]] = 1. For m ∈ [0, 1], using the independence and uniformity of U1, U2, and U3, we have: P[M ≤ m] = P[U1 ≤ m, U2 ≤ m, U3 ≤ m] = (P[U1 ≤ m] )3 = m 3. So the distribution function FM of M is given by FM (m) =    0 for m < 0, m3 for 0 ≤ m ≤ 1, 1 for m > 1. We see that FM is continuous and piecewise continuously differentiable. Hence, the density exists and for m ∈ (0, 1): fM (m) = d dm FM (m) = 3m 2. We thus obtain the density fM (m) =    0 for m < 0, 3m 2 for 0 ≤ m ≤ 1, 0 for m > 1. To find the density of L, we first note that we have P[L ≥ ℓ] = P[U1 ≥ ℓ, U2 ≥ ℓ, U3 ≥ ℓ] = (P[U1 ≥ ℓ])3 = (1 − ℓ)3. Consequently, the distribution function FL of L is given by FL(ℓ) =    0 for ℓ < 0, 1 − (1 − ℓ) 3 for 0 ≤ ℓ ≤ 1, 1 for ℓ > 1. Thus, analogously as before, we obtain that the density exists and is given by fL(ℓ) = 3(1 − ℓ) 21{ℓ∈[0,1]}. 3 Probability and Statistics (D-INFK) Lecturer: Prof. Dr. Dylan Possamaï Coordinator: Daniel Kršek (b) Let ϕ, ψ : R → R be piecewise continuous and bounded. Using the joint density of U1, U2, U3 we compute E[ϕ(M ) · ψ(L)] = ∫ 1 0 ∫ 1 0 ∫ 1 0 ϕ(max{u1, u2, u3})ψ(min{u1, u2, u3})du1du2du3. We distinguish cases depending on which variable is the maximum and which is the minimum. For the case u3 ≤ u2 ≤ u1 (i.e. m = u1 and ℓ = u3) we have: ∫ 1 0 ∫ 1 0 ∫ 1 0 ϕ(u1)ψ(u3)1{u3≤u2≤u1}du1du2du3 = ∫ 1 0 ϕ(u1)( ∫ u1 0 ψ(u3) ( ∫ u1 u3 du2 ) du3 )du1 = ∫ 1 0 ∫ 1 0 ϕ(u1)ψ(u3)(u1 − u3)1{u3≤u1}du1du3 Since there are in total 3! = 6 options how u1, u2, u3 can be ordered (u3 ≤ u2 ≤ u1, u2 ≤ u3 ≤ u1, u2 ≤ u2 ≤ u3, . . . ), which are symmetric, we obtain E[ϕ(M )ψ(L)] = 6 × ∫ ∞ −∞ ∫ ∞ −∞ ϕ(m)ψ(ℓ)(m − ℓ)1{0≤ℓ≤m≤1}dmdℓ, as required. Remark: Note that, due to continuity of the uniform distribution and independence, we have P[U1 = U2] = P[U1 = U3] = P[U2 = U3] = 0, which can be verified by straightforward compu- tations. We thus do not care that the cases u1 = u2 = u3, u2 = u3 < u1, etc..., are counted multiple times. (c) For a, b ∈ R, choose ϕ(x) := 1{x≤a} and ψ(x) := 1{x≤b} to get FM,L(a, b) = P[M ≤ a, L ≤ b] = E[1{M ≤a}1{L≤b}] = ∫ a −∞ ∫ b −∞ 6(m − ℓ)1{0≤ℓ≤m≤1}dmdℓ So the joint density is fM,L(m, ℓ) = 6(m − ℓ)1{0≤ℓ≤m≤1}. To compute the joint distribution function, we evaluate the integral above. • For a ≤ 0 or b ≤ 0, we have FM,L(a, b) = 0. • For b ≥ 1 and a ∈ [0, 1], FM,L(a, b) = FM (a) = a3. • For a ≥ 1 and b ∈ [0, 1], FM,L(a, b) = FL(b) = 1 − (1 − b)3. • For 0 ≤ a ≤ b ≤ 1, FM,L(a, b) = P[M ≤ a] = a 3. 4 Probability and Statistics (D-INFK) Lecturer: Prof. Dr. Dylan Possamaï Coordinator: Daniel Kršek • For 0 ≤ b ≤ a ≤ 1, P[M ≤ a, L ≤ b] = ∫ a 0 ( ∫ min{b,m} 0 6(m − ℓ)dℓ )dm = ∫ a 0 6mℓ − 3ℓ 2∣ ∣ ∣min{b,m} ℓ=0 dm = ∫ a 0 [6m × min{b, m} − 3 min{b, m} 2]dm = ∫ b 0 [6m × min{b, m} − 3 min{b, m} 2]dm + ∫ a b [ 6m × min{b, m} − 3 min{b, m} 2] dm = ∫ b 0 [6m × m − 3m 2]dm + ∫ a b [ 6m × b − 3b2] dm = ∫ b 0 3m 2dm + ∫ a b 3b(2m − b)dm = b 3 + 3ab(a − b). Exercise 10.3. Let (Xi)i∈N, (Yi)i∈N, and (Zi)i∈N be sequences of i.i.d. random variables with P[X1 = 1] = P[X1 = −1] = 1/2, and similarly P[Y1 = 1] = P[Y1 = −1] = 1/2 as well as P[Z1 = 1] = P[Z1 = −1] = 1/2, which are also independent of each other. Put differently, the sequence (X1, Y1, Z1, X2, Y2, Z2, X3, Y3, Z3, . . .) is a sequence of i.i.d. random variables. We define the partial sums S(x) n := n∑ i=1 Xi, S(y) n := n∑ i=1 Yi, and S(z) n := n∑ i=1 Zi. The sequence ((S(x) n , S(y) n , S(z) n ) ) n∈N is called a random walk in Z 3. Let α > 1/2. Show that lim n→∞ P [∥(S(x) n , S(y) n , S(z) n )∥ ≤ nα] = 1, where ∥(x, y, z)∥ := √ x2 + y2 + z2 is the Euclidean norm. Hint: First, apply the CLT to show that for all α > 1/2, we have lim n→∞ P [|S(x) n | ≤ nα] = lim n→∞ P [|S(y) n | ≤ nα] = lim n→∞ P [|S(z) n | ≤ nα] = 1. 5 Probability and Statistics (D-INFK) Lecturer: Prof. Dr. Dylan Possamaï Coordinator: Daniel Kršek Then, notice that for α′ ∈ (1/2, α) we have: ({|S(x) n | ≤ nα ′} ∩ {|S(y) n | ≤ nα′} ∩ {|S(z) n | ≤ nα ′}) ⊆ { ∥(S(x) n , S(y) n , S(z) n )∥ ≤ √3nα ′} . Use this to conclude. Solution: We first show that for all α > 1/2, we have lim n→∞ P [|S(x) n | ≤ nα] = 1. (1) Since E[X1] = 0 and Var[X1] = 1, we obtain by the CLT that for any a ∈ R: P [S(x) n ≤ a √n] = P [ S(x) n √n ≤ a ] n→∞ −−−−→ Φ(a), and therefore also P [|S(x) n | ≤ a √n] = P [ S(x) n ≤ a √n] − P [ S(x) n ≤ −a √n] n→∞ −−−−→ Φ(a) − Φ(−a) = 2Φ(a) − 1. Since, for a fixed a > 0 and for sufficiently large n we have a √n ≤ nα, monotonicity implies lim n→∞ P [ |S(x) n | ≤ nα] ≥ lim n→∞ P [|S(x) n | ≤ a √n] = 2Φ(a) − 1, a > 0. As this inequality holds for every a > 0, we can write lim n→∞ P [ |S(x) n | ≤ nα] ≥ sup a>0{2Φ(a) − 1} = 1, and so property (1) follows. Analogously, we can show that the same result holds for S(y) n and S(z) n . We now show that for all α > 1/2, we have limn→∞ P [∥(S(x) n , S(y) n , S(z) n )∥ ≤ nα] = 1. Choose α′ ∈ (1/2, α) and observe ({|S(x) n | ≤ nα ′} ∩ {|S(y) n | ≤ nα′} ∩ {|S(z) n | ≤ nα ′}) ⊆ { ∥(S(x) n , S(y) n , S(z) n )∥ ≤ √3nα ′} . Since nα ≥ √3nα′ for large n, we get lim n→∞ P [ ∥(S(x) n , S(y) n , S(z) n )∥ ≤ nα] ≥ lim n→∞ P [∥(S(x) n , S(y) n , S(z) n )∥ ≤ √3 · nα ′] ≥ lim n→∞ P [|S(x) n | ≤ nα ′, |S(y) n | ≤ nα′, |S(z) n | ≤ nα ′] = 1, where the equality in the last step follows from the union bound, since we have lim n→∞P [|S(x) n | ≤ nα ′, |S(y) n | ≤ nα ′, |S(z) n | ≤ nα′] = 1 − lim n→∞ P [{ |S(x) n | > n α ′} ∪ {|S(y) n | > n α ′} ∪ {|S(z) n | > n α ′}] ≥ 1 − lim n→∞ (P [ |S(x) n | > n α′] + P [|S(y) n | > n α ′] + P [|S(z) n | > n α ′] ) = 1, where we have used (1) in the last step. 6 Probability and Statistics (D-INFK) Lecturer: Prof. Dr. Dylan Possamaï Coordinator: Daniel Kršek Exercise 10.4. The median m of a distribution F is defined by m := F −1(1/2) = inf{x ∈ R : F (x) ≥ 1/2}. Let X1, X2, . . . be i.i.d. random variables with distribution function F and median m = 0. Let Zn denote the sample median of X1, . . . , Xn, that is, Zn is the middle observation. More formally Zn = X(k) where k = [ n 2 + 1] and X(1) ≤ · · · ≤ X(n) denote the order statistics of X1, . . . , Xn (i.e. X(1) = min{Xi | i ∈ {1, . . . , n}}, X(n) = max{Xi | i ∈ {1, . . . , n}}, etc.), and [x] denotes the integer part of x. (a) Let Y x i = 1{Xi≤x} and define Sx n := ∑n i=1 Y x i . Compute E[Sx n] and Var[Sx n]. (b) Express the event {Zn ≤ x} using the random variable Sx n. (c) Using the CLT, give an approximation for P[Zn ≤ x] as n → ∞. (d) (*) Find the limit lim n→∞ 1/2 − αn√ 1 n αn(1 − αn) , where αn := F ( x√n ) . Solution: (a) We have 1 − P[Y x i = 0] = P[Y x i = 1] = P[Xi ≤ x] = F (x), and so Y x i ∼ Bernoulli(F (x)). Consequently Sx n ∼ Binomial(n, F (x)), and so we have E[Sx n] = nF (x), Var[Sx n] = nF (x)(1 − F (x)). (b) We observe that {Zn ≤ x} = {Sx n ≥ k}, where k = [ n 2 + 1]. (c) Using the central limit theorem, we obtain P[Zn ≤ x] = P[Sx n ≥ k] = P [ Sx n n ≥ k n ] = P   Sx n n − F (x) √ 1 n F (x)(1 − F (x)) ≥ k n − F (x) √ 1 n F (x)(1 − F (x))   ≈ 1 − Φ   k/n − F (x) √ 1 n F (x)(1 − F (x))   = Φ  − k n − F (x) √ 1 n F (x)(1 − F (x))   . 7 Probability and Statistics (D-INFK) Lecturer: Prof. Dr. Dylan Possamaï Coordinator: Daniel Kršek (d) Define αn := 1 n E [Sn( x√n )] = F ( x√n ) and note Var [Sn( x √n )] = αn(1 − αn) n . We examine the limit lim n→∞ 1/2 − αn√ 1 n αn(1 − αn) = lim n→∞ − αn − 1/2 x√n x √αn(1 − αn) . Note that lim n→∞ αn − 1/2 x√n = lim n→∞ F ( x√n ) − F (0) x√n = F ′(0), and lim n→∞ αn = lim n→∞ F ( x √n ) = F (0) = 1/2. Thus, lim n→∞ 1/2 − αn√ 1 n αn(1 − αn) = −2F ′(0)x. Exercise 10.5. Let X1, . . . , Xn be i.i.d. random variables with Xi ∼ U([θ − 1, θ]) under Pθ, where θ ∈ R is an unknown parameter. We consider the following estimators for θ: T (n) 1 = 1 n n∑ i=1 ( Xi + 1 2 ) and T (n) 2 = max{X1, . . . , Xn}. (a) Determine whether the estimators are unbiased. (b) Compute the variances Varθ[T (n) 1 ] and Varθ[T (n) 2 ]. (c) Compute the mean squared error MSEθ[T (n) i ] := Eθ[(T (n) i − θ) 2], i ∈ {1, 2}. Remark: Here, Eθ and Varθ denote the mean and the variance under probability measure Pθ. Solution: (a) Fix θ ∈ R. As Xi ∼ U([θ − 1, θ]), we have Eθ[T (n) 1 ] = ( 1 n n∑ i=1 Eθ[Xi] ) + 1 2 = Eθ[X1] + 1 2 = θ − 1 2 + 1 2 = θ. So, T (n) 1 is unbiased. 8 Probability and Statistics (D-INFK) Lecturer: Prof. Dr. Dylan Possamaï Coordinator: Daniel Kršek Define Y θ i := Xi − (θ − 1) so that Yi ∼ U([0, 1]) under Pθ. We further define Y (n) θ := max{Y θ 1 , . . . , Y θ n } = T (n) 2 − (θ − 1). The distribution function and the density of Y (n) θ are respectively (see Exercise 10.2) F Y (n) θ (a) =    0 if a < 0, a n if a ∈ [0, 1], 1 if a > 1, and fY (n) θ (a) = nan−11{a∈[0,1]}. (2) Hence, Eθ[Y (n) θ ] = ∫ ∞ −∞ af Y (n) θ (a)da = n ∫ 1 0 a nda = n n + 1 , and so Eθ[T (n) 2 ] = Eθ[Y (n) θ ] + (θ − 1) = θ − 1 n + 1 . Thus, T (n) 2 is not unbiased. (b) We have Varθ[T (n) 1 ] = 1 n2 n∑ i=1 Varθ[Xi] = 1 n Varθ[X1] = 1 12n . Now, using (2), we compute Eθ[(Y (n) θ )2] = n ∫ 1 0 a n+1da = n n + 2 , so Varθ[T (n) 2 ] = Varθ[Y (n) θ ] = Eθ[(Y (n) θ )2] − (Eθ[Y (n) θ ]) 2 = n n + 2 − ( n n + 1 )2 = n (n + 1)2(n + 2) . (c) Finally, we can compute MSEθ[T (n) 1 ] = Varθ[T (n) 1 ] + (Eθ[T (n) 1 ] − θ) 2 = Varθ[T (n) 1 ] = 1 12n . MSEθ[T (n) 2 ] = Varθ[T (n) 2 ] + (Eθ[T (n) 2 ] − θ)2 = n (n + 1)2(n + 2) + 1 (n + 1)2 = 2 (n + 1)(n + 2) . Exercise 10.6. We model the water level above the critical flood mark (140 cm above normal) in Lake Zurich. Let X denote the water height (in cm) above the critical mark. We use a generalized Pareto distribution: fX (x; θ) = { 1 θ (1 + x)−(1+ 1 θ ) for x > 0, 0 for x ≤ 0, where θ > 0 is an unknown parameter to be estimated based on observations x1, . . . , xn. These are modeled as realizations of i.i.d. random variables X1, . . . , Xn with density fX (x; θ). We define the estimator by T (n) = 1 n n∑ i=1 log(1 + Xi). 9 Probability and Statistics (D-INFK) Lecturer: Prof. Dr. Dylan Possamaï Coordinator: Daniel Kršek (a) Compute the expectation and variance of T (n) under Pθ for each θ > 0. Hint: Define Yi := log(1 + Xi). Then Yi ∼ Exp(1/θ), i.e., the density of Yi is fYi(y) = 1 θ e−y/θ1{y≥0}. (b) Is T (n) an unbiased estimator for θ? (c) Compute the mean squared error MSEθ[T (n)]. (d) Find the maximum likelihood estimator for θ. Solution: (a) Linearity of expectation gives Eθ[T (n)] = Eθ [ 1 n n∑ i=1 log(1 + Xi) ] = 1 n n∑ i=1 Eθ[log(1 + Xi)] = Eθ[log(1 + X1)]. By the hint, Y1 := log(1 + X1) ∼ Exp(1/θ). Therefore, Eθ[Y1] = θ, and Varθ[Y1] = θ2. We thus conclude Eθ[T (n)] = Eθ[Y1] = θ. Similarly, we can compute thanks to independence Varθ[T (n)] = Varθ [ 1 n n∑ i=1 log(1 + Xi) ] = 1 n2 n∑ i=1 Varθ[log(1 + Xi)] = 1 n Varθ[log(1 + X1)] = 1 n θ2. (b) From (a), we know that Eθ[T (n)] = θ, so T (n) is an unbiased estimator of θ. (c) Since the estimator is unbiased (see part b), the mean squared error simplifies to: MSEθ[T (n)] = Varθ[T (n)] = θ2 n . (d) The likelihood function is L(x1, . . . , xn; θ) = n∏ i=1 fX (xi; θ) = 1 θn n∏ i=1(1 + xi) −(1+ 1 θ )1{x1≥0,...,xn≥0}. Thus, the log-likelihood function is log L(x1, . . . , xn; θ) = log ( 1 θn n∏ i=1(1 + xi) −(1+ 1 θ )) 1{x1≥0,...,xn≥0} = ( − n log θ − (1 + 1 θ ) n∑ i=1 log(1 + xi) )1{x1≥0,...,xn≥0}. 10 Probability and Statistics (D-INFK) Lecturer: Prof. Dr. Dylan Possamaï Coordinator: Daniel Kršek Differentiating with respect to θ gives ∂ ∂θ log L(x1, . . . , xn; θ) = − n θ + 1 θ2 n∑ i=1 log(1 + xi), x1 ≥ 0, . . . , xn ≥ 0. We further have − n θ + 1 θ2 n∑ i=1 log(1 + xi) = 0 ⇐⇒ nθ = n∑ i=1 log(1 + xi) ⇐⇒ θ = 1 n n∑ i=1 log(1 + xi). Thus, the point θ⋆ = 1 n n∑ i=1 log(1 + xi) is a unique stationary point. We compute the second derivative: ∂2 ∂θ2 log L(x1, . . . , xn; θ) = n θ2 − 2 θ3 n∑ i=1 log(1 + xi), x1 ≥ 0, . . . , xn ≥ 0. Plugging in θ⋆, we verify ∂2 ∂θ2 log L(x1, . . . , xn; θ⋆) = n (θ⋆)2 − 2 (θ⋆)3 n∑ i=1 log(1 + xi) = 1 θ⋆ ( n θ⋆ − 2 (θ⋆)2 n∑ i=1 log(1 + xi)) = 1 θ⋆ ( − 1 (θ⋆)2 n∑ i=1 log(1 + xi) ) = − 1 (θ⋆)3 n∑ i=1 log(1 + xi) < 0. This confirms that θ⋆ is a maximum of the log-likelihood function. We conclude that the maximum likelihood estimator for θ is TM L = 1 n n∑ i=1 log(1 + Xi) = T (n). 11","libVersion":"0.5.0","langs":""}