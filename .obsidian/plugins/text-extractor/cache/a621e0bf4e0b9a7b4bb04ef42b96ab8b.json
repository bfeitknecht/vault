{"path":"sem5/VLSI1/VRL/extra/Top-Down-Digital-VLSI-Design/Chapter-8---Acquisition-of-Asynchronous-Dat_2015_Top-Down-Digital-VLSI-Desig.pdf","text":"CHAPTER 8 ACQUISITION OF ASYNCHRONOUS DATA 8.1 MOTIVATION Most digital systems that interact with the external world must handle asynchronous inputs because events outside that system appear at random points in time with respect to the system’s internal operation. As an example, a crankshaft angle sensor generates a pulse train regardless of the state of operation of the electronic engine management unit that processes those pulses. Synchronization problems are not confined to electromechanical interfaces. Much the same situation occurs when electronic systems interact that are mutually independent, in spite — or precisely because — of the fact that each of them is operating in a strictly synchronous way. Just think of a workstation that exchanges data with a file server over a local area network (LAN), of audio data that are being brought to a D/A converter for output, or of a data bus that traverses the borderline from one clock domain to another. Obviously, the processing of asynchronous inputs is more frequent in digital systems than one would like. The subsequent episode can teach us a lot about the difficulty of accommodating asynchronously changing input signals. The account is due to Charles E. Molnar who was honest enough to tell us about all misconceptions he and his colleagues went through until the problems of synchronization were fully understood. Historical example Back in 1963, a team of electronics engineers was designing a computer for biological researchers that was to be used for collecting data from laboratory equipment. In order to influence program execution from that apparatus, a mechanism was included to conditionally skip one instruction depending on the binary status of some external signal. The basic idea behind that design, sketched in fig.8.1a, was to increment the computer’s program counter one extra time via a common enable input of its flip-flops iff the external signal was at logic 1. When monitoring the computer’s operation, the engineering team observed that the next instruction was occasionally being fetched from a bogus address after the external signal had been asserted thereby causing the computer to lose control over program execution. As an example, either of 0 1111 = 15 or 1 0000 = 16 is expected to follow after address 0 1110 = 14 depending on the external input. Instead, the program counter could be observed to enter a state such as 1 1101 = 29 or 0 0010 = 2. The team soon found out such failures developed only if the external signal happened to change just as the active Top-Down Digital VLSI Design © 2015 Elsevier Inc. All rights reserved. 445 446 CHAPTER 8 ACQUISITION OF ASYNCHRONOUS DATA from external input Clk_C computer putative clock boundary DQ +1 1 0 program counter +2 clock boundary synchronization flip-flop Clk_C DQ from external input DQ +1 1 0 program counter +2 computer ! (a) (b) FIGURE 8.1 Control unit with conditional instruction skip mechanism (simpliﬁed). Original design without synchronization (a), improved design with external signal synchronized (b). clock edge was about to occur, causing the program counter to end up with a wild mix of old and new bits. The obvious solution was to insert a synchronization flip-flop such as to make a single decision as to whether the external level was 0 or 1 at the time of clock. Although the improved design, sketched in fig.8.1b, performed much better than the initial one, the team continued to observe sporadic jumps to unexpected memory locations, a failure pattern for which it had no satisfactory explanation at the time. It took almost a decade before Molnar and others who worked on high-speed interfaces1 dare publicly report on the anomalous behavior of synchronizers and before journals would accept such reports that contrasted with general belief. \u0002 Two failure mechanisms are exposed by this case, namely data inconsistency and synchronizer metastability, both of which will be discussed in this chapter together with advice for how to get them under control. 1 What was considered high speed yesterday, no longer is high speed today. In the context of synchronization, “high-speed” always refers to circuits that operate at clock frequencies and data rates approaching the limits of the underlying technology. 8.2 DATA CONSISTENCY IN VECTORED ACQUISITION 447 8.2 DATA CONSISTENCY IN VECTORED ACQUISITION We speak of vectored acquisition when a clock boundary — the hypothetical line that separates two clock domains — is being traversed by two or more electrical lines that together form a data, address, control or status word, or some other piece of information. 8.2.1 PLAIN BIT-PARALLEL SYNCHRONIZATION Consider the situation of fig.8.2 where data words traverse a clock boundary on w parallel lines before being synchronized to the receiver clock ClkQ in a register of w flip-flops. Due to various imperfections of practical nature,2 some of the bits will switch before others do whenever the bus assumes a new value. If this happens near the active clock edge, the register is bound to store a jumbled data pattern that mixes old and new bits in an arbitrary way. slow bit fast bit ClkQ data get sampled while inconsistent delay w delay ..... clock boundary synchronizer ClkQ ..... ..... ClkP Data Data FIGURE 8.2 Non-simultaneous switching in a parallel bus may result in jumbled data. Depending on context, the impact of occasional data jumble on a circuit’s operation ranges between benign malfunction and fatal disaster: ◦ Data error for one cycle before being overwritten with correct value. ◦ Episodic disturbance lasting for several clock cycles (e.g. in a filter). ◦ Value outside legal range (e.g. if data is a complementary pair, follows one-hot encoding, or is coded in some other redundant format). ◦ Finite state machine (FSM) deflected to a state unplanned for. ◦ Finite state machine (FSM) trapped in a lockup situation. ◦ An address counter pointing to mistaken memory locations. 2 Such as unmatched gate delays, unlike loads, distinct wire routes, unequal layout parasitics, clock skew, PTV variations, OCV, and crosstalk. 448 CHAPTER 8 ACQUISITION OF ASYNCHRONOUS DATA Warning example A stream of digital audio data available in a 16bit parallel format is resynchronized to a 44.1 kHz output clock before being fed to a D/A converter. As audio samples are coded in 2’s complement format (2’C), their range covers the interval [−32 768 , +32 767]. Only one sample will be affected in the occurrence of a synchronization failure. Yet, to apprehend the impact of jumbled data, imagine two consecutive samples of low amplitude end up intermingled as follows. sample decimal relative 2’s complement code s(t) (correct) +47 +0.0014 → 0000 0000 0010 1111 s(t + 1) (correct) −116 −0.0035 → 1111 1111 1000 1100 maximum mix +32 687 +0.9976 ← 0111 1111 1010 1111 random outcome +5 388 +0.1644 ← 0001 0101 0000 1100 minimum mix −32 756 −0.9997 ← 1000 0000 0000 1100 \u0002 Observation 8.1. Data that cross a clock boundary on parallel lines cannot be synchronized with the aid of parallel registers alone. Data may otherwise get jumbled and upset downstream circuits. 8.2.2 UNIT-DISTANCE CODING We first observe that any pattern that results from jumbling two data words necessarily matches either of the two words iff their Hamming distance is one or less. The consistency problem can thus be solved by adopting a unit-distance code3 provided data are known never to change by more than one step in either direction at a time, a requirement that confines unit-distance coding to applications such as the acquisition of position and angle encoder data. unsigned binary Gray coding clock count decimal code decimal code c(t) +47 → 0010 1111 +47 → 0011 1000 c(t + 1) +48 → 0011 0000 +48 → 0010 1000 maximum mix +63 ← 0011 1111 +48 → 0010 1000 minimum mix +32 ← 0010 0000 +47 → 0011 1000 3 Unit-distance codes have the particularity that any two adjacent numbers are assigned code words that differ in a single bit. They include the well-known Gray code (2w) and the Glixon, O’Brien, Tompkins, and reflected excess-3 codes (binary coded decimal (BCD)). 4bit Gray coding, for instance, goes as follows: decimal Gray code decimal Gray code decimal Gray code decimal Gray code 15 1000 11 1110 7 0100 3 0010 14 1001 10 1111 6 0101 2 0011 13 1011 9 1101 5 0111 1 0001 12 1010 8 1100 4 0110 0 0000 8.2 DATA CONSISTENCY IN VECTORED ACQUISITION 449 Example A sampling rate converter IC for digital audio applications included a subfunction for tracking the ratio of the two sampling frequencies in real time. The circuit was built on the basis of an all-digital phase locked loop (PLL) and asked for a counter clocked at frequency fp the state of which had to be read out periodically with frequency fq. The frequency ratio of the two clocks was known to be contained in the interval [ 1 2 ...2] and to vary slowly. Experience had shown that any jumbled clock counts would impair the final audio signal in a critical way, even if increment and read-out operations happened to coincide only sporadically. An error-safe solution had thus to be sought. Fig.8.3 shows the relevant hardware portion which eliminated data jumble by having the clock count data traverse the clock boundary as Gray code numbers. \u0002 8.2.3 SUPPRESSION OF JUMBLED DATA PATTERNS The idea here is to detect and ignore transients by comparing subsequent data words at the receiver end. If any mismatch is detected, data are declared corrupted and are discarded by having the synchronizer output the same value as in the cycle before. A comparison over just two words as in the circuit of fig.8.4 suffices, provided input data are guaranteed to settle within one clock period. To avoid loss of data, any data item must get sampled correctly at least two times in a row which implies that up to three clock intervals may be necessary until a new item becomes visible at the synchronizer output. A related but more onerous proposition is to use error detection coding to find out when a freshly acquired data word should be ignored. Gray coded clock count Gray counter next state logic DQ clock boundary QklCPklC Gray to binary mapping synchronizer DQDQ binary coded clock count ±1 taDoceDtaDocnE FIGURE 8.3 Frequency ratio estimator with vectored synchronizer based on a unit-distance code (partial view). 450 CHAPTER 8 ACQUISITION OF ASYNCHRONOUS DATA clock boundary ClkQ DQ ENA DQ DQ = 0 1 data valid synchronizer ClkP Data FIGURE 8.4 A vectored synchronizer that detects and ignores jumbled data. Note that specialized hardware is not the only possible approach to reject jumbled data patterns. The same idea can also be implemented in software provided data rate, data transition time, and sampling rate can be arranged to be consistent with each other.4 8.2.4 HANDSHAKING This approach contrasts with the previous one in that it prevents jumbled data from being admitted into the receiving circuit. The general idea is to avoid sampling data vectors while they might be changing. Instead, the updating and the sampling of the data get coordinated by a handshake protocol that involves both the producing and the consuming subsystems. As will become clear later in this section, handshake protocols also have other applications than just avoiding the emergence of jumbled data at clock boundaries. Full handshaking is essentially symmetrical and requires two control lines, termed request Req and acknowledge Ack respectively, that run in opposite directions, see fig.8.5a. Observation 8.2. The rules of full handshaking demand that any data transfer gets initiated by some specific event on the request line and that it gets concluded by an analogous event on the acknowledge line. Handshake protocols come in many variations, let us focus on a few important ones.5 4 Observe the resemblance to the debouncing of mechanical contacts. 5 What follows are comments on further variations of the basic handshake protocol. Notice from fig.8.5 that the request line has the same orientation as the data bus. This assumption, which we have made throughout our discussion, is referred to as a push protocol because any data transfer gets initiated by the producer. The data-valid message is transmitted over the request line, and the data secured message over the acknowledge line. In a pull protocol everything is reversed. The request line runs in the opposite direction than the data bus, transfers get initiated by the consumer, the request line carries the data secured message (that prompts the producer to deliver another data item), and the data-valid message is encoded on the acknowledge line. Yet, it is important to understand that these designations relate to naming conventions only, by no means do they imply that an active producer is driving a passive consumer, or vice versa. Further observe in fig.8.5d that the data bus holds valid data for only half of the time. The precise name of this scenario is “four-phase prolonged early push protocol”. The freedom in deciding when to withdraw or overwrite a data vector, represented in fig.8.5d by the event labelled Val-, suggests this is not the only option. You may want to refer to [203] for an exhaustive discussion of handshake protocols and their terminology. 8.2 DATA CONSISTENCY IN VECTORED ACQUISITION 451 Ack Ack2 Req Req2 Req Ack no level restoration, no unproductive events, no unnecessary latency Data ClkP ClkQ producer consumer 1st transfer 2nd transfer 3rd transfer levels restored before a new transfer begins, unproductive events, longer latency Data ClkP ClkQ producer consumer 1st transfer 2nd transfer Req Ack ............ consumer, synchronous subsystemproducer, synchronous subsystem Data clock boundaryClkP ClkQ ww DQ ENA w DQ ENA (optional) synchronizer subcircuit scalar synchronizer subcircuit scalar finite state machine finite state machine Val+ = data valid Sec+ = data secured Val- = data withdrawn (optional) producer consumer push protocol: acknowledge reception request reception Req+ Val+ Req2+ Ack+Ack2+ Val+ Sec+ Req- Req2- Sec+ Ack-Ack2- odd transfer even transfer Req+ Req2+ Ack+Ack2+ Val- Req- Req2- Ack-Ack2- Sec+ transfer Val+ (b) (a) (d) (c) (e) FIGURE 8.5 Full handshaking put to service for vectored synchronization. Circuit (a), waveforms with NRZ protocol (b), signal transition graph (STG) (c), same for RZ protocol (d,e). NRZ or two-phase handshake protocol The waveforms and event sequences of figs.8.5b and c show a possible scenario. Let both control lines be at logic 0 before the first data transfer begins. When the producer has finished preparing a new data word, he stores it in his output register. By toggling the Req line, he then informs the consumer that the Data vector has settled to a new valid state and requests it to ingest that data item. He thereby becomes liable of maintaining this state until data reception is confirmed by the consumer as the latter is free to accommodate and process the pending data item whenever he wants to do so. 452 CHAPTER 8 ACQUISITION OF ASYNCHRONOUS DATA When, some time later, the consumer has safely got hold of that data item and he is ready to accept another one, he toggles the Ack line running back to the producer to make this manifest.6 Upon arrival of this confirmation, the producer does no longer need to hold the present data item but becomes free to withdraw or to overwrite it at any time. When the first data transfer comes to an end, both control lines are at 1, ready for a second transfer. Notification occurs with transition signaling and each data transfer operation involves one event on each of the two control lines. This explains why the non-return-to-zero (NRZ) protocol is also known as two-phase, two-stroke, and transition protocol. An implication is that it always takes two consecutive data transfers before the control lines return to their initial logic values. RZ or four-phase handshake protocol The alternative scenario depicted in fig.8.5d and e contrasts with the above in that the control lines get restored to their initial values between any two consecutive transfer operations, hence the name return-to-zero (RZ). Four-phase, four-stroke, and level protocol are synonyms as this variation is based on level signaling and involves two extra events. Although data rate is cut to half of what is achievable with a NRZ protocol at the same clock frequency, this approach is more common because the hardware tends to be somewhat simpler and easier to understand. Observe that latency also suffers from the extra two phases that must be traversed. With either signaling scheme, the handshake protocol makes sure that data vectors get sampled only while the electrical lines are kept in a stable and consistent state. The need for synchronization — together with the risk of metastability — is confined to two scalar control signals no matter how wide the data word is. Two-stage synchronizers are typically being used. Full handshaking works independently of the relative operating speeds of the subsystems involved.7 If, for instance, the producer were replaced by a much faster implementation, data transfers would continue correctly with no change to the consumer nor to the interface circuits. Although the producer would spend much less time in preparing a new data item, the mutual exclusion principle inherent in the proto- col would take care that data items do not get produced at a rate that is incompatible with the consumer. Observation 8.3. The strict sequence of events imposed by a full handshake protocol precludes any loss of data, confines synchronization to two control lines, and makes it possible to design communicating subsystems largely independently from each other. The latter property greatly facilitates the modular design of complex systems as nothing prevents VLSI designers from taking advantage of full handshaking to govern data transfer operations even when all subsystems involved sit in the same clock domain. On the negative side, the symmetric protocol requires that both subsystems involved be stallable. That is, they must be capable of withholding their data production and consumption activities for an indeterminate amount of time if necessary. This is not always possible, though. 6 Note that this does not necessarily imply that the processing of the last data item is actually completed. A pipeline, for instance, can accommodate new data as soon as the result from the processing of the previous item by its first stage has been properly stored in the subsequent pipeline register. There is no need for this data item to have finished traversing the second or any later stage. It should be obvious, however, that data must have propagated through any combinational logic placed between the producer’s output register and the first data register in the consumer before the acknowledge is asserted. 7 A questionable implementation where this is not the case is being discussed in problem 4 of section 8.6. 8.2 DATA CONSISTENCY IN VECTORED ACQUISITION 453 8.2.5 PARTIAL HANDSHAKING Sec+ Val- Req- Req2- Val+ Req+ Req2+ transfer Req Req2 ...... consumer, stallable subsystemproducer, spontaneous subsystem Data clock boundaryClkP ClkQ ww DQ ENA w DQ ENA (optional) synchronizer subcircuit scalar D A Req ClkP ClkQ producer consumer Data 1st transfer 2nd transfer 3rd transfer ClkP ClkQ producer consumer Req Data 1st transfer 2nd transfer ! finite state machine finite state machine Val+ = data valid Sec+ = data secured Val- = data withdrawn (optional) producer consumer push protocol: acknowledge reception request reception Val+ Val+ Sec+ Sec+ Req- Req2- even transfer Req+ Req2+ odd transfer (e) (c) (a) (b) (d) FIGURE 8.6 Partial handshaking in the occurrence of a ﬁxed-rate producer. Circuit (a), waveforms and STG with NRZ protocol (b,c), and the same for RZ protocol (d,e). Consider a digital data source operating with a fixed sampling rate. An example is given in fig.8.6a with an A/D converter acting as producer of audio samples. Such a data source works spontaneously and neither needs nor cares about protocols. Input terminal Ack thus becomes meaningless and must be dropped, thereby rendering the circuit and the protocol unsymmetrical. More importantly, a fixed-rate producer is not stallable. This is to say that it does not wait, thereby forcing its counterpart to always complete the data processing in time before the next data item becomes available. Data items may otherwise get lost. By making assumptions about the response time of the partners involved, partial handshaking becomes exposed to failure when these assumptions change. In the example of fig.8.6c, Sec+ must occur before Val+ under any circumstances, which imposes restrictions on the mutual relationship of producer and consumer clock rates and latencies. 454 CHAPTER 8 ACQUISITION OF ASYNCHRONOUS DATA It is important to understand that porting an existing design to a new technology, reusing part of it in a different context, or applying dynamic voltage and/or frequency scaling are likely to challenge such assumptions. As opposed to this, full handshaking scales with frequency and operating speed because no assumptions are made. Observation 8.4. Partial handshake protocols cannot function properly unless the response times of the communicating subsystems are a priori known and can firmly be guaranteed to respect specific relationships under all operating conditions. For reasons of symmetry, all the above also holds for the converse situation where it is the consumer that is of non-stallable nature, see fig.8.7. Note the presence of an Ack line and the absence of a Req. The typical failure mode here is that data items will get read multiple times by the consumer should its counterpart fail to deliver new data in time. Ack Ack2 ...... consumer, spontaneous subsystemproducer, stallable subsystem Data clock boundaryClkP ClkQ ww DQ ENA w DQ ENA (optional) synchronizer subcircuit scalar D A ! finite state machine finite state machine Val+ Sec+ Ack-Ack2- Ack+Ack2+ transfer Val- Val+ = data valid Sec+ = data secured Val- = data withdrawn (optional) (a) (b) FIGURE 8.7 Partial handshaking in the occurrence of a ﬁxed-rate consumer. Circuit (a) and STG with RZ protocol (b). 8.2.6 FIFO SYNCHRONIZERS The idea is to insert some form of first-in first-out (FIFO) queue between two clock domains where writing in occurs from one domain and reading out from the other. Some people seem to believe this does away with all synchronization problems. Be cautioned, however, that keeping track of the fill level is more tricky than in a synchronous FIFO where read and write pointers belong to one clock domain. FIFO synchronizers clearly are about asynchronous circuit design! Observation 8.5. Subtle design flaws in FIFO synchronizers may become manifest under highly specific delay and timing conditions exclusively and are, therefore, unlikely to be found with RTL and other simulations due to idealizing assumptions there. Fig.8.8 shows a proven architecture where a dual-port RAM sits across the clock boundary and gets operated as a cyclic buffer by a read and a write pointer located in the producer and consumer domains respectively.8 Observe that what gets actually synchronized are the memory pointers, not the data per se. 8 One might as well use a register file instead of a RAM. 8.2 DATA CONSISTENCY IN VECTORED ACQUISITION 455 WREN D1 D2 dual-port memory A1 A2 unit-distance-coded ClkP ClkQ synchronizer vectored synchronizer vectored clock boundary read pointer v+1 binary & Gray counter +1 bin to Gray DQ ENA DQ w/o MSB v+1 ClkQ_C memory location currently being read v+1 write pointer v-1 MSB & 2nd MSB DQ ENA binary & Gray counter +1 bin to Gray DQ w/o MSB v+1 ClkP_C FIFO sychronizer v w v w location where data get stored when says so WRINP == Data_DOData_DI Read_SIWrite_SI Empty_SOFull_SO RdPtr_SWrPtr_S WrAddr_D RdAddr_D FIGURE 8.8 FIFO synchronizer built around a dual-port memory (simpliﬁed).9 The generation of the Full and Empty flags deserves further explanations. As both the full and the empty conditions are identified by the read and write counters pointing to the same address in a circular buffer, something more must be consulted to tell the two situations apart. A common trick is to prefix the address pointers with an extra digit. Make sure to understand that this new MSB is used in comparing the pointers exclusively, it is not part of the memory address busses. If buffer capacity is 2v by w bit, then the pointer includes bits (v, v−1, ..., 1, 0) out of which (v−1, ..., 1, 0) actually serve to access the memory. Next the two pointer vectors must be made to the traverse the clock boundary without being jumbled. Let us use a Gray code. As one can easily verify, the two conditions then become: empty iff WrPtr = RdPtr full iff WrPtr(i) = RdPtr(i) for i = v and i = v−1and WrPtr(i) = RdPtr(i) for all other bits i = v−2, ..., 1, 0. There is one more difficulty. Simply using the lower bits (v−1, ..., 1, 0) from the v+1-bit pointers to address the memory will not work with Gray codes as the two pointers would not visit memory locations 9 Not shown is the reset mechanism which is straightforward to add. The two odd-looking icons near the top have been created as a more distinctive alternative for the box typically used for (arithmetic) comparators. The two dimples are meant to suggest the two pans of a kitchen scale. 456 CHAPTER 8 ACQUISITION OF ASYNCHRONOUS DATA in the same order, causing some data words to be duplicated and others to be lost. The reason is that truncating the MSB from the v+1-bit Gray code does not yield a v-bit Gray code because the lower bits are reflected rather than repeated, see footnote 3. One solution is to start from a v+1-bit Gray counter and to combine its MSB and 2nd MSB into the MSB of the v-bit code using an xor operation. Instead, the approach behind fig.8.8 is to come up with a combined counter that produces binary and Gray codes in pairs, the former being used for accessing the memory and the latter for pointer comparison purposes. The circuit shown here works by synchronizing the two pointers into the respective other clock domain. Variations of this architecture and more detailed advice can be found in [204] [205]. A somewhat leaner alternative where read and write pointers get compared asynchronously before synchronizing just the “full” and “empty” outputs is described in [206]. When compared to handshaking, FIFO synchronizers are more costly in terms of latency and circuit complexity. Their strength is that they can handle temporarily unequal rates of production and consumption with an amount of leeway proportional to buffer length v. Still, the producer must honor the “full” flag and never attempt to overstuff the queue, and vice versa for the consumer. This leaves the designer with two options. ◦ Producer and consumer must be arranged to converge to the same data rate before exhausting the buffer’s length, e.g. by making data production and consumption dependent on extra “nearly full” and “nearly empty” flags. ◦ Either the producer or the consumer can be ensured to operate consistently faster than the other, in which case it must be made stallable by its slower counterpart. What contributes to the popularity of FIFO synchronizers is that they are available as virtual and DesignWare components. As an aside, note that a synchronous FIFO queue can be useful for interfacing between major building blocks even when these are driven from a common clock. 8.3 DATA CONSISTENCY IN SCALAR ACQUISITION 457 8.3 DATA CONSISTENCY IN SCALAR ACQUISITION As the name suggests, scalar acquisition implies that a clock boundary is being traversed by just one line. It may appear surprising that the acquisition of a single bit should give rise to any problem that is worth mentioning, yet, there are a few subtle pitfalls. In order to better understand the peculiarities, let us first examine how unsophisticated schemes fail in the presence of asynchronously changing inputs before proceeding to more adequate approaches. 8.3.1 NO SYNCHRONIZATION WHATSOEVER In the circuit of fig.8.9a, a scalar input signal Data is being fed into two combinational subcircuits g and h that are part of a synchronous consumer circuit without any prior synchronization to the local clock ClkQ. Two deficiencies are likely to lead to system failure. Firstly, signals G and H emanating from g and h respectively will occasionally get sampled during the time span between contamination and propagation delay when their values correspond neither to the settled values from the past interval t nor to those for the upcoming interval t + 1.10 In the timing diagram of fig.8.9a such unfortunate circumstances apply to the central clock event. Secondly, even though G and H may happen to be stable at sampling time, they may relate to distinct time intervals if tcd g > tpd h. If so, an inconsistent set of data gets stored in the two registers before being passed on to the downstream circuitry for further processing. This undesirable situation typically occurs when one of the paths includes combinational logic whereas the other does not. For an example, check the rightmost clock event in fig.8.9a. 8.3.2 SYNCHRONIZATION AT MULTIPLE PLACES Adding synchronization flip-flops in front of all combinational subcircuits as in fig.8.9b improves the situation but does not suffice. This is because the flip-flops involved would sample the input slightly offset in time as a consequence from unbalanced delays along the paths Data → Datag and Data → Datah, clock skew, unlike switching thresholds, noise, etc. Every once in a while, input data would get interpreted in contradicting ways as depicted in the timing diagram. 10 Remember from section A.5 that combinational outputs do not necessarily transit from one stable value to the next in a monotonous fashion. Rather, they may temporarily assume arbitrary values due to hazards. 458 CHAPTER 8 ACQUISITION OF ASYNCHRONOUS DATA 8.3.3 SYNCHRONIZATION AT A SINGLE PLACE To stay clear of inconsistencies across multiple flip-flops, synchronization must be concentrated at a single place before any data are being distributed. This is the only way to make sure that all downstream circuitry operates on consistent data sampled at a single point in time. It is, therefore, standard practice to use synchronizers similar to those depicted in figs.8.9c and d. Observation 8.6. Any scalar signal that travels from one clock domain to another must be synchronized at one place by a single synchronization subcircuit driven from the receiving clock. When dealing with two complementary signals, it is best to transmit only one of them and to (re)obtain the complement after synchronization. 8.3.4 SYNCHRONIZATION FROM A SLOW CLOCK A synchronizer clocked at rate fclkq is bound to miss part of the input data unless all data pulses are guaranteed to last for at least one clock period 1/fclkq = Tclkq < min(tdata valid i). Note the incongruity of data rate and sampling rate in fig.8.9c and the ensuing data loss. Actually, the sampling period of the synchronizer must provide sufficient leeway to accommodate setup and hold times as well as data transients. What if the clock available on the consumer side is simply too slow? • Convert the data stream from its bit-serial into a bit-parallel format with the aid of a shift register, thereby making it possible to transmit data at a much lower rate. Then use one of the vectored acquisition schemes found to be safe. • Use an analog phase locked loop (PLL) to clock the synchronizer at a faster rate. • Recur to dual-edge-triggered one-phase clocking if clocking the synchronizer at twice its original frequency suffices. Even with the best synchronization scheme, an active clock edge and an input transition will occa- sionally coincide when signals get exchanged between two independent clock domains, see fig.8.9d. For the system to function correctly, the synchronizer must then decide for either one of the two valid outcomes as the downstream circuitry cannot handle ambiguous data. This is precisely the subject of the subsequent section. 8.3 DATA CONSISTENCY IN SCALAR ACQUISITION 459 poor clock boundary ClkQ Data synchronizerdelay...... h g ClkQ Data clock boundary very poor no synchronizer ...... h g H G payload circuit payload circuit better clock boundary Data ClkQ synchronizer ...... h g payload circuit clock boundary Data ClkQ synchronizer ...... h g payload circuit poor ! data loss as signal does not get sampled while in that state ClkQ Data timing violation as signal gets sampled while changing ClkQ ! Data ClkQ data get sampled while inconsistent gData hData gData hData ClkQ data get sampled while inconsistent Data G H (b) (a) (d) (c) FIGURE 8.9 Acquisition of scalar inputs (irrelevant details not shown). No synchronization (a), synchronization at multiple places (b), and synchronization at a single place with inadequate (c), and with adequate sampling rate (d) (recommended). 460 CHAPTER 8 ACQUISITION OF ASYNCHRONOUS DATA 8.4 MARGINAL TRIGGERING AND METASTABILITY 8.4.1 METASTABILITY AND HOW IT BECOMES MANIFEST Reconsider the various synchronization schemes discussed. What they all have in common is a flip- flop — or a latch — the data input of which is connected to the incoming signal. As no fixed timing relationship can be guaranteed between data and clock, the data signal will occasionally switch in the immediate vicinity of a clock event, thereby ignoring the requirement that input data must remain stable throughout a bistable’s data-call window. “What happens when an input change violates a bistable’s setup or hold condition?” Technically, this kind of incident is referred to as timing violation or marginal triggering.11 Until the mid 1970s, it was generally believed that the bistable would then decide for either of the possible two outcomes right away and output either a 0 or a 1. As stated in the introduction to this chapter, it took a long time until the design community began to understand this was not necessarily true. UQ UQ FIGURE 8.10 Measured behavior of a CMOS latch in response to marginal triggering (photo reprinted from [207] with permission). What had been observed were intermediate voltages and excessive delays before the two complementary outputs of the bistable eventually settled to a normal steady-state condition in response to marginal triggering. It was found that delays occasionally exceeded officially specified propagation delay figures by orders of magnitude, which made it clear that a better understanding of the process was imperative. We will, in the remainder of this chapter, summarize important results from empirical measurements and theoretical analysis of this phenomenon. 11 Please recall that clocked (sub)circuits not only impose • minimum setup and hold times tsu and tho but also • minimum clock pulse widths tclk lo min and tclk hi min, • maximum clock rise and fall times tclk ri max and tclk fa max,and • absence of runt pulses and glitches on clock and asynchronous (re)set inputs. Disregarding any such timing condition is likely to cause marginal triggering. The customary abstraction of flip-flops, latches and RAMs into bistable memory devices the behavior of which is entirely captured by way of truth tables, logic equations, and the like then no longer holds. 8.4 MARGINAL TRIGGERING AND METASTABILITY 461 Today’s digital subcircuits are inherently analog networks, and bistables are no exceptions. The data retention capability of latches, flip-flops, and SRAM cells is essentially obtained from closed feedback loops built from two inverting amplifiers. Fig.8.11b shows the transfer characteristics of two CMOS inverters where the output of either inverter drives the input of the other. Two stable points of equilibrium reflect the binary states 0 and 1 respectively. A third and unstable point of equilibrium exists in between. More precisely, the state space features two “valleys of attraction” separated by a line of unstable equilibrium. Marginal triggering implies bringing a bistable very close to that separation line before leaving it torecover. The bistableis thensaidtohang inanevanescent metastable condition.12 Q Q UQ U dd0 U dd 0 U dd1/2 metastable marginally triggered U dd1/2 stable stable \"1\" \"0\" QU (b) (a) FIGURE 8.11 Points of equilibrium and marginal triggering. Cross-coupled inverting ampliﬁers (a) and state space with superimposed transfer curves (b). Mathematical analyses of the metastable behavior of cross-coupled inverters can be found in the literature such as in [208] [207] [60]. While correctly modeling the behavior of the feedback loop, they do not necessarily describe the waveforms observable at the output terminals of actual latches and flip-flops. Similarly, node voltages half-way between zero and Udd, such as those depicted in fig.8.10, are not normally visible at the outputs of a library cell or physical component. This is because of the various auxiliary subcircuits inserted between the metastable memory loop itself and the I/O terminals. Output buffers, for instance, tend to restore intermediate voltages to regular logic 0sand 1s. Device characteristics and physical layout also matter. Fig.8.12 shows how metastable conditions normally become manifest in various bistables. Eventually the circuit returns to either of the two states of stable equilibrium. Yet, not only is the outcome of the decision process unpredictable, but the time it takes to recover from a metastable equilibrium condition necessarily exceeds the bistable’s customary propagation delay, see fig.8.13. In fact, metastability resolution time has been reported to outrun propagation delay by orders of magnitude on occasion, so that we must write tmr > tpd and sometimes even tmr ≫ tpd. Most alarming are the facts that it is not possible to guarantee any upper bound for the time a bistable takes to recover 12 A philosophical concept known as Buridan’s Principle states that a discrete decision based upon input having a continuous range of values cannot be made within a bounded length of time. It is named after the fourteenth century philosopher Jean Buridan who claimed that a donkey placed at the same distance from two bales of hay would starve to death because it had no reasons to choose one bale over the other. 462 CHAPTER 8 ACQUISITION OF ASYNCHRONOUS DATA and that the behavior of a physical part is no longer consistent with the one published in the datasheet and captured in the simulation model. Observation 8.7. Metastability is a problem because of unpredictable delay, not because of unpre- dictable logic outcome. While it is true that actual behavior greatly varies from one type of bistable to the next, it should be understood that the phenomenon of metastability is a fundamental one observed independently from circuit and fabrication technology. There are currently no flip-flops or latches that are free of metastable behavior. CLK D data-call window tpd tmr Q Q Q Q Q Q Q Q a timing violation non-complementary voltages possibly combined with glitching results in a metastable condition that becomes manifest as or oscillations outputs hovering within the forbidden interval or any of which gets depicted as or excessive delay Q CMOS feedback loop TTL feedback loop and CMOS with substantial delay in feedback loop typical behavior for CMOS latch master-slave flip-flop observable at cell outputs confined to inside loop FIGURE 8.12 Patterns of metastable behavior in response to timing violations. 8.4 MARGINAL TRIGGERING AND METASTABILITY 463 8.4.2 REPERCUSSIONS ON CIRCUIT FUNCTIONING As a consequence from a synchronizer hanging in the metastable condition, ◦ The downstream circuitry may process wrong data, or ◦ May be presented with voltages within the forbidden interval, or ◦ May find itself short of time for settling to a steady-state value before the next clock event arrives, see fig.8.14. In the latter two cases, some downstream storage elements will be subject to marginal triggering themselves thereby permitting metastability to spread further into the clock domain. In any of the three cases, part of the system is likely to malfunction. Warning example The erratic behavior that Molnar and his colleagues had observed with their laboratory computer even after they had added a flip-flop for synchronizing the external input was in fact the result of metastability in the program counter. The problem was exacerbated by the slow germanium transistors then available and the poorly designed flip-flop circuits. Yet, it is unfair to blame the circuit designers for this as nobody was aware of metastability and of the importance of quick recovery in synchronizers at the time. \u0002 Observation 8.8. Metastability together with its fatal consequences is completely banned from within a clock domain iff care is taken to meet all timing requirements under all operating conditions. As opposed to this, there is no way to truly exclude metastability from occurring at the boundaries of independently clocked domains. 464 CHAPTER 8 ACQUISITION OF ASYNCHRONOUS DATA tmr new input properly latched [ps] 200 400 observed delay from active clock edge until bistable settles to new stable state tpd as published in data sheet and simulation model 600 800 1000 1200 active clock edge data call [ps]100 200−200 tsu tho −100 0 published in data sheet −300 0 excessive delaysynchronizationfailure Tclk tsu ff dnst tpd cmax( ) tal difficult to predict timingwise variability tpd as observed when input settles well ahead of active clock edge new input missed, but previous state safely kept headroom outcome unpredictable tpd time available for recovering from a metastable condition timing of input change wrt to active clock edgesafety margin FIGURE 8.13 Measured timing characteristics of a bistable when input data change in the vicinity of the data-call window (arbitrary numerical data). 8.4.3 A STATISTICAL MODEL FOR ESTIMATING SYNCHRONIZER RELIABILITY As metastability has been found to be unavoidable in synchronizers, it is essential to ask “How serious is the metastability problem really?” As illustrated by the two quotes below,13 even experts do not always agree. “Having spent untold hours at analyzing and measuring metastable behavior, I can assure you that it is (today) a highly overrated problem. You can almost ignore it.” “Having spent untold hours debugging digital designs, I can assure you that metastable behavior is a real problem, and every digital designer had better understand it.” Within their respective contexts, both statements are correct which indicates that we must develop a more precise understanding of the phenomenon. Theoretical and experimental research has come up with statistical models, that can serve as a basis for estimating the probability of system failures 13 By Peter Alfke and Bruce Nepple respectively. 8.4 MARGINAL TRIGGERING AND METASTABILITY 465 timing conditions metastability D Q CLK resolved in time violated tpd none tsu tho respected tpd persisting tpd tpd respectedviolated none metastablesettled in transit data-call windowclocked bistable ! CLK DQ edge-triggered level-sensitive or FIGURE 8.14 Recovery from marginal triggering and the impact on downstream circuitry. due to metastability. Consider a flip-flop driven by a clock of frequency fclk that is connected to an asynchronous data signal with an average edge rate of fd. We will speak of a synchronization failure whenever a metastable condition persists for longer (after the synchronizer has been clocked) than the situation allows, i.e. when tmr > tal.The mean time between errors (MTBE) for such an arrangement has been found to obey the general law tMTBE = eK2 tal K1 fclk fd (8.1) where parameters K1 and K2 together quantify the characteristics of the synchronizer flip-flop with respect to marginal triggering and metastability resolution [208] [209]. K2 is an indication of the gain- bandwidth product around the memory loop of its master latch. K1 represents a timewise window of susceptibility for going metastable. clock boundary ClkQ K1 K2 ..... payload circuit tsu ff dnsttpd c synchronizer Data FIGURE 8.15 Circuit diagram of single-stage synchronizer. Now consider a single-stage synchronizer like the one of fig.8.15 and ask yourself for how much time the first flip-flop is allowed to rave in an out-of-the-normal condition for the downstream logic to stay clear of trouble? The timewise allowance for resolving metastability tal is simply the clock period Tclkdiminished by the setup time of the downstream flip-flop(s) tsu ff dnst and the propagation delay 466 CHAPTER 8 ACQUISITION OF ASYNCHRONOUS DATA max(tpd c) along the longest path through any combinational circuitry inserted between the flip-flops. You want to refer back to fig.8.13 for an illustration. tal = Tclk − max(tpd c) − tsu ff dnst with Tclk = 1 fclk (8.2) Example The metastability characteristics of a flip-flop in a XC2VPro4 FPGA from the Xilinx Virtex II Pro family are listed in table 8.1 for typical operating conditions. For fclk = 100 MHz, fd = 10 MHz, tsu ff dnst = 0.5 ns and tpd c = 4 ns, the calculated MTBE exceeds the age of the universe. Just doubling the clock frequency reduces the MTBE to a mere 4 s, which demonstrates that the impact of metastability is extremely dependent on a circuit’s operating speed. Whether the single-stage synchronizer of fig.8.15 is acceptable or not, thus turns out to be a quantitative question. \u0002 8.4.4 PLESIOCHRONOUS INTERFACES Being a statistical model, (8.1) assumes that there is no predictable relationship between the frequencies and phases of the data and clock signals. This does not always apply, though. A notable exception are plesiochronous systems where data producer and consumer are clocked from separate oscillators that operate at the same nominal frequency. Think of a local area network (LAN), for instance. Once clock and data have aligned in an unfortunate way at the receiver end, successive data updates are subject to occur during the critical data-call window many times in a row, thereby rendering the notion of MTBE meaningless. Observation 8.9. Plesiochronous interfaces are exposed to burst-like error patterns and are not amenable to analysis by simple statistical models that assume uncorrelated clocks. Plesiochronous interfaces require some sort of self-regulating mechanism that avoids consecutive timing violations and repeated misinterpretation of data. A tapped delay line may be used from which an adaptive circuit selects a tap such as not to sample a data signal in the immediate vicinity of a transient [210]. Two related ideas are adaptively shifting the data or the clock signal via a digitally adjustable delay line [211], and oversampling the input data before discarding unsettled and duplicate data samples. None of these approaches is free of occasional coincidences of clock events and data changes, however. The problem is just transferred from the data acquisition flip-flop to the subcircuit that adaptively selects a tap or controls sampling time. 8.4.5 CONTAINMENT OF METASTABLE BEHAVIOR Limiting the harmful effects of metastability is based on insight that directly follows from (8.1)and (8.2). Keep the number of synchronization operations as small as possible and allow as much time as practical for any metastable condition to resolve. More specific suggestions follow. 8.4 MARGINAL TRIGGERING AND METASTABILITY 467 Estimate reliability at the system level Having a fairly accurate idea of the expected system reliability is always a good starting point. It makes no sense to try to improve synchronization reliability further when the MTBE already exceeds the expected product lifetime. However, always remember that (8.1) and all further indications in this text refer to one scalar synchronizer and that a system may include many of them. Also keep in mind that statistical models do not apply to plesiochronous operation. In practice, a frequent problem is that only few bistables come with datasheets that specify their metastability resolution characteristics. Luckily, there exists a workaround. Observation 8.10. As a rule of thumb, synchronization failure is highly infrequent if a flip-flop is allowed three times its propagation delay or more to recover from a metastable condition. In the absence of numerical K1 and K2 values, refraining from detailed analysis is probably safe if the application is not overly critical and if tal ≥ 3 tpd can be guaranteed throughout. Select ﬂip-ﬂops with good metastability resolution Flip-flops optimized for synchronizer applications feature a small K1 and, above all, a large K2.They shall be preferred over general-purpose flip-flops with inferior or unknown metastability recovery char- acteristics.14 Unfortunately, component manufacturers and library vendors continue to be extremely reticent to disclose quantitative metastability data.15 Table 8.1 Metastability resolution characteristics of various CMOS flip-flops. Note: The figures below have been collected from distinct sources and do not necessarily relate to the same operating conditions. Still, a massive improvement over the years is evident. D-type ﬂip-ﬂop Metastability reference or vendor cell type or name technology F K 1 K 2 [nm] [ps] [GHz] Horstmann et al. [208] n.a. std cell 1500 47 600 3.23 VLSI Technology DFNTNS std cell 800 140 000 12.3 Ginosar [211] n.a. “conservative” std cell 180 50 100 Xilinx [209] XC2VPro4 CLB FPGA 130 ≈ 100 27.2 14 [212] finds that static bistables should be preferred over their dynamic counterparts. 15 There are two reasons for this. For one thing, the issue is no longer perceived as urgent now that the speed and metastability resolution characteristics of flip-flops have improved so much when compared to older fabrication technologies. For another thing, it takes a considerable effort and degree of sophistication to properly determine the K1 and K2 parameters for a cell library. The burden of doing so is thus left to VLSI designers in critical high-speed applications, see [208] [209] [213] for measurement set-ups. It is important that such characterizations be carried out under operating conditions that are as identical as possible to those actually encountered by the synchronizer when put into service in the target environment. Relevant conditions include capacitive load, layout parasitics, clock slew rate, fabrication process, and, last but not least, PTV conditions. 468 CHAPTER 8 ACQUISITION OF ASYNCHRONOUS DATA clock boundary ClkQ synchronizer ..... payload circuit Data FIGURE 8.16 Two-stage synchronizers obtained from adding an extra ﬂip-ﬂop. Remove combinational delays from synchronizers Recall from (8.2) that any extra delay tpd c between two cascaded flip-flops is at the expense of recovery time tal for the first bistable. As a consequence, the one-stage synchronizer circuit of fig.8.15 is far from being optimal at high clock rates. When the MTBE proves insufficient, a better solution must be sought. Two flip-flops cascaded with no combinational logic in between extend the time available for metasta- bility resolution to almost an entire clock period. The low error rates typically obtained in this way have contributed to the popularity of two-stage synchronizers shown in fig.8.16. In case the additional cycle of latency resulting from the extra flip-flop proves unacceptable, try to reshuffle the existing registers or check [214] for a proposal that operates multiple two-stage synchronizers in parallel. Drive synchronizers with fast-switching clock Experience has shown that clocking synchronizers with fast slew rates tends to accelerate recovery from a marginal triggering condition [208]. What’s more, overly slow clock ramps tend to dilate setup and hold times beyond their nominal values as obtained from library characterization under the assumption of a sharp clock edge with zero or close-to-zero ramp time. Yet, these are the figures stored in simulation models and listed in datasheets on which design engineers necessarily base all their reasoning. Free synchronizers from unnecessary loads Not surprisingly, capacitive loading has been found to slow down the metastability resolution process in a bistable. It is therefore recommended to keep the loads on synchronizer outputs as small as possible by using buffers and buffer trees where necessary. Lower clock frequency at the consumer end As a minor change of the clock frequency has a large impact on the MTBE, it is always worthwhile to check whether it is not possible to operate the entire consumer from a somewhat slower clock.16 16 A more exotic proposal is the concept of a pausable clock, where metastability is detected by way of analog circuitry designed for that purpose, and where the consumer’s clock is frozen until it has been resolved [215]. 8.4 MARGINAL TRIGGERING AND METASTABILITY 469 Use multi-stage synchronizers In those — extremely infrequent — situations where a two-stage synchronizer does not leave enough time for metastability to resolve, the available time span can be extended well beyond one clock period by recurring to synchronizer circuits that make use of multiple flip-flops. Please refer to [215] where the merits of cascaded and clock-divided synchronizers are evaluated. Keep feedback path within synchronizers short Digital VLSI designers normally work with predeveloped cell libraries. In case you must design your own synchronizers at the transistor level, consult the specialized literature on the subject [216] [217] [208] [207] [215]. For the purpose of analysis, the two cross-coupled amplifiers can be replaced by a linear model in the vicinity of the metastable point of equilibrium. As a rule, the internal feedback path should be kept as fast as possible. A fast master latch is desirable because a high gain-bandwidth product tends to improve recovery speed, K2 and MTBE. This is also why a higher supply voltage has been found to be beneficial. A variety of misguided approaches to synchronization, such as a “metastability blocker” or a “pulse synchronizer”, for instance, are collected in [211]. 470 CHAPTER 8 ACQUISITION OF ASYNCHRONOUS DATA 8.5 SUMMARY • Asynchronous interfaces give rise to two problems, namely jumbled data and metastability. While it is always possible to avoid data inconsistencies altogether by making use of adequate data acquisition schemes and protocols, the metastability problems that follow from marginal triggering can be tackled in a probabilistic fashion only. • Metastability is often put forward as a welcome explanation for synchronization failures because of its unavoidable and unpredictable nature. Yet, we tend to believe that most problems actually result from data consistency problems that have been overlooked. • Metastability becomes a threat to system reliability when synchronizers are being operated close to their maximum clock and data frequencies, and/or when synchronizers are involved in very large quantities. Circuits that can afford a few extra ns of the clock period for the synchronizers to recover should be safe, on the other hand. Two-stage synchronizers normally prove more than adequate in such situations. • Do not expect functional simulation or testing to uncover many synchronization problems as fatal flaws may pass unnoticed. In addition to the usual coverage problem, provoking a synchronization failure may require many repeated runs that just differ in relative shifts of events in the sub-ns range. This is neither efficient nor does it inspire much confidence. A procedure for finding which clock domain crossings in a circuit may need to be improved is proposed in [218]. • To steer clear of the imponderabilia and the extra costs associated with synchronization, implement the rules below. - Eliminate uncontrolled asynchronous interfaces wherever possible, that is partition a system into as few clock domains as technically feasible. - If you must traverse a clock boundary, do so where data bandwidth is smallest. - Estimate error probability or mean time between errors at the system level. - Whenever possible, set aside some extra time for synchronizers to settle. - Within each clock domain, strictly adhere to a synchronous clocking discipline. - Avoid (sub)circuits that tend to fail in a catastrophic manner when presented with corrupted data. 8.6 PROBLEMS 471 8.6 PROBLEMS 1. ∗ What is wrong with the two-stage synchronizer circuit of fig.8.17? clock boundary ClkQ synchronizer ..... payload circuit w Data FIGURE 8.17 Bad synchronizer circuit. 2. ∗ Reconsider Molnar’s original circuit of fig.8.1a and recall that the computer failed when its program counter became filled with a bogus address as a consequence from a synchronization failure. At first sight, it appears that only a scalar signal is being acquired from externally, so that there should be no chance for an inconsistent address word to develop. Find out why this is not so. 3. ∗∗ Establish detailed state diagrams for the two finite state machines in fig.8.5. Generate all necessary control signals and try to keep latency small. Depending on how the interfaces with the surrounding circuitry are defined, there may be more than one acceptable solution. Can you design the models such as to minimize the differences in the HDL codes of two- and four-phase protocols? 4. ∗∗ Fig.8.18 shows an arrangement that has a long tradition for carrying data vectors from one clock domain to another. What sets it apart from the handshake circuit of fig.8.5 is a bistable that sits right on the clock boundary. This has earned the circuit names such as “shared flip-flop” or “signaling latch” synchronizer although an unclocked data-edge-triggered seesaw is typically being used (a level-sensitive seesaw is sometimes also found). The shared bistable functions as a flag, set by the producer and reset by the consumer, that instructs one partner to carry out its duty and the other to wait. Much as in fig.8.5, each of the two control signals gets accepted into the local clock domain by a standard two-stage synchronizer. Compare the two circuits and their detailed operation. The correct functioning of the circuit of fig.8.18 rests on an assumption that may or may not hold in real world applications, however. Find out what that assumption is. Establishing a signal transition graph (STG) may help. Hint: Consider situations where the two clock frequencies greatly differ from each other. 472 CHAPTER 8 ACQUISITION OF ASYNCHRONOUS DATA ...... consumer, synchronous subsystemproducer, synchronous subsystem Data clock boundaryClkP ClkQ ww DQ ENA DQ ...... w DQ ENA (optional) SQ RQ SQ RQ Val Req2 Ack Ack2 Req Sec synchronizer subcircuit scalar synchronizer subcircuit scalar DQ ! FIGURE 8.18 Vectored synchronization on the basis of a shared bistable. 5. ∗ In fig.8.13, three points are marked by empty circles or a propeller respectively. Explain what the three points have in common. What sets the propeller apart from the other two marks?","libVersion":"0.5.0","langs":""}