{"path":"sem4/CN/VRL/extra/slides/CN-s20-probabilistic-techniques.pdf","text":"Computer Networks: Probabilistic Techniques in Networking Adrian Perrig Partially based on slides by Markus Legner and Ankit Singla Photo: ETH ZÃ¼rich / Gian Marco Castelberg Plan for this lecture 2 Load balancing Membership testing â€¢ Application: duplicate detection â€¢ Based on hash functions and Bloom filters Traffic monitoring â€¢ General-purpose measurement: Ciscoâ€™s (original) NetFlow â€¢ Identifying large flows: elephant detectors â€¢ Estimating the number of flows: probabilistic counting Load balancing 3 You run a popular network application â€¢ Application is deployed on multiple servers â€¢ Requests hit â€œload balancerâ€ â€¢ Any server can handle any request â€¢ How to distribute requests? â€¢ Goal: - Keep response time uniformly low - Uniform load at servers 4â€¦ Server image (on many slides): RRZEicons via Wikimedia Round-robin? Sequential distribution First request to S1 Second to S2 â€¦ What if requests follow a pattern? 5â€¦ 1 2 n-1 n Round-robin? Sequential distribution First request to S1 Second to S2 â€¦ What if requests follow a pattern? What if there are multiple load balancers? 6â€¦ 1 2 n-1 n Send to the least loaded server? Query / track server load Send request to least loaded server Overhead of querying / tracking 7â€¦ 1 2 n-1 n Send to the least loaded server? Query / track server load Send request to least loaded server Overhead of querying / tracking What if there are multiple load balancers? 8â€¦ 1 2 n-1 n Randomization to the rescue? â€¢ Pick a server uniformly at random â€¢ How often does imbalance occur? 9â€¦ 1 2 n-1 n ïƒ  â€œBalls into binsâ€ problem â€¢ Balls: requests â€¢ Bins: servers ğ‘š balls into ğ‘› bins 10 â€¦ 1 2 3 n Each ball is put into a bin chosen uniformly at random ğ‘š balls into ğ‘› bins 11 â€¦ 1 2 3 n Q1: Probability that ball ğ‘– and ğ‘— land in the same bin? A: ! \" ğ‘š balls into ğ‘› bins 12 â€¦ 1 2 3 n Q2: What is the expected number of pairwise (2-way) collisions? Hint: ğ‘‹#$ = 1 if ğ‘– and ğ‘— collide, 0 otherwise. E âˆ‘ ğ‘‹#$ = âˆ‘ P ğ‘‹#$ = 1 A: ! \" â‹… ğ‘š 2 = % %&! '\" ğ‘š balls into ğ‘› bins 13 â€¦ 1 2 3 n Q3: How many balls will cause this expectation to exceed 1? A: Given any ğ‘›, we can solve % %&! '\" = 1 ïƒ  ğ‘š â‰ˆ 2ğ‘› ğ‘› = 365 means ğ‘š = 28 â€” â€œBirthday paradoxâ€ ğ‘› balls into ğ‘› bins 14 â€¦ 1 2 3 n Q4: What is the expected maximum load on any bin? How do we improve this? A: O ()* \" ()* ()* \" ğ‘› = 2'!: '! + Power of two choices 15 â€¦ 1 2 3 n Instead of picking one random bin, pick two. Then query their load, and use the less loaded one. O ()* \" ()* ()* \" ïƒ  O ()* ()* \" ()* ' ğ‘› = 2'!: '! + ïƒ  + ()* ' Power of ğ‘š choices 16 â€¦ 1 2 3 n Instead of picking one random bin, pick ğ‘š. Then query their load, and use the less loaded one. O ()* \" ()* ()* \" ïƒ  O ()* ()* \" ()* ' ğ‘› = 2'!: '! + ïƒ  + ()* ' ğ‘š ğ‘š Power of ğ‘š choices 17E[maxload] ğ‘› Only approximations here, without constants from the big-O Logarithmic gapRandom Constant gap Diminishing gain 2 choices 3 choices 4 choices Sometimes fully random is not ideal Your application is interactive User requests randomized to servers â€¢ Interaction 1 with server a â€¢ Interaction 2 with server b â€¢ b: hi, how can I help you? Randomize over sessions? 18â€¦ 1 2 n-1 n Hash functions to the rescue server id = H(user-id) % ğ‘› Hash function is assumed uniform â€¢ Standard hash functions are good enough Preserves session continuity! Applied to network path selection 19â€¦ 1 2 n-1 n + 1 Basis of common traffic balancing Can be applied to network path selection â€¢ â€œEqual-cost multi-pathâ€ (ECMP) Use hash of flow 5-tuple: â€¢ H(src-IP, dest-IP, src-port, dest-port, protocol) Caution: H() is a deterministic function 20Image: packetmischief.ca What if a server fails? Rename all servers server id = H(user-id) % (n - 1) Will re-assign most sessions! 1 âˆ’ ! \" How to prevent this? ïƒ  â€œConsistent hashingâ€ 21â€¦ 1 n-2 n-1 2 n-1 n + 1 Consistent hashing â€¢ Hash the server IDs also to the same space! â€¢ Map each session to the â€œsuccessorâ€ server 22 ? #00â€¦0 #ffâ€¦f Consistent hashing 23 #00â€¦0 #ffâ€¦f Consistent hashing 24 Consistent hashing with multiple server hashes Give servers multiple IDs to improve failover ïƒ  Clients of failed server are distributed over multiple servers 25 #00â€¦0 #ffâ€¦f Consistent hashing: two readings Section 1 here: http://theory.stanford.edu/~tim/s17/l/l1.pdf [Roughgarden & Valiant] In use at Vimeo: https://medium.com/vimeo-engineering-blog/improving-load-balancing- with-a-new-consistent-hashing-algorithm-9f1bd75709ed 26 Membership testing 27 I want to â€¦ â€¦ check if an object is in my CDN cache â€¦ check if a TLS certificate has been revoked â€¦ check if I have seen this packet before (replay suppression and loop detection) 28 ïƒ  Membership testing: given some object and a set of objects, check if that object is contained in the set Algorithmen und Wahrscheinlichkeit, Angelika Steger & Emo Welzl Duplikate finden in grossen DatensÃ¤tzen A&W Lecture 15_02 Precision vs. accuracy â€¢ Accurate: Average of measurements is close to the real value (low bias) â€¢ Precise: Measurements are close to each other (low variance) 29https://archive-media.formlabs.com/upload/accuracy-precision-3d-printing.jpg Different types of errors 31 Actually Positive Actually Negative Positive Prediction True Positive (TP) False Positive (FP) Negative Prediction False Negative (FN) Different types of errors False-positive rate: FP / (FP + TN) 32 Actually Positive Actually Negative Positive Prediction True Positive (TP) False Positive (FP) Negative Prediction False Negative (FN) True Negative (TN) Hash tables? 33 [ Jorge Stolfi via Wikimedia ] Problem: need to store all the entries A yes/no version of a hash table How to handle collisions? â€¢ Ignore them! False positives? â€¢ Reduce rate and live with it â€¢ OK for many applications! How can we improve this? â€¢ Use multiple hash functions instead of just one ïƒ  Bloom filter 34 0 1 0 0 1 1 0 0 1 0 Bloom filter Setup: bit vector V with m bits, V = 0 Insert element e: â€¢ Compute k hash functions hi = Hi(e), where 0 < hi â‰¤ m â€¢ Set all bits V [hi] = 1 Test if element eâ€™ has already been seen: â€¢ Recompute k hash functions, check all V [hi] â€¢ If all bits are 1 ïƒ  â€œseen beforeâ€, otherwise â€œnewâ€ 35 Advantages of Bloom filters: no FN, constant-time insertion and test Bloom filter Initialization: Insertion of e: H1(e1) = 5, H2(e1) = 2 Test of e2 = e1: H1(e2) = 5, H2(e2) = 2 â‡’ seen before Test of e3 â‰  e1: H1(e3) = 5, H2(e3) = 1 â‡’ new Test of e4 â‰  e1: H1(e4) = 2, H2(e4) = 5 â‡’ seen before ?! 36 0 0 0 0 0 0 0 0 Bit Vector Index 8 1 0 0 0 1 0 0 1 0 Bit Vector 8 1 Bloom filter â€¢ n elements to represent â€¢ m bits of memory in table V â€¢ k hash functions H1, H2, â€¦, Hk â€¢ hash range {0, 1, â€¦, m-1} 37 Q: After n insertions, what is the probability that V[i] = 0? A: 1 âˆ’ ! % +\" â‰ˆ e&+\"/% Broder & Mitzenmacher: Network Applications of Bloom Filters Bloom filter â€¢ n elements to represent â€¢ m bits of memory in table V â€¢ k hash functions H1, H2, â€¦, Hk â€¢ hash range {0, 1, â€¦, m-1} 38 Q: Whatâ€™s the false-positive rate, P[FP]? (Given an item not in the set, the probability to predict it to be in the set) A: P FP â‰ˆ 1 âˆ’ e& â„+\" % + Bloom filter â€¢ n elements to represent â€¢ m bits of memory in table V â€¢ k hash functions H1, H2, â€¦, Hk â€¢ hash range {0, 1, â€¦, m-1} 39 Q: What value of k minimizes this false positive probability? A: ğ‘˜âˆ— â‰ˆ % \" ln 2 â‰ˆ 0.7 % \" Bloom filter â€¢ n elements to represent â€¢ m bits of memory in table V â€¢ k hash functions H1, H2, â€¦, Hk â€¢ hash range {0, 1, â€¦, m-1} 40 Q: What is this minimized false positive probability? A: P FP â‰ˆ 2 & \" # (/ ' Bloom filter: false-positive rate â€¢ This is still O(n) â€¢ Whatâ€™s the benefit? - Constants differ: elements themselves may be much larger than 10 bit 41 Under 1% with 10 bits / elementP[FP] m / n Bloom filter size examples n = 106 p = 1% (FP probability) m = 9.6 Ã— 106 bits ~ 1.2 MB k â‰ˆ 6.6 \u0000 7 p = 0.1% m = 14.4 Ã— 106 bits ~ 1.8 MB k = 10 42 â€¢ Problem: Bloom filter â€œfills upâ€ â€¢ Simple approach: reset Bloom filter periodically â€¢ Problems with simple approach? - Some false detections: fundamental problem with Bloom filters - Fail to provide â€œno FNâ€ guarantee: some duplicates are not detected! - If duplicate arrives after reset, then it is not detected as duplicate â€¢ Solution: primary and secondary filter - Only insert into primary filter - Check both filters when testing - Periodically empty secondary filter and swap them How to find duplicates in practice? 43 How to find duplicates in practice? 44 t I0 I1 I2 I3 I4 I5 Set Check Reset D0 D1 D2 D3 D2 D1 Expired FP TP Example application: cache filtering Cache on second request Avoid â€œone-hit wondersâ€ 45 [ Adapted from Maggs & Sitaraman: Algorithmic Nuggets in Content Delivery] Number of accesses % of objects Example application: cache filtering 46 Log objects requested in a Bloom filter. Check filter: Yes â†’ cache it. No â†’ donâ€™t cache, but log. Cache on second request Avoid â€œone-hit wondersâ€ Example application: cache filtering 47 DateByte hit rate (%) Cache filtering enabled Example application: cache filtering 48 DateDisk writes per second Cache filtering enabled Quick Summary of Bloom Filters 49 â€¢ Basic operations - Insert an element - Membership test â€¢ Properties - No FN, low FP - Time efficient: O(1) insertion and membership checking - Space efficient: constant bits per element given FP rate - But still O(n) memory overhead â€¢ Challenges of duplicate detection in practice Traffic monitoring 50 Applications of traffic monitoring 51 â€¢ Volume-based (denial of service â€“ DoS) or port scans â€¢ ISP wants to enforce quality of service or maximal sending rate Anomaly detection â€¢ Accounting, such as usage-based pricing - Often done with deterministic methods â€¢ Traffic engineering, such as balancing traffic loads Network management Image: http://senseable.mit.edu/nyte/visuals.html On what granularity do we perform monitoring? ïƒ  Traffic flows 52 â€¢ A set of packets with common values in one or more header fields (or a flow identifier) observed in a given time interval â€¢ A flow identifier (FID) is typically a 5-tuple: {Src IP, Dst IP, Src Port, Dst Port, Protocol} â€¢ IPv6 has an explicit 20bit â€œFlow Labelâ€ Whatâ€™s a flow? â€¢ DDoS detection: {*, Dst IP, *, *, *} â€¢ Source bandwidth monitoring for usage- based pricing: {Src IP, *, *, *, *} Some applications are interested in a subset of header fields: Why is traffic monitoring difficult? 53 â€¢ Core routers in the Internet forward multiple terabits of traffic per second - Links of 40/100 Gbps are widespread; 400 Gbps are being deployed - Routers can have dozens of these links â€¢ Empirical evidence: 20 million different flows on a 1 Tbps router CISCO ASR 9922 https://twitter.com/olesovhcom/status/215452504614379521 Why is traffic monitoring difficult? â€¢ Consider a 1 Tbps router and 1KB packets â€¢ 1 Tbps â‰ˆ 240 bit/s, 1KB = 213 bit â†’ 240 / 213 = 240-13 = 227 â‰ˆ 108 = 100 million packets per second â†’ One packet every 10 ns - 10 ns: one AES operation, light travels 3 m - DRAM lookup takes ~70 ns â†’ Packet processing must be extremely efficient 54 Traffic grows fast ïƒ  expensive to add additional memory Probabilistic traffic monitoring 55 â€¢ Probabilistic traffic monitoring can reduce overhead â€¢ Input: network traffic, â€¢ Output: estimate of traffic statistics (e.g., #of packets in a flow) Intuition: trade accuracy/precision for efficiency â€¢ Measure with limited memory/processing â€¢ Output an â€œaccurateâ€ estimate with high probability - Accurate: close to the real value (the output of the naÃ¯ve per-flow-counter algorithm) Challenges S Overview of probabilistic monitoring 56 Probabilistic Monitoring Algorithm Traffic Dataset R Dataset Dataset R2 R3 â€¢ Router (R) summarizes traffic into a compact dataset â€¢ (Optional) R periodically reports the dataset to a server (S) â€¢ (Optional) S estimates certain statistics based on multiple datasets Warmup exercises 57 Answer: take one (pseudo-) random value ğ‘£ âˆˆ [0,1) ïƒ  Select individual ğ‘– if (ğ‘– âˆ’ 1) / ğ‘› â‰¤ ğ‘£ < ğ‘– / ğ‘› Situation 1: population of known size n Task: select one individual with uniform probability 1/ğ‘› Warmup exercises 58 Answer: replace the currently â€œcachedâ€ individual by the ğ‘–th individual with probability 1/ğ‘– â€¢ Select last â€œcachedâ€ individual â€¢ Fact: after ğ‘› individuals, probability of selection is exactly 1/ğ‘› for each one Situation 2: Population of unknown size ğ‘› â€¢ You see every individual exactly once â€¢ There is a â€œcacheâ€ where you can temporarily keep exactly one individual Task: select one individual with uniform probability 1/ğ‘› Traffic monitoring General-purpose measurements 59 Sampled NetFlow Standard NetFlow vs. Sampled NetFlow â€¢ Standard NetFlow samples every packet â€¢ Sampled NetFlow samples every k-th packet Keep flow entry {Cpkt, Cbyte} for each flow and update it with the sampled packet â€¢ Cpkt = # of sampled packets â€¢ Cbyte = # of sampled bytes Estimate number of packets & bytes of a flow by multiplying recorded values by k â€¢ Ã±pkt= k * Cpkt; Ã±byte= k * Cbyte â€¢ Example below when k = 3: the estimate of F1 is {6, 9}, while the real value is {5, 8} 60 F2F1F1 F1 F1 F2 F2 update F3 F1F3F1 F1 = {1, 1}F1 Flow cache = {1, 1} = {1, 1} F1 F2 = {2, 3} = {1, 1} F1 F2 Estimation = {6, 9} = {3, 3} = {0, 0} F1 F2 F3 estimate Analysis of sampled NetFlow 61 â€¢ Simple to implement â€¢ Reduced processing time (process only a subset of packets) Advantages â€¢ Memory overhead: one entry per flow in the worst case â€¢ Imprecise estimate, especially for short- lived flows - Most precise for flows sending many small packets Disadvantages Issues with General-Purpose Measurement 62 â€¢ General-purpose flow measurement is either imprecise or infeasible Observation Focus on specific traffic information â€¢ Large flows â€¢ Subpopulations (small-volume flows) â€¢ Number of flows â€¢ Address access patterns â€¢ Per-source behavior of small flows â€¢ Flow distribution Solution Traffic monitoring Large-flow (elephant) detectors 63 Focus on the elephants, ignore the mice 64 What are large flows? â€¢ Flows that consume more than a given threshold of link capacity during a given measurement interval â€¢ For example, flows that take more than 1% of link capacity Rise of the Hyper Giants [Arbor Networks 2009]:1 Already in 2009, 30 large providers (â€œhyper giantsâ€) generated 30% of all Internet traffic Less than 1% of flows account for more than 90% of traffic volume (measurements in a university network in 2018)2 Image: http://noticing.co/wp-content/uploads/2015/08/elephant-and-mouse.jpg 1 http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.478.6620&rep=rep1&type=pdf 2 https://arxiv.org/pdf/1809.03486.pdf Identifying large flows 65 â€¢ Possible to efficiently identify large flows because #of large flows â‰ª # of flows in total - Without keeping per-flow state on routers Possible to efficiently identify large flows â€¢ False positive: wrongly include a small flow in the result â€¢ False negative: fail to identify a large flow â€¢ Measurement error: error in traffic volume of the identified large flows Accuracy metrics for large flow detection: â€¢ Sampling-based: NetFlow, sample-and-hold â€¢ Sketch-based: Multistage filters â€¢ Eviction-based: EARDet We will look at three types of large-flow detectors: System 1 (sampling-based): Sample-and-Hold [Estan & Varghese 2002] 66 â€¢ byte-sampling technique (in contrast to packet sampling in NetFlow) taking packet size into account - Samples each byte with probability p - Practically, samples a packet of size s with probability ps - ps = 1-(1-p)s â‰ˆ pÃ—s (when p is small) - Reduces memory overhead due to the non-uniform sampling biased toward â€œelephantâ€ flows Sample â€¢ updates a flow entry for all subsequent packets once it is created - Requires flow-table lookup for every incoming packet Hold = {1, 1}F1 = {2, 2}F1 = {3, 5}F1 System 1 (sampling-based): Sample-and-Hold [Estan & Varghese 2002] 1. For every packet, check if its flow record exists â€¢ If yes, â€œholdâ€ ( ) the packet â€¢ If no, â€œsampleâ€ ( ) with probability ps 2. Update flow entries {Cpkt, Cbyte} associated with â€œsampled & heldâ€ packets 3. Flows in the cache = identified large flows â€¢ Estimated number of packets & bytes: Ã±pkt = Cpkt; Ã±byte= Cbyte 67 F2F1 F1 F2F1 F1 F1 F1 F3 F3 F1 F1 F1 F1 F3 F3 Identified Large Flows F1 update Flow cache = {3, 5} = {1, 2} F1 F3 = {4, 7} = {1, 2} F1 F3 = {4, 7} = {2, 5} F1 F3 = {5, 8} = {2, 3} F1 F3 Sample-and-Hold vs. NetFlow 68 Sample-and-Hold NetFlow Sampling technique Byte sampling Packet sampling Estimate of number of packets/bytes in a flow No over-counting May over-count or underestimate Error rate (Given memory overhead M) O(M-1) O(M-1/2) Inspect headers of all packets? Yes No 0 0 0 0 0 0 0 0 Index 1 Index 8 System 2 (sketch-based): multistage filter [Estan & Varghese 2002] Single-stage filter â‰ˆ counting Bloom filter with one hash function: 1. Keep array of n counters 2. Router hashes the flow ID of the incoming packet - Output of hash function (H) is used as pseudorandom integer in {1, â€¦, n} 3. Increase i-th counter if the hash output is i 4. A flow is considered a large flow if the counter value in the associated counter â‰¥ threshold 69 Threshold = 7 update F1 H(F1) = 3 1 F2 H(F2) = 7 1 F1 2 Identified Large Flows F1 F4 update F2F1F3 H(F3) = 4 22 F1F4 H(F4) = 3 578 False Positive! System 2 (sketch-based): multistage filter [Estan & Varghese 2002] 70 â€¢ Multiple filters operate in parallel to reduce FPs - Each stage uses an independent hash function - A flow is considered a large flow if the associated counter values at all stages â‰¥ threshold â€¢ Properties: fixed memory resources, no FNs, low FPs - FP rate decreases exponentially in the number of stages System 3 (eviction-based): finding frequent items â€¢ Equivalent to the â€œfinding frequent itemsâ€ problem in database literature - Given a stream of items - Find those items: frequency > a threshold Î¸ â€¢ Item ïƒ  packet â€¢ A frequent item ïƒ  a large flow â€¢ E.g., identify flows that take more than 1% of link capacity 71 Majority algorithm [Fischer and Salzberg 1982, Demaine et al. 2002, Karp et al. 2003] An exact, linear-time algorithm to find frequent items in two passes with 1/Î¸ space â€¢ The first pass identifies all frequent items (no FN!) â€¢ The second pass eliminates all FPs Example question: in an election, how to identify the candidate that won more than a half of the votes (if there is one)? â€¢ Keep only one counter and make two passes â€¢ Generalized question: how to identify candidates (if exist) winning more than Î¸n votes using only 1/Î¸ - 1 counters? 72 Majority algorithm [Fischer and Salzberg 1982, Demaine et al. 2002, Karp et al. 2003] â€¢ Example question: in an election, how to identify the candidate that won more than a half of the votes (if there is one)? â€¢ Intuition: repeatedly remove pairs of distinct items ïƒ  If one type occurs more than 50% of the time, some items of that type must remain. Algorithm: â€¢ Keep one item type (kept_item) and one counter â€¢ First pass: for each new_item: - If counter = 0, then kept_item â† new_item, counter++ - Else if new_item == kept_item, then counter++ - Else counter-- â€¢ Second pass: test the last kept_item 73 953 6 841 72 Frequent-item finding (MG algorithm) [Misra and Gries, 1982] Goal: Find all items that appear in a stream of m items more than k times with no false negatives Limited space: n = m/k - 1 counters 74 Item Stream: a b b b c c d d d m = 9 k = 2.25 n = m/k - 1 = 3 1 a a 1 3 a b b b b 1 3 2 a b c c c 2 2 1 d b c d d 2 1 b c Cancel one item in each counter d J. Misra and D. Gries. Finding Repeated Elements. Science of Computer Programming, 2(2):143â€“152, 1982. Frequent-item finding (MG algorithm) [Misra and Gries, 1982] â€¢ Initialize n empty counters with associated empty labels â€¢ for each new_item: - If there is an i with counteri with labeli == new_item, counteri ++ - Else if there is an i with counteri == 0, labeli := new_item, counteri := 1 - Else for all i, counteri -- â€¢ Labels at the end are candidates for items that occur more than k times (k = m / (n + 1)) - Require second pass to be sure 75J. Misra and D. Gries. Finding Repeated Elements. Science of Computer Programming, 2(2):143â€“152, 1982. EARDet algorithm [Wu, Hsiao, Hu, 2014] â€¢ Modification of MG-Algorithm â€¢ Does not require second pass â€¢ No false negative for large flows â€¢ No false positive for small flows â€¢ Deterministic: keep performance regardless of input traffic â€¢ Relatively small storage cost, but many counters are needed 76https://www.csie.ntu.edu.tw/~hchsiao/pub/2014_ACM_IMC.pdf Disadvantage: per-packet counter update is an expensive operation Traffic monitoring Estimating the number of flows with probabilistic counting 77 First ideas to estimate the number of flows 78 â€¢ Record all distinct flow IDs - Often infeasible to keep one record for every distinct flow - O(N) memory overhead, where N is number of flows â€¢ Increase a counter when the incoming packet belongs to a â€œnewâ€ flow - Use Bloom filter to test whether a flow has been recorded or is new - Bloom filter enables O(1) time membership checking but still O(N) memory overhead Estimating the number of flows with probabilistic counting â€¢ Example of a simple probabilistic counting: - Hash flow ID to generate a value in [0,1) - Keep the flow ID associated with the smallest hash value - Expectation value of smallest value is 1 / (number of flows + 1) - Assumption: Hash values are uniformly distributed in interval [0,1) ïƒ  Estimate the number of flows by the smallest value v seen so far: Ã± â‰ˆ 1/v - 1 79 # of flows â‰ˆ 1/0.1 - 1 = 9 F2 F1F4 F3F5 F1F6 F4 update F1F3F5 H(F1) = 0.6H(F3) = 0.4H(F5) = 0.1 H(F1) = 0.6 H(F2) = 0.85 H(F3) = 0.4 H(F4) = 0.92 H(F5) = 0.1 H(F6) = 0.26 Example: Estimating the number of flows with probabilistic counting â€¢ Problems: - Minimum has very large variance ïƒ  not very robust - An attacker controlling only 1 input can bias the estimation â€¢ Proposal by Bar-Yossef et al. (2002): - Keep track of the k smallest hash values - Expectation value of k-th smallest value is k/(n+1) - Estimate the number of flows by the k-th smallest value vk that has been seen so far: Ã± â‰ˆ k / vk â€“ 1 - Variance of result is smaller than when using smallest value â€¢ Can additionally use sampling in order to reduce overhead due to hashing 80 Summary 81 Probabilistic techniques have many applications in networking Can make applications more efficient or more robust to failures Allow the processing of large amounts of data with limited memory Different types of errors â€¢ False positives and/or false negatives â€¢ Make sure the error type is ok for your specific application Tradeoff between accuracy and efficiency/ overhead","libVersion":"0.5.0","langs":""}