{"path":"sem2/A1/PV/summaries/A1-HS21-momuell.pdf","text":"Analysis 2 Cheat Sheet (20/21) ∗ Mose M¨uller † January 29, 2021 Contents 1 Ordinary Diﬀerential Equations 2 1.1 Linear Diﬀerential Equations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2 1.2 Linear Diﬀerential Equations of Order 1 . . . . . . . . . . . . . . . . . . . . . . . . . 2 1.3 Linear Diﬀerential Equation with Constant Coeﬃcients . . . . . . . . . . . . . . . . . 3 1.4 Separation of Variables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 2 Diﬀerential Calculus in Rn 6 2.1 Continuity in R n . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 2.2 Partial Derivatives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8 2.3 The Diﬀerential . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9 2.4 Change of Variable . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12 2.5 Higher Derivatives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13 2.6 Taylor Polynomials . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14 2.7 Critical Points . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 3 Integration in R n 17 3.1 Line integrals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17 3.2 The Riemann Integral in R n . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22 3.3 Improper Integrals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26 3.4 The Change of Variable Formula . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26 3.5 Geometric Applications of Integrals . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27 3.5.1 Centre of Mass . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27 3.5.2 Surface Area (n = 3) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27 3.6 Green’s Formula . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27 A Useful Theorem 30 B ”Set Theory” 30 ∗This document can be accessed via https://www.overleaf.com/read/tsstbfjkzbgq and can be used at will. It was created by me during the semester HS 2020 as a summary of the Analysis 2 course held by Prof. ¨Ozlem Imamoglu (https://metaphor.ethz.ch/x/2020/hs/401-0213-16L/). If you spot any mistakes feel free to write me a mail. †mosmuell@ethz.ch 1 1 Ordinary Diﬀerential Equations Deﬁnition 1.1 (Ordinary diﬀerential equation (ODE)). Deﬁnition 1.2 (Order of a diﬀerential eq.). 1.1 Linear Diﬀerential Equations Deﬁnition 1.3 ((In-)Homogeneous linear ODE’s). Theorem 1.1. 1. The dimension of the vector space S0 of the solutions to the homoge- neous problem of a linear diﬀerential equation is given by the order of the diﬀerential equation. 2. For any initial condition, there exists a unique solution for the homogeneous problem. 3. The solution to an inhomogeneous problem can be divided into a homogeneous solution and one particular solution fp: Sinh = {f0 + fp | f0 ∈ S0}. 4. For any initial condition, there exists a unique solution for the inhomogeneous problem. Deﬁnition 1.4 (Initial conditions for a linear ODE). Deﬁnition 1.5 (Diﬀerential operator). 1.2 Linear Diﬀerential Equations of Order 1 We want to solve the following problem: y′ + ay = b. (1) Recipe: 1. Solve the homogeneous problem y′ + ay = 0. y′ = dy dx = −ay (y = 0 is a solution. Let y ̸= 0) ⇐⇒ ∫ dy y = − ∫ adx (Integrate) ln |y(x)| = −A(x) (A is a primitive of a) y(x) = z · exp(−A(x)) (2) 2. Find a particular solution fp : I → C such that f ′ p + afp = b. There are three ways to do this: (a) Educated guess If b(x) is for instance a polynomial (or an exponential, ...) we can guess that the solution has to contain this function as well! See for example Table 1 (this is actually for linear ODEs with constant coeﬃcients, but can be applied here as well). 2 (b) Variation of Constants Ansatz: fp = z(x) exp(−A(x)). (3) Plug our ansatz into the ODE and get an ODE for z(x): z′(x) = b(x) exp(A(x)) =⇒ z(x) = ∫ dx b(x) exp(A(x)) (4) Hence, the particular solution to the ﬁrst order linear ODE is given by fp(x) = (∫ dx b(x) exp(A(x)) ) exp(−A(x)). (5) (c) Integrating Factor Method i. Determine the integrating factor (IF) exp (∫ a(x)dx) = exp(A(x)) (6) ii. Multiply the linear ODE with the IF exp(A(x)) ( dy dx + a(x)y) ︸ ︷︷ ︸ d dx (y·exp(A(x))) = b(x) exp(A(x)) =⇒ d dx ( y · exp(A(x)) ︸ ︷︷ ︸ ≡z(x) ) = b(x) exp(A(x)) (integrate!) (7) iii. Same solution as variation of constants: Rewrite the expressions z(x) = y · exp(A(x)) =⇒ y = z(x) exp(−A(x)) z′(x) = b(x) exp(A(x)) =⇒ z(x) = ∫ dx b(x) exp(A(x)) 1.3 Linear Diﬀerential Equation with Constant Coeﬃcients Our problem is the diﬀerential equation y(k) + ak−1y(k−1) + · · · + a1y′ + a0y = b(x), ak−1, . . . , a1, a0 ∈ C. (8) Deﬁnition 1.6 (Companion or characteristic polynomial of the homogeneous ODE). Deﬁnition 1.7 (Eigenvalues of the characteristic polynomial). Recipe 1. Solution to homogeneous problem y(k) + ak−1y(k−1) + · · · + a1y′ + a0y = 0 Ansatz y(x) = exp(λ) =⇒ exp(λ) [ λ k + ak−1λk−1 + · · · + a1λ + a0︸ ︷︷ ︸ ≡P (λ) ] = 0 (9) 3 Theorem 1.2 (Solution to the homogeneous ODE). Let λ1, . . . , λr be pairwise distinct eigen- values of P (λ), the characteristic polynomial to the homogeneous ODE y(k) + ak−1y(k−1) + · · · + a1y′ + a0y = 0 (10) with corresponding multiplicities m1, . . . , mr. Then, the functions fj,l : R → C x ↦→ xl exp(λjx) for 1 ≤ j ≤ r, 0 ≤ l < mj form a system of solutions to the homogeneous ODE (10). Theorem 1.3. If y(k) + ak−1y(k−1) + · · · + a1y′ + a0y = 0 (11) has real coeﬃcients, then each pair of complex conjugate roots βk ± iγj of P (λ) with multiplicity mj leads to solutions y(x) = xl · exp(βjx) · (cos(γjx) + i sin(γjx))) (12) for 0 ≤ l < mj, which then can be replaced by the solutions y1(x) = xl · exp(βjx) · cos(γjx), y2(x) = xl · exp(βjx) · sin(γjx) (13) 2. Particular solution to the inhomogeneous problem y(k) + ak−1y(k−1) + · · · + a1y′ + a0y = b(x) According to Theorem 1.1, the solution to the inhomogeneous problem (8) will be of the form y = yh + yp where yh is the homogeneous solution we found already and yp is a particular solution. There are, again, two methods for ﬁnding the particular solution: (a) Method of undetermined coeﬃcients/Ansatz The solution will be similar to the disturbance function b(x). The ansatz yp for special disturbance functions b(x) is given in Table 1. (b) Variation of constants1 (dim = 2) We want to solve y′′(x) + a1y′(x) + a0y(x) = b(x) (14) with homogeneous solution yh = z1f1(x) + z2f2(x) with f1, f2 two linearly independent solutions. As for the dim=1 case, we try to ﬁnd a particular solution of the form yp = z1(x)f1(x) + z2(x)f2(x). We need one more equation to solve the ODE. As one can prove, the system of equations is given by z′ 1(x)f1(x) + z′ 2(x)f2(x) = 0 (constraint) (15) z′ 1(x)f ′ 1(x) + z′ 2(x)f ′ 2(x) = b(x). (16) This system of equations can also be written down as a matrix equation ( f1 f2 f ′ 1 f ′ 2 ) ︸ ︷︷ ︸ =A · ( z′ 1(x) z′ 2(x) ) = ( 0 b(x) ) . (17) 1https://en.wikipedia.org/wiki/Variation_of_parameters#Description_of_method FYI: Wronskian approach http://math.bd.psu.edu/faculty/jprevite/250sp10/250bookSec3.7.pdf 4 Table 1: Ansatz yp for special disturbance functions b(x). Pn(x), Qn(x), Rn(x) and Sn(x) are polynomials of degree n. disturbance function b(x) Ansatz yp(x) a · exp(αx) b · exp(αx) a sin(βx) b cos(βx) c sin(βx) + d cos(βx) a exp(αx) sin(βx) b exp(αx) cos(βx) exp(αx)[c sin(βx) + d cos(βx)] Pn(x) exp(αx) Rn(x) exp(αx) Pn(x) exp(αx) sin(βx) Qn(x) exp(αx) cos(βx) exp(αx)[Rn(x) sin(βx) + Sn(x) cos(βx)] Note: If λ = α + iβ is an eigenvalue of the characteristic polynomial P (λ) of multiplicity m, then the ”Ansatz” has to be multiplied by xm! As A is invertible 2, we have ( z′ 1(x) z′ 2(x) ) = A −1 · ( 0 b(x) ) , (18) which then gives us the primitives z1(x), z2(x) and with that the particular solution! 1.4 Separation of Variables In order to solve a ﬁrst order ODE (which does not have to be linear!), given by dy dx = b(x)g(y) (19) we can separate the variables x, y ∫ dy g(y) = ∫ dx b(x). (20) Note that the LHS is only dependent on y whereas the RHS is only dependent on x! Additionally, if for any y0 it is g(y0) = 0, the constant function y = y0 is a solution to (19). 2The determinant f1f ′ 2 − f ′ 1f2 does not vanish due to the fact that (f1, f2) is a basis of the vector space of the homogeneous solutions. 5 2 Diﬀerential Calculus in Rn Deﬁnition 2.1 (Polynomial in n variables of degree d). Deﬁnition 2.2 (Monomial of degree d). We can build new functions using old ones by 1. taking the Cartesian product, 2. multiplying functions f1, . . . , fn on R fi : R → R, x ↦→ fi(x), ∀i ≤ n to get a new multivariable function of separated variables f : R n → R, (x1, . . . , xn) ↦→ f1(x1) · · · fn(xn) 3. composing functions. 2.1 Continuity in Rn Deﬁnition 2.3 (Euclidean norm (length of a vector)). Deﬁnition 2.4 (Convergence of a sequence). Proposition 2.1. The sequence (xk)k∈N, xk ∈ R n converges to y ∈ R n as k → +∞ if and only if the following two equivalent conditions hold: 1. For each i, the sequence (xk,i) ∈ R converges to yi ∈ R. 2. The sequence of real numbers ∥xk − y∥ converges to 0 as k → +∞. Deﬁnition 2.5 (Limit of a function). Proposition 2.2. Let f : X ⊂ R n → Rm with x0 ∈ X, y ∈ Rm. We have lim x→x0 x̸=x0 f (x) = y (21) if and only if for every sequence (xk)k∈N ∈ X which converges to x0 the sequence (f (xk))k∈N converges to y. Remark. The above proposition means that we can use the convergence and the limit of a function interchangeably! Deﬁnition 2.6 (Continuity of a function). Proposition 2.3. Let f : X ⊂ Rn → R m with x0 ∈ X. Then f is continuous at x0 if and only if for every sequence (xk)k∈N ∈ X that converges to x0 the sequence (f (xk))k∈N converges to f (x0). Symbolically: f is continuous at x0 ⇐⇒ (∀(xk)k∈N ∈ X : lim k→+∞ xk = x0 =⇒ lim k→+∞ f (xk) = f ( lim k→+∞ xk) ︸ ︷︷ ︸ =f (x0) ). (22) 6 Remark. This means that for a continuous function, you can take the limit into the function! Additionally, together with the other proposition we have that a function f is continuous at x0 if and only if the limit of f is f (x0) as x → x0. Symbolically: f is continuous at x0 ⇐⇒ lim x→x0 x̸=x0 f (x) = f (x0). (23) Proposition 2.4. 1. Polynomials are continuous functions. 2. The composition of continuous functions is continuous. 3. The sum of continuous functions is continuous. 4. The product of continuous functions is continuous. Lemma 2.1 (Sandwich lemma). Let f, g, h : R n → R with f (x) < g(x) < h(x), ∀x ∈ R n and a ∈ R n. If limx→a f (x) = limx→a h(x) = L ∈ R, then limx→a g(x) also exists is equal to L. Theorem 2.2 (Min-max theorem for functions of one variable). Let a < b ∈ R and f : [a, b] → R be a continuous function on a compact interval. Then f attains both its minimum and maximum in this set. Remark. We want to have analogues to the terms closed and compact intervals on R in R n in order to state similar theorems to the min-max theorem in Rn! Deﬁnition 2.7 (Bounded, closed, open and compact subsets). A subset X ⊂ Rn is called 1. bounded if the set of ∥x∥ for x ∈ X is bounded in R. 2. closed if for every sequence (xk)k ∈ X that converges to y ∈ R n we have that y ∈ X. 3. open if for any x = (x1, . . . , xn) ∈ X there exists a δ > 0 such that the set Bδ(x) = {y ∈ R n | ∥x − y∥ < δ}, called the open ball with radius δ around x, is contained in X. 4. compact if it is bounded and closed. Remark. A set is X ⊂ R n is open/closed if and only if its complement Y = {x ∈ Rn | x ̸∈ X} (24) is closed/open. Proposition 2.5. 1. The open ball Br(x0) = {x ∈ R n | ∥x − x0∥ < r} is bounded and not closed (but open). 2. The closure of the open ball (closed ball) Br(x0) = {x ∈ R n | ∥x − x0∥ ≤ r} is compact (i.e. closed and bounded). 3. If X1 ⊂ R n and X2 ⊂ Rm are bounded (resp. closed, resp. compact), then X1 ×X2 = {(x1, x2) ∈ R n+m | x1 ∈ X1, x2 ∈ X2} ⊂ R n+m is bounded (resp. closed, resp. compact), as well. Proposition 2.6. Let f : R n → R m be a continuous map. For any closed (open) set Y ⊂ Rm, the set f −1(Y ) = {x ∈ R n | f (x) ∈ Y } ⊂ R n is closed (open). Theorem 2.3 (Min-max theorem for functions of several variables). Let f : X → R be a continuous map with X ⊂ R n a compact subset. Then f is bounded and attains its maximum and minimum. In other words, there exist x+, x− ∈ X such that f (x+) = sup x∈X f (x), f (x−) = inf x∈X f (x). 7 2.2 Partial Derivatives Deﬁnition 2.8 (Partial derivative of a function). Let X ⊂ R n be an open set, f : X → Rm be a function and 1 ≤ i ≤ n. We say that f has a partial derivative on X with respect to the i-th variable, if for all x0 = (x0,1, . . . , x0,n) ∈ X, the function deﬁned by g(t) =      g1(t) g2(t) ... gm(t)      = f (x0,1, . . . , x0,i−1, t, x0,i+1, . . . , x0,n) on the set I = {t ∈ R | (x0,1, . . . , x0,i−1, t, x0,i+1, . . . , x0,n) ∈ X} is diﬀerentiable at t = x0,i. Its derivative g′(x0,i) at x0,i is denoted ∂f ∂xi (x0) , ∂xif (x0), ∂if (x0). (25) Remark. Intuitively, this deﬁnition means that we are freezing all the variables except the i-th one and consider the corresponding function of one variable, g(t). Graphically (n = 1, 2, m = 1), the partial derivative is the slope on the graph of f where all the variables except the i-th one are ﬁxed. Deﬁnition 2.9 (Jacobi matrix of a function). Let X ⊂ R n be an open set and f : X → R m be a function with partial derivatives on X. Write f (x) =      f1(x) f2(x) ... fm(x)      For any x ∈ X, the matrix Jf (x) = ( ∂fi ∂xj (x) ) i=1,...,m; j=1,...,n =       ∂f1 ∂x1 · · · ∂f1 ∂xn ... . . . ... ∂fm ∂x1 · · · ∂fm ∂xn       (26) is called the Jacobi matrix of f at x. Deﬁnition 2.10 (Gradient of a function). Let X ⊂ R n be an open set and f : X → R be a function whose partial derivatives at x0 ∈ X exist. Then the column vector    ∂x1f (x0) ... ∂xnf (x0)    =: ∇f (x0) (27) is called the gradient of f and x0. 8 Remark. In this case, m = 1, the gradient is the transpose vector of the Jacobi matrix ∇f (x0) = (∂x1f (x0), . . . , ∂xnf (x0)) T = (Jf (x0)) T . The interpretation of the gradient invokes the concept of the directional derivatives (Deﬁnition 2.13). Suppose that ∇f (x0) ̸= 0 and let v ∈ Rn be a unit vector ∥v∥ = 1. Then the directional derivative along v at the point x0 is given by the inner product ⟨∇f (x0), v⟩ = ∥∇f (x0)∥∥v∥ cos(θ), (28) which is biggest for θ = 0. Hence, we can determine the direction in which the derivative of the function is largest by looking at its gradient! Proposition 2.7. Let f, g : X ⊂ R n → R m be functions and i = 1, . . . , n. If f and g have partial derivatives w.r.t the i-th coordinate on X, then 1. f + g does so, as well: ∂xi(f + g) = ∂xif + ∂xig. (29) 2. if m = 1, f g also does (product rule) ∂xi(f g) = ∂xi(f )g + f ∂xi(g). (30) Additionally, if g(x) ̸= 0∀x ∈ X, then f /g has a partial derivative w.r.t the i-th coordinate on X given by (quotient rule) ∂xi(f /g) = (∂xi(f )g − f ∂xi(g))/g2. (31) Proposition 2.8 (Revision). Let f : X ⊂ R → R be diﬀerentiable at x0 ∈ X. Then f is continuous at x0. Remark. As we know, the function f = { xy x2+y2 (x, y) ̸= (0, 0) 0 (x, y) = (0, 0) is not continuous at (x, y) = (0, 0), but all its partial derivatives (and hence all directional derivatives) exist and are equal to 0! In light of Proposition 2.8, we see that partial derivatives are not strong enough to be used as an analogue to diﬀerentiability in one-variable calculus! We want to have this analogue in order to be able to approximate a function by a linear map. This leads us to the concept of the diﬀerential. 2.3 The Diﬀerential Deﬁnition 2.11 (Diﬀerentiability and the diﬀerential of a function). Let X ⊂ Rn be an open set and f : X → R m be a function. Let u : R n → Rm be a linear map and x0 ∈ X. We say that f is diﬀerentiable at x0 if lim x→x0 x̸=x0 f (x) − f (x0) − u(x − x0) ∥x − x0∥ = 0, (32) where the limit is in Rm. The linear map u is then called the diﬀerential of f at x0 and is denoted by df (x0) = dx0f . If f is diﬀerentiable at every x ∈ X, we say that f is diﬀerentiable on X. 9 Remark. This deﬁnition means that we can approximate f (x) by a linear map df such that f (x) = f (x0) + df (x0) · (x − x0) + R(x, x0), where the remainder R(x, x0) goes faster to zero than ∥x − x0∥ as x → x0, meaning lim x→x0 R(x, x0) ∥x − x0∥ = 0. We also say that f is well approximated around x0 by the linear map u : R n → Rm. Theorem 2.4. Let X ⊂ Rn be an open set and f : X → R m be a function that is diﬀerentiable at x0. Then 1. f is continuous at x0, 2. f has all partial derivatives at x0 and the diﬀerential of f (i.e. the linear map that approximates the function f well) is given by the linear map df (x0) : Rn → R m, x ↦→ Ax which in the canonical bases of R n and R m is given by the Jacobi matrix of f at x0, i.e. A = Jf (x) = ( ∂fi ∂xj (x) ) i=1,...,m; j=1,...,n (33) Proposition 2.9. Let X ⊂ R n be an open set and f : X → R m and g : X → R m be functions that are diﬀerentiable at x0. Then 1. the function f + g is diﬀerentiable at x0 with the diﬀerential d(f + g)(x0) = df (x0) + dg(x0), 2. if m = 1, f g is diﬀerentiable at x0, 3. if m = 1 and g ̸= 0, f /g is also diﬀerentiable at x0. Remark. The diﬀerentials for the functions in (2) and (3) are given by the chain rule. Theorem 2.5. Let X ⊂ R n be an open set and f : X → R m be a function. If f has all partial derivatives ∂jfi : X → Rm and they are continuous on X, then f is diﬀerentiable on X. Remark. This theorem states that we can claim that a function is diﬀerentiable (and hence continuous) if we know that the partial derivatives of that function exist and are continuous, as well! !!The existence of partial derivatives itself does not imply the diﬀerentiability of a function!! Deﬁnition 2.12 (Tangent space at x0 of the graph of a function). Let X ⊂ R n be open and f : X → R m be a diﬀerentiable function. Let x0 ∈ X and u ≡ df (x0) be the diﬀerential of f at x0. The graph of the aﬃne linear approximation g(x) = f (x0) + u(x − x0) (34) from R n to R m, i.e. the set {(x, y) ∈ R n × R m | y = f (x0) + u(x − x0)} (35) is called the tangent space at x0 of the graph of f . 10 Theorem 2.6 (Chain rule). Let X ⊂ Rn, Y ⊂ R m be open sets and f : X → Y and g : Y → R p be diﬀerentiable functions on X and Y , respectively. Then g ◦ f : X → Rp is diﬀerentiable on X and the diﬀerential is given by d(g ◦ f )(x0) = dg(f (x0)) ◦ df (x0) (36) for all x0 ∈ X. In particular, the Jacobi matrix satisﬁes Jg◦f (x0) = Jg(f (x0))Jf (x0) (37) where the RHS is the matrix product. Remark. The theorem can be visualised by the following. The functions have the following setup: X f −→ Y g −→ R p x0 ↦−→ y0 = f (x0) ↦−→ g(f (x0)) = g(y0) The diﬀerentials look like Rn df (x0) −→ R m dg(y0) −→ Rp x ↦−→ y = Ax ↦−→ BAx, where A = Jf (x0) and B = Jg(f (x0)). The former representation is a m × n matrix, the latter a p × m matrix. Hence, the matrix product BA will be a p × n matrix. Deﬁnition 2.13 (Directional derivative of a function). Let X ⊂ R n be open and f : Rn → Rm be a function. Let v ̸= 0 ∈ Rn a vector and x0 ∈ R n. When it exists, the limit Dvf (x0) = lim h→0 f (x0 + hv) − f (x0) h (38) is called the directional derivative of f along v at a point x0. Equivalently, we say that f has directional derivative w ∈ R m in the direction v, if the function φ deﬁned by φ : I → R m, (39) t ↦→ φ(t) = f (x0 + tv) (40) where I = {t ∈ R | x0 + tv ∈ X} has a derivative at t = 0 which is equal to Dvf (x0) = w. Remark. In particular, along the coordinate directions ei = (0, . . . , 0, 1, 0, . . . , 0) we have Deif = ∂f ∂xi = ∂xif . Theorem 2.7. Let X ⊂ R n be open and f : R n → R be a diﬀerentiable function. Then, for any v ̸= 0 ∈ R and x0 ∈ Rn, the directional derivative of f along v at point x0 exists and is given by Dvf (x0) = d dt f (x0 + tv) ∣ ∣t=0 = df (x0)(v) (41) = Jf (x0) · v. (42) 11 Remark. For calculating the directional derivative of the function f along v at point x0 ∈ R n you can use diﬀerent methods: 1. Use the deﬁnition, i.e. the diﬀerential quotient Dvf (x0) = lim h→0 f (x0 + hv) − f (x0) h (43) 2. Deﬁne φ : [−δ, δ] → R, t ↦→ φ(t) = f (x0 + tv) and use φ ′(t)∣ ∣t=0 = φ ′(0) = lim h→0 φ(h) − φ(0) h = lim h→0 f (x0 + hv) − f (x0) h = Dvf (x0). (44) This means that you can just diﬀerentiate the function φ using one dimensional calculus! 3. Using Theorem 2.7 (which you investigate in Exercise 6.2) we have (for m = 1) Dvf (x0) = ⃗v · df (x0) = v1 · ∂1f (x0) + v2 · ∂2f (x0) + · · · + vn · ∂nf (x0), (45) where df (x0) is the diﬀerential of the function f at point x0 and v = (v1, v2, ..., vn) ∈ Rn. This means that the directional derivative is a linear combination of the partial derivatives of f . 2.4 Change of Variable Deﬁnition 2.14 (Change of variables). Let X ⊂ R n be open and f : X → R n be diﬀerentiable. We say that f is a change of variables around x0 ∈ X if there is a radius ρ > 0 such that the image of f restricted to the open ball B = Bρ(x0) around x0 of radius ρ, namely Y = f (B), is open in R n and there exists a diﬀerentiable map g : Y → B such that f ◦ g = idY and g ◦ f = idB. Remark. This deﬁnition is essentially saying that f ∣ ∣B is a bijection on Y with the inverse function g which is also diﬀerentiable. Theorem 2.8 (Inverse function theorem). Let X ⊂ R n be open and f : X → Rn be diﬀerentiable. If for x0 ∈ X we have that det(Jf (x0)) ̸= 0, i.e. the Jacobi matrix is invertible at x0, then f is a change of variables around x0 with inverse function g. Moreover, the Jacobi matrix of g at x0 is then given by Jg(f (x0)) = (Jf (x0)) −1 . (46) There are three important examples of changes of variables f : U ⊂ R n → X ⊂ Rn 1. Polar coordinates f : (0, ∞) × [0, 2π) → R 2, (47) (r, θ) ↦→ (x, y) = (r cos(θ), r sin(θ)). (48) The Jacobi matrix is given by Jf (r, θ) = ( ∂f1 ∂r ∂f1 ∂θ ∂f2 ∂r ∂f2 ∂θ ) = (cos(θ) −r sin(θ) sin(θ) r cos(θ) ) (49) with determinant det(Jf (r, θ)) = r. (50) 12 2. Cylindrical coordinates f : (0, ∞) × [0, 2π) × R → R3, (51) (r, θ, z) ↦→ (x, y, z) = (r cos(θ), r sin(θ), z). (52) The Jacobi matrix is given by Jf (r, θ) =  cos(θ) −r sin(θ) 0 sin(θ) r cos(θ) 0 0 0 1   (53) with determinant det(Jf (r, θ)) = r. (54) 3. Spherical coordinates f : (0, ∞) × [0, 2π) × (0, π) → R 3, (55) (r, θ, φ) ↦→ (x, y, z) = (r sin(φ) cos(θ), r sin(φ) sin(θ), r cos(φ)). (56) The Jacobi matrix is given by Jf (r, θ, φ) =   cos(θ) sin(φ) −r sin(θ) sin(φ) r cos(θ) cos(φ) sin(θ) sin(φ) −r cos(θ) sin(φ) r sin(θ) cos(φ) cos(φ) 0 −r sin(φ)   (57) with determinant det(Jf (r, θ, φ)) = −r2 sin(φ). (58) 2.5 Higher Derivatives Deﬁnition 2.15 (Class C k, smooth functions). Let X ⊂ Rn be open and f : X → Rn be diﬀer- entiable. We say that f is of class C 1 if it is diﬀerentiable on X and all its partial derivatives are continuous. The set of functions of class C 1 from X to R n is denoted C 1(X; R n). For k ≥ 2, we say that f is of class C k, or f ∈ C k(X; Rn), if it is diﬀerentiable and all its partial derivatives are of class C k−1. If f ∈ C k(X; R n) for all k ≥ 1 then we say that f is smooth or of class C ∞. Remark. All polynomials, trigonometric functions and exponentials are smooth! Theorem 2.9. Let f ∈ C k for k ≥ 2. Then the partial derivatives of order ≤ k are independent of the order of diﬀerentiation. Remark. This means that mixed partial derivatives up to the order of k all commute! We have seen this already in exercise 6.3 where we noted that the partial derivatives do not commute as they are not continuous. For k = 2, we then have Schwarz’s theorem. 13 Deﬁnition 2.16 (Hessian matrix ). Let X ⊂ Rn be open, x0 ∈ X and f ∈ C 2(X; Rn). Then the n × n-matrix Hessf (x0) = ( ∂2f ∂xi∂xj (x0) ) i=1,...,n; j=1,...,n =              ∂2f ∂x2 1 ∂2f ∂x1 ∂x2 · · · ∂2f ∂x1 ∂xn ∂2f ∂x2 ∂x1 ∂2f ∂x2 2 · · · ∂2f ∂x2 ∂xn ... ... . . . ... ∂2f ∂xn ∂x1 ∂2f ∂xn ∂x2 · · · ∂2f ∂x2 n              (59) is called the Hessian matrix of f at point x0. Remark. The Hessian matrix is symmetric (due to theorem 2.9 and the assumption that f ∈ C 2). Figure 1: Recipe of determining whether a function is diﬀerentiable, continuous or of class Ck. The labels on green ground show those properties, the text besides the arrows states which deﬁnition/theorem/proposition is providing the connection. 2.6 Taylor Polynomials We are now looking at functions whose co-domain is R f : R n → R. Deﬁnition 2.17 (Taylor polynomial ). Let X ⊂ R n be open, x0 ∈ X, k ≥ 1 and f ∈ C k(X; R). The 14 k-th Taylor polynomial of f at point x0 is a polynomial in n variables of degree ≤ k given by Tkf (y; x0) = f (x0) + n∑ i=1 ∂f ∂xi yi + · · · + ∑ m1+...+mn=k 1 m1! · · · mn! ∂kf ∂m1 1 · · · ∂mn n ym1 1 · · · ymn n (60) = ∑ m, |m|≤k 1 m!∂mf (x0)ym. (61) Remark. In Eq. (61) we used the multi-index notation. This notation is given by m = (m1, . . . , mn), m! = m1! · · · mn! together with ym = ym1 1 · · · ymn n and ∂m = ∂m1 1 ∂m2 2 . . . ∂mn n , where ∂mi i := ∂mi ∂xmi i . Theorem 2.10 (Taylor expansion/approximation). Let X ⊂ Rn be open, x0 ∈ X, k ≥ 1 and f ∈ C k(X; R). Then we have f (x) = Tkf (x − x0; x0) + Ek(f, x, x0), (62) where lim x→x0 x̸=x0 Ek(f, x, x0) ∥x0 − x∥k = 0. (63) 2.7 Critical Points Here again, we are looking at functions whose co-domain is R f : R n → R. Deﬁnition 2.18 (Critical point, local maximum/minimum, saddle point). Let X ⊂ R n be open and f : X → R a diﬀerentiable function. We say that 1. a point x0 ∈ X is a critical point of the function f if ∇f (x0) = 0. 2. f has a local maximum (minimum) at x0 ∈ X if we can ﬁnd a radius r > 0 such that the open ball Br(x0) around x0 is a subset of X and we have that f (x) ≤ f (x0) ∀x ∈ Br(x0) (64) (f (x) ≥ f (x0) ∀x ∈ Br(x0)). (65) 3. a point critical point which is neither a local maximum nor a local minimum is a saddle point. 15 Theorem 2.11. Let X ⊂ R n be open and f : X → R be diﬀerentiable. If x0 ∈ X is a local extrema (i.e. a local minimum or a local maximum), then we have that ∇f (x0) = 0, (66) ⇐⇒ ∂f ∂x1 = · · · = ∂f ∂xn = 0. (67) Theorem 2.12. Let X ⊂ Rn be compact and f : X → R a function that is diﬀerentiable on the interior of X. Then the global extrema of f exist and they are either at a critical point of f or on the boundary of X. Remark. This theorem is quite similar to the min-max theorem of several variables (Theorem 2.3). The point here is that now we can relate the extrema to the critical points of the set or the boundary! The convention used here is X = int(X) ∪ bd(X), where int(X) is the interior of X and bd(X) is the boundary of X. Deﬁnition 2.19 (Non-degenerate critical point). A non-degenerate critical point x0 of a function f ∈ C 2 is a critical point for which det(Hessf )(x0) ̸= 0. Deﬁnition 2.20 (Positive deﬁnite, negative deﬁnite and indeﬁnite symmetric matrix ). As deﬁned in previous linear algebra courses, we have that a symmetric matrix A = (aij) ∈ Rn×n with det(A) ̸= 0 (meaning that its eigenvectors are non-zero) is 1. positive deﬁnite (A > 0) if xT Ax > 0 ∀x ∈ R n \\ {0}, meaning that its eigenvalues are strictly positive 2. negative deﬁnite (A < 0) if xT Ax < 0 ∀x ∈ R n \\ {0}, meaning that its eigenvalues are strictly negative 3. indeﬁnite otherwise, meaning that its eigenvalues are both positive and negative. Proposition 2.10. A symmetric matrix A ∈ R n×n is positive deﬁnite if and only if for every 1 ≤ j ≤ n the determinant of Aj = (akl)1≤k≤j 1≤l≤j is greater than zero det(Aj) > 0. Remark. For a 2 × 2 matrix A ∈ R 2×2, we have that it is positive deﬁnite if a11, det(A) > 0. Theorem 2.13 (2nd derivative test). Let X ⊂ R n be open and f ∈ C 2(X; R). Let x0 ∈ X be a non-degenerate critical point of f (∇f (x0) = 0, det(Hessf (x0)) ̸= 0). Then 1. x0 is a local minimum if Hessf (x0) > 0, 2. x0 is a local maximum if Hessf (x0) < 0, 3. x0 is a saddle point if the Hessian matrix is indeﬁnite. Remark. If x0 is a degenerate critical point (det(Hessf (x0)) = 0), the Hessian does not allow us to conclude anything concerning local extrema at x0: there could be either a local maximum or local minimum or a saddle point! 3 3http://www.optimization-online.org/DB_FILE/2017/08/6165.pdf, http://www.jstor.com/stable/ 2689393 16 degenerate critical point non-degenerate critical point local minimum local maximum saddle point Function f critical point p0 df(p0) = 0 (Def 2.18) ! cannot use 2nd derivative test :( (Def 2.20, Theorem 2.13) det(Hessf(p0)) ≠ 0 (Def 2.19)det(Hessf(p0)) = 0 Hessf(p0) > 0 Hessf(p0) < 0 Hessian matrix indeﬁnite Figure 2: Recipe of determining what kind of critical points we have. The last step (for non-degenerate critical points) is called the 2nd derivative test. 3 Integration in Rn For a function f : R → Rn, t ↦→    f1(t) ... fn(t)    we can calculate the integral as ∫ b a f (t)dt =    ∫ b a f1(t)dt ... ∫ b a fn(t)dt    . We now want to generalise the integral to functions f : R n → R n and start with line integrals. 3.1 Line integrals Deﬁnition 3.1 (Curve). A parametrised curve in R n is a continuous map γ : [a, b] → R n that is piecewise of class C 1, i.e. there exists a k ≥ 1 and a partition a = t0 < t1 < ... < tk = b such that the restriction γ∣ ∣[tj ,tj+1] is of class C 1 for all j ≤ k. Remark. You can always get a parametrisation of the graph of a function f : R → R k by taking the curve γf : R → R k+1, t ↦→ (t, f (t)). Note that the derivative of the curve γ′(t) is the tangent vector of the curve at the point γ(t)! 17 Deﬁnition 3.2 (Line integral 4). Let γ : [a, b] → R n be a parametrisation of a curve. Let X ⊂ R n be a subset which contains the image of γ and f : X → R n be a continuous function. The integral ∫ b a f (γ(t)) · γ′(t)dt ∈ R (68) is called the line integral of f along γ. It is denoted as ∫ γ f (s) · d⃗s (69) Remark. • You might also see the notation ∫ f (s) · d⃗s = ∫ f1(x)dx1 + · · · + ∫ fn(x)dxn (70) which can be motivated by a curve γ : [a, b] → R n, t ↦→ (γ1(t), γ2(t)) = (x, y) with derivative γ′(t) = ( dx dt , dy dt ) giving the line integral ∫ γ f (s) · d⃗s = ∫ b a (f1(γ(t)) f2(γ(t)) ) · (γ1(t) γ2(t) ) dt = ∫ γ(b) γ(a) f1(x, y)dx + ∫ γ(b) γ(a) f2(x, y)dy • You can also ”connect” paths γ1 : [a, b] → Rn and γ2 : [c, d] → R n with γ1(b) = γ2(c) by forming the juxtaposition (concatenation) γ1 + γ2 ≡ { γ1(t) t ∈ [a, b] γ2(t − b + c) t ∈ [b, b + d − c] (71) with ∫ γ1+γ2 f (s) · d⃗s = ∫ γ1 f (s) · d⃗s + ∫ γ2 f (s) · d⃗s (72) • The line integral has a physical interpretation. Assuming that a point mass is moving along a path γ under the inﬂuence of the force ﬁeld F : R 2 → R 2 during its motion. Then the line integral ∫ γ F (s) · d⃗s is the work that is done by the force ﬁeld on the point mass. If F is the only force applied to the object, the work describes the change of (kinetic/potential) energy of the point mass. Proposition 3.1. Let γ : [a, b] → R n be a parametrisation of a curve. Let X ⊂ R n be a subset which contains the image of γ and f : X → Rn be a continuous function. Let φ : [c, d] → [a, b] be of class C 1 such that φ′(t) > 0, φ(c) = a and φ(d) = b. Then we have that the line integrals of γ and ˜γ ≡ γ ◦ φ are the same ∫ ˜γ f (s) · d⃗s = ∫ γ f (s) · d⃗s (73) Remark. In other word, the proposition says that the line integral is invariant under orientation preserving reparametrisation of the curve. (Orientation preserving means that the starting point and the end point of the curves are the same!) 4Have a look at the visualisation of the line integral for n = 2: https://www.geogebra.org/m/YZWyyM47. 18 Proposition 3.2. Let γ : [a, b] → R n be a parametrisation of a curve. Let X ⊂ Rn be a subset which contains the image of γ and f : X → Rn be a continuous function. Let −γ : [a, b] → R n be the same path in the opposite direction, i.e. −γ(t) = γ(a + b − t). Then we have ∫ −γ f (s) · d⃗s = − ∫ γ f (s) · d⃗s (74) Remark. In other words: going along a parameterised curve ”backwards” leads to the opposite value of the line integral! Deﬁnition 3.3 (Potential ). Let g ∈ C 1(X; R) be a scalar function on a set X ⊂ R n such that ∇g = f , where f : X → R n is a multi-variable vector-valued function. Then g is called the potential of f . Remark. • For n = 1, a potential is the same as a primitive. • If f : R → R is continuous, it always has a primitive. This is, however, not in general true for n ≥ 2. This is only true if the vector ﬁeld is conservative! Deﬁnition 3.4 (Conservative vector ﬁeld ). Let X ⊂ R n be a subset and f : X → Rn be a continuous vector ﬁeld. If for any x1, x2 ∈ X the line integral ∫ γ f (s) · d⃗s is independent of the parametrisation of the curve γ in X from x1 to x2, then we say that the vector ﬁeld f is conservative. Deﬁnition 3.5 (Path connected sets). As set X ⊂ R n is said to be path connected if for every two points x1, x2 ∈ X there exists a parametrised curve γ ∈ C 1((0, 1]; X) with γ(0) = x1 and γ(1) = x2. Theorem 3.1. Let f : X → R n be a continuous vector ﬁeld on an open path connected set X ⊂ Rn. Then the following statements are equivalent: 1. f is the gradient of a function g : X → R, i.e. ∇g = f , or g is the potential of f , 2. f is conservative, meaning that the line integral of f is independent of the path between two points, i.e. for two curves γ1 : [a, b] → R n and γ2 : [c, d] → R n with γ1(a) = γ2(c) = A and γ1(b) = γ2(d) = B we have ∫ γ1 f (s) · d⃗s = ∫ γ2 f (s) · d⃗s (75) 3. the line integral of f for any closed curve γ : [a, b] → R n (meaning γ(a) = γ(b)) is zero: ∫ γ f (s) · d⃗s = 0. (76) Proof. • (1) =⇒ (2): you have seen an example in the lecture that shows the contraposition (¬(2) =⇒ ¬(1)) • (3) ⇐⇒ (2): 0 = ∫ γ1−γ2 f (s) · d⃗s = ∫ γ1 f (s) · d⃗s − ∫ γ2 f (s) · d⃗s ⇐⇒ ∫ γ1 f (s) · d⃗s = ∫ γ2 f (s) · d⃗s 19 • (2) =⇒ (1): (similar to one-variable case) For each x ∈ X let γ : (0, 1] → X be a path with γ(0) = p0 and γ(1) = x. Deﬁne g(x) = ∫ x p0 f ds. Since it is independent of the path we write ∫ x p0 instead of ∫ γ Remark. 1. If f has a potential g s.t. ∇g = f , we can calculate the line integral along γ : [0, 1] → R n as ∫ γ f (s) · d⃗s = g(γ(1)) − g(γ(0)). (77) The line integral thus only depends on the end points of the curve! 2. If we have a conservative vector ﬁeld f : X → R n, we can ﬁnd its potential by integrating its coordinate functions w.r.t. the corresponding variable (w.l.o.g. i = 1 here), namely g(x1, x2, ..., xn) = ∫ f1(x1, x2, ..., xn)dx1 + h1(x2, ..., xn) ∂g ∂x2 = ∫ ∂f1 ∂x2 dx1 + ∂h1 ∂x2 ! = f2(x1, x2, ..., xn) ... ∂g ∂xn = ∫ ∂f1 ∂xn dx1 + ∂h1 ∂xn ! = fn(x1, x2, ..., xn) where the functions hi are only dependent on the variables {xj}j̸=i. The RHS is the condition ∇g = f ⇐⇒ ∂ig = fi. We can also use this method to obtain that a function is not conservative, i.e. we won’t obtain a function that is satisfying this condition! Theorem 3.2. Let X ⊂ Rn be open and f ∈ C 1(X; Rn) a vector ﬁeld. If f is conservative, then ∂fi ∂xj = ∂fj ∂xi . (78) Proof. If f is conservative, then there exists a potential g ∈ C 2(X, R) such that f = ∇g. Then we know from Theorem 2.9 that the partial derivatives are independent of the order of diﬀerentiation, meaning ∂2g ∂xi∂xj = ∂2g ∂xj∂xi =⇒ ∂ ∂xi ∂g ∂xj ︸︷︷︸ ≡fj = ∂ ∂xj ∂g ∂xi︸︷︷︸ ≡fi Remark. Note that this is only a necessary condition, so the inverse does not hold!! ∂fi ∂xj = ∂fj ∂xi ̸=⇒ f is conservative! (79) Deﬁnition 3.6 (Star-shaped set). A subset X ⊂ R n is called star-shaped if there exists a point x0 ∈ X such that ∀x ∈ X the line segment joining x and x0 is contained in X. 20 Remark. Note that every convex set is also star-shaped! Theorem 3.3. Let X ⊂ R n be a star-shaped open subset and f ∈ C 1(X; R n) a vector ﬁeld such that ∂fi ∂xj = ∂fj ∂xi ∀i, j. (80) Then f is conservative! Yes No (Thm 3.2) No Yes (Thm 3.3) Potential does not exist Potential exists (Thm 3.1) Thm 3.1 Def 3.2 Figure 3 Deﬁnition 3.7 (Curl of function f in R n, n = 2, 3). 1. n = 2: Let X1 ⊂ R 2 be open and f ∈ C 1(X1; R 2) a vector ﬁeld. Then the curl of f is the vector ﬁeld on X1 deﬁned by curl(f ) = ∂f2 ∂x − ∂f1 ∂y (81) 2. n = 3: Let X2 ⊂ R3 be open and g ∈ C 1(X2; R 3) a vector ﬁeld. Then the curl of g is the vector ﬁeld on X2 deﬁned by curl(g) =  ∂yg3 − ∂zg2 ∂zg1 − ∂xg3 ∂xg2 − ∂yg1   (82) 21 Remark. 1. Note that the curl of a two-dimensional vector ﬁeld f (x, y) = (f1(x, y), f2(x, y)) is sort of the curl of a three-dimensional vector ﬁeld where we choose the z-coordinate to be zero f (x, y, z) = (f1(x, y), f2(x, y), 0) curl(f ) =  ∂yf3 − ∂zf2 ∂zf1 − ∂xf3 ∂xf2 − ∂yf1   =   0 0 ∂xf2 − ∂yf1   Since both the x- and y-coordinates are zero, the curl always points in the z-direction. Thus, we can think of it as a scalar measuring the rotation of the vector ﬁeld around the point (x, y). 2. The curl is sometimes also written as the cross (or wedge) product of the gradient with the function ∇ × f =  ∂yf3 − ∂zf2 ∂zf1 − ∂xf3 ∂xf2 − ∂yf1   3.2 The Riemann Integral in Rn Deﬁnition 3.8 (Closed rectangle). For n ≥ 1, a closed rectangle Q in R n is the Cartesian product of closed intervals Q = [a1, b1] × · · · × [an, bn] ⊂ Rn (83) for ai, bi ∈ R ∀i ≤ n with volume vol(Q) = n∏ i=1(bi − ai). (84) Deﬁnition 3.9 (Partition of a closed rectangle, norm of a partition). A partition P of a closed rectangle Q is a set of rectangular boxes Q1, . . . , Qk ⊂ Q such that their union forms the whole closed rectangle Q = k⋃ j=1 Qj and they are mutually exclusive (or their interior does not intersect), namely int(Qi) ∩ int(Qj) = ∅ The norm/ﬁnesse of the partition is deﬁned by Norm(P ) = δP = max i {longest edge of Qi}. (85) Deﬁnition 3.10 (Upper/Lower Riemann sum). Given a function f : Q → R on a closed rectangle Q ⊂ R n with partition P = {Q1, . . . , Qk} and intermediate points ξ = (ξ1, . . . , ξk) with ξi ∈ Qi, • the Riemann sum is given by R(f, P, ξ) = k∑ j=1 f (ξj) · vol(Qj) (86) 22 • the lower Riemann sum is given by Lf (P ) = k∑ j=1 inf x∈Qj f (x) · vol(Qj) (87) • the upper Riemann sum is given by Uf (P ) = k∑ j=1 sup x∈Qj f (x) · vol(Qj) (88) Deﬁnition 3.11 (Riemann Integral, Integrability). Let f : Q → R be a function on a closed rectangle Q ⊂ R n. Then • the lower Riemann integral is deﬁned by I(f ) = ∫ Qf dx = sup{Lf (P ) | P is a partition of Q} (89) • the upper Riemann integral is deﬁned by I(f ) = ∫ Qf dx = inf{Uf (P ) | P is a partition of Q} (90) • f is called integrable if I(f ) = I(f ) ≡ ∫ Q f (x)dx = ∫ Q f (x1, . . . , xn)dx1 · · · dxn (91) Theorem 3.4. Let f : Q → R be a function on a closed rectangle Q ⊂ R n. If f is continuous on Q, then it is integrable. Theorem 3.5. Let f, g : Q → R be integrable functions on a closed rectangle Q ⊂ R n. Then 1. αf + βg : Q → R is integrable and ∫ Q αf + βgdx = α ∫ Q f dx + β ∫ Q gdx. (92) 2. if f (x) ≤ g(x) ∀x ∈ Q, then ∫ Q f (x)dx ≤ ∫ Q g(x)dx. (93) 3. if f (x) ≥ 0 then ∫ Q f (x)dx ≥ 0. (94) 4. the triangle identity (and upper bound) is given by ∣ ∣ ∣ ∣ ∫ Q f dx ∣ ∣ ∣ ∣ ≤ ∫ Q |f |dx ≤ sup Q |f | · vol(Q) (95) 23 5. for f = 1 we get ∫ Q 1 dx = vol(Q). (96) Remark. If f ≥ 0, the integral of f is the volume of the set {(x, y) ∈ Q × R | 0 ≤ y ≤ f (x)}, i.e. the volume under the graph of f above Q. Note that this is also true for more general compact sets Q over which the function is continuous! Theorem 3.6 (Fubini for a rectangular box). Let f : Q → R be an integrable function that is continuous on the closed rectangle Q = [a1, b1] × · · · × [an, bn] with ai, bi ∈ R ∀i ≤ n. Then ∫ Q f (x1, . . . , xn) dxndxn−1 · · · dx1 = ∫ b1 a1 · · · ∫ bn an f (x) dxndxn−1 · · · dx1 (97) and we can change the order of integration! Remark. Fubini’s theorem allows us to calculate the integral as iterated integrals of one dimensional integrals! The order of the integration is not important as we can simply change the order of the arguments and the rectangle Q.5 Corollary 3.6.1 (Fubini in 2D). Let f : D → R be an integrable function that is continuous on the a subset D ⊂ R n. If D is of the form 1. DI = {(x, y) | a ≤ x ≤ b; g(x) < y < h(x)} then 6 ∫ D f (x, y)dxdy = ∫ b a (∫ h(x) g(x) f (x, y)dy ) dx (98) 2. DII = {(x, y) | c ≤ y ≤ d; G(y) < x < H(y)} then ∫ D f (x, y)dxdy = ∫ d c (∫ H(y) G(y) f (x, y)dx ) dy (99) where a, b ∈ R. Remark. This corollary shows that you have to look at the domain over which you are integrating your function. Sometimes you have to change the order of integration in order to be able to evaluate the integral (like you have seen for ∫ 1 0 ∫ 1 x exp(y2) dydx)! Theorem 3.7 (Fubini, general form). Let f : X → R be a function on a compact subset X ⊂ R n with positive integers n1, n2 such that n = n1 + n2. For x1 ∈ R n1, let Yx1 = {x2 ∈ R n2 | (x1, x2) ∈ X} ⊂ Rn2. Let X1 be the set of the points x1 ∈ R n1 such that Yx1 is not empty. Then X1 is compact in Rn1 and Yx1 is compact in R n2 for all x1 ∈ X1. If the function g(x1) = ∫ Yx1 f (x1, x2)dx2 5Have a look at the illustration in geogebra https://www.geogebra.org/m/KtskFc4a. 6Have a look at the illustration in geogebra https://www.geogebra.org/m/ypbjEFuv. 24 is continuous on X1, then ∫ X f (x1, x2)dx = ∫ x1 g(x1)dx1 = ∫ X1 (∫ Yx1 f (x1, x2)dx2 ) dx1 (100) and similarly, exchanging the role of x1 and x2, we have ∫ X f (x1, x2)dx = ∫ X2 (∫ Zx2 f (x1, x2)dx1 ) dx2 (101) where Zx2 = {x1 ∈ Rn1 | (x1, x2) ∈ X} ⊂ R n1, if the integral over x1 is a continuous function. Remark. Note that we have the assumption that g(x1) or the integral over x1 have to be continuous functions. We can however relax that condition a bit: if our set can be divided into disjoint subsets over which the function is continuous, we can add the integrals over those regions to get the integral of our function over the whole set. This statement is also given in Theorem 3.8. Theorem 3.8. If X1, X2 ∈ R n are compact subsets and f is continuous on X1 ∪ X2, then ∫ X1∪X2 f (x)dx = ∫ X1 f (x)dx + ∫ X2 f (x)dx − ∫ X1∩X2 f (x)dx. (102) If the sets X1, X2 are disjoint (meaning that X1 ∩ X2 = ∅), we have that ∫ X1∪X2 f (x)dx = ∫ X1 f (x)dx + ∫ X2 f (x)dx. (103) Remark. Note that the statement still holds if the intersection of the sets is negligible, meaning that the integral over the intersection is equal to zero. Deﬁnition 3.12 (parametrised m-set, negligible set). 1. For 1 ≤ m ≤ n a parametrised m-set in R n is a continuous function ϕ : [a1, b1] × · · · × [am, bm] → Rn with ai, bi ∈ R, which is of class C 1 on (a1, b1) × · · · × (am, bm). 2. A set Y ⊂ R n is negligible if there exist ﬁnitely many ϕi : Xi → R n parametrised mi-sets with mi < n such that Y ⊂ ⋃ i ϕi(Xi) Proposition 3.3. Let X ⊂ R n be a compact set. If X is negligible, then for any continuous function f : X → R we have ∫ X f (x)dx = 0. (104) Remark. This would be the case for a ﬁnite amount of points in R, for lines and points in R 2, planes, lines and points in R 3 and so on. This means that we can divide any set that we are integrating over into two sets and add the integral over those sets to get the integral over the whole set (if the function is continuous over those subsets). 25 3.3 Improper Integrals Deﬁnition 3.13 (Improper Riemann integral ). Let X ⊂ R n be a non-compact subset and f : X → R a function such that ∫ K f (x)dx exists for every compact subset K ⊂ X. Suppose we have a sequence of regions Xk for k = 1, 2, . . . such that 1. Xk is compact, 2. Xk+1 ⊃ Xk, 3. ⋃∞ k=1 Xk = X. If the limit lim k→∞ ∫ Xk f (x)dx exists, then we say that the it converges and deﬁne the improper Riemann integral ∫ X f (x)dx := lim k→∞ ∫ Xk f (x)dx. (105) Remark. As in the single-variable case, we can therefore also integrate over non-bounded regions and get a well deﬁned result if the function behaves well (continuous, integrable,...). Fubini’s theorem applies for those functions as well with iterated ”ordinary” improper integrals (1D cases). 3.4 The Change of Variable Formula We want to come up with an analogue to the change of variable in the single-variable case ∫ f (g(x))g′(x)dx = ∫ f (y)dy. Theorem 3.9 (Change of variables). Let X, Y ⊂ R n be compact and ϕ : X → Y a continuous function that is of class C 1 and bijective on the interiors X0 and Y0 of X and Y where X = X0 ∪ B and Y = Y0 ∪ C with X0, Y0 open and C, D negligible. Then 1. the Jacobi matrix Jϕ(x) is invertible for all x ∈ X0, 2. for any continuous function f on Y we have ∫ Y f (y)dy = ∫ X f (ϕ(x)) · |det Jϕ(x)|dx. (106) Remark. 1. Intuition Intuitively, the factor |det Jϕ(x)| is coming from the volume transformation of the domain of X. We can see this by choosing ϕ : X → Y to be a linear map with representation A ∈ R n×n. Integrating the constant function f = 1 both with and without the change of variable yields exactly vol(Y ) = |det(A)| vol(A −1Y ) ⇐⇒ vol(AX) = |det(A)| vol(X). As now every function can locally be approximated by a linear map, this formula should hold for every variable substitution ϕ. 26 2. Examples for changes of variable (see 2.4) (a) polar coordinates dxdy = rdrdϕ (107) (b) cylindrical coordinates dxdydz = rdθdrdz (108) (c) spherical coordinates dxdydz = r2 sin ϕdrdθdϕ (109) 3.5 Geometric Applications of Integrals 3.5.1 Centre of Mass For a compact set X ⊂ R n with uniform density, the centre of mass is a point where X is ”perfectly balanced”. This point is given by ˜x = (˜x1, ˜x2, ..., ˜xn) ∈ R n, where the ˜xi’s are the ”average over the xi coordinates”, namely ˜xi = 1 vol(X) ∫ X xi dx1dx2...dxn. (110) For a general density given by a function f (x1, ..., xn), the coordinates of the centre of mass are given by ˜xi = ∫ X xi · f (x1, ..., xn) dx1dx2...dxn∫ X f (x1, ..., xn) dx1dx2...dxn , (111) where the denominator is sometimes called mass of the set X. 3.5.2 Surface Area (n = 3) Suppose a surface S in R 3 is given by the graph of a function f : X ⊂ R 2 → R, namely S = {(x, y, z) ∈ R 3 | (x, y) ∈ X, z = f (x, y)}, then its surface is given by Area(S) = ∫ ∫ S √ 1 + ( ∂f ∂x (x, y) )2 + ( ∂f ∂y (x, y))2dxdy (112) 3.6 Green’s Formula Green’s formula is sort of a generalisation of the fundamental theorem of calculus in the sense that it relates a two-dimensional integral of over a compact region X to a one-dimensional (line-) integral along the boundary of the region ∂X. Deﬁnition 3.14 (Simple closed parameterized curve). A simple closed parameterized curve γ : [a, b] → R 2 is a closed parameterized curve such that γ(t) ̸= γ(s) unless t = s or {t, s} = {a, b} and such that γ′(t) ̸= 0 for a < t < b. 27 Theorem 3.10 (Green’s Theorem (curl-circulation form / tangential form)). Let X ⊂ R 2 be compact with boundary ∂X that is the union of ﬁnitely many simple closed parameterized curves γ1, . . . , γk. Assume that γi : [ai, bi] → R2 has the property that X lies always ”to the left” of the tangent vector based at γi(t). Let f = (f1, f2) be a vector ﬁeld of class C 1 deﬁned on some open set containing X. Then we have ∫ ∫ X ( ∂f2 ∂x − ∂f1 ∂y ) ︸ ︷︷ ︸ curl(f ) dxdy = k∑ i=1 ∫ γi f (⃗s) · d⃗s (113) Remark. 1. There are compact regions (like the doughnut/torus in 2D) that are compact but cannot be parameterized using one simple closed parameterized curve. Rather we have to use multiple (in this case two) simple closed parameterized curves (one for the outer circle, one for the inner circle). Note that the orientation of the parameterization (clockwise/counter- clockwise) depends on the form of the region and is given by the condition that X has to lie to the left of the tangent vector. 2. Green’s Theorem can be used to calculate the area of a region as a line integral (as we have done in the exercise class) by taking the vector ﬁeld to be f (x, y) = (0, x) for example k∑ i=1 ∫ γi f (⃗s) · d⃗s = ∫ ∫ X ( ∂f2 ∂x − ∂f1 ∂y ) ︸ ︷︷ ︸ =1 dxdy = vol(X) 3. We can use Green’s Theorem to calculate the line integral if the double integral of the curl of f looks easier (especially if the curl is equal to zero!) and vice versa. Deﬁnition 3.15 (Divergence of a vector ﬁeld). Let f ∈ C 1(R n) be a vector ﬁeld. Then the divergence of f is deﬁned by div(f ) = ∇ · f = ∂f1 ∂x1 + · · · + ∂fn ∂xn (114) which is a scalar function div(·) : Rn → R. Deﬁnition 3.16 (Exterior normal to a parameterized curve). Let n = 2 and X ∈ R2 be a compact set which can be parameterized by a simple closed parameterized curve γ : [a, b] → R2, γ(t) ↦→ (γ1(t) γ2(t) ) . Then the exterior normal to that parametrization is given by n(t) = ( γ′ 2(t) −γ′ 1(t) ) (115) which satisﬁes n(t) · γ′(t) = 0 and ∥n(t)∥ = ∥γ′(t)∥. 28 Remark. The deﬁnition in the three-dimensional case with a parameterized surface Σ : [a, b] × [c, d] → R 3, Σ(s, t) = (s, t, g(s, t)) involves the cross product of the partial derivatives of the parameterization ⃗n(s, t) = ∂sΣ × ∂tΣ. Theorem 3.11 (Green’s Theorem (divergence ﬂux form / normal form)). Let X ⊂ R 2 be compact with boundary ∂X that is the union of ﬁnitely many simple closed parameterized curves γ1, . . . , γk. Assume that γi : [ai, bi] → R2 has the property that X lies always ”to the left” of the tangent vector based at γi(t). Let f = (f1, f2) be a vector ﬁeld of class C 1 deﬁned on some open set containing X. Then we have ∫ ∫ X div(f )dxdy = k∑ i=1 ∫ γi f (⃗s) · d⃗ni, (116) where ⃗ni is the exterior normal vector to the curve γi. Remark. 1. One can easily derive that form from Theorem 3.10 by choosing ˜f = (f2, −f1) and applying Green’s formula (in its tangential form) to it. 2. This form is speciﬁcally interesting when considering higher dimensions (n = 3). With it, we can measure how much ﬂux is passing through the boundary of a compact region we are looking at so that we can determine whether or not we have a source inside the region (e.g. a charge distribution). 29 A Useful Theorem Theorem A.1 (Limits and polar coordinates7). Let f : R 2 → R be a function. Assume that f (r · cos(θ), r · sin(θ)) = F (r)G(θ), where F, G : R → R are functions which obey F (r) → 0 as r → 0 and |G(θ)| < M for all θ and some M ∈ R. Then lim (x,y)→(0,0) f (x, y) = 0. (117) Remark. Note that the condition is needed by considering the following example: Let f (x, y) = xy2 x2 + y4 be the function we want to consider. We calculate the limit using polar coordinates and the square root path and show that the limit does not exist although the polar coordinates lead to a plausible solution: 1. Polar coordinates: lim r→0 f (r cos(θ), r sin(θ)) = lim r→0 r cos(θ)r2 sin 2(θ) r2 cos2(θ) + r4 sin 4(θ) = lim r→0 r cos(θ) sin 2(θ) cos2(θ) + r2 sin 4(θ) = 0 (for ﬁxed θ) 2. Square root ((x, y) = (t 2, t)): lim t→0 f (t 2, t) = lim t→0 t 2t 2 t4 + t4 = lim t→0 t4 2t4 = 1 2 Hence, we have shown that the polar coordinates cannot be used in general to calculate the limit of a function! B ”Set Theory” Deﬁnition B.1 (Bounded, closed, open and compact subsets). A subset X ⊂ R n is called 1. bounded if the set of ∥x∥ for x ∈ X is bounded in R. 2. closed if for every sequence (xk)k ∈ X that converges to y ∈ R n we have that y ∈ X. 3. open if for any x = (x1, . . . , xn) ∈ X there exists a δ > 0 such that the set Bδ(x) = {y ∈ Rn : ∥x − y∥ < δ}, called the open ball with radius δ around x, is contained in X. 4. compact if it is bounded and closed. Proposition B.1. A set is X ⊂ R n is open/closed if and only if its complement Y = R n \\ X = {x ∈ R n | x ̸∈ X} (118) is closed/open. 7https://www.youtube.com/watch?v=9KrSU9E5PhY 30 Proposition B.2. Let f : Rn → R m be a continuous map. For any closed (open) set Y ⊂ R m, the set f −1(Y ) = {x ∈ R n | f (x) ∈ Y } ⊂ R n is closed (open). Remark. If you want to prove that a set C ⊂ R n is closed, you would most likely look at its complement O = Rn \\ C and prove that this is open (the statements are the same as proposition B.1 shows). Additionally, you can investigate whether it is the inverse image of a closed set, following proposition B.2. If you want to prove that a set X ∈ R n is open, you can just choose any point r ∈ X in your set and then ﬁnd a δ > 0 such that Bδ(r) ⊂ X. This is then fulﬁlling the deﬁnition of an open set. Additionally, you can investigate whether it is the inverse image of an open set, following proposition B.2. To show that a set is not closed, you can try to ﬁnd a convergent sequence whose limit does not lie in the set you are looking at. 31","libVersion":"0.3.2","langs":""}