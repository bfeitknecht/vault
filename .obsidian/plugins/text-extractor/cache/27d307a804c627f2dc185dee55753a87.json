{"path":"sem3/LinAlg/VRL/extra/LinAlg-lecture-notes-pt.2.pdf","text":"Linear Algebra Fall 2023 (ETHZ 401-0131-00L) Lecture Notes Part II Afonso S. Bandeira ETH Z¨urich Last update on January 27, 2024 1 2 “READ ME” FOR PART II My webpage, with contact information, is: https://people.math.ethz.ch/˜abandeira These lecture notes serve as a continuation 1 of Part I, taught by Prof. Bernd G¨artner, available at https://ti.inf.ethz.ch/ew/courses/LA23/notes_part_I.pdf. Please read the Preface there. Please note there may be some changes in notation. Furthermore, we will try to stay close to the notation in [Str23], but there also be some differences. [Str23] Gilbert Strang. Introduction to Linear Algebra. Wellesley - Cambridge Press, 6th ed., 2023. The course page has relevant information for the course: https://ti.inf.ethz.ch/ew/ courses/LA23. I offer office hours (in HG G23.1) almost weekly, feel free to stop by, to chat about the course, Mathematics, Computer Science, or University in general. Most office hours visitors stop by to learn more about research in Mathematics and Computer or Data Science. You can see the schedule on my webpage, on the calendar applet on the left. There are countless excellent Linear Algebra books with the material covered in this course. For Part II we will roughly continue to follow, in structure and content, [Str23], with some small devi- ations. I will try to keep the numbering of Chapters/Sections and Sections/Subsections consistent with [Str23] (as far as the deviations allow). See Appendix A for some important preliminaries and some remarks on notation. Throughout the notes, and the lectures, I will try to motivate some of the material with Guid- ing Questions. For students who would like to explore the topic further, I will include some Exploratory Challenges and Further Reading, these often will include difficult problems or topics. I will also take some opportunities to share some active Research Questions related to the topics we covered (we are still discovering new phenomena in Linear Algebra today and for many years to come!). After deriving a result, we will often do some Sanity Checks, and some things I will leave as a Challenge: these should be accessible and of difficulty comparable to homework questions, a ⋆ indicates a harder problem (but still within the scope). On the other hand, Exploratory Challenges are generally outside the scope of the course or of substantial higher difficulty. 1If you are reading these notes and did not follow Part I, please read Appendix A. 3 Linear Algebra is a beautiful topic, connecting Algebra with Geometry2, and has countless applications making it a key component of almost all quantitative pursuits. I sincerely hope you will enjoy the course as much as I enjoy teaching this subject! MISCELLANEOUS THOUGHTS I believe the Questions, Sanity Checks, Challenges, etc are very useful to learn the material, but when you want to review the material, or do a last read before the exam, you can focus on the Definitions, Propositions, Theorems, etc (and focus less on the blue parts). In many of my side comments (usually in blue), and in some of the CS Lens Lectures, I do not include specific citations, and sometimes use technical terms that you might not have seen before. My goal is, quoting my collaborator Dustin Mixon, “to provide enough breadcrumbs for the interested reader to find more information online”. 3 If you would like specific references, tell me a bit more about your interests and I would be happy to point you to some references (different references are better for different takes/interest on each of the topics). While CS Lens Lectures are not covered in the lecture notes, slides can be accessed in the course website: https: //ti.inf.ethz.ch/ew/courses/LA23/index.html As your mathematical level matures over the semester, the notes will have less illustrations and more definitions and mathematical statements. My recommendation is to read the notes with pen & paper next to you and to draw the picture yourself, this “translation” you will be doing — from mathematical statement to picture — will (I believe) help you greatly in the learning of Mathematics! There are also countless high-quality videos and other content online about Linear Algebra, for example there is also an excellent series of videos by Gil Strang filmed ∼15 years ago: https: //www.youtube.com/playlist?list=PLE7DDD91010BC51F8. Strang actually retired just a few months ago, at almost 90 years of age! You can see his last lecture online: https://www.youtube.com/watch?v=lUUte2o2Sn8 2and Analysis, as you will likely see later in your academic life. For example, when Joseph Fourier invented Fourier Series to developed a theory of heat transfer he was essentially finding good orthonormal bases for functions. 3This itself also provides enough breadcrumbs for you to find the lecture notes I am quoting; they are excellent Linear Algebra lecture notes! (the order and content is somewhat different from our course) 4 Moreover, there are many excellent animations online giving lots of great intuition on several Linear Algebra topics and phenomena. While it is a great idea to take advantage of this, I would recommend first trying yourself to develop an intuition of the concept/phenomenon (e.g. by drawing a picture) and using these tools only after — use them to improve your intuition, not to create it! As these Lecture Notes are being continuously updated, and sometimes the discussion in lectures leads us into proving an extra result, or suggests a remark, etc, I will try to add then and not change the numbering of things downstream, I do this by numbering them with +1000. After each lecture, we post the handwritten notes from lecture on the course website https: //ti.inf.ethz.ch/ew/courses/LA23/index.html. My suggestion would be to use the Lecture Notes to review the material, not the handwritten notes (which are mainly meant to support my oral exposition). CONTENTS “Read me” for Part II 2 Miscellaneous Thoughts 3 4. Orthogonality, Projections, and Least Squares 6 4.2. Projections 6 4.3. Least Squares Approximation 10 4.4. Orthonormal Bases and Gram Schmidt 14 4.5. The Pseudoinverse, also known as Moore–Penrose Inverse 19 5. Linear Transformations and Determinants 23 5.1. The Determinant 28 6. Eigenvalues and Eigenvectors 35 6.0. Complex Numbers 36 6.1. Introduction to Eigenvalues and Eigenvectors 39 6.2. Diagonalizing a Matrix and Change of Basis of a Linear Transformation 49 5 6.3. Symmetric Matrices and the Spectral Theorem 51 7. Singular Value Decomposition; and some open questions in Linear Algebra 58 7.1. The Singular Value Decomposition 58 7.2. Vector and Matrix Norms 61 7.3. Low-Rank Modelling, Images, Data, and Principal Component Analysis 62 7.10. Some Mathematical Open Problems 62 Acknowledgements 64 Appendix A. Some Important Preliminaries and Remarks on Notation 64 Appendix B. Weekly Schedule 65 Appendix C. CS Lens Lectures (not part of Chapter 7) 65 Appendix D. A “Simple proof” of Fundamental Theorem of Algebra 66 66 6 Linear Algebra — A. Bandeira (ETHZ) — Week 7 - Part II - 2023.11.08 Please find most up to date notes at: https://ti.inf.ethz.ch/ew/courses/LA23/notes_part_II.pdf 4. ORTHOGONALITY, PROJECTIONS, AND LEAST SQUARES Guiding Question 1. If we have a system of linear equations that has no solution, how do we find the “solution” that has the smallest error? This question is central in countless applications 4. Before diving into systems of equations, we will study Projections of vectors in a subspace. 4.2. Projections. Definition 4.2.1 (Projection of a vector onto a subspace). The projection of a vector b ∈ Rm on a subspace S (of Rm) is the point in S that is closest to b. In other words (1) projS(b) = argmin p∈S ∥b − p∥. Sanity Check 2. This is only a proper definition if the minimum exists and is unique. Can you show it exists and is unique? (perhaps at the end of the lecture?) Let us build us some intuition by starting with projections to a line. Let S be the subspace corresponding to the line that goes through the vector a, i.e. S = Span(a). FIGURE 1. Projection on a line. 4as you will see later on, it is in a sense what Machine Learning is all about. 7 The projection p is the vector in the subspace S such that the “error vector” e = b − p is perpen- dicular to a (i.e. b − p ⊥ a). Since p ∈ S we have p = ˆxa, for some ˆx ∈ R. Since b − p ⊥ a we have a⊤(b − p) = 0. Substituting gives a⊤(b − ˆxa) = 0 ⇐⇒ ˆx = a⊤b a⊤a ⇐⇒ p = a⊤b a⊤aa ⇐⇒ p = aa⊤ a⊤ab. Indeed, we have the following Proposition. Proposition 4.2.2. Let a ∈ Rm be a non-zero vector. The projection of a vector b ∈ Rm on S = Span(a) the span of a, is given by projS(b) = aa⊤ a⊤ab. Sanity Check 3. The projection of a vector that is already a multiple of a should be the identity operation. Check that this is the case! (and do it later, again, for general subspaces). For general subspaces the idea is precisely the same. Let S be a subspace in Rm with dimension n. Let a1, . . . , an be a basis of S, meaning that S = Span(a1, . . . , an) and S = C(A) where A =    | | | a1 a2 · · · an | | |    . FIGURE 2. Projection on a subspace. Similarly to the case of a line, it is easy to see (see Figure 2) that the projection p of a vector b on S is such that the error vector e = b − p is perpendicular to each of the ak’s. To prove this fact rigorously we start by showing existence of such a vector p: take the orthogonal complement S⊥ of S and write b as a sum of e ∈ S⊥ and p ∈ S, then e = b − p is orthogonal to the subspace S. Now, let us assume that there exists another point p′ (as in Figure 2) and note that since p′ − p ∈ S we have that b − p ⊥ p′ − p, and so, by Pythagoras’ Theorem we have ∥p′ − b∥2 = 8 ∥p − p′∥2 + ∥p − b∥2, which implies that ∥p′ − b∥2 ≥ ∥p − b∥2 (with equality holding only when p = p′).5 6 We just showed that a⊤ k (b − p) = 0 for k = 1, . . . , n. In matrix-vector notation    | | | a1 a2 · · · an | | |    ⊤ (b − p) = 0 ⇐⇒ A⊤(b − p) = 0. Since p ∈ C(A) we have p = A ˆx for some ˆx ∈ Rn. This means that A ⊤(b − A ˆx) = 0 ⇐⇒ A ⊤A ˆx = A ⊤b. We just proved the following Proposition. Proposition 4.2.3. The projection p of a vector b ∈ Rm on a subspace S with a basis a1, . . . , an can be written as p = A ˆx where ˆx ∈ Rn satisfies the normal equations A⊤A ˆx = A⊤b, where A =    | | | a1 a2 · · · an | | |    is the matrix whose columns are a basis of S. If we can show that A⊤A is invertible then we would have p = A ˆx = A ( A⊤A )−1 A⊤b. Let’s make a detour to show that it is indeed invertible. Proposition 4.2.4. A⊤A is invertible if and only if A has linearly independent columns. Proof. We show this by showing that A⊤A and A have the same nullspace. This is enough because, since A⊤A is a square matrix it is invertible if and only if its nullspace only has the 0 vector, and A has linearly independent columns if and only if its nullspace only has the 0 vector. 7 If x ∈ N(A) then Ax = 0 and so A⊤Ax = 0, thus x ∈ N(A⊤A). The other implication is more interesting. If x ∈ N(A⊤A) then A⊤Ax = 0. This implies that x⊤A⊤Ax = x⊤0 = 0. But x⊤A⊤Ax = (Ax)⊤(Ax) = ∥Ax∥2 so Ax must be a vector with norm 0 which implies that Ax = 0 and so x ∈ N(A). 2 5We have also, as a byproduct, answered the question in Sanity Check 2. 6Sometimes the projection is simply defined as the point on the subspace such that the error vector is orthogonal to the subspace, here we showed the two possible definitions are equivalent. 7We usually call a nullspace with only the zero vector, a trivial nullspace. 9 Corollary 4.2.5. If A has linearly independent columns then A⊤A is a square matrix, it is invert- ible and symmetric. 8 Now back to deriving a formula for projections: Since the columns of A are a basis they are linearly independent and so A⊤A is indeed invertible. We just proved the following. Theorem 4.2.6. Let S be a subspace in Rm and A a matrix whose columns are a basis of S. The projection of b ∈ Rm to S is given by projS(b) = Pb, where P = A ( A⊤A)−1 A⊤ is the projection matrix. The matrix P = A ( A⊤A)−1 A⊤ is known as a Projection Matrix, it maps a vector b to its projection Pb on a subspace S. For the case of lines, P was given by P = aa⊤ a⊤a = a 1 a⊤a a⊤. Caution! 4. The matrix A (and A⊤) are not necessarily square, and so they don’t have inverses. The expression A ( A⊤A)−1 A⊤ cannot be simplified by expanding ( A⊤A)−1 (which would yield I = P, this would only make sense if S was all of Rm and note that, unsurprisingly, this would correspond exactly to the case when A is invertible). Just as with the “sanity check” above, we should have P2 = P, because if we project a point twice, the second time should not do anything as the point is already in S and indeed P 2 = ( A ( A⊤A )−1 A ⊤)2 = A ( A⊤A )−1 A ⊤A ( A⊤A)−1 A⊤ = A ( A⊤A )−1 A ⊤ = P. Challenge 5. Is I − P a projection? If so, which projection does it correspond to? Challenge 6. How does the rank of P depend on properties of the subspace S? Exploratory Challenge 7. We derived all of the formulas for projections using geometry. If you have taken Analysis/Calculus (I know many of you haven’t, but you will in a few months) you can try to re-derive everything using the fact that derivatives at the minimum should be zero. You will see that you will get exactly the same answers. In lecture, when discussing Figure 2 we explicitly proved the following proposition. Proposition 4.2.7. Let S be a subspace in Rm with a basis a1, . . . , an. For v ∈ Rm, v being orthogonal to all vectors in S is equivalent to being orthogonal to ak for all 1 ≤ k ≤ n. 8Corollary is like a Theorem or a Proposition but one that follows directly from another one, this one follows directly from the Proposition above. 10 Proof. Since a1, . . . , an are in S, if v is perpendicular to all vectors in S, it is in particular perpen- dicular to a1, . . . , an. On the other hand, any w ∈ S can be written as w = α1a1 + · · · + αnan and w⊤v = α1a⊤ 1 v + · · · + αna⊤ n v = 0. 2 Linear Algebra — A. Bandeira (ETHZ) — Week 8 - 2023.11.10 & 2023.11.15 Please find most up to date notes at: https://ti.inf.ethz.ch/ew/courses/LA23/notes_part_II.pdf 4.3. Least Squares Approximation. We go back to the guiding question of what to do when we want to “solve” a linear system that does not have an exact solution. More precisely let us suppose we have a linear system Ax = b, for which no solution x exists (for example, with too many equations, which would happen if A ∈ Rm×n and m > n). A natural approach is to try to find x for which Ax is as close as possible to b (2) min ˆx∈Rn ∥A ˆx − b∥ 2. Further Remark 8. This seemingly simple observation is key to countless technologies. Mea- surement systems often have errors and so it is impossible to find the target object/signal x that satisfies them all exactly, and we look for the one that satisfies them the best possible. In Data Science and Learning Theory we often want to find a predictor that best describes a set of training data, but usually no predictor described the data exactly, so we look for the best possible, etc etc. We’ll see a couple of applications later. We can solve this problem using the ideas we developed above. What we are looking for is a vector ˆx for which the error e = b − A ˆx is as small as possible. Since the set of possible vectors y = A ˆx is exactly C(A), A ˆx is precisely the projection of b on C(A). As we saw in the Section above, this means that A⊤(b − A ˆx) = 0. These are known as the normal equations and can be rewritten as (3) A⊤A ˆx = A ⊤b. Remark 4.3.1. For this to make sense it must be that (3) always has a solution. If we think geometrically, it is relatively easy to see that it must, because of how we constructed the normal equations. Can you give a rigorous algebraic proof of this fact? Note that essentially what you are proving is the Proposition below. Proposition 4.3.2. For any matrix A, C(A⊤) = C(A⊤A). 11 Challenge 9. Try to prove this Proposition. This can be done in a few different ways. I suggest starting by trying to show that rank(A) = rank(A⊤) = rank(A⊤A) = rank(AA⊤). We know that if A has linearly independent columns, then A⊤A is invertible and so we can write ˆx = (A⊤A)−1A⊤b. We will address the case in which A has dependent columns shortly. Fact 4.3.3. A minimizer of (2) is also a solution of (3). When A has independent columns the unique minimizer ˆx of (2) is given by (4) ˆx = (A⊤A) −1A ⊤b Exploratory Challenge 10. Similarly to the projections derivation, this derivation can also be done by differentiating (2). Try it. 4.3.2. Linear Regression — fitting a line to data points. One of the most common tasks in data analysis is linear regression, to fit a line through data points. Let us consider data points (t1, b1), (t2, b2), . . . , (tm, bm), perhaps representing some attribute b over time t. If the relation between t and b is (at least partly) explained by a linear relationship then it makes sense to search for constants α0 ∈ R and α1 ∈ R such that bk ≈ α0 + α1tk. See Figure 3. In particular, it is natural to search for α0 and α1 that minimize the sum of squares of the errors (“least squares”), min α0,α1 m ∑ k=1 (bk − [α0 + α1tk]) 2 . In matrix-vector notation (5) min α0,α1 ∥ ∥ ∥ ∥ ∥ b − A [ α0 α1 ]∥ ∥ ∥ ∥ ∥ 2 , where b =         b1 b2 ... bm−1 bm         and A =         1 t1 1 t2 ... ... 1 tm−1 1 tm         . 12 FIGURE 3. Fitting a line to points As long as A has independent columns (see Remark 4.3.4) the solution to (5) is given by [ α0 α1 ] = (A⊤A) −1A⊤b = [ m ∑ m k=1 tk ∑ m k=1 tk ∑ m k=1 t2 k ]−1 [ ∑ m k=1 bk ∑ m k=1 tkbk ] Remark 4.3.4. It is worth working out what it means for the columns of A, in this example, to be linearly dependent. It essentially corresponds to all points tk being the same, which is clearly a degenerate case of linear regression. Remark 4.3.5. If the columns of A are pairwise orthogonal, then A⊤A is a diagonal matrix, which is easy to invert. In this example, the columns of A being orthogonal corresponds to ∑ m k=1 tk = 0. We could simply do a change of variables to a new time tnew k = tk − 1 m ∑ m i=1 ti to achieve this. If indeed ∑ m k=1 tk = 0 then the equation above could be easily simplified: [ α0 α1 ] = [ m 0 0 ∑ m k=1 t2 k ]−1 [ ∑ m k=1 bk ∑ m k=1 tkbk ] = [ 1 m 0 0 1 ∑ m k=1 t2 k ] [ ∑ m k=1 bk ∑ m k=1 tkbk ] = [ 1 m ∑ m k=1 bk (∑ m k=1 tkbk) / ( ∑ m k=1 t2 k ) ] , this is an instance where having orthogonal vectors is beneficial. In this next Section we will see how to build orthonormal basis for subspaces, and some of the many benefits they have. 13 Challenge 11. Try to work out the actual change of variables that makes the tk’s add up to zero and derive a formula for fitting a line to points without the assumption in Remark 4.3.5 Example 4.3.6 (Fitting a Parabola). We can use Linear Algebra to do fits of many other curves (or functions), not just lines. If we believe the relationship between tk and bk is quadratic we could attempt to fit a Parabola: bk ≈ α0 + α1tk + α2t2 k . While this isn’t a linear function in tk, this is still a linear function on the coefficients α0, α1, and α2, and this is what is important. Similarly as with linear regression, it is natural to attempt to minimze (6) min α0,α1,α2 ∥ ∥ ∥ ∥ ∥ ∥ ∥ b − A    α0 α1 α2    ∥ ∥ ∥ ∥ ∥ ∥ ∥ 2 , where b =         b1 b2 ... bm−1 bm         and A =         1 t1 t2 1 1 t2 t2 2 ... ... 1 tm−1 t2 m−1 1 tm t2 m         , and we can use the technology we developed in this section to solve this problem as well. Challenge 12. Try to work out the example of fitting a parabola further. What is A⊤A? When is A⊤A diagonal? Further Reading 13. There is a whole (beautiful) area of Mathematics related to studying so- called Orthogonal Polynomials. The basic idea can be already hinted at from these examples: In the example of the parabola we wrote a function of t as a linear combination of the polynomials 1, t, and t2. But we could have picked other polynomials, we could have e.g. written something like b ≈ α ′ 0 + α ′ 1(t − 2023) + α2(t2 +t), and a particularly good choice (that would depend on the distribution of the points tk) might have resulted in a diagonal matrix A⊤A... search “orthogonal polynomials” online to learn more. Further Reading 14. A lot of Machine Learning includes Linear Regression as a key component. The idea is to create, find, or learn features of the data points. Given n data points t1, . . .tn (which now can be perhaps pixel images, rather than just timepoints) we might want to do classification (for example, in the case of images, maybe we want a function that is large when the picture has a dog in it and small when it has a cat in it). It is hard to imagine that this can be done with 14 a linear fit, but if we build good feature vectors ϕ(tk) ∈ Rp for very large p then the function can depend on all coordinates of ϕ(tk) (the p features) and this is incredible powerful. There are several ways to construct features, a bit over a decade ago they were sometimes handmade, now they are often learned (this is in a sense what Deep Learning does). Another important way to build (or compute with) features are the so-called Kernel Methods, you can see more in the CS Lens Lecture (Appendix C). 4.4. Orthonormal Bases and Gram Schmidt. When we think of (or draw) a basis of a sub- space, we tend to think of (or draw) vectors that are orthogonal (have an angle of 90◦) and that have the same length (length 1). Indeed, these bases have many advantages, this section is about these bases, some of their advantages, and how to find them. Definition 4.4.1 (Orthonormal vectors). We say n vectors q1, . . . , qn ∈ Rm are orthonormal if they are orthogonal and have norm 1. In other words, for all i, j = 1, . . . , n qT i q j = δi j, where δi j is the Kronecker delta (7) δi j = { 0 if i ̸= j 1 if i = j. If Q is the matrix whose columns are the vectors qi’s, then the condition that the vectors are orthonormal can be rewritten as Q⊤Q = I. Caution! 15. Q may not be a square matrix, and so it is not necessarily the case that QQ⊤ = I. Example 4.4.2. A classical example of an orthonormal set of vectors is the canonical basis, e1, . . . , en ∈ Rn where ei is the vector with a 1 in the i-th entry and 0 in all other entries: (ei) j = δi j. When Q is a square matrix then Q⊤Q = I implies also that QQ⊤ = I and so Q−1 = Q⊤. We call such matrices orthogonal matrices. This corresponds to the case when the qi’s are an orthonormal basis for all of Rn. Definition 4.4.3 (Orthogonal Matrix). A square matrix Q ∈ Rn×n is an Orthogonal Matrix when Q⊤Q = I. In this case, QQ⊤ = I, Q−1 = Q⊤, and the columns of Q form an orthonormal basis for Rn. Remark 4.4.4. It is often useful to think of an m × n matrix A as a function from Rn to Rm, that takes x ∈ Rn to Ax ∈ Rm. A : Rn → Rm x → Ax 15 Later in the course, when we discuss Linear Transformations, we will, among other things, dis- cuss which functions can be described by a matrix this way (and some properties of these func- tions/transformations). For now, let us just keep in mind that a matrix can be thought of as a function. It is also worth noting that this explains why in some Linear Algebra books the Nullspace is called the Kernel (it is the set of vectors x that are mapped to 0 by this function) and the Column Space is called Image, or Range, as it is the set of vectors in Rm that is the image of this function. Example 4.4.5. The 2 × 2 matrix Q that corresponds to rotating, counterclockwise, the plane by θ , Rθ = [ cos θ − sin θ sin θ cos θ ] is an orthogonal matrix. Challenge 16. Prove that the rotation matrices Rθ are orthogonal matrices. Are there other 2 × 2 orthogonal matrices? If so, can you describe them all? Example 4.4.6. Permutation matrices are another example of orthogonal matrices. Challenge 17. Show that indeed permutation matrices are orthogonal matrices Exploratory Challenge 18. One of the most important structures in Algebra is that of a group. The set of Permutations of n elements is an example of a group, two permutations can be com- posed to form another permutation and for every permutation there is one corresponding to un- doing it (called the inverse). Permutation matrices represent the permutations, composing cor- responds to matrix multiplication and the inverse permutation corresponds to the matrix inverse of the permutation matrix. There is a whole field of Mathematics, called Representation Theory, that studies matrix representations of groups (and in many important cases the matrices involved are orthogonal). Can you come up with a matrix representation of addition modulo 2? What about addition modulo 5? Challenge 19 (⋆). Show that for every permutation matrix P there exists a positive integer k such that Pk = I. Proposition 4.4.7. Orthogonal matrices preserve norm and inner product of vectors. In other words, if Q ∈ Rn×n is orthogonal then, for all x, y ∈ Rn ∥Qx∥ = ∥x∥ and (Qx) ⊤(Qy) = x⊤y 16 Proof. To show the second inequality note that, for x, y ∈ Rn we have that (Qx)⊤(Qy) = x⊤Q⊤Qy = x⊤Iy = x⊤y. To show the first equality note that, since for x ∈ Rn we have that ∥Qx∥ ≥ 0 and ∥x∥ ≥ 0, it suffices to show that the squares are equal and indeed ∥Qx∥2 = (Qx)⊤(Qx) = x⊤x = ∥x∥2. 2 4.4.1. Projections with Orthonormal Basis. One of the advantages of orthonormal basis is that projections become much simpler. The reason is simple: when we discussed projections and least squares, many of the expressions we derived included A⊤A, but in the case when A has or- thonormal columns, these all simplify as A⊤A = I. We collect these observations in the following proposition. Proposition 4.4.8. Let S be a subspace of Rm and q1, . . . , qn be an orthonormal basis for S. Let Q be the m × n matrix whose columns are the qi’s; Q = [ q1 , · · · , qn ] . Then the Projection Matrix that projects to S is given by QQ⊤ and the Least Squares solution to Qx = b is given by ˆx = Q⊤b. Remark 4.4.9. When Q is a square matrix then the projection QQ⊤ is simply the identity (corre- sponding to projecting to the entire ambient space Rn. Even in this seemingly trivial instance, it is useful to look closer at what this operation does: For a vector x ∈ Rn it gives x = q1 ( q⊤ 1 x) + q2 ( q⊤ 2 x) + · · · + q1 ( q ⊤ n x) . It is writing x as a linear combination of the orthonormal basis {qi}n i=1 (as we will see later this is sometimes referred to as a change of basis).9 4.4.2. Gram-Schmidt Process. Hopefully by now you are convinced that orthonormal basis are useful, now we discuss how to construct them. Fortunately, there is a relatively simple process to construct orthonormal bases, that will also suggest a new matrix factorization. The idea is simple: If we have 2 linearly independent vectors a1 and a2 which span a subspace S, it is straightforward to transform them into an orthonormal basis of S: we first normalize a1: q1 = a1 ∥a1∥ , then subtract from a2 a multiple of q1 so that it becomes orthogonal to q1, followed by a normalization step: q2 = a2 − (a⊤ 2 q1)q1∥ ∥a2 − (a⊤ 2 q1)q1∥ ∥. 9There are countless instances in which doing this operation is beneficial, for example one of the most important algorithms, the Fast Fourier Transform, is an instance of this operation. 17 Let us check that indeed these vectors are orthonormal: By construction they have unit norm, and q⊤ 1 q2 = q⊤ 1 a2 − (a⊤ 2 q1)q1∥ ∥a2 − (a⊤ 2 q1)q1∥ ∥ = q⊤ 1 a2 − (a⊤ 2 q1)q⊤ 1 q1∥ ∥a2 − (a⊤ 2 q1)q1∥ ∥ = 0 ∥ ∥a2 − (a⊤ 2 q1)q1∥ ∥ = 0. Note that the denominator is not zero because a1 and a2 are linearly independent; and that, since q1 has unit norm, (a⊤ 2 q1)q1 = projSpan(q1)(a2). For more vectors, the idea is to this process recursively, by removing from a vector ak+1 the projection of it on the subspace spanned by the k vectors before it. More formally: Algorithm 4.4.10. [Gram-Schmidt Process] Given n linearly independent vectors a1, . . . , an that span a subspace S, the Gram-Schmidt process constructs q1, . . . qn the following way: • q1 = a1 ∥a1∥ . • For k = 2, . . . , n do q′ k = ak − ∑ k−1 i=1 (a⊤ k qi)qi qk = q′ k ∥q′ k∥ . Theorem 4.4.11 (Correctness of Gram-Schmidt). Given n linearly independent vectors a1, . . . , an, the Gram-Schmidt process outputs an orthonormal basis for the span of a1, . . . , an. Proof. 10 We prove this by induction. 11 Let Sk be the subspace spanned by a1, . . . , ak. Then S = Sn. We will show, by induction, that q1, . . . , qk are an orthonormal basis for Sk. It is enough to show that they are orthonormal and are in Sk since orthonormality implies linearly independence and Sk has dimension k. For the base case, note that ∥q1∥ = 1 and q1 is a multiple of a1 and so q1 ∈ S1. Now we assume the hypothesis for i = 1, . . . k −1 and prove it for k. By the hypothesis q1, . . . , qk−1 are orthonormal, so we have to show that ∥qk∥ = 1 and that q⊤ i qk = 0 for all 1 ≤ i ≤ k − 1. • Since ak is linearly independent from the other original vectors it is not in Sk−1 and so q′ k ̸= 0. Thus ∥qk∥ = 1. • By construction ak ∈ Sk and so qk ∈ Sk. • Let 1 ≤ j ≤ k − 1. Since q1, . . . , qk−1 are orthonormal, we have q⊤ j ( ak − k−1 ∑ i=1(a⊤ k qi)qi ) = q⊤ j ak − k−1 ∑ i=1(a ⊤ k qi)q ⊤ j qi = q⊤ j ak − (a ⊤ k q j) = 0, and q⊤ j qk = 1 ∥q′ k∥ q⊤ j q′ k = 0. 10This is a good Theorem to try to prove yourself before reading the proof. 11Since this is our first proof by Induction, we will do it slowly. 18 2 Challenge 20. Try to do the Gram-Schmidt process for the columns of      1 2 3 0 0 4 5 6 0 0 7 8 0 0 0 9      . Is it the case that the Gram-Schmidt process of the columns of an upper triangular matrix (with non-zero diagonal elements) is always a subset of the canonical basis? Can you come up with an example of a set of vectors for which Gram-Schmidt does not output elements of the canonical basis? Linear Algebra — A. Bandeira (ETHZ) — Week 9 - 2023.11.17 & 2023.11.22 Please find most up to date notes at: https://ti.inf.ethz.ch/ew/courses/LA23/notes_part_II.pdf Gram-Schmidt actually provides us with a new matrix factorization. Let A be an m × n matrix with linearly independent columns a1, . . . , an and Q the m×n matrix whose columns are q1, . . . , qn as outputted by Algorithm 4.4.10. Let R = Q⊤A, since each qk is orthogonal to every ai for i < k we have that R is upper triangular. Q is not necessarily a square matrix, and so not necessarily invertible. But QQ⊤ is the projection on the span of the qi’s and thus also on the ai’s, this means that QQ⊤A = A and so we have that QR = QQ⊤A = A. We call A = QR the QR decomposition. Definition 4.4.12 (QR decomposition). Let A be an m × n matrix with linearly independent columns the QR decomposition is given by A = QR, where Q is an m × n matrix with orthonormal columns (they are the output of Gram Schmidt, Algorithm 4.4.10, on the columns of A) and R is an upper triangular matrix given by R = Q⊤A. Remark 4.4.13. Note that R is a square matrix (n × n), and since the columns of A are linearly independent we have N(A) = {0} and so, since A = QR, we have also N(R) = {0} and so both R and R⊤ are invertible. Fact 4.4.14. The QR decomposition greatly simplifies calculations involving Projections and Least Squares. • Since the C(A) = C(Q) then projections on C(A) can be done with Q which means they are given by projC(A)(b) = QQ⊤b. 19 • The least squares solution to Ax = b is ˆx solution of the normal equations (recall (3)) A⊤A ˆx = A ⊤b. Furthermore, A⊤A = (QR)⊤(QR) = R⊤Q⊤QR = R⊤R, and so we can write (8) R⊤R ˆx = R⊤Q⊤b. Since R has independent columns (is full column rank) then N(R) = {0} and so we can simplify (8) to (9) R ˆx = Q⊤b, which can be efficiently solved by back-substitution since R is a triangular matrix. 4.5. The Pseudoinverse, also known as Moore–Penrose Inverse. The goal of this Section is to construct an analogue to the inverse of a matrix A for matrices that have no inverse, this is called the Pseudoinverse, or the Moore-Penrose Inverse, and we will denote it by A†. It is also commonly denoted by A+. Guiding Question 21. While not all matrices are A invertible, we saw that we can still aim to find the (or a) vector x such that Ax is as close as possible to a target vector b. Can we develop this idea to define a “pseudoinverse” for any matrix A, a matrix that is, in a sense, closest to being an inverse for A? What should “closest to being an inverse” even mean? There are (at least) three issues we need to overcome to try to define a pseudoinverse for a non- invertible matrix A: (i) For some vectors b there might not be a vector x such that Ax = b, (ii) For some vectors b there may be more than one x such that Ax = b and we would have to pick one, and (iii) even if we make such choices, it is not clear that such operation will correspond to multiplying by a matrix A†. Let A ∈ Rm×n be an m × n matrix. There are a couple of different ways we could try to define a pseudoinverse A† for a non-invertible matrix A. Let us start by building on what we discussed on Section 4.3 (Least Squares Approximations), if the columns of A are linearly independent that it would make sense to build A† such that A†b is the Least Squares Solution ˆx = (A⊤A)−1A⊤b (the vector ˆx such that A ˆx is as close as possible to b), and so for matrices A with independent columns we will define A† = (A⊤A)−1A⊤. This motivates the following definition. Definition 4.5.1 (Pseudoinverse for matrices with full column rank). For A ∈ Rm×n with rank(A) = n we define the pseudo-inverse A† ∈ Rn×m of A as A † = (A ⊤A) −1A⊤. 20 Proposition 4.5.2. For A ∈ Rm×n with rank(A) = n, the pseudoinverse A† is a left inverse of A, meaning that A†A = I. Proof. Since rank(A) = n, A⊤A is invertible. Furthermore, A†A = (A⊤A)−1A⊤A = I. 2 Let us know consider the case for which the rows are linearly independent (in other words, A ∈ Rm×n is full row rank; or equivalently rank(A) = m). One natural way to define pseudoinverse is by noting that A⊤ is full column rank and to define A† as (( A⊤)†)⊤ = ((( A⊤)⊤ ( A⊤))−1 ( A⊤)⊤)⊤ = (( AA ⊤)−1 A )⊤ = A ⊤ ( AA ⊤)−1 . Definition 4.5.3 (Pseudoinverse for matrices with full row rank). For A ∈ Rm×n with rank(A) = m we define the pseudo-inverse A† ∈ Rn×m of A as A † = A⊤(AA ⊤) −1. Proposition 4.5.4. For A ∈ Rm×n with rank(A) = m, the pseudoinverse A† is a right inverse of A, meaning that AA† = I. Proof. Since rank(A) = m, AA⊤ is invertible. Furthermore, AA† = AA⊤(AA⊤)−1 = I. 2 Let us try to understand what A† is achieving for full row rank matrices A. Since A is full row rank, for all b ∈ Rm, there exists x ∈ Rn such that Ax = b. The issue is that there are potentially many such vectors. A natural strategy in this case is to pick, among all such vectors, the one with smallest norm. 12 In other words to solve min x∈Rn ∥x∥2(10) s.t. Ax = b, where s.t. stands for “subject to” or “such that”. If x1 and x2 are vectors such that Ax1 = Ax2 = b then x1 − x2 ∈ N(A), and conversely, if Ax = b and y ∈ N(A) then A(x + y) = b. Thus, given one vector x1 such that Ax1 = b the set of solutions to Ax = b are all vectors of the form x1 + y where y ∈ N(A). So we would like to find the minimum ∥x1 + y∥ among all vectors y ∈ N(A). Let us write x1 = ( x1 − projN(A)(x1) ) + projN(A)(x1). Since y ∈ N(A) we have that 12This idea, of picking the smallest (or simplest) solution among many possibilities goes far beyond Linear Algebra and is known as “regularization” in Statistics, Machine Learning, Signal Processing, and Image Processing, etc. It can be viewed as a mathematical version of the famous “Occam’s razor” principle in Philosophy. 21 ( x1 − projN(A)(x1) ) ⊥ ( y + projN(A)(x1) ) and so, by Pythagoras, ∥x1 + y∥ 2 = ∥ ∥ ∥ ( x1 − projN(A)(x1) ) + projN(A)(x1) + y ∥ ∥ ∥ 2 = ∥ ∥ ∥x1 − projN(A)(x1) ∥ ∥ ∥ 2 + ∥ ∥ ∥projN(A)(x1) + y ∥ ∥ ∥ 2 , and so picking y = − projN(A)(x1) yields the smallest norm solution. Since the vectors orthogonal to N(A) are precisely the vectors that are in the row space of A, C(A⊤). We just proved: Proposition 4.5.5. For a full row rank matrix A, the (unique) solution to (10) is given by the vector ˆx ∈ C(A⊤) that satisfies the constraint A ˆx = b. A† is precisely the matrix that “takes b to ˆx solution of (10)”. Proposition 4.5.6. For a full row rank matrix A, the (unique) solution to (10) is given by the vector ˆx = A†b. Proof. By using Proposition 4.5.5 we just need to show that ˆx = A†b satisfies A ˆx = b and that ˆx = A†b is in C(A⊤). Both these are easy to verify: A ˆx = AA†b = AA⊤(AA⊤)−1b = b and ˆx = A†b = A⊤ ( (AA⊤)−1b) and so ˆx ∈ C(A⊤). 2 Guiding Question 22. We would like to define A† for all matrices, not just full rank matrices. A natural construction would be to try to define A† to be the matrix that takes a vector b to the smallest norm solution of the normal equations (3). To define pseudoinverse of a non full rank matrix A we can do it via de A = CR decomposition (recall from Part I of the course and/or see Appendix A(2)). For A ∈ Rm×n, with rank(A) = r, the CR decomposition writes A = CR where C ∈ Rm×r has the first r linearly independent columns of A and R ∈ Rr×n is upper triangular. Note that C is full column rank and R is full row rank. Definition 4.5.7 (Pseudoinverse for all matrices). For A ∈ Rm×n, with rank(A) = r, with CR decomposition A = CR we define the pseudoinverse A† as A † = R†C†, which can be rewritten as A† = R ⊤ ( RR ⊤)−1 ( C⊤C)−1 C⊤ = R⊤ ( C⊤CRR ⊤)−1 C⊤ = R ⊤ ( C⊤AR ⊤)−1 C⊤. The following proposition shows that indeed this definition achieves what was asked in Guiding Question 22. 22 Proposition 4.5.8. Given A ∈ Rm×n and a vector b ∈ Rn, the (unique) solution to min x∈Rn ∥x∥2(11) s.t. A⊤Ax = A⊤b, is given by ˆx = A†b. Proof. Let r be the rank of A and A = CR with C ∈ Rm×r and R ∈ Rr×n. Then ˆx = A†b = R⊤ ( C⊤AR⊤)−1 C⊤b. Thus, A ⊤A ˆx = A ⊤AR ⊤ ( C⊤AR ⊤)−1 C⊤b = R ⊤C⊤AR⊤ ( C⊤AR ⊤)−1 C⊤b = R⊤C⊤b = A ⊤b. Using Proposition 4.5.5, to show that it is the smallest norm solution we just need to show that ˆx ∈ C(A⊤A), but by Proposition 4.3.2 it is enough to show that ˆx ∈ C(A⊤) and since C(A⊤) = C(R⊤) we have that ˆx = R⊤ ( C⊤AR⊤)−1 C⊤b ∈ C(A⊤). 2 In this proof, the only property of the matrices CR we used is that A = CR and both C and R are full rank. So we have actually shown that we can compute the pseudoinverse from any full rank factorization, not just specifically the CR decomposition. We write it here as a proposition. Proposition 4.5.9. For A ∈ Rm×n, with rank(A) = r, and let S ∈ Rm×r and T ∈ Rr×n such that A = ST . Then, A† = T †S†. Remark 4.5.10. Note that If A = ST and rank(A) = r then rank(S) ≥ r and rank(T ) ≥ r and so the matrices ST in Proposition 4.5.9 are indeed full rank (either full column rank or full row rank). Proposition 4.5.11. Given A ∈ Rm×n and B ∈ Rn×p, we have (1) (AB) † = B†A†, as long as rank(A) = rank(B) = n. (2) ( A⊤)† = ( A†)⊤, (3) AA† is symmetric, and is the projection matrix for projection on C(A), (4) A†A is symmetric, and is the projection matrix for projection on C(A⊤). Challenge 23. Prove Proposition 4.5.11. (Hint: use Proposition 4.5.9). Challenge 24. Given A ∈ Rm×n, show that AA †A = A and A †AA † = A†. 23 Proposition 4.5.12. Let A ∈ Rm×n be a matrix and recall that C(A) and C(A⊤) denote respec- tively its column and row spaces. When A : x → Ax is viewed as a function from C(A⊤) to C(A) it is a bijection. In other words, for all b ∈ C(A) there is one and only one x ∈ C(A⊤) such that Ax = b. Challenge 25. Prove Proposition 4.5.12 Further Remark 26. A different way to define the Pseudo-Inverse of a matrix A is to ask for a matrix A† that satisfies the conditions in the Challenge 24 and that both AA† and A†A are symmetric. It is nontrivial, but it turns out these conditions are enough to define A†. 5. LINEAR TRANSFORMATIONS AND DETERMINANTS Further Remark 27. In this part of the course we slightly deviate from [Str23] and will introduce Linear Transformations, before Determinants. To keep the numbering compatible [Str23] we number the section on Linear Transformations as 5.0.2 (this material is in Chapter 8 of [Str23]). As we pointed out in Remark 4.4.4, we can view an m × n matrix A as a function from Rn to Rm, that takes x ∈ Rn to Ax ∈ Rm A : Rn → Rm x → Ax. These functions have a very important property, they are linear: for all x1, x2 ∈ Rn and any α ∈ R we have A(x1 + x2) = Ax1 + Ax2 and A(αx1) = αAx1. We will call functions satisfying these properties Linear Transformations. 5.0.1. Linear Transformations as transformations of Rn. We will now focus on linear transformations from Rn to itself, corresponding to square matrices A ∈ Rn×n. Instead of thinking simply of how x → Ax maps a vector x to Ax, it is useful to think of this transformation being applied to all of Rn and to view it as a transformation of the entire Rn. Let us focus on R2 for better visualization and look at a few examples. Example 5.0.1 (Stretch). The matrix A = [ 1 0 0 2 ] corresponds to stretching by a factor of 2 in the vertical axis. Notice: the first column of A is the image of e1 and the second the image of e2. 24 Example 5.0.2 (Shear). The matrix A = [ 1 0 1 1 ] corresponds to a shearing transformation given by [ x1 x2 ] → [ 1 0 1 1 ] [ x1 x2 ] = [ x1 x1 + x2 ] . Example 5.0.3 (Rotation). The matrix A = [ 1√ 2 − 1√ 2 1√ 2 1√ 2 ] corresponds to a counter-clockwise rotation by π 4 (or 45o) Example 5.0.4 (Reflection). The matrix A = [ 0 1 1 0 ] corresponds to a reflection by the diagonal line x2 = x1. 25 Challenge 28. Draw a few more linear transformations. Draw also one corresponding to a non- invertible matrix A. Try to draw one in R3 as well. Challenge 29. Show that a linear transformation takes a triangle to either a triangle, a line seg- ment connecting two points, or a point. What can you say about the rank or invertibility of the corresponding matrix, depending on which of the three objects is the image of a triangle? 5.0.2. Definition of Linear Transformations. We now treat linear transformations more formally and write the definition of Linear Transfor- mation for any vector space U and V even though in this section we will only treat U = Rn and V = Rm (so you can, for now, fully restrict your attention to this setting). Later on, this more gen- eral definition will allow us, for example, to discuss linear transformations between subspaces of Rn and Rm. Definition 5.0.5 (Linear Transformation). Given two vectors spaces U and V , a Linear Transfor- mation is a function T : U → V such that, for all u1, u2 ∈ U and α ∈ R we have T (u1 + u2) = T (u1) + T (u2) 26 and T (αu1) = α T (u1). Before proceeding let us “collect” a few facts about Linear Transformations. Proposition 5.0.6. Let T : U → V be a linear transformation and k a positive integer. For all u1, . . . , uk ∈ U and α1, . . . , αk ∈ R we have T ( α1u1 + · · · + αkuk) = α1T (u1) + · · · + αkT (uk). Challenge 30. Prove Proposition 5.0.6, iteratively (by induction) using the properties of Linear Transformations. The most central implication of Proposition 5.0.6 is the fact that the value of T in a basis of U fully determines T . Proposition 5.0.7. Let T : U → V and L : U → V be two linear transformations that take the same value in a basis u1, . . . , un of U. Then T = L. Proof. Since u1, . . . , un is a basis of U, any u ∈ U can be written as u = α1u1 + · · · + αnun. Using Proposition 5.0.6 we have T (u) = T ( α1u1 + · · · + αnun) = α1T (u1) + · · · + αnT (un) = = α1L(u1) + · · · + αnL(un) = L( α1u1 + · · · + αnun) = L(u) 2 Linear Algebra — A. Bandeira (ETHZ) — Week 10 - 2023.11.24 & 2023.11.29 Please find most up to date notes at: https://ti.inf.ethz.ch/ew/courses/LA23/notes_part_II.pdf Proposition 5.0.8. Given a basis u1, . . . , un of U, and any v1, . . . , vn ∈ V there is a Linear Trans- formation T : U → V such that, for all 1 ≤ i ≤ n, T (ui) = vi. Challenge 31. Prove Proposition 5.0.8. Example 5.0.9. A few examples of linear transformations: (1) The identity map T : Rn → Rn given by T (x) = x, (2) For any matrix A, the map x → Ax, (3) For a vector v ∈ Rn the map T : Rn → R given by T (x) = v⊤x, (4) The map T : Rn → Rn given by T (x) = 0. 27 A few examples of functions that are not linear transformations: (1) For a vector v ∈ Rn (such that v ̸= 0) the map T : Rn → R given by T (x) = v + x, (2) The map T : Rn → R given by T (x) = ∥x∥, (3) The map T : Rn → R given by T (x) = ∥x∥2, (4) The map T : Rn → Rn given by T (x) = 1 ∥x∥x. Challenge 32. Show that the first batch of examples above indeed correspond to linear transfor- mations, and that the second does not. It is easy to see that given an m × n matrix A, the function x → Ax is a Linear Transformation from Rn to Rm, what we will show now that the converse is also true. Proposition 5.0.10. For any Linear Transformation T : Rn → Rm, there exists an m × n matrix A such that T (x) = Ax for all x ∈ Rn. Proof. We will prove this proposition constructively. Let e1, . . . , en be the canonical basis of Rn (the i-th basis element has a 1 in the i-th entry and zeros elsewhere, or in other words (ei) j = δi j). We write x = x1e1 + · · · + xnen, then by linearity of T , T (x) = x1T (e1) + · · · + xnT (en) =    | | T (e1) · · · T (en) | |       x1 ... xn    = Ax, for A =    | | T (e1) · · · T (en) | |   . 2 Challenge 33. (⋆) Can you describe the linear transformation corresponding to A†? (in terms of the linear transformation corresponding to a matrix A) Exploratory Challenge 34. Can you describe the linear transformation corresponding to A⊤? (in terms of the linear transformation corresponding to a matrix A) Further Remark 35. Since we are restricting ourselves to U = Rn and V = Rm we are identifying an element x ∈ Rn with its coordinates in the canonical basis x = x1e1 + · · · + xnen. In general, if we use a different basis for U and V we will have a matrix representation for each linear transformation, but it will potentially correspond to a different matrix, it will be the matrix that describes the map from the coordinates in the basis of U to the ones in the basis of V , later in the course we will see some examples, and we will briefly discuss how to go from a matrix representation in one basis to that on another basis, a so-called change of basis. 28 Proposition 5.0.11. Given two linear transformations T : Rn → Rm and L : Rm → Rp, with corresponding matrices (as given by Proposition 5.0.10) A ∈ Rm×n and B ∈ Rp×m the linear transformation L ◦ T (given by L ◦ T (x) = L(T (x)) corresponds to multiplying by the matrix BA. In other words L ◦ T (x) = BAx. Challenge 36. Prove Proposition 5.0.11. 5.1. The Determinant. We will now introduce the notion of determinant det(A) of a square matrix A. While this has a somewhat involved definition for n × n matrices, it is useful to first discuss what the determinant geometrically corresponds to, and to focus on small matrices. In a nutshell, the determinant of a matrix is a number that corresponds to how much the associated linear transformation inflates space, it corresponds precisely to the volume (or area, in R2) of the image of the unit cube (the red square in the pictures above in R2); with a negative sign when the orientation changes (in the pictures above in R2, when the order of the colored dots, on the red square, changed). If we think about the determinant this way, then many of the properties we will list below can be intuitively understood (while it is hard to do so from the formula for the n × n determinant). For this reason, this section will be somewhat less proof-based, and rather focus on the most relevant properties of the determinant. Remark 5.1.1. Grant Sanderson has a website https://www.3blue1brown.com/ and Youtube channel https://www.youtube.com/3blue1brown with excellent animation- heavy explanations of topics in Mathematics, including Linear Algebra. I particularly recom- mend the video on Determinants, it has also 3 dimensional visualizations that are harder to do on a static medium. You can find it here https://youtu.be/Ip3X9LOh2dk or here https://www.3blue1brown.com/lessons/determinant. See also Figure 4. A calculation of the area of the image of the unit square by left-multiplication by a 2 × 2 matrix shows (see Figure 4) that ∣ ∣ ∣ ∣ ∣ a b c d ∣ ∣ ∣ ∣ ∣ := det [ a b c d ] = ad − bc. Before we actually formally define determinant for n × n matrices we will state some of the most important properties of the determinant, you can find the actual definition in Definition 5.1.6. 5.1.1. Determinant and invertibility. Since a square matrix is invertible if the image is full- dimensional, which corresponds to the image of the unit square/cube having non-zero area/volume, then det(A) ̸= 0 if and only if A is invertible. This is the following proposition. 29 FIGURE 4. Calculation in 3Blue1Brown’s video (see Remark 5.1.1) computing the determinant of a 2 × 2 matrix as the area of the image of the unit square after a linear transformation (that does not change orientation). Proposition 5.1.2. A matrix A ∈ Rn×n is invertible if and only if det(A) ̸= 0. In fact, let us try to invert the matrix [ a b c d ] , just by naive calculations, not using elimination. 13 If a = b = 0 or c = d = 0 then the matrix is not invertible (it has a 0 row).14 Let’s assume either a or b is non-zero and that either c or d is nonzero. We are looking for a matrix [ w x y z ] such that [ a b c d ] [ w x y z ] = [ 1 0 0 1 ] . Since either a or b is non-zero and either c or d is nonzero, neither of the rows of the matrix are zero. Also, the second/first column of the inverse needs to be orthogonal to the first/second row of the original matrix, and vice-versa. 13Elimination is a much better way to do it in general, but bear with me as I am trying to illustrate something, not invert the matrix as efficiently as possible. 14Note that this is not a necessary condition for non-invertibility, as the all-ones matrix is not invertible while having no zero rows. 30 We then must have [ x z ] = α1 [ −b a ] and [ w y ] = α2 [ d −c ] for some α1, α2 ∈ R. Since [ a b ] [ w y ] = 1 we have: α1 [ a b ] [ d −c ] = 1, which gives α1 = 1 ad−bc , note that the denominator is exactly det(A) which is non-zero when A is invertible. A similar calculation gives α2 = 1 ad−bc = 1 det(A). This gives a formula for the inverse of 2 × 2 matrices. Proposition 5.1.3. Given a 2 × 2 matrix A with det(A) ̸= 0, the inverse is given by A−1 = 1 det(A) [ d −b −c a ] . 5.1.2. Determinant and volumes. Proposition 5.1.4. Given matrices A, B ∈ Rn×n we have det(AB) = det(A) det(B). FIGURE 5. Approximation of a region by unit squares While proving this from the definition of determinant is nontrivial, it is relatively easy to intu- itively see why it is true if we recall that determinant measures areas/volumes: Since the area of the image of a square/cube does not change depending on the location of the initial square/cube, and any (nice enough) region of Rn can be approximated by the union of small squares/cubes (see Figure 5), then the determinant is also the area/volume of the image any (nice enough) unit area/volume 1 region, and so det(AB) = det(A) det(B) (since the image by AB of a unit square/cube is the image by B of the image by A of the same unit square/cube). 31 5.1.3. The Definition. We now give the definition of determinant for n × n matrices. Before we define determinant we need first to discuss permutations. Definition 5.1.5 (Sign of Permutation). Given a permutation σ : {1, . . . , n} → {1, . . . , n} of n elements, its sign sgn(σ ) can be 1 or −1. The sign counts the parity of the number of pairs of elements that are out of order (sometimes called inversions) after applying the permutation. In other words, sgn(σ ) =    1 if |(i, j) ∈ {1, . . . , n} × {1, . . . , n} such that i < j and σ (i) > σ ( j)| is even, −1 if |(i, j) ∈ {1, . . . , n} × {1, . . . , n} such that i < j and σ (i) > σ ( j)| is odd. Exploratory Challenge 37. The sign of a permutation has many nice properties. Try to prove a couple of them: (1) The sign of a permutation is multiplicative, i.e.: for two permutations σ , γ we have that sgn(σ ◦ γ) = sgn(σ )sgn(γ). (2) For all n ≥ 2, exactly half of the permutations have sign 1 and exactly half have sign −1. the identity if 1, the sign of a transposition (a permutation that only swaps two elements) is −1 and for two permutations σ , γ we have that sgn(σ ◦ γ) = sgn(σ )sgn(γ). Definition 5.1.6. Given a square matrix A ∈ Rn×n the determinant det(A) is defined as det(A) = ∑ σ ∈Πn sgn(σ ) n ∏ i=1 Ai,σ (i), where Πn is the set of all permutations of n elements. From this Definition one can verify the following propositions. Proposition 5.1.7. Given a permutation matrix P ∈ Rn×n corresponding to a permutation σ , then det(P) = sgn(σ ). We sometimes also write sgn(P). Proposition 5.1.8. Given a triangular (either upper- or lower-) matrix T ∈ Rn×n we have det(T ) = n ∏ k=1 Tkk, in particular, det(I) = 1. Proposition 5.1.9. Given a matrix A ∈ Rn×n we have det(A⊤) = det(A). 32 The following is a consequence of the propositions above (and the only proof we’ll do in this section) Proposition 5.1.10. If Q ∈ Rn×n is an orthogonal matrix then det(Q) = 1 or det(Q) = −1. Proof. By Propositions 5.1.8 and 5.1.4 we have 1 = det(I) = det(Q⊤Q) = det(Q⊤) det(Q). by Proposition 5.1.9 we have 1 = det(Q)2 and so det(Q) is 1 or -1. 2 Following the same line of argument we also have Proposition 5.1.11. Given a matrix A ∈ Rn×n such that det(A) ̸= 0, then A is invertible and det(A−1) = 1 det(A). 5.1.4. 3 × 3 matrices. If A is a 1 × 1 matrix, since there is only one permutation of 1 element (the permutation σ (1) = 1, which has sign 1), we have det(A) = A11 = A. For 2 × 2 matrices: There are two permutations σ1 the identity permutation (that doesn’t move any element, which has sign 1) and σ2 the permutation that swaps the two elements (which has sign −1). So, for A a 2 × 2 matrix, we have det(A) = ∑ σ ∈Π2 sgn(σ ) 2 ∏ i=1 Ai,σ (i) = (+1) 2 ∏ i=1 Ai,σ1(i) + (−1) 2 ∏ i=1 Ai,σ2(i) = A11A22 − A12A21. This corresponds precisely to, ∣ ∣ ∣ ∣ ∣ a b c d ∣ ∣ ∣ ∣ ∣ = ad − bc. 33 For 3 × 3 matrices there are 3! = 6 permutations, so there will be 6 terms. For A a 3 × 3 matrix, we can write its determinant as (where an empty entry corresponds to a zero entry) det(A) = ∣ ∣ ∣ ∣ ∣ ∣ ∣ A11 A12 A13 A21 A22 A23 A31 A32 A33 ∣ ∣ ∣ ∣ ∣ ∣ ∣ = ∣ ∣ ∣ ∣ ∣ ∣ ∣ A11 A22 A33 ∣ ∣ ∣ ∣ ∣ ∣ ∣ + ∣ ∣ ∣ ∣ ∣ ∣ ∣ A12 A21 A33 ∣ ∣ ∣ ∣ ∣ ∣ ∣ + ∣ ∣ ∣ ∣ ∣ ∣ ∣ A12 A23 A31 ∣ ∣ ∣ ∣ ∣ ∣ ∣ + ∣ ∣ ∣ ∣ ∣ ∣ ∣ A13 A22 A31 ∣ ∣ ∣ ∣ ∣ ∣ ∣ + ∣ ∣ ∣ ∣ ∣ ∣ ∣ A13 A21 A32 ∣ ∣ ∣ ∣ ∣ ∣ ∣ + ∣ ∣ ∣ ∣ ∣ ∣ ∣ A11 A23 A32 ∣ ∣ ∣ ∣ ∣ ∣ ∣ = A11A22A33 − A12A21A33 + A12A23A31 − A13A22A31 + A13A21A32 − A11A23A32. There is another convenient way of writing this determinant (12) ∣ ∣ ∣ ∣ ∣ ∣ ∣ A11 A12 A13 A21 A22 A23 A31 A32 A33 ∣ ∣ ∣ ∣ ∣ ∣ ∣ = A11 ∣ ∣ ∣ ∣ ∣ A22 A23 A32 A33 ∣ ∣ ∣ ∣ ∣ − A12 ∣ ∣ ∣ ∣ ∣ A21 A23 A31 A33 ∣ ∣ ∣ ∣ ∣ + A13 ∣ ∣ ∣ ∣ ∣ A21 A22 A31 A32 ∣ ∣ ∣ ∣ ∣ . In general, these terms are called the co-factors of A. Definition 5.1.12. Given A ∈ Rn×n, for each 1 ≤ i, j ≤ n let Ai j denote the (n − 1) × (n − 1) matrix obtained by removing row i and column j from A. Then we define the co-factors of A as Ci j = (−1) i+ j det(Ai j). Just as in (12), the determinant can be written in terms of the co-factors. Proposition 5.1.13. Let A ∈ Rn×n, for any 1 ≤ i ≤ n, det(A) = n ∑ j=1 Ai jCi j. The formula we derived above for the inverse of 2 × 2 matrices (Proposition 5.1.3), also has an analogue in n dimensions. Proposition 5.1.14. Given A ∈ Rn×n with det(A) ̸= 0 we have A −1 = 1 det(A)C⊤, where C is the n × n matrix with the co-factors of A as entries. 34 One good way to think of this proposition is as the identity AC⊤ = det(A)I. Remark 5.1.15. Computationally speaking, this is not a good way to compute the inverse, as it involves computing many determinants. Challenge 38. Verify that Proposition 5.1.14 indeed corresponds to Proposition 5.1.3 when n = 2. Exploratory Challenge 39. Try to prove Proposition 5.1.14 by showing that AC⊤ = det(A)I. Perhaps start with n = 3. You can also use Cramer’s Rule (below) to prove this. 5.1.5. Cramer’s Rule. The determinant also allows us to write a formula for the solution of the linear system of the type Ax = b when A ∈ Rn×n and det(A) ̸= 0. The idea is simple, we will illustrate it here for n = 3. If    A11 A12 A13 A21 A22 A23 A31 A32 A33       x1 x2 x3    =    b1 b2 b3   , then we have    A11 A12 A13 A21 A22 A23 A31 A32 A33       x1 0 0 x2 1 0 x3 0 1    =    b1 A12 A13 b2 A22 A23 b3 A32 A33    . Since the determinant is multiplicative, and the determinant of the second matrix in the expression is x1, we have det(A)x1 = det(B1), where B1 is the matrix obtained by A by replacing the first column of A with the vector b. Since we can do this for any of the columns, we have x j = det(B j)/ det(A). In general Proposition 5.1.16 (Cramer’s Rule). Let A ∈ Rn×n such that det(A) ̸= 0 and b ∈ Rn then the solution x ∈ Rn of Ax = b is given by x j = det(B j) det(A) , where B j is the matrix obtained by A by replacing the j-th column of A with the vector b. Remark 5.1.17. As with the formula for the inverse: computationally speaking, this is not a good way to solve linear systems, as it involves computing many determinants. 5.1.6. Elimination and the Determinant. The definition we used for Determinant involves a for- mula with n! terms, it is computational infeasible for even moderate levels of n (it is faster than exponential! For example, 100! has almost 160 digits!), in practice the determinant of a matrix A 35 is computed by Gaussian Elimination and the matrix decomposition PA = LU (P permutation and so det(P) = sgn(P), U is upper triangular and L is lower triangular with only 1s in the diagonal, and so det(L) = 1) and so we would have (13) det(A) = 1 det(P) det(L) det(U) = sgn(P) det(U), and since U is a triangular matrix its determinants can be easily computed by Proposition 5.1.8. Alternatively, one can also think of Gaussian Elimination as directly computing the determinant via the following two propositions Proposition 5.1.18. If A is an n × n matrix and P is a permutation that swaps two elements, meaning that PA corresponds to swapping two rows of A then det(PA) = − det(A). Proposition 5.1.19. The determinant is linear in each row (or each column). In other words, for any a0, a1, a2 . . . , an ∈ Rn and α0, α1 ∈ R we have ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ — α0a⊤ 0 + α1a⊤ 1 — — a⊤ 2 — ... — a⊤ n — ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ = α0 ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ — a⊤ 0 — — a⊤ 2 — ... — a⊤ n — ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ + α1 ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ — a⊤ 1 — — a⊤ 2 — ... — a⊤ n — ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ ∣ , and ∣ ∣ ∣ ∣ ∣ ∣ ∣ | | | α0a0 + α1a1 a2 · · · an | | | ∣ ∣ ∣ ∣ ∣ ∣ ∣ = α0 ∣ ∣ ∣ ∣ ∣ ∣ ∣ | | | a0 a2 · · · an | | | ∣ ∣ ∣ ∣ ∣ ∣ ∣ + α1 ∣ ∣ ∣ ∣ ∣ ∣ ∣ | | | a1 a2 · · · an | | | ∣ ∣ ∣ ∣ ∣ ∣ ∣ . Exploratory Challenge 40. The more mathematical way of presenting this material is to define Determinant as a function that goes from n × n matrices to R that (i) is linear in each column, (ii) det(I) = 1 and (iii) det(A) = 0 whenever A has two identical columns. It is then possible to prove that the only function satisfying these three properties is the determinant as we defined it. Try to prove it! Linear Algebra — A. Bandeira (ETHZ) — Week 11 - 2023.12.01 & 2023.12.06 Please find most up to date notes at: https://ti.inf.ethz.ch/ew/courses/LA23/notes_part_II.pdf 6. EIGENVALUES AND EIGENVECTORS We are (almost) ready for one of the most important concepts (if not the most important one) in Linear Algebra, eigenvalues and eigenvectors. In a sense, it has all been building up to this! 36 Guiding Strategy 41. Given a square matrix A, as we will see below, an eigenvalue λ and eigen- vector v will be, respectively, a scalar and a non-zero vector satisfying Av = λ v. This means that (A − λ I)v = 0 and so (A − λ I) is not invertible, or equivalently det(A − λ I) = 0. We can look for eigenvalues as solutions of det(A − λ I) = 0 which is a polynomial15 in λ but unfortunately, not all polynomials have real zeros. 16 For example if A = [ 0 −1 1 0 ] , det(A − λ I) = 0 corresponds to λ 2 + 1 = 0 which only has solutions in C, the Complex Numbers. For this reason we will start this Chapter with a brief introduction to Complex Numbers. It all starts with asking for a number λ such that λ 2 + 1 = 0. Further Reading 42. Complex Analysis is a beautiful topic in Mathematics, what we will cover here is just a tiny peak at it, there is a all bookshelf of excellent books in this topic in our li- brary. I have personally taught a course at ETH on Complex Analysis, and since it was during the COVID pandemic I made videos available online, which are still available at https://www. youtube.com/playlist?list=PLiud-28tsatLRRGqO_Eg_x0S4LVyxuV5p (In par- ticular, the first lecture covers roughly the content here). 6.0. Complex Numbers. If we start with the natural numbers N and want to solve equations like x + 10 = 1, we need negative numbers. This motivates considering the integers Z. Similarly, rational numbers Q are needed to solve equations like 10x = 1 and real numbers R are needed to solve x2 = 2.17 Similarly, the Complex Numbers are needed to solve equations such as x2 + 1 = 0. It starts with the introduction of an imaginary number i ∈ C such that i2 = −1. You can think of i as √ −1. The complex numbers are numbers of the form z = a + ib for a ∈ R and b ∈ R. C = {a + ib : a, b ∈ R}. Keeping in mind that i2 = −1 we can do operations with complex numbers: • (a + ib) + (x + iy) = (a + x) + i(b + y), • (a + ib)(x + iy) = ax + i(ay + bx) + i2by = ax + i(ay + bx) − by = (ax − by) + i(ay + bx), • (a + ib)(a − ib) = a2 + b2, • a+ib x+iy = (x−iy)(a+ib) (x−iy)(x+iy) = (ax+by)+i(bx−ay) x2+y2 = ( ax+by x2+y2 ) + i ( bx−ay x2+y2 ) . 15This is one of the main reasons we had to cover determinants. 16A zero of a polynomial P is a point x such that P(x) = 0, this is also called a root of the polynomial. In German, it’s a “Nullstelle”. In fact, a (rather deep) multidimensional version of Theorem 6.0.3, and one of the most important facts in Algebraic Geometry, is called “Hilbert’s Nullstellensatz”. 17If you have never seen the proof that there exists no x ∈ Q such that x2 = 2 I highly recommend trying to do it: set x = a/b for a, b ∈ Z and try to count how many times 2 divides both a and b and find a contradiction. 37 Given z ∈ C with z = a + ib we have the following notation ℜ(a + ib) := a called the real part of z = a + ib,(14) ℑ(a + ib) := b called the imaginary part of z = a + ib,(15) |z| := √ a2 + b2 called the modulus of z = a + ib,(16) a + ib := a − ib called the complex conjugate of z = a + ib.(17) Note that for z1, z2 ∈ C, we have |z|2 = zz, z1z2 = z2z1, z1 + z2 = z1 + z2, and 1 z = z |z|2 . Fact 6.0.1 (Euler’s Formula). Given θ ∈ R, we have (18) e iθ = cos θ + i sin θ . This means, in particular, that eiπ = −1. This is usually written as eiπ + 1 = 0. Further Reading 43. In order to prove Euler’s Formula, we need to first define what we mean by eiθ , this can be done, for example, by the Taylor series of the exponential, but this is outside the scope of this course (see Further Reading 42). Fact 6.0.2 (Polar Coordinates). A complex number z ∈ C can be written as (19) z = re iθ , where r ≥ 0 is the modulus of z and θ ∈ R (we can restrict to θ ∈ [0, 2π[) is an angle, also called the argument of z. The most important property of Complex Numbers, and what makes them a very natural mathe- matical object, is that any univariate polynomial equation with complex number coefficients has a (complex) solution, in a certain sense we don’t need to extend numbers further, C is Alge- braically closed. Theorem 6.0.3 (Fundamental Theorem of Algebra). Any degree n non-constant (n ≥ 1) poly- nomial P(z) = αnzn + αn−1zn−1 + · · · + α1z + α0 (with αn ̸= 0) has a zero: λ ∈ C such that P(λ ) = 0. Further Reading 44. As the name suggests, Theorem 6.0.3 is a central result in Complex Anal- ysis. Proving it is outside the scope of this course.18 Complex analysis (which leads to the proof of this theorem) is a beautiful example of interaction between analysis, algebra, and geometry. In a nutshell the idea for the classical proof is that differentiable functions in the complex plane 18But you can see Appendix D for a relatively elementary proof. 38 FIGURE 6. A complex number z = 4 + 3i in the Complex plane. f : C → C are very special and, in a sense, need to behave like polynomials (this is a deep state- ment that needs a significant amount of background to properly state and prove). If a polynomial P(z) doesn’t have a zero then 1/P(z) is a differentiable function that cannot behave like a non- constant polynomial because it does not grow sufficiently far away from zero, and so it must be a constant function which means that P(z) had to be constant, so any non-constant polynomial has a zero. For more on Complex Analysis see Further Reading 42. Further Remark 45. Once we have λ a zero of P(z), we can divide P(z) by (z − λ ) to get P(z) = (z − λ )P1(z), then use a zero of P1 to reiterate, and so on. This argument (carried out carefully) gives the following corollary. Corollary 6.0.4. Any degree n non-constant (n ≥ 1) polynomial P(z) = αnzn + αn−1zn−1 + · · · + α1z + α0 (with αn ̸= 0) has n zeros: λ1, . . . , λn ∈ C, perhaps with repetitions, such that (20) P(z) = αn(z − λ1)(z − λ2) · · · (z − λn). The number of times λ ∈ C appears in this expansion is called the algebraic multiplicity of the zero. 6.0.1. Complex-valued Matrices and Vectors. Analogously to Rn we also define Cn as the set of n-dimentional complex valued vectors. We can have complex valued vectors v ∈ Cn and matrices A ∈ Cm×n. The natural operation of “transposing” for complex vectors and matrices is that of 39 “conjugate transpose” or “hermitian transpose” denoted by A∗, or sometimes AH, (21) A∗ = A T . Given v ∈ Cn we have ∥v∥ 2 = v ∗v = v T v = n ∑ i=1 vivi = n ∑ i=1 |vi| 2. The inner-product (or dot-product) in Cn is given by ⟨v, w⟩ = w∗v. Similarly to the situation in Rn, we say v1, . . . , vk ∈ Cn are linearly independent if there is no (complex valued) non-zero linear combination giving zero, meaning that if α1v1 + · · · + αkvk = 0 for α1, . . . , αk ∈ C we must have α1 = · · · = αk = 0. Also, the span of v1, . . . , vk ∈ Cn is the set of possible linear combinations α1v1 + · · · + αkvk for α1, . . . , αk ∈ C. If v1, . . . , vk is a spanning set of a subspace and linearly independent we say it is a basis of that subspace. As with Rn if we have v1, . . . , vn ∈ Cn that are either a spanning set of Cn or linearly independent then they must actually be both (and so are a basis). Further Reading 46. With these definitions you can already understand the Discrete Fourier Transform (which is the linear transformation corresponding to the DFT matrix, one of the most important complex valued matrices). This is the key object behind signal processing, you can read more about it on the lecture notes of another course I usually teach [BM23]. You can also see a discussion of Fourier Transform, circulant matrices, and signal convolutions in [Str23] (end of Section 6.4). 6.1. Introduction to Eigenvalues and Eigenvectors. Even though the theory can be analo- gously developed for complex valued matrices, we will focus on real valued matrices. Guiding Example 47. We will use a guiding example to illustrate both some of the power, and some of the properties, of eigenvalues and eigenvectors. In Guiding Example numbers 47 through 53 we will derive a formula for the n-th Ficonacci Number. The Fibonacci numbers are defined by the recurrence: (22) F0 = 0, F1 = 1, and, for n ≥ 2, Fn = Fn−1 + Fn−2. The recurrence can be rewritten in linear algebraic notation as, for n ≥ 2, (23) [ Fn+1 Fn ] = [ 1 1 1 0 ] [ Fn Fn−1 ] . Definiting (24) M = [ 1 1 1 0 ] and gn = [ Fn+1 Fn ] , 40 the recurrence can be rewritten as g0 = [ 1 0 ] and gn = Mgn−1, meaning that (25) gn = Mng0. Definition 6.1.1. Given A ∈ Rn×n, we say λ ∈ C is an eigenvalue of A and v ∈ Cn \\ {0} is an eigenvector of A, associated with with the eigenvalue λ , when the following holds: Av = λ v. We call them an eigenvalue-eigenvector pair. If λ ∈ R then we will call λ a real eigenvalue, and the associated eigenvalue-eigenvector pair a real eigenvalue-eigenvector pair. Guiding Example 48. Let us try to find eigenvalues (and later the eigenvectors) of M = [ 1 1 1 0 ] . We are looking for v ∈ R2 \\ {0} and λ ∈ R such that Mv = λ v, but this can be rewritten as (M − λ I)v = 0 and since v ̸= 0 it means that M − λ I is non-invertible (also called singular).19 This is equivalent to det(M − λ I) = 0 and so we can find the eigenvalues λ with this equation: (26) 0 = det(M − λ I) = ∣ ∣ ∣ ∣ ∣ 1 − λ 1 1 0 − λ ∣ ∣ ∣ ∣ ∣ = (1 − λ )(0 − λ ) − 1 = λ 2 − λ − 1. By the quadratic formula, 20 the solutions to (26) are given by (27) λ1 = 1 + √ 5 2 and λ2 = 1 − √ 5 2 . Further Reading 49 (Golden Ratio). The number ϕ = 1+ √ 5 2 is the celebrated Golden Ratio; believed, since the ancient Greeks, to be the ideal aspect ratio for a rectangle. “Some of the greatest mathematical minds of all ages, from Pythagoras and Euclid in ancient Greece, through the medieval Italian mathematician Leonardo of Pisa and the Renaissance astronomer Johannes Kepler, to present-day scientific figures such as Ox- ford physicist Roger Penrose, have spent endless hours over this simple ratio and its properties. [. . . ] Biologists, artists, musicians, historians, architects, psychologists, and even mystics have pondered and debated the basis of its ubiquity and appeal. In fact, it is 19Normally, we would have to look for λ ∈ C and v ∈ Cn but in this case the eigenvalues, as we will see, are real. 20Recall that the quadratic formula says that the zeros of ax2 + b + c are given by x = −b± √ b2−4ac 2a . 41 probably fair to say that the Golden Ratio has inspired thinkers of all disciplines like no other number in the history of mathematics.” — The Golden Ratio: The Story of Phi, the World’s Most Astonishing Number The following is the original definition which dates back to Euclid around 2300 years ago (they called the number “extreme and mean ratio” back then) “A straight line is said to have been cut in extreme and mean ratio when, as the whole line is to the greater segment, so is the greater to the lesser” Guiding Example 50. Now we can try to find the eigenvectors v1 and v2 such that Av1 = λ1v1 and Av2 = λ2v2. Let us start with v1. We are looking for a non-zero element of N ( A − 1+ √ 5 2 I) . In other words [ 0 0 ] = [ 1 − 1+ √ 5 2 1 1 − 1+ √ 5 2 ] [ (v1)1 (v1)2 ] . This is an under-determined system and we are looking for a non-zero solution, so let us start by setting (v1)2 = 1. The second equation gives us (v1)1 = 1+ √ 5 2 . Indeed v1 = [ 1+ √ 5 2 1 ] is an eigenvector of M associated to the eigenvalue λ1 = 1+ √ 5 2 . A similar calculation for λ2 = 1− √ 5 2 gives that v2 = [ 1− √ 5 2 1 ] . Indeed (28) [ 1 1 1 0 ] [ 1+ √ 5 2 1 ] = 1 + √ 5 2 [ 1+ √ 5 2 1 ] and [ 1 1 1 0 ] [ 1− √ 5 2 1 ] = 1 − √ 5 2 [ 1− √ 5 2 1 ] Challenge 51. Carry out the calculations in Guiding Example 50 and confirm that we have indeed found two eigenvectors (check the two equalities in (28)). Further Remark 52. The v1 and v2 we constructed in 50 are not the only possible choices, for example any non-zero scalar multiples of these would have also been a possible choice. Normally one picks a unit-norm representative, but in this case we picked vectors that make the calculations the cleanest. What we carried out in the example above is very general and we now develop the theory for general matrices. Let λ and v be an eigenvalue-eigenvector pair of a matrix A. Since v ̸= 0 and (A − λ I)v = Av − λ v = 0 we have that det(A − λ I) = 0. Conversely, if det(A − λ I) = 0 for some λ , then there 42 exists v ∈ N(A − λ I) \\ {0} and so λ is an eigenvalue. This gives a procedure to find eigenvalues and eigenvectors: (i) eigenvalues are the solution of det(A − λ I) = 0, which is a polynomial equation, and (ii) an associated eigenvector is a non-zero element of N(A − λ I). Let us first formulate this for real eigenvalues and eigenvectors. Proposition 6.1.2. Let A ∈ Rn×n. λ ∈ R is a (real) eigenvalue of A if and only if det(A − λ I) = 0. A vector v is an eigenvector associated with the eigenvalue λ if (and only if) it is a non-zero element of N(A − λ I). A direct inspection of the formula for the determinant (Definition 5.1.6) gives the following. Proposition 6.1.3. det(A − λ I) is a polynomial, in λ , of degree n. The coefficient of the λ n term is (−1)n. The Fundamental Theorem of Algebra (Theorem 6.0.3) immediately implies Theorem 6.1.4. Every matrix A ∈ Rn×n has an eigenvalue (perhaps complex-valued). Remark 6.1.5. For now we will focus on real eigenvalues, and address complex valued ones later on. Essentially all the properties we will describe below also hold for complex valued eigenvalues (just by replacing R by C and doing the appropriate adjustments). For example, Proposition 6.1.2 also holds for complex-valued eigenvalues, one just needs to think of N(A − λ I) as a subspace of Cn, meaning the vectors v ∈ Cn such that (A − λ I)v = 0. Guiding Example 53. Let us return to our guiding example. Notice that v1 and v2 are linearly independent, and so they are a basis for R2. We can write g0 = α1v1 + α2v2. [ 1 0 ] = g0 = α1v1 + α2v2 = [ α1 1+ √ 5 2 + α2 1− √ 5 2 α1 + α2 ] = [ (α1 + α2) 1 2 + (α1 − α2) √ 5 2 α1 + α2 ] , and so α1 = 1√ 5 and α2 = − 1√ 5. Recall that gn = Ang0 and so gn = A n ( 1 √ 5v1 − 1 √ 5v2 ) = 1 √ 5Anv1 − 1 √ 5A nv2 = 1 √ 5 (Anv1 − Anv2) . Since Av1 = λ1v1 we have that A2v1 = A(λ1v1) = λ 2 1 v1 and iterating this procedure 21 gives Anv1 = λ n 1 v1. This means that gn = Anv1 − Anv2√ 5 = ( 1+ √ 5 2 )n v1 − ( 1− √ 5 2 )n v2 √ 5 = ( 1+ √ 5 2 )n √ 5 [ 1+ √ 5 2 1 ] − ( 1− √ 5 2 )n √ 5 [ 1− √ 5 2 1 ] . 21A formal proof would use induction 43 Since Fn is the second coordinate of gn, we derived a closed formula for the n-th terms of the Fibonacci sequence: (29) Fn = 1 √ 5 (1 + √ 5 2 )n − 1 √ 5 (1 − √ 5 2 )n . An important property that allowed us to do the calculation above was that applying a power of a matrix to an eigenvector was a simple operation, this is the next proposition. Proposition 6.1.6. If λ and v are an eigenvalue-eigenvector pair of a matrix A, then, for k ≥ 1, λ k and v are an eigenvalue-eigenvector pair of the matrix Ak. Proof. Proof by Induction: The base case k = 1 is trivial. For the induction step, since λ k and v are an eigenvalue-eigenvector pair then Akv = A ( Ak−1v ) = A ( λ k−1v ) = λ kv. 2 Proposition 6.1.7. Let A be an invertible matrix. If λ and v are an eigenvalue-eigenvector pair of a matrix A, then, 1 λ and v are an eigenvalue-eigenvector pair of the matrix A−1. Proof. Since Av = λ v we have A−1(λ v) = v and so λ A−1v = v, which (since λ ̸= 0) is equivalent to A−1v = 1 λ v. 2 Another important property, was that we were able to write a vector as a linear combination of eigenvectors, which was possible because the eigenvectors were linearly independent. Proposition 6.1.8. Let An×n and let v1, . . . , vk ∈ Rn be eigenvectors corresponding to eigenvalues λ1, . . . , λk ∈ R. If λ1, . . . , λk are all distinct, the eigenvectors v1, . . . , vk are linearly independent. Proof. We will prove this by contradiction. Assume that v1, . . . , vk are linearly dependent. For i = 1, . . . , k, let di denote the dimension of the span of v1, . . . , vi. Since v1 ̸= 0 we have d1 = 1. By the hypothesis dk < k. Let j be the smallest positive integer for which d j < j. Note that, by construction, d j−1 = d j = j − 1, this means that v1, . . . , v j−1 are linearly independent but that v j is in the span of v1, . . . , v j−1. We can then write (30) v j = α1v1 + · · · α j−1v j−1. If we multiply by A both sides we get λ jv j = Av j = A ( α1v1 + · · · α j−1v j−1) = α1λ1v1 + · · · λ j−1α j−1v j−1. Replacing the v j in the left hand side with the right hand side of (30) we get λ j ( α1v1 + · · · α j−1v j−1) = α1λ1v1 + · · · λ j−1α j−1v j−1, 44 which we can rearrange as (31) α1 ( λ j − λ1) v1 + α2 ( λ j − λ2) v2 + · · · + α j−1 ( λ j − λ j−1) v j−1 = 0. Since λ j − λi ̸= 0 for all i ≤ j − 1 and not all αi’s are zero, this is a non-zero linear combination of v1, . . . , v j−1 adding to zero, which would be a contradiction with d j−1 = j − 1. 2 A very important consequence of this is that if a matrix has n distinct real eigenvalues then the eigenvectors form a basis for Rn. Theorem 6.1.9. Let A ∈ Rn×n with n distinct real eigenvalues (meaning that the n zeros of det(A− λ I), as described in Corollary 6.0.4, are all distinct) then there is a basis of Rn, v1, . . . , vn, made up of eigenvectors of A. Guiding Example 54. Guiding Example 47 is yet to stop providing us with insight into properties of eigenvalues and eigenvectors! Here are a couple of observations, which although outside of the core scope of this course, have significant impact in several areas: • Notice that since |λ2| < |λ1|, the contribution of λ n 2 α2v2 becomes negligible (when com- pared to λ n 1 α1v1) as n → ∞. This observation can be used in a clever way: we can approximate the eigenvector v1 by Ang0 and so if we have a fast way to do matrix-vector multiply, we can approximate eigenvalues and eigenvectors. This is often referred to as the Power Method. In a CS Lens I plan to show you how Google’s celebrated PageR- ank algorithm is based on the idea of how eigenvectors can be used for ranking (you can also read more about it here [BSS]. 22), calculating the eigenvector using a version of the Power Method is a crucial part of the algorithm. 23 • The vector gn gets larger and larger as n → ∞ because |λ1| > 1. If both eigenvalues satisfied |λ | < 1 then gn → 0 as n → ∞. This illustrates the importance of the largest absolute values of the eigenvalues of a matrix in understanding the long term behaviour of systems of the form Ang0 for some A. If it represents a dynamical system it is related to stability or instability/chaos, if it represents e.g. the evolution of an economical system over time (or the finances of a company) it can be the difference between growth or ruin.24 22Take a look also at “Landau on Chess Tournaments and Google’s PageRank” by Rainer Sinn and G¨unter M. Ziegler (https://arxiv.org/pdf/2210.17300.pdf). 23An important advantage is that if we already have a good approximation of v1, e.g. the page ranks from last week, we can compute a better approximation of v1 (of this week’s rankings) with very few matrix multiplies, you can read more about it here [BSS] and in the references therein. 24Try to modify the Fibonacci recurrence rule so that the new numbers go to zero as n → ∞. Can you pick a recurrence such that they stabilize as n → ∞ (without going to ∞ or 0)? Maybe linear algebra students in 2823 years will be studying your sequence! 45 A few properties of the eigenvalues follow from the fact that, by Corollary 6.0.4, (32) (−1) n det(A − zI) = det(zI − A) = (z − λ1)(z − λ2) · · · (z − λn). The polynomial (32) is called the Characteristic Polynomial of the matrix A.25 Proposition 6.1.10. Given A ∈ Rn×n the eigenvalues of A are the same as the ones of A⊤. Proof. This follows from (32), and the fact that, for det(A − zI) = det((A − zI)⊤) = det(A⊤ − zI). 2 Definition 6.1.11. Given a matrix A ∈ Rn×n, the trace of A is defined as Tr(A) = n ∑ i=1 Aii. Proposition 6.1.12. Let A ∈ Rn×n and λ1, . . . , λn its n eigenvalues as they show up in (32) (mean- ing that a value λ may be repeated, the number of times it shows up is the algebraic multiplicity of the eigenvalue) then (33) Tr(A) = n ∑ i=1 λi, (34) det(A) = n ∏ i=1 λi. Remark 6.1.13. When calculating eigenvalues, Proposition 6.1.12 is very useful to check com- putations. Challenge 55. Given B ∈ Rn×m and C ∈ Rm×n show that Tr(BC) = Tr(CB). Challenge 56. Verify Proposition 6.1.12 on your favorite matrix, and also on M in Guiding Ex- ample 47. Challenge 57. Given A, B,C ∈ Rn×n show that we always have Tr(ABC) = Tr(BCA) = Tr(CAB), but that the following is not always true: Tr(ABC) = Tr(ACB). Linear Algebra — A. Bandeira (ETHZ) — Week 12 - 2023.12.08 & 2023.12.13 Please find most up to date notes at: https://ti.inf.ethz.ch/ew/courses/LA23/notes_part_II.pdf 25There is a converse to this in the sense that any monic polynomial can be written as a characteristic polynomial of a matrix, there is a particularly elegant way to build the matrix, if you are interested in learning more, look-up “companion matrix”. 46 Proof. [of Proposition 6.1.12] To prove (34), simply set z = 0 in (32) and note that it gives (−1)n det(A) = (−1)n ∏ n i=1 λi. For (33) note that the coefficient of zn−1 in the characteristic polynomial (32) is given in the right hand side by (− ∑ n i=1 λi). On the other hand, on the left hand side the coefficient of zn−1 is given by the summand in the determinant that multiplies the diagonal elements of zI − A, and so it is the coefficient of zn−1 of ∏ n i=1 (z − Aii) which is − ∑ n i=1 Aii = − Tr(A). 2 Caution! 58. We write this caution as Remark 6.1.14 below given how important it is (so that it appears in black font). Remark 6.1.14. A few important words of caution: (1) Even though the eigenvalues of A and A⊤ are the same, the eigenvectors are not! (2) The eigenvalues of A + B are not easily computed from the eigenvalues of A and the ones of B, in particular they are not their sum! (3) The eigenvalues of AB or BA are not easily computed from the eigenvalues of A and the ones of B, in particular they are not their product! 26 (4) Gaussian Elimination doesn’t preserve eigenvalues and eigenvectors. The eigenvalues are not the diagonal elements of the U matrix in the PA = LU factorization. 27 While we focus on real eigenvalues on this course, let us see an example of a matrix that has no real eigenvalues and only complex valued ones. Example 6.1.15. The eigenvalues of the matrix A = [ 0 −1 1 0 ] , corresponding to a 90o counter- clockwise rotation, are the solutions to 0 = det(A − λ I) = λ 2 + 1, which are λ1 = i and λ2 = −i. The eigenvectors are given by v1 = [ i 1 ] and v2 = [ −i 1 ] . Challenge 59. Try to work out an example for another rotation of R2. This is a particular case of an orthogonal matrix, whose eigenvalues have a special property. 26Interesting, there is a deep connection between A and B commuting (meaning AB = BA) and having the same eigenvectors. This is important in a few fields, in particular in Quantum Physics. If you want to learn more look-up “simultaneously diagonalizable”. 27How to actually compute eigenvalues efficiently is outside of the scope of this course, it turns out that one can do it using the QR decomposition that we learned here as a subroutine. If you want to learn more look-up “QR algorithm”. 47 Proposition 6.1.16. Let Q ∈ Rn×n be an orthogonal matrix.28 If λ ∈ C is an eigenvalue of Q, then |λ | = 1. Proof. Let λ ∈ C be an eigenvalue of Q and v ∈ Cn an associated eigenvector. Then Qv = λ v. Since Q is an orthogonal matrix we have ∥v∥2 = ∥Qv∥2 = ∥λ v∥2 = |λ |2∥v∥2. Since v ̸= 0 we have |λ | = 1. 2 Further Fact 60. If λ ∈ C is an eigenvalue of a matrix A with real entries, then λ is also an eigenvalue of A. This can be shown by noticing that det ( A − λ I) = det (A − λ I) (due to the polynomial representation of the determinant in (32), the fact that A has real entries). 6.1.1. Repeated eigenvalues. An important part of the success of the strategy we took in Guiding Example 47 was the fact that we were able to build a basis of R2 with eigenvectors of the matrix M. In Theorem 6.1.9 we showed that we can always build a basis of Rn with eigenvectors of an n × n matrix A if A has n distinct real eigenvalues. One obstacle could be if some of the eigenvalues are not real valued but, even though we have not focused in complex valued eigenvalues, a straightforward adaption of the proof shows that if A has n distinct eigenvalues (not necessarily real) then there is a basis of Cn made up of eigenvectors of A. However, repeated eigenvalues can (but doesn’t have to) pose a real obstacle to building a basis. Example 6.1.17. The matrix A = [ 0 1 0 0 ] does not have two linearly independent eigenvectors. Indeed, det(A − λ I) = λ 2 which means that λ = 0 is the only eigenvalue and has algebraic multiplicity 2. However, N(A − 0I) = N(A) only has dimension 1, so there is only one eigenvector (and multiples of it). with Example 6.1.18. The zero matrix A = [ 0 0 0 0 ] does have two linearly independent eigenvectors. Indeed, det(A − λ I) = λ 2 which means that λ = 0 is the only eigenvalue and has algebraic multiplicity 2. But, unlike in Example 6.1.17, N(A − 0I) = N(A) has dimension 2, so there is a basis made up of two eigenvectors (in fact any two linearly independent vectors will be such a basis). Further Remark 61. Notice that in Example 6.1.17, N ( A2) does have dimension 2. When there exist a positive integer k such that Ak = 0 we call A Nilpotent. There is a (rather deep) Theorem that essentially says nilpotency is the only obstacle to getting a complete set of eigenvectors. It 28The Cn analogue of orthogonal matrices are unitary matrices, U ∈ Cn×n that satisfy U ∗U = I. 48 roughly says that when there are “missing” eigenvectors they can be found in the Nullspace of powers of A − λ I, and this gives rise to something called “Jordan Normal Form”. Definition 6.1.19. If, given a matrix A ∈ Rn×n, we can build a basis of Rn with eigenvectors of A we say that A has a complete set of real eigenvectors. 29 Theorem 6.1.9 states that a matrix with n distinct eigenvalues always has a complete set of real eigenvectors. Proposition 6.1.20 (Eigenvalues and Eigenvectors of a Projection Matrix). Let P be the projec- tion matrix on the subspace U ⊆ Rn. Then P has two eigenvalues, 0 and 1, and a complete set of real eigenvectors. Proof. Let m be the dimension of U. Let u1, . . . , um be a orthonormal basis of U, and w1, . . . , wn−m an orthonormal basis of U ⊥. It is easy to see that Puk = 1uk for any 1 ≤ k ≤ m and Pwk = 0wk for any 1 ≤ k ≤ n − m, so all n vectors are eigenvectors of P (with eigenvalues either 1 or 0). By construction of U ⊥, they form an orthonormal basis. 2 In general when there is an eigenvalue λ with algebraic multiplicity larger than 1, it can be that N(A − λ I) is of large enough dimension to find enough linearly independent eigenvectors (as it is the case in projection matrices above, but not in the nilpotent example). Definition 6.1.21. Given a matrix A ∈ Rn×n and an eigenvalue λ of A we call the dimension of N(A − λ I) the geometric multiplicity of λ . Further Fact 62. A matrix has a complete set of eigenvectors when the geometric multiplicities are the same as the algebraic multiplicites of all eigenvalues. Exploratory Challenge 63. Prove Further Fact 62 Example 6.1.22. For D ∈ Rn×n a diagonal matrix, the eigenvalues of D are the diagonal entries of D. The canonical basis e1, . . . , en is a set of eigenvectors of D. Challenge 64. Prove the statement in Example 6.1.22 29If the matrix A has complex valued eigenvalues and we can instead build a basis of Cn we say it has a complete set of eigenvectors. Essentially everything we do below can be (straightforwardly) extended to this case but we will focus on real eigenvalues and eigenvectors for ease of exposition. 49 Further Fact 65. The eigenvalues of an n × n triangular matrix are the n values in the diagonal. However, triangular matrices may not have a complete set of eigenvectors. Challenge 66. Prove this fact. Hint: For the positive part use (32), for the negative part recall Example 6.1.17. Challenge 67. Let’s say a matrix A ∈ Rn×n has an LU decomposition (without the need for P in PA = LU). Remark 6.1.14 says the eigenvalues of A ∈ Rn×n are not the ones of U in the LU decomposition PA = LU. The eigenvalues of U are indeed their diagonal entries, and the eigenvalues of L are all 1 (by Further Fact 6.1.1). Why is it the case that the eigenvalues of A are not the diagonal entries of U? 6.2. Diagonalizing a Matrix and Change of Basis of a Linear Transformation. Let us con- tinue dissecting Guiding Example 47. Essentially, what we did was to write g0 in the basis v1, v2 of eigenvectors of M and then exploit the fact that linear transformation given M had a very simple behaviour when written in the basis v1, v2 (the coefficients simply were multiplied by the eigenvalues of M). This motivates us to take a detour in briefly studying linear transformations written in different basis, and to discuss “change of bases”. 6.2.1. Change of basis. For this detour we will briefly consider m × n matrices, before returning to square matrices when discussing eigenvalues and eigenvectors. Let Am×n be a matrix representing a linear transformation L : Rn → Rm given by x ∈ Rn → Ax ∈ Rm, with both input and output written in the canonical bases as x = ∑ n j=1 x je j and Ax = ∑ m i=1(Ax)iei. Recall (Example 4.4.2) that (ei) j = δi j, and that (Ax)i is the i-th entry of the vector Ax. Now, let’s say we have a basis for Rn given by u1, . . . , un and one for Rm given by v1, . . . , vm (neither being necessarily the canonical basis) and we want to understand the the linear transfor- mation L written in this basis. Then L takes a vector x = ∑ n j=1 α ju j and outputs L(x) = ∑ n j=1 βivi. We want to compute the matrix B that takes α =    α1 ... αn    to    β1 ... βm   . In other words, such that Bα = β . Let U ∈ Rn×n be the matrix whose columns are the basis elements u1, . . . , un and V ∈ Rm×m the matrix whose columns are the basis elements v1, . . . , vm. Then, x = Uα and L(x) = V β and so β = V −1AUα, the matrix B, corresponding to the linear transformation L writ- ten in the new bases is B = V −1AU. Note that we can do change of basis between any pair of basis, it needs not be from the canonical basis to another basis, in that case the role of U and V 50 would be played by the change of basis matrix (the matrix that maps the coefficients of a vector written in the old basis, to its coefficients when written in the new basis). L : R n → R m linear transformation L ( n ∑ j=1 x je j ) = n ∑ i=1 (Ax)iei x =    x1 ... xn   (35) L ( n ∑ j=1 α ju j ) = n ∑ i=1 (Bα)ivi α =    α1 ... αn    where B = V −1AU ∈ R m×n, U = [ u1 · · · un ] ∈ R n×n, V = [ v1 · · · vm ] ∈ Rm×m. 6.2.2. Diagonalizing a Matrix. Let us focus back on square matrices A ∈ Rn×n. In particular, let A be a matrix with a complete set of real eigenvectors (in the sense of Definition 6.1.19) and let v1, . . . , vn ∈ Rn×n be a basis formed with eigenvectors of A. The crucial fact we used in Guiding Example 47 also holds in this general situation: if we write a vector x ∈ Rn as x = ∑ n i=1 αivi then Ax = ∑ n i=1 λiαivi (and also Akx = ∑ n i=1 λ k i αivi, for k ≥ 1, where λi is the eigenvalue associated with the eigenvector vi). One way to think about this is that the linear transformation corresponding to the matrix A, when written in the basis V is simply a diagonal matrix/transformation. This is the key idea behind Matrix Diagonalization. This is one of the most important facts in Linear Algebra. Theorem 6.2.1. Let A ∈ Rn×n be a matrix with a complete set of real eigenvectors (in the sense of Definition 6.1.19) and let v1, . . . , vn ∈ Rn×n be a basis formed with eigenvectors of A and let λ1,. . . ,λn be the associated eigenvalues (λi associated to vi). Let V be the matrix whose columns are the eigenvectors vi, V = [ v1 · · · vm ] ∈ Rn×n. Then, (36) A = V ΛV −1, where Λ is a diagonal matrix with Λii = λi (and Λi j = 0 for all i ̸= j). Proof. Since v1, . . . , vn is a basis, V is an invertible matrix, so it suffices to prove that (37) V −1AV = Λ. This can be done by direct calculation: For any 1 ≤ j ≤ n, the j-th column of the matrix V −1AV is given by ( V −1AV ) · j := ( V −1AV ) e j = V −1Av j = V −1λ jv j = λ jV −1v j = λ je j, 51 since V −1v j = V −1Ve j = e j. Recall that e j is the vector in Rn with a 1 in j-th entry and zero else- where. Since for any 1 ≤ j ≤ n, λ je j is also the j-th column of Λ, we have that V −1AV = Λ. 2 Definition 6.2.2 (Diagonalizable Matrix). A matrix A ∈ Rn×n is called a diagonalizable matrix if there exists an invertible matrix V such that V −1AV = Λ, where Λ is a diagonal matrix. Challenge 68. Most properties of the eigenvalues are very easy to prove by using Theorem 6.2.1 (for the matrices that have a complete set of eigenvectors). Try it! The eigenvalues of Λ are also λ1, . . . , λn (recall Example 6.1.22). More generally, for an invertible matrix S we always have that A and S−1AS have the same eigenvalues. Definition 6.2.3 (Similar Matrices). 30 We say that A ∈ Rn×n and B ∈ Rn×n are similar matrices if there exists an invertible matrix S such that B = S−1AS. Proposition 6.2.4. Similar matrices have the same eigenvalues. Challenge 69. Try to show Proposition 6.2.4 via (32). Challenge 70. Try to show that if λ is an eigenvalue of S−1AS (with associated eigenvector v) then it is also an eigenvalue of A and compute the associated eigenvector in terms of v and S. (without using Proposition 6.2.4 or (32). Remark 6.2.5 (Diagonalizing a matrix and finding a good basis for a linear transformation). If we have a matrix A ∈ Rn×n with a complete set of real eigenvectors then Theorem 6.2.1 tells us that the corresponding linear transformation, when viewed in the bases v1, . . . , vn is simply a diagonal matrix (recall that in this case B = Λ, see (35)). This is a remarkable fact: since most matrices have a full set of eigenvectors (in particular all for which the eigenvalues are all distinct do) this says that all the corresponding linear combinations, regardless of how complicated they might seem, are actually just a diagonal operation when viewed in the basis v1, . . . , vn. 6.3. Symmetric Matrices and the Spectral Theorem. This section is devoted to real symmet- ric matrices,31 meaning matrices A ∈ Rn×n for which A⊤ = A (see Further Remark 74 for a brief discussion of how symmetric matrices appear naturally in several settings). The main goal of this section is to prove the Spectral Theorem. 30The operation A → S−1AS is sometimes called conjugation but this is not to be confused with complex conju- gation z → z, one term comes from “conjugation” in group theory, the other from “conjugation” in complex analysis. 31The same theory can be developed (by a straightforward adaption) to complex matrices but the property of being symmetric is replaced by being Hermitian, which means that a matrix A = A∗ = A ⊤ . In both situations we say A is self-adjoint. 52 Theorem 6.3.1 (Spectral Theorem). Any symmetric 32 matrix A ∈ Rn×n has n real eigenvalues and an orthonormal basis made of eigenvectors of A. Together with Theorem 6.2.1 this implies the following corollary. Corollary 6.3.2. For any symmetric matrix A ∈ Rn×n there exists an orthogonal matrix V ∈ Rn×n (whose columns are eigenvectors of A) such that A = V ΛV ⊤, where Λ ∈ Rn×n is a diagonal matrix with the eigenvalues of A in its diagonal (and V ⊤V = I). Remark 6.3.3 (Eigendecomposition). The decompositions in Corollary 6.3.2 and Theorem 6.2.1 are called Eigendecompositions. The following follows easily from the Spectral Theorem. Corollary 6.3.4. The rank of a real symmetric matrix A is the number of non-zero eigenvalues (counting repetitions). Remark 6.3.5. For general n × n (non-symmetric) matrices, the rank is n minus the dimension of the nullspace, so it is n minus the geometric multiplicity of λ = 0. Since symmetric matrices always have a complete set of eigenvalues and eigenvectors, the geometric multiplicities are always the same as the algebraic multiplicities. Proposition 6.3.6. Let A be a real n × n symmetric matrix and let v1, . . . , vn be an orthonormal basis of eigenvectors of A (the columns of the matrix V in Corollary 6.3.2) and λ1, . . . , λn the associated eigenvalues. Then A = n ∑ k=1 λiviv ⊤ i Proof. Follows directly by Corollary 6.3.2. 2 We “build up” to the proof of Theorem 6.3.1 with a few propositions. Proposition 6.3.7. Let A ∈ Rn×n be a symmetric matrix and λ ∈ C an eigenvalue of A, then λ ∈ R. 32Essentially the same proof shows that any A ∈ Cn×n that is Hermitian, also called self-adjoint A∗ = A, has n real eigenvalues and a complete set of orthonormal eigenvectors in Cn (the matrix whose columns are these vectors is a Unitary matrix U satisfying U ∗U = I. 53 Proof. Let v ∈ Cn be an eigenvector associated with the eigenvalue λ . We have Av = λ v. Recall that, for a matrix (or vector) M, its Hermitian conjugate is given by M∗ = M⊤. Since A is real symmetric we have A∗ = A. Thus, we have λ ∥v∥ 2 = λ v ∗v = (λ v) ∗v = (Av) ∗v = v ∗A∗v = v ∗Av = v ∗λ v = λ ∥v∥ 2. Since v ̸= 0, then ∥v∥ ̸= 0 and so λ = λ . This implies that λ ∈ R. 2 This, together with Theorem 6.1.4, immediately implies the following. Corollary 6.3.8. Every symmetric matrix A ∈ Rn×n (satisfying A = A⊤) has a real eigenvalue λ . Remark 6.3.9. The fact that two eigenvectors of a real symmetric matrix are orthogonal follows from Theorem 6.3.1 but it is useful to see a simple argument of that (the main difficulty of proving Theorem 6.3.1 is proving that the matrix indeed has a complete set of eigenvectors). Let’s say we have λ1 ̸= λ2 eigenvalues of a real symmetric matrix A and v1, v2 ∈ Rn \\ {0} corresponding eigenvectors. Then λ1v ⊤ 1 v2 = (Av1) ⊤ v2 = v ⊤ 1 A ⊤v2 = v ⊤ 1 Av2 = v ⊤ 1 (Av2) = λ2v ⊤ 1 v2, since λ1 ̸= λ2 we must have that v⊤ 1 v2 = 0 Further Remark 71. Corollary 6.3.8 is a great example of the usefulness of complex numbers. Even though it is a statement just about real matrices and real eigenvalues we proved it by going through the complex numbers and using results in Complex Analysis. There is an alternative proof without going through the complex numbers but it would need more background, and I find this one more transparent. 33 Linear Algebra — A. Bandeira (ETHZ) — Week 13 - 2023.12.15 & 2023.12.20 Please find most up to date notes at: https://ti.inf.ethz.ch/ew/courses/LA23/notes_part_II.pdf Proof. [of Theorem 6.3.1] Let A ∈ Rn×n be a symmetric matrix. We will prove the following by induction, which for k = n implies the theorem we want to show: • For any 1 ≤ k ≤ n there are k orthogonal eigenvectors of A. The base case k = 1 follows directly by Corollary 6.3.8 as we can always normalize the eigen- vector to have norm 1. 33If you would like to see a nice example of how improving knowledge can lead to much simpler and more transparent methods/models, search “Ptolemaic Epicycle Machine”. 54 We now assume that the statement is true for k and show it for k + 1. We will show that if a real symmetric matrix A has k (with 1 ≤ k < n) orthonormal eigenvectors then we can build an extra one, orthogonal to the others (to achieve norm 1 we simply need to normalize it). 34 Let v1, . . . , vk denote k orthonormal eigenvectors of A and λ1, . . . , λk the respective eigenvalues. Let uk+1, . . . , un be an orthonormal basis of the orthogonal complement of the span of v1, . . . , vk. Let Vk be the n × n matrix whose i-th column is vi if i ≤ k and ui if i > k. Vk is an orthogonal matrix. Moreover, let us define B ∈ Rn×n as B = V ⊤AV , then: B = V ⊤AV =            — v⊤ 1 — ... — v⊤ k — — u⊤ k+1 — ... — u⊤ n —                    | | | | Av1 · · · Avk Auk+1 · · · Aun | | | |         =            — v⊤ 1 — ... — v⊤ k — — u⊤ k+1 — ... — u⊤ n —                    | | | | λ v1 · · · λ vk Auk+1 · · · Aun | | | |         = [ Λk 0k×(n−k) 0(n−k)×k C ] , where Λk is a diagonal matrix with λ1, . . . , λk in the diagonal, 0(n−k)×k and 0k×(n−k) are zero matrices of size respectively (n − k) × k and k × (n − k). C is a (n − k) × (n − k) symmetric matrix. Since C is a (n − k) × (n − k) symmetric matrix, Theorem 6.3.8 implies it has a real eigenvalue λk+1 and a real eigenvector y ∈ Rn−k. Let w ∈ Rn be the vector with 0 in the first k coordinates and y in the remaining n − k, in other words wi = { 0 if i ≤ k yi−k if i > k. 34In a first reading of the proof I recommend taking k = 1 in the induction step, as it is simpler while already containing all the relevant ideas. 55 We have Bw = [ Λk 0k×(n−k) 0(n−k)×k C ] [ 0k×1 y ] = [ 0k×1 Cy ] = [ 0k×1 λk+1y ] = λk+1w. Let vk+1 := V w. Since V is orthogonal we have that A = V BV ⊤. Thus, Avk+1 = V BV ⊤vk+1 = V Bw = V λk+1w = λk+1vk+1, so vk+1 is an eigenvector of A. To see that it is orthogonal to v1, . . . , vk note that the inner products v⊤ i vk+1 for i ≤ k appear in the first k entries of V ⊤vk+1 = w and that w has its first k coordinates 0 by construction. By normalizing the vector we can have it have unit norm. 2 Proposition 6.3.10 (Rayleigh Quotient). Given a symmetric matrix A ∈ Rn×n the Rayleigh Quo- tient, defined for x ∈ Rn \\ {0}, as R(x) = x⊤Ax x⊤x attains its maximum at R(vmax) = λmax and its minimum at R(vmin) = λmin where λmax and λmin are respectively the largest and smallest eigenvalues of A and vmax, vmin their associated eigen- vectors. Proof. It is easy to see that R(vmax) = λmax and R(vmin) = λmin so it suffices to show that, for all x ∈ Rn \\ {0} we have λmin ≤ R(x) ≤ λmax. Using Proposition 6.3.6 we can write, for x ∈ Rn \\ {0}, R(x) = x⊤ ( ∑ n i=1 λiviv⊤ i ) x ∥x∥2 = ∑ n i=1 λi ( x⊤vi)2 ∥x∥2 , where v1, . . . , vn form an orthonormal basis of eigenvectors of A and λ1, . . . , λn are the associated eigenvalues. Since ( x⊤vi)2 ≥ 0 for all 1 ≤ i ≤ n we have that, for all 1 ≤ i ≤ n, λmin ( x⊤vi)2 ≤ λi ( x⊤vi)2 ≤ λmax ( x⊤vi)2 . Collecting all these inequalities we get λmin ∑ n i=1 ( x⊤vi)2 ∥x∥2 ≤ ∑ n i=1 λi ( x⊤vi)2 ∥x∥2 ≤ λmax ∑ n i=1 ( x⊤vi)2 ∥x∥2 . To conclude the proof note that, since the vi’s are orthonormal, the matrix V with the vi’s as columns is orthogonal and ∑ n i=1 ( x⊤vi)2 = ∥V x∥2 = ∥x∥2 and so ∑ n i=1(x⊤vi) 2 ∥x∥2 = 1. 2 56 Definition 6.3.11 (Positive Definite and Positive Semidefinite matrix). A symmetric matrix A ∈ Rn×n is said to be Positive Semidefinite (PSD) if all its eigenvalues are non-negative. If all the eigenvalues of A are strictly positive then we say A is Positive Definite (PD). Exploratory Challenge 72. Even though the eigenvalues of A + B are not easily described by the eigenvalues of A and the ones of B it turns out that if both are PSD (or PD) then so is the sum. Can you show that? The following follows directly from Proposition 6.3.10. Proposition 6.3.12. A symmetric matrix A ∈ Rn×n is Positive Semidefinite if and only if x⊤Ax ≥ 0 for all x ∈ Rn. Analogously, a symmetric matrix A ∈ Rn×n is Positive Definite if and only if x⊤Ax > 0 for all x ∈ Rn \\ {0}. Fact 6.3.13. If two n × n matrices A and B are PSD (or PD) then their sum is PSD (or PD). Challenge 73. Exploratory Challenge 72 looked pretty difficult, but now with Proposition 6.3.12 it is much easier, try to prove Fact 6.3.13. Definition 6.3.14 (Gram Matrix). Given n vectors, v1, . . . , vn in Rm we call their Gram Matrix the n × n matrix of inner products Gi j = v ⊤ i v j. Note that if V ∈ Rm×n is the matrix whose columns are the n vectors, then G = V ⊤V is the Gram matrix of V . Remark 6.3.15. Given a matrix A ∈ Rm×n, as an abuse of notation, we sometimes also call AA⊤ a Gram matrix of A. Notice that, if a1, . . . , an ∈ Rm are the columns of A then AA⊤ is m × m and (38) AA ⊤ = n ∑ i=1 aia ⊤ i . Proposition 6.3.16. Given a real matrix A ∈ Rm×n, the non-zero eigenvalues of A⊤A ∈ Rn×n are the same as the ones of AA⊤ ∈ Rm×m. Both matrices are symmetric and positive semidefinite. Proof. Let r be the rank of A. We know rank(A) = rank(A⊤) = rank(A⊤A) = rank(AA⊤) (recall Proposition 4.3.2 and Challenge 9). It is straightforward that both A⊤A and AA⊤ are symmetric. Let us prove they are positive semidefinite. We have x⊤A⊤Ax = ∥Ax∥2 ≥ 0 for all x which implies A⊤A is PSD, and the same argument can be used for AA⊤. Now, both AA⊤ and A⊤A have a complete set of real eigenvalues and orthogonal eigenvectors. Let λ1, . . . , λr be the r non-zero eigenvalues of A⊤A and v1 . . . , vr be the corresponding eigenval- ues. We have, for 1 ≤ k ≤ r, A⊤Avk = λkvk, multiplying by A both sides we get AA⊤Avk = λkAvk 57 and so λk is an eigenvalue of AA⊤ with eigenvector Avk (note that Avk ̸= 0). Furthermore, For j ̸= k we have (Av j)⊤(Avk) = v⊤ j A⊤Avk = v⊤ j λkvk = λkv⊤ j vk = 0 and so the r eigenvectors of AA⊤ built this way are orthogonal, and so λ1, . . . , λr are the nonzero eigenvectors of AA⊤. 2 Proposition 6.3.17. [Cholesky decomposition] Every symmetric positive semidefinite matrix M is a gram matrix of an upper triangular matrix C. M = C⊤C is known as the Cholesky Decom- position. 35 Proof. Let M be a symmetric positive semidefinite matrix. Corollary 6.3.2 gives us a de- composition M = V ΛV ⊤ with Λ a diagonal matrix with the eigenvalues of M in the diagonal. Since M is PSD, the diagonal entries of Λ are non-negative and so we can build Λ1/2 by tak- ing the square root of each diagonal entry of Λ. Then M = ( V Λ1/2) ( V Λ1/2)⊤. To make the matrices in the decomposition be upper triangular, simply take the QR decomposition (re- call Definition 4.4.12) ( V Λ1/2)⊤ = QR with Q such that Q⊤Q = I and R upper triangular. We have M = ( V Λ1/2) ( V Λ1/2)⊤ = (QR) ⊤ (QR) = R⊤Q⊤QR = R⊤R. Taking C = R establishes the Proposition. 36 2 Further Remark 74. At first glance, Symmetric matrices look very special (since we must have A⊤ = A) but they they actually appear very often in both applications and pure mathematics. For example, in my own work, I rarely encounter non-symmetric matrices. There are (at least) two reasons for this: (i) For any matrix B we can form a symmetric matrix B⊤B from which we can study B, this is going to be the key idea behind the Singular Value Decomposition. (ii) In many instances, matrices represent relationship between objects — for example, Ai j can represent a friendship connection (or a similarity measure) between person (or data point) i and j and in many cases such relationships are symmetric. Exploratory Challenge 75. Recall the symmetric LU decomposition from the first part of the course, what can you say of such a decomposition for PSD matrices? How is it related to the Cholesky Decomposition? 35To compute an upper triangular matrix C such that M = C⊤C one can use the LU decomposition and needs not to compute an eigendecomposition of the matrix M. 36This is not the classical construction of the Cholesky Decomposition. The classical construction is with Gauss- ian Elimination, but at this stage of the course I think this is more transparent. Note also that when using Gaussian Elimination C will be a square matrix, while here R can be rectangular if M is not full rank (which makes it a more economical decomposition). 58 7. SINGULAR VALUE DECOMPOSITION; AND SOME OPEN QUESTIONS IN LINEAR ALGEBRA 7.1. The Singular Value Decomposition. We are now reaching what I view as “the ultimate theorem of our class”, the Singular Value Decomposition (SVD). In fact, a mentor of mine once said: “If I could take only one algorithm with me to a desert island, it would be the SVD”. The SVD is a way to generalize the eigendecomposition to non-symmetric, and even non-square, matrices. Instead of eigenvalues we will have singular values and instead of eigenvectors we will have (right and left) singular vectors. Definition 7.1.1 (SVD — Singular Value Decomposition). Let A ∈ Rm×n. There exist orthogonal matrices U ∈ Rm×m and V ∈ Rn×n such that (39) A = UΣV ⊤, where Σ ∈ Rm×n is a diagonal matrix, in the sense that Σi j = 0 when i ̸= j, and the diagonal elements are non-negative and ordered in descending order. U ⊤U = I and V ⊤V = I. The columns u1, . . . um of U are called the left singular vectors of A and are orthonormal. The columns v1, . . . vn of V are called the right singular vectors of A and are orthonormal. The diagonal elements of Σ, σi = Σii are called the singular values of A and are ordered as σ1 ≥ · · · ≥ σmin{m,n}. Remark 7.1.2. If A has rank r we can write the SVD in a more compact form: (40) A = UrΣrV ⊤ r , where Ur ∈ Rm×r contains the first r left singular vectors, Vr ∈ Rn×r contains the first r right singular vectors and Σr ∈ Rr×r is a diagonal matrix with the first r singular values. Notice that storing such a decomposition in the computer requires storing r ×(m+n+1) real numbers rather than m × n real numbers which would be required to store A naively. When a matrix has small rank these are crucial savings. 37. Oftentimes the subscript is omitted and the compact SVD is simply written as UΣV ⊤ while spec- ifying the dimensions of the matrices involved to specify which form of the SVD is being consid- ered. Remark 7.1.3. Let A ∈ Rm×n and A = UΣV ⊤ be its SVD (as in (39)) then AA⊤ = U ( ΣΣ⊤)U ⊤, 37Taking this one step forward, when a matrix is well approximated by a low rank matrix, oftentimes one stores only a small rank approximation of a matrix A, this is a crucial idea in tasks ranging from Image Compressions, Numerical Analysis, and Machine Learning, see Section 7.3. 59 and so the left singular vectors of A, the columns of U, are the eigenvectors of AA⊤ and the singular values of A are the square-root of the eigenvalues of AA⊤ (note that ΣΣ⊤ is m × m diagonal). If m > n, A has n singular values and AA⊤ has m eigenvalues (which is larger than n), but the “missing” ones are 0. Analogously, A⊤A = V ( Σ ⊤Σ)V ⊤, and so the right singular vectors of A, the columns of V , are the eigenvectors of A⊤A and the sin- gular values of A are the square-root of the eigenvalues of A⊤A (note that Σ⊤Σ is n × n diagonal). If n > m, A has m singular values and A⊤A has n eigenvalues (which is larger than m), but the “missing” ones are 0. This observation makes it easier to write the singular values and singular vectors of A in terms of eigenvalues and eigenvectors of AA⊤ and A⊤A, which are symmetric matrices (and directly implies, e.g., uniqueness of singular values; and the fact that the rank of a matrix is the number of nonzero singular values). In fact, the proof of the existence SVD will heavily rely on the Spectral Theorem. An important direct consequence of the SVD, and in particular of (40) is that we can write any rank-r matrix A ∈ Rm×n as a sum of r rank-1 matrices: Proposition 7.1.4. Let A ∈ Rm×n be a matrix with rank r. Let σ1, . . . , σr be the non-zero singular values of A, u1, . . . , ur the corresponding left singular vectors and v1, . . . , vr the corresponding right singular vectors. Then (41) A = r ∑ k=1 σkukv ⊤ k . Challenge 76. The SVD is a powerful tool. Many of the things we did in this course become significantly simpler with the SVD. Now that you have the SVD, try to reread these notes and try to re-interpret the results we derived in terms of the SVD. For example, the Moore-Penrose Pseudoinverse has a very simple description of the SVD, it corresponds to swapping U and V and replacing the non-zero singular values by their inverses, while keeping the zero ones zero. Try to derive this! Theorem 7.1.5. [The SVD – the Ultimate Theorem of ETHZ 401-0131-00L] Every matrix A ∈ Rm×n has an SVD decomposition of the form (39). In other words: 60 Every linear transformation is diagonal when viewed in the bases of the singular vectors. Proof. Let Am×n. Let r be the rank of Am×n. We will build a compact SVD as in (40). It is easy to see that we can get an SVD in the sense of (39) from a compact one by adding singular values that are zero and extending the singular vectors in both Ur and Vr to orthonormal bases. By Theorem 6.3.1 and Corollary 6.3.2 the matrix AA⊤ has a complete set of orthonormal eigen- vectors and can be written as (42) AA ⊤ = UΛU ⊤, where U ∈ Rm×m is orthogonal and Λ is diagonal. Let us write (42) ordering the diagonal entries of Λ in decreasing order. Furthermore, let us write (42) also in a compact form, by keeping only the r non-zero eigenvalues (and corresponding eigenvectors), so AA ⊤ = UrΛrU ⊤ r for Ur ∈ Rm×r such that U ⊤ r Ur = I and Λr is r × r diagonal with the non-zero eigenvalues of AA⊤. By Proposition 6.3.16 the eigenvalues of AA⊤ are non-negative and so the diagonal entries of Λr are positive. Let Σr ∈ Rr×r be the diagonal matrix with diagonal entries σi := (Σr)ii = √ Λii. Our goal is to show that there is a n × r matrix Vr, with orthonormal columns, such that A = UrΣrV ⊤ r . We would have Σ−1 r U ⊤ r A = Σ−1 r U ⊤ r UrΣrV ⊤ r = V ⊤ r , or equivalently Vr = A⊤UrΣ−1 r . Motivated by this, let’s set Vr := A⊤UrΣ −1 r , this corresponds to a matrix with columns v1, . . . , vr given by vk = 1 σk A⊤uk. To conclude we need to show that this construction indeed gives a compact SVD, for this we still need to show two things: (1) V ⊤ r Vr = I. This can be verified by direct computation, while recalling that AA⊤ = UrΛrU ⊤ r : V ⊤ r Vr = ( A⊤UrΣ −1 r )⊤ A ⊤UrΣ−1 r = Σ −1 r U ⊤ r AA ⊤UrΣ −1 r = Σ−1 r U ⊤ r UrΛrU ⊤ r UrΣ−1 r = Σ −1 r ΛrΣ−1 r = I (2) A = UrΣrV ⊤ r . Note that UrΣrV ⊤ r = UrΣr ( A⊤UrΣ −1 r )⊤ = UrU ⊤ r A, but, by construction, UrU ⊤ r is the projection on C(AA⊤) and so it is also the projection on C(A) (since C(A) = C(AA⊤), use Proposition 4.3.2 for A⊤). Thus UrU ⊤ r A = A. 2 61 7.2. Vector and Matrix Norms. A short section on vector and matrix norms. So far, the norm of a vector x ∈ Rn was simply given by ∥x∥ = x⊤x but there are instances where it makes sense to measure the “lenght” of vectors in other ways. One popular way is called the “Manhattan distance” since when traveling in Manhattan one cannot take advantage of Pythagoras Theorem because that would involve cutting through buildings, that norm is given by ∥x∥1 = ∑ n i=1 |xi|. In general, for 1 ≤ p ≤ ∞ the ℓp norm is given by (43) ∥x∥p = ( n ∑ i=1 |xi|p)1/p , for p < ∞, and ∥x∥∞ = maxi |xi|. Notice that ∥ · ∥2 corresponds to the Euclidean norm that we have used in this course. 38 Challenge 77 (⋆). Prove that, for all x ∈ Rn, we have ∥x∥2 ≤ ∥x∥1 ≤ √ n∥x∥2. In several situations one also needs to “measure” the size of matrices (for example, when talking about a matrix being close to another one, we need a notion of distance, or norm of the difference). Definition 7.2.1 (Two matrix norms). Given a matrix A ∈ Rm×n we define two matrix norms: • ∥A∥F , known as the Frobenius norm, is defined as ∥A∥F = √ m ∑ i=1 n ∑ j=1 A2 i j, • ∥A∥op, known as operator or specral norm, is defined as ∥A∥op = max x∈Rn s.t.∥x∥=1 ∥Ax∥. Further Proposition 78. Given A ∈ Rm×n with singular values σ1 ≥ · · · ≥ σmin{m,n}. We have (1) ∥A∥ 2 F = Tr ( AT A) (2) ∥A∥ 2 F = min{m,n} ∑ i=1 σ 2 i (3) ∥A∥op = σ1 (4) ∥A∥op ≤ ∥A∥F ≤ √ min{m, n}∥A∥op. Challenge 79 (⋆). Prove Further Proposition 78. 39 38The ℓ1 norm is notable for promoting sparsity when one attempts to minimize it to solve underdetermined linear systems. This is the key idea behind “Compressed Sensing”, and plays a crucial role in many imaging/sensing technologies. You can read more about it in Section 12 of [BM23] or Chapter 10 of [BSS] and references therein. 39Hint: The order I chose for Further Proposition 78 was so that it is easiest to prove these properties in this order. This is a good exercise to help consolidate your understanding of the SVD, and other concepts in this course. 62 7.3. Low-Rank Modelling, Images, Data, and Principal Component Analysis. Further Remark 80. This section serves as a CS Lens. One of the most powerful ideas in Lin- ear Algebra, from an application perspective, is that if one takes a matrix A, writes the SVD of A and then sets of zero all singular values except the largest r, the resulting matrix Ar is good rank-r approximation to the original matrix. In fact, in several senses it is the best possible rank-r approximation. The celebrated Eckart-Young-Minsky Theorem says that it is the best approxi- mation in the sense that ∥A − Ar∥ is the smallest possible in a large class of matrix norms ∥ · ∥. This idea is key for recommendation systems (and played a central role in the “Netflix prize”), it is the basis for Principal Component Analysis (PCA), and can even do image compression. I will describe some of these things as a CS Lens but you can also read an exposition of it in lecture notes from another course. With your background you can now read Sections 3–5 of [BM23], or you can wait to learn all of this in other classes in your degree. 40 7.10. Some Mathematical Open Problems. Now that we have covered the notion of eigenval- ues and eigenvectors, there are a few fascinating open questions we can state. These are questions (or conjectures), that we currently do now know the answer to (or that we are not sure they are true). I have a list of 42 open problems in some lecture notes [Ban16] I wrote almost a decade ago, some have been solved in the meantime, but many remain open. I write below a few for which you have all the necessary background to understand: Let me know if you solve any of them; regardless of whether its days or decades from now, I will be very happy to hear about your solution! Conjecture 81 (Hadamard conjecture). For any n multiple of 4 there exists an Hadamard matrix H that is n × n. An Hadamard matrix H ∈ Rn×n is a matrix with only entries 1 or −1 that is a multiple of an orthogonal matrix. In other words Hi, j = ±1 for all i, j and H⊤H = nI. Yet in other words: the columns of H are an orthogonal basis for Rn formed with only vectors with entries ±1. Open Problem 82 (Mutually Unbiased Bases). See Open Problem 6.2 in [Ban16]. Conjecture 83 (Zauner’s Conjecture). See Open Problem 6.3 in [Ban16]. Conjecture 84 (Komlos Conjecture). See Open Problem 0.1 in [Ban16]. 40A particularly striking example of PCA is the “Genes mirror geography within Europe” experi- ment by J. Novembre et al, available at https://www.nature.com/articles/nature07331 and https://faculty.eeb.ucla.edu/Novembre/Novembreetal2008Nature.pdf. Also dis- cussed on National Geographic https://www.nationalgeographic.com/science/article/ european-genes-mirror-european-geography. 63 Conjecture 85 (Matrix Spencer Conjecture). See Open Problem 4.3 in [Ban16]. Open Problem 86 (Rank of the Matrix Multiplication Tensor). What is the rank of the Matrix Multiplication Tensor corresponding to multiplication of 3 × 3 matrices. A d1 × d2 × d3 tensor T is what we can think of as a cubic matrix. It has d1d2d3 entries given by Ti jl. We say T has rank r if r is the smallest integer such that we can write T = r ∑ k=1 ak ⊗ bk ⊗ ck, for ak ∈ Rd1, bk ∈ Rd2, ck ∈ Rd3, for k = 1, . . . , r. In other words Ti jl = r ∑ k=1(ak)i(bk) j(ck)l. Recall Proposition 7.1.4 to see why for matrices this corresponds to the notion of rank we have been using. While computing the rank of a matrix is computationally easy, doing so for tensors is notoriously difficult (because they lack a spectral theory of eigenvalues and eigenvectors). There is a way to think of Strassen’s algorithm (that you saw in a CS Lens in Part I of the course) in terms of a decomposition of a certain Tensor in terms of rank-1 tensors. In this description we focus on n × n matrices, but the same thing can be done for rectangular matrices. The n × n matrix multiplication tensor is a n2 × n2 × n2 tensor, where each dimension is indexed by pairs (i1, i2), ( j1, j2), (l1, l2) and T is given by T(i1,i2),( j1, j2),(l1,l2) = { 1 if i1 = j1, j2 = l1, l2 = i2 0 o.w. . Strassen’s algorithm can be viewed as the fact that the rank of the 2 × 2 matrix multiplication tensor (a 4 × 4 × 4 tensor) is ≤ 7. The rank of the 3 × 3 matrix multiplication tensor (a 9 × 9 × 9 tensor) remains unknown. 41 42 41To the best of my knowledge, the current “world records” for lower and upper bounds are 19 and 23, see for example https://mathoverflow.net/questions/249256/ best-known-bounds-on-border-ranks-of-small-matrix-multiplication-tensors? noredirect=1&lq=1 or https://mathoverflow.net/questions/151058/ best-known-bounds-on-tensor-rank-of-matrix-multiplication-of-3x3-matrices. 42See https://www.youtube.com/watch?v=fDAPJ7rvcUw for a very nice description of how AI methods found a better low rank decomposition for the matrix multiplication tensor for some dimensions, and https://www.nature.com/articles/s41586-022-05172-4 for the paper the video discusses (make sure to take a look at the table in Figure 3 in that article). 64 Conjecture 87 (The Paley ETF Conjecture). See Open Problem 6.4 in [Ban16] (see also Open Problem 5.1. in the same reference). 43 Conjecture 88 (Ellipsoid Problem). See Conjecture 1.1. in either https://arxiv.org/ pdf/2307.01181.pdf or https://arxiv.org/pdf/2310.05787.pdf.44 Acknowledgements. Many thanks to all the Linear Algebra teaching team! A particularly shout out to many of you that caught countless typos on these notes and gave countless suggestions that improve them! Thanks (a partial list): Bernd G¨artner, Sebastian Haslebacher, Felix Breuer, Till Schnabel, Mattia Taiana, Tanguiy Magne, Aviv Segall, Matthieu Croci, Sergey Prokudin, Mia Filic, Sofia Giampietro, Seyedmorteza Sadat, Mark Sosman, Elia Trachsel, Silvan Bolt. Felix and Till in particular are responsible for countless improvements in the notes, thanks! Thanks also to Dustin Mixon (OSU) for inspiring discussions on how to teach linear algebra, and for showing us eigenquiz! And, most importantly, a special thanks to the ETH D-INFK students of Linear Algebra in Fall 2023 that made teaching this course an absolute pleasure! I hope you also enjoyed it! APPENDIX A. SOME IMPORTANT PRELIMINARIES AND REMARKS ON NOTATION To follow these notes the reader needs to be familiar with basics of vector and matrix operations and manipulations; understand what is dimension of a subspace, and in particular that is well- defined (that every basis of a subspace has the same size); and understand what is the rank of a matrix (and in particular that the dimension of the column space and the row space are the same). Even though Gaussian Elimination is not a core ingredient of this part of the course, we still assume that the reader is familiar with it. The students of 401-0131-00L are familiar with all this via Part I of this course. Some further important preliminaries and/or remarks: (1) The dot product x · y between two real valued vectors is sometimes also called inner product and written as ⟨x, y⟩ (it is equal to x⊤y). For Cn the inner product is given by ⟨x, y⟩ = y∗x. (2) Matrix Factorization for A an m × n matrix with rank r: A = CR, C is m × r with linearly independent columns (they are the first r linearly independent 43This one needs some (light) Number Theory background to understand. I posed this one (with collaborators) and spent many hours trying to make progress on it. . . 44This one needs some (light) Probability Theory to understand 65 columns of A). R is r × n, it is upper triangular (i.e. Ri j = 0 if i > j), and it has an r × r identity as a submatrix, corresponding to the locations of the first r linearly independent columns of A. (3) For V a subspace (or a vector space) with dimension n the following holds: • Any basis of V has size n. • Any spanning set of V has size ≥ n. • Any spanning set of V with size n is also a basis. • Any set of linearly independent vectors in V has size ≤ n. • Any set of linearly independent vectors in V with size n is also a basis. APPENDIX B. WEEKLY SCHEDULE Numbers represent Fr-Wed weeks of 4×45min lectures (except first and last). CS lenses are generally not in the script (nor do they appear in this schedule). (7) 8.11.2023: Introductions; 4.2. Projections (8) 4.3. Least Squares & Fitting a line, 4.4 Orthonormal bases (9) QR decomposition, 4.5. and Intro to Linear Transformations (10) Determinants. Finish 5. Gentle intro to 6.1 (11) Complex Numbers. Start of Eigenvalues/Eigenvectors (6). (12) Continue with 6, matrix diagonalization, Change of basis for LT, start Spectral Thm. (13) Finish Spectral Theorem. The Singular Value Decomposition. Matrix norms (brief intro). (14) 22.12.2023: Entire lecture of “CS lenses”, a sort of technical “Ask Me Anything” session. If you have any particular topic you would like to me cover, suggest it on the forum! APPENDIX C. CS LENS LECTURES (NOT PART OF CHAPTER 7) • Kernel Methods: https://ti.inf.ethz.ch/ew/courses/LA23/slides/CS_ Lens_kernels.pdf • Graphs, Networks, and Linear Algebra https://ti.inf.ethz.ch/ew/courses/ LA23/slides/CS_Lens_Graphs1.pdf. I also cover this in some classes I have taught, you can see manuscripts here: [BSS, BM23]. • Google’s PageRank algorithm. I also cover this in a class I have taught, see [BSS]. • See Chapter 7 for more. 66 APPENDIX D. A “SIMPLE PROOF” OF FUNDAMENTAL THEOREM OF ALGEBRA In this appendix I present a brief sketch of a (relatively) simple proof of the Fundamental Theorem of Algebra I learned from Alessio Figalli. Let P(z) be a polynomial of degree n. Without loss of generality we can assume it is monic P(z) = zn + αn−1zn−1 + · · · + α0. Suppose P(z) has no zeros/roots. There is a r ∈ R large enough such that the infimum of |P(z)| inside the close disc Dr of radius r centered at zero is smaller than that outside the disc Dr (because far from the origin the term zn dominates and forces |P(z)| to be large outside of Dr. Since Dr is a compact set and |P(z)| is continuous it needs to attain its minimum 45 at a point z0 ∈ Dr. Note that P(z0) ̸= 0. Write Q(z) = P(z−z0), it is also a polynomial of degree n, Q(z) = β0 + β1z + β2z2 + · · · + zn. Notice that β0 = P(z0). let k be the first coefficient of Q(z) (not including β0) that is nonzero (meaning that βk ̸= 0 but βi = 0 for all 0 < i < k. Then Q(z) = β0 + βkzk + βk+1zk + · · · . Take ε > 0 arbitrarily small and consider Q ( ε ( − β0 βk ) 1 k ) . It is not difficult to see that for ε small enough the higher order terms are negligible and the term β0 + βkzk has smaller modulus and so one can pick ε such that ∣ ∣ ∣ ∣Q ( ε ( − β0 βk ) 1 k )∣ ∣ ∣ ∣ < |Q(0)| which is a contradiction with the fact that |P(z0)| was minimum. References 46 [Ban16] Afonso S. Bandeira. Ten lectures and forty-two open problems in the mathematics of data science. Available online at: https://people.math.ethz.ch/˜abandeira/ TenLecturesFortyTwoProblems.pdf. See also https://ocw.mit.edu/courses/ 18-s096-topics-in-mathematics-of-data-science-fall-2015/, 2016. [BM23] Afonso S. Bandeira and Antoine Maillard. Mathematics of signals, networks, and learning. Available online at: https://anmaillard.github.io/teaching/msnl_spring_2023.pdf. Videos from an earlier version of the course available at https://youtube.com/playlist?list= PLiud-28tsatL0MbfJFQQS7MYkrFrujCYp, 2023. [BSS] A. S. Bandeira, A. Singer, and T. Strohmer. Mathematics of data science. Book draft available at https: //people.math.ethz.ch/˜abandeira/BandeiraSingerStrohmer-MDS-draft. pdf. Videos available at: https://www.youtube.com/playlist?list= PLiud-28tsatIKUitdoH3EEUZL-9i516IL. [Str23] Gilbert Strang. Introduction to Linear Algebra. (Table of contents available at https://math.mit. edu/˜gs/linearalgebra/ila6/indexila6.html). Wellesley - Cambridge Press., sixth edi- tion, 2023. 45This part needs some extra analysis/topology background: the fact that continuous functions on a compact (think closed and bounded) set needs to attain a minimum. You will learn about this in Analysis. Perhaps not surprisingly, the “other proof” of Corollary 6.3.8 that does not involve complex numbers, also needs this fact. 46In some PDF viewers the ∼ in the urls above does not show as the correct character, if the link appears broken delete the ˜ and write a new ∼ on the url.","libVersion":"0.3.2","langs":""}