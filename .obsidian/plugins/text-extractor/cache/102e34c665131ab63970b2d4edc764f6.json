{"path":"sem2/DDCA/VRL/slides/DDCA-L25b-prefetching.pdf","text":"Digital Design & Computer Arch. Lecture 25b: Prefetching Frank K. Gürkaynak Rahul Bera Prof. Onur Mutlu ETH Zürich Spring 2024 30 May 2024 The (Memory) Latency Problem 1 10 100 1999 2003 2006 2008 2011 2013 2014 2015 2016 2017DRAM Improvement (log)Capacity Bandwidth Latency Recall: Memory Latency Lags Behind 128x 20x 1.3x Memory latency remains almost constant DRAM Latency Is Critical for Performance In-Memory Data Analytics [Clapp+ (Intel), IISWC’15; Awan+, BDCloud’15] Datacenter Workloads [Kanev+ (Google), ISCA’15] In-memory Databases [Mao+, EuroSys’12; Clapp+ (Intel), IISWC’15] Graph/Tree Processing [Xu+, IISWC’12; Umuroglu+, FPL’15] DRAM Latency Is Critical for Performance In-Memory Data Analytics [Clapp+ (Intel), IISWC’15; Awan+, BDCloud’15] Datacenter Workloads [Kanev+ (Google), ISCA’15] In-memory Databases [Mao+, EuroSys’12; Clapp+ (Intel), IISWC’15] Graph/Tree Processing [Xu+, IISWC’12; Umuroglu+, FPL’15] Long memory latency → performance bottleneck New DRAM Types Increase Latency! n Saugata Ghose, Tianshi Li, Nastaran Hajinazar, Damla Senol Cali, and Onur Mutlu, \"Demystifying Workload–DRAM Interactions: An Experimental Study\" Proceedings of the ACM International Conference on Measurement and Modeling of Computer Systems (SIGMETRICS), Phoenix, AZ, USA, June 2019. [Preliminary arXiv Version] [Abstract] [Slides (pptx) (pdf)] [MemBen Benchmark Suite] [Source Code for GPGPUSim-Ramulator] [Source Code for Ramulator modeling Hybrid Memory Cube (HMC)] 6 Latency Tolerance, Latency Hiding, and Latency Reducing Techniques Latency Tolerance, Hiding, Reducing n Tolerate (or, amortize) latency seen by the processor q Processor-centric approach q Out-of-order Execution, Multithreading n Hide latency seen by the processor q Processor-centric approach q Caching, Prefetching n Fundamentally reduce latency as much as possible q Data-centric approach q See Lecture 9: Memory Latency q https://www.youtube.com/live/HToR2HGyCeU?si=UJaYFiyRpIeJwp8J 89 Conventional Latency Tolerance Techniques n Out-of-order execution [initially by Tomasulo, 1967] q Tolerates cache misses that cannot be prefetched q Requires extensive hardware resources for tolerating long latencies n Multithreading [initially in CDC 6600, 1964] q Works well if there are multiple threads q Improving single thread performance using multithreading hardware is an ongoing research effort n Caching [initially by Wilkes, 1965] q Widely used, simple, effective, but inefficient, passive q Not all applications/phases exhibit temporal or spatial locality n Prefetching [initially in IBM 360/91, 1967] q Works well for regular memory access patterns q Prefetching irregular access patterns is difficult, inaccurate, and hardware- intensive Lectures on Latency Tolerance & Hiding n Out-of-order Execution, Runahead Execution q http://www.youtube.com/watch?v=EdYAKfx9JEA q http://www.youtube.com/watch?v=WExCvQAuTxo q http://www.youtube.com/watch?v=Kj3relihGF4 n Multithreading q http://www.youtube.com/watch?v=bu5dxKTvQVs q https://www.youtube.com/watch?v=iqi9wFqFiNU q https://www.youtube.com/watch?v=e8lfl6MbILg q https://www.youtube.com/watch?v=7vkDpZ1-hHM n Caching q http://www.youtube.com/watch?v=mZ7CPJKzwfM q http://www.youtube.com/watch?v=TsxQPLMXT60 q http://www.youtube.com/watch?v=OUk96_Bz708 q And more here: https://safari.ethz.ch/architecture/fall2018/doku.php?id=schedule n Prefetching q Today q Also: http://www.youtube.com/watch?v=CLi04cG9aQ8 10 Prefetching Prefetching n Idea: Fetch the data before it is needed (i.e., pre-fetch) by the program n Why? q Memory latency is high. If we can prefetch accurately and early enough, we can reduce/eliminate that latency. q Can eliminate compulsory cache misses q Can it eliminate all cache misses? Capacity, conflict? Coherence? n Involves predicting which address will be needed in the future q Works if programs have predictable miss address patterns 12 Prefetching and Correctness n Does a misprediction in prefetching affect correctness? n No, prefetched data at a “mispredicted” address is simply not used n There is no need for state recovery q In contrast to branch misprediction or value misprediction 13 Basics n In modern systems, prefetching is usually done at cache block granularity n Prefetching is a technique that can reduce both q Miss rate q Miss latency n Prefetching can be done by q Hardware q Compiler q Programmer q System 14 How a HW Prefetcher Fits in the Memory System 15Mutlu+, “Using the First-Level Caches as Filters to Reduce the Pollution Caused by Speculative Memory References”, IJPP 2005. Prefetching: The Four Questions n What q What addresses to prefetch (i.e., address prediction algorithm) n When q When to initiate a prefetch request (early, late, on time) n Where q Where to place the prefetched data (caches, separate buffer) q Where to place the prefetcher (which level in memory hierarchy) n How q How does the prefetcher operate and who operates it (software, hardware, execution/thread-based, cooperative, hybrid) 16 Challenge in Prefetching: What n What addresses to prefetch q Prefetching useless data wastes resources n Memory bandwidth n Cache or prefetch buffer space n Energy consumption n These could all be utilized by demand requests or more accurate prefetch requests q Accurate prediction of addresses to prefetch is important n Prefetch accuracy = used prefetches / sent prefetches n How do we know what to prefetch? q Predict based on past access patterns q Use the compiler’s/programmer’s knowledge of data structures n Prefetching algorithm determines what to prefetch 17 Some Predictable Address Access Patterns? n Cache Block Addresses A, A+1, A+2, A+3, A+4, … B, B+42, B+84, B+126, … C, C+2, C+5, C+9, C+11, C+14, C+18, C+20, C+23, C+27, … X, Y, T, Q, R, S, X, Y, T, A, B, C, D, E, X, Y, T, F, G, H, X, Y, T, … A+1, A+67, A+18, A+7, A+99, Z+1, Z+67, Z+18, Z+7, Z+99, P+1, P+67, P+18, P+7, P+99, … 18 Challenges in Prefetching: When n When to initiate a prefetch request q Prefetching too early n Prefetched data might not be used before it is evicted from storage q Prefetching too late n Might not hide the whole memory latency n When a data item is prefetched affects the timeliness of the prefetcher n Prefetcher can be made more timely by q Making it more aggressive: try to stay far ahead of the processor’s demand access stream (hardware) q Moving the prefetch instructions earlier in the code (software) 19 Challenges in Prefetching: Where (I) n Where to place the prefetched data q In cache + Simple design, no need for separate buffers -- Can evict useful demand data à cache pollution q In a separate prefetch buffer + Demand data protected from prefetches à no cache pollution -- More complex memory system design - Where to place the prefetch buffer - When to access the prefetch buffer (parallel vs. serial with cache) - When to move the data from the prefetch buffer to cache - How to size the prefetch buffer - Keeping the prefetch buffer coherent n Many modern systems place prefetched data into the cache q Many Intel, AMD, IBM systems and more … 20 Challenges in Prefetching: Where (II) n Which level of cache to prefetch into? q Memory to L4/L3/L2, memory to L1. Advantages/disadvantages? q L3 to L2? L2 to L1? (a separate prefetcher between levels) n Where to place the prefetched data in the cache? q Do we treat prefetched blocks the same as demand-fetched blocks? q Prefetched blocks are not known to be needed n With LRU, a demand block is placed into the MRU position n Do we skew the replacement policy such that it favors the demand-fetched blocks? q E.g., place all prefetches into the LRU position in a way? 21 Challenges in Prefetching: Where (III) n Where to place the hardware prefetcher in the memory hierarchy? q In other words, what access patterns does the prefetcher see? q L1 hits and misses q L1 misses only q L2 misses only n Seeing a more complete access pattern: + Potentially better accuracy and coverage in prefetching -- Prefetcher needs to examine more requests (bandwidth intensive, more ports into the prefetcher?) 22 A Modern Memory Hierarchy 23 Register File 32 words, sub-nsec L1 cache ~10s of KB, ~nsec L2 cache 100s of KB ~ few MB, many nsec L3 cache, many MBs, even more nsec Main memory (DRAM), Many GBs, ~100 nsec Swap Disk ~100 GB or few TB, ~10s of usec-msec manual/compiler register spilling automatic demand paging automatic HW cache management Memory Abstraction Hybrid Main Memory Extends the Hierarchy Meza+, “Enabling Efficient and Scalable Hybrid Memories,” IEEE Comp. Arch. Letters, 2012. Yoon+, “Row Buffer Locality Aware Caching Policies for Hybrid Memories,” ICCD 2012 Best Paper Award. CPU DRAM Ctrl Fast, durable Small, leaky, volatile, high-cost Large, non-volatile, low-cost Slow, wears out, high active energy PCM Ctrl DRAM Phase Change Memory (or Tech. X) Hardware/software manage data allocation & movement to achieve the best of multiple technologies Remote Memory in Large Servers n Memory hierarchy extends beyond a single node n This enables even higher memory capacity q Needed to support modern data-intensive workloads 25Calciu+, “Rethinking Software Runtimes for Disaggregated Memory”, ASPLOS 2021. Local memory (and hierarchy) Remote memory Compute node (Local) Memory node (Remote) Low-latency network Remote Memory in Large Servers Irina Calciu, M. Talha Imran, Ivan Puddu, Sanidhya Kashyap, Hasan Al Maruf, Onur Mutlu, and Aasheesh Kolli, \"Rethinking Software Runtimes for Disaggregated Memory\" Proceedings of the 26th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS), Virtual, March-April 2021. [2-page Extended Abstract] [Source Code (Officially Artifact Evaluated with All Badges)] Officially artifact evaluated as available, reusable and reproducible. 26 Challenges in Prefetching: How n Software prefetching q ISA provides prefetch instructions q Programmer or compiler inserts prefetch instructions into code q Usually works well only for “regular access patterns” n Hardware prefetching q Specialized hardware monitors memory accesses q Memorizes, finds, learns address strides/patterns/correlations q Generates prefetch addresses automatically n Execution-based prefetching q A “thread” is executed to prefetch data for the main program q Can be generated by either software/programmer or hardware 27 Challenges in Prefetching: How n Software prefetching q ISA provides prefetch instructions q Programmer or compiler inserts prefetch instructions into code q Usually works well only for “regular access patterns” n Hardware prefetching q Specialized hardware monitors memory accesses q Memorizes, finds, learns address strides/patterns/correlations q Generates prefetch addresses automatically n Execution-based prefetching q A “thread” is executed to prefetch data for the main program q Can be generated by either software/programmer or hardware 28 Software Prefetching (I) n Idea: Compiler/programmer places prefetch instructions into appropriate places in code n Mowry et al., “Design and Evaluation of a Compiler Algorithm for Prefetching,” ASPLOS 1992. n Prefetch instructions prefetch data into caches n Compiler or programmer can insert such instructions into the program 29 X86 PREFETCH Instruction 30 microarchitecture dependent specification different instructions for different cache levels Software Prefetching (II) n Can work for very regular array-based access patterns. Issues: -- Prefetch instructions take up processing/execution bandwidth q How early to prefetch? Determining this is difficult -- Prefetch distance depends on hardware implementation (memory latency, cache size, time between loop iterations) à portability? -- Going too far back in code reduces accuracy (branches in between) q Need “special” prefetch instructions in ISA? n Alpha load into register 31 treated as prefetch (r31==0) n PowerPC dcbt (data cache block touch) instruction -- Not easy to do for pointer-based data structures 31 for (i=0; i<N; i++) { __prefetch(a[i+8]); __prefetch(b[i+8]); sum += a[i]*b[i]; } while (p) { __prefetch(pànext); work(pàdata); p = pànext; } while (p) { __prefetch(pànextànextànext); work(pàdata); p = pànext; } Which one is better? Software Prefetching (III) n Where should a compiler insert prefetches? q Prefetch for every load access? n Too bandwidth intensive (both memory and execution bandwidth) q Profile the code and determine loads that are likely to miss n What if profile input set is not representative? q How far ahead before the miss should the prefetch be inserted? n Profile and determine probability of use for various prefetch distances from the miss q What if profile input set is not representative? q Usually need to insert a prefetch far in advance to cover 100s of cycles of main memory latency à reduced accuracy 32 Challenges in Prefetching: How n Software prefetching q ISA provides prefetch instructions q Programmer or compiler inserts prefetch instructions into code q Usually works well only for “regular access patterns” n Hardware prefetching q Specialized hardware monitors memory accesses q Memorizes, finds, learns address strides/patterns/correlations q Generates prefetch addresses automatically n Execution-based prefetching q A “thread” is executed to prefetch data for the main program q Can be generated by either software/programmer or hardware 33 Hardware Prefetching n Idea: Specialized hardware observes load/store access patterns and prefetches data based on past access behavior n Tradeoffs: + Can be tuned to system implementation + Does not waste instruction execution bandwidth -- More hardware complexity to detect patterns - Software can be more efficient in some cases 34 Next-Line Prefetchers n Simplest form of hardware prefetching: always prefetch next N cache lines after a demand access (or a demand miss) q Next-line prefetcher (or next sequential prefetcher) q Tradeoffs: + Simple to implement. No need for sophisticated pattern detection + Works well for sequential/streaming access patterns (instructions?) -- Can waste bandwidth with irregular patterns -- And, even with regular patterns: - What is the prefetch accuracy if access stride = 2 and N = 1? - What if the program is traversing memory from higher to lower addresses? - Also prefetch “previous” N cache lines? 35 Stride Prefetchers n Consider the following strided memory access pattern: q A, A+N, A+2N, A+3N, A+4N… q Stride = N n Idea: Record the stride between consecutive memory accesses; if stable, use it to predict next M memory accesses n Two types q Stride determined on a per-instruction basis q Stride determined on a per-memory-region basis 36 Instruction Based Stride Prefetching n Each load/store instruction can lead to a memory access pattern with a different stride q Can only detect strides caused by each instruction n Timeliness of prefetches can be an issue q Initiating the prefetch when the load is fetched the next time can be too late q Potential solution: Look ahead in the instruction stream 37 Load Inst. Last Address Last Confidence PC (tag) Referenced Stride ……. ……. …… Load Inst PC Memory-Region Based Based Stride Prefetching n Can detect strided memory access patterns that appear due to multiple instructions q A, A+N, A+2N, A+3N, A+4N … where each access could be due to a different instruction n Stream prefetching (stream buffers) is a special case of memory-region based stride prefetching where N = 1 38 Address tag Stride Control/Confidence ……. …… Cache Block Address Tradeoffs in Stream/Stride Prefetching n Instruction based stride prefetching vs. memory region based based stride prefetching n The latter can exploit strides that occur due to the interaction of multiple instructions n The latter can more easily get further ahead of the processor access stream q No need for lookahead PC n The latter is more hardware intensive q Usually there are more data addresses to monitor than instructions 39 Instruction-Based Stride Prefetching Baer & Chen, “An effective on-chip preloading scheme to reduce data access penalty,” SC 1991. Instruction-Based Stride Prefetching Doweck, “Inside Intel® Core™ Microarchitecture and Smart Memory Access,” Intel White Paper, 2006. 41https://www.all-electronics.de/wp-content/uploads/migrated/document/196371/413ei0507-intel-sma.pdf Memory-Region-Based Stride Prefetching 42Jouppi, “Improving direct-mapped cache performance by the addition of a small fully-associative cache and prefetch buffers,” ISCA 1990. What About More Complex Access Patterns? n Simple regular patterns q Stride, stream prefetchers do well n Complex regular patterns q E.g., multiple regular strides q +1, +2, +3, +1, +2, +3, +1, +2, +3, … n Irregular patterns q Linked data structure traversals q Indirect array accesses q Random accesses q Multiple data structures accessed concurrently q … 43 Multi-Stride Detection in Modern Prefetchers 44 GemsFDTD Complex but predictable set of strides Path Confidence Based Lookahead Prefetching n Key Idea: q Given a history/signature/pattern of strides, learn and predict what stride might come next n {7,-6,12} à 6, {-6,12,6} à -5, … q Bootstrap prediction to generate new predictions, until the cascaded path confidence drops below a threshold 45 History of Strides Prediction Prediction Confidence Path Confidence Pass 1 {7,-6,12} 6 85% 85% Bootstrap Pass 2 {-6,12,6} -5 70% 85%*70%=60% Bootstrap Pass 3 {12,6,-5} -6 82% 60%*82%=49% STOP How to Prefetch More Irregular Access Patterns? n Regular patterns: Stride, stream prefetchers do well n More irregular access patterns q Indirect array accesses q Linked data structures q Multiple regular strides (1,2,3,1,2,3,1,2,3,…) q Random patterns? q Generalized prefetcher for all patterns? n Correlation based prefetchers n Content-directed prefetchers n Precomputation or execution-based prefetchers 46 Address Correlation Based Prefetching (I) n Consider the following history of cache block addresses A, B, C, D, C, E, A, C, F, F, E, A, A, B, C, D, E, A, B, C, D, C n After referencing a particular address (say A or E), some addresses are more likely to be referenced next 47 A B C D E F 1.0 .33 .5 .2 1.0.6 .2 .67 .6 .5 .2 Markov Model Address Correlation Based Prefetching (II) n Idea: Record the likely-next addresses (B, C, D) after seeing an address A q Next time A is accessed, prefetch B, C, D q A is said to be correlated with B, C, D n Prefetch up to N next addresses to increase coverage n Prefetch accuracy can be improved by using multiple addresses as key for the next address: (A, B) à (C) (A,B) correlated with C n Joseph and Grunwald, “Prefetching using Markov Predictors,” ISCA 1997. q Also called “Markov prefetchers” 48 Cache Block Addr Prefetch Confidence …. Prefetch Confidence (tag) Candidate 1 …. Candidate N ……. ……. …… .… ……. …… …. Cache Block Addr Address Correlation Based Prefetching (III) n Advantages: q Can cover arbitrary access patterns n Linked data structures n Streaming patterns (though not so efficiently!) n Disadvantages: q Correlation table needs to be very large for high coverage n Recording every miss address and its subsequent miss addresses is infeasible q Can have low timeliness: Lookahead is limited since a prefetch for the next access/miss is initiated right after previous q Can consume a lot of memory bandwidth n Especially when Markov model probabilities (correlations) are low q Cannot reduce compulsory misses 49 Prefetcher Performance 50 Prefetcher Performance (I) n Accuracy (used prefetches / sent prefetches) n Coverage (prefetched misses / all misses) n Timeliness (on-time prefetches / used prefetches) n Bandwidth consumption q Memory bandwidth consumed with prefetcher / without prefetcher q Good news: Can utilize idle bus bandwidth (if available) n Cache pollution q Extra demand misses due to prefetch placement in cache q More difficult to quantify but affects performance 51 Prefetcher Performance (II) n Prefetcher aggressiveness affects all performance metrics n Aggressiveness dependent on prefetcher type n For most hardware prefetchers: q Prefetch distance: how far ahead of the demand stream q Prefetch degree: how many prefetches per demand access 52 Predicted StreamPredicted Stream X Access Stream Pmax Prefetch Distance Pmax Very Conservative Pmax Middle of the Road Pmax Very Aggressive P Prefetch Degree X+1 1 2 3 Prefetcher Performance (III) n How do these metrics interact? n Very Aggressive Prefetcher (large prefetch distance & degree) q Well ahead of the load access stream q Hides memory access latency better q More speculative + Higher coverage, better timeliness -- Likely lower accuracy, higher bandwidth and pollution n Very Conservative Prefetcher (small prefetch distance & degree) q Closer to the load access stream q Might not hide memory access latency completely q Reduces potential for cache pollution and bandwidth contention + Likely higher accuracy, lower bandwidth, less polluting -- Likely lower coverage and less timely 53 Prefetcher Performance (IV) 54 -100% -50% 0% 50% 100% 150% 200% 250% 300% 350% 400% 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1Percentage IPC change over No Prefetching Pref etcher Accuracy Prefetcher Performance (V) n Srinath et al., “Feedback Directed Prefetching: Improving the Performance and Bandwidth-Efficiency of Hardware Prefetchers“, HPCA 2007. 55 0.0 1.0 2.0 3.0 4.0 5.0 bz i p2 gap mc f par s er vo r t e x vp r am m p appl u ar t equak e fa c e r e c gal gel me s a mg r i d si xt r a ck sw i m wu p wi s e gm eanInstructions per Cycle No Prefetching Very Conservative Middle-of-the-Road Very Aggressive â48% â 29% Feedback-Directed Prefetcher Throttling (I) n Idea: q Dynamically monitor prefetcher performance metrics q Throttle the prefetcher aggressiveness up/down based on past performance q Change the location prefetches are inserted in cache based on past performance 56 High Accuracy Not-Late Polluting Decrease Late Increase Med Accuracy Not-Poll Late Increase Polluting Decrease Low Accuracy Not-Poll Not-Late No Change Decrease Feedback-Directed Prefetcher Throttling (II) n Srinath et al., “Feedback Directed Prefetching: Improving the Performance and Bandwidth-Efficiency of Hardware Prefetchers“, HPCA 2007. n Srinath et al., “Feedback Directed Prefetching: Improving the Performance and Bandwidth-Efficiency of Hardware Prefetchers“, HPCA 2007. 57 á11% á13% Feedback-Directed Prefetcher Throttling (III) n BPKI - Memory Bus Accesses per 1000 retired Instructions q Includes effects of L2 demand misses as well as pollution induced misses and prefetches n A measure of bus bandwidth usage 58 No. Pref. Very Cons Mid Very Aggr FDP IPC 0.85 1.21 1.47 1.57 1.67 BPKI 8.56 9.34 10.60 13.38 10.88 Feedback Directed Prefetching n Santhosh Srinath, Onur Mutlu, Hyesoon Kim, and Yale N. Patt, \"Feedback Directed Prefetching: Improving the Performance and Bandwidth-Efficiency of Hardware Prefetchers\" Proceedings of the 13th International Symposium on High-Performance Computer Architecture (HPCA), pages 63-74, Phoenix, AZ, February 2007. Slides (ppt) One of the five papers nominated for the Best Paper Award by the Program Committee. 59 Coordinated Prefetching in Multi-Core Systems n Eiman Ebrahimi, Onur Mutlu, Chang Joo Lee, and Yale N. Patt, \"Coordinated Control of Multiple Prefetchers in Multi-Core Systems\" Proceedings of the 42nd International Symposium on Microarchitecture (MICRO), pages 316-326, New York, NY, December 2009. Slides (ppt) 60 Prefetching-Aware Shared Resource Management n Eiman Ebrahimi, Chang Joo Lee, Onur Mutlu, and Yale N. Patt, \"Prefetch-Aware Shared Resource Management for Multi-Core Systems\" Proceedings of the 38th International Symposium on Computer Architecture (ISCA), San Jose, CA, June 2011. Slides (pptx) 61 Multi-Core Issues in Prefetching 62 Real Systems: Prefetching in Multi-Core n Prefetching shared data q Coherence misses n Prefetching efficiency is a lot more important q Bus bandwidth more precious q Cache space more valuable n One cores’ prefetches interfere with other cores’ requests q Cache conflicts at multiple levels q Bus contention at multiple levels q DRAM bank, rank, channel, row buffer contention q … 63 Bandwidth-Efficient Hybrid Prefetchers n Eiman Ebrahimi, Onur Mutlu, and Yale N. Patt, \"Techniques for Bandwidth-Efficient Prefetching of Linked Data Structures in Hybrid Prefetching Systems\" Proceedings of the 15th International Symposium on High-Performance Computer Architecture (HPCA), pages 7-17, Raleigh, NC, February 2009. Slides (ppt) Best paper session. One of the three papers nominated for the Best Paper Award by the Program Committee. 64 Coordinated Control of Prefetchers n Eiman Ebrahimi, Onur Mutlu, Chang Joo Lee, and Yale N. Patt, \"Coordinated Control of Multiple Prefetchers in Multi-Core Systems\" Proceedings of the 42nd International Symposium on Microarchitecture (MICRO), pages 316-326, New York, NY, December 2009. Slides (ppt) 65 Prefetching-Aware Shared Resource Management n Eiman Ebrahimi, Chang Joo Lee, Onur Mutlu, and Yale N. Patt, \"Prefetch-Aware Shared Resource Management for Multi-Core Systems\" Proceedings of the 38th International Symposium on Computer Architecture (ISCA), San Jose, CA, June 2011. Slides (pptx) 66 Prefetching-Aware DRAM Control (I) n Chang Joo Lee, Onur Mutlu, Veynu Narasiman, and Yale N. Patt, \"Prefetch-Aware DRAM Controllers\" Proceedings of the 41st International Symposium on Microarchitecture (MICRO), pages 200-209, Lake Como, Italy, November 2008. Slides (ppt) 67 Prefetching-Aware DRAM Control (II) n Chang Joo Lee, Veynu Narasiman, Onur Mutlu, and Yale N. Patt, \"Improving Memory Bank-Level Parallelism in the Presence of Prefetching\" Proceedings of the 42nd International Symposium on Microarchitecture (MICRO), pages 327-336, New York, NY, December 2009. Slides (ppt) 68 Prefetching-Aware Cache Management n Vivek Seshadri, Samihan Yedkar, Hongyi Xin, Onur Mutlu, Phillip P. Gibbons, Michael A. Kozuch, and Todd C. Mowry, \"Mitigating Prefetcher-Caused Pollution using Informed Caching Policies for Prefetched Blocks\" ACM Transactions on Architecture and Code Optimization (TACO), Vol. 11, No. 4, January 2015. Presented at the 10th HiPEAC Conference, Amsterdam, Netherlands, January 2015. [Slides (pptx) (pdf)] [Source Code] 69 Advanced Prefetching: Self-Optimizing Prefetcher Self-Optimizing Memory Prefetchers 71 Rahul Bera, Konstantinos Kanellopoulos, Anant Nori, Taha Shahroodi, Sreenivas Subramoney, and Onur Mutlu, \"Pythia: A Customizable Hardware Prefetching Framework Using Online Reinforcement Learning\" Proceedings of the 54th International Symposium on Microarchitecture (MICRO), Virtual, October 2021. [Slides (pptx) (pdf)] [Short Talk Slides (pptx) (pdf)] [Lightning Talk Slides (pptx) (pdf)] [Talk Video (20 minutes)] [Lightning Talk Video (1.5 minutes)] [Pythia Source Code (Officially Artifact Evaluated with All Badges)] [arXiv version] Officially artifact evaluated as available, reusable and reproducible. https://arxiv.org/pdf/2109.12021.pdf Rahul Bera, Konstantinos Kanellopoulos, Anant V. Nori, Taha Shahroodi, Sreenivas Subramoney, Onur Mutlu Pythia A Customizable Hardware Prefetching Framework Using Online Reinforcement Learning https://github.com/CMU-SAFARI/Pythia 73 Basics of Reinforcement Learning (RL) • Algorithmic approach to learn to take an action in a given situation to maximize a numerical reward • Agent stores Q-values for every state-action pair - Expected return for taking an action in a state - Given a state, selects action that provides highest Q-value Agent Environment State (St)State (St) Action (At)Action (At)Reward (Rt+1)Reward (Rt+1) 74 Brief Overview of Pythia Pythia formulates prefetching as a reinforcement learning problem Agent Environment State (St)State (St) Action (At)Action (At)Reward (Rt+1)Reward (Rt+1) Prefetcher Processor & Memory Subsystem Reward Prefetch from address A+offset (O) Features of memory request to address A (e.g., PC) 75 What is State? • k-dimensional vector of features • Feature = control-flow + data-flow • Control-flow examples - PC - Branch PC - Last-3 PCs, … • Data-flow examples - Cacheline address - Physical page number - Delta between two cacheline addresses - Last 4 deltas, … 76 What is Action? Given a demand access to address A the action is to select prefetch offset “O” • Action-space: 127 actions in the range [-63, +63] - For a machine with 4KB page and 64B cacheline • A zero offset means no prefetch is generated • We further prune action-space by design-space exploration 77 What is Reward? • Defines the objective of Pythia • Encapsulates two metrics: - Prefetch usefulness (e.g., accurate, late, out-of-page, …) - System-level feedback (e.g., mem. b/w usage, cache pollution, energy, …) • We demonstrate Pythia with memory bandwidth usage as the system-level feedback 78 What is Reward? • Five distinct reward levels - Accurate and timely (RAT) - Accurate but late (RAL) - Loss of coverage (RCL) - Inaccurate • With low memory b/w usage (RIN-L) • With high memory b/w usage (RIN-H) - No-prefetch • With low memory b/w usage (RNP-L) • With high memory b/w usage(RNP-H) • Values are set at design time via automatic design- space exploration - Can be customized further in silicon for higher performance 79 Steering Pythia’s Objective via Reward Values • Example reward configuration for - Generating accurate prefetches - Taking bandwidth-aware prefetch decisions +20+12-2-4-8-14 RATRALRNP-HRNP-LRIN-LRIN-H AT = Accurate & timely; AL = Accurate & late; NP = No-prefetching; IN = Inaccurate; H = High mem. b/w; L = Low mem. b/w Highly prefers to generate accurate prefetches Prefers not to prefetch if memory bandwidth usage is low Strongly prefers not to prefetch if memory bandwidth usage is high 80 Steering Pythia’s Objective via Reward Values • Customizing reward values to make Pythia conservative towards prefetching +20+12+4+2-20-22 RATRALRNP-HRNP-LRIN-LRIN-H AT = Accurate & timely; AL = Accurate & late; NP = No-prefetching; IN = Inaccurate; H = High mem. b/w; L = Low mem. b/w Highly prefers to generate accurate prefetches Otherwise prefers not to prefetch 81 Steering Pythia’s Objective via Reward Values • Customizing reward values to make Pythia conservative towards prefetching +20+12+4+2-20-22 RATRALRNP-HRNP-LRIN-LRIN-H AT = Accurate & timely; AL = Accurate & late; NP = No-prefetching; IN = Inaccurate; H = High mem. b/w; L = Low mem. b/w Highly prefers to generate accurate prefetches Otherwise prefers not to prefetch Server-class processors Bandwidth-sensitive workloads 82 Steering Pythia’s Objective via Reward Values • Customizing reward values to make Pythia conservative towards prefetching +20+12+4+2-20-22 RATRALRNP-HRNP-LRIN-LRIN-H AT = Accurate & timely; AL = Accurate & late; NP = No-prefetching; IN = Inaccurate; H = High mem. b/w; L = Low mem. b/w Highly prefers to generate accurate prefetches Otherwise prefers not to prefetch Pythia’s objective can be easily customized in silicon without changing the hardware 83 Basic Pythia Configuration • Derived from automatic design-space exploration • State: 2 features - PC+Delta - Sequence of last-4 deltas • Actions: 16 prefetch offsets - Ranging between -6 to +32. Including 0. • Rewards: - RAT = +20; RAL = +12; RNP-H=-2; RNP-L=-4; - RIN-H=-14; RIN-L=-8; RCL=-12 84 More Detailed Pythia Overview • Q-Value Store: Records Q-values for all state-action pairs • Evaluation Queue: A FIFO queue of recently-taken actions Evaluation Queue (EQ) Demand Request 1 Assign reward to corresponding EQ entry Look up QVStoreState Vector Q-Value Store (QVStore) 2 3 5 Insert prefetch action & State-Action pair in EQ 6 Prefetch Fill A1 A2 A3 Memory Hierarchy Generate prefetch Evict EQ entry and update QVStore 4 Find the Action with max Q-Value 7 S1 S2 S3 S4 Set filled bit Max 85 1.1 1.15 1.2 1.25 1.3 1.35 0 2 4 6 8 10 12Geomean speedupover no prefetching Number of cores Performance with Varying Core Count Bingo MLOP SPP Pythia 3.4% 7.7% 86 1.1 1.15 1.2 1.25 1.3 1.35 0 2 4 6 8 10 12Geomean speedupover no prefetching Number of cores Performance with Varying Core Count Bingo MLOP SPP Pythia 3.4% 7.7% 1. Pythia consistently provides the highest performance in all core configurations 2. Pythia’s gain increases with core count 87 0.8 0.85 0.9 0.95 1 1.05 1.1 1.15 1.2 1.25 100 200 400 800 1600 3200 6400 1280 0Geomean speedup over no prefetching DRAM MTPS (in log scale) Performance with Varying DRAM Bandwidth ~Intel Xeon 6258R ~AMD EPYC Rome 7702P ~AMD Threadripper 3990x SPP Bingo MLOP Pythia Baseline 3% 17% 88 0.8 0.85 0.9 0.95 1 1.05 1.1 1.15 1.2 1.25 100 200 400 800 1600 3200 6400 1280 0Geomean speedup over no prefetching DRAM MTPS (in log scale) Performance with Varying DRAM Bandwidth ~Intel Xeon 6258R ~AMD EPYC Rome 7702P ~AMD Threadripper 3990x SPP Bingo MLOP Pythia Baseline 3% 17% Pythia outperforms prior best prefetchers for a wide range of DRAM bandwidth configurations 89 Pythia is Open Source https://github.com/CMU-SAFARI/Pythia • MICRO’21 artifact evaluated • Champsim source code + Chisel modeling code • All traces used for evaluation Pythia Talk Video https://www.youtube.com/watch?v=6UMFRW3VFPo&list=PL5Q2soXY2Zi--0LrXSQ9sST3N0k0bXp51&index=8 A Lot More in the Pythia Paper 91 Rahul Bera, Konstantinos Kanellopoulos, Anant Nori, Taha Shahroodi, Sreenivas Subramoney, and Onur Mutlu, \"Pythia: A Customizable Hardware Prefetching Framework Using Online Reinforcement Learning\" Proceedings of the 54th International Symposium on Microarchitecture (MICRO), Virtual, October 2021. [Slides (pptx) (pdf)] [Short Talk Slides (pptx) (pdf)] [Lightning Talk Slides (pptx) (pdf)] [Talk Video (20 minutes)] [Lightning Talk Video (1.5 minutes)] [Pythia Source Code (Officially Artifact Evaluated with All Badges)] [arXiv version] Officially artifact evaluated as available, reusable and reproducible. https://arxiv.org/pdf/2109.12021.pdf Rahul Bera, Konstantinos Kanellopoulos, Anant V. Nori, Taha Shahroodi, Sreenivas Subramoney, Onur Mutlu Pythia A Customizable Hardware Prefetching Framework Using Online Reinforcement Learning https://github.com/CMU-SAFARI/Pythia Recommended Book on Reinforcement Learning 93http://incompleteideas.net/book/RLbook2020.pdf Can We Do Better? n Prefetching and caching are latency hiding techniques n Pythia saves ~50% memory requests from going to main memory n What about the remaining 50%? Can we do something to hide their latency too? 94 Off-Chip Prediction Rahul Bera, Konstantinos Kanellopoulos, Shankar Balachandran, David Novo, Ataberk Olgun, Mohammad Sadrosadati, and Onur Mutlu, \"Hermes: Accelerating Long-Latency Load Requests via Perceptron-Based Off-Chip Load Prediction\" Proceedings of the 55th International Symposium on Microarchitecture (MICRO), Chicago, IL, USA, October 2022. [Slides (pptx) (pdf)] [Longer Lecture Slides (pptx) (pdf)] [Talk Video (12 minutes)] [Lecture Video (25 minutes)] [arXiv version] [Source Code (Officially Artifact Evaluated with All Badges)] Officially artifact evaluated as available, reusable and reproducible. Best paper award at MICRO 2022. 95https://arxiv.org/pdf/2209.00188.pdf Rahul Bera, Konstantinos Kanellopoulos, Shankar Balachandran, David Novo, Ataberk Olgun, Mohammad Sadrosadati, Onur Mutlu Accelerating Long-Latency Load Requests via Perceptron-Based Off-Chip Load Prediction https://github.com/CMU-SAFARI/Hermes 97 Key Observation 1 50% successfully prefetched # off-chip loads without any prefetcher 50% still go off-chip even with a state-of-the-art prefetcher 70% of the off-chip loads block the ROB Many loads still go off-chip 98 40% of the stalls can be eliminated by removing on-chip cache access latency from critical path Key Observation 2 On-chip cache access latency significantly contributes to off-chip load latency L1 L2 LLC Main Memory Saved cycles 50% still go off-chip L1 L2 LLC Main Memory 99 Caches are Getting Bigger and Slower… Hardavellas+, “Database Servers on Chip Multiprocessors: Limitations and Opportunities”, CIDR, 2007On-chip Cache Size (KB) 0 512 1024 1536 2048 2560 Sk y l a k e ( 2 0 1 5 ) Su n n y C o v e ( 2 0 1 9 ) Wi l l o w C o v e ( 2 0 2 0 ) Go l d e n C o v e P - c o r e ( 2 0 2 1 ) Ra p t o r L a k e P - c o r e ( 2 0 2 2 )L2 Size (KB) 11 12 13 14 15 16 17 Sk y l a k e ( 2 0 1 5 ) Su n n y C o v e ( 2 0 1 9 ) Wi l l o w C o v e ( 2 0 2 0 ) Go l d e n C o v e P - c o r e ( 2 0 2 1 ) Ra p t o r L a k e P - c o r e ( 2 0 2 2 )L2 Latency (processor cycles) 100 Improve processor performance by removing on-chip cache access latency from the critical path of off-chip loads Our Goal Predicts which load requests are likely to go off-chip Starts fetching data directly from main memory while concurrently accessing the cache hierarchy 102 Hermes Overview Core L1-D L2 LLC MC Off-Chip Main Memory L1 L2 LLC Main Memory Baseline Processor is stalled Latency tolerance limit of ROB 103 Hermes Overview Core L1-D L2 LLC MC Off-Chip Main Memory L1 L2 LLC Main Memory POPET L1 L2 LLC Main Memory Baseline Hermes Saved stall cycles Processor is stalled Latency tolerance limit of ROB Predict Issue a Hermes request Wait Train Perceptron-based off-chip load predictor 104 Key Contribution Hermes employs the first perceptron-based off-chip load predictor That predicts which loads are likely to go off-chip By learning from multiple program context information 105 Single-Core Performance Improvement 1 1.05 1.1 1.15 1.2 1.25 1.3 1.35 SPEC06 SPEC17 PARSEC Ligra CVP GEOMEANGeomean speedupover the No-prefetching system Hermes Pythia Pythia + Hermes Pythia + Ideal Hermes 11.5% 20.3% 5.4% Hermes alone provides nearly 50% performance benefits of Pythia with only 1/5th storage overhead Hermes on top of Pythia outperforms Pythia alone in every workload category Hermes provides nearly 90% performance benefit of Ideal Hermes that has an ideal off-chip load predictor 106 Summary Hermes employs the first perceptron-based off-chip load predictor High coverage (74%) High accuracy (77%) Low storage overhead (4KB/core) High performance improvement over best prior baseline (5.4%) High performance per bandwidth Rahul Bera, Konstantinos Kanellopoulos, Shankar Balachandran, David Novo, Ataberk Olgun, Mohammad Sadrosadati, Onur Mutlu Accelerating Long-Latency Load Requests via Perceptron-Based Off-Chip Load Prediction https://github.com/CMU-SAFARI/Hermes Challenges in Prefetching: How n Software prefetching q ISA provides prefetch instructions q Programmer or compiler inserts prefetch instructions into code q Usually works well only for “regular access patterns” n Hardware prefetching q Specialized hardware monitors memory accesses q Memorizes, finds, learns address strides/patterns/correlations q Generates prefetch addresses automatically n Execution-based prefetching q A “thread” is executed to prefetch data for the main program q Can be generated by either software/programmer or hardware 108 An Execution-Based Prefetcher: Runahead Execution Runahead Execution n A technique to obtain the memory-level parallelism benefits of a large instruction window n When the oldest instruction is a long-latency cache miss: q Checkpoint architectural state and enter runahead mode n In runahead mode: q Speculatively pre-execute instructions (with minimal stalling) q The purpose of pre-execution is to generate prefetches q Long-latency instructions are marked INV and dropped n Enables room for later instructions in the window n When the original miss returns: q Restore checkpoint, flush pipeline, resume normal execution n Mutlu et al., “Runahead Execution: An Alternative to Very Large Instruction Windows for Out-of-order Processors,” HPCA 2003. 110 Compute Compute Compute Load 1 Miss Miss 1 Stall Compute Load 2 Miss Miss 2 Stall Load 1 Hit Load 2 Hit Compute Load 1 Miss Runahead Load 2 Miss Load 2 Hit Miss 1 Miss 2 Compute Load 1 Hit Saved Cycles Perfect Caches: Small Window: Runahead: Runahead ExampleRunahead Execution Pros and Cons n Advantages: + Very accurate prefetches for data/instructions (all cache levels) + Follows the program path + Simple to implement: most of the hardware is already built in + No waste of hardware context: uses the main thread context for prefetching + No need to construct a special-purpose pre-execution thread for prefetching n Disadvantages/Limitations -- Extra executed instructions -- Limited by branch prediction accuracy -- Cannot prefetch dependent cache misses -- Effectiveness limited by available “memory-level parallelism” (MLP) -- Prefetch distance (how far ahead to prefetch) limited by memory latency n Implemented in Sun ROCK, IBM POWER6, NVIDIA Denver 112113 12% 35% 13% 15% 22% 12% 16% 52% 22% 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 1.1 1.2 1.3 S95 FP00 INT00 WEB MM PROD SERV WS AVGMicro-operations Per Cycle No prefetcher, no runahead Only prefetcher (baseline) Only runahead Prefetcher + runahead Performance of Runahead Execution 114 Runahead Execution vs. Large Windows 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 1.1 1.2 1.3 1.4 1.5 S95 FP00 INT00 WEB MM PROD SERV WS AVGMicro-operations Per Cycle 128-entry window (baseline) 128-entry window with Runahead 256-entry window 384-entry window 512-entry window 115 Runahead on In-order vs. Out-of-order 39% 50%28% 14% 20% 17% 73% 73% 15% 20% 47%15% 12% 22% 13% 16% 23% 10% 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 1.1 1.2 1.3 S95 FP00 INT00 WEB MM PROD SERV WS AVGMicro-operations Per Cycle in-order baseline in-order + runahead out-of-order baseline out-of-order + runahead Effect of Runahead in Sun ROCK n Shailender Chaudhry talk, Aug 2008. 116 Effective prefetching can both improve performance and reduce hardware cost More on Runahead Execution n Onur Mutlu, Jared Stark, Chris Wilkerson, and Yale N. Patt, \"Runahead Execution: An Alternative to Very Large Instruction Windows for Out-of-order Processors\" Proceedings of the 9th International Symposium on High-Performance Computer Architecture (HPCA), pages 129-140, Anaheim, CA, February 2003. Slides (pdf) One of the 15 computer arch. papers of 2003 selected as Top Picks by IEEE Micro. HPCA Test of Time Award (awarded in 2021). [Lecture Slides (pptx) (pdf)] [Lecture Video (1 hr 54 mins)] [Retrospective HPCA Test of Time Award Talk Slides (pptx) (pdf)] [Retrospective HPCA Test of Time Award Talk Video (14 minutes)] 117 More on Runahead in Sun ROCK 118Chaudhry+, “High-Performance Throughput Computing,” IEEE Micro 2005. More on Runahead in Sun ROCK 119Chaudhry+, “Simultaneous Speculative Threading,” ISCA 2009. Runahead Execution in IBM POWER6 120 Cain+, “Runahead Execution vs. Conventional Data Prefetching in the IBM POWER6 Microprocessor,” ISPASS 2010. Runahead Execution in IBM POWER6 121 Runahead Execution in NVIDIA Denver 122 Boggs+, “Denver: NVIDIA’s First 64-Bit ARM Processor,” IEEE Micro 2015. Runahead Execution in NVIDIA Denver 123 Boggs+, “Denver: NVIDIA’s First 64-Bit ARM Processor,” IEEE Micro 2015. Gwennap, “NVIDIA’s First CPU is a Winner,” MPR 2014. Digital Design & Computer Arch. Lecture 25b: Prefetching Frank K. Gürkaynak Rahul Bera Prof. Onur Mutlu ETH Zürich Spring 2024 30 May 2025 Benefits of Runahead Execution Instead of stalling during an L2 cache miss: n Processor speculative pre-executes the program far ahead into the instruction stream without stalling for long-latency instructions n Pre-executed loads and stores independent of L2-miss instructions generate very accurate data prefetches: q For both regular and irregular access patterns n Instructions on the predicted program path are prefetched into the instruction cache and outer cache levels n Hardware prefetcher and branch predictor tables are trained using future access information Runahead Execution Mechanism n Entry into runahead mode q Checkpoint architectural register state n Instruction processing in runahead mode n Exit from runahead mode q Restore architectural register state from checkpoint Instruction Processing in Runahead Mode Compute Load 1 Miss Runahead Miss 1 Runahead mode processing is the same as normal instruction processing, EXCEPT: n It is purely speculative: Architectural (software-visible) register/memory state is NOT updated in runahead mode. n L2-miss dependent instructions are identified and treated specially. q They are quickly removed from the instruction window. q Their results are not trusted. L2-Miss Dependent Instructions Compute Load 1 Miss Runahead Miss 1 n Two types of results produced: INV and VALID n INV = Dependent on an L2 miss n INV results are marked using INV bits in the register file and store buffer. n INV values are not used for prefetching/branch resolution. Removal of Instructions from Window Compute Load 1 Miss Runahead Miss 1 n Oldest instruction is examined for pseudo-retirement q An INV instruction is removed from window immediately. q A VALID instruction is removed when it completes execution. n Pseudo-retired instructions free their allocated resources. q This allows the processing of later instructions. n Pseudo-retired stores communicate their data to dependent loads. Store/Load Handling in Runahead Mode Compute Load 1 Miss Runahead Miss 1 n A pseudo-retired store writes its data and INV status to a dedicated memory, called runahead cache. n Purpose: Data communication through memory in runahead mode. n A dependent load reads its data from the runahead cache. n Does not need to be always correct à Size of runahead cache is very small. Branch Handling in Runahead Mode Compute Load 1 Miss Runahead Miss 1 n INV branches cannot be resolved. q A mispredicted INV branch causes the processor to stay on the wrong program path until the end of runahead execution. n VALID branches are resolved and initiate recovery if mispredicted. A Runahead Processor Diagram 132 Mutlu+, “Runahead Execution,” HPCA 2003.","libVersion":"0.3.2","langs":""}