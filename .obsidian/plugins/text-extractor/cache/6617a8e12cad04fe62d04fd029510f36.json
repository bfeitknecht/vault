{"path":"HS23/LinAlg/UE/s/LinAlg-u12-s.pdf","text":"D-INFK Linear Algebra HS 2023 Afonso Bandeira Bernd G¨artner Solution for Assignment 12 1. a) Let v be an eigenvector of AB corresponding to eigenvalue λ. Observe that (BA)Bv = B(AB)v = Bλv = λBv. Hence, if Bv ̸= 0 then Bv is an eigenvector of BA with corresponding eigenvalue λ. Otherwise, we must have Bv = 0 and hence λ = 0. But this implies that B is not full rank which also means that BA is not full rank. Thus, λ = 0 must be an eigenvalue of BA. b) Let v1, . . . , vn ∈ Rn be a complete set of real eigenvectors of AB. By subtask a) and the addi- tional assumption that B is invertible, we know that the vectors Bv1, . . . , Bvn ∈ Rn are all real eigenvectors of BA. Moreover, the n vectors Bv1, . . . , Bvn are linearly independent since B is invertible and because the vectors v1, . . . , vn are linearly independent. Hence, Bv1, . . . , Bvn form a basis of Rn and therefore they are a complete set of real eigenvectors of BA. c) From subtask b), we know that if AB has a complete set of real eigenvectors, then so does BA. Using subtask b) again with matrices A and B exchanged (we can do this because here A is invert- ible as well), we also get that if BA has a complete set of real eigenvectors, then so does AB. This proves the claim. d) Consider the matrices A = [1 0 0 0 ] and B = [ 0 1 0 0 ] with AB = [0 1 0 0 ] and BA = [ 0 0 0 0 ] . The matrix BA has a complete set of real eigenvectors because N(BA) = R2. However, the matrix AB does not have a complete set of real eigenvectors: the eigenvalue 0 appears with algebraic multiplicity 2, but AB only has rank 1 and hence dim(N(AB)) = 1. In other words, the geometric multiplicity of eigenvalue 0 is only 1. We conclude that there is no complete set of real eigenvectors for AB. 2. a) Since λ1, . . . , λn are distinct, we know from Proposition 6.1.7 that any n corresponding real eigen- vectors v1, . . . , vn ∈ Rn of A are linearly independent. In other words, A has a complete set of real eigenvectors v1, . . . , vn. By assumption, B also has this complete set of real eigenvectors v1, . . . , vn. Let V be the matrix with columns v1, . . . , vn. From Theorem 6.2.1 it follows that A = V ΛAV −1 for some diagonal matrix ΛA ∈ Rn×n. Analogously, it follows that B = V ΛBV −1 for some diagonal matrix ΛB ∈ Rn×n. Now observe that since both ΛA and ΛB are diagonal matrices, we have ΛAΛB = ΛBΛA. Thus, we get AB = (V ΛAV −1)(V ΛBV −1) = V ΛA(V −1V )ΛBV −1 = V ΛAΛBV −1 = V ΛBΛAV −1 = V ΛB(V −1V )ΛAV −1 = (V ΛAV −1)(V ΛBV −1) = BA which concludes the proof. b) By similarity of A and B, there exists an invertible matrix S ∈ Rn×n with A = SBS−1. By similarity of B and C, there exists an invertible matrix T ∈ Rn×n with B = T CT −1. Hence, we get A = SBS−1 = ST CT −1S−1 = P CP −1 where P = ST is invertible with inverse P −1 = T −1S−1. We conclude that A and C are similar. c) Since A has n distinct real eigenvalues, it must have a complete set of real eigenvectors and hence it must be diagonalizable (by Proposition 6.1.7 and Theorem 6.2.1), i.e. there exists an invertible matrix V ∈ Rn×n and a diagonal matrix Λ ∈ Rn×n with A = V ΛV −1. Analogously, B is diagonalizable with B = W ΛW −1 for some invertible matrix W ∈ Rn×n. Note that we can assume that the diagonalization of both A and B use the same diagonal matrix Λ because A and B are assumed to have the same eigenvalues. We observe that this means that A is similar to Λ and that Λ is similar to B. By using the statement from the previous subtask for A, Λ, B, it follows that A is similar to B. d) Let λ ∈ R be an arbitrary real eigenvalue of A with corresponding eigenvector v ∈ Rn. By similarity of A and B, there exists an invertible matrix S ∈ Rn×n with B = SAS−1. Now consider the vector w = Sv. We have Bw = SAS−1(Sv) = SAv = λSv = λw and hence w is a real eigenvector of B with corresponding real eigenvalue λ. Since λ was arbitrary, we conclude that every real eigenvalue of A is also a real eigenvalue of B. By a symmetric argument (swapping the roles of A and B above) we get that every real eigenvalue of B is also a real eigenvalue of A. We conclude that A and B have the same set of real eigenvalues. 3. a) Consider the vector u = v + w. By linear independence of v and w we get that u ̸= 0. We also get Au = A(v + w) = Av + Aw = 3w + 3v = 3u and hence 3 is an eigenvalue of A. Analogously, we define u′ = v − w. By linear independence of v and w we get that u′ ̸= 0. Again, we compute Au ′ = A(v − w) = Av − Aw = 3w − 3v = −3u and hence −3 is also an eigenvalue of A. b) First of all, note that the proof that we used in the previous subtask breaks down because without linear independence we cannot conclude that u and u′ are both non-zero. The key insight for this subtask is that we can still guarantee that at least one of them is non-zero. Concretely, define u = v + w and u′ = v − w as before and assume for a contradiction that u = u′ = 0. This would imply u + u′ = 2v = 0 which is a contradiction to v ̸= 0. Hence, either u ̸= 0 or u′ ̸= 0 and by the arguments from a), either 3 or −3 is an eigenvalue of A. 4. The key insight of this exercise is to look at the real eigenvalues of A. For this, we first compute the characteristic polynomial p(λ) = ∣ ∣ ∣ ∣ ∣ ∣ −λ 1 3 1 2 −λ 0 0 1 3 −λ ∣ ∣ ∣ ∣ ∣ ∣ = −λ 3 + 1 2 λ + 1 2 . We can guess one of the roots of this polynomial to be 1. Hence, we obtain p(λ) = (λ − 1)(−λ2 − λ − 1 2 ) and it turns out that the other roots of p are complex-valued. Hence, the only real eigenvalue of A is 1. Recall that our goal is to find an initial population that yields a stable population over time. The idea here is that an eigenvector corresponding to eigenvalue 1 is a suitable choice. Such an eigenvector can be found in N(A − I) and one example would be to choose v0 = [6 3 1]⊤ . Indeed, we get Av0 =  0 1 3 1 2 0 0 0 1 3 0     6 3 1   =   6 3 1   , and hence vt = Atv0 = v0 for all t ∈ N0. We conclude that this choice of initial population yields a stable population. 5. a) One could solve this by first computing A and then computing its real eigenvalues. But in this case, it is not hard to guess eigenvectors of A. In particular, choosing x = y = 1 yields A [ 1 1 ] = [1 1 ] and hence 1 is an eigenvalue of A. Moreover, guessing x = 1 and y = −1 yields A [−1 1 ] = [ 1 −1 ] and hence −1 is an eigenvalue of A. We conclude that the two real eigenvalues of A are 1 and −1. Since A is a 2 × 2 matrix, there cannot be more thant 2 eigenvalues. b) The simplest choice is the diagonal matrix A =   0 0 0 0 1 0 0 0 2   . Indeed, we have Ae1 = 0, Ae2 = e2, and Ae3 = 2e3. Hence, A has the desired eigenvalues. It does not have any other eigenvalues because a 3 × 3 matrix can have at most 3 eigenvalues. c) Consider the matrix A from the previous subtask and the basis v1 = e1 + e2, v2 = e1 − e2, v3 = e3 of R3. In particular, consider the matrix V =   | | | v1 v2 v3 | | |   =   1 1 0 1 −1 0 0 0 1   with inverse V −1 =   1 2 1 2 0 1 2 − 1 2 0 0 0 1   and define B as B = V AV −1 =   1 2 − 1 2 0 1 2 1 2 0 0 0 2   . The matrices A and B are similar and hence have the same eigenvalues.","libVersion":"0.3.2","langs":""}