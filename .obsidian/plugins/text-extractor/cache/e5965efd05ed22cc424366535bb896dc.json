{"path":"sem1/LinAlg/PV/cheatsheets/LinAlg-cheatsheet-gassmann.pdf","text":"1 Lizenziert unter CC BY-SA 4.0. Für Urheber, Quellen und Lizenzinformationen, siehe: github.com/thomasgassmann/eth-cheatsheets Dieses Cheatsheet ist inhaltlich inspiriert von Danny Camen- isch’s Cheatsheet. Stylistisch basiert das Cheatsheet auf dem Ana- lysis I Cheatsheet von xyquadrat. 1 Vorwissen 1.1 Komplexe Zahlen Definition Ein Ausdruck der Form z = a + ib, wobei i2 = −1. a = Re(z) ist der Realteil, b = Im(z) ist der Imaginärteil. Addition erfolgt komponentenweise, Multiplikation erfolgt unter Annahme des Binomialgesetzes und i2 = −1 (i.e. zw = (ac − bd) + i(ad + bc)). Für Division gilt z w = c+id a+ib = zw ww = (ca+bd)+i(ad−cb) a2+b2 . Die Norm ist definiert als |z| = √Re(z)2 + Im(z)2 = √z · z. Für z = x + iy ist z = x − iy konjugiert-komplex. zz = Re(z)2 + Im(z)2 Eine komplexe Zahl kann in Polarkoordinaten dargestellt wer- den. Es gilt z = reiϕ = r(cos(ϕ) + i sin(ϕ)). Radizieren: n√a = z ⇐⇒ a = zn ⇐⇒ |a|eiα = rneiϕn wobei r = n√|a| und ϕ = α+2kπ n . Fundamentalsatz der Algebra Sei p(z) = anzn + · · · + a0 ein Polynom mit an ̸= 0 und reellen oder komplexen Koeﬀizienten ai ∈ C. Dann hat p(z) genau n Nullstellen (mit ihren Vielfachen gezählt). Es gilt z ± w = z ± w, zw = zw, ( z w ) = z w , |z| = |z|, |z + w| ≤ |z| + |w|, |zw| = |z||w|. θ =    arctan( y x ) if z on positive x-axis π 2 if x = 0, y > 0 π + arctan( y x ) if z on negative x-axis 3π 2 if x = 0, y < 0 1.2 Polynome Bei Polynomen mit reellen Nullstellen treten die Nullstellen als komplex-konjugiertes Paar auf. Für Grad 2, verwende z = −b±√b2−4ac 2a um ein Polynom p(z) = 0 zu lösen. Für azn + c = 0, verwende z = n √− c a . Bei einem Polynom über C mit ungeradem Grad gibt es min- destens eine reelle Nullstelle. 2 LGS / Gauss-Elimination Ein homogenes LGS hat die Form Ax = 0. 1. Wir können Zeilen vertauschen, eine Zeile mit a ∈ R \\ {0} multiplizieren und Zeilen voneinander subtrahieren bzw. ad- dieren. 2. Wir schreiben das lineare Gleichungssystem (LGS) in Ma- trixform. 3. Wir transformieren das LGS in die Zeilenstufenform. 4. Wir lösen das LGS von unten nach oben. Verträglichkeitsbedingungen (VTB): Falls die Verträglich- keitsbedingungen cr+1 = · · · = cm = 0 nicht erfüllt sind, so gibt es keine Lösung. Bei homogenen LGS sind die Verträglichkeitsbe- dingungen immer erfüllt. Nur wenn r < n gibt es also nicht-triviale Lösungen für das homogene LGS. Lösungen Ax = b hat mindestens eine Lösung gdw. (r = m) oder (r < m und VTB sind erfüllt). In diesem Fall gibt es 1 Lösung falls r = n, andernfalls eine n − r Schar an Lösungen. • r = m: {r = m = n: eindeutige Lösung, regulär r < n: ∞ Lösungen mit n − r freien Variablen • r < m: {r = n: eindeutige Lösung r < n: ∞ Lösungen mit n − r freien Variablen Es gilt immer r ≤ min(m, n). 3 Matrizen & Vektoren Eine m × n Matrix hat m Zeilen und n Spalten wobei das i, j Ele- ment mit ai,j oder (A)i,j bezeichnet wird. Ein m × 1 Vektor ist ein Spaltenvektor und ein 1 × n Vektor ist ein Zeilenvektor (n-Tupel). Die Elemente ajj , j = 1, 2, · · · , min(m, n) heissen Dia- gonalelemente. Für eine Diagonalmatrix A gilt (A)ij = 0 für i ̸= j. Wir bezeichnen die Matrix dann durch die Elemente auf der Diagonale: A = diag(d11, · · · , dnn). Für eine obere Dreiecksma- trix gilt: (R)ij = 0 für i > j. Für eine untere Dreiecksmatrix gilt: (L)ij = 0 für i < j. Wir definieren als Tr(A) die Summe der Diagonalelemente. Multiplikation Für eine m × n Matrix A und eine n × p Matrix B gilt (AB)ij = ∑n k=1 aikbkj wobei AB eine m × p Matrix ist. Generell nicht kommutativ. Falls zwei Matrizen A, B ∈ En×n kommutieren (AB = BA), dann gilt (AB)k = AkBk. Rechenregeln für Matrizen i: (αβ)A = α(βA) · ii: (αA)B = α(AB) = A(αB) · iii: (α + β)A = αA + βA · iv: α(A + B) = αA + αB · v: A + B = B + A · vi: A + (B + C) = (A + B) + C · vii: (AB)C = A(BC) · viii: (A + B)C = AC + BC · ix: A(B + C) = AB + AC Eine Linearkombination der Vektoren a1, · · · , an ist α1a1 + · · · + αnan. Falls AB = 0, so sind A und B Nullteiler. Transponierte Matrizen Transponiert: A⊤ wird definiert als (A⊤)ij = (A)ji. Hermitesch-transponiert: AH = (A)⊤ = A⊤. Eine Matrix ist symmetrisch falls A⊤ = A und hermitesch falls AH = A (Diagonale somit reell). Schiefsymmetrisch ist sie falls A⊤ = −A. Sind A und B hermitesche Matrizen, so ist AB genau dann hermitesch falls AB = BA. AHA und AAH sind für alle Matrizen A hermitesch. Rechenregeln für hermitesche Matrizen i: (A⊤)⊤ = A · ii: (AH)H = A · iii: (αA)⊤ = αA⊤ · iv: (αA)H = αAH · v: (A + B)⊤ = A⊤ + B⊤ · vi: (A + B)H = AH + BH · vii: (AB)⊤ = B⊤A⊤ · viii: (AB)H = BHAH Spalten- und Reihensichtweise Ist A = ( a1 ··· am ) eine m × n Matrix, B = ( b1 b2 ··· bp ) eine n × p Matrix, dann gilt AB = ( Ab1 Ab2 ··· Abp ) = ( a1B ··· amB ). Ist umgekehrt A = ( a1 a2 ··· an ) eine m × n Matrix, B =( b1 ··· bn ) eine n × p Matrix, dann gilt AB = ∑n i=1 aibi. Multipliziert man eine Diagonalmatrix von links, dann skaliert dies die Zeilen. Multipliziert man eine Diagonalmatrix von rechts, dann multipliziert dies die Spalten. 2 Definite Eine Matrix A ist positiv-definit (positiv-semidefinit), falls ∀x ∈ En, xHAx > 0 (≥ 0). Eine symmetrische Matrix A ist genau dann positiv-definit (positiv-semidefinit) falls alle Eigenwerte strikt positiv (positiv) sind. Um zu zeigen, dass eine Matrix A indefinit ist, zeigt man, dass es ein x mit x⊤Ax > 0 und ein y mit y⊤Ay < 0 gibt. Es gilt e⊤ i Aej = aij . Folglich ist A indefinit, falls es auf der Diagonalen von A verschiedene Vorzeichen gibt (vorrausgesetzt det(A) ̸= 0). Marlene Trick: Sei M eine symmetrische Matrix und Ak die k × k Blockmatrix der oberen, linken Ecke. Mit ∆k bezeichnen wir die Determinante von Ak. Nun ist M pos. def. wenn ∆k > 0 für alle k, weiter ist M neg. def. wenn (−1)k∆k > 0 für alle k. A ist indefinit falls das erste ∆k das keiner der beiden Regeln entspricht ein falsches Vorzeichen hat. Das äussere Produkt für zwei Vektoren x und y ist definiert als xy⊤. Die orthogonale Projektion von x auf y ist Pyx = 1 ∥y∥2 yyHx. Entsprechend ist die Orthogonalprojektion PA := A(AHA)−1AH (mit A ∈ Em×n und Rank A = n) die Projektion auf den Kolon- nenraum R(A). Für diese Orthogonalprojektion gilt ∥y − P y∥2 = minz∈Im P ∥y − z∥2 (siehe Least Squares). Eine Matrix P heisst Projektor falls P 2 = P . 3.1 Inverse Eine n×n Matrix ist invertierbar wenn eine Matrix A−1 existiert, so dass AA−1 = In = A−1A. Die Inverse ist eindeutig bestimmt und existiert genau dann wenn Rank A = n. Rechenregeln für Inverse Sind A, B regulär. Dann: • A−1 regulär und (A−1)−1 = A. • AB regulär und (AB)−1 = B−1A−1. • AH regulär und (AH)−1 = (A−1)H. Die Inverse findet man durch Gauss-Jordan Elimination. [A|In] Zeilen- ========⇒ operationen [In|A−1] Für A = ( a b c d ) und det(A) ̸= 0 (also A invertierbar) gilt A−1 = 1 ad−bc ( d −b −c a ). 3.2 Orthogonale und unitäre Matrizen Eine n × n Matrix heisst unitär falls AHA = In. Eine Matrix heisst orthogonal falls A⊤A = In. Die Matrix ist genau dann unitär/or- thogonal wenn alle Spalten orthonormal sind. Für A, B unitär (dasselbe für orthogonal) gilt: i: A regulär und A−1 = AH · ii: AAH = In = AHA · iii: A−1, AB unitär Für unitäre Matrizen gilt | det(A)| = 1, für orthogonale Matri- zen somit det(A) = ±1. Ausserdem gilt |λ| = 1 für alle Eigenwerte λ einer unitären Matrix. Entsprechend gilt λ = ±1 für alle Eigen- werte einer orthogonalen Matrix. Eine Rotationsmatrix R ist eine unitäre Matrix. In 2D ist R =( cos θ − sin θ sin θ cos θ ). Verallgemeinert wird dies durch Givens-Rotationen. In 4D gilt bspw. ( cos θ 0 − sin θ 0 0 1 0 0 sin θ 0 cos θ 0 0 0 0 1 ). Die Householder-Matrix Qu = I − 2uu⊤ stellt eine Spiege- lung an der Hyperebene welche orthogonal zu u ist dar. 4 LR-Zerlegung Die LR-Zerlegung ist ein weiteres Verfahren zum lösen von LGS. Wir erhalten dadurch P A = LR. Es ist besonders effektiv wenn wir mehrere LGS mit gleichem A haben.   I ︷ ︸︸ ︷ 1 0 0 A ︷ ︸︸ ︷ 2 1 2 I ︷ ︸︸ ︷ 1 0 0 0 1 0 1 2 3 0 1 0 0 0 1 2 2 2 0 0 1   − 1 2 −1 =⇒  1 0 0 2 1 2 1 0 0 0 1 0 0 3 2 2 1 2 1 0 0 0 1 0 1 0 1 0 1   − 2 3 =⇒  1 0 0 2 1 2 1 0 0 0 1 0 0 3 2 2 1 2 1 0 ︸ ︷︷ ︸ P 0 0 1 ︸ ︷︷ ︸ R 0 0 − 4 3 ︸ ︷︷ ︸ L 1 2 3 1   Wenn wir Zeilen in R vertauschen, dann vertauschen wir auch die selben Zeilen in P und L (die 1 Einträge auf der Diagonalen in L werden nicht vertauscht!). Im Allgemeinen gilt A ∈ Em×n sowie P, L ∈ Em×m und R ∈ Em×n. Die Matrix L hat immer 1 auf der Diagonalen und Einträge unten links sind nur ungleich 0 für Spalten kleiner oder gleich Rank A. Die i-te Spalte in L entspricht also dem i-ten Pivot von A. Haben wir eine LR-Zerlegung von A können wir Ax = b eﬀi- zienter lösen. Zuerst lösen wir dazu P A = LR und lösen Lc = P b nach c. Dann lösen wir Rx = c nach x. 5 Vektorräume Eine Vektorraum V über K ist eine nichtleere Menge auf der eine Vektoraddition und Skalarmultiplikation definiert sind. V1: x + y = y + x · V2: (x + y) + z = x + (y + z) · V3: ∃0 ∈ V : x + 0 = x (∀x ∈ V ) · V4: ∀x ∈ V existiert −x: x + (−x) = 0 · V5: α(x + y) = αx + αy · V6: (α + β)x = αx + βx · V7: (αβ)x = α(βx) · V8: ∃1 ∈ K so dass ∀x ∈ V : 1x = x i: 0x = 0 · ii: α0 = 0 · iii: αx = 0 =⇒ x = 0 ∨ α = 0 · iv: (−α)x = α(−x) = −(αx) 5.1 Unterräume Ein Unterraum U ist eine nichtleere Teilmenge von V der abge- schlossen bzgl. Addition und Multiplikation ist. U beinhaltet immer den Nullvektor. Jeder Unterraum ist ein Vektorraum. Für zwei Un- terräume U, W ⊆ V ist U ∪ W genau dann ein Unterraum von V falls U ⊆ W oder W ⊆ U . U ∩W ist hingegen immer ein Unterraum. Für eine lineare Abbildung F : X ↦→ Y und einen Unterraum U ⊆ X ist F U ⊆ Y auch ein Unterraum. Ist W ⊆ Im F ein Unter- raum so ist auch F −1W ⊆ X ein Unterraum. Lineare Hülle (span) Die Menge der Linearkombinationen der Vektoren v1, · · · , vn ist der Unterraum aufgespannt durch diese Vektoren. Man bezeichnet ihn mit span{v1, · · · , vn}. Falls span{v1, · · · , vn} = V , dann sind v1, · · · , vn ein Erzeu- gendensystem von V . Um zu zeigen dass U ⊆ V ein Unterraum von V ist zeigt man i: 0V ∈ U · ii: ∀x, y ∈ U gilt x + y ∈ U · iii: ∀x ∈ U, ∀α ∈ K gilt αx ∈ U . Unterräume sind bspw. auch Ker(A) und Im(A). 5.2 Lineare Abhängigkeit, Basen und Dimensionen Lineare Unabhängigkeit Vektoren v1, · · · , vn sind linear unabhängig genau dann wenn: n∑ k=1 αkvk = 0 =⇒ α1 = · · · = αn = 0 Andernfalls sind die Vektoren linear abhängig, d.h. es exis- tiert eine nicht-triviale Nullsumme bzw. ein Vektor lässt sich als Linearkombination der anderen schreiben. Basis Ein Erzeugendensystem v1, · · · , vn von V ist eine Basis von V genau dann wenn v1, · · · , vn linear unabhängig sind. Jeder Vektor in V lässt sich eindeutig als Linearkombination der Basisvektoren schreiben. Jede Menge {v1, · · · , vm} ⊆ V mit dim V < m ist linear ab- hängig. In jedem endlichen Vektorraum V mit dim V = n ist eine Menge von n linear unabhängigen Vektoren eine Basis. In einem Vektorraum V beeinflusst die Wahl der Skalare die Dimension. Cn über C hat Dimension n, Cn über R hat Dimension 2n. Die Koeﬀizienten ξ = (ξ1, · · · , ξn)⊤ sind Koordinaten von x bzgl. einer Basis B falls x = ∑n i=1 ξibi gilt. Die Summe wird Koordinatendarstellung genannt. Zwei Unterräume U, W ⊆ V heissen komplementär falls jedes x ∈ V eine eindeutige Darstellung x = u+w mit u ∈ U und w ∈ W 3 hat (V ist dann direkte Summe von U und W ). Man schreibt V = U ⊕ W . Falls zwei Unterräume komplementär sind, folgt U ∩ W = {0V }. Um zu zeigen dass U ⊕ W = V gilt zeigt man dim V = dim U + dim W und U ∩ W = {0V }. Falls f : U → V und g : V → W lineare Abbildungen zwischen Vektorräumen sind so dass g ◦ f ein Isomorphismus ist, dann gilt V = Im f ⊕ Ker g. 5.3 Basiswechsel und Koordinatentransformation Wenn wir von einer alten Basis B zu einer neuen Basis B′ wechseln, können wir die neue Basis mit der alten darstellen. Es gilt dann b′ k = ∑n i=1 τikbi mit der Basiswechselmatrix T = (τik). Es gilt ξ = T ξ′ sowie ξ′ = T −1ξ, da T regulär ist. Dabei sind ξ′ die Koordinaten in der Basis B′ und ξ sind die Koordinaten in der Basis B. Es gilt dann B′ = BT , was aus Bξ = x = B′ξ′ und ξ = T ξ′ folgt. Wir schreiben auch T = Mat(B′)B. Wir können T zwischen B und B′ auch durch Gauss-Elimination finden (denn T = B−1B′): [B|B′] Zeilen- ========⇒ operationen [In|T ] 6 Lineare Abbildungen Eine Abbildung F : V ↦→ W ist linear, wenn F (v+w) = F (v)+F (w) sowie F (αv) = αF (v) für alle v, w ∈ V und α ∈ K gilt. Einfacher ausgedrückt ist F linear falls F (αv + w) = αF (v) + F (w). Für eine Funktion F : X ↦→ Y gilt: • injektiv: ∀x, x′ ∈ X f (x) = f (x′) =⇒ x = x′ • surjektiv: ∀y ∈ Y gibt es x ∈ X mit f (x) = y • bijektiv: injektiv und surjektiv, F −1 existiert Sei F : X ↦→ Y eine lineare Abbildung ist wobei span{b1, · · · , bn} = X und span{c1, · · · , cm} = Y Basen sind. Dann lässt sich F (bl) = ∑m k=1 aklck schreiben. Die Matrix Am×n mit Elementen akl heisst die Abbildungsmatrix bezüglich X, Y . Jede lineare Abbildung lässt sich also durch eine Matrix dar- stellen. Ist F bijektiv ist es ein Isomorphismus, ist zusätzlich X = Y so ist F ein Automorphismus. Wenn F ein Isomorphismus ist, existiert ein Isomorphismus F −1. 6.1 Kern, Bild und Rang Sei F : X ↦→ Y eine lineare Abbildung. Es gilt Rank F := dim Im F . Kern und Bild Wir definieren den Kern als Ker F := {x ∈ X|F (x) = 0}. Der Kern ist ein Unterraum von X und F ist injektiv genau dann wenn Ker F = {0} Wir definieren das Bild als Im F := {F (x)|x ∈ X}. Das Bild ist ein Unterraum von Y und F ist surjektiv genau dann wenn Im F = Y . Rangsatz dim X − dim Ker F = dim Im F = Rank F Zwei Vektorräume endlicher Dimension sind genau dann iso- morph wenn sie die gleiche Dimension haben. Foldergungen des Rangsatzes Seien F : X ↦→ Y, G : Y ↦→ Z lineare Abbildungen: • F injektiv ⇐⇒ Rank F = dim X • F surjektiv ⇐⇒ Rank F = dim Y • F bijektiv ⇐⇒ Rank F = dim X = dim Y • Rank G ◦ F ≤ min(Rank F, Rank G) • G injektiv =⇒ Rank GF = Rank F • F surjektiv =⇒ Rank GF = Rank G 6.2 Matrizen als lineare Abbildungen Sei A ∈ Em×n. Der Kolonnenraum oder Wertebereich von A ist der Unterraum R(A) = Im A = span{a1, · · · , an} ⊆ Em. Der Nullraum von A ist der Unterraum N (A) = Ker A ⊆ En. Rangsatz für Matrizen Sei r := Rank A = dim Im A. Dann ist dim Ker A = n − r und r entspricht der Anzahl Pivotelemente in REF. Zusätzlich: Rank A⊤ = Rank AH = Rank A Somit gilt für Matrizen A ∈ Em×n und B ∈ Ep×m auch: • Rank BA ≤ min(Rank B, Rank A) • Rank B = m ≤ p =⇒ Rank BA = Rank A • Rank A = m ≤ n =⇒ Rank BA = Rank B Für quadratische Matrizen A ∈ En×n sind also folgende Aussa- gen äquivalent: i: A ist regulär · ii: Rank A = n · iii: Spalten (oder Zeilen) sind linear unabhängig · iv: Ker A = {0} · v: Im A = En Falls x0 eine Lösung für Ax = b ist, so ist die Lösungsmenge Lb = x0 + Ker A ein aﬀiner Unterraum. 6.3 Zusammenfassende Eigenschaften von A Sei A ∈ M m×n mit r := Rank A: i: dim Im A = dim Im AH = r · ii: dim Ker A = n − r · iii: dim Ker AH = m − r r = n ⇔ Ker A = {0} r = m ⇔ Ker AH = {0} ⇔ Spalten von A l.u. ⇔ Spalten von A erzeugend ⇔ Zeilen von A erzeugend ⇔ Zeilen von A l.u. ⇔ A injektiv ⇔ A surjektiv Basis für Im A und Ker A finden Zuerst bringt man A auf Zeilenstufenform. • Basis für Im A: Die Spalten in der originalen Ma- trix welche den Pivotspalten in der Zeilenstufenform entsprechen sind Basisvektoren für Im A. • Basis für Ker A: Zuerst parameterisiert man die frei- en Variablen und findet dann die Lösungesmenge L0 von Ax = 0. Die Zeile i des Lösungsvektors enthält dann den Wert der i-ten Variable. Man wählt dann die Koeﬀizienten der freien Variablen entsprechend um Basisvektoren zu finden (e.g. man setzt einen der Koef- fizienten auf 1 und alle anderen auf 0 pro Basisvektor). 6.4 Abbildungen von Koordinatentransformation x ∈ X y ∈ Y ξ ∈ En η ∈ Em ξ′ ∈ En η′ ∈ Em F κX κY A κ−1 X T −1 κ−1 Y S−1T B S • A = SBT −1 • B = S−1AT • Rank F = Rank A = Rank B Ist F Rank F = r, so besitzt F bzgl. geeigneten Basen X, Y die Abbildungsmatrix: [ Ir 0 0 0 ] 7 Vektorräume mit Skalarprodukt Skalarprodukt Skalarprodukt in einem Vektorraum V über E ist eine Funk- tion ⟨·, ·⟩ : V × V ↦→ E mit folgenden Eigenschaften: • Linear im zweiten Faktor: ⟨x, y + z⟩ = ⟨x, y⟩ + ⟨x, z⟩ und ⟨x, αy⟩ = α⟨x, y⟩. Für reelle Skalare auch linear im ersten Faktor. • Symmetrisch wenn E = R und hermitesch wenn E = C: ⟨x, y⟩ = ⟨y, x⟩ • Positiv definit: ∀x ∈ V , ⟨x, x⟩ ∈ R (auch für komplexe Vektorräume!), ⟨x, x⟩ ≥ 0, ⟨x, x⟩ = 0 ⇐⇒ x = 0 Falls E = C, nennt man V auch einen unitären Vektorraum, für E = R auch euklidischer oder orthogonaler Vektorraum. Für ein Skalarprodukt definiert man die induzierte Norm als ∥x∥ =√⟨x, x⟩. 4 Norm Norm in Vektorraum V über E ist eine Funktion ∥·∥ : V ↦→ R mit: • positiv definit: ∀x ∈ V , ∥x∥ ≥ 0 und ∥x∥ = 0 ⇐⇒ x = 0 • homogen: ∥αx∥ = |α| · ∥x∥ ∀x ∈ V, ∀α ∈ E • Dreiecksungleichung: ∥x + y∥ ≤ ∥x∥ + ∥y∥ ∀x, y ∈ V Ein Vektorraum mit einer Norm heisst normierter Vektor- raum. Euklidischer Raum Das euklidische Skalarprodukt ist definiert als: ⟨x, y⟩ := xHy Die euklidische 2-Norm ist entsprechend definiert als: ∥x∥2 =√xHx Die p-Norm ist definiert als ∥x∥p = (|x1|p + · · · + |xn|p) 1 p . Skalarprodukte ⟨x, y⟩V ist ein Skalarprodukt genau dann wenn ⟨x, y⟩V := xHAy wobei A eine hermitesche Matrix mit strikt positiven Eigenwerten ist (positiv-definit). Es gilt dann aij = ⟨ei, ej ⟩. Cauchy-Schwarz Sei V ein Vektorraum über E mit beliebigem Skalarprodukt. |⟨x, y⟩| ≤ ∥x∥∥y∥ ⇐⇒ |⟨x, y⟩|2 ≤ ⟨x, x⟩⟨y, y⟩ Das Gleichheitszeichen gilt genau dann wenn x und y linear abhängig sind. Winkel Seien x, y, ∈ V . Der Winkel zwischen x und y ist gegeben als: φ := arccos Re⟨x, y⟩ ∥x∥∥y∥ Zwei Vektoren sind orthogonal, falls ⟨x, y⟩ = 0 (x ⊥ y). Zwei Teilmengen sind orthogonal, wenn ∀x ∈ M, ∀y ∈ N gilt: ⟨x, y⟩ = 0 (M ⊥ N ). Eine Basis ist orthogonal wenn ⟨bk, bl⟩ = δkl für alle k, l, wobei das Kroneckerdelta definiert ist als δkl = {0 falls k ̸= l 1 falls k = l . Gilt zusätzlich ∥bi∥ = 1 ∀i ist sie orthonormal. Pythagoras Falls x ⊥ y, so folgt ∥x ± y∥2 = ∥x∥2 + ∥y∥2. Allgemein gilt ∥x − y∥2 = ∥x∥2 + ∥y∥2 − 2∥x∥∥y∥ cos(φ). Eine Menge von paarweise orthogonalen Vektoren ist linear un- abhängig wenn alle Vektoren ungleich null sind. Der Nullvektor ist orthogonal zu allen Vektoren. Für eine Orthonormalbasis {b1, · · · , bn} und x ∈ V gilt x =∑n k=1⟨bk, x⟩bk. Parsevalsche Formel Seien x, y ∈ V , {b1, · · · , bn} eine Orthonormalbasis, so dass ξk := ⟨bk, x⟩V und ηk := ⟨bk, y⟩V . Für k = 1, · · · , n sind ξk und ηk die Koordinatenvektoren. ⟨x, y⟩V = Σn k=1ξkηk = ξHη = ⟨ξ, η⟩En i: ∥x∥V = ∥ξ∥En · ii: ∠(x, y)V = ∠(ξ, η)En · iii: x ⊥ y ⇐⇒ ξ ⊥ η 7.1 Gram-Schmidt-Orthonormalisierungsverfahren Für einen Unterraum U von V ist U ⊥ := {y ∈ V |{y}⊥U } das orthogonale Komplement. U und U ⊥ sind komple- mentäre Unterräume in direkter Summe zu V . Algorithmus Sei {a1, · · · , an} eine Menge linear unabhängiger Vektoren. Wir definieren rekursiv: • b1 := a1 ∥a1∥ • ̃bk := ak − ∑k−1 j=1 ⟨bj , ak⟩bj bk := ̃bk ∥̃bk∥ Nach k Schritten sind {b1, · · · bk} paarweise orthonormal und span{a1, · · · , ak} = span{b1, · · · , bk}. Wenn {a1, · · · , an} ei- ne Basis von V ist, ist {b1, · · · bn} auch eine. Jeder endlich- dimensionale Vektorraum hat eine Orthonormalbasis. Fundamentale Unterräume einer Matrix Sei A ∈ Em×n mit r := Rank A. N (A) = R(AH)⊥ ⊆ En N (A) ⊕ R(AH) = En N (AH) = R(A)⊥ ⊆ Em N (AH) ⊕ R(A) = Em dim R(A) = r dim N (A) = n − r dim R(AH) = r dim N (AH) = m − r 7.2 Basiswechsel und Koordinatentransformation von Orthonormalbasen Wir wollen von B nach B′, wobei beides Orthonormalbasen sind. Wir können b′ k = ∑n j=1 τjkbj schreiben und erhalten die Basis- wechselmatrix T . Da B, B′ orthonormal sind gilt T H = T −1. Daher gilt ξ = T ξ′ und ξ′ = T Hξ. Zudem ist B = B′T und B′ = BT H, wobei alle Matrizen unitär/orthogonal sind. Die Trans- formationsmatrix einer Basistransformation zwischen Orthonormal- basen ist unitär/orthogonal. T ist ausserdem längen- und winkeltreu. Definition Eine lineare Abbildung F : X ↦→ Y ist unitär/orthogonal falls ⟨F (v), F (w)⟩Y = ⟨v, w⟩X . Es gilt dann dass i: F ist längen- und winkeltreu · ii: Ker F = {0}, injektiv. Haben wir zusätzlich dim X = dim Y gilt i: F ist ein Isomor- phismus · ii: {b1, · · · , bn} ist eine Orthonormalbasis von X ⇐⇒ {F (b1), · · · , F (bn)} ist eine Orthonormalbasis von Y · iii: F −1 unitär/orthogonal · iv: Abbildungsmatrix A ist unitär/orthogonal. 8 QR-Zerlegung Eine Matrix kann dargestellt werden als A = QR wobei Q orthogo- nal und R eine obere Dreiecksmatrix ist. • Wende Gram-Schmidt auf den Spalten von A an ( =⇒ Q) • R = Q⊤A lösen und R zu erhalten (alternativ r11 = ∥a1∥, rjk = ⟨qj , ak⟩, rkk = ∥ ̃qk∥) Im allgemeinen Fall ist dabei für A ∈ Em×n Q ∈ Em×m und R ∈ Em×n. Man definiert dann: A = QR = [ ̃Q| ̃Q⊥] [ ̃R 0 ] = ̃Q ̃R wobei ̃Q ∈ Em×r, ̃R ∈ Er×n. 9 Least Squares Sei A ∈ Em×n Ax = b ein überbestimmtes LGS (m > n). Im Allgemeinen gibt es keine Lösung, wir möchten deshalb ∥Ax − b∥2 2 minimieren. Wir definieren also x∗ = argminx∈En ∥Ax − b∥2 2. Dies trifft für alle x ∈ En für welche Ax − b senkrecht auf R(A) steht. Ax∗ − b ⊥ R(A) ⇐⇒ Ax∗ − b ∈ N (AH) ⇐⇒ AH(Ax∗ − b) = 0 Es gilt N (AHA) = N (A), denn AHAx = 0 =⇒ xHAHAx = 0 =⇒ ∥Ax∥2 2 = 0 ⇐⇒ Ax = 0 und Ax = 0 =⇒ AHAx = 0. 5 Normalengleichungen Eine Lösung des Least Squares Problems findet man durch lösen der Normalengleichungen: AHAx = AHb Ist Rank A = Rank AHA = n ≤ m, so ist AHA invertierbar und x∗ = (AHA)−1AHb. Man nennt A+ = (AHA)−1AH auch die Pseudoinverse von A, da A+A = I. Haben wir eine QR-Zerlegung von A, gilt Rx∗ = QT b für A = QR. Für A ∈ Em×n mit Rank A = n ≤ m ist die Least Squares Lö- sung eindeutig bestimmt. Wenden wir Gram-Schmidt auf die Spal- ten von A a1, · · · , an und den Vektor an+1 := y an ergibt sich die eindeutige Lösung Ax∗ = y −̃bn+1 durch das Lot ̃bn+1 = y − Ax∗ ⊥ R(A). 10 Determinanten Determinante Die Determinante einer quadratischen Matrix A ∈ En×n ist definiert als: det(A) = ∑ p∈Sn sign(p)a1,p(1) · · · an,p(n) Sn ist dabei die symmetrische Gruppe n mit Ordnung n!. Jede Permutation p kann als Produkt von Transpositionen (Permutation bei welcher nur zwei Elemente vertauscht wer- den) dargestellt werden. sign(p) = {1 p Produkt gerader Anzahl Transpositionen −1 p Produkt ungerader Anzahl Transpositionen Für 3 × 3 Matrizen gilt (Sarrus): a11 a12 a13 a11 a12 a21 a22 a23 a21 a22 a31 a32 a33 a31 a32 + + + − − − Für 1 × 1 Matrizen gilt det(a11) = a11 und für 2 × 2 Matrizen gilt det( a11 a12 a21 a22 ) = a11a22 − a12a21. Für Blockdreiecksmatrizen gilt: det [ A C 0 B ] = det(A) det(B) Eigenschaften der Determinante Für die Determinante einer Matrix A ∈ En×n gilt: 1. det ist eine lineare Funktion in jeder Zeile. det       a11 a12 ··· a1n ... ... . . . ... γal1+γ′a′ l1 γal2+γ′a′ l2 ··· γaln+γ′a′ ln ... ... . . . ... an1 an2 ··· ann       = γ det       a11 a12 ··· a1n ... ... . . . ... al1 al2 ··· aln ... ... . . . ... an1 an2 ··· ann      +γ′ det       a11 a12 ··· a1n ... ... . . . ... a′ l1 a′ l2 ··· a′ ln ... ... . . . ... an1 an2 ··· ann       2. Vertauscht man zwei Zeilen, dann wechselt det(A) das Vorzeichen 3. det(I) = 1 Die Determinante ist die einzige auf En×n definierte Funktion mit den obigen drei Eigenschaften. Folgerungen für det(A), A ∈ En×n • det(A) = 0 ⇐⇒ A ist singulär – hat A also eine Nullzeile oder Nullspalte, so ist det(A) = 0 – hat A zwei gleiche Spalten oder zwei gleiche Zei- len, so ist det(A) = 0 • Addiert man zu einer Zeile (oder Spalte) eine ande- re Zeile (oder Spalte) multipliziert mit einem Skalar, dann ändert sich det(A) nicht • det(γA) = γn det(A) • ist A eine Diagonalmatrix oder Dreiecksmatrix, so ist det(A) = ∏n i=1 aii • wenden wir Gauss auf A an so gilt det(A) = (−1)v ∏n k=1 rkk wobei v die Anzahl Zeilenvertau- schungen ist und rkk die Pivotwerte der REF sind • det(AH) = det(A) sowie det(A⊤) = det(A) (auch für komplexe Matrizen) – wenn man zwei Spalten vertauscht, dann wech- selt det(A) das Vorzeichen – det ist eine lineare Funktion in jeder Spalte • det(AB) = det(A) det(B) und det(A−1) = 1 det(A) falls A−1 existiert Kofaktor Zu jedem Element akl einer n×n Matrix A ist A[k,l] definiert als die (n − 1) × (n − 1) Matrix mit der Zeile k und Spalte l gestrichen. Für den Kofaktor κkl von akl gilt dann: κkl = (−1)k+l det(A[k,l]) Dann gilt für jedes feste k, l (Entwicklung nach Zeile k bzw. Spalte l): det(A) = n∑ i=1 akiκki det(A) = n∑ i=1 ailκil 11 Eigenwerte und Eigenvektoren Definition Eine Zahl λ ∈ K heisst Eigenwert der linearen Abbildung F : V ↦→ V falls es v ∈ V mit v ̸= 0 gibt so dass F (v) = λv. v ist dabei ein Eigenvektor. Eine lineare Abbildung F und ihre Matrixdarstellung haben die gleichen Eigenwerte und die Eigenvektoren sind über die Koordina- tenabbildung κV verbunden. Spektrum Die Menge aller Eigenwerte von F heisst Spektrum von F und wird mit σ(F ) bezeichnet. Die Menge aller Eigenvektoren die zu einem Eigenwert λ gehö- ren bilden einen Unterraum Eλ = {v ∈ V |F (v) = λv}. λ ist ein Eigenwert von A genau dann wenn A − λI einen nicht- trivialen Nullraum hat. Es gilt dann Eλ = Ker(A − λI). Die geo- metrische Vielfachheit von λ entspricht dann dim Eλ. Charakteristisches Polynom Das charakteristische Polynom von A ist das Polynom χA(λ) = det(A − λI). χA(λ) = 0 ist dabei die charakte- ristische Gleichung. χA(λ) = (−1)nλn + (−1)n−1 Tr(A)λn−1 + · · · + det(A) Die algebraische Vielfachheit eines Eigenwertes λ ist die Vielfachheit von λ als Nullstelle von χA über C. Sei A ∈ E2×2. Dann ist das charakteristische Polynom χA(λ) = λ2 − Tr(A)λ + det(A). Für schiefsymmetrische Matrizen sind alle Eigenwerte ent- weder 0 oder imaginär. Bei Dreiecksmatrizen entsprechen die Ei- genwerte den Werten auf der Diagonalen. 6 Eigenschaften von Eigenwerten und Eigenvektoren • Für jede Matrix A ∈ En×n gilt: det(A) = n∏ i=1 λi Tr(A) = Σn i=1λi • λ ist ein Eigenwert von A genau dann wenn λ eine Nullstelle von χA ist • Für einen Eigenwert λ gilt: 1 ≤ geo. Vfh. von λ ≤ alg. Vfh. von λ ≤ n • Eigenvektoren zu verschiedenen Eigenwerten sind li- near unabhängig • Eine Matrix ist genau dann singulär wenn sie 0 als Eigenwert hat Eigenwerte und Eigenvektoren finden 1. Bestimmte das charakteristische Polynom χA(λ) = det(A − λI) 2. Bestimme die Nullstellen λ1, λ2, · · · , λn von χA (Ei- genwerte) 3. Für jedes λk bestimme eine Basis für Ker(A − λkI) (Eigenvektoren) Ähnliche Matrizen Zwei Matrizen A, C ∈ En×n sind ähnlich falls es ein T ∈ En×n gibt so dass C = T −1AT . Ähnliche Matrizen haben das gleiche charakteristische Poly- nom, die gleiche Spur, die gleiche Determinante, den selben Rang und die gleichen Eigenwerte inkl. algebraischer und geo- metrischer Vielfachheit (im Allgemeinen aber nicht die selben Eigenvektoren). 12 Spektral- und Eigenwertzerlegung Eine Matrix A ∈ En×n besitzt genau dann eine Eigenbasis wenn es eine ähnliche Diagonalmatrix Λ gibt. Man nennt die Matrix dann diagonalisierbar. A [ | | | v1 ··· vn | | | ] ︸ ︷︷ ︸ V = [ | | | v1 ··· vn | | | ] ︸ ︷︷ ︸ V   λ1 ··· 0 ... . . . ... 0 ··· λn   ︸ ︷︷ ︸ Λ A = V ΛV −1, Avj = vj λj , j = 1, · · · , n A wird durch V diagonalisiert. Die Kolonnen von V sind Ei- genvektoren, die Diagonalelemente von Λ Eigenwerte. Eine Matrix ist genau dann diagonalisierbar wenn für alle Eigenwerte die geo- metrische Vielfachheit der algebraischen Vielfachheit ent- spricht. Dies trifft immer zu wenn alle n Eigenwerte verschieden sind. Spektralsatz Sei A ∈ En×n hermitesch (AH = A). Dann gilt: • Alle Eigenwerte sind reell • Die Eigenvektoren sind paarweise orthogonal • Es gibt eine orthonormale Basis aus Eigenvektoren U so dass A = U ΛU H Dasselbe gilt für reell-symmetrische Matrizen. In diesem Fall sind die Eigenvektoren reell. Für Matrizen mit reellen Eigen- werten und einer reellen Orthonormalbasis aus Eigenvektoren gilt auch die Umkehrung. Normale Matrizen Eine Matrix A ∈ En×n ist normal, falls AHA = AAH. A ist genau dann diagonalisierbar durch eine unitäre Matrix über C falls A normal ist. 13 Singulärwertszerlegung Die Singulärwertszerlegung existiert für jede Matrix A ∈ Em×n. AHA ist immer hermitesch und positiv-definit. Es existiert also eine Spektralzerlegung: AHAV = V Λ λ=σ2 ========⇒ Umordnung AHAVr = VrΣ2 r =⇒ V H r AHAVr = Σ2 r =⇒ (Σ−1 r V H r AH) ︸ ︷︷ ︸ U H r (r×m) (AVrΣ−1 r ) ︸ ︷︷ ︸ Ur (m×r) = I Ur kann zu einer unitären m×m Matrix U ergänzt werden. U, V sind unitär, Σ ist nicht-negativ, reell und diagonal mit σ1 ≥ · · · σr > σr+1 = · · · = σn = 0 wobei r = Rank A = Rank AHA = Rank AAH. V diagonalisiert AHA, U diagonalisiert AAH. Singulärwertszerlegung A = U ΣV H = r∑ k=1 ukσkvH k AAH = U Σ2 mU H, AHA = V Σ2 nV H Ausserdem gilt AH = V Σ⊤U H und falls A invertierbar ist A−1 = V Σ−1U H. Aus AV = U Σ und AHU = V Σ⊤ folgt: • {u1, · · · , ur}: Orthonormalbasis von Im A = R(A) • {ur+1, · · · , um}: Orthonormalbasis von Ker AH = N (AH) • {v1, · · · , vr}: Orthonormalbasis von Im AH = R(AH) • {vr+1, · · · , vn}: Orthonormalbasis von Ker A = N (A) Bei einer Selbstabbildung gilt: A = U ΣV H = V V HU︸ ︷︷ ︸ R ΣV H = V RΣV H wobei R eine Rotation/Spiegelung ist und Σ die Hauptachsen ska- liert. AVr = UrΣr ist die reduzierte Form der Singulärwertszerle- gung. Es gilt dabei U H r Ur = I sowie V H r Vr = I. Aus der Singulärwertszerlegung folgt auch dass die r positiven Eigenwerte von AHA und AAH gleich sind, die Vielfachheit des Eigenwertes 0 aber n − r respektive m − r ist. 14 Synthetische Division Berechne 6x3+5x2−7 3x2−2x−1 = 2x + 3 + 8x−4 3x2−2x−1 : 15 Trigonometrie • sin(z) = e iz −e −iz 2i • cos(z) = e iz +e−iz 2 • sin(x) − sin(y) = 2 sin( x−y 2 ) cos( x+y 2 ) • cos(x) − cos(y) = −2 sin( x−y 2 ) sin( x+y 2 ) • sin(α + β) = sin(α) cos(β) + cos(α) sin(β) • cos(α + β) = cos(α) cos(β) − sin(α) sin(β) deg 0° 30° 45° 60° 90° 180° rad 0 π 6 π 4 π 3 π 2 π cos 1 √ 3 2 √ 2 2 1 2 0 -1 sin 0 1 2 √ 2 2 √3 2 1 0 tan 0 1√ 3 1 √3 +∞ 0","libVersion":"0.5.0","langs":""}