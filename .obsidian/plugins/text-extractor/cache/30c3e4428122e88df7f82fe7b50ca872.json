{"path":"sem3/A&D/UE/s/A&D-s-u05.pdf","text":"Eidgen¨ossische Technische Hochschule Z¨urich Ecole polytechnique f´ed´erale de Zurich Politecnico federale di Zurigo Federal Institute of Technology at Zurich Departement of Computer Science 21 October 2024 Johannes Lengler, David Steurer Kasper Lindberg, Lucas Slot, Hongjie Chen, Manuel Wiedmer Algorithms & Data Structures Exercise sheet 5 HS 24 The solutions for this sheet are submitted on Moodle until 27 October 2024, 23:59. Exercises that are marked by ∗ are challenge exercises. They do not count towards bonus points. You can use results from previous parts without solving those parts. The solutions are intended to help you understand how to solve the exercises and are thus more detailed than what would be expected at the exam. All parts that contain explanation that you would not need to include in an exam are in grey. Exercise 5.1 Max-Heap operations (1 point). (a) Consider the following max-heap: 82 63 43 24 27 54 10 61 19 48 Draw the max-heap after inserting the elements 70 and 51 in that order. Solution: 82 70 43 24 27 63 10 54 61 51 19 48 (b) Consider the following max-heap: 36 27 23 11 9 5 12 8 10 31 14 6 3 25 1 17 Draw the max-heap after two ExtractMax operations. Solution: 27 23 11 9 5 12 8 10 25 14 6 3 17 1 Guidelines for correction: Award 1/2 point per correctly solved part. Exercise 5.2 Guessing an interval. Alice and Bob play the following game: • Alice selects two integers 1 ≤ a < b ≤ 200, which she keeps secret. • Then, Alice and Bob repeat the following: – Bob chooses two integers 0 ≤ a′ < b′ ≤ 201. – If a = a′ and b = b′, Bob wins. – If a′ < a and b < b′, Alice tells Bob ‘my numbers are strictly between your numbers!’. – Otherwise, Alice does not give any clue to Bob. Bob claims that he has a strategy to win this game in 12 attempts at most. Prove that such a strategy cannot exist. Hint: Represent Bob’s strategy as a decision tree. Each edge of the decision tree corresponds to one of Alice’s answers, while each leaf corresponds to a win for Bob. 2 Hint: After defining the decision tree, you can consider the sequence k0 = 1 and kn = 2kn−1 + 2 for n ≥ 1, and prove that kn = 3 · 2n − 2 for any n ∈ N0 = N ∪ {0}. The number of vertices in the decision tree should be related to kn. Solution: The solution is divided into three parts: the construction of the decision tree, proving an upper bound for the number of vertices in the tree and an argument about the non-existence of the required strategy. We will show that Bob cannot cover all possible choices for a and b in twelve steps by comparing the possible scenarios covered in the tree and the choices. Construction of decision tree: Bob’s strategy can be represented as follows, where green arrows correspond to a win, red arrows to ‘my numbers are strictly between your numbers!’, and black arrows to the absence of a clue. The vertices that are not leaves in this tree correspond to the guesses he makes. . . . . . . . . . Each vertex of the corresponding tree has at most three children, of which one (corresponding to Bob winning the game) has no child. The two others can again have three children with the same structure as their parent. Note that not all vertices need to have exactly three children since it is possible that after a sequence of guesses Bob makes, not all answers of Alice are still possible for the next guess. For example, if he figured out the numbers of Alice with certainty after some number of steps, the next vertex has only one child (the one corresponding to Bob winning). Upper bound for the number of vertices: The tree contains all possible scenarios of the game. The depth is the number of guesses Bob needs to make in the worst case. Denoting by kn the maximum number of vertices in a tree of depth n ∈ N0 of the above form, we see that { k0 = 1 kn = 2kn−1 + 2 ∀n ≥ 1. The second equality is true since for a tree of depth n for n ≥ 1, we have the root and a leaf (endpoint of the green edge) as well as two subtrees of depth n − 1 of the same form (rooted at the endpoints of the red and black edges). We will now prove by induction that, for all n ∈ N0, we have kn = 3 · 2n − 2. • Base Case. For n = 0, we have k0 = 1 = 3 · 20 − 2, so the base case holds. • Induction Hypothesis. Assume that the statement holds for j ∈ N0, i.e., kj = 3 · 2j − 2. • Inductive Step. We compute kj+1 = 2kj + 2 = 2 · (3 · 2 j − 2) + 2 = 3 · 2j+1 − 4 + 2 = 3 · 2 j+1 − 2. 3 Thus, the statement also holds for j + 1. By the principle of mathematical induction, we have kn = 3 · 2n − 2 for any n ∈ N0. Non-existence of the required strategy: We want to compare the number of pairs Alice can choose and Bob can determine in 12 steps. Once Alice has chosen b, she has b − 1 possibilities for a (the numbers in the set {1, 2, . . . , b − 1}). Thus, the total number of pairs Alice can choose is 200∑ b=1(b − 1) = ( 200∑ b=1 b ) − 200 = 200 · 201 2 − 200 = 19900. In order for Bob’s strategy to allow him to win for any pair of integers chosen by Alice, the tree repre- senting his strategy must have at least 19900 leaves (one for each choice of Alice). If Bob’s statement is true (i.e. he wins after at most 12 turns), this tree has depth at most 12 and therefore at most k12 vertices. Since k12 = 12286 < 19900, the decision tree corresponding to Bob’s strategy cannot have 19900 leaves, hence Bob cannot certainly win in at most 12 attempts. Exercise 5.3 Quick(?) sort (1 point). Recall the pseudocode for the quick sort algorithm from the lecture: Algorithm 1 quick sort 1: function QuickSort(A, ℓ, r) 2: if ℓ < r then 3: k = Partition(A, ℓ, r) 4: QuickSort(A, ℓ, k − 1) 5: QuickSort(A, k + 1, r) 6: function Partition(A, ℓ, r) 7: i ← ℓ 8: j ← r − 1 9: p ← A[r] ▷ Choose the rightmost entry as pivot 10: repeat 11: while i < r and A[i] ≤ p do 12: i ← i + 1 13: while j > ℓ and A[j] > p do 14: j ← j − 1 15: if i < j then 16: Swap A[i] and A[j] 17: until i > j 18: Swap A[i] and A[r] ▷ At the end, the correct place for the pivot is i 19: Return i We want to study the number of comparisons between array entries the quick sort algorithm performs when we apply it to an array A[1 . . . n] consisting of n unique integers which is already sorted in ascending order (so A[1] < A[2] < . . . < A[n]). (a) Show that the number of comparisons T (n) between array entries that QuickSort(A, 1, n) per- forms when applied to a sorted array A as above, and with the above rule to select the pivot satisfies the recursive relation T (1) = 0, T (n) = T (n − 1) + (n − 1) ∀n ≥ 2. 4 You may assume for simplicity that Partition(A, ℓ, r) always performs exactly r − ℓ comparisons between entries. In your argument, refer to the pseudocode above. Solution: We note that, if A is a sorted array with unique entries, then for any ℓ, r with ℓ < r, the function Partition(A, ℓ, r) does not perform any swap operations. Indeed, the pivot A[r] is strictly larger than all other entries of A[ℓ . . . r]. Therefore, at the end of the two while-loops in Partition, the index i will equal r, and the index j will equal r − 1. So, the condition of the if-statement will never be met. After the repeat − until, the index i will be equal to r, and so ‘swapping’ A[i] and A[r] at the end does not change anything. Now, consider the call QuickSort(A, 1, n), with n ≥ 2. First, Partition(A, 1, n) is called. As we have seen, this will return k = n, and make no changes to A. By assumption, it performs exactly n−1 comparisons between entries. Then, QuickSort(A, 1, n−1) and QuickSort(A, n+1, n) are called. The latter call immediately terminates. As A has remained sorted, the former corresponds exactly to calling QuickSort( ̃A, 1, n − 1) on a sorted array ̃A of length n − 1. We conclude that T (n) = T (n − 1) + (n − 1). Finally, we note that the call QuickSort(A, 1, 1) immediately terminates, showing that T (1) = 0. (b) Assume n ≥ 3. Show that T (n) = Θ(n2). To do so, first give an exact expression for T (n) based on the recursive formula of part (a) (your exact expression does not need to be maximally simplified, e.g., it is allowed to contain summation-symbols). Hint: Based on the recursive formula from part (a), how could you write T (n) in terms of T (n − 2)? How could you write it in terms of T (n − 3)? Repeat this process. Solution: By ‘telescoping’ the recursive relation from part (a), and using T (1) = 0, we find that T (n) = T (n − 1) + (n − 1) = T (n − 2) + (n − 2) + (n − 1) = . . . = n∑ i=1(n − i) = n−1∑ j=0 j. Now, we see that ∑n−1 j=0 j ≤ ∑n−1 j=0 n ≤ O(n2). On the other hand, n−1∑ j=0 j ≥ n−1∑ j=⌊n/2⌋ j ≥ n−1∑ j=⌊n/2⌋⌊n/2⌋ ≥ (⌊n/2⌋ − 1)2 ≥ Ω(n2). See also Exercise 1.2. Alternatively, you could find an exact expression for ∑n−1 j=0 j using induction. Guidelines for correction: Award 1/2 point per correctly solved part. (a) It is important that the argument refers to the pseudocode. It is not enough to just claim for instance that Partition makes no changes to A. If the only mistake is that the case T (1) = 0 is not mentioned explicitly, award 1/2 point. (b) The telescoping argument is the most important part. From the exact expression, a very formal argument that T (n) = Θ(n2) is not needed. For instance, a reference to an earlier exercise would suffice. 5 Exercise 5.4 Building a Heap (1 point). Recall that a binary tree is called complete if all of its layers are fully filled, except possibly the last layer, which should be filled from left to right. A (max-)heap is a complete binary tree with the extra property that for any node C with parent P , key(P ) ≥ key(C). (heap-condition) Also recall that for a tree T , the root is at level 0 and the leaves are at level height(T ); for a node at level ℓ, its children are at level ℓ + 1. In this exercise, we formally prove the correctness of the following algorithm from lecture, which converts any complete binary tree into a heap. Algorithm 2 Heap Construction function Heapify(T) for t = height(T ) − 1, . . . , 0 do for nodes N at level t do for ℓ = t, . . . , height(T ) − 1 do C1 ← the left child of N , if no such child exists assign it key −∞. C2 ← the right child of N , if no such child exists assign it key −∞. if key(C1) ≥ key(C2) and key(C1) > key(N ) then Swap the keys of nodes N and C1. N ← C1 else if key(C1) < key(C2) and key(C2) > key(N ) then Swap the keys of nodes N and C2. N ← C2 else Exit inner for loop Let T be a complete binary tree consisting of n nodes with n ≥ 2. Let H be the data structure that results from executing Heapify(T ). (a) Prove that the executing Heapify(T ) returns a valid heap. Hint: Use the invariant I(t) for 0 ≤ t ≤ height(T ): all nodes from levels height(T ), . . . , t satisfy the heap condition, namely key(P ) ≥ key(C) where P is the parent node of level at least t, and C is a child of P . Solution: We prove the invariant in the hint by mathematical induction on t (going in the opposite direction of standard induction). • Base Case. We prove the statement I(t) is true for t = height(T ). We have that all nodes from level height(T ) are leaves and hence are not parent nodes. Thus the invariant holds vacuously. • Inductive Hypothesis. We assume the statement I(t) is true for some t ∈ N, height(T ) ≥ t > 0, i.e. after height(T )− t iterations of the outermost loop. • Inductive Step. We must show the statement I(t − 1) also holds. 6 By the inductive hypothesis all nodes from levels height(T ), . . . , t satisfy the heap condition. Now consider a node N at level t − 1. If it has no children then the node satisfies the heap condition and we are done. Otherwise, let C1 be its left child and C2 its right child if it exists, otherwise we assume key(C2) = −∞. Our algorithm swaps they key of N with the key of the larger child if we satisfy key(C1) > key(N ) or key(C2) > key(N ). Thus N now satisfies the heap condition but the swapped child may not. The innermost for loop however follows the swapped child and repeats this process until either we have processed a node at level height(T ) − 1 or the node satisfies the heap condition. Since nodes at level height(T ) are leaf nodes and not parents, we do not need to consider these nodes. Thus any modified node along these sequence of swaps still satisfy the heap condition and this includes our original node N . Since we do this process on all nodes at level t − 1, this shows that the algorithm after this loop iteration has all nodes from levels height(T ), . . . , t − 1 satisfying the heap condition. Thus I(t − 1) holds. By the principle of mathematical induction, I(t) is true for all t ∈ N, height(T ) ≥ t ≥ 0. In particular, I(0) holds, which means that after the first height(T ) iterations of the outer loop, the nodes from levels height(T ), . . . , 0 satisfy the heap condition. In particular, all nodes satisfy the heap condition. This shows that after height(T ) steps the binary tree is now a heap, which shows correctness of the Heapify algorithm. Guidelines for correction: Award 1/2 point if base case and inductive hypothesis is correct, while inductive step is attempted and on the right track but not fully correct. Award 1 point if inductive step is also correct. (b)* Prove that the runtime of executing Heapify(T ) takes time O(n). You may use the fact that for any k ∈ N k∑ i=1 i 2i ≤ 2 . Hint: Write the runtime as an outer sum over the various levels and an inner sum over all the nodes of that level. Solution: Define f (t) = height(T ) − t. For each node N , the second for loop says that our algorithm processes the subtree rooted at N exactly once. Lets analyze the runtime of the inner most loop on this subtree. Say our node N is at level t, then for its two children, we check if either are bigger than N and if so, swap with the bigger one. Then we make this swapped child our new N and repeat downwards until we either hit the bottom of the tree or our node satisfies the heap condition. In the worst case, we hit the bottom of the tree, and since each intermediate height used only a constant number of operations, we executed O(f (t)) number of comparisons and swaps. Thus for each node N at level t, we use at most O(f (t)) steps. 7 Thus the total runtime is height(T )−1∑ t=0 ∑ N : node of level t O(f (t)) ≤ height(T )∑ t=0 ∑ N : node of level t c · f (t) = c height(T )∑ t=0 ∑ N : node of level t f (t) for some c ∈ R+. To further simplify this formula, we can bound the number of nodes at each level. For a binary tree, we can see that there is at most one node at level 0, namely the root and each node has at most 2 children, so there are at most 2 nodes of level 1. Continuing this reasoning inductively, we can see that there are at most 2t nodes of level t. Thus for a fixed t we can bound the inner sum by ∑ N : node of level t f (t) ≤ 2 tf (t) = 2t+f (t) f (t) 2f (t) = 2 height(T ) f (t) 2f (t) Then our original sum becomes c height(T )∑ t=0 ∑ N : node of level t h(t) ≤ c height(T )∑ t=0 2 height(T ) f (t) 2f (t) = c · 2height(T ) height(T )∑ t=0 f (t) 2f (t) Note that our inner sum is just the hint but with reverse indexing, thus we can bound the sum by 2. This gets us c · 2height(T ) · 2 = 2c · 2height(T ) As height(T ) ≤ ⌊log2(n)⌋, we can see that 2height(T ) ≤ 2⌊log2(n)⌋ ≤ 2log2(n)+1 = 2n. Thus the total runtime can be bounded by 2c · 2n ≤ O(n). Data structures. Exercise 5.5 Implementing abstract data types. In the lecture, you saw how we can implement the abstract data type list with operations insert, get, delete and insertAfter. In this exercise, the goal is to see how we can implement two other abstract data types, namely the stack (german “Stapel”) and the queue (german “Schlange” or “Warteschlange”). The abstract data type stack is, as the name suggests, a stack of elements. For a stack S, we want to implement the two following operations; see also Figure 1. • push(x, S): Add x on top of the stack S. • pop(S): Remove (and return) the top element of the stack S. The abstract data type queue is a queue of elements. For a queue Q, we want to implement the following two operations; see also Figure 2. • enqueue(x, Q): Add x to the end of Q. • dequeue(Q): Remove (and return) the first element of Q. (a) Which data structure from the lecture can be used to implement the abstract data type stack effi- ciently? Describe for the operations push and pop how they would be implemented with this data structure and what the run time would be. 8 x push pop S Figure 1: Abstract data type stack x enqueuedequeue Q Figure 2: Abstract data type queue Solution: We can use a linked list to implement a stack. The elements of the stack are saved as the keys of the linked list in the same order as in the stack, where the first element of the list is the top element of the stack. The operation push(x, S) adds a new element at the start of the list with key x and pointer to the old start of the list. The pointer to the new start of the list is then a pointer to the newly created element. The operation pop(S) accesses the first element of the list (we have a pointer to this element) and returns it. We then set the pointer that saves the start of the list to the pointer that is stored in the current first element. After this, we can delete the first element. For both push(x, S) and pop(S), we only need to do a constant number of operations, so the run time is O(1). (b) Which data structure from the lecture can be used to implement the abstract data type queue effi- ciently? Describe for the operations enqueue and dequeue how they would be implemented with this data structure and what the run time would be. Solution: We can use a doubly linked list to implement a queue. The elements of the queue are saved as the keys of the doubly linked list in the same order as in the queue. We assume that the pointer to the start of the list points to the first element in the queue and the pointer to the end of the list to the last element in the queue. The operation enqueue(x, Q) adds the new element at the end of the list using the pointer to the end of the list. The operation dequeue(Q) accesses, returns and deletes the first element in the queue using the pointer to the beginning of the list. For both operations we then adjust the pointers accordingly, similar as we did in part (a) for the stack. The pointers we need to change are the pointers of the last and the newly added element (for enqueue(x, Q)) and of the second element (for dequeue(Q)) as well as the pointers to the start and end of the list. Since we have pointers in both directions, we can access and change these elements in constant time. Thus, both operations enqueue(x, Q) and dequeue(Q) have run time O(1). 9","libVersion":"0.5.0","langs":""}