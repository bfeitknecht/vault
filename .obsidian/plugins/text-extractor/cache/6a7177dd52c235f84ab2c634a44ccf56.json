{"path":"sem2/AuW/PV/extra/pvw/AuW-pvw.pdf","text":"ETH Zurich Department of Computer Science Algorithms and Probability PVK Skript Author: Marc Himmelberger, Soel Micheletti, Simone Guggiari Preface This script is a summary of the ETH Course Algorithmen und Wahrscheinlichkeit. Some topics are more presented more in depth than others, but we hope you’ll get a good overview about all the important concepts taught in the course. This script only serves as additional material for practice purposes and should not serve as a substitute for the lecture material. We neither guarantee that this script covers all relevant topics for the exam, nor that it is always correct. If an attentive reader ﬁnds any mistakes or has any suggestions on how to improve the scrips, they are encouraged to contact the authors under anw-pvw-skript@vis.ethz.ch or, preferably, through a gitlab issue on https://gitlab.ethz.ch/vis/luk/pvw_script_anw. Contents Contents ii 1 Graph Theory 1 1.1 Recap from Algorithms and Data Structures . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 Concepts and Notation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 Important Graphs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2 Bipartite Graphs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2 Sequences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2 Degree . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 Graph Data Structures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 Trees . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 1.2 Minimum Spanning Tree . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 1.3 Advanced Graph Concepts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 1.4 Blocks and the Block-Cut Tree . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7 1.5 Matchings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8 Perfect Matching . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8 Augmenting Paths . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9 Hopcroft-Karp Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9 Blossom’s Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10 Hall’s Marriage Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10 1.6 Eulerian Tour . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11 1.7 Hamiltonian Cycles . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12 1.8 Travelling Salesman Problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12 2-Approximation Algorithm for the Metric TSP . . . . . . . . . . . . . . . . . . . . . . . . . . 13 1.5-Approximation Algorithm for the Metric TSP . . . . . . . . . . . . . . . . . . . . . . . . . 13 1.9 Graph coloring . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14 1.10 Network Flow . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16 2 Probability Theory 22 2.1 Basic Concepts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22 Inclusion/ Exclusion Principle . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23 Laplace Spaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 Conditional Probability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 Independence of Events . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27 2.2 Discrete Random Variables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29 Independence of Random Variables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30 Expected Value . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31 Variance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32 Multiple Random Variables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33 2.3 Important Discrete Distributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34 Bernoulli Distribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34 Binomial Distribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35 Geometric Distribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35 Negative Binomial Distribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36 Poisson Distribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36 2.4 Coupon Collector Problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36 2.5 Important Inequalities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37 3 Randomized Algorithms 40 3.1 Success Probability Ampliﬁcation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40 Monte Carlo Algorithms: One Sided Errors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41 Monte Carlo Algorithms: Two Sided Errors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41 Monte Carlo Algorithms: Optimisation Problems . . . . . . . . . . . . . . . . . . . . . . . . . 41 3.2 Primality Tests . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42 3.3 Target Shooting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43 3.4 Long Paths . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44 Colorful-Path Problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45 Short Long Path . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46 3.5 Min Cut . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47 Some important facts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47 Basic Version . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47 Bootstrapping . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49 3.6 Hashing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49 Hash Tables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49 Bloom Filters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50 3.7 Smallest Enclosing Circle . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50 Naive Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50 3.8 Convex Hull . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51 Jarvis Wrap . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51 Local Repair . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52 4 Exercise Solutions 53 4.1 Graph Theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53 4.2 Probability Theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54 4.3 Randomized Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59 Graph Theory 1 In the course Algorithms and Data Structures last semester, you had a ﬁrst encounter with graphs: you have seen the deﬁnition, some properties and algorithms such as BFS, DFS, Topological Sorting, algorithms for shortest paths... In this course we go deeper in the topic, and we introduce other exciting concepts. Some of them are exposed in this chapter, others that exploit randomization are presented in Chapter 3. 1.1 Recap from Algorithms and Data Structures Concepts and Notation Graphs are a powerful mathematical tool that allows us to model many diﬀerent problems. The main idea is to deﬁne a collection of elements called nodes or vertices (Knoten), and a relation between them that we call the edges (Kanten). Edges can be directed or undirected, yielding directed or undirected graphs respectively. An example could be to model locations as nodes and the streets between them as edges, or people and the relationship between them (e. g. \"is the father of\" or \"are friends\"). Both nodes and edges can also have additional information associated with them such as color, location, length, capacity, ﬂow and more. If the underlying relationship used for the edges is symmetric (meaning that E is connected to D if and only if also D is connected to E) then the graph is said to be undirected (ungerichtet). Deﬁnition 1.1.1 (Undirected Graph) An undirected graph is a tuple ˝ = (+ , ˆ) where + = {E1, ..., E= }, |+ | = = is the set of nodes, and ˆ = {41, ..., 4< } ⊆ {{D, E}|D, E ∈ + }, |ˆ| = < is the set of edges between them. We call vertices D, E adjacent (benachbart) if and only if D, E ∈ ˆ and we call a vertex E and an edge 4 incident (inzident) if and only if E ∈ 4. If however there is the possiblity of having E in relation to D without D being in relation to E (e. g. father-of relationship) then the graph is called directed (gerichtet). The direction is represented by an arrow. We use some diﬀerent letters to make the distinction easier. Deﬁnition 1.1.2 (Directed Graph) A directed graph is a tuple ˇ = (+ , \u0016) where + = {E1, ..., E= }, |+ | = = is the set of nodes, and \u0016 = {41, ..., 4< } ⊆ + × + , |ˆ| = < is the set of edges between them. 1 Graph Theory 2 Figure 1.1: Simple undirected graph Important Graphs Some graphs are used a lot and deserve their own name and symbol: ▶ A Complete Graph (vollständiger Graph) = has = vertices and every pair of diﬀerent vertices is connected by an edge. ▶ A Circle (Kreis) ˘= is a graph that has = vertices that are connected in one big circle. (A connected graph that has exactly once cycle through any pair of diﬀerent edges) ▶ A Path (Pfad) %= is ˘=, but with any one edge removed. ▶ A Hypercube (Hyperwürfel) of dimension 3, called &3, is a graph with + = {0, 1}3 (bitstrings of length 3) and edges between exactly those vertices that diﬀer at exactly one position. It’s important to remember these, as they can often serve as counterex- amples, even in small sizes. Figure 1.2: The graphs 6, ˘7 and %5 Bipartite Graphs If our graph can be split into two sets of vertices such that there are no edges within those sets then the graph is said to be bipartite (bipartit). (This deﬁnition is easily extended to : independent sets (stabile Mengen) to yield k-partite Graphs). Formally: + = \u0016 ∪ \u0017, two disjoint sets of nodes (\u0016 ∩ \u0017 = ∅) ˆ ⊆ {{D, F}|D ∈ * , F ∈ , } (undirected bipartite graph) ˆ ⊆ (* × ,) ∪ (, × *) (directed bipartite graph) Sequences We can select an ordered sequence of vertices, and based on edges between them and repetition of edges/vertices in our sequence, we 1 Graph Theory 3 Figure 1.3: Simple bipartite graph can distinguish them as follows (note that this is not consistent in the literature, we simply refer to the deﬁnitions in the lecture): Let’s consider the sequence ⟨E1, E2, . . . , E:⟩ ▶ Walk (Weg): a sequence with edges between E8 and E8+1 for all 8 ∈ {1, . . . , : − 1}. The length is : − 1. ▶ Path (Pfad): a walk without repeated vertices. ▶ Closed Walk (Zyklus): a walk where E1 = E: ▶ Cycle (Kreis): a closed walk where all vertices except start and end are diﬀerent ▶ Loop (Schleife): a sequence ⟨E8 , E8⟩. If nothing else is mentioned, we only consider graphs without loops and without multiple edges between the same vertices (note that strictly speaking, our deﬁnitions already enforce that). Here it’s a handy table that summarize these concepts: Restrictions Name Name when closed any vertices Walk (Weg) Closed Walk (Zyklus) distinct vertices Path (Pfad) Cycle (Kreis) A path starting at a given vertex B and ending at another given vertex C is called an s-t-path (s-t-Pfad). Degree The degree (Grad) of a vertex E of a graph is the number of edges incident to the vertex (with loops counted twice). It is denoted 34 6(E) := |#˝(E)|, where #˝(E) is the neighborhood of E (all vertices adjacent to E). For directed graphs, we have the in-degree (Eingangsgrad) deg − ˝(E) and the out-degree (Ausgangsgrad) deg + ˝(E) which are deﬁned as the number of incoming and outgoing edges respectively. Theorem 1.1.1 ∑ E∈+ deg(E) = 2|ˆ| This formula implies the following: ▶ the amount of vertices with odd degree is even ▶ the average degree in a graph is 2< = 1 Graph Theory 4 Graph Data Structures Until now, a graph ˝ = (+ , ˆ) is an abstract data type that can be represented as follows (ignore the weights on the edges, but we’ll use the numbers on each node to refer to that node): But how can we represent a graph in a computer? How can you code algorithms that works with graphs? In order to do this, you need a data structure to represent the graph. Here there are the three most popular solutions. Adjacency matrix We simply create a matrix with |+ | rows and |+ | columns. The entry at position (8, 9) (counting from zero here) has value 0 if there is no edge between nodes 8 and 9, and it has value 1 if there is an edge. In the case of the previous image we would have the following adjacency matrix (again, ignoring weights):           0 1 0 1 1 1 0 1 1 0 0 1 0 1 0 1 1 1 0 1 1 0 0 1 0           Note that if the graph is undirected, the matrix is symmetric, i. e. \u0016) = \u0016. You don’t need to know a lot of runtimes for this course, and most depend on how exactly the data structures are implemented. Important to know are: ▶ Memory space / time to create: O(|+ |2) ▶ Add edge: O(1) ▶ Remove edge: O(1) ▶ Are D and E adjacent? O(1) Adjacency list We create a list of lists. The big list has an entry list for every node (again, a list). The list corresponding to the node E contains all edges that are incident to E. For example, in the case of the previous image, we would have the following adjacency list: 1 Graph Theory 5 {{{0, 1}, {0, 3}, {0, 4}}, {{1, 0}, {1, 3}, {1, 2}}, {{2, 1}, {2, 2}}, {{3, 2}, {3, 1}, {3, 0}, {3, 4}}, {{4, 0}, {4, 3}}} Here, the important runtimes look as follows (assuming edges don’t have references to their copies): ▶ Memory space / time to create: O(|+ | + |ˆ|) ▶ Add edge: O(1) ▶ Remove edge {D, E}: O(34 6(D) + 34 6(E)) ▶ Are D and E adjacent? O(min(34 6(D), 34 6(E))) Obviously, if you have a complete graph (i. e. every node is connected to all other nodes), the runtimes of many operations become the same as the ones for adjacency matrices. The intuition behind using adjacency matrices is that the eﬃciency of many operations is inversely proportional to the number of edges (which, you should remember, is ≤ |+ |2 in the graphs you consider in this course): if the graph has no edges, the operations are very eﬃcient, but if the graph has many edges, the operations have the same eﬃciency they would have in adjacency matrices. Figure 1.4: diﬀerent ways of storing a graph. Adj. Matrix and Adj. List Trees A tree (Baum) ) is a graph on = vertices that satisﬁes the following properties: ▶ it is connected ▶ it has no cycles ▶ it has = − 1 edges Note that if ) satisﬁes two of these properties, it automatically also satisﬁes the third. In this context, a leaf (Blatt) is a vertex with degree 1. Trees also have a few simple properties. (Let ) be a tree with = ≥ 2) ▶ ) contains at least two leaves. ▶ by removing a leaf from ), we obtain a graph ˝′ which is also a tree. ▶ between any two nodes D, E, there is exactly one u-v-path in ). (any graph that satisﬁes this is also a tree!) 1 Graph Theory 6 1.2 Minimum Spanning Tree Given a graph ˝ = (+ , ˆ) and a cost function 2 : ˆ → R that assigns a cost 2(4) to each edge in ˝, a minimum spanning tree (MST) ) over ˝ is a tree nodes + and edges ˆ()), such that the sum ∑ 4∈ˆ()) 2(4) is minimized. This means that among all possible complete trees over ˝, ) minimizes the total cost. Spanning Trees have many applications, for example in networking, where they are applied to prevent forwarding network packets in cycles while maintaining connectivity. 1.3 Advanced Graph Concepts Deﬁnition 1.3.1 A graph is k-connected (k-zusammenhängend) if and only if ▶ |+ | ≥ : + 1, and ▶ ∀- ⊆ + , |- | < :: ˝[+ \\ -] is connected Deﬁnition 1.3.2 A graph k-edges-connected (k-kanten-zusammenhängend) if and only if ▶ |ˆ| ≥ : + 1 ▶ ∀- ⊆ ˆ, |- | < :: (+ , ˆ \\ -) is connected Informally: The graph has at least : + 1 vertices/edges, and we need to remove at least : vertices/edges to make the graph disconnected. Theorem 1.3.1 (Menger) A graph is k-connected if and only if for all D, E ∈ + , D < E there exist at least : internally-vertex-disjoint u-v-paths. (only D, E may appear on multiple paths) A graph is k-edge-connected if and only if for all D, E ∈ + , D < E there exist at least : edge-disjoint u-v-paths. (no edges may appear on multiple paths) Since there are k paths that do not share vertices/edges, we cannot disconnect the graph by removing any : − 1 vertices/edges, which implies that the graph is k-(edge)-connected. Single nodes and edges that disconnect the graph (therefore with : = 1) are called Articulation points (Artikulationsknoten) and Bridges (Brücken) respectively. Figure 1.5: One Bridge and three articu- lation points marked darker in the graph (there are more bridges) 1 Graph Theory 7 Finding articulation points and bridges eﬃciently We can use the following to ﬁnd articulation points and bridges for each connected component of the graph. Note that the search tree we will use now is a directed graph. ▶ perform DFS on ˝ starting at an arbitrary vertex B and assign a number 35 B[E] to each vertex E according to the order they were visited (the ﬁrst vertex gets 0, the next 1, etc.) ▶ compute for each vertex E ;>F[E] = minimum dfs number of all vertices reachable from E via arbitrarily many edges in the search tree and at most one other edge that is in the graph (use DP) ▶ Proﬁt: • E is an articulation point if and only if ∗ E = B and s has degree ≥ 2 in ), or ∗ E < B and ∃F ∈ + with {E, F} ∈ ˆ()) : low[F] ≥ dfs[E]. (So the root is an articulation point if DFS was only able to explore part of the graph before having to return, and the other vertices are articulation points if one of their children in the search tree has a low number at least as big as their dfs number) • {D, E} is a bridge if and only if (D, E) ∈ ˆ()) ∧ low[E] > dfs[D] or (E, D) ∈ ˆ()) ∧ low[D] > dfs[E] (note the diﬀerent orientations of the edge in the search tree). (So an edge can only be a bridge if it’s part of the search tree and the parent node has a strictly lower dfs number than the low number of the child) Figure 1.6: A possible execution of this algorithm. Dark edges are in the search tree, light edges are in the original graph but were not used by DFS, and dark ver- tices are articulations. The notation G/H indicates the dfs and low numbers, respec- tively. Both articulation points and bridges can be found in O(|ˆ|) after DFS (which takes a further O(|+ | + |ˆ|)) with this algorithm. The full algorithm can be found in the lecture script on page 35. 1.4 Blocks and the Block-Cut Tree This section oﬀers a possibly less confusing approach to the problem of ﬁnding bridges and articulation points than above. Given two edges 4, 5 , we say that they are in the same block if and only if there exists a cycle (Kreis) that contains both of them. This gives us the set of all blocks \u0017. There might be some vertices that are incident to edges in diﬀerent blocks (you could say the blocks \"overlap\"), these are exactly the articulation 1 Graph Theory 8 points. This way, we can easily ﬁnd all the set of articulation points % given the blocks \u0017. Theorem 1.4.1 Let ˝ = (+ , ˆ) be a graph. A vertex is an articulation point in ˝ if and only if it is incident to at least two edges that are in diﬀerent blocks of ˝. An edge is a bridge if and only if it is the only edge in its block. Finally, given a graph ˝ = (+ , ˆ), we can create its block-cut tree (Block- Zerlegung) as a graph with vertices \u0017 ∪ % where a block is connected to an articulation point if and only if that point is incident to an edge in that block (and there are no edges between blocks or between articulation points). Though this does not help us with ﬁnding bridges or articulation points, it can be useful for various divide-and-conquer methods. All of this is possible in linear time, i. e. O(|+ | + |ˆ|). 1.5 Matchings A set of edges \" ⊆ ˆ is called matching in a graph ˝ = (+ , ˆ) if no vertex in the graph is incident to more than an edge in \". Formally: 4 ∩ 5 = ∅ for all 4, 5 ∈ \" with 4 < 5 We call all the vertices that are incident to an edge from \" covered (überdeckt) by the matching. \"as big as possible\" If we are interested in computing a matching \" that is \"as big as possible\", we can mean two things: ▶ A matching \" is maximal (inklusions-maximal) if and only if no edge in ˆ \\ \" can be added to the matching without violating the deﬁnition of a matching. It’s easy to ﬁnd, e. g. with greedy strategy by iterating over all the edges (and picking only those who have both vertices still free) in time O(|ˆ|). ▶ A matching \" is maximum (kardinalitäts-maximal) if and only if there is no matching in the graph that contains more edges. The algorithm by Hopcroft & Karp computes a maximum matching in O( √|+ |(|+ | + |ˆ|)). For any maximal matching \" and any maximum matching \"∗, it holds that |\" | ≤ |\"∗| ∧ |\" | ≥ 1/2|\"∗| Perfect Matching A matching \"? is called perfect (perfekt) if and only if |\"? | = |+ | 2 , i. e. every vertex of the graph is incident to exactly one edge of the matching. 1 Graph Theory 9 Not every graph possesses a perfect matching (e. g. no perfect matching can exist if there is an odd number of vertices). Matchings in Regular Graphs If ˝ = (+ , ˆ) is a 2:-regular bipartite graph, then we can ﬁnd a perfect matching in O(|ˆ|). If ˝ = (\u0016⊎\u0017, ˆ) is a :-regular bipartite graph, then there exist \"1, ..., \": such that ˆ = \"1 ⊎ ... ⊎ \": and all \"8 with 1 ≤ 8 ≤ : are perfect match- ings. Augmenting Paths A path in ˝ = (+ , ˆ) is called an augmenting path (augmentierender Pfad) for a given matching \" if and only if exactly every second edge in the path is in \", starting and ending with vertices not covered by \". This means that an augmenting path has odd length. We can use an augmenting path to increase the size of our matching by 1 if we remove all the edges along the path from \" and add the ones previously not contained. (in a way \"swapping\" them) Figure 1.7: Augmenting path once the edges are swapped Hopcroft-Karp Algorithm This algorithm works on a bipartite graph ˝ = (\u0016 ⊎ \u0017, ˆ). This has the advantage that an augmenting path can be found in O(= + <) simply using a slightly modiﬁed BFS. The algorithm works by ﬁnding all the augmenting paths that have minimal length, selecting an inclusion-maximal set of vertex-disjoint paths and augmenting along all of those at once. This repeats until there are no more augmenting paths, implying that the matching is maximum. The runtime is O(√|+ |(|+ | + |ˆ|)). 1 Graph Theory 10 Figure 1.8: The layer structure produced by BFS. Black edges are not in the match- ing, red ones are. White vertices are not covered by the matching, black ones are. Theorem 1.5.1 (Berge) Let \" be a matching in the graph ˝ = (+ , ˆ). \" is a maximum matching in ˝ if and only if there exists no \"-augmenting path in ˝. Blossom’s Algorithm This one works for any graph. It repeatedly ﬁnds augmenting paths and uses those to increase the size of the matching until no more augmenting path exists (meaning the matching is maximum). You don’t need to know how it ﬁnds augmenting paths, just know that it has runtime O(|ˆ| · |+ |2). Hall’s Marriage Theorem Necessary and suﬃcient condition to have a matching that covers at least one side of the bipartite graph. Theorem 1.5.2 (Hall) Let ˝ = (\u0016 ∪ \u0017, ˆ) be a bipartite graph and let #(-) denote the neighbourhood of - ⊆ \u0016 ∪ \u0017 in ˝ (all vertices in \u0016 ∪ \u0017 adjacent to at least one vertex in -). There exists a matching that covers all vertices in \u0016 if and only if ∀, ⊆ \u0016 : |, | ≤ |#(,)| Informally Every subset , has enough adjacent vertices in \u0017. Other- wise, we could not pair up the vertices in , with diﬀerent vertices in \u0017. 1 Graph Theory 11 1.6 Eulerian Tour An Eulerian Tour (Eulertour) is a closed walk that visits every edge in the graph exactly once. Graphs that contain an Eulerian tour are called Eulerian (eulersch). Theorem 1.6.1 A connected graph ˝ = (+ , ˆ) is Eulerian if and only if all vertices in ˝ have even degree. One algorithm to ﬁnd an Eulerian tour is the following: Imagine two runners, a fast and a slow one. They start at an arbitrary vertex. First, the fast runner starts and moves from vertex to vertex, randomly choosing which neighbor to visit next (but not using the same edge twice) until the runner arrives at the start. The fast runner has now traversed a closed walk. Only now, the slow runner starts and moves one step along the closed walk we just found. At the new vertex, we check if there are any edges that the fast runner has not yet traversed. If so, we send the fast runner oﬀ again along one of these unvisited edges. When the fast runner returns, we will have a new closed walk that contains none of the edges we already found. We merge this new walk into the ﬁrst one by inserting it at the current vertex (think of it as a detour). Now, the slow runner makes another step along the newly merged path, checks for unvisited edges and sends oﬀ the fast runner again if it ﬁnds any and so on. The algorithm terminates once we arrive back at the vertex we started at in the very beginning. This algorithm ﬁnds an Eulerian tour in O(|ˆ|). An implementation is given below (the fast runner works like RandomTour and the slow runner like EulerTour) Algorithm 1.1: EulerTour(G,v)1 , ← RandomTour(˝, EBC0AC) 2 EB;>F ← successor of EBC0AC in , 3 while EB;>F is not equal to EBC0AC do 4 if EB;>F has unvisited incident edges 5 , ′ ← RandomTour(˝, EB;>F) 6 Merge , ′ into , 7 // We have , = ,1 + ⟨E⟩ + ,2 and create , = ,1 + , ′ + ,2 8 EB;>F ← successor of EB;>F in , Algorithm 1.2: RandomTour(G,v)1 , ← ⟨E⟩ 2 E8=8C ← E 3 while E is not E8=8C do 4 Choose an arbitrary unvisited edge {E, D} incident to E 5 Add D to , 6 Mark {E, D} as visited 7 E ← D 8 return , 1 Graph Theory 12 1.7 Hamiltonian Cycles In the previous chapter we studied Eulerian Tours, and we have seen a linear time algorithm to ﬁnd an Eulerian Tour. In this chapter, we study an apparently similar problem: ﬁnding a cycle in a graph that uses each vertex exactly once. Although this might seem just a variant of the previous problem, this is very diﬀerent and more diﬃcult. In facts, determining whether a graph contains a Hamiltonian cycle is an NP-complete problem. This means that, given a possible solution, we can check in polynomial time whether it is correct or not. However, it is not known whether a polynomial time algorithm for ﬁnding such a solution exists (and it’s not likely). In the script, there is a dynamic programming approach that solves the problem in exponential time, but we don’t present it here. However, we state some important results about special cases of this problem: ▶ A grid graph with < rows and = columns contains a Hamiltonian cycle if and only if < · = is even. ▶ A bipartite graph with partitions \u0016 and \u0017 can only contain a Hamiltonian cycle if |\u0016| = |\u0017|. ▶ Hypercubes of any dimension 3 ≥ 1 contain a Hamiltonian cycle. ▶ Dirac’s theorem: every graph ˝ = (+ , ˆ) with |+ | ≥ 3 and minimal degree ≥ |+ |/2 contains a Hamiltonian cycle. 1.8 Travelling Salesman Problem The travelling salesman problem (TSP) is a famous generalization of the Hamiltonian cycle problem. A businessman wants to visit = cities, each exactly once, by starting from their own city and returning in the end. He knows the time it takes to travel between every pair of cities, and he wants to pick the shortest cycle possible. Formally: we are given a complete graph = and a function \u0012 : ([=] 2 ) → N0 that assigns a cost to each edge. We want to ﬁnd a Hamiltonian cycle ˘ in = with ∑4∈˘ \u0012 (4) minimal. The problem is of course at least as hard as determining whether a graph contains a Hamiltonian cycle. In fact, if we had an algorithm to ﬁnd the optimal tour, we could decide whether a graph ˝ contains a Hamiltonian cycle in the following way: we build a complete graph with the same number of vertices as ˝ and we give cost zero to the edges in ˝ and cost one to the others. If and only if the optimal tour has weight zero, then ˝ contains a Hamiltonian cycle. In general, in the context of optimization problems such as TSP, one could be happy about ﬁnding a solution that is not worse than \r times the value of the optimal solution. But this is still not feasible: If we had such an algorithm, we could deter- mine whether a graph contains a Hamiltonian cycle just as before (because \r time zero, i. e. the value of the optimal solution if the graph contains a Hamiltonian cycle, is still zero). Hence, ﬁnding an \r-approximation algorithm for TSP has to be at least as hard as ﬁnding a Hamiltonian cycle. 1 Graph Theory 13 For this reason, we introduce a more restricted version of the TSP that can still be useful: the metric TSP (metrisches TSP). The setting is the same as before, but now we also require that \u0012 ({G, I}) ≤ \u0012 ({G, H}) + \u0012 ({H, I}) for all G, H, I This property is called the triangle inequality, and it expresses that going from G to I directly is never longer than taking a detour over H. 2-Approximation Algorithm for the Metric TSP In this section, we explain an algorithm that ﬁnds an answer to the metric TSP that not more than twice as costly as the optimal solution. The runtime of the algorithm is O(=2). The algorithms work as follows: 1. We compute an MST in the graph in O(=2). 2. We create a multi-graph by doubling every edge in the MST. 3. We ﬁnd an Eulerian Tour in that multi-graph (which exists because all degrees are even). 4. We now translate this tour in the multi-graph to a cycle in the original graph: We try to use the same sequence of vertices, but whenever we would visit a vertex for the second time, we instead go directly to the next unvisited vertex in the tour. We can show that the MST has at most the same cost as an optimal solution. Thus, the tour has at most twice that cost, and that’s already enough to conclude that this gives us a 2-approximation for the metric TSP in O(=2). 1.5-Approximation Algorithm for the Metric TSP We can do this in exactly the same way, but we improve step 2. The goal there is to end up in a graph where we can ﬁnd an Eulerian Tour (which is easy) that we can then correct to an answer to the TSP. We notice that the problem is that we might have some vertices with odd degrees in the MST. But, we also know that there is always an even number of vertices with odd degree in any graph. We can now look at a new complete graph with only these vertices, and there we can ﬁnd a perfect matching with minimal cost in O(=3) (you don’t need to know how, this is Satz 1.50 in the lecture script). The new algorithm is: 1. We compute an MST in the graph in O(=2). 2. Find a perfect matching with minimal cost in the subgraph contain- ing only the vertices with odd degree. Then add these edges to the MST to obtain a multi-graph where all vertices have even degrees. 3. We ﬁnd an Eulerian Tour in that multi-graph (which exists because all degrees are even). 4. We now translate this tour in the multi-graph to a cycle in the original graph: We try to use the same sequence of vertices, but whenever we would visit a vertex for the second time, we instead go directly to the next unvisited vertex in the tour. This is a 1.5-approximation, because the perfect matching we ﬁnd can at most have half the cost of an optimal solution to the TSP. 1 Graph Theory 14 1.9 Graph coloring One can solve a lot of problems by ﬁnding a partition of vertices such that edges connect only vertices in diﬀerent partitions. As an example, consider exam scheduling: We have a graph ˝ = (+ , ˆ) where + is the set of exams, and we have an edge {D, E} if and only if a student takes both exam D and exam E; hence we can say that edges represent \"conﬂicts\" (we call these kinds of graphs \"Interference Graphs\" or \"Interferenz-Graphen\"; they are often useful in exams too). Our goal is to ﬁnd the minimal number of partitions (here used as time slots) of this graph such that there are no edges within the same partition (an edge within the same partition would mean that two exams with at least one student in common would take place at the same time). Such a partition can then give us an easy way to schedule exams without conﬂicts. In general, we deﬁne a (vertex) coloring (Färbung) of a graph ˝ = (+ , ˆ) with : colors as a mapping 2 : + → [:] such that 2(D) < 2(E) for all edges {D, E} ∈ ˆ. Moreover, we deﬁne the chromatic number (chromatische Zahl) \"(˝) as the minimum number of colors needed to color ˝. An important example is the case where \"(˝) = 2, such graphs are bipartite. In general, we have: Theorem 1.9.1 A graph ˝ = (+ , ˆ) is k-partite ⇐⇒ it can be colored with no more than : colors (i. e. \"(˝) ≤ :). A classical graph coloring problem is the coloring of maps, where neighbouring lands should be assigned to diﬀerent colors. This problem comes with the assumption that the territory of each land is connected and that lands that touch in a single point can be colored with the same color. An important theoretical result in graph coloring is that every such map can be colored with (at most) four colors. How can we determine the coloring of a graph with the least number of colors? In order to decide, whether a graph is bipartite or not, a BFS is suﬃcient, but how can we extend this to larger chromatic numbers? In general, graph coloring is a diﬃcult problem. Already the question does a given graph ˝ = (+ , ˆ) have \"(˝) ≤ 3? is NP complete. This means that there is (probably) no polynomial time algorithm that computes the chromatic number of a graph. In practice, this means that we have to ﬁnd approximations of the optimal solution. The following algorithm computes a coloring of the vertices of the graph by visiting the vertices in an arbitrary order E1, . . . , E= and assigning to each vertex the lowest color that is not used among its neighbours. Algorithm 1.3: GreedyColoring(G)1 Choose an arbitrary order of the nodes E1, . . . , E= 2 for 8 = 1, . . . , = do 3 2(E8) ← min(N \\ {2(D) | D ∈ #(E8) ∩ {E1, . . . , E8−1}) 1 Graph Theory 15 This can be implemented in O(|+ | + |ˆ|). It is clear that the algorithm returns a valid coloring, because given this construction, the color of a vertex is always diﬀerent from the one of its neighbours. Now the question is, how many colors does the algorithm use in the worst case? Since the algorithm chooses the smallest color that is not yet used in one of the neighbours, we have the worst case scenario when the neighbours of a vertex E8 are already colored with colors 1, . . . , 34 6(E8). In this case, E8 gets the color 34 6(E8) + 1. This means that the algorithm uses at most \u0001(˝) + 1 colors, where \u0001(˝) is the maximal degree of a node in ˝. The number of colors used by the algorithm depends on which sequence of vertices we consider. There is always a sequence that yields to the best solution, but since we don’t know this sequence we can get a worse result. Exercise 1. Given a graph ˝ = (+ , ˆ) and a coloring 2 that uses : colors, show how we could construct a sequence of vertices where the greedy algorithm uses ≤ : colors. Then show why this implies that there is a vertex sequence where the greedy algorithm uses exactly \"(˝) colors for any graph. In general, choosing the correct sequence is not an easy task, and there are various heuristics that could be used to improve our chances. Some additional results are given below: Theorem 1.9.2 (Brooks) Given a connected graph that is neither complete nor a circle of odd length, there is an algorithm that computes a sequence with which we will use at most \u0001(˝) colors in time O(|ˆ|). Complete graphs and circles of odd length are the only graphs where \"(˝) = \u0001(˝) + 1 colors are needed (the coloring is trivial). In all other graphs \"(˝) ≤ \u0001(˝). By extension: Theorem 1.9.3 Let ˝ = (+ , ˆ) be a graph and : a natural number such that any induced sub-graph of ˝ has at least one vertex with degree ≤ :. It holds, \"(˝) ≤ : + 1 and there is an algorithm that computes a coloring with : + 1 colors in O(|ˆ|) Proof. Repeatedly remove one of the vertices with maximum degree and use those to ﬁll in the vertex sequence starting at the end. Since the maximum degree is ≤ : for all the intermediate graphs, we can always color them with : + 1 colors (Brooks) and the next vertex in the sequence may never have more than : neighbors, so it cannot be assigned a color greater than \u0001(˝) + 1. The following theorem shows us that there are also non-complete graphs that need many colors. 1 Graph Theory 16 Theorem 1.9.4 (Mycielski Construction) For all : ≥ 2 there is a triangle- free graph ˝: with \"(˝:) ≥ :. Theorem 1.9.5 A graph ˝ = (+ , ˆ) with \"(˝) ≤ 3 can be colored in O(|ˆ|) with O( √|+ |) colors. The algorithm can be found on page 84 of the lecture script. Applications We already discussed an example where we were able to use colorings in a conﬂict graph to ﬁnd conﬂict-free schedules of events. Colorings can also be applied to the following scenarios: ▶ Map coloring (duh) ▶ Frequency/channel assignment ▶ Register allocation ▶ Sudoku (try this on your own or check out this blog post ∗) It always comes down to some notion of \"conﬂicts\" between entities (expressed as edges) and \"groupings\" of entities (the color assignments). If this makes sense to you, you should be able to apply this easily to many similar problems. 1.10 Network Flow Network Deﬁnition A network (Netzwerk) is a tuple (+ , \u0016, 2, B, C) with ▶ ˝ = (+ , \u0016) a directed graph ▶ 2 : \u0016 → R+ 0 the capacity function ▶ B, C ∈ + , B < C two special nodes, the source (Quelle) and the sink (Senke) This is simply a directed graph with two nodes marked as source and target, and a non-negative value assigned to each edge stating the capacity for transport of that edge. Figure 1.9: Simple network with capacities marked above the edges ∗ https://medium.com/code-science/sudoku-solver-graph-coloring-8f1b4df47072 1 Graph Theory 17 Flow Deﬁnition A ﬂow is a function 5 : \u0016 → R+ 0 with the following conditions: ▶ Capacity constraint (Zulässigkeit): ∀4 ∈ \u0016 : 0 ≤ 5 (4) ≤ 2(4) ▶ Flow Conservation (Flusserhaltung): ∀E ∈ + \\ {B, C} : ∑ D∈+ (D,E)∈\u0016 5 (D, E) = ∑ D∈+ (E,D)∈\u0016 5 (E, D) The ﬁrst condition means that on every edge a non-negative amount of ﬂow not exceeding the capacity of the edge. The second condition states that no ﬂow disappears or appears at any given node other than the source and sink, i. e. ﬂow is conserved in the network. Inﬂow and Outﬂow We can deﬁne the following quantities (they al- ready appear above): ▶ inﬂow(E) := ∑ D∈+ (D,E)∈\u0016 5 (D, E) ▶ outﬂow(E) := ∑ D∈+ (E,D)∈\u0016 5 (E, D) With these quantities we can simplify the second condition to ∀E ∈ + \\ {B, C} : inﬂow(E) = outﬂow(E) Flow Value Let’s ﬁrst deﬁne two corresponding quantities for B and C: netoutﬂow(B) := outﬂow(B) − inﬂow(B) netinﬂow(C) := inﬂow(C) − outﬂow(C) It follows from ﬂow conservation that netoutﬂow(B) = netinﬂow(C). The value (Wert) of a ﬂow is deﬁned as val( 5 ) := netoutﬂow(B) Max-Flow Min-Cut The algorithmic problem we study in this section, is to eﬃciently compute a maximum ﬂow in the graph, i. e. a ﬂow with maximum value. In order to gain some insights about this, we introduce a dual problem. A B − C cut ˘ = ((, )) in a network is a partition of + into (, ) ⊂ + such that ( ∩ ) = ∅, ( ∪ ) = + with B ∈ (, C ∈ ). The capacity of this cut is informally how much ﬂow can at most pass from ( to ) and is deﬁned as cap((, )) = ∑ (D,E)∈((×))∩\u0016 2(D, E) 1 Graph Theory 18 Since a B − C cut limits how much ﬂow can move through the network, we know for every B − C cut ˘ = ((, )) and every ﬂow 5 : val( 5 ) ≤ cap((, )) Theorem 1.10.1 (Max-Flow Min-Cut) In any network # = (+ , \u0016, 2, B, C), there exists a ﬂow 5<0G and a B − C cut ˘ = ((, )) such that val( 5<0G) = cap((, )) We can now be sure that, yes, there is a maximum ﬂow in any network and its value is equal to the minimum ( − ) cut in the graph. We will look to eﬃcient algorithms to compute minimal cuts in Chapter 3, but since there are ﬁnitely many cuts in a graph, we already have a naive algorithm to determine a minimal cut (i. e. enumerating them and keeping the minimum). Further (important!): If we ﬁnd a ﬂow 5 and a ( − ) cut with cap((, )) = val( 5 ), this is a proof that 5 is a maximum ﬂow. Augmenting Paths The idea of the algorithm that we present here is to improve a given ﬂow. In order to do that, we’ll ﬁrst try to look for a path from B to C in the graph, such that the ﬂow on each edge of the path is strictly less than the capacity of the edge. Then, we can increase the ﬂow on each edge on the path by the quantity \u0011 := min4∈ path 2(4) − 5 (4) without violating capacity constraints or ﬂow conservation. Unfortunately, there are sub-optimal ﬂows that can not be improved in this way†. A key observation here is that increasing the ﬂow on an incoming edge can not only be compensated with an increase of the ﬂow on an outgoing edge, but also with the decrease of the ﬂow of another incoming edge. (instead of sending out more ﬂow, we can accept less and still satisfy ﬂow conservation) Residual Network We want to ﬁx the problem above by introducing two new concepts: residual networks and augmenting paths (mind you that these have nothing to do with augmenting paths for matchings!). Given a network # = (+ , \u0016, 2, B, C) with a ﬂow 5 , we deﬁne the residual network (Restnetzwerk) # 5 = (+ , \u0016′, 2′, B, C) as follows: ▶ For each edge 4 ∈ \u0016 with 5 (4) < 2(4) in # (not yet at capacity), we add an edge in the same direction with capacity 2′(4) = 2(4) − 5 (4) (remaining capacity) to # 5 . ▶ For each edge 4 ∈ \u0016 with 5 (4) > 0 in # (there is ﬂow we could potentially decrease), we add an edge in the opposite direction with capacity 2′(4) = 5 (4) to # 5 . This way, for every edge in #, there are edges in the residual network with their capacities representing the \"margins\" of the ﬂow over this edge in #. † for an example, you can take a look at https://de.wikipedia.org/wiki/Algorithmus_ von_Ford_und_Fulkerson#Beispiel 1 Graph Theory 19 We can now apply the same idea as before and try to ﬁnd B − C paths in # 5 , and indeed: given such an augmenting path we can increase the value of 5 as follows: First, calculate \u0011 := min4∈ path in # 5 2′(4). If the augmenting path traverses an edge in # 5 in the same direction as it exists in #, we increase the ﬂow over that edge by \u0011, and if the augmenting path traverses an edge in # 5 in the opposite direction as it exists in #, we decrease the ﬂow over that edge by \u0011. Exercise 2. Prove that a ﬂow 5 ′ constructed in this way from a ﬂow 5 given an augmenting path in # 5 is again a valid ﬂow (in particular, prove that it satisﬁes the capacity constraints and ﬂow conservation) and also show that it has a strictly bigger value. Residual networks are very useful because we can now prove the follow- ing: Theorem 1.10.2 Let 5 be a ﬂow in the network #. 5 is a maximum ﬂow in # if and only if there is no directed B − C path in # 5 Ford-Fulkerson Algorithm This algorithm ﬁnds a maximal ﬂow in a network by repeatedly ﬁnding an augmenting ﬂow in the residual network and using that to increase the ﬂow value. The algorithm is guaranteed to terminate only if the capacities are integers or rational (yielding an integer ﬂow or a rational ﬂow respectively), but not if we allow irrational capacities! ‡ An integer ﬂow (ganzzahliger Fluss) is a ﬂow where 5 (4) ∈ N0 for every edge 4 (and consequently the value of such a ﬂow is also an integer). Similarly, for rational ﬂows. It works as follows: Algorithm 1.4: Ford-Fulkerson1 5 (4) ← 0 for all 4 ∈ \u0016 2 while there is an s−t path % in the residual network do 3 increase ﬂow 5 along % 4 return 5 In networks with integer capacities and without opposite edges: Since in every iteration the ﬂow is augmented by at least 1 unit, and the algorithm stops when we have a ﬂow with maximum value (as only then, no augmenting path exists), we know the algorithm will run through a maximum of val( 5<0G) iterations. Each time we must construct the residual network, and this takes O(|ˆ|). Therefore, the algorithm has a worst case runtime of O(val( 5<0G) · |ˆ|). We can give an upper bound for val( 5<0G) as |+ | · *, where * = max4∈ˆ cap(4). This corresponds to the fact that no ﬂow can be higher than the maximum capacity in the network times the amount of vertices (you could use a B − C cut with ‡ A counterexample is given here: https://faculty.math.illinois.edu/~mlavrov/ docs/412-spring-2018/infinite-loop.pdf 1 Graph Theory 20 ( = {B}). This gives a total runtime of: O(* · |+ | · |ˆ|) For networks with rational capacities, we can simply multiply them all by the largest denominator to get a network with integer capacities. Useful Tricks Flow problems have many application in the real world, most of which have to do with scheduling the use of a ﬁnite amount of resources or ﬁnd some optimal utilization and transport of quantity in a network. The following tricks allow adapting problems that appear to not ﬁt into this model: ▶ We can model Multiple sources/sinks by adding a \"master\" source/sink and connect it to each source/sink that we are given with a big enough capacity (you could again use |+ | · *) ▶ We can model undirected graphs by doubling all the edges and making them directed using the original capacity ▶ We can model capacities at vertices by splitting the vertex into two vertices with only incoming/outgoing edges respectively, and limit the capacity from \"incoming\" to \"outgoing\" using a new edge between them. The following ﬁgure should help remember these tricks. Figure 1.10: The tricks shown graphically Another interesting observation is that we can compute maximum matchings in bipartite graphs with a network ﬂow approach: we add a source B with edges of capacity 1 to each node of the ﬁrst partition, and from each node of the other partition we add edges with capacity 1 to the target. Edges of the bipartite graph have capacity one. By following a maximum integer ﬂow in this graph, we ﬁnd a maximum matching in the underlying bipartite graph. Exercise 3. 1. Sheryl tries to use this algorithm in graphs that are not bipartite as follows: She connects B with capacity 1 to all vertices in the graph, she connects those according to the edges in the graph to a second copy of + and then connects all those vertices in the second copy to C with capacity 1 (so instead of using \u0016 and \u0017 as described above, she uses + twice). Show why this approach does not work (Give an example of a graph where this fails). 2. Show why it’s essential to use an integer ﬂow to ﬁnd the matching. (Give an example of a maximum non-integer ﬂow where it’s not clear how we could construct a maximum matching) This is one of these things where you can easily lose points on an exam even though you might have a perfectly correct idea! 1 Graph Theory 21 3. Argue why it’s always possible to ﬁnd a maximum integer ﬂow in such a network (the correct one, not the one from 1.). Exercise 4. Design an algorithm that, given a graph ˝ = (+ , ˆ) and two vertices D, E, ﬁnds the number of edge-disjoint D − E paths using network ﬂow. (Describe a suitable network, describe how maximum ﬂow values are related to the amount of disjoint paths and prove that your claim is correct) Probability Theory 2 In this course, you had your ﬁrst contact with discrete probability theory, at least with respect to your ETH studies in Computer Science. Probability theory is a wonderful theory that has both mathematical foundations (which you will study in more detail in the course Wahrscheinlichkeit and Statistik in the fourth semester) and practical applications (which you have seen in this course, and you will see in other courses at CADMO or in the area of Machine Learning). In the context of this course, the chapter about probability theory can be divided into two fundamental parts: a mathematical approach to the essential concepts of (discrete) probability and an algorithmic approach to some problems that can be solved by exploiting the idea of randomization. In this chapter, we ﬁrst introduce notions of probability theory which will be useful in the next chapter, where we will exploit them in the context of randomized algorithms. 2.1 Basic Concepts When doing a random experiment (ﬂipping a coin, picking a group of 10 people by chance, trying not to lose all of your money in a casino), there is a set of possible outcomes. Deﬁnition 2.1.1 We describe such an experiment as a probability space (Wahrscheinlichkeitsraum). For us, a probability space consists of two parts: A discrete sample space (Ereignisraum) = {$1, . . . , $= } (that is in turn composed of elementary events (Elementarereignisse) $8 ∈ ) and a probability function (Wahrscheinlichkeitsfunktion) Pr : → [0, 1]. The probability function is a function that measures how \"likely\" an event is, and it has two fundamental properties: ▶ ∀$8 ∈ : 0 ≤ Pr [$8] ≤ 1 ▶ ∑= 8=1 Pr [$8] = 1 A set ˆ ⊆ is called event (Ereignis). The probability Pr [ˆ] is deﬁned as the sum of the elementary events included in ˆ. We also deﬁne the complementary event (Komplementärereignis) ¯ˆ := \\ ˆ. It’s noteworthy that we usually ignore elementary events with probability 0 in this course. When trying to ﬁgure out what the probability space is, it can be useful to notice that every time you run the experiment, exactly one of the elementary events takes place. So if your experiment has multiple \"results\" (rolling two dice at once, selecting 10 people from 100, shuﬄing cards) you need to make sure that one experiment results in exactly one element of the sample space being randomly selected. You can sometimes make a 2 Probability Theory 23 choice about how to best model your experiment and there is not only one correct way, it’s just important to then be consistent. For example, you could model two dice rolled at the same time using 36 diﬀerent ordered 2-tuples (in a way assuming you can distinguish one die from the other, usually the best approach) or using 21 diﬀerent unordered sets with up to two elements (sets with one element could be used if you roll the same number twice) or if you’re only interested in the sum you rolled, you could use the 12 diﬀerent possible sums. Roll 1st option 2nd option 3rd option 3 and 5 (3, 5) {3, 5} 8 5 and 3 (5, 3) {3, 5} 8 4 and 4 (4, 4) {4, 4} = {4} 8 These probability spaces are entirely diﬀerent, but they will not disagree on questions like \"how likely is it to roll a sum of 8\" if you’re consistent when calculating. Exercise 5. Explicitly (and formally) write down these three ways of modelling the experiment and calculate the probability of getting a sum equal to 8 for each one. Which one is most intuitive for you? In general, the probability is a function from the set of events (often denoted 2 or P( )) to the interval [0, 1]. The probability function satisﬁes the following properties (some of those are axioms, others can be easily derived): Pr [ ] = 1 Pr [∅] = 0 Pr [ ¯\u0016] = 1 − Pr [\u0016] \u0016 ⊆ \u0017 ⇒ Pr [\u0016] ≤ Pr [\u0017] Pr [\u0016 ∪ \u0017] = Pr [\u0016] + Pr [\u0017] − Pr [\u0016 ∩ \u0017] Inclusion/ Exclusion Principle If we are interested in the probability of the union of some events, we could use the following: Theorem 2.1.1 If the events \u00161, . . . , \u0016= are pairwise disjoint (i. e. if for all pairs 8 < 9 it holds \u00168 ∩ \u00169 = ∅), then Pr [ =⋃ 8=1 \u00168 ] = =∑ 8=1 Pr [\u00168] 2 Probability Theory 24 But what happens if the events are not disjoint? The general case is covered by the inclusion/exclusion principle, stated in the Theorem 2.1.2. Theorem 2.1.2 (Inclusion/ exclusion principle) For any events \u00161, . . . , \u0016= (= ≥ 2), we have Pr [ =⋃ 8=1 \u00168 ] = =∑ \u0012 =1 (−1) \u0012 −1 ∑ 1≤81<···<8\u0012 ≤= Pr [\u001681 ∩ · · · ∩ \u00168\u0012 ] = =∑ 8=1 Pr [\u00168] − ∑ 1≤81<82≤= Pr [\u001681 ∩ \u001682 ] + ∑ 1≤81<82<83≤= Pr [\u001681 ∩ \u001682 ∩ \u001683 ] − . . . + (−1)=+1 · Pr [\u00161 ∩ · · · ∩ \u0016=] This result is very important and gives the exact result to the problem. However, for a large value of =, it gets tedious to compute. The Union Bound comes to rescue by giving a upper bound to the value of the same probability. Theorem 2.1.3 For any events \u00161, . . . , \u0016= it holds Pr [ =⋃ 8=1 \u00168 ] ≤ =∑ 8=1 Pr [\u00168] For the sake of simplicity, we avoid giving a formal proof of Theorems 2.1.2 and 2.1.3. The intuition for Theorem 2.1.2 can be given with Venn diagrams, for Theorem 2.1.3 just consider that we don’t subtract any overlap and that the intersection of more events becomes strictly less likely. Laplace Spaces We call a probability space with ﬁnitely many elementary events that all have the same probability (namely 1/| |) a Laplace space (Laplace-Raum). This simpliﬁes all the calculations a lot since we only need to worry about how big the events we’re interested in are and not exactly which elementary events comprise them: If we are considering a discrete sample space of cardinality =, then all elementary events have probability 1 = . And we get Pr [\u0016] = |\u0016| = 2 Probability Theory 25 In other words, we describe the probability of an event as the number of \"favorable\" events divided by the total number of \"possible\" events. This reduces complex probability calculations to counting how many favorable and possible outcomes there are. Combinatorics A quick refresher on things you should already know: If we want to count how many ways of picking : out of = elements there are, it matters whether we are allowed to pick the same element twice (repetitions) and whether we consider the order important. The number of diﬀerent ways of picking those elements is best summa- rized in a small table: order matters order does not matter repetition allowed = : (=+:−1)! :!(=−1)! no repetition allowed =! (=−:)! =! :!(=−:)! Exercise 6. Find a real-world example for each of the situations in the table above. Conditional Probability Until now, we considered the case where we don’t have any prior infor- mation regarding the probability space. But what happens if we already have some information about the outcome of an experiment? e. g. our friend could tell us they rolled two fair dice and did not get a sum of 7; it makes sense to say that the event corresponding to a sum of 7 should now have probability 0 Example 2.1.1 Throw a fair dice. You know that the result is an odd number. What is the probability that you got a prime number? The prior knowledge that the result is odd reduces the probability space from {1, 2, . . . , 6} to {1, 3, 5}. Since both 3 and 5 are prime, we get a probability of 2 3 . We can formalize the argument in the following way. Let \u0016 := {1, 3, 5} be the event that the result is odd and \u0017 := {2, 3, 5} the event that the result is prime. We can read the probability of observing a result from \u0017 when we already know that we observed a result from \u0016 as follows: we know that the \"good events\" are the one contained in the set \u0016 ∩ \u0017 and the \"possible events\" are the one contained in \u0016. Hence, we get Pr [\u0017 given \u0016] = Pr [\u0016 ∩ \u0017] Pr [\u0016] In general, given two events \u0016 and \u0017 in a sample space , we want to determine the probability of event \u0017 knowing already that some elementary event in \u0016 happened. We write Pr [\u0017 | \u0016] and we speak of the probability of \u0017 given \u0016 (\u0017 gegeben \u0016). Just as above, we argue that the fact that \u0016 happened leads to a new probability space. This explains the following theorem. 2 Probability Theory 26 Deﬁnition 2.1.2 (Conditional Probability) Let be a sample space and \u0016, \u0017 two events over . We deﬁne Pr [\u0017 | \u0016] = Pr [\u0016 ∩ \u0017] Pr [\u0016] We can rearrange the formula for condition probability in the following way Pr [\u0016 ∩ \u0017] = Pr [\u0017 | \u0016] · Pr [\u0016] This situation (ﬁrst knowing about \u0016 and then ﬁnding out about \u0017) can be graphically represented as a tree: ¯\u0016 ¯\u0016 ∩ ¯\u0017 1 − Pr [ \u0017 | ¯\u0016 ] ¯\u0016 ∩ \u0017 Pr[ \u0017 | ¯\u0016 ] 1 − Pr [\u0016 ] \u0016 \u0016 ∩ ¯\u0017 1 − Pr [\u0017 | \u0016] \u0016 ∩ \u0017 Pr [\u0017 | \u0016] Pr [\u0016 ] Exercise 7. Prove or disprove: For any events \u0016, \u0017 with 0 < Pr [\u0017] < 1, it holds that Pr[\u0016 | \u0017] = 1 − Pr[\u0016 | \u0017] The above tree can be useful to compute other probabilities: Pr [\u0017] and Pr [\u0016 | \u0017]. We ﬁrst compute them in the example above, and then we present two theorems than generalize the concept. ▶ Pr [\u0017] = Pr [\u0016] · Pr [\u0017 | \u0016] + Pr [ ¯\u0016] · Pr [\u0017 | ¯\u0016] Theorem 2.1.4 (Total Probability) If we divide into a disjoint partition of \u00168 for 8 ∈ [<], we have Pr [\u0017] = <∑ 8=1 Pr [\u00169 ] · Pr [\u0017 | \u00169 ] ▶ Pr [\u0016 | \u0017] = (Pr [\u0017 | \u0016] · Pr [\u0016]) /Pr [\u0017] Theorem 2.1.5 (Bayes) If we divide into a disjoint partition of \u00168 2 Probability Theory 27 for 8 ∈ [<] and we consider an event \u0017 with Pr [\u0017] > 0, we get Pr [\u00168 |\u0017] = Pr [\u00168 ∩ \u0017] Pr [\u0017] = Pr [\u0017 | \u00168] · Pr [\u00168] ∑< 9=1 Pr [\u0017 | \u00169 ] · Pr [\u00169 ] Most often, we simply pick \u00161 = \u0016 and \u00162 = \u0016 given some event \u0016. Exercise 8. You are on a TV show and the questions are chosen u.a.r (uniformly at random) from a pool of questions. With probability ? you know the answer, and you answer correctly. Otherwise, you guess, and you give the right answer with probability 1/4. What is the probability that you answer correctly to a randomly chosen question? Exercise 9. You are on another TV show and you have to choose one out of three closed doors. Behind one door (chosen uniformly at random in advance) there is a wonderful race car, behind the other two doors some friendly goats. You choose a door uniformly at random. The TV host opens a door that you have not chosen (he will never open your chosen door) and behind it is a goat (he will never open the door with the car). Then he makes you an oﬀer: You can change your choice to the other closed. Do you have a probabilistic advantage in accepting the oﬀer? First describe a suitable probability space (there are three random choices happening in each experiment), then deﬁne suitable events and then calculate your chance of winning with and without switching. Independence of Events In the previous subsection, we studied the formula for conditional probability. When we consider Pr [\u0017 | \u0016], the prior knowledge that \u0016 happened can increase, decrease or not inﬂuence the probability of \u0016. If the event \u0016 does not inﬂuence the event \u0017, we have Pr [\u0017 | \u0016] = Pr [\u0017]. This fact also implies that \u0017 does not inﬂuence \u0016: Pr [\u0017] = Pr [\u0017 | \u0016] = Pr [\u0016 ∩ \u0017] Pr [\u0016] ⇐⇒ Pr [\u0016] · Pr [\u0017] = Pr [\u0016 ∩ \u0017] ⇐⇒ Pr [\u0016] = Pr [\u0016 ∩ \u0017] Pr [\u0017] = Pr [\u0016 | \u0017] The intuitive concept of independence is the following: two events are independent if they don’t inﬂuence each other. In other words, knowing that an event happened does not give us any additional information about the probability of another event. This last observation motivates the following deﬁnition. Deﬁnition 2.1.3 (Independence of two Events) Two events \u0016 and \u0017 are (stochastically) independent (unabhängig) if and only if one of the following three (equivalent) proposition hold. ▶ Pr [\u0017 | \u0016] = Pr [\u0017] 2 Probability Theory 28 ▶ Pr [\u0016 | \u0017] = Pr [\u0016] ▶ Pr [\u0016 ∩ \u0017] = Pr [\u0016] · Pr [\u0017] How can we generalize the concept of independence of more than two events? Deﬁnition 2.1.4 (Independence of = Events) The events \u00161, . . . , \u0016= are (stochastically) independent if and only if for all subsets ˚ ⊆ {1, . . . , =} with ˚ = {81, . . . , 8: } we have Pr [\u001681 ∩ · · · ∩ \u00168: ] = :∏ 9=1 Pr [\u00168 9 ] We point out that checking independence only for all pair of events does not work, as shown in the following example. Example 2.1.2 Consider the Laplace space = {1, 2, 3, 4} and the following events: ▶ \u0016 := {1, 2} ▶ \u0017 := {1, 3} ▶ ˘ := {1, 4} We have Pr [\u0016] = Pr [\u0017] = Pr [˘] = 1 2 We also have that Pr [\u0016 ∩ \u0017] = Pr [\u0016 ∩ ˘] = Pr [\u0017 ∩ ˘] = Pr [{1}] = 1 4 Hence ▶ Pr [\u0016 ∩ \u0017] = Pr [\u0016] · Pr [\u0017] ▶ Pr [\u0016 ∩ ˘] = Pr [\u0016] · Pr [˘] ▶ Pr [\u0017 ∩ ˘] = Pr [\u0017] · Pr [˘] Thus, the events \u0016, \u0017, ˘ are pairwise independent. Now consider Pr [\u0016 ∩ \u0017 ∩ ˘] = Pr [{1}] = 1 4 But Pr [\u0016] · Pr [\u0017] · Pr [˘] = 1 8 < 1 4 = Pr [\u0016 ∩ \u0017 ∩ ˘] Summarizing: Independence implies pairwise independence, but the opposite direction does not hold. Independence is useful because it allows us to compute the probability of the intersection of events simply by multiplying the individual events. But what can we do to compute the intersection of non-independent events? The next theorem helps us to solve the problem in general. Theorem 2.1.6 (Full Multiplication Rule) For any events \u00161, . . . , \u0016= with Pr [\u00161 ∩ · · · ∩ \u0016=] > 0 we have Pr [\u00161 ∩ · · · ∩ \u0016=] = Pr [\u00161] · Pr [\u00162|\u00161] · Pr [\u00163|\u00161 ∩ \u00162] . . . Pr [\u0016= |\u00161 ∩ · · · ∩ \u0016=−1] 2 Probability Theory 29 For independent events, this theorem reduces nicely to the deﬁnition of independence. Exercise 10. Show that if Pr [\u0016 | \u0017] = Pr [\u0016 | ¯\u0017] then \u0016 and \u0017 are independent. 2.2 Discrete Random Variables Often we are not really interested in the result of a random experiment. What we are really interested in are the consequences of such a result. For example, when we play roulette, we are not really interested in the number that comes out, but in the possible increase or decrease of our money. Example 2.2.1 Consider an unfair coin with a probability of head (H) of 0.4 and a probability of tail (T) of 0.6. We make the following bet: if the coin shows head, we lose $100, otherwise we win $80. We model this idea using a function - that maps every possible result of the random experiment (H or T) to the corresponding gain or loss. We have: -(˛) = − 100 with probability 0.4 -()) =80 with probability 0.6 This example motivates the following deﬁnition. Deﬁnition 2.2.1 (Random Variable) A function - : → ,- ⊆ R is called random variable (Zufallsvariable) - ,- or -( ) = {-($) | $ ∈ } is the set of all possible values - can take. The function 5- : ,- → [0, 1] G 7→ 5- (G) := Pr [- = G] is called probability mass function (Dichtefunktion) of the random variable -. The function ˙- : ,- → [0, 1] G 7→ ˙- (G) := Pr [- ≤ G] = ∑ :∈R :≤G 5- (:) is called cumulative distribution function (Verteilungsfunktion) of the random variable -. The PMF tells us how likely each value of - is while the CDS tells us how likely it is to see a value of - that is at most some value. 2 Probability Theory 30 Example 2.2.2 (Indicator Random Variable) A very important example of random variable is the indicator variable (Indikatorvariable). For an event \u0016 ⊆ , we deﬁne the corresponding indicator variable as ˚\u0016($) = {1 if $ ∈ \u0016 0 otherwise Note that we can use ˙- (G) also to express Pr [- > G] Pr [- > G] = 1 − Pr [- ≤ G] = 1 − ˙(G) There are also conditional random variables (bedingte Zufallsvariablen) - | \u0016 that are only deﬁned on the elementary events of some event \u0016 instead of all of . While this does not change the values - takes given an outcome of the experiment, the distribution of - might change drastically because certain outputs might now be more or less likely. Many concepts from conditional probability can be applied to conditional random variables as well, but if in doubt, you can always refer to E [- | \u0016] = ∑ $∈ -($) · Pr [{$} | \u0016] = ∑ $∈\u0016 -($) · Pr [{$} | \u0016] Independence of Random Variables Deﬁnition 2.2.2 The random variables -1, . . . , -= are independent if and only if for all {G1, . . . , G= } in the codomain of -1, . . . , -= it holds Pr [-1 = G1, . . . , -= = G=] = =∏ 8=1 Pr [-8 = G8] Note that, interestingly enough, here we don’t have to look at subsets of the variables being independent. An important theorem about independent random variables is the fol- lowing. Theorem 2.2.1 Let 51, . . . , 5= be real functions. If the random variables -1, . . . , -= are independent, then so are the transformed random variables 51(-1), . . . , 5=(-=). (Given a real function 6 : R → R and a random variable -, the transformed random variable 6(-) is simply a function that maps $ 7→ 6(-($))) This means that we cannot make independent random variables depen- dent by only applying functions to each variable separately. For example: Given any independent random variables - , ., the variables - - and (log(.) + sin(.)) 1 .2+1 are also independent. 2 Probability Theory 31 Expected Value The expected value is useful to determine the \"average\" outcome of a random experiment (though most of the time that’s not an outcome that can appear in any one experiment). Deﬁnition 2.2.3 (Expected Value) The expected value (Erwartungswert) of a random variable - is E [-] := ∑ $∈ -($) · Pr [$] or sometimes more useful if we don’t know the probability space but only the distribution of -: E [-] = ∑ G∈,- G · 5- (G) It’s possible to show that, if the random variable maps results of a random experiment to natural numbers, an equivalent deﬁnition of expected value is given by E [-] = ∞∑ 8=1 Pr [- ≥ 8] A crucial property of the expected value is linearity of expectation (Linearität des Erwartungswertes). This property is very useful to solve a lot of exercises in this course. Theorem 2.2.2 (Linearity of Expectation) For any two random variables - , . and 0 ∈ R it holds that: E [- + .] = E [-] + E [.] E [0 + -] = 0 + E [-] E [0 · -] = 0 · E [-] We can generalize this to ﬁnitely many random variables: For any random variables -1, . . . , -= and 01, . . . , 0= , 1 ∈ R, we have E [ 1 + =∑ 8=1 08 -8 ] = 1 + =∑ 8=1 08E [-8] This property is particularly useful if a random variable can be expressed as a sum of simpler random variables with known expected value (e. g. indicator variables). Example 2.2.3 We toss a fair coin 100 times. How many heads do we expect? We denote with - the number of heads and for 8 = 1, . . . , 100 we deﬁne -8 as the indicator random variable for head. We get E [-] = E [ 100∑ 8=1 -8 ] = ∑ 8=1 100E [-8] = 100∑ 8=1 1 2 = 50 2 Probability Theory 32 Theorem 2.2.3 For independent random variables -1, . . . , -=, we have E [ =∏ 8=1 -8 ] = =∏ 8=1 E [-8] Exercise 11. Given a graph ˝ with 2= vertices, with = vertices blue and = vertices red. The probability that there is an edge between any two nodes is 1 2 for all pair of nodes. What is the expected number of edges between vertices of the same color? Do you need the existence of those edges to be independent? Exercise 12. In the same situation as above, let - be the random variable deﬁned as the number of edges that stay within a color. Assume now that the existence of the edges are independent events. Calculate E [- 2]. (Write - as a sum of indicator variables ˚1, . . . , ˚< and use the fact that ( ∑< 8=1 ˚8) 2 = ∑< 8=1 ∑< 9=1 ˚8 · ˚9) Variance The expected value E [-] of a random variable - gives some useful information about it, but it does not say how - is \"spread out\". We would \"like\" to be able to distinguish random variables whose values all lie very close to the expected value and ones where the spread is very big. To that end, we introduce another quantity to measure the spread of the distribution around its mean. Deﬁnition 2.2.4 (Variance) For a random variable - with ˘ := E [-], we deﬁne its variance (Varianz) (sometimes written as ˚2) as Var [-] := E [(- − ˘) 2] = ∑ $∈ (-($) − ˘) 2 Pr [$] We also deﬁne the standard deviation ˚ := √Var You can think of the variance as \"average distance from the mean squared\". Computing the variance with the deﬁnition can be quite tedious. The following formula is often easier to use: Var [-] = E [- 2] − E [-]2 Two other important results about variance are given in the following theorems. Theorem 2.2.4 For any random variable - and 0, 1 ∈ R we have Var [0 · - + 1] = 02 · Var [-] 2 Probability Theory 33 (shifting the values has no eﬀect on the spread, but scaling them is even more important) Theorem 2.2.5 For independent random variables -1, . . . , -=, we have Var [ =∑ 8=1 -8 ] = =∑ 8=1 Var [-8] Exercise 13. Given is the following distribution of a random variable - G -4 -2 5 8 10 5- (G) 0.25 0.10 0.20 0.15 0.30 Compute E [2- + 8] and Var [2- + 8]. Multiple Random Variables We might ﬁnd experiments in which we always observe multiple random variables at the same time, such as throwing a die with both number and color on each face. We then obtain a joint probability density function 5- ,.(G, H) = Pr[- = G, . = H] which expresses the probability of observing values G and H for the two random variables - and . respectively. It is possible to extract the individual probability distributions as marginal distributions (Randdichten) 5- (G) = ∑ H∈,. 5- ,.(G, H) And similarly for Pr[. = H]. Theorem 2.2.6 Let - and . be two independent random variables, and / = - + .. We then have 5/(I) = ∑ G∈,- 5- (G) · 5.(I − G) This allows us to compute the density function of a random variable using those of others, which might be helpful in some cases. A special case that can come in very handy in an exam is the following. Deﬁnition 2.2.5 We call the random variables -1, . . . , -= independent and identically distributed (unabhängig und identisch verteilt) - or just i.i.d. - if and only if ▶ they all have the same distribution ( 5-1 = 5-2 = . . . or equivalently ˙-1 = ˙-2 = . . . ) ▶ and they are all independent 2 Probability Theory 34 (We know you didn’t cover this in the course, but you should be able to grasp this concept fairly well if you understand the density function, and it makes it easier to talk about certain things here) Exercise 14. Find an example of a set of random variables that are i.i.d. Find an example of a set of random variables that are independent but not i.i.d. Find an example of a set of random variables that are identically dis- tributed but not i.i.d. Theorem 2.2.7 (Wald’s Identity) Let - and # be two independent random variables, with ,# ⊆ N. If we now deﬁne /($) = ∑#($) 8=1 -8($) where the variables - , -1, -2, . . . , -= are i.i.d. Then it holds that E[/] = E[#] · E [-] This can be used when we have many independent outcomes, we ran- domly select a number # and then sum up the ﬁrst # of the independent outcomes. 2.3 Important Discrete Distributions Following, some of the most common and useful distributions are given, with density, expected value and variance. Bernoulli Distribution A Bernoulli random variable can take two values, 1 and 0 with probability ? and 1−? respectively. This is useful to describe events that either happen or don’t (indicator variables also follow a Bernoulli Distribution). - ∼ Bernoulli(?) 5- (G) =    ? for G = 1 1 − ? for G = 0 0 else E [-] = ? Var [-] = ?(1 − ?) Exercise 15. Compute the variance of - ∼ Bernoulli(?) by hand. 2 Probability Theory 35 Binomial Distribution A Binomial random variable is useful when we sum up multiple inde- pendent(!) Bernoulli random variables that all have the same success probability. For example, we can describe the probability of scoring a certain number of points in a game or look at how many light bulbs are expected to break during delivery. - ∼ Bin(=, ?) = =∑ 8=1 Bernoulli(?) 5- (G) = {(= G) ?G(1 − ?)=−G for G ∈ N0 0 else E [-] = =? Var [-] = =?(1 − ?) Exercise 16. Consider a football league. A victory gives 3 points and a loss 1 points. There is no draw possible. Each team plays 25 games. Consider a team that wins each game independently with probability ? = 0.6. Let - be the random variable for the number of points obtained by the team. Compute E [-] and Var [-]. Geometric Distribution A geometric random variable is used to describe the amount of time or number of events we have to wait for a Bernoulli event to happen. For example, it can be used to estimate the MTBF (mean time between failures), or the expected life of a hard disk before it dies. - ∼ Geo(?) 5- (G) = {?(1 − ?)G−1 for G ∈ N 0 else E [-] = 1 ? Var [-] = 1 − ? ?2 An important property of the geometric distribution is the fact that it is memoryless (gedächtnislos). The fact that an event happened in the past doesn’t change the probability of it happening in the future (for independent variables). Pr[- ≥ B + C | - > B] = Pr[- ≥ C] Exercise 17. Compute the expected value of - ∼ Geo(?) by hand using ∑∞ 8=0 ?8 = 1 1−? . 2 Probability Theory 36 Negative Binomial Distribution The negative binomial distribution is a generalization of the geometric distribution: instead of waiting for the ﬁrst success, we repeat the ex- periment until we have = successes. Of course, if = = 1, we are back to the geometric distribution. But what if = is larger? The random variable - describes the number of repetitions until we see = successes of an independently repeated event with probability ?. - ∼ NegativeBinomial(=, ?) 5- (G) = {(G−1 =−1) ?=(1 − ?)G−= for G ∈ N0 0 else E [-] = = ? Var [-] = =(1 − ?) ?2 Poisson Distribution Let’s say you wanted to model births per week in some city (or any random event in time) as a random variable, knowing that there are on average 6 births per week. Two problems arise if you try to use a Binomial distribution: Firstly, 6 cannot be the ? parameter of the Binomial distribution and secondly, if you do try to do Bin(7, 6/7) (modelling births per day for each of the seven days), it’s impossible to have 8 births per week. Maybe Bin(7 · 24, 6 7·24 ) (modelling births per hour for each hour) is better. If you continue to do this, you will end up (in the limit) with the Poisson distribution. The Poisson distribution is used to model events happening randomly in time when you know the rate of those events (in our case, 6 births per week) and when you assume that a birth in any equally sized time frame is equally likely. The parameter \u0017 now does not take a probability, but rather a rate over time for the interval we’re interested in! - ∼ Po(\u0017) 5- (G) = { 4−\u0017\u0017G G! for G ∈ N0 0 else E [-] = \u0017 Var [-] = \u0017 When we consider the limit lim=→∞ Bin(=, \u0017 = ) we could see that it equals Po(\u0017). 2.4 Coupon Collector Problem Imagine that you have to complete a collection of = items 00, . . . , 0=−1. At each iteration, you receive one object independently and uniformly at random from the set of all objects. The Coupon Collector Problem 2 Probability Theory 37 studies the random variable - that describes the number of iterations until the collection is complete. The key idea to approach the situation is to break it down into phases: phase 8 describes the time between getting our (8 − 1)-st object until we get the 8-th object. Let -8 be the number of iterations in phase 8. We have - = ∑= 8=1 -8. We observe that phase 8 ends, when we get one of the =−8+1 items that we don’t have yet. Hence, -8 has a geometric distribution with parameter =−8+1 = and E [-8] = = =−8+1 . We can now compute the expected value of the Coupon Collector Problem. E [-] = =∑ 8=1 E [-8] = =∑ 8=1 = = − 8 + 1 = = =∑ 8=1 1 = − 8 + 1 We can now observe that this sum is simply 1 = + 1 =−1 + · · · + 1 1 and we can rearrange those terms to give us a sum that’s easier to work with: = = =∑ 8=1 1 8 = = · ˛= Where ˛= is the n-th harmonic number deﬁned simply as ˛= = ∑= 8=1 1 8 . It’s a very useful fact now that we can write ˛= also as ln = + O(1). = = ln = + O(=) And ﬁnally we have the expected number of iterations (asymptotically anyway, but this is ﬁne). Exercise 18. Consider the coupon collector of = elements, starting with 0 elements and ending after collecting =/2 elements. Compute the expected number of rounds until completion. Watch carefully for where you need to do something diﬀerent from above. 2.5 Important Inequalities In the script, you have seen an important example that shows that the expected value describes the mean of the results of several repetitions of a random experiment. However, if we consider a single realization of the random experiment, this value could be quite far from the expected value in some cases. This fact motivates us to introduce three useful inequalities. Theorem 2.5.1 (Markov’s Inequality) Let - be a random variable that takes only non-negative values. Then, for all C ∈ R+, we have Pr [- ≥ C] ≤ E [-] C 2 Probability Theory 38 Proof. We have E [-] = ∑ $∈ -($) Pr [$] ≥ ∑ $∈ ,-($)≥C -($) Pr [$] ≥ C ∑ $∈ ,-($)≥C Pr [$] = C · Pr [- ≥ C] Theorem 2.5.2 (Chebyshev’s Inequality) Let - be any random variable with non-zero variance and C ∈ R+. We have Pr [|- − E [-] | ≥ C] ≤ Var [-] C2 Proof. We have Pr [|- − E [-] | ≥ C] = Pr [(- − E [-]) 2 ≥ C2]. Since the latter is a non-negative random variable, we can apply Markov’s inequality and get Pr [|- − E [-] | ≥ C] = Pr [(- − E [-]) 2 ≥ C2] ≤ E [- − E [-]) 2] C2 = Var [-] C2 Informally, this estimates the probability that the realization of a random variable will be far from the expected value. This is the probability that - 8 [E [-] − C, E [-] + C]. An upper bound is given, that depends on the variance (the larger the variance, the more likely that - will lie far from the mean) and t (the bigger, the less likely it is to ﬁnd a value that far out). We conclude this section by stating, without proof, three other important inequalities known as Chernoﬀ’s bounds. The inequality of Chernoﬀ gives a usually much more precise result than Markov and Chebyshev but is not applicable to all random variables. This is because Chernoﬀ works only for the sum of independent Bernoulli variables with the same parameter ?. Theorem 2.5.3 (Chernoﬀ Inequalities) Let -1, ..., -= be i.i.d. Bernoulli variables with the same parameter ?. We have for - = ∑= 8=1 -8 and every 0 < \u0010 ≤ 1 Pr[- ≥ (1 + \u0010)E [-]] ≤ 4− 1 3 \u00102E[-] Pr[- ≤ (1 − \u0010)E [-]] ≤ 4− 1 2 \u00102E[-] Pr[- ≥ C] ≤ 2 −C for C ≥ 24E [-] Exercise 19. Find an example of a random variable (that can take negative values) where Markov’s inequality does not hold. 2 Probability Theory 39 Exercise 20. Consider a test with 18 questions and two possible answers to each question. If one guesses each answer, what is the probability of answering correctly to at least 5 and at most 13 answers? Give an expression for the exact probability (no need to calculate it). Then use all three inequalities if possible to give bounds on that probability and compare the results. Exercise 21. A factory produces 45’000 screw per day. The screws are supposed to be 30 mm long, but sometimes manufacturing errors add up, and some screws have to be discarded. The probability that any given screw is too short is 1 3 and the probability that a screw is too long is 1 5 (the diﬀerent screws’ lengths are independent). 1. Let - be the number of screws discarded over a whole day. How is - distributed? Calculate the expected value ˘. 2. Tom is a supervisor at the factory and if 0.9˘ < - and - < 1.2˘, he reports to his boss that the day was uneventful. Use Chernoﬀ to give an upper bound on the probability that a given day is eventful. 3. Another factory produces nails and its supervisor tells Tom that their expected number of discarded nails per day is not more than 50. Without knowing anything else about the other factory (except that their nails are also produced independently and have identical chances of being too long or too short), use Chernoﬀ to give an upper bound on the probability that at least 300 nails have to be discarded in a given day. Randomized Algorithms 3 In the course Algorithms and Data Structures and in the ﬁrst chapter of this course you have encountered many algorithms: Karatsuba’s algorithm, dynamic programming algorithms, merge sort, DFS, Kruskal’s algorithm... All these algorithms are deterministic, i. e. given an input they always return the same output. Here we study randomized algorithms (QuickSort is an example you already know), i. e. algorithms that may return diﬀerent outputs in diﬀerent executions. A more formal approach to deﬁne randomized algorithms (which is not absolutely essential for the purpose of this course) could be: randomized algorithm are deterministic algorithms if we consider that their input consists not only of the data relative to the problem, but also of an (inﬁnite long) sequence of random bits. A more intuitive deﬁnition could be that randomized algorithms have access to the (pseudo) random number generator of Java, and hence you can generate samples from a distribution of your choice. Randomized algorithms are beautiful. First, they are often very elegant, and their analysis is often very clean. Second, they are powerful: some problems that are very diﬃcult to solve, such as the NP-complete longest path problem, can be eﬃciently approximated via randomized algorithms (at least for short longest paths). We distinguish two classes of randomized algorithms: Monte Carlo al- gorithms and Las Vegas algorithms. Monte Carlo algorithms have a deterministic runtime, but they can return wrong answers. Las Vegas al- gorithms always return the correct answer, but their runtime is a random variable. Exercise 22. Which of the following Monte Carlo algorithms are useful for a problem that requires a binary answer? ▶ An algorithm that returns the correct answer with probability ≥ 0.6 ▶ An algorithm that returns the correct answer with probability ≥ 0.5 ▶ An algorithm that returns the correct answer with probability ≥ 0.4 Exercise 23. Does the answer to the previous exercise change if we consider a maximization problem? (assuming the result is never too big) Exercise 24. How can we transform a Las Vegas algorithm to a Monte Carlo algorithm? What about the opposite direction? 3.1 Success Probability Ampliﬁcation In this section, we explore some techniques useful to design Las Vegas and Monte Carlo algorithms with certain properties. 3 Randomized Algorithms 41 Monte Carlo Algorithms: One Sided Errors We say that a Monte Carlo algorithm \u0016 for a decision problem has a one-sided error if and only if, for some \u0011 > 0: Pr [\u0016(˚) = .ˆ(] = 1 if the correct answer for input ˚ is YES, and Pr [\u0016(˚) = # $] ≥ \u0011 if the correct answer for input ˚ is NO for any input ˚. Right now, this algorithm has a success probability of \u0011 and whenever it returns NO, we can be sure that NO is the correct answer whereas the answer YES could be wrong. (really think about this if it doesn’t make sense) We can let this algorithm run multiple times to increase its chance of success to 1 − \u0010 for an arbitrary \u0010 > 0: Theorem 3.1.1 Let \u0016 be a Monte Carlo algorithm for a decision problem with one-sided error that has a success probability of \u0011. Let 0 < \u0010 ≤ 1 be a real number. If we repeat \u0016 at least # = \u0011−1 ln \u0010−1 times and answer YES only if every execution of \u0016 returned YES, the resulting algorithm will have a success probability of at least 1 − \u0010. Monte Carlo Algorithms: Two Sided Errors We say that a Monte Carlo algorithm \u0016 for a decision problem has a two-sided error if and only if Pr [\u0016(˚) is correct] ≥ 1/2 + \u0011 for some \u0011 > 0 for any input ˚. Right now, this algorithm has a success probability of 1/2 + \u0011 and both answers could be wrong. It should make sense that we will need to \"vote\" on the correct answer and that we will need more executions to have a high chance of success: Theorem 3.1.2 Let \u0016 be a Monte Carlo algorithm for a decision problem with two-sided error that has a success probability of 1/2 + \u0011. Let 0 < \u0010 ≤ 1 be a real number. If we repeat \u0016 at least # = 4\u0011−2 ln \u0010−1 times and answer with the majority of results from all executions, the resulting algorithm will have a success probability of at least 1 − \u0010. Monte Carlo Algorithms: Optimisation Problems We can also do a similar thing for maximization problems, so long as the algorithm never gives an answer that is bigger than is possible (it always returns valid answers, but not always optimal ones) and has a non-zero chance to return a \"good\" answer (≥ 5 (˚)): 3 Randomized Algorithms 42 Theorem 3.1.3 Let \u0016 be a Monte Carlo algorithm for a maximization problem as above and Pr [\u0016(˚) ≥ 5 (˚)] ≥ \u0011 for some \u0011 > 0 If we repeat \u0016 at least # = \u0011−1 ln \u0010−1 times and answer with the maximum of the results from all executions, the resulting algorithm will output an answer that is not smaller than 5 (˚) with probability at least 1 − \u0010. This might sound confusing, but you can simply pick 5 (˚) to be whatever suits you. e. g. the optimal answer or 50% of the optimal answer, etc. We can use the exact same strategy for minimization problems if we replace \"≥ 5 (˚)\" with \"≤ 5 (˚)\" and return the minimum result instead. 3.2 Primality Tests An easy example for Monte Carlo algorithms are primality tests. These are often used when we need to be fairly sure that a certain big number is prime (e. g. for applications in cryptography). We will look at three diﬀerent ways of testing whether a given number = is prime or not, using a randomly chosen number 0 (1 ≤ 0 < =) that has some (hopefully high) chance of serving as a counterexample if = is not in fact prime. Euclid One of the most obvious ways to check whether = is prime, given a randomly chosen 0, is to see whether they have any factors in common. gcd(0, =) < 1 =⇒ = is not prime Giving us the Monte Carlo algorithm: Algorithm 3.1: EuclidPrimalityTest(n)1 Choose 1 ≤ 0 < = uniformly at random 2 if gcd(0, =) < 1 3 return \"not prime\" 4 else 5 return \"prime\" This algorithm is quite bad as we can’t give a good lower bound on the success probability (it depends heavily on the amount of factors = has) Fermat A slightly less obvious property of primes that you might remember from discrete maths is that = is prime =⇒ ∀0 ∈ {1, . . . , = − 1} : 0=−1 ≡= 1 Which leads us to a second algorithm: Algorithm 3.2: FermatPrimalityTest(n)1 Choose 1 < 0 < = uniformly at random 2 if 0=−1 .= 1 3 return \"not prime\" 4 else 3 Randomized Algorithms 43 5 return \"prime\" This algorithm is better. It has a one-sided error and its success probability is at least 1/2. But, sadly, there are some numbers (Carmichael numbers) that will always be categorized as prime even though they are not. Miller-Rabin So we take the same idea one level further and also remember that = is prime =⇒ the equation G2 ≡= 1 has exactly the solutions 1 and = − 1 We ﬁrst check that 0=−1 ≡= 1. We then observe that = − 1 is even (it has to be if = is prime), so we can apply what we just remembered and check that the square root 0 =−1 2 is indeed either 1 or = − 1. We can keep going like this until we either get = − 1 as a square root or the exponent is not even anymore (in which case we don’t have an equation of the form G2 ≡= 1 anymore). Following this train of thought, we get the Miller-Rabin primality test: Algorithm 3.3: MillerRabinPrimali- tyTest(n) 1 Write = − 1 as 2: · 3 with 3 odd 2 Choose 1 < 0 < = uniformly at random 3 if 03 .= 1 4 for i = 1, \\dots, k do 5 if 028 ·3 ≡= = − 1 6 return \"prime\" 7 return \"not prime\" 8 else 9 return \"prime\" This is ﬁnally good! It has one-sided error, and has a success probability of at least 3/4 for any =. This algorithm is widely used and thanks to the eﬃciency of modular exponentiation it is also remarkably fast. Exercise 25. We stated multiple times that these algorithms have one- sided error. Which answer (\"prime\" or \"not prime\") is always correct? Give an algorithm that has a success probability of at least 99.99% using the Miller-Rabin primality test. 3.3 Target Shooting The goal of this section is to analyze an algorithm to compute |(| |* | , where ( and * are ﬁnite sets with ( ⊆ *. Here we assume that we can generate elements D ∈ * u.a.r. and that we can eﬃciently compute an indicator function ˚((D) which, given an element D ∈ *, tells us whether D is in ( or not. Popular examples of these algorithms include computing the area of an object in a geographic map. The algorithm that we use is simple: We 3 Randomized Algorithms 44 sample # elements from * and we use the samples with the indicator function to estimate the desired ratio. Formally Algorithm 3.4: Target-Shooting1 Choose D1, . . . , D# u.a.r. from * 2 return 1 # ∑# 8=1 ˚((D8) It is intuitively clear that this value approximates |(| |* | and that a larger value # leads to a better result. We want to give quantitative arguments to this idea and give criteria for the choice of the parameter #. Theorem 3.3.1 The expected value of the returned result is equal to the true ratio |(| |* | . Proof. We can deﬁne a variable .8 for each sample D8 that indicates whether or not D8 ∈ (. .8 is Bernoulli distributed with parameter |(| |* | . We return 1 # ∑# 8=1 .8, which has expected value |(| |* | . The variance of the same random variable is 1 # ( |(| |* | − ( |(| |* | ) 2): this tells us that larger values of # lead to an approximation closer to the expected value, and hence closer to the true result. We want to ﬁnd a value of # such that the relative error of our answer is less than some \u0011 > 0, i. e.: Pr [| | | |. − |(| |* | | | | | ≤ \u0011 |(| |* | ] ≥ 1 − \u0010 for a given \u0010 > 0. Theorem 3.3.2 Let \u0010, \u0011 > 0. If # ≥ 3 |* | |(| · \u0011−2 · log( 2 \u0010 ), then the output of the Target-Shooting algorithm is in the interval [(1 − \u0011) |(| |* | , (1 + \u0011) |(| |* | ] with probability 1 − \u0010. 3.4 Long Paths In the course Algorithms and Data Structures you have seen multiple algorithms to compute shortest paths. At this point, a natural question is: Can we modify those algorithms in order to compute longest paths? We start by looking at a special case, directed acyclic graphs. Computing longest paths in this situation is easy: we just multiply all the weights by minus one, and we use the tools from the previous semester. For most graphs, this transformation is not useful because it creates cycles of negative length in the graph. But in the case of directed acyclic graphs, then no negative cycles can be created. In the general case, however, this problem is NP complete and hence it is at least plausible to think that no polynomial time algorithm exists for this task. But this does not mean that we have to give up completely. For example, we might want to solve the problem of deciding whether 3 Randomized Algorithms 45 a graph ˝ has a path of length (at least) \u0017 for small \u0017, concretely for \u0017 ∈ O(log =). In order to solve this task, we ﬁrst introduce another problem, the colorful-path problem, and then we show how an algorithm for this new problem can be used as a subroutine for a randomized algorithm for the long path problem that will be relatively eﬃcient for small \u0017. Colorful-Path Problem Let : ∈ N. We color a graph ˝ = (+ , ˆ) with the function \u000f : + → [:] (where \u000f describes an arbitrary assignment of colors, e. g. neighbors can get the same color). We say that a path is called colorful (bunt) if and only if all its vertices have diﬀerent colors. We now deﬁne the colorful-path problem: given a graph ˝, an integer : and a color function \u000f, decide whether there is a colorful path of length : − 1 in ˝ colored with \u000f. In order to determine whether the colored graph contains a colorful path of length : − 1, we deﬁne the following quantity %8(E) := {( ∈ ( [:] 8 + 1 ) | | | | there exists a colorful path of length 8 that ends in E with exactly the colors in ( } We can easily initialize %0(E) = {{\u000f(E)}} for all E ∈ + This is useful because there is a colorful path of length : − 1 in ˝ if and only if ⋃E∈+ %:−1(E) < ∅. Hence, to solve the colorful-path problem, we compute %:−1(E) for all vertices E and we check, whether these sets are all empty or not. The natural question now is how to compute %8(E) given E and all %8−1(E). We can do this by looking at the colors that we could use to get to a neighbor G of E and checking whether \u000f(E) is already used or not: %8(E) = ⋃ G∈#(E){' ∪ {\u000f(E)} | ' ∈ %8−1(G) and \u000f(E) 8 '} Since we have the base case for 8 = 0, we can implement this using a dynamic programming approach with increasing values of 8. Algorithm 3.5: ColoredPath(G, i)1 for all E ∈ + 2 %8(E) ← {} 3 for all G ∈ #(E) 4 for all ' ∈ %8−1(G) with \u000f(E) 8 ' 5 %8(E) ← %8(E) ∪ {' ∪ {\u000f(E)}} Algorithm 3.6: ShortLongPath(G)1 for all E ∈ + 2 %0(E) ← {{\u000f(E)}} 3 for 8 = 1, . . . , : − 1 3 Randomized Algorithms 46 4 ColoredPath(˝, 8) 5 return whether ⋃E∈+ %:−1(E) < ∅ Since ColoredPath(G,i) has complexity O (∑ E∈+ 34 6(E) · (: 8 ) · 8 ) ∈ O ((: 8 ) · 8 · <) our ﬁnal algorithm has complexity O ( |+ | + :−1∑ 8=1 ((: 8 ) · 8 · <) + |+ | ) = O(2: :<) which is polynomial if : ∈ O(log =). Short Long Path Now that we have a deterministic algorithm for the colorful-path problem (which is polynomial for : = O(log =)), we go back to our original problem of determining whether ˝ contains a path of length \u0017. We use the following Monte Carlo algorithm: 1. Set : = \u0017 + 1 and color ˝ randomly with : colors. 2. Use the algorithm of the previous section to ﬁnd a colorful path with : vertices. Repeat these ﬁrst two steps \u0017 · 4 : times. 3. If at least one of the repetition of the previous step found a colorful path with : vertices, we have a long path of length \u0017. Otherwise, we return that no such path exists. The runtime of the Monte Carlo algorithm is simply given by multiplying the complexity of the algorithm of the previous section with the number of repetitions. We get O (\u0017 · 4 : · 2: :<) which is polynomial if : = O(log =). For the success probability, we observe the following: ▶ If there is no path with \u0017 + 1 vertices, the success probability is one (we will never answer YES). ▶ If there is a path with \u0017 + 1 vertices, we fail only if in every iteration our random coloring is unlucky (we always have some color twice on this path=. The probability of coloring the path of length \u0017 + 1 with : diﬀerent colors is :! : : ≥ 4−:. With this argument, it is easy to see that the failure probability is at most (1 − 4−:)\u0017·4 : ≤ (4 4−: )\u0017·4 : ≤ 4−\u0017. 3 Randomized Algorithms 47 3.5 Min Cut In this section, we consider undirected multigraphs ˝ = (+ , ˆ) without loops. We say that a set ˘ ⊆ ˆ is a cut (Kantenschnitt) in ˝ if and only if ˝′ = (+ , ˆ \\ ˘) is disconnected. We are interested in ﬁnding a cut of minimum cardinality, a so-called min cut. With ˘(˝) we denote the size of such a minimum cut in ˝ (note that the minimum cut does not have to be unique). Some important facts Throughout this section, we repeatedly use an operation called edge contraction (Kantenkontraktion). Let ˝ be a multigraph and let 4 = {D, E} be an edge of ˝. The contraction of 4 means that we \"glue\" D and E together into a single new vertex, removing the edges between D and E, but keeping all other edges incident to D, E, simply redirecting them to the new vertex. The resulting graph is denoted by ˝/4. This is illustrated in the following ﬁgure: Every contraction reduces the number of nodes by exactly one and the number of edges by at least one. Lemma 3.5.1 Let ˝ be a multigraph and 4 an edge of ˝. Then ˘(˝/4) ≥ ˘(˝). Moreover, if there exists a minimum cut ˘ in ˝ such that 4 8 ˘, then ˘(˝/4) = ˘(˝). This means that contracting edges can never decrease the size of a minimum cut. Lemma 3.5.2 Let ˝ = (+ , ˆ) be a multigraph with = vertices. Then the probability that we preserve the size of a min cut (˘(˝) = ˘(˝/4)) for a uniformly randomly chosen edge 4 ∈ ˆ is at least 1 − 2 = . Basic Version In this section, we assume that the input graph is connected. We assume that the graph is represented in a way such that we can: ▶ perform an edge contraction in O(=) ▶ choose an edge uniformly at random among all edges of the current multigraph in O(=) 3 Randomized Algorithms 48 ▶ ﬁnd the number of edges connecting two given vertices in O(1) The idea is very simple: we consider a graph with two vertices as base case (i. e. in this case we count the number of edges between the two vertices, and we return the size of the min cut). If we have more than two vertices we repeatedly choose a random edge of the current graph, and we contract it, until only two vertices are left (in which case a minimum cut has to remove every edge, giving us an easy answer). Algorithm 3.7: Cut(G)1 while ˝ has more than two vertices do 2 4 ← u.a.r. edge from ˝ 3 ˝ ← ˝/4 4 return number of edges left over The runtime of this algorithm is O(=2). By using the observations of Lemma 3.5.1, we see that this algorithm always returns a number at least as large as ˘(˝). If ˘ is a minimum cut in the input graph ˝, and if we never contract an edge of ˘ during the whole algorithm, then the returned number is exactly ˘(˝). In general, we note that the algorithm is correct if and only if: ▶ ˘(˝) = ˘(˝/4) for the ﬁrst contracted edge 4 (which has a proba- bility of at least 1 − 2 = ) ▶ Cut succeeds for ˝/4 Which gives us the following recurrence relation for the success proba- bility ?(=) (deﬁned as the worst-case success probability for graphs with = vertices): ?(=) ≥ ( 1 − 2 = ) · ?(= − 1) From which we get: ?(=) ≥ = − 2 = · = − 3 = − 1 · = − 4 = − 2 . . . 2 4 · 1 3 · ?(2) ︸︷︷︸ =1 = 2 =(= − 1) In order to get an algorithm with an arbitrary good success probability we use a Monte Carlo approach by running Cut # times and return the smallest cut size found in all these runs. If the returned size is not correct, it means that the algorithm failed # times in a row. The failures in diﬀerent runs are independent, and hence the probability of # failures in a row is bounded by: ( 1 − 2 =(= − 1) ) # ≤ 4− 2# =(=−1) where we used the inequality 1 + G ≤ 4 G. If we set, for example, # = 10=(= − 1), then the failure probability is bounded above by 4−20 (which is very small). By increasing the number of repetitions #, the failure probability can be further decreased. Altogether, by setting # = 2 · =2we have an algorithm with runtime O(=4) (because we have O(=2) repetitions 3 Randomized Algorithms 49 of the O(=2) algorithm) and arbitrary small constant probability of failure. Bootstrapping Let’s develop a better algorithm and let’s call it BetterCut. The probability of error in Cut increases with the number of contractions (i. e. with the ﬁrst contraction we have an error probability of 2 = , but the last iteration has an error probability of 2 3 ). Why should we take the risk of contracting edges until we have only two vertices? It is better to do less contractions (let’s say, until the graph has C nodes, where C has to be chosen carefully) and then moving on in a way that is perhaps slower but has a better success probability. What algorithms can we use in order to calculate the min cut when we have reached the threshold C? Well, once we have a constant number of vertices left (for example 6 because why not), we can use a deterministic approach that would then have constant runtime and certainly be correct. But what do we do until we get there? We can just call BetterCut again on our smaller graph with C nodes! If we do this multiple times, we can amplify the success probability just like we did in the beginning of this chapter. Suﬃce to say, we can use a recursive algorithm to achieve a runtime of O(=2 log:(=)) for a constant :, which is practically O(=2). 3.6 Hashing Hash Tables A hash table is a data structure used to implement an associative array, a structure that can map keys to values. If we have many possible keys, implementing this with an actual array is extremely costly. Linked Lists come to mind, but we can do better! A hash function \u0011(:) is used to map an arbitrary type of key (string, number, etc) to an index of the array, also called bucket or slot (we could for example take the hash modulo our array size to determine the bucket). The idea being to have much fewer buckets than possible keys, but enough so this approach is still better than a linked list. Ideally, the hash function assigns to each key a unique value, but often this is not possible. This causes collisions, when two or more keys map to the same index. The better the hash function and the more buckets we use, the lower the number of collisions. There are many ways to handle collisions. One is to save a linked list for each bucket, and if we get a collision, we simply append the new key-value pair to the list in that bucket. 3 Randomized Algorithms 50 Bloom Filters Another application of hash functions are Bloom Filters. Those are data structures whose job it is to save a boolean value for each key. Instead of using an array and a single hash function to calculate an index into the array where we read the value, Bloom Filters use multiple hash functions at once. When we insert an element into the Bloom Filter, we run it through all hash functions and calculate indices into our array and write a 1 into each of those positions. When we later ask ourselves whether we have already seen a certain key, we simply check whether all corresponding entries in the array are 1. Note that Bloom Filters cannot remove elements (so they are usually cleared periodically), and can give false positives (say that we have seen an element when we have not). But they are still very useful when we want to quickly determine whether we have a certain web resource cached, for example. 3.7 Smallest Enclosing Circle Deﬁnition 3.7.1 (Smallest Enclosing Circle) Given a set of points % in the plane with |%| = = (i. e. % = {?1, ..., ?= }, ?8 = (G8 , H8) ∈ R2) the Smallest Enclosing Circle problem asks us to ﬁnd a circle ˘(%) with minimum possible radius A such that all points are contained in this circle. The following holds: ▶ points are allowed to be on the boundary ▶ the smallest enclosing circle is unique ▶ for any set % with |%| ≥ 3, there exists a subset & ⊆ % with |&| = 3 such that ˘(%) = ˘(&). The points in & determine ˘ uniquely. Naive Algorithm A trivial (and ineﬃcient algorithm) is to simply go over all possible sets &, compute the enclosing circle in constant time, check if it contains all points, and if so return it. In this case, we are guaranteed to have found the smallest enclosing circle. However, the algorithm has runtime O(=4). Algorithm 3.8: Naive smallest enclosing circle - \"CompleteEnumeration\" 1 for all & ∈ (% 3) do 2 compute ˘(&) 3 if % ⊆ ˘(&) 4 return ˘(&) There are two more algorithms (\"SmartEnumeration\" and \"Randomized_PrimitiveVersion\") that are not particularly interesting or fast. 3 Randomized Algorithms 51 Randomized Algorithm A better algorithm is to pick points randomly. Every time a point is found to be outside the circle, we know that with higher probability it will be on the border instead of those that are contained. Therefore, we increase the probability of picking it in the future by duplicating it. Algorithm 3.9: Randomized smallest en- closing circle - \"Randomized_CleverVer- sion\" 1 while true 2 pick & ∈ ( % 11) uniformly at random 3 compute ˘(&) 4 if % ⊆ ˘(&) 5 return ˘(&) 6 double all points outside ˘(&) The algorithm computes the smallest enclosing circle in expected time O(= log =). The drastic diﬀerence comes from the fact that we make points that are unlikely to be inside ˘(&) exponentially more likely to be chosen. We pick 11 to strike a balance between having too many points outside ˘(&) (and having to deal with a lot of points) and having a bad runtime because calculating ˘(&) takes too long. 3.8 Convex Hull Deﬁnition 3.8.1 A set ( is called convex if and only if, for every two points ?1, ?2 contained in the set (, all the points on the straight line connecting the two points are also included in the set (. Formally ( convex ⇐⇒ ∀?1, ?2 ∈ (, C ∈ [0, 1] : ?1 + C(?2 − ?1) ∈ ( Given a set of points %, the convex hull 2>=E(%) of % is the smallest convex set that contains %. In the plane, the convex hull is always delimited by straight lines between points in %. We describe a convex hull by the points in % that are used in the border, starting anywhere and moving counterclockwise around the set. Figure 3.1: Convex hull of a set of points Jarvis Wrap For simplicity, we here assume that all points are in general position (allgemeiner Lage), meaning that no three points on the same line and no two points have the same x-coordinate. The idea of the algorithm is to start with a point ?0 guaranteed to be on conv(%) (e. g. the left-most point) and iteratively looking for a next point such that all other points are left of the connecting line to the next point, 3 Randomized Algorithms 52 which guarantees that it will also be on conv(%). This point can be found in O(=) by iterating through the other points and keeping the one which is the rightmost (this is FindNext in the lecture script). Informally, the algorithm proceeds as follows: 1. Select point ?0 that lies on convex hull (e. g. lowest x-coordinate) 2. Find next point ?8+1, such that all other points lie left of the line going through ?8, ?8+1. 3. Repeat until you reach ?0 again This algorithm computes the convex hull in O(= · \u0011), where \u0011 is the number of points that lie on the convex hull’s edge. This because the algorithm takes O(=) for each iteration, and it requires \u0011 loop iterations. This time isn’t optimal, and there are algorithms that perform much better. Moreover, some modiﬁcations are required to address our general position assumption, but the runtime does not change. Local Repair LocalRepair is another algorithm for the convex hull problem. Here the idea lies in starting with a polygon without self-intersections that contains all of % and progressively ﬁxing parts of it that are not convex. During the algorithm, we’ll keep as invariants that we never have self-intersections and that no points are below the vertex sequence from ?1 to ?= and none are above the sequence from ?= to ?1. If we then manage to construct a locally convex polygon (only left turns), we have constructed the convex hull. Note that both of the invariants are necessary for this to work, oth- erwise we might be locally convex but not globally convex (due to self-intersections and \"loops\") or we might not enclose all points. The algorithm proceeds as follows: 1. Sort all the points in % by ascending x coordinate to give ?1, . . . , ?= 2. Start with the polygon deﬁned by the border ⟨?1, . . . , ?=−1, ?= , ?=−1, . . . , ?2⟩ 3. Start at ?1 and move towards ?=. Every time we make a right turn (?8+2 is right of the line through ?8 , ?8+1), we remove the middle vertex (?8+1) and check again for possibly newly created right turns starting at the previous vertex (?8−1). 4. Once we arrive at ?=, we repeat the same process to go back towards ?2. This algorithm has runtime O(= log =) including sorting and runtime O(=) if the vertices are already sorted. Examples and illustrations as well as an implementation can be found in the lecture script on page 211. Exercise Solutions 4 4.1 Graph Theory Ex 1 We use the given coloring 2 as follows: Let ˘8 be the set of all vertices colored with 8, then the sequence we use is ˘1, ˘2, ˘3, . . . , ˘: (order of the elements in those sets doesn’t matter). This works because the greedy algorithm will assign the smallest color not in use by neighbors. Thus, we will always be able to use the color from 2 (no neighbors are colored the same) or a smaller one (because bigger colors have not yet been used). Which gives us a coloring with ≤ : colors. Ex 2 We won’t provide a solution here, as there could be many valid proofs. Just show us your solution if you’re unsure. It’s probably easiest to prove ﬂow conservation for each vertex using two nested case distinctions about whether inﬂow increases or decreases and whether outﬂow increases or decreases after augmentation. The capacity constraints should follow from the deﬁnition of \u0011 and the capacities in # 5 . The strictly bigger value can be proved by an increase of netoutﬂow in B. Ex 3 1. Consider the graph ˘3. A maximum ﬂow in the network Sheryl builds has value 3, indicating that a perfect matching should exist. However, a maximum matching in this graph has only cardinality 1. 2. Consider the graph ˝ = ({0, 1, 2, 3}, {0, 1} × {2, 3}) and the net- work build (correctly) from it. A maximum matching has cardinality 2 and the maximum ﬂow also has value 2 - so far so good. One way to get a ﬂow value of 2 is to assign edges from B to 0, 1 ﬂow 1, each edge between 0, 1 and 2, 3 ﬂow 0.5 as well as each edge from 2, 3 to C ﬂow 1. This is a maximum ﬂow that is not an integer ﬂow. 3. Because we have only integer capacities, a maximum integer ﬂow exists. It could be found, for example, using Ford-Fulkerson. Ex 4 The network is # = (+ , \u0016, 2, D, E) where 2(4) = 1 for all edges 4 and \u0016 is built by adding two edges for each edge in ˆ, one for each direction. The amount of edge-disjoint D − E paths is given by the value of a maximum ﬂow. 4 Exercise Solutions 54 You can prove this by looking at an integer maximum ﬂow and showing how to construct the edge-disjoint paths from it. And also showing how to construct a maximum ﬂow from all the edge-disjoint D − E paths. 4.2 Probability Theory Ex 5 1. = [6]2, ∀$ : Pr[$] = 1/36 2. = ([6] 2 ) ∪ [6], Pr[$] = |$|/36 3. = {2, 3, . . . , 12}, Pr[$] = 6−|7−$| 36 Ex 6 Repetition allowed, order matters: Rolling an =-sided die : times. Repetition allowed, order doesn’t matter: Choosing which : items to buy from a store that has unlimited amounts of = diﬀerent items. No repetition allowed, order matters: The number of possible ﬁrst : places on a scoreboard of = people. No repetition allowed, order doesn’t matter: Drawing : cards from a deck of = cards. Ex 7 Let \u0016, \u0017 be independent events with Pr[\u0016] < 1/2, because Pr[\u0016 | \u0017] = Pr[\u0016] = Pr[\u0016 | \u0017]. For example, we could use a fair six-sided die and the events \u0016 = {3, 6} and \u0017 = {1, 2, 3}. Ex 8 We use Total Probability: Pr[2>AA42C] = Pr[2>AA42C | :=>F=] · Pr[:=>F=] + Pr[2>AA42C | :=>F=] · Pr[:=>F=] = 1 · ? + 1 4 · (1 − ?) = ? + 1 4 − 1 4 · ? = 3 4 ? + 1 4 Ex 9 We deﬁne the probability space by looking at all possibilities for the car position (2), our guess (6) and the host’s choice of door to open (\u0011): = {(2, 6, \u0011) ∈ [3] 3 | \u0011 < 2 ∧ \u0011 < 6} Because the host sometimes has no choice and sometimes chooses the door with probability 1 2 , we get: Pr[(2, 6, \u0011)] = { 1 9 if 2 < 6 1 18 if 2 = 6 4 Exercise Solutions 55 So for example, say we choose door 2 and the host opens door 3 (you could also do this more generally), we look at the conditional probability of ﬁnding the car behind our door: Pr[2 = 2 | 6 = 2 ∧ \u0011 = 3] = Pr[2 = 2 ∧ 6 = 2 ∧ \u0011 = 3] Pr[6 = 2 ∧ \u0011 = 3] = Pr[{(2, 2, 3)}] Pr[{(1, 2, 3), (2, 2, 3)}] = 1/18 3/18 = 1 3 Ex 10 We work backwards and try to stay fairly close to the terms in the question. We ﬁrst notice that two events are independent if Pr[\u0016] = Pr[\u0016 | \u0017]. To get Pr[\u0016] using Pr[\u0016 | \u0017] and Pr[\u0016 | \u0017], we use Total Probability: Pr[\u0016] = Pr[\u0016 | \u0017] · Pr[\u0017] + Pr[\u0016 | \u0017] · Pr[\u0017] And using the assumption given in the question: Pr[\u0016] = Pr[\u0016 | \u0017] · Pr[\u0017] + Pr[\u0016 | \u0017] · Pr[\u0017] = Pr[\u0016 | \u0017] · (Pr[\u0017] + · Pr[\u0017]) = Pr[\u0016 | \u0017] · 1 And that concludes the proof. Ex 11 We ﬁrst deﬁne a random variable - as the number of edges within both colors. We deﬁne an indicator random variable -8 for each of the =(= − 1) edges that run within colors. Each indicator random variable has expected value 1 2 and we can write - as their sum. Thus: E[-] = E [=(=−1)∑ 8=1 -8 ] = =(=−1)∑ 8=1 E[-8] = =(= − 1) 1 2 = =(= − 1) 2 We used Linearity of Expectation, but not independence here. Ex 12 We deﬁne - just as above and use the hint: E[- 2] = E       (=(=−1)∑ 8=1 -8 ) 2      = E [=(=−1)∑ 8=1 =(=−1)∑ 9=1 -8 · -9 ] = =(=−1)∑ 8=1 =(=−1)∑ 9=1 E [-8 · -9 ] To calculate the expected value, we have to diﬀerentiate two cases: When 8 < 9, the two indicator random variables are independent (it says so in the question) and we can do E [-8 · -9 ] = E [-8] · E [-9 ]. If, however 8 = 9, then they are dependent, and we have to do something 4 Exercise Solutions 56 else. Here, we notice that an indicator random variable squared is exactly the same indicator random variable (since it only takes values 0 and 1). There are =(= − 1) terms in the ﬁrst case, each with expected value 1 4 and = terms in the second case, each with expected value 1 2 . We get: E[- 2] = =(=−1)∑ 8=1 =(=−1)∑ 9=1 E [-8 · -9 ] = =(= − 1) 1 4 + = 1 2 = =2 − = 4 + = 2 = =2 + = 4 Ex 13 This exercise is mostly to show that we don’t actually need to know to calculate expected value and variance if we have the distribution. We can calculate the expected value as: E[-] = ∑ G∈,- G · 5- (G) = 4 We can get the variance as Var[-] = E [- 2] − E[-]2. To calculate the ﬁrst term, we can view it as a new random variable whose distribution is derived from -: G2 16 4 25 64 100 5-2 (G2) 0.25 0.10 0.20 0.15 0.30 And we can calculate this expected value in the same way as before to get: E[- 2] = ∑ G∈,-2 G · 5-2 (G) = 49 We now have: E[-] = 4 Var[-] = 49 − 42 = 33 And using the Linearity of Expectation and the corresponding rule for variance: E[2- + 8] = 2 · E[-] + 8 = 16 Var[2- + 8] = 4 · Var[-] = 132 Ex 14 1. 2 independent rolls of the same die. For example modelled as: = [6]2 ∀$ : Pr[$] = 1 36 -1((0, 1)) := 0 -2((0, 1)) := 1 2. Rolling not two identical dice but maybe a four-sided one and a twenty-sided one. 4 Exercise Solutions 57 3. Looking at the same die roll as three random variables -1 = -2 = -3. Ex 15 We calculate the variance again as: Var[-] = E [- 2] − E[-]2 And we get (Bernoulli variables are the same when squared): Var[-] = ? − ?2 = ?(1 − ?) Ex 16 We ﬁrst deﬁne a variable . for the number of victories, and we see that it has a binomial distribution (unlike -): . ∼ Bin(25, 0.6). We can write - = 2 · . + 25 and because E[.] = =? = 15 Var[.] = =?(1 − ?) = 6 we know that: E[-] = 2 · E[.] + 25 = 55 Var[-] = 4 · Var[.] = 24 Ex 17 Because the Geometric distribution only takes on natural numbers, we can use: E [-] = ∞∑ 8=1 Pr [- ≥ 8] Giving us (with a little Analysis): E [-] = ∞∑ 8=1 Pr [- ≥ 8] = ∞∑ 8=1 (1 − ?) (8 − 1) = ∞∑ 8=0 (1 − ?)8 = 1 1 − (1 − ?) = 1 ? Ex 18 This is the modiﬁed calculation. You need to watch out not to make a mistake when rewriting the sum. E [-] = =/2∑ 8=1 E [-8] = =/2∑ 8=1 = = − 8 + 1 = = =/2∑ 8=1 1 = − 8 + 1 = = =∑ 8==/2+1 1 8 = = · (˛= − ˛=/2) = = · (ln = + O(1) − ln(=/2) − O(1)) = = · (ln = + O(1) − ln = + ln 2 − O(1) = = · O(1) = O(=) 4 Exercise Solutions 58 Ex 19 Any random variable with negative expected value will break Markov (probabilities can’t be negative after all). It’s also possible to construct a random variable with positive expected value that breaks Markov as follows: 1. Pick some E[-] that is pleasing to you 2. Pick some C > E[-] that you want to use later 3. Pick probability ? < 1 you like for the event - ≥ C 4. Now deﬁne a probability space to make this all work as follows: = {−H, C} Pr[−H] = 1 − ?, Pr[C] = ? 5. And ﬁnally solve for the variable H: −H · (1 − ?) + C · ? = E[-] ⇐⇒ C · ? − E[-] 1 − ? = H Ex 20 First oﬀ, let - be the number of correct answers. It is clear that - ∼ Bin(18, 0.5). That means we have E[-] = 9 and Var[-] = 9 2 . The true probability is ∑1 :=5 3(18 : ) 1 218 (no closed form expression for that sum exists) and if we were to calculate it, we’d get 7939 8192 ≈ 0.969. ▶ Markov cannot be used because it only gives us an upper bound for the probability to have too big values but here we also need to have an upper bound for the probability of too small values, and you cannot subtract two upper bounds to get another upper bound (do it step by step if that’s not clear). ▶ Chebyshev can be used to calculate an upper bound for the proba- bility of the complementary event: Pr [|- − E[-]| ≥ C] ≤ Var[-] C2 =⇒ Pr [|- − 9| ≥ 5] ≤ 9/2 25 = 9 50 =⇒ Pr [|- − 9| < 5] ≥ 1 − 9 50 = 41 50 = 0.82 ▶ Chernoﬀ can be used once for the probability that - is too small (≤ 4) and once for the case where - is too big (≥ 14). Pr [- ≥ (1 + \u0010)E[-]] ≤ 4− 1 3 \u00102E[-] =⇒ Pr [- ≥ 14] ≤ 4− 1 3 ·( 5 9 ) 2·9 = 4−25/27 Pr [- ≤ (1 − \u0010)E[-]] ≤ 4− 1 2 \u00102E[-] =⇒ Pr [- ≤ 4] ≤ 4− 1 2 ·( 5 9 ) 2·9 = 4−25/18 We can now add those upper bounds together to get the probability 4 Exercise Solutions 59 of the complementary event again (yes, adding bounds is ﬁne). Pr [|- − 9| ≥ 5] = Pr [- ≥ 14] + Pr [- ≤ 4] =⇒ Pr [|- − 9| ≥ 5] ≤ 4−25/27 + 4−25/18 =⇒ Pr [|- − 9| < 5] ≥ 1 − 4−25/27 − 4−25/18 ≈ 0.646 To conclude, of course both bounds are correct. Chebyshev is better in this case, because we’re very \"close\" to the expected value and = is very small, but there’s no general rule to know which is better. Ex 21 1. - ∼ Bin(45′000, 1 3 + 1 5 = 8 15 ). Thus the expected value is E[-] = =? = 45′000 · 8 15 = 24′000. 2. We use Chernoﬀ to get bounds for Pr [- ≥ 1.2E[-]] and Pr [- ≤ 0.9E[-]]: Pr [- ≥ (1 + \u0010)E[-]] ≤ 4− 1 3 \u00102E[-] =⇒ Pr [- ≥ 1.2E[-]] ≤ 4− 1 3 ·0.22·24000 = 4−320 Pr [- ≤ (1 − \u0010)E[-]] ≤ 4− 1 2 \u00102E[-] =⇒ Pr [- ≤ 0.9E[-]] ≤ 4− 1 2 ·0.12·24000 = 4−120 And we get: Pr [0.9˘ < - < 1.2˘] = Pr [- ≥ 1.2E[-]] + Pr [- ≤ 0.9E[-]] =⇒ Pr [0.9˘ < - < 1.2˘] ≤ 4−320 + 4−120 ≈ 7.67 · 10−53 3. Using the normal inequality is not possible because we’d need to choose a \u0010 > 1. Using the third inequality instead (which we’re allowed to do because 300 = 6 · 50 ≥ 6 · E[-] ≥ 24 · E[-]) gives: Pr [- ≥ 300] ≤ 2−300 ≈ 4.9 · 10−91 4.3 Randomized Algorithms Ex 22 Only the ﬁrst algorithm is useful. As soon as we get 50% success probability, we might as well ﬂip a coin. Ex 23 Now all of these algorithms are suddenly useful! 4 Exercise Solutions 60 Ex 24 We can stop the execution of the Las Vegas algorithm after a predetermined time and return any (possibly wrong) result we like. The other direction is not possible (we cannot produce results that are always correct from results that are only sometimes correct). Ex 25 The answer \"not prime\" is always correct. Thus, we design the algorithm as follows: 1 for 8 = 1, . . . , # do 2 if MillerRabin(=) = \"not prime\" 3 return \"not prime\" 4 return \"prime\" Correctness proof: As we know, the Miller Rabin test has one-sided error with a success probability of \u0011 = 3/4. We can use the Theorem we saw earlier to conclude that our algorithm has a 99.99% success probability by choosing \u0010 = 0.0001 and calculating # as: # = \u0011−1 ln \u0010−1 = 4 3 ln(104) = 16 3 ln 10 And because ln 10 ≈ 2.303 ≤ 3 it suﬃces to use: # = 16 3 · 3 = 16 (because this implies # ≥ \u0011−1 ln \u0010−1) Runtime analysis: Because we do a constant amount of iterations, the runtime of this algorithm is O()\"') where )\"' is the runtime of the Miller Rabin test.","libVersion":"0.3.2","langs":""}