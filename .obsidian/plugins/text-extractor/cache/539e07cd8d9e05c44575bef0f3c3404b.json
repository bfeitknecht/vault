{"path":"sem4/DMDB/VRL/extra/slides/DAnalytics-II.pdf","text":"Gustavo Alonso, Vasilis Mageirakos alonso@inf.ethz.ch Data Modelling & Databases Lecture 12: Analytics II (K- Means and Near Neighbor Search) Interactive SQL Session May 2nd â€˜25 after Easter - Bring your laptop Overview â–ª Clustering â–ª Classification Clustering â€¢ In general a grouping of objects such that the objects in a group (cluster) are similar (or related) to one another and different from (or unrelated to) the objects in other groups Inter-cluster distances are maximized Intra-cluster distances are minimized Clustering â€¢ Notion of a Cluster can be Ambiguous How many clusters? Four ClustersTwo Clusters Six Clusters Clustering Tid Refund Marital Status Taxable Income 1 Yes Single 125K 2 No Married 100K 3 No Single 70K 4 Yes Married 120K 5 No Divorced 95K 6 No Married 60K 7 Yes Divorced 220K 8 No Single 85K 9 No Married 75K 10 No Single 90K 10 Each row corresponds to one point in space. By clustering these points together we are forming clusters of entities stored in a DB. Clustering â–ª A clustering is a set of clusters â–ª Important distinction between hierarchical and partitional sets of clusters â–ª Partitional Clustering â¢ A division of data objects into subsets (clusters) such that each data object is in exactly one subset â–ª Hierarchical clustering â¢ A set of nested clusters organized as a hierarchical tree Clustering â–ª Partitional Clustering Original Points A Partitional Clustering Clustering â–ª Hierarchical Clustering p4 p1 p3 p2 p4 p1 p3 p2 p4p1 p2 p3 p4p1 p2 p3 Traditional Hierarchical Clustering Non-traditional Hierarchical Clustering Non-traditional Dendrogram Traditional Dendrogram Types of Clusters: Well-Separated â–ª Well-Separated Clusters: â¢ A cluster is a set of points such that any point in a cluster is closer (or more similar) to every other point in the cluster than to any point not in the cluster. 3 well-separated clusters Types of Clusters: Center-Based â–ª Center-based â¢ A cluster is a set of objects such that an object in a cluster is closer (more similar) to the â€œcenterâ€ of a cluster, than to the center of any other cluster â¢ The center of a cluster is often a centroid, the minimizer of distances from all the points in the cluster, or a medoid, the most â€œrepresentativeâ€ point of a cluster 4 center-based clusters Types of Clusters: Contiguity-Based â–ª Contiguous Cluster (Nearest neighbor or Transitive) â¢ A cluster is a set of points such that a point in a cluster is closer (or more similar) to one or more other points in the cluster than to any point not in the cluster. 8 contiguous clusters Types of Clusters: Density-Based â–ª Density-based â¢ A cluster is a dense region of points, which is separated by low-density regions, from other regions of high density. â¢ Used when the clusters are irregular or intertwined, and when noise and outliers are present. 6 density-based clusters Types of Clusters: Conceptual Clusters â–ª Shared Property or Conceptual Clusters â¢ Finds clusters that share some common property or represent a particular concept. 2 Overlapping Circles K-means â–ª Partitional clustering approach â–ª Each cluster is associated with a centroid (center point) â–ª Each point is assigned to the cluster with the closest centroid â–ª Number of clusters, K, must be specified â–ª The objective is to minimize the sum of distances of the points to their respective centroid K-means â€¢ Most common definition is with Euclidean distance, minimizing the Sum of Squares Error (SSE) function â¢ Sometimes K-means is defined like that â–ª Problem: Given a set X of n points in a d-dimensional space and an integer K group the points into K clusters C= {C1, C2,â€¦,Ck} such that ğ¶ğ‘œğ‘ ğ‘¡ ğ¶ = à· ğ‘–=1 ğ‘˜ à· ğ‘¥âˆˆğ¶ğ‘– ğ‘¥ âˆ’ ğ‘ğ‘– 2 is minimized, where ci is the mean of the points in cluster Ci K-means â€¢ NP-hard if the dimensionality of the data is at least 2 (d>=2) â¢ Finding the best solution in polynomial time is infeasible â€¢ For d=1 the problem is solvable in polynomial time â€¢ A simple iterative algorithm works quite well in practice K-means â–ª Also known as Lloydâ€™s algorithm. â–ª K-means is sometimes synonymous with this algorithm K-means Source: https://www.youtube.com/watch?v=2R9cZCMWwcw K-means (run #1) -2 -1.5 -1 -0.5 0 0.5 1 1.5 2 0 0.5 1 1.5 2 2.5 3 xy Iteration 1 -2 -1.5 -1 -0.5 0 0.5 1 1.5 2 0 0.5 1 1.5 2 2.5 3 xy Iteration 2 -2 -1.5 -1 -0.5 0 0.5 1 1.5 2 0 0.5 1 1.5 2 2.5 3 xy Iteration 3 -2 -1.5 -1 -0.5 0 0.5 1 1.5 2 0 0.5 1 1.5 2 2.5 3 xy Iteration 4 -2 -1.5 -1 -0.5 0 0.5 1 1.5 2 0 0.5 1 1.5 2 2.5 3 xy Iteration 5 -2 -1.5 -1 -0.5 0 0.5 1 1.5 2 0 0.5 1 1.5 2 2.5 3 xy Iteration 6 K-means (run #2) -2 -1.5 -1 -0.5 0 0.5 1 1.5 2 0 0.5 1 1.5 2 2.5 3 xy Iteration 1 -2 -1.5 -1 -0.5 0 0.5 1 1.5 2 0 0.5 1 1.5 2 2.5 3 xy Iteration 2 -2 -1.5 -1 -0.5 0 0.5 1 1.5 2 0 0.5 1 1.5 2 2.5 3 xy Iteration 3 -2 -1.5 -1 -0.5 0 0.5 1 1.5 2 0 0.5 1 1.5 2 2.5 3 xy Iteration 4 -2 -1.5 -1 -0.5 0 0.5 1 1.5 2 0 0.5 1 1.5 2 2.5 3 xy Iteration 5 K-means â–ª Dealing with Initialization â¢ Do multiple runs with different initialization seeds and select the clustering with the smallest error [SLOW@scale] â¢ Select original set of points by methods other than random . E.g., pick the most distant (from each other) points as cluster centers (K-means++ algorithm) [FASTER@scale] K-means â–ª The centroid depends on the distance function â¢ The minimizer for the distance function â–ª â€˜Closenessâ€™ is measured by Euclidean distance (Sum of Squared Error), cosine similarity, correlation, etc. â–ª Centroid: â¢ The mean of the points in the cluster for SSE, and cosine similarity â¢ The median for Manhattan distance. â–ª Finding the centroid is not always easy â¢ It can be an NP-hard problem for some distance functions K-means â–ª K-means will converge for common similarity measures mentioned above. â¢ Most of the convergence happens in the first few iterations. â¢ Often the stopping condition is changed to â€˜Until relatively few points change clustersâ€™ â–ª Complexity is O( n * K * I * d ) â¢ n = number of points, K = number of clusters, I = number of iterations, d = dimensionality â–ª In general, a fast and efficient algorithm K-means â–ª K-means has problems when clusters are of different â¢ Sizes â¢ Densities â¢ Non-globular shapes â–ª K-means has problems when the data contains outliers. K-means â€“ Variable Size Clusters Original Points K-means (3 Clusters) K-means â€“ Variable Density Clusters Original Points K-means (3 Clusters) K-means - Continuity Original Points K-means (2 Clusters) K-means Original Points K-means Clusters One solution is to use many clusters. Find parts of clusters, but need to put together. K-means Original Points K-means Clusters K-meansLots of research on optimizations Use lower and upper bounds plus triangle inequality to reduce the number of comparisons in every round Classification Classification Tid Refund Marital Status Taxable Income Cheat 1 Yes Single 125K No 2 No Married 100K No 3 No Single 70K No 4 Yes Married 120K No 5 No Divorced 95K Yes 6 No Married 60K No 7 Yes Divorced 220K No 8 No Single 85K Yes 9 No Married 75K No 10 No Single 90K Yes 10 Refund Marital Status Taxable Income Cheat No Married 80K ? 10 Tax-return data for year 2011 A new tax return for 2012 Is this a cheating tax return? An instance of the classification problem: learn a method for discriminating between records of different classes (cheaters vs non-cheaters) Classification â–ª Classification is the task of learning a target function f that maps attribute set x to one of the predefined class labels y Tid Refund Marital Status Taxable Income Cheat 1 Yes Single 125K No 2 No Married 100K No 3 No Single 70K No 4 Yes Married 120K No 5 No Divorced 95K Yes 6 No Married 60K No 7 Yes Divorced 220K No 8 No Single 85K Yes 9 No Married 75K No 10 No Single 90K Yes 10 One of the attributes is the class attribute In this case: Cheat Two class labels (or classes): Yes (1), No (0) Classification â–ª Predicting tumor cells as benign or malignant â–ª Classifying credit card transactions as legitimate or fraudulent â–ª Categorizing news stories as finance, weather, entertainment, sports, etc â–ª Identifying spam email, spam web pages, adult content â–ª Understanding if a web page has commercial intent or not Classification â–ª Training set consists of records with known class labels â–ª Training set is used to build a classification model â–ª A labeled test set of previously unseen data records is used to evaluate the quality of the model. â–ª The classification model is applied to new records with unknown class labels Apply Model Induction Deduction Learn Model Model Tid Attrib1 Attrib2 Attrib3 Class 1 Yes Large 125K No 2 No Medium 100K No 3 No Small 70K No 4 Yes Medium 120K No 5 No Large 95K Yes 6 No Medium 60K No 7 Yes Large 220K No 8 No Small 85K Yes 9 No Medium 75K No 10 No Small 90K Yes 10 Tid Attrib1 Attrib2 Attrib3 Class 11 No Small 55K ? 12 Yes Medium 80K ? 13 Yes Large 110K ? 14 No Small 95K ? 15 No Large 67K ? 10 Test Set Learning algorithm Training Set Classification - Evaluation â–ª Counts of test records that are correctly (or incorrectly) predicted by the classification model â–ª Confusion matrix Class = 1 Class = 0 Class = 1 f11 f10 Class = 0 f01 f00 Predicted ClassActual Class 00011011 0011 sprediction of # total spredictioncorrect # Accuracy ffff ff +++ + == 00011011 0110 sprediction of # total sprediction wrong# rateError ffff ff +++ + == Instance-based learning Atr1 â€¦â€¦... AtrN Class A B B C A C B Set of Stored Cases Atr1 â€¦â€¦... AtrN Unseen Case â€¢ Store the training records â€¢ Use training records to predict the class label of unseen cases Nearest Neighbor Classifier â€¢ Basic idea: â€¢ â€œIf it walks like a duck, quacks like a duck, then itâ€™s probably a duckâ€ Training Records Test Record Compute Distance Choose k of the â€œnearestâ€ records Nearest Neighbor Classifier Requires three things â€“ The set of stored records â€“ Distance Metric to compute distance between records â€“ The value of k, the number of nearest neighbors to retrieve To classify an unknown record: 1. Compute distance to other training records 2. Identify k nearest neighbors 3. Use class labels of nearest neighbors to determine the class label of unknown record (e.g., by taking majority vote) Unknown record Nearest Neighbor Classifier â–ª Compute distance between two points: â¢ Euclidean distance â–ª Determine the class from nearest neighbor list â¢ take the majority vote of class labels among the k-nearest neighbors â¢ Weigh the vote according to distance â€¢ weight factor, w = 1/d2 ïƒ¥ âˆ’= i ii qpqpd 2)(),( Nearest Neighbor Classifier X X X (a) 1-nearest neighbor (b) 2-nearest neighbor (c) 3-nearest neighbor K-nearest neighbors of a record x are data points that have the k smallest distance to x Approximate nearest neighbor search â–ª Given: a set of database vectors â–ª Input: a query vector â–ª Output: K (approximate) nearest neighbors in the database â–ª Exact nearest neighbor search is expensive for large datasets â–ª Need to prune the search space by indexes Inverted-file (IVF) index Training: cluster database vectors into IVF lists Inverted-file (IVF) index Training: cluster database vectors into IVF lists Inverted-file (IVF) index Searching: scan only a subset of IVF lists Inverted-file (IVF) index Searching: scan only a subset of IVF lists Inverted-file (IVF) index Searching: scan only a subset of IVF lists Locality Sensitive Hashing â–ª Another common way to implement approximated NNS is to use locality sensitive hashing (LSH) â–ª LSH hashes the data aiming at placing similar data in the same bucket â–ª A NNS is conducted by hashing across all hash functions, checking all the buckets, and picking the closest point Locality Sensitive Hashing â–ª Note: LSH is a form of very approximated clustering (not the same as K-means) â–ª Nearest neighbor search using LSH: start from exactly the same hash buckets, then expand to neighbor buckets ANN search nowadays: performance at scale matters â–ª More algorithms: not only indexing, but also compressing the dataset to save memory footprint and memory bandwidth â€¢ Product quantization (PQ) â–ª Better systems: leveraging the features of modern processors, memory, and storage â–ª Hardware acceleration: GPUs and FPGAs Example in Real DB http://madlib.apache.org/docs/latest/group__grp__knn.html https://services.google.com/fh/files/misc/scann_for_alloydb _whitepaper.pdf https://www.databricks.com/product/machine- learning/vector-search Nearest Neighbor Search ïƒ³ Vector Search Related Research in our Group","libVersion":"0.3.2","langs":""}