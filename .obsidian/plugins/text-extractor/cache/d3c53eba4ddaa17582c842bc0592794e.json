{"path":"sem2/PProg/VRL/slides/PProg-L27-distributed-memory.pdf","text":"spcl.inf.ethz.ch @spcl_eth TORSTEN HOEFLER Parallel Programming Transactions and Distributed Memory Programming https://www.youtube.com/watch?v=StuSFc7aCJc spcl.inf.ethz.ch @spcl_eth ▪ Consensus (hierarchy) Atomic registers CAS Queues ▪ Transactions Motivation Definitions Usage of Scala-STM Intro to implementation 2 Last time spcl.inf.ethz.ch @spcl_eth ▪ Transactional memory ▪ Finish implementation ▪ Distributed memory ▪ Maybe: Message Passing Interface ▪ Due to calendar variation, we’re a day short this year! May not get to this – if you want to know: Design of Parallel and High-Performance Computing in MSc ▪ Standard library for high-performance parallel programming ▪ Collectives 3 Learning goals for today spcl.inf.ethz.ch @spcl_eth Transaction life time time birthdate of T X.date Y.date Z.date T reads Y T reads X T reads Z read set of T 4 spcl.inf.ethz.ch @spcl_eth ▪ Lock all objects of read- and write-set (in some defined order to avoid deadlocks) ▪ Check that all objects in the read- and write-set provide a time stamp ≤ birthdate of the transaction, otherwise return \"abort\" ▪ Increment and get the value T of current global clock ▪ Copy each element of the write set back to global memory with timestamp T ▪ Release all locks and return \"commit\" Commit 5 spcl.inf.ethz.ch @spcl_eth Successful commit time birthdate of T X.date Y.date Z.date T reads Y T reads X T writes Y (local copy!) T writes X (local copy!) T commits read set of T write set of T 6 spcl.inf.ethz.ch @spcl_eth Aborted commit time birthdate of T X.date Y.date Z.date T reads Y T reads X T writes Y (local copy!) T writes Z (local copy!) T commits read set of T 7 write set of T spcl.inf.ethz.ch @spcl_eth ● 5 philosophers ● 5 forks ● each philosopher requires 2 forks to eat ● forks cannot be shared Dining philosophers image source: Wikipedia 8 spcl.inf.ethz.ch @spcl_eth Philosopher: ● think ● lock left ● lock right ● eat ● unlock right ● unlock left Solution that can lead to deadlock P1 takes F1, P2 takes F2, P3 takes F3, P4 takes F4, P5 takes F5 → Deadlock 9 spcl.inf.ethz.ch @spcl_eth private static class Fork { public final Ref.View<Boolean> inUse = STM.newRef(false); } class PhilosopherThread extends Thread { private final int meals; private final Fork left; private final Fork right; public PhilosopherThread(Fork left, Fork right) { this.left = left; this.right = right; } public void run() { … } } 10 Dining Philosophers Using TM spcl.inf.ethz.ch @spcl_eth Fork[] forks = new Fork[tableSize]; for (int i = 0; i < tableSize; i++) forks[i] = new Fork(); PhilosopherThread[] threads = new PhilosopherThread[tableSize]; for (int i = 0; i < tableSize; i++) threads[i] = new PhilosopherThread(forks[i], forks[(i + 1) % tableSize]); Dining Philosophers Using TM 11 spcl.inf.ethz.ch @spcl_eth class PhilosopherThread extends Thread { … public void run() { for (int m = 0; m < meals; m++) { // THINK pickUpBothForks(); // EAT putDownForks(); } } … } Dining Philosophers Using TM 12 spcl.inf.ethz.ch @spcl_eth class PhilosopherThread extends Thread { … private void pickUpBothForks() { STM.atomic(new Runnable() { public void run() { if (left.inUse.get() || right.inUse.get()) STM.retry(); left.inUse.set(true); right.inUse.set(true); }}); } … } Dining Philosophers Using TM 13 spcl.inf.ethz.ch @spcl_eth class PhilosopherThread extends Thread { … private void putDownForks() { STM.atomic(new Runnable() { public void run() { left.inUse.set(false); right.inUse.set(false); }}); } … } Dining Philosophers Using TM 14 spcl.inf.ethz.ch @spcl_eth ▪ It is not clear what are the best semantics for transactions ▪ Getting good performance can be challenging ▪ I/O operations (e.g., print to screen) Can we perform I/O operations in a transaction? Issues with transactions 15 spcl.inf.ethz.ch @spcl_eth ● Locks are hard and limited! Lock-free is quite (too?) hard! ● Transactional Memory tries to remove the burden from the programmer ● STM / HTM ● Remains to be seen whether it will be widely adopted in the future Summary 16 spcl.inf.ethz.ch @spcl_eth Simon Peyton Jones, Beautiful concurrency http://research.microsoft.com/pubs/74063/beautiful.pdf Dan Grossman, The Transactional Memory / Garbage Collection Analogy https://homes.cs.washington.edu/~djg/papers/analogy_oopsla07.pdf Additional Reading 17 spcl.inf.ethz.ch @spcl_eth Distributed Memory & Message Passing 18 spcl.inf.ethz.ch @spcl_eth Considered ▪ Parallel / Concurrent ▪ Fork-Join / Threads ▪ OOP on Shared Memory ▪ Locking / Lock Free / Transactional ▪ Semaphores / Monitors 19 So far spcl.inf.ethz.ch @spcl_eth Many of the problems of parallel/concurrent programming come from sharing state ▪ Complexity of locks, race conditions, …. What if we avoid sharing state? 20 Sharing State spcl.inf.ethz.ch @spcl_eth Functional Programming ▪ Immutable state → no synchronization required Message Passing: Isolated mutable state ▪ State is mutable, but not shared: Each thread/task has its private state ▪ Tasks cooperate via message passing 21 Alternatives spcl.inf.ethz.ch @spcl_eth Programming Models ▪ CSP: Communicating Sequential Processes ▪ Actor programming model – Go language Framework/library ▪ MPI (Message Passing Interface) 22 Concurrent Message Passing spcl.inf.ethz.ch @spcl_eth 23 Shared vs Distributed memory CPU CPU CPU Mem CPU CPU CPU Mem Mem Mem Interconnect Network spcl.inf.ethz.ch @spcl_eth 24 Isolated mutable state state state state Mutable (private) state Tasks exchange messages spcl.inf.ethz.ch @spcl_eth 25 Example: Shared state counting counter .inc() .get() .inc() .get() .inc() .get() .inc() .get() → shared state must be protected (lock/atomic counter) spcl.inf.ethz.ch @spcl_eth 26 Isolated mutability: counting .inc() Local cnt .inc() Local cnt .inc() Local cnt .inc() Local cnt spcl.inf.ethz.ch @spcl_eth 27 Isolated mutability: accessing count Local cnt Local cnt Local cnt .get() Local cnt spcl.inf.ethz.ch @spcl_eth Bank account – Sequential programming ● Single balance – Parallel programming: sharing state ● Single balance + protection – Parallel programming: distributing state ● Each thread has a local balance (a budget) ● Threads exchange amounts at coarse granularity (only when needed) 28 Rethinking managing state spcl.inf.ethz.ch @spcl_eth Total balance: 100 + 300 + 150 = 550 ▪ Each task can operate independently ▪ And communicate with other tasks only when needed ▪ This lecture: via messaging 29 Distributed Bank account 100 300 150 spcl.inf.ethz.ch @spcl_eth Synchronous: – sender blocks until message is received Asynchronous: – sender does not block (fire-and-forget) – placed into a buffer for receiver to get 30 Synchronous vs Asynchronous messages spcl.inf.ethz.ch @spcl_eth Concurrent programming language from Google Language support for: – Lightweight tasks (called goroutines) – Typed channels for task communications ● channels are synchronous (or unbuffered) by default ● support for asynchronous (buffered) channels Inspired by CSP Language roots in Algol Family: Pascal, Modula, Oberon [Prof. Niklaus Wirth, ETH] [One of the inventors, Robert Griesemer: PhD from ETH] 31 Go programming language spcl.inf.ethz.ch @spcl_eth func main() { msgs := make(chan string) done := make(chan bool) go hello(msgs, done); msgs <- \"Hello\" msgs <- \"bye\" ok := <-done fmt.Println(\"Done:\", ok); } func hello(msgs chan string, done chan bool) { for { msg := <-msgs fmt.Println(\"Got:\", msg) if msg == \"bye\" { break } } done <- true; } 32 Go example spcl.inf.ethz.ch @spcl_eth func main() { msgs := make(chan string) done := make(chan bool) go hello(msgs, done); msgs <- \"Hello\" msgs <- \"bye\" ok := <-done fmt.Println(\"Done:\", ok); } func hello(msgs chan string, done chan bool) { for { msg := <-msgs fmt.Println(\"Got:\", msg) if msg == \"bye\" { break } } done <- true; } 33 Go example Create two channels: - msgs: for strings - done: for boolean values spcl.inf.ethz.ch @spcl_eth func main() { msgs := make(chan string) done := make(chan bool) go hello(msgs, done); msgs <- \"Hello\" msgs <- \"bye\" ok := <-done fmt.Println(\"Done:\", ok); } func hello(msgs chan string, done chan bool) { for { msg := <-msgs fmt.Println(\"Got:\", msg) if msg == \"bye\" { break } } done <- true; } 34 Go example Create a new task (goroutine), that will execute function hello with the given arguments spcl.inf.ethz.ch @spcl_eth func main() { msgs := make(chan string) done := make(chan bool) go hello(msgs, done); msgs <- \"Hello\" msgs <- \"bye\" ok := <-done fmt.Println(\"Done:\", ok); } func hello(msgs chan string, done chan bool) { for { msg := <-msgs fmt.Println(\"Got:\", msg) if msg == \"bye\" { break } } done <- true; } 35 Go example Hello takes two channels as arguments for communication spcl.inf.ethz.ch @spcl_eth func main() { msgs := make(chan string) done := make(chan bool) go hello(msgs, done); msgs <- \"Hello\" msgs <- \"bye\" ok := <-done fmt.Println(\"Done:\", ok); } func hello(msgs chan string, done chan bool) { for { msg := <-msgs fmt.Println(\"Got:\", msg) if msg == \"bye\" { break } } done <- true; } 36 Go example Write arguments to msgs channel Read result via done channel spcl.inf.ethz.ch @spcl_eth func t(in chan string, done chan bool) { m := <-in // receive from in channel fmt.Println(\"Got message:\", m); // print received message done <- true // send true to done channel } func main() { c := make(chan string) // create a string channel done := make(chan bool) // create a boolean channel go t(c,done) // spawn goroutine ok := <-done // receive from done channel fmt.Println(\"Got ok:\", ok); // print ok c <- \"Hello\" // send hello to channel c } 37 Q: what will happen in this program? A: fatal error: all goroutines are asleep - deadlock! spcl.inf.ethz.ch @spcl_eth Each station removes multiples of the first element received and passes on the remaining elements to the next station 38 Example: Concurrent prime sieve G F2 F3 F5 ... 9 8 7 6 5 4 3 2 .... 9 7 5 3 ... 7 5 ... 7 spcl.inf.ethz.ch @spcl_eth func main() { ch := make(chan int) go Generate(ch) for i := 0; i < 10; i++ { prime := <-ch fmt.Println(prime) ch1 := make(chan int) go Filter(ch, ch1, prime) ch = ch1 } } 39 Concurrent prime sieve source code from golang.org func Generate(ch chan <- int) { for i := 2; ; i++ { ch <- i } } func Filter(in <- chan int, out chan <- int, prime int) { for { i := <-in // Receive value from 'in' if i % prime != 0 { out <- i // Send 'i' to 'out' } } } G F2 F3 F5 ... 7 6 5 4 3 2 .... 7 5 3 ... 7 5 ... 7 G Fprime spcl.inf.ethz.ch @spcl_eth Message Passing Interface (MPI) 42 spcl.inf.ethz.ch @spcl_eth Message passing libraries: ▪ PVM (Parallel Virtual Machines) 1980s ▪ MPI (Message Passing Interface) 1990s MPI = Standard API • Hides Software/Hardware details • Portable, flexible • Implemented as a library 43 Message Passing Interface (MPI) Program MPI library Standard TCP/IP Standard Network HW Specialized Driver Custom Network HW spcl.inf.ethz.ch @spcl_eth ▪ MPI processes can be collected into groups ▪ Each group can have multiple colors (sometimes called context) ▪ Group + color == communicator (it is like a name for the group) ▪ When an MPI application starts, the group of all processes is initially given a predefined name called MPI_COMM_WORLD ▪ The same group can have many names, but simple programs do not have to worry about multiple names ▪ A process is identified by a unique number within each communicator, called rank ▪ For two different communicators, the same process can have two different ranks: thus, the meaning of a “rank” is only defined when you specify the communicator Process Identification 44 spcl.inf.ethz.ch @spcl_eth ▪ Defines the communication domain of a communication operation: set of processes that are allowed to communicate with each other. ▪ Initially all processes are in the communicator MPI_COMM_WORLD. ▪ The rank of processes are associated with (and unique within) a communicator, numbered from 0 to n-1 45 MPI Communicators P0 P1 P2 P3 P0 P1 P2 P3 c1 c2 c3 MPI_COMM_WORLD spcl.inf.ethz.ch @spcl_eth Communicators When you start an MPI program, there is one predefined communicator MPI_COMM_WORLD Can make copies of this communicator (same group of processes, same ranks, but different “aliases”) Communicators do not need to contain all processes in the system Every process in a communicator has an ID called as “rank” 0 1 2 3 4 5 6 7 2 3 4 5 0 1 6 7 The same process might have different ranks in different communicators Communicators can be created “by hand” or using tools Simple programs typically only use the predefined communicator MPI_COMM_WORLD (which is sometimes considered bad practice because of modularity issues) mpiexec -np 16 ./test 46 spcl.inf.ethz.ch @spcl_eth Processes are identified by nonnegative integers, called ranks p processes are numbered 0, 1, 2, .. p-1 47 Process Ranks public static void main(String args []) throws Exception { MPI.Init(args); // Get total number of processes (p) int size = MPI.COMM_WORLD.Size(); // Get rank of current process (in [0..p-1]) int rank = MPI.COMM_WORLD.Rank(); MPI.Finalize(); } spcl.inf.ethz.ch @spcl_eth Single Program Multiple Data (Multiple Instances) 48 SPMD if (rank == 0) do this else do that if (rank == 0) do this else do that P0 if (rank == 0) do this else do that P1 if (rank == 0) do this else do that P2 if (rank == 0) do this else do that P3 we compile one program the if-else makes it SPMD spcl.inf.ethz.ch @spcl_eth void Comm.Send( communicator Object buf, pointer to data to be sent int offset, int count, number of items to be sent Datatype datatype, data type of items, must be explicitely specified int dest, destination process id int tag data id tag ) 49 Communication int int int int int int offset count * sizeof(int) buf array from MPJ Spec spcl.inf.ethz.ch @spcl_eth Parallel Sort using MPI Send/Recv 8 23 19 67 45 35 1 24 13 30 3 5 8 19 23 35 45 67 1 3 5 13 24 30 Rank 0 Rank 1 8 19 23 35 3045 67 1 3 5 13 24 sort in parallel ~2* (N/2 log N/2) 1 3 5 8 6713 19 23 24 30 35 45 Rank 0 Rank 0 Rank 0 merge in O(N) send in O(N) send in O(N) 50 spcl.inf.ethz.ch @spcl_eth How to compute fast? March 2015 51 spcl.inf.ethz.ch @spcl_eth ▪ First of all, read all instructions ▪ Then, read the whole exam paper through ▪ Look at the number of points for each question ▪ This shows how long we think it will take to answer! ▪ Find one you know you can answer, and answer it ▪ This will make you feel better early on. ▪ Watch the clock! ▪ If you are taking too long on a question, consider dropping it and moving on to another one. ▪ Always show your working ▪ Write down steps and partial thoughts ▪ You should be able to explain most of the slides ▪ Tip: form learning groups and present the slides to each other ▪ If something is unclear: Ask your friends Re-watch the videos (playlist on Youtube – youtube.com/@spcl) Read the book (Herlihy and Shavit for the second part) Ask your TAs Last lecture -- basic exam tips 52 spcl.inf.ethz.ch @spcl_eth ▪ Computation is the third pillar of science Why computing fast? 53 spcl.inf.ethz.ch @spcl_eth 54 But why do I care!!?? Maybe you like the weather forecast? Tobias Gysi, PhD Student @SPCL spcl.inf.ethz.ch @spcl_eth 55 Or you wonder about the future of the Earth? spcl.inf.ethz.ch @spcl_eth 56 Source: https://medium.com/ Source: https://www.ft.com/ Source: https://www.medpagetoday.com/ What is left for us humans? Or you wonder about the future of humanity? spcl.inf.ethz.ch @spcl_eth 1 Teraflop in 1997 $67 Million 57 spcl.inf.ethz.ch @spcl_eth 1 Teraflop 17 years later (2014) 1 TF “Amazon.com by Intel even has the co- processor selling for just $142 (plus $12 shipping) though they seem to be now out of stock until early December.” (Nov. 11, 2014) [Update 2024 – H100] 67 TC Tflop/s FP64 precision 989 Tflop/s TF32 precision 3958 Tflop/s FP8 precision Want to play with any of these? 58 2022 spcl.inf.ethz.ch @spcl_eth 1 Teraflop 20 years later (2017) 59 spcl.inf.ethz.ch @spcl_eth My looking glass slide from 2015: “1 Teraflop 25 years later (2022)” 60 2023 spcl.inf.ethz.ch @spcl_eth 1 Petaflop 35 years later (2032???) 61 Not so fast … (or: performance became interesting again) spcl.inf.ethz.ch @spcl_eth 62 Changing hardware constraints and the physics of computing [1]: Marc Horowitz, Computing’s Energy Problem (and what we can do about it), ISSC 2014, plenary [2]: Moore: Landauer Limit Demonstrated, IEEE Spectrum 2012 130nm 90nm 65nm 45nm 32nm 22nm 14nm 10nm 0.9 V [1] 32-bit FP ADD: 0.9 pJ 32-bit FP MUL: 3.2 pJ 2x32 bit from L1 (8 kiB): 10 pJ 2x32 bit from L2 (1 MiB): 100 pJ 2x32 bit from DRAM: 1.3 nJ … Three Ls of modern computing: How to address locality challenges on standard architectures and programming? D. Unat et al.: “Trends in Data Locality Abstractions for HPC Systems” IEEE Transactions on Parallel and Distributed Systems (TPDS). Vol 28, Nr. 10, IEEE, Oct. 2017 spcl.inf.ethz.ch @spcl_eth 63 Load-store vs. Dataflow architectures Memory Cache RegistersControl x=a+b ld a, r1 ALU ald b, r2 badd r1, r2 ba x bast r1, x Memory + c d y y=(a+b)*(c+d) a b + x a b c d a+b c+d y Turing Award 1977 (Backus): \"Surely there must be a less primitive way of making big changes in the store than pushing vast numbers of words back and forth through the von Neumann bottleneck.\" Load-store (“von Neumann”) Energy per instruction: 70pJ Source: Mark Horowitz, ISSC’14 Energy per operation: 1-3pJ Static Dataflow (“non von Neumann”) spcl.inf.ethz.ch @spcl_eth 64 Single Instruction Multiple Data/Threads (SIMD - Vector CPU, SIMT - GPU) Memory Cache RegistersControl ALUALU ALUALU ALUALU ALUALU ALUALU 45nm, 0.9 V [1] Random Access SRAM: 8 kiB: 10 pJ 32 kiB: 20 pJ 1 MiB: 100 pJ Memory + c d ya b + x a b c d 45nm, 0.9 V [1] Single R/W registers: 32 bit: 0.1 pJ [1]: Marc Horowitz, Computing’s Energy Problem (and what we can do about it), ISSC 2014, plenary High Performance Computing really became a data management challenge spcl.inf.ethz.ch @spcl_eth High-performance Computing (Supercomputing) Vectorization Multicore/SMP GPU/FPGA Computing IEEE Floating Point Datacenter Networking/RDMA …. next: specialized, dataflow, and spatial computing 65 spcl.inf.ethz.ch @spcl_eth ▪ A benchmark, solve Ax=b ▪ As fast as possible! → as big as possible ☺ ▪ Reflects some applications, not all, not even many ▪ Very good historic data! ▪ Speed comparison for computing centers, states, countries, nations, continents  ▪ Politicized (sometimes good, sometimes bad) ▪ Yet, fun to watch Top 500 RTX 3080 My Xeon Phi 66 spcl.inf.ethz.ch @spcl_eth www.top500.org The June 2024 List (released last Monday!) 67 Want to run on that system? DPHPC class of 2015 x 2015-2018 NVIDIA GTC’21 (https://www.youtube.com/watch?v=eAn_oiZwUXA at 01:03:30) spcl.inf.ethz.ch @spcl_eth 68 Computing Pi on a supercomputer! int main( int argc, char *argv[] ) { // definitions … MPI_Init(&argc,&argv); MPI_Comm_size(MPI_COMM_WORLD, &numprocs); MPI_Comm_rank(MPI_COMM_WORLD, &myid); double t = -MPI_Wtime(); for (j=0; j<n; ++j) { h = 1.0 / (double) n; sum = 0.0; for (i = myid + 1; i <= n; i += numprocs) { x = h * ((double)i - 0.5); sum += (4.0 / (1.0 + x*x)); } mypi = h * sum; MPI_Reduce(&mypi, &pi, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD); } t+=MPI_Wtime(); if (!myid) { printf(\"pi is approximately %.16f, Error is %.16f\\n\", pi, fabs(pi - PI25DT)); printf(\"time: %f\\n\", t); } MPI_Finalize(); } spcl.inf.ethz.ch @spcl_eth ▪ 6 undergrads (you!), 2 advisors, 1 cluster, 2x13 amps ▪ >20 teams, most continents @SC or @ISC ▪ 48 hours, five applications, non-stop! ▪ top-class conference (>13,000 attendees) ▪ Lots of fun ▪ Even more experience! ▪ Introducing team Racklette ▪ https://racklette.ethz.ch/ ▪ Search for “Student Cluster Challenge” ▪ HPC-CH/CSCS is helping ▪ Let me know, my assistants are happy to help! ▪ If we have a full team Student Cluster Competition Want to become an expert in HPC? 69 spcl.inf.ethz.ch @spcl_eth ▪ Thanks for being such fun to teach ☺ ▪ Comments (also anonymous) are always appreciated! ▪ If you are interested in parallel computing research, talk to me or my assistants! ▪ Large-scale (datacenter) systems ▪ Next-generation parallel programming (e.g., FPGAs, CGRAs) ▪ Parallel computing (SMP and MPI) ▪ GPUs (NVIDIA, Intel Xe), FPGAs, Manycore … ▪ … spcl-friends mailing list (subscribe on webpage) ▪ … on twitter: @thoefler or the lab: @spcl_eth ▪ … follow us on youtube.com/@spcl ▪ Hope to see you again! Maybe in Design of Parallel and High-Performance Computing in the Masters ☺ ▪ … or in ETH’s Student Cluster Competition Team! ▪ … or for theses/research projects: http://spcl.inf.ethz.ch/SeMa/ Finito 70","libVersion":"0.3.2","langs":""}