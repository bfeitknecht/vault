{"path":"sem3/A&D/UE/s/A&D-s-u03.pdf","text":"Eidgen¨ossische Technische Hochschule Z¨urich Ecole polytechnique f´ed´erale de Zurich Politecnico federale di Zurigo Federal Institute of Technology at Zurich Departement of Computer Science 7 October 2024 Johannes Lengler, David Steurer Kasper Lindberg, Lucas Slot, Hongjie Chen, Manuel Wiedmer Algorithms & Data Structures Exercise sheet 3 HS 24 The solutions for this sheet are submitted on Moodle until 13 October 2024, 23:59. Exercises that are marked by ∗ are challenge exercises. They do not count towards bonus points. You can use results from previous parts without solving those parts. The solutions are intended to help you understand how to solve the exercises and are thus more detailed than what would be expected at the exam. All parts that contain explanation that you would not need to include in an exam are in grey. Asymptotic Notation The following two definitions are closely related to the O-notation and are also useful in the running time analysis of algorithms. Let N be again a set of possible inputs. Definition 1 (Ω-Notation). For f : N → R+, Ω(f ) := {g : N → R+ | f ≤ O(g)}. We write g ≥ Ω(f ) instead of g ∈ Ω(f ). Definition 2 (Θ-Notation). For f : N → R+, Θ(f ) := {g : N → R+ | g ≤ O(f ) and f ≤ O(g)}. We write g = Θ(f ) instead of g ∈ Θ(f ). In other words, for two functions f, g : N → R+ we have g ≥ Ω(f ) ⇔ f ≤ O(g) and g = Θ(f ) ⇔ g ≤ O(f ) and f ≤ O(g). We can restate Theorem 1 from exercise sheet 2 as follows. Theorem 1. Let N be an infinite subset of N and f : N → R+ and g : N → R+. • If lim n→∞ f (n) g(n) = 0, then f ≤ O(g), but f ̸= Θ(g). • If lim n→∞ f (n) g(n) = C ∈ R+, then f = Θ(g). • If lim n→∞ f (n) g(n) = ∞, then f ≥ Ω(g), but f ̸= Θ(g). Exercise 3.1 Asymptotic growth (2 points). For all the following functions n ∈ N and n ≥ 2. (a) Prove or disprove the following statements. Justify your answer. (1) 3n5 + 5n3 = Θ(4n4) Solution: False by Theorem 1, since lim n→∞ 3n5 + 5n3 4n4 = lim n→∞ 3 4 n + lim n→∞ 5 4n = lim n→∞ 3 4 n + 0 = ∞. (2) n2 + n log(n) ≥ Ω(n2 log(n)) Solution: False, by Theorem 1, since lim n→∞ n2 + n log(n) n2 log(n) = lim n→∞ 1 log(n) + lim n→∞ 1 n = 0 + 0 = 0. (3) 1 6 n6 + 10n4 + 100n3 = Θ(6n6) Solution: True by Theorem 1, since lim n→∞ 1 6 n6 + 10n4 + 100n3 6n6 = lim n→∞ 1 36 + 10 6n2 + 100 6n3 = 1 36 . (4) 3n ≥ Ω(n3/ ln(n)en) Solution: True by Theorem 1, since lim n→∞ 3n n3/ ln nen = lim n→∞ eln(3)n e3 ln(n)/ ln(n)en = lim n→∞ e ln(3)n−3−n = lim n→∞ e(ln(3)−1)n−3 = ∞. Since ln(3) − 1 > 0 and so limn→∞(ln(3) − 1)n − 3 = ∞. (b) Prove the following statements. Hint: For these examples, computing the limits as in Theorem 1 is hard or the limits do not even exist. Try to prove the statements directly with inequalities as in the definition of the O-notation. (1) √n2 + n + 1 = Θ(n) Solution: We have n2 + n + 1 ≤ n2 + 2n + 1 = (n + 1)2 so √ n2 + n + 1 ≤ n + 1 ≤ O(n). In the other direction, n2 + n + 1 ≥ n2 so √ n2 + n + 1 ≥ n = Ω(n). Together we get √n2 + n + 1 = Θ(n). 2 (2) ∑n i=1 log(ii) ≥ Ω(n2 log n) Hint: Recall exercise 1.2 and try to do an analogous computation here. Solution: To compute the sum, we first simplify each term from log(ii) to i log(i). Then, we lower bound the sum by only looking at the last n/2 terms, so n∑ i=1 log (i i) = n∑ i=1 i log(i) ≥ n∑ i=⌈n/2⌉ i log(i) Since we only look at terms indexed from i = ⌈n/2⌉ to n, we have that i ≥ n/2. Plugging this bound into each term we get n∑ i=⌈n/2⌉ i log(i) ≥ n∑ i=⌈n/2⌉ n 2 log ( n 2 ) = (n − ⌈ n 2 ⌉ + 1) n 2 log ( n 2 ) ≥ n 2 n 2 log ( n 2 ) = n2 4 log ( n 2 ) where the second inequality comes from ⌈n/2⌉ − 1 ≤ n/2. Now, we just use properties of log to further simplify n2 4 log ( n 2 ) = n2 4 (log (n) − log(2)) From here we can use Theorem 1 to argue that the right hand side is Ω(n2 log(n)). Computing the limit we have lim n→∞ n2 4 (log (n) − log(2)) n2 log(n) = lim n→∞ 1 4 − lim n→∞ log(2) 4 log(n) = 1 4 − 0 = 1 4 . Using this fact alongside the earlier inequalities, we get n∑ i=1 log (ii) ≥ n2 4 (log (n) − log(2)) ≥ Ω(n2 log(n)). (3) log(n2 + n) = Θ(log(n + 1)) Solution: First we factor n2 + n as n(n + 1) getting log(n2 + n) = log(n(n + 1)) = log(n) + log(n + 1). Then in one direction, log(n) + log(n + 1) ≥ log(n + 1) ≥ Ω(log(n + 1)). As log is monotone, log(n) ≤ log(n + 1) so in the other direction, log(n) + log(n + 1) ≤ 2 log(n + 1) ≤ O(log(n + 1)). Together we get log(n2 + n) = Θ(log(n + 1)). 3 (4)* ∑n i=1 1√i = Θ( √n) Hint: For the lower bound, recall exercise 1.2 and try to do an analogous computation here. For the upper bound, first prove the following inequality 1√i ≤ 2( √i − √i − 1) for all i ∈ N with i ≥ 1. Then analyze the new sum with these bounds. Solution: We first show ∑n i=1 1√i ≥ Ω(√n). For all 1 ≤ i ≤ n we have 1√i ≥ 1√n and thus n∑ i=1 1 √i ≥ n∑ i=1 1 √n = 1 √n n = √n, which shows that ∑n i=1 1√i ≥ Ω( √n). Second, we show that ∑n i=1 1√i ≤ O( √n). We have that √i ≥ √i − 1 so 2√i ≥ √ i + √i − 1 and therefore we get the bound 1 √ i ≤ 2 √i + √i − 1 . Multiplying the top and bottom by √i − √i − 1 we get 2( √i − √i − 1) ( √i + √i − 1)(√i − √i − 1) = 2( √i − √i − 1) i + √i √i − 1 − √i − 1 √i − (i − 1) = 2( √i − √i − 1) 1 . Combining this with the previous inequality we get 1 √ i ≤ 2( √i − √i − 1) Using this inequality on each term we have n∑ i=1 1 √i ≤ 2 n∑ i=1 √i − √i − 1 Notice that this sum telescopes, meaning that for each k ∈ N such that 1 ≤ k < n, a √k comes from the k entry of the sum and a − √k comes from the k + 1 entry of the sum. Therefore the sum simply equals the terms which do not get cancelled out. This will be √n from the nth entry and a − √0 from the 1st entry. So 2 n∑ i=1 √i − √i − 1 = 2( √n − √0) = 2 √n ≤ O( √n), getting us ∑n i=1 1√i ≤ O( √n). Combining everything we have ∑n i=1 1√i = Θ( √ n). Guidelines for correction: The exercise consists of 7 items (excluding b.4). Award 1/2 point for the first correctly solved item. Then award 1/2 point for each two additional correctly solved items (so 3 gives 1 point, 5 gives 1.5 points, and 7 gives 2 points). Exercise 3.2 Substring counting. 4 Given a n-bit bitstring S[0..n − 1] (i.e. S[i] ∈ {0, 1} for i = 0, 1, ..., n − 1), and an integer k ≥ 0, we would like to count the number of nonempty substrings of S with exactly k ones. Assume n ≥ 2. For example, when S = “0110” and k = 2, there are 4 such substrings: “011”, “11”, “110”, and “0110”. (a) Design a “naive” algorithm that solves this problem with a runtime of O(n3). Describe the algorithm using pseudocode. Justify the runtime (you don’t need to provide a formal proof, but you should state your reasoning). Solution: We can for example use the following algorithm: The following algorithm works by creating every possible substring and checking if it fulfills the required condition. Therefore, it is considered to be “naive”. Algorithm 1 Naive substring counting c ← 0 ▷ Initialize counter of substrings with k ones for i ← 0, . . . , n − 1 do ▷ Enumerate all nonempty substrings S[i..j] for j ← i, . . . , n − 1 do x ← 0 ▷ Initialize counter of ones for ℓ ← i, . . . , j do ▷ Count ones in substring if S[ℓ] = 1 then x ← x + 1 if x = k then ▷ If there are k ones in substring, increment c c ← c + 1 return c ▷ Return number of substrings with k ones Runtime: The nested for-loops have three levels and each level has at most n iterations, leading to O(n3) iterations in total. Each iteration runs in O(1) time. Thus the total running time is O(n3). Correctness: Follows directly from the description of the algorithm (see comments above). (b) We say that a bitstring S′ is a (non-empty) prefix of a bitstring S if S′ is of the form S[0..i] where 0 ≤ i < length(S). For example, the prefixes of S = “0110” are “0”, “01”, “011” and “0110”. Given a n-bit bitstring S, we would like to compute a table T indexed by 0..n such that for all i, T [i] contains the number of prefixes of S with exactly i ones. For example, for S = “0110”, the desired table is T = [1, 1, 2, 0, 0], since, of the 4 prefixes of S, 1 prefix contains zero “1”, 1 prefix contains one “1”, 2 prefixes contain two “1”, and 0 prefix contains three “1” or four “1”. Design an algorithm prefixtable that computes T from S in time O(n), assuming S has size n. Describe the algorithm using pseudocode. Justify the runtime (you don’t need to provide a formal proof, but you should state your reasoning). Solution: 5 Algorithm 2 function prefixtable(S) T [0..n] ← a new array of size (n + 1) ▷ Initialize array s ← 0 for i ← 0, . . . , n − 1 do ▷ Enumerate all prefixes S[0..i] s ← s + S[i] ▷ s saves the number of “1” in S[0..i] T [s] ← T [s] + 1 ▷ S[0..i] is a prefix with s “1” return T The idea of the algorithm is iterating over all prefixes and counting the 1’s. For each prefix we increase the table’s count for the current number of 1’s. As this can only increase, we can create the table in O(n). Runtime: The for loop has n iterations and each iteration runs in O(1) time, so the total runtime is O(n). Correctness: The correctness directly follows from the description of the algorithm (see comments above). Remark: This algorithm can also be applied on a reversed bitstring to compute the same table for all suffixes of S. In the following, you can assume an algorithm suffixtable that does exactly this. (c) Consider an integer m ∈ {0, 1, . . . , n − 2}. Using prefixtable and suffixtable, design an al- gorithm spanning(m, k, S) that returns the number of substrings S[i..j] of S that have exactly k ones and such that i ≤ m < j. For example, if S = “0110”, k = 2, and m = 0, there exist exactly two such strings: “011” and “0110”. Hence, spanning(m, k, S) = 2. Describe the algorithm using pseudocode. Mention and justify the runtime of your algorithm (you don’t need to provide a formal proof, but you should state your reasoning). Hint: Each substring S[i..j] with i ≤ m < j can be obtained by concatenating a string S[i..m] that is a suffix of S[0..m] and a string S[m + 1..j] that is a prefix of S[m + 1..n − 1]. Solution: Each substring S[i..j] with i ≤ m < j is obtained by concatenating a string S[i..m] that is a suffix of S[0..m] and a string S[m + 1..j] that is a prefix of S[m + 1..n − 1], such that the numbers of “1” in S[i..m] and S[m + 1..j] sum up to k. Moreover, from each S[i..m] that contains p ≤ k ones, we can build as many different sequences S[i..j] with k ones as there are substrings S[m + 1..j] with k − p ones. We obtain the following algorithm: Algorithm 3 function spanning(m, k, S) T1 ← suffixtable(S[0..m]) T2 ← prefixtable(S[m + 1..n − 1]) return ∑min(k,m+1) p=max(0,k−(n−m−1))(T1[p] · T2[k − p]) Runtime: O(n). 6 (d)* Using spanning, design an algorithm with a runtime1 of at most O(n log n) that counts the number of nonempty substrings of a n-bit bitstring S with exactly k ones. (You can assume that n is a power of two.) Justify its runtime. You don’t need to provide a formal proof, but you should state your reasoning. Hint: Use the recursive idea from the lecture. Solution: Whenever n ≥ 2, we can distinguish between: • Substrings with k ones located entirely in the first half of the bitstring, which we compute recursively; • Substrings with k ones located entirely in the second half of the bitstring, which we also compute recursively; • Substrings with k ones that span the two halves, which we can count using (c). We obtain the following algorithm: Algorithm 4 Clever substring counting function countsubstr(S, k, i = 0, j = n − 1) if i = j then if k = 1 and S[i] = 1 then return 1 else if k = 0 and S[i] = 0 then return 1 else return 0 else m ← ⌊(i + j)/2⌋ return countsubstr(S, k, i, m) + countsubstr(S, k, m + 1, j) + spanning(m, k, S[i..j]) Runtime: By subpart (c), the spanning subroutine needs time O(n). Thus, if we denote the com- plexity of the algorithm by A(n), then it is given by the recursive expression A(n) ≤ 2A( n 2 )+O(n) (with the base case being A(2) ≤ O(1)). Simplifying this, we get A(n) ≤ 2A ( n 2 ) + O(n) ≤ 2 · (2A ( n 4 ) + O ( n 2 )) + O(n) ≤ 4A ( n 4 ) + 2O ( n 2 ) + O(n) ≤ 4A ( n 4 ) + O(n) + O(n) ≤ . . . ≤ O (n log(n)) . The reason this is O(n log(n)) is that for halving n, we get a factor of O(n). Thus, to get n down to a constant, we need log(n) steps and thus overall A(n) is bounded by O (n log(n)). Note that we cannot absorb the factor “2” in 2O(n/2) in the O-notation and then conclude that the runtime is O(n) + O(n/2) + O(n/4) + . . ., which would be O(n). The reason is that when we write the full expression (assuming for simplicity that n is a power of 2), we get A(n) ≤ ∑log(n) i=0 2i · O(n/2i) and thus in general the factor 2i is not a constant factor. 1For this running time bound, we let n range over natural numbers that are at least 2 so that n log(n) > 0. 7 To prove the above formally, we first show the statement for n = 2k for k ∈ N. Let C by a constant such that A(2k) ≤ 2A(2k−1) + C · 2k for all k ∈ N and A(2) ≤ C. Then, we can prove the identity A(n) = A(2k) ≤ C · 2k · k = C · n log(n) for any n = 2k by induction over k ∈ N: Base case. Let k = 1, then A(21) ≤ C ≤ C · 2 log(2) by assumption. Induction hypothesis. Assume A(2ℓ) ≤ C · 2ℓ · ℓ holds for some ℓ ∈ N. Induction step. We now want to show A(2ℓ+1) ≤ C · 2ℓ+1 · (ℓ + 1). Using the recursive formula from above and then the induction hypothesis we get A(2 ℓ+1) ≤ 2A(2 ℓ) + C · 2ℓ+1 ≤ 2 · (C · 2 ℓ · ℓ) + C · 2 ℓ+1 = C · 2 ℓ+1(ℓ + 1). By the principle of mathematical induction, we get A(n) ≤ C · n log(n) for all n of the form n = 2k for k ∈ N. To prove the general case for any n ≥ 2, we use that A(n) ≤ A(2⌈log(n)⌉) to get A(n) ≤ A(2 ⌈log(n)⌉) ≤ C · 2⌈log(n)⌉ · ⌈log(n)⌉ ≤ 8C · n log(n), where the last step used that ⌈log(n)⌉ ≤ 2 log(n) and 2⌈log(n)⌉ ≤ 2log(n)+1 = 2n. Thus, we get A(n) ≤ 8C · n log(n) ≤ O(n log(n)) for every n ∈ N≥2 as wanted. Exercise 3.3 Counting function calls in loops (1 point). For each of the following code snippets, compute the number of calls to f as a function of n ∈ N. Provide both the exact number of calls and a maximally simplified asymptotic bound in Θ notation. In your expression for the exact number of calls, you are allowed to use summation signs. For exam- ple, ‘ ∑2n−1 k=1 (k + 2) + 32n’ is a valid expression. In your simplified Θ notation this is not allowed. Furthermore, it should not have any unnecessary terms or factors. For example, ‘Θ(3n + 2)’ is not valid. Algorithm 5 (a) i ← 0 while i ≤ n do i ← i + 1 f () j ← 1 while j ≤ n do f () f () j ← j + 1 Solution: This algorithm performs ∑n i=0 1 + ∑n i=0 ∑n j=1 2 = (n + 1) + (n + 1) · 2n = n + 1 + 2n2 + 2n = 2n2 + 3n + 1 = Θ(n2) calls to f . 8 Algorithm 6 (b) i ← 1 while i ≤ 2n do j ← 1 while j ≤ i3 do k ← n while k ≥ 1 do f () k ← k − 1 j ← j + 1 i ← i + 1 Hint: See Exercise 1.2. You are allowed to use any statement from that exercise without proof. Solution: This algorithm performs ∑2n i=1 ∑i3 j=1 ∑n k=1 1 = ∑2n i=1 ∑i3 j=1 n = n ∑2n i=1 ∑i3 j=1 1 = n ∑2n i=1 i3 = Θ(n · (2n)4) = Θ(n · 24 · n4) = Θ(n5) calls to f . For the fourth equality, we have used the fact that there are constants C1, C2 > 0 such that C1·m4 ≤∑m i=1 i3 ≤ C2 · m4 for any m ∈ N, plugging in m = 2n. This was shown in Exercise 1.2. Guidelines for correction: The exercise consists of four items, two per part: the exact and the simplified Θ expression. Award 1 point if all for items are correct. Award 1/2 point if at least two items are correct. Do not subtract points for computational mistakes if the final simplified expression is correct. Exercise 3.4 Fibonacci numbers. There are a lot of neat properties of the Fibonacci numbers that can be proved by induction. Recall that the Fibonacci numbers are defined by f0 = 0, f1 = 1 and the recursion relation fn+1 = fn + fn−1 for all n ≥ 1. For example, f2 = 1, f5 = 5, f10 = 55, f15 = 610. (a) Prove that fn ≥ 1 3 · 1.5n for n ≥ 1. In your solution, you should address the base case, the induction hypothesis and the induction step. Solution: Base Case. We prove that the inequality holds for n = 1 and n = 2. For n = 1: fn = 1 and 1 3 · 1.5n = 0.5, so fn ≥ 1 3 · 1.5n. For n = 2: fn = 1 and 1 3 · 1.5n = 0.75, so fn ≥ 1 3 · 1.5n. As the recursive definition of the Fibonacci Numbers relies on two previous instances, they require two base cases and two induction hypothesis. 9 Induction Hypothesis. We assume that it is true for n = k and n = k + 1, i.e., fk ≥ 1 3 1.5 k fk+1 ≥ 1 3 1.5 k+1 Inductive Step. We must show that the property holds for n = k + 2, k ≥ 1. We have: fk+2 by def. = fk+1 + fk I.H. ≥ 1 3 1.5k+1 + 1 3 1.5 k = 1 3 1.5 k · (1.5 + 1) = 1 3 1.5 k · 2.5 ≥ 1 3 1.5 k · 2.25 = 1 3 1.5 k · 1.5 2 = 1 3 1.5 k+2 By the principle of mathematical induction, this is true for every integer n ≥ 1. Remark: In a similar way to the proof above, one can also show that fn+1 ≤ 1.75n for n ≥ 0. (b) Design an O(n) algorithm that computes the nth Fibonacci number fn for n ∈ N. Describe the algorithm using pseudocode. Justify the runtime (you don’t need to provide a formal proof, but you should state your reasoning). Remark: As shown in part (a), fn grows exponentially (e.g., at least as fast as Ω(1.5n)). On a physical computer, working with these numbers often causes overflow issues as they exceed variables’ value limits. However, for this exercise, you can freely ignore any such issue and assume we can safely do arithmetic on these numbers. Solution: Algorithm 7 F [0..n] ← an array of (n + 1) integers F [0] ← 0 F [1] ← 1 for i ← 2, . . . , n do F [i] ← F [i − 2] + F [i − 1] return F [n] This algorithm is a simple iterative implementation the Fibonacci Numbers, defining the non- recursive cases and then iterating over the recursive ones. Runtime: Each of the n iterations has complexity O(1), yielding a total complexity of O(n). Correctness: At the end of iteration i of this algorithm, we have F [j] = fj for all 0 ≤ j ≤ i. Hence, at the end of the last iteration, F [n] contains fn. 10 (c) Given an integer k ≥ 2, design an algorithm that computes the largest Fibonacci number fn such that fn ≤ k. The algorithm should have complexity O(log k). Describe the algorithm using pseu- docode and formally prove its runtime is O(log k). Hint: Use the bound proved in part (a). Solution: Consider the following algorithm, where we can just assume for now that K is ‘large enough’ so that no access outside of the valid index range of the array is performed. We will decide the value of K later. Algorithm 8 F [0..K] ← an array of (K + 1) integers F [0] ← 0 F [1] ← 1 i = 1 while F [i] ≤ k do i ← i + 1 F [i] ← F [i − 2] + F [i − 1] return F [i − 1] Runtime: After the ith iteration, we have F [j] = fj for all 0 ≤ j ≤ i. The loop exists when the condition F [i] = fi > k is satisfied for the first time, and, in this case, F [i − 1] = fi−1 is the largest Fibonacci number smaller or equal to k. Using part (a), we have k ≥ fi ≥ 1 3 · 1.5i. We can rewrite k ≥ 1 3 · 1.5i as i ≤ log1.5(3k) = log 3 + log k log 1.5 ≤ 3(2 + log k) ≤ O(log k). Although we use base 2 for logarithms, it’s not necessary to specify the base of logarithms within O-notation in this case, since different bases are equivalent up to constants, which are irrelevant for the O-Notation. Therefore, the while loop can only execute O(log k) iterations. Also, we can choose K = ⌈3(2 + log k)⌉ to always create an array of sufficient size. Since every iteration of the while-loop has complexity O(1), we get an overall complexity of O(log k). The algorithm is the same as the one in (b), except that we check if the Fibonacci number we just compute is at most k after each iteration. We know from (b) that it takes O(n) time to compute fn. Here, since we only need to compute fn for n ≤ O(log k), the runtime is O(log k). Exercise 3.5 Iterative squaring. In this exercise you are going to develop an algorithm to compute powers an, with a ∈ Z and n ∈ N, efficiently. (a) Assume that n is even, and that you already know an algorithm An/2(a) that efficiently computes an/2, i.e., An/2(a) = an/2. 11 Given the algorithm An/2, design an efficient algorithm An(a) that computes an. (You don’t need to argue correctness or runtime.) Solution: Algorithm 9 An(a) x ← An/2(a) return x · x (b) Let n = 2k, for k ∈ N0. Find an algorithm that computes an efficiently. Describe your algorithm using pseudo-code. (You don’t need to argue correctness or runtime.) Solution: Algorithm 10 Power(a, n) if n = 1 then return a else x ← Power(a, n/2) return x · x (c) Determine the number of integer multiplications required by your algorithm for part (b) in O- notation. You may assume that bookkeeping operations don’t cost anything. This includes handling of counters, computing n/2 from n, etc. Solution: Let T (n) be the number of integer multiplications that the algorithm from part (b) performs on input a, n. Then T (n) ≤ T (n/2) + 1 ≤ T (n/4) + 2 ≤ T (n/8) + 3 ≤ . . . ≤ T (1) + log n ≤ O(log n) .2 The log n can be deduced by the fact that n halves in every step and recursion stops when a non- recursive definition is reached. (d) Let Power(a, n) denote your algorithm for the computation of an from part b). Prove the correctness of your algorithm via mathematical induction for all n ∈ N that are powers of two. In other words: show that Power(a, n) = an for all n ∈ N of the form n = 2k for some k ∈ N0. In your solution, you should address the base case, the induction hypothesis and the induction step. Solution: 2For this asymptotic bound, we let n range over natural numbers that are at least 2 so that log(n) > 0. 12 Base Case. Let k = 0. Then n = 1 and Power(a, n) = a = a1. Induction Hypothesis. Assume that the property holds for some positive integer k. That is, Power(a, 2k) = a2k . Inductive Step. We must show that the property holds for k + 1. Power(a, 2k+1) = Power(a, 2k) · Power(a, 2k) I.H. = a2k · a 2k = a2k+1. By the principle of mathematical induction, this is true for any integer k ≥ 0 and n = 2k. (e)* Design an algorithm that can compute an for a general n ∈ N, i.e., n does not need to be a power of two. You don’t need to argue about correctness or runtime. Hint: Generalize the idea from part (a) to the case where n is odd, i.e., there exists k ∈ N such that n = 2k + 1. Solution: Algorithm 11 Power(a, n) if n = 1 then return a else if n is odd then x ← Power(a, (n − 1)/2) return x · x · a else x ← Power(a, n/2) return x · x 13","libVersion":"0.3.2","langs":""}