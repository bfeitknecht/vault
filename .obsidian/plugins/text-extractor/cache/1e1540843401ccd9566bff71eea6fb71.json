{"path":"sem1/LinAlg/PV/cheatsheets/LinAlg-cheatsheet-laurence.pdf","text":"COMPLEX NUMBERS Hermitian/conjugate transposed Skew (= schief) De f : A\" = (A)\" = IA) · (AB)\" = B + At symmetric NT= - A# in = -1 , löst - = -1 Division · A\" = A A hermitian /A+ B)\" = A*+B + Imaginary Number : i = Fr & 2 H Complex Number : Ex t iy , Re(z) = X , (m(z) = Y Graphisch: (diag = reel) · (xA)\" = 2 A Ro tatio n der durch Sin(0) Im die er s t e und dritte Con * - m(z) >r . e2 - 0 jugate : E = X- in Im (z) = - 1 · (A + )* = A Achse aufg. Ebene 7 : (z1= y = mModulus knorm, I * z v Jetrag te i ↑ Addition : 21 = zz = (xn + xz) = i(yn + yz)- Rotationsmatrix Blockmatrix sindMultiplication : = latib) (c + id) = dc - bd + ilad + b) · Kosusind a od e r(4 O( A= Wurzel : \" d = zu Es lal . ei = ph . einn es =al , w = · E - since o cosul·arctant 1 . Q O O 1 Def : ↑ [arctan = + + 2 . /3 .0 D Forms : arc t an * + 2 pi 4 . Q .- Cartesian form : Ex i Polar form : · Erle r form : Freit , It = 0 is modulus of z) * invertierbar = T . cos(d) + i · Sin(d) · Trigonomical form: z = r(cos e + sing) - (kann rechnenWo b e i x = ro s ( t ) , Polynomials : eit sie erleichter Therootopdynomia ar e pairwise conjugat VEKTOREN UND MATRIZI 22 Def : az\" + c = 0Ez = uz Vektoren Icos(x)1 . IIv11 . II wll ~ Th : Any degreenon-constant polynomia PE nt Cauchy Schwarz : Iv . w = Ilvll . IIw/ X n - 1 - Y O Winkel : cosk) = wil for viw -o ! perplanesa Zero : 1 t ( S . t . P(X ) = 0 d = 0 I · n ze ro s : X1 , ..., Xn EK , perhaps with repetitions , S . t. Triangle inequality :Ilv + wll = IIv11 + 11 w/l the set EvER\" : v . d = 03 ↑ (z) =Xn (z - (1)(z - (2) ... (z - 2n) Matrices algebraic multiplicity of the ze ro : number of times X- D appears Z · le zuerst : man : M Zeilen , n Spalten , dij : Element Zeile i , Spaltej - er* Anzahl unabh . Kolonnenrang(A) = Det : Matrix Set : Thesetofmyn-matrice WrittensEr D ABDef : Matrix Mult : If C = AB then on e can Wr it e (ij =) Lij S 2 . 1 : = n jAI LOS Def : Zerodiviser : If AB = 0 A ,B Zerodiviser (Nullteiler) GAuss : (Ziel : Zeilenstufenform) versch . Matrizen Mor] · Upper Triangular : 11 Xn · Identity : [i] · Diagonal · (Rij = 0 for is j O - · Il lij = 0 if i j · Lower Triangular : (4) : : PivotsrankAe · Diagh, · Diag(den , ..., dun · (Rij = 0 for i < j~of ... X I3 m-r : zero ro w s (VB) regulär : Voller Rang > · Nullmatrix : [0]LHS Ax = br vx = C We n n RHS = 0 · Arist reg . und IA)\" = A Singulär : so ns t W in triangular form homogen · AB ist reg . und (AB) = BA · A) = 0 L : Untere Dreiecksmatrix ,Infos über Zeilensubtraktionen Anzahl Lösungen : · AH ist reg . und (At)\" = (A) + · Wenn A quadr : O ist E-value V : = A in Echelon form ~ r = n = m1 L regulär · LGS Ax = b hat eindeutige m : X = A - 1 b& Lösungen : 1000/f0) =maran g Lösung A- Im * A = LU , sprich (Ux = b = (c = b mit Ux = > = me a Lg (n-r) freie Var · det (A) = 0 · det(A-) = Get(A) - 1 En 1Lg trisch .. mit Permutation1 Lösung: Le s · Lsg . In-r) freie Var. · AA ist invertible n symme PA = LV , sprich (Ux = Pb = Pb mit+ VI3 · A und A dieselben E-Values , nicht E-Vectors. Vertra glichkeitsbed : brtn =... = bm = 0 Achtung: Pivotieren vertauscht auch die Lijsvon e re i c h t = Pb Bei homogenen LGS sind die VB immer erfüllt. Tran Ose ① Garss : (A] n [U] => Lu x = Pb Statt P( c = Pb) viale LGS · Si · (AT)\" = A CNur we n n ran gibt es nicht tri · detiAT) = det(A) A-y = ABBAABC T & L = Pb lösen forward' subs tituti on => C negaler.Back substitution equation substitution so lutio n 3 + xz = 14 Xz = 2 ↓ &= [ YS (: ) . + (5 : b.] ③ lösenbackward substitutionim msmmetric lef : AT = A A is symmetric V => LDLT , sprich LDLTx = b ande El . dof Cheatsheet? => b mit mit d Elimindpir · AB = BA Es AB is symm . if A and B symm.. Spieg, - - . #S S . 13 · ATA and AAT are symm . 2 - 1 ... - · L : (b) , D : [2] , <T : (* ]-**- T ~ · A symm. => Arsymm . (falls existiert) = Callg . gilt A = LD , we n n A = A = U = LT) #... Zun d - Permutationsmatrix : ↑ : = I mit Zeilenvertauschun n ,ge · zB : /0 desgla s T &O det (PA) = - det(A) n) Inverse I A quadratisch un, rang(A) = n Es AA = AA = I > A invertierbar · B A = (AB) + NIA) only ↑ E X is t eindeutigcon t ain sa- v e c t. Ex A regular nxn- ma tr ix - &· 0 % => AC\" = det(A) I det (A-) = A) For inv . S : A an d S-AS have the sa me e-values 1 and va r e e- e . pair => and r are e- e . pair of A1 onal/Unitär Matrix Orthog · AAT = In/AAt = In JA = AT Outer Product and Proj · AA = In /A * A = In Sind A und Bunitär/orthonormal so gilt : · A is t regulär und A = A*. Aris t unitàr/orthogonal -n · AA\" = 1 · AB ist Unitär/orthogonal -e in e orthogonale Matrix hat orth on ormale Spalten · det(A) = = 1 · If X & is e-value of Q , then 1x1 = 1 · If the columns of A ar e pairwise Orthogonal , then AA is a diagonal Matrix , whic h is easy to invert · Images from unitary/orthonormal matrices are conformal (längen-Winkeltreu LU-Decomp (Ax)\"(Ax) = ll Ax112 ~ CR-Decomp . I A = CR , wo b e i rank(A) = rank(C) und AMu n I CEMmxr , ReMrxn Skalarprodukt und Norm + cial/autorepeS parti sol Rank W Let B 2 E3⇥1 and C 2 E1⇥3 : BC can have rank 3. Vectorspaces W Let V be a vector space over R with scalar product h·, ·i and let F : V 7! V be a linear map. If it holds that 8v 2 V, hv, F (v)i =0,thenFis necessarily the null map, i.e., F (v)=0 for all v 2 V . W Given a vector space V with a norm k·k.For all u, v 2 V ,wehave kvkkv + uk. counterexample: u = \u0000v =(1, 1)T C Let S ⇢ V and W be a subspace of V : S ⇢ W ) span(S) ⇢ W C In a vector space of ﬁnite dimension with scalar product, one can complete any set of orthonormal vectors to form an orthonormal basis. C Avector space of ﬁnite dimension withscalar product has an orthonormal basis. Corollary 6.7 W Consider the vector space Rn with the Euclidean scalar product. The scalar product of two unit vectors can be arbitrarily large. Cauchy Schwarz, S 6.1: hv, wi2 hv, vihw, wi =1 · 1=1 W We again consider Rn with the Euclidean scalar product. Can we ﬁnd any number of pairwise orthogonal unit vectors in this vector space? Let V the standardvectorspace of all 2 ⇥ 2 matrices. Which of the following are subspaces of V ?(B = 12 34 ) W {A 2 V|A is invertible} W nA 2 V|A2 =0o C nA 2 V|AT = Ao C nA 2 V|AT B = BA o Consider the vector space F of functions of R 7! R with the operations addition(f + g)(x)= f (x)+ g(x) and scalar multiplication (\u0000f )(x)= \u0000f (x).This includes the subspace P 2= {a0 + a1 x + a2 x2 |ai 2 R} of polynomials of degree  2.Which of the following statements are correct? C Span{x +1,x1,x2 +1,x2 1} is equal to P2 . W x +1,x1,x2 +1,x2 1 2 P2 are linearly independent W x +1,x1,x2 +1,x2 1 2 P2 form a generating set (spanning set) of F . C x +1,x1,x2 +1,x2 1 2 P2 form a generating set (spanning set) of P2 . W The polynomials x +1,x1,x2 +1,x2 1 2 P2 form a basis of P2 Let V, W be ﬁnite dimensional vector spaces over a space K.Let F : V 7! W be a linear mapping and (v1 , ·· · ,vn ) abasis of V .Then it holds that: W F (v1 ), ·· · ,F (vn ) are linearly independent if F is surjective C F (v1 ), ·· · ,F (vn ) are linearly independent if F is injective C F (v1 ), ·· · ,F (vn ) form a generating end system if F is surjective W F (v1 ), ·· · ,F (vn ) form a generating end system if F is injective C F (v1 ), ·· · ,F (vn ) form a basis if and only if F is an isomorphism Let V, W be two real vector spaces with scalar products, let B be an orthonormal basis of V and let F : V 7! W be an orthogonal mapping. Which of the following statements is true? W F is an isomorphism. Fis onlyanisomorphism if dimV = dimW < 1 C kF (v)kW = kvkv for all v 2 V . Follows directly from the deﬁnition of the scalar product induced norm and the orthogonality of F. W If dimV, dimW < 1,it ispossiblethat dimV > dimW With dimV > dimW there is no orthogonal mapping F : V 7! W . W F is not injective. C is angle-preserving, i.e., for all v, w 2 V it holds that ^(F (v),F (w)) = ^(v, w). C F is an isomorphism to the image of F . F is injective as shown before and obviously surjective to its image. Since the inverses of linear mappings are also linear, F is therefore an isomorphism W The set of images F (B) is an orthonormal basis of W . Since F is not necessarily an isomorphism, the image space can be smaller than W . C The set of images F (B) is an orthonormal basis of Im(F )This follows from the isomorphism property of F from the question above C If it exists,F \u00001 : Im(F ) 7! V is orthogonal. It exists as seen above. The orthogonality of the inverse follows directly from the deﬁnition of orthogonal mappings. Det W Let Q be unitary and A 2 En⇥n , det(QA)= det(A) |detQ| = ±1 C if A, B, P 2 En⇥n and P is invertible with A = PBP \u00001 then: det(A)= det(B) Given is a matrix A 2 Rn⇥n with entries aij = ij and n> 1.Which statement is correct? W det(A)=1 C det(A)=0 W det(A)=(\u00001)n W det(A)=(\u00002)n Which of the following statements are not correct for arbitrary n ⇥ n-matrices A and B? C det(A + B)= det(A)+ det(B) W det(AB)= det(BA) W If A is singular then AB is also singular W det(AAT A)=(det(A))3 Let A, B 2 Rn⇥n with AB = \u0000BA C det(AB)= det(\u0000BA) W det(A)det(B)) = \u0000det(A)det(B) nhas to be even )S8.4 v W Either A or B has a zero-determinant W Aand Bhave to be singular W ABx =0 has more than one solution (L¨osungsschar) C ABx = c can have no, one and 1 many solutions, if c 2 R2 ,c 6=0 W It has to be A =0 or B =0 Which of the following statements are correct for an arbitrary n ⇥ n-matrix A and for arbitrary n? W det(2A)=2det(A) W det(\u0000A)= det(A) C det(A4 )= det(A)4 W Let A be a triangular matrix with the property ai,j =0fori + j> n +1 (so there are zeros at the bottom right). The determinant can be calculated using the formula det(A)= a1,n · a2,n1 ·· · an,1 . Let A, P, Q 2 Rn⇥n where P is permutation matrix and Q is a unitary matrix. W det(PA)= det(A) C det(PAP )= det(A) W det(QA)= det(A) Eigenvalue/Eigenvectors C with XA (\u0000)=(\u0000 \u0000 1)3 +3 is A 2 E3⇥3 invertible 0isn’t eigenvalue ) A is regular W Let v1 and v2 be eigenvectors of A,so is v1 + v2 an eigenvector C If A 2 En⇥n and XA (\u0000)=(\u0000 \u0000 1)n +2,Ais invertible C Similar matrices have the same eigenvalues W Similar matrices have the same eigenvectors W Every n ⇥ n matrix has linear independant eigenvectors W Eigenvectors, which correspond to the same eigenvalue are always linear dependant C If a real matrix has a eigenvector, it follows that the matrix has inﬁnity many eigenvectors C Every rotation in R3 has the eigenvalue \u0000 =1 W If \u00001 with v and \u00002 with w,so is (\u00001 + \u00002 ) a eigenvalue with eigenvector v + w Let A 2 R3⇥3 with eigenvalues \u00001 , \u00002 , \u00003 C Ais diagonisable if all eigenvalues are di↵erent W if A is diagonisable, all eigenvalues have to be di↵erent W Ais diagonisable if it has 3 eigenvectors They have to bo linearly independant C If \u00001 =2, \u00002 = \u00002, \u00003 =1 and B = A3 \u0000 3A3 then is B diagonisable W If AP = PD and D is a diagonalmatrix then the columns of P are eigenvectors of A Only if the eigenvalues of A are on the diagonal of D Let A 2 Rn⇥n be positiv deﬁnite and symmetric. Further, let \u00001 , ·· · , \u0000n be the eigenvalues to the eigenvectors v1 , ·· · ,vn W A2 has at least one eigenvalue with a strictly positive imaginary part C it holds that \u0000j > 0 for all j =1, ·· · ,n W A hast at least one eigenvalue which satisﬁes: geom.mult. < alg.mult W The eigenvalues are pairwise distinct: \u0000j 6= \u0000i ,if j 6= i C There exist positiv real numbers ↵ > 0 such that vT Av \u0000 vT v for all v 2 Rn Let A 2 E2⇥2 with Rank(A)=1 and Trace(A)=5.What are the eigenvalues W 1 is an eigenvalue C 0 is an eigenvalue W 2 is an eigenvalue W \u00005 is an eigenvalue C 5 is an eigenvalue Since det(A)=0 and trace(A)=5 ) 0= \u00001 · \u00002 and 5= \u00001 + \u00002 Decompositions C Amatrix A 2 Rn⇥n with n eigenvalues has 2n normed spectral decompositions since the sign can be changed n-times Let A 2 Rm⇥n m \u0000 n be a matrix with rank k. Denote the QR-decomposition of A as A = QR,where Q 2 Rm⇥k has orthonormal columns, and R 2 Rk⇥n is an upper (right) triangular matrix. Which one of the following statements is always true? W Rank A < Rank R W QQT = I W If A has linearly independant columns, we have Rank R = m C If A has linearly independant columns, we have Rank R = n Let A 2 Rm⇥n with linearly independant columns and A = Q1 R1 = Q2 R2 ,twoQR-Decompositionsof A C QT 1 Q2 is orthogonal C QT 1 Q2 is a upper and lower triangular matrix and therefore a diagonal matrix W QT 1 Q2 = I W Rank(R1 )= m C Rank(R1 )= n W Rank(R2 )= m C Rank(R2 )= n C QT 1 Q2 is regular Let A, B 2 Rn⇥m,B is regular and B = QR W f : Rn 7! R,x 7! kAxk2 ,is anorm in Rn C If 69k, such that Ak is invertible, so is A not invertible C Vx 2 Rn , kAxk2 kAk2 · kxk2 W det(B)= det(R) C kBk2 = kRk2 C g : Rn 7! R,x 7! kQxk2 ,is anorm in Rn W AB is regular, but BA is not necessarily W kABk2 kAk2 Basis W The transformation matrix of a basis transformation between orthonormal bases is the identity matrix. The transformation matrix of base transformation between orthonormal bases is orthogonal, the identity matrix is only one possibility. C The inverse of the transformation matrix of a base transformation between orthonormal bases is its Hermitian transpose. C A 2 Rn⇥n is an orthogonal matrix if and only if its columns form an orthonormal basis of R with respect to the Euclidean scalar product. C The change of basis matrix is unitary (if E = C) or orthonormal (if E = R)if bothbasesare orthonormal. Procedures W The Gram-Schmidt orthogonalization method can be used to compute an equally large set of linearly independent vectors from a set of linearly dependent vectors. W Let v1 , ·· · ,vn ⇢ Rn be a set of n vectors. Using the Gram-Schmidt process, we can always produce n unit-length and pairwise orthogonal vectors. Gram Schmidts needs linear independant vectors Let A 2 Rn⇥n ,m < n.Let Ax = b be a system of linear equations and let x be a solution in the least squares sense. Which statement is always correct? W The vector (bAx) is orthogonal to the row space of A. C The vector (bAx) is orthogonal to the column space of A. ) normal equations W x is in the null space of A. W The solution x does not always exist Kernel/Image W If the nullspace of an 8 ⇥ 7 matrix is 5-dimensional, the rowspace has dimension 3 n \u0000 dim(Ker(A)) = 7 \u0000 5=2= dim(Im(A)) Let A 2 Rm⇥n be such that Ax =0 has only the trivial solution. Then it holds that: C dimIm(A)= n W dimIm(A)=1 C dimKer(A)=0 W dimKer(A)=1 The kernel of A is exactly the solution set of the system of equations Ax =0. Since Ax =0 has only the trivial solution, dimKer(A)=0.Furthermore, it holds that dimKer(A)+ dimIm(A)= n.Therefore dimIm(A)= n. Which of the following statements with A 2 Rn⇥n is generally true C im(A)= im(2A) C ker(A))ker(2A) W im(A)= im(A2 ) W im(A)= im(A + I) W im(A)= im(AT ) W ker(A))ker(A2 ) W ker(A))ker(A + I) W ker(A))ker(AT ) Proofs 1) Prove that AH A and AAH have the same eigenvalues We have that AH A and AAH are similar with T = A and T \u00001 = AH Therefore by Satz 9.7 they have the same eigenvalues (and also the same trace and det) 2) Let Q 2 En⇥n be an orthogonal matrix. Prove that, if n is odd, that at least one of the matrices (Q + I) and (Q \u0000 I) singular. Let KQ (x) be the characteristic polynomial of Q. \u0000 is an eigenvalue of Q , \u0000 is a root of KQ (x) Q is orthogonal ) |\u0000| =1 By Lemma 9.2 we have: \u0000 is eigenvalue ) (A \u0000 \u0000I) is singular therefore with \u0000 = ±1 at least one of them has to be singular 3) Prove that for an orthogonal matrix Q it holds that kQxk2 = kxk2 kQxk2 1 = p hQx, Qxi 2 = q (Qx)T Qx S.2.6 = qxT QT Qx S.2.20 = p xT Ix 2 = p hx, xi 1 = kxk2 1= defofnorm, 2defof scalarproduct 4) Prove that, if \u0000 is an eigenvalue of orthogonal Q then \u0000 = ±1 Qv = \u0000v(1) ,kQvk = k\u0000vk 2 ,kvk = k\u0000vk N2 , kvk = |\u0000| kvk, |\u0000| =1 1= defeigenvalue,2 =asproven before, N2= norm is homogeneous 5) Prove that for a arbitrary matrix A with its eigenvalue \u0000 it holds that (A \u0000 \u0000I)is singular Let v be the eigenvector to the coresponding \u0000 in (A \u0000 \u0000I) (A \u0000 \u0000I)v =(Av \u0000 \u0000Iv)= Av \u0000 \u0000v 1 = \u0000v \u0000 \u0000v =0v As v is eigenvector we have v 6=0(2) ) \u0000 =0 L.9.6 , (A \u0000 \u0000I) is singular 1= as v is eigenvalue, 2 = def eigenvalue 6) Let A 2 Rn⇥n be a real matrix and x 2 Rn be a vactor.Prove that: kAxk2 \u0000 \u0000min kxk2 .Where \u0000min is the smallest singular value of A Let A = U ⌃V T be the SVD of A kAxk2 = \u0000 \u0000 \u0000U ⌃V T x\u0000 \u0000 \u00002 1 = \u0000 \u0000 \u0000⌃V T x\u0000 \u0000 \u00002 2 \u0000 \u0000 \u0000 \u0000⌃min V T x\u0000 \u0000 \u00002 = \u0000 \u0000 \u0000\u0000min IV T x\u0000 \u0000 \u00002 N2 = |\u0000min | \u0000 \u0000 \u0000IV T x\u0000 \u0000 \u00002 = \u0000 \u0000 \u0000V T x\u0000 \u0000 \u00002 3 = |\u0000min | kxk2 1= U is orthogonal and proof 3,2 = ⌃min = diag(\u0000min ·· · ),3 = V T is orthogonal and proof 3 7) Prove that for a regular matrix A the two SLE Ax = b and AT Ax = AT b yield the same solution AT Ax = AT b 1 ) A\u0000T AT Ax = A\u0000T AT b ) Ax = b 1= Since A is regular det(A) 6=0 S.8.9 ) det(AT ) 6=0,hence AT is regular as well 8) For A 2 Rn⇥n holds that A = AT .Prove that all eigenvalues of A2k k 2 N are not negative. Lets deﬁne the SVD as follows: A = V ⌃V \u00001 Further we know A is orthogonal A = V ⌃V \u00001 S.2.20 ) A = V ⌃V T One has to prove inductively that An = V ⌃n V T B.C for n =1 it holds that: A = V ⌃1 V T I.H Assume it holds for any k 2 N I.S k 7! k +1: Ak+1 = AAk I.H = AV ⌃k V T n=1 = V ⌃V T V ⌃k V T S.2.20 = V ⌃⌃k V T = V ⌃k+1 V T The eigenvalue for An can be found in the diagonal of ⌃n .With \u0000q as the original eigenvalues of A, one can write: ⌃2 k = diag(\u00002k 1 ·· · \u00002k n ) Therefore no eigenvalue can be negative. 9) Prove that AH Ax = AH b has inﬁnitely many solutions. AH Ax = AH b ) AH (Ax \u0000 b)=0 ) AH (Ax \u0000 (b? + bk )) = AH ((Ax \u0000 b? ) \u0000 bk ) 1 = AH (\u0000b? ) )\u0000(AH b? ) 2 =0 1 as b? 2 R(A) it follows: Ax \u0000 b? =0 2 b 2 N (AH ) Therefore at least one solution exists. Since rank(A) <n , dim(N (A)) > 0 and as every solution of the system is in Sp + ↵Sh for any ↵.Where Sp is a arbitrary particular solution and Sh is the homogeneous solution. 9) Prove Satz 9.7. Namely, proof for any two similar matrices that the characteristict polynomial and det is equal Assume A and C are similar. Therefore we have that C = T \u00001 AT . Characterstic Polynomial: XC (\u0000) 1 =(C \u0000 \u0000I) 2 = det(T \u00001 (AT \u0000 \u0000T )) S.8.7 = det(T \u00001 )det(AT \u0000 \u0000T )= det(AT \u0000 \u0000T )det(T \u00001 ) S.8.7 = det((AT \u0000 \u0000T )T \u00001 )= det(AT T \u00001 \u0000 \u0000TT \u00001 )= det(A \u0000 \u0000I) 1 = XA (\u0000) 1 =def ofcharacteristic polynomial 2 =since C = T \u00001 AT Det: det(C) 1 = det(T \u00001 AT ) S.8.7 = det(T \u00001 ) · det(A) · det(T )= det(T \u00001 ) · det(T ) · det(A) C.8.8 =1 · det(A)= det(A) 1 =def of C = T \u00001 AT 10) let V and W be two vectors spaces. Let \u0000 : V 7! W be a linear mapping. Show, that Im(\u0000) is a subspace of W 1 im(\u0000) isn’t empty: 0= \u0000(0) since \u0000 is linear 2 For x, y 2 Im(\u0000) holds that x + y 2 im(\u0000): 9a, 9b such that \u0000(a)= x and \u0000(b)= y Therefore, x + y = \u0000(a)+ \u0000(b)= \u0000(a + b) 2 Im(\u0000) since \u0000 is linear 3 For x 2 Im() and ↵ 2 R holds that ↵x 2 Im() : 9a, such that \u0000(a)= x Therefore, ↵x = ↵\u0000(a)= \u0000(↵a) 2 Im(\u0000) since \u0000 is linear 11) Let B =(1,x, x2 ) and B0 =(x +1,x \u0000 1,x2 ).The columns (spalten ) of T are the elements of B0 in the basis B Then TB0 !B = 0 @1 \u000010 11 0 00 1 1 A by inverting TB!B0 we get TB0 !B = 0 @ 0.50.50 \u00000.50.50 00 1 1 A The mapping matrix D0 is then given by D0 = TB!B0 DT B0 !B = 0 @ 0.50.51 \u00000.5 \u00000.51 00 0 1 A","libVersion":"0.5.0","langs":""}