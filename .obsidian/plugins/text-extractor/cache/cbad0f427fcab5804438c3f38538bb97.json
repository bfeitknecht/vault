{"path":"sem4/FMFP/PV/summaries/FMFP-pvw-script.pdf","text":"Formal Methods and Functional Programming PVW Script Last Updated: June, 2022 Authors: Marc Himmelberger luk@vis.ethz.ch Disclaimer: This script only serves as additional material for practice purposes and should not serve as a substitute for the lecture material. We neither guarantee that this script covers all relevant topics for the exam, nor that it is correct. If an attentive reader finds any mistakes or has any suggestions on how to improve the script, they are encouraged to contact the authors under the indicated email address or, preferably, through a gitlab issue https://gitlab.ethz.ch/vis/luk/pvwscriptf mf p. Arrow associates to the right, function application associates to the left. a → b → c = a → (b → c) f g h = (f g) h – D. Basin, The FMFP Mantra Topic 09 12 13 14 15 16 17 18 19 20 21 HS21 avg Typing Haskell 13% 8% 8% 6% 7% 5% 5% 3% 4% 4% 4% 3% 6% Mini-Haskell 9% 6% 8% 9% 8% 9% 9% 6% 5% Evaluation 3% 4% 1% Nat. Deduction FOL 8% 9% 7% 10% 10% 4% Induction CYP-style 9% 10% 10% 9% 10% 11% 10% 9% 9% 10% 11% 8% 10% Programming Lists 9% 10% 13% 8% 6% 10% 13% 13% 13% 9% 8% ADTs 13% 14% 10% 22% 21% 16% 14% 16% 16% 19% 19% 25% 17% Nat. Semantics Application 3% 3% 1% Proof 9% 10% 14% 23% 19% 15% 18% 23% 11% Extension 3% 4% 1% SO Semantics Application 4% 6% 3% 6% 8% 3% 6% 3% Proof 12% 5% 13% 17% 4% Extension 9% 3% 6% 8% 2% Ax. Semantics Proof Outlines 17% 11% 14% 12% 9% 10% 13% 11% 13% 15% 13% 12% 13% Proof w.r.t Ax 10% 11% 13% 3% Extension 4% 5% 1% Modelling LTL 13% 6% 10% 7% 7% 10% 13% 13% 9% 7% Promela 11% 2% 12% 18% 10% 9% 5% An overview of the points (in percent) assigned to each common type of question during the exams from FS09 to HS21 available via https://exams.vis.ethz.ch on 14th June, 2022. Where not otherwise noted, the exams were spring semester exams (i.e. 09 means FS09). The average in the rightmost column is calculated directly over the percentages from each year. Values over 10% are marked in bold, values equal to 0 are left out. 1 Contents 1 Functional Programming 4 1.1 Haskell fundamentals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 1.1.1 Functional programming in general . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 1.1.2 I/O . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 1.1.3 Lists . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 1.1.4 List Comprehension . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7 1.1.5 Evaluation Order in Haskell . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7 1.1.6 Higher-Order Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9 1.1.7 Lambda Expressions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9 1.1.8 Prelude Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9 1.1.9 Folding with Lists . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10 1.2 Typing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12 1.2.1 Typeclasses . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12 1.2.2 Typing in Haskell . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13 1.2.3 Typing in Mini-Haskell . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13 1.3 Algebraic Data Types . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16 1.3.1 Canonical Fold Type . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16 1.3.2 Canonical Fold Definition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17 1.3.3 Instantiating typeclasses . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 1.4 Evaluation in Mini-Haskell . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20 1.5 Solutions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 1.5.1 Solutions for Hands-On 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 1.5.2 Solutions for Hands-On 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22 1.5.3 Solutions for Hands-On 3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22 1.5.4 Solutions for Hands-On 4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23 1.5.5 Solutions for Hands-On 5 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23 1.5.6 Solutions for Hands-On 6 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 1.5.7 Solutions for Hands-On 6 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26 2 Formal Proofs 27 2.1 Logic fundamentals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27 2.1.1 Free variables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27 2.1.2 Structures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27 2.1.3 Substitution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27 2.1.4 Alpha Conversion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27 2.2 Natural Deduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29 2.2.1 FOL proof strategies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29 2.3 Induction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31 2.3.1 Weak and Strong Induction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31 2.3.2 Induction as a Proof Rule . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34 2.3.3 Structural Induction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34 2.3.4 Induction over any ADT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35 2.3.5 CYP-style proofs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37 2.4 Solutions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39 2.4.1 Solutions for Hands-On 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39 2.4.2 Solutions for Hands-On 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39 2.4.3 Solutions for Hands-On 3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40 2.4.4 Solutions for Hands-On 4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42 2.4.5 Solutions for Hands-On 5 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42 3 Language Semantics 45 3.1 Background . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45 3.2 Natural Semantics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48 3.2.1 Executions with Natural Semantics . . . . . . . . . . . . . . . . . . . . . . . . . . . 48 2 3.2.2 Proofs with respect to Natural Semantics . . . . . . . . . . . . . . . . . . . . . . . 49 3.2.3 Extensions of Natural Semantics . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50 3.3 Structural Operational Semantics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53 3.3.1 Executions with Structural Operational Semantics . . . . . . . . . . . . . . . . . . 53 3.3.2 Proofs with respect to Structural Operational Semantics . . . . . . . . . . . . . . . 54 3.3.3 Extensions of Structural Operational Semantics . . . . . . . . . . . . . . . . . . . . 54 3.4 Axiomatic Semantics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56 3.4.1 Proof Outlines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56 3.4.2 Partial and Total Correctness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57 3.4.3 Invariants and Variants . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57 3.5 Solutions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61 3.5.1 Solutions for Hands-On 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61 3.5.2 Solutions for Hands-On 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63 3.5.3 Solutions for Hands-On 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66 4 Model Checking 69 4.1 Promela . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69 4.2 Linear Temporal Logic . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71 4.2.1 Transition Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71 4.2.2 Linear Time Properties . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71 4.2.3 Linear Temporal Logic . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71 4.2.4 Safety and Liveness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 74 4.3 Model Checking . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 76 4.4 Solutions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80 4.4.1 Solutions for Hands-On 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80 4.4.2 Solutions for Hands-On 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 81 4.4.3 Solutions for Hands-On 3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 81 Appendices 84 A Mini-Haskell Typing Rules . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 84 B First Order Logic Rules . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 84 C Hierarchy of FOL Rules . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 85 D IMP Syntax . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 86 E Big-Step Semantics Rules . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 86 F Small-Step Semantics Rules . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 86 G Axiomatic Semantics Rules . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 87 3 1 Functional Programming This course extensively used the popular functional programming language Haskell. In the following, we’ll cover the basics of functional programming and we will also take a look at a couple of concepts in more detail. 1.1 Haskell fundamentals I feel like teaching you Haskell syntax all over again would be a waste of space and time, so let’s instead go over the most important concepts and if you feel unsure with the syntax at any point, you can recap that on your own. Your go-to website for that should be ”Learn You a Haskell for Great Good!” 1 and you can also use the exercises on CodeExpert to practice writing Haskell. If you need to look up the documentation for a specific function or you want to know if there is a function that has some given type or if you want to look up the definition of some built-in typeclass, your go-to website should be ”Hoogle” 2. 1.1.1 Functional programming in general The whole point of functional programs is to reduce the number of things that could affect the execution of a program. To give an example, in Java you could easily write a method that returns 0 the first time it is called, 1 the next time, then 2 and so on. We would say that this method is impure because it does not work like a mathematical function - i.e. you cannot determine the output uniquely given only the method parameters. And we would also say that the method has side-effects because it changes variables outside the scope of the method (e.g. a global counter). As a programming language, having impure methods means that you cannot cache the result of a com- putation: Say you computed foo(5) before and you later need the value of foo(x) where x = 5. In Java, you could not reuse the result because the output might be different now. Having methods with side-effects is a slightly different problem (imagine a function that only reads a global counter but does not change it - this method would be impure but still side-effect free). If you have methods with side-effects, the evaluation strategy matters a lot: If you need the value of foo(3)+bar(2), do you evaluate foo or bar first? If you get to a statement like if(x > 0 && foo()), do you still execute foo even if x ≤ 0? As you can see, all this can cause quite a bit of a headache. This is where functional programming comes in: Instead of worrying about how the computation works in detail, programmers should only have to worry about what it is that they want to compute. Haskell is (almost) a pure functional language, meaning that any function (notice I’m not using the word method anymore) you write is pure and without side-effects. This allows us to do all kinds of fancy tricks such as caching function outputs, not computing function outputs until we actually need them (lazy evaluation), only computing outputs as far as we need them (e.g. only the first 10 elements of an infinite list) and so on. What you need to take away from this is: • What ”pure” and ”side-effects” means • Haskell functions (by default) are pure and don’t have side-effects - that also means that functions in Haskell might need more arguments than Java methods because there is more state that needs to be passed explicitly • Haskell can handle infinite data structures • Haskell can employ other tricks (like caching) to achieve high performance 1http://learnyouahaskell.com/ 2https://hoogle.haskell.org/ 4 1.1.2 I/O I’m sure you’ve noticed that I didn’t say ”Haskell is a pure language” but I added an annoying ”almost”. The truth is, you can write function in Haskell that are not pure. This can happen in two ways: Firstly, you could use functions like unsafePerformIO which already tells you that it might mess things up - and since we don’t use functions like that in this course, let’s forget about them again. Secondly, you could use IO. Any useful program needs to have some connection to user input - either reading input or printing strings to the output. So if Haskell only allowed for pure functions, it could not adapt to user input at all. The solution Haskell went for is something they call an IO context. Think of it like an encapsulation of an impure function: Instead of writing impure functions, you write a single block of impure code (an IO action) and you make the function output not the result of running the code but just ”the code” in some sense. Another IO action can then execute that code for you. And finally, because you have to start that impure execution somewhere, there’s one predefined IO action called main that is run each time you start the program. If you have an impure function that returns a value of type ’a’, the type of its IO action is ’IO a’. If your impure function would take parameters, say your function would be impure and have type ’a -> b’ then the type of your new function would be ’a -> IO b’ (given the parameter, the function computes an IO action that would return something of type b). The type of main is ’IO ()’ - meaning that the execution of the IO action would return nothing. So, to write IO actions you have to pick a different type but the code also changes. Namely, everything gets enclosed in a do block. Here, calls work like normal if you don’t need the result. If you need the result of an IO action execution you write res <- action. And if you want to assign a value to a variable without using IO actions, you write let var = 5 + 8. Finally, you can output a value from an IO action using return val, or if your IO action does not have a return type you can omit val. This code is then evaluated in sequence, so it’s kind of like Java again. This is a full working example: main :: IO () main = do putStr \" What ␣ is ␣ your ␣ first ␣ name ? ␣ \" first <- getLine putStr \" And ␣ your ␣ last ␣ name ? ␣ \" last <- getLine let full = first ++ \" ␣ \" ++ last putStrLn ( \" Pleased ␣ to ␣ meet ␣ you , ␣ \" ++ full ++ \" ! \" ) You should be able to deduce the types of putStr, putStrLn and getLine. See the solution below3. Takeaways: • Having I/O requires impure functions in some way • Haskell has pure functions that can output IO actions which stand for impure functions • IO actions can be executed from inside other IO actions or by being defined as main • IO actions can be constructed using parameters, they can output values and they are written using do notation. You should also know the syntax for each of these (best to try an example). 3putStr, putStrLn :: String -> IO () and getLine :: IO String 5 Hands-On 1 1.1. Write a program in Haskell that asks for the user’s name and their age. The program should then output ”[name] is [age] years old” 1.2. Modify the above program to output foo name age where the function foo should compute the string ”[name] is [age] years old” 1.3. Modify the above program again to output foo name age where the function foo should compute the string ”[name] is the square root of [age*age] years old”. Hint: Use the functions show and read. 1.4. Modify the above program a last time to output ”[name] is old” if the user’s age is at least 50 and ”[name] is young” if the user’s age is below 50. Hint: If you don’t know what ”guards” are, look at a. ahttp://learnyouahaskell.com/syntax-in-functions#guards-guards 1.1.3 Lists Haskell can get by with very little in terms of different types of data structures. Linked lists, maps, queues, stacks are all just specialized lists. Haskell lists are written like for example [1,2,3,5], [1..5], [1,3..15] or even [’a’..’z’]. As mentioned before, Haskell also supports infinite lists like [1..] (all positive natural numbers). Lists in Haskell are homogeneous meaning all elements have the same type. We write ”list with elements of type a” as [a]. All lists are based on the empty list that has a generic type [] :: [a] - so the empty list is an [Int] list as well as a [Bool] list and any other type you can think of. Every other list is constructed as ”take a list and add an element to the front” which we write using the cons operator: [1,2,3] = 1:2:3:[]. That means that lists in Haskell are a simple Algebraic Data Type (ADT - we’ll talk about those more in detail later). As with any ADT, you can use pattern matching on the different constructors and give the arguments names, so for lists we can make a distinction between empty and non-empty lists and in the second case, we name the first element and the smaller list. In practice, this looks like sumOfSquares [] = 0 sumOfSquares ( x : xs ) = x * x + sumOfSquares xs There are also a couple of built-in functions to facilitate working with lists: • xs !! i is the i-th element of the list xs • head xs is the first element of the list xs • tail xs is everything but the first element of the list xs • last xs is the last element of the list xs • init xs is everything but the last element of the list xs If this confuses you, here’s a List Monster to help out ;) 6 Source: http://learnyouahaskell.com/starting-out 1.1.4 List Comprehension Maybe you know that in Python you can write [x*x for x in range(1,20)] and that can be really useful. Haskell has exactly the same feature, called List Comprehension. In Haskell you would write [x*x | x <- [1..19]]. You can add conditions like [x | x <- [1..], prime x] to get e.g. a list of all prime numbers - which you can then use to take the first 100 of them or something. Note that the condition must come after the ”generation” of x. You can also generate multiple variables. Say you want all combinations of two natural numbers that sum to 69: [(a,b) | a <- [0..69], b <- [0..69], a + b == 69] - easy (note that we cannot use infinite lists as the computation would not terminate). Takeaways: • Using pattern matching you can only access the first element of a list, but library functions can help • Infinite lists can be super useful, but watch that your program terminates • Use ranges with adjustable step size for creating simple lists • List comprehension is extremely powerful, so try to use it when possible Hands-On 2 2.1. Implement the five functions head, tail, init, last using only pattern matching. Use error with a string argument if the function parameters make no sense. For more exercises, refer to CodeExpert. Look for exercises that practice recursion and general problem solving skills in Haskell, probably from the first few weeks. The testing framework on CodeExpert and the master solutions there should give you a good indication of the quality of your solution. 1.1.5 Evaluation Order in Haskell Okay, by now you should be able to design simple recursive programs without much of an issue. Let’s dive deeper into the details that make Haskell special. Starting with: How can you possibly have infinite lists? Easy: Don’t compute the whole list. To be precise, Haskell uses something called Lazy Evaluation to make sure that nothing is computed needlessly. This is in contrast to Eager Evaluation where every expression is evaluated as soon as possible. The latter has a less complicated and more compact memory layout (no need to store how a number is calculated, we can just use 4 bytes for any 32-bit integer). In 7 contrast, Haskell’s lazy evaluation strategy leads to more complicated data values in memory and can consume more memory as a result. In pure languages, the order of evaluation does not matter. If you can compute anything as often as you want and at whatever time you want, then the result will be the same under Eager or Lazy Evaluation. But Eager Evaluation would not allow for infinite values. It’s possible that you will need to construct step-by-step evaluations using these two strategies. In the first week we saw how that works for Lazy Evaluation in Haskell. Later on, both strategies were defined in detail for Mini-Haskell. Lazy Evaluation in Haskell works outside-in and left-to-right. Some features (like arithmetic operators, logical operators but also pattern matching or guards) force early evaluation of some parts of the expres- sion. Let’s look at an example: Example. Consider the definition fibLouis :: Int -> Int fibLouis 0 = 0 fibLouis 1 = 1 fibLouis n = fibLouis ( n - 1) + fibLouis ( n - 2) The task would be: ”Find all the evaluation steps in Haskell for fibLouis 3”. Let’s walk through this. First, there’s really no choice but to substitute 3 into the definition of fibLouis. We get = fibLouis (3-1) + fibLouis (3-2) Note that we did nothing other than checking which case of the definition applies and substituting the variable with the value. Next, we evaluate the ’+’ sign (it is the outermost function). But to evaluate it, we need its arguments as fully evaluated integers. So instead, we should evaluate fibLouis (3-1) (left before right). To do this, we need to know which case of the definition applies, so really, we want to evaluate 3-1 next. = fibLouis 2 + fibLouis (3-2) The same thinking as before (’+’ forces evaluation, left before right) leads us to now substitute the definition again and the next step is similar to what we’ve seen as well. = ( fibLouis (2-1) + fibLouis (2-2) ) + fibLouis (3-2) = ( fibLouis 1 + fibLouis (2-2) ) + fibLouis (3-2) Now comes the first time we substitute a different case of the definition and since the 1 is then fully evaluated, the evaluation of the second ’+’ sign moves to the right operand where we proceed as before. = ( 1 + fibLouis (2-2) ) + fibLouis (3-2) = ( 1 + fibLouis 0 ) + fibLouis (3-2) = ( 1 + 0 ) + fibLouis (3-2) Again because the ’+’ (the first one) forces its arguments to be fully evaluated, we also need to evaluate the 1 + 0 term. We repeat the process for the right side and get the rest of the sequence: = 1 + fibLouis (3-2) = 1 + fibLouis 1 = 1 + 1 = 2 As long as you remember to only evaluate arguments when their value is needed and how to break ties, this should be easy. Takeaways: • Haskell evaluates lazily 8 • Evaluation is from the outermost (last) function to the innermost (most nested) function • Break ties by going from left to right • Arithmetic and pattern matching might force the evaluation of arguments if they are relevant for determining the next step Hands-On 3 3.1. Given the definition fibEva :: Int -> Int fibEva n = fst ( aux n ) where aux 0 = (0 , 1) aux n = next ( aux ( n - 1) ) next (a , b ) = (b , a + b ) Find all the evaluation steps in Haskell for fibEva 4. 1.1.6 Higher-Order Functions Haskell is also special because you can manipulate functions. Say you want to define a function to increment a value. That’s like ”adding one”, so in Haskell we just write (+1) for that function - note the parentheses, it’s best to always keep them to avoid weird errors. We can also choose which argument we fix: If we want ”2 to the power of”, we write (2**) whereas for ”squaring” we write (**2). Sadly, this only works for binary functions and you need to make sure that the function is infix. Infix means that the function name is written between the arguments - this is the case by default for all function names that only contain special characters. But you can also force this by enclosing the function name back ticks like ‘mod‘. What I’ve just described is called Partial Application. In a sense, we’ve produced a new function that is the ’+’ function but with parts of it already evaluated - like the second argument here. Haskell always does this and what we’ve come to expect as ”normal” - i.e. supplying all arguments and getting a value back - is just an extreme case of partial application. In particular, whenever you call a function, say foo 1 2 3, what you’re really saying is ((foo 1) 2) 3. In other words, Haskell evaluates this as three separate partial applications. But functions can also do more than that in Haskell. They can be passed as arguments. So you can have a function that applies another function twice: applyTwice f x = f (f x). And with these Higher-Order Functions, we can do some very nice things as we will see soon. 1.1.7 Lambda Expressions Sometimes you need a very simple function in Haskell. Something that would cost you one line to define. You already saw that partial application can give us simple functions by fixing some arguments. Sometimes that’s not enough though: What if you want ”second argument divided by the first argument”? There’s a lot of built-in functions that you could use for these applications (look up curry, uncurry, flip and so on) - but often it’s easy enough to get the same behavior without them. We use Lambda Expressions. A Lambda Expression is written in parentheses, has arguments and a return value: (\\acc b -> acc + b) takes two arguments acc and b and returns their sum. It’s safe so say that both are integers because that’s what ’+’ needs, so this expression has the type Int -> Int -> Int. So to get back to ”second argument divided by the first argument”, that would be (\\a b -> b ‘div‘ a) 1.1.8 Prelude Functions Haskell gives us a whole bunch of really useful functions for free. These ”built-in” functions are part of the ”Prelude”, a module that is always loaded when Haskell runs any program. You’ve already seen some 9 of them, like head, tail, init, last, show and read. But there are many more. Take this as a quick overview of the most useful functions. Yes, you should learn these by heart. Yes, you can use them at the exam. No, it’s not worth it building each of them from scratch as some ”aux” function. Remember what they do and remember the order of the arguments and it will save you tons of time at the exam. You should be aware of: • mod, div for integer arithmetic, most natural as infix function (‘mod‘, ‘div‘) • foldr, foldl will both be covered in the next section down • fst, snd provide easy access to tuple elements • map f xs applies the function f to every element of the list xs, returing a list • filter f xs returns a list of only those elements x in xs where f x == True • length, reverse are self-explanatory • and, or combine lists of Bools into one Bool • concat combines a list of lists into a single list • Optional: iterate, repeat, replicate, cycle produce infinite lists in different ways • take n xs returns the first n elements of a list xs • drop n xs returns all but the first n elements of a list xs • takeWhile f xs returns elements from the start of the list xs so long as f (head xs) == True • dropWhile f xs is analogous • elem x xs returns True iff x is an element of the list xs • zip as bs combines two lists into a list of tuples (length equal to the shorter list’s length) Many of these can be implemented using things you know already (e.g. list comprehension) or also with different Prelude functions, but the point here is that your code will be easier to read, quicker to write and less error-prone if you learn to use these. Sadly, there’s no way around writing lots of Haskell programs to try and get to know them. Takeaways: • Functions can also be passed like values • Essentially, all functions only take a single argument (possibly yielding another function) • Lambda Expressions can be super helpful to define simple functions inline • Learn and use Prelude functions when possible - don’t reinvent the wheel 1.1.9 Folding with Lists Two special Prelude functions whose explanation I left for now are foldr and foldl. You see, often we traverse a list once and compute some final value. This could be adding up all elements, taking the logical AND of them, computing the reversed list, filtering elements, applying a function to each element, finding an element, sorting a list, and so on. All that changes for these different scenarios is: What do we do with each value? What is the output? In which order do we traverse the list? These are things perfectly suited for Folding. The concept is simple: Start with some base element (the output you would want for an empty list). Then consume the list one element at a time (either from the left or the right) and apply some function to the intermediate result and the list element to compute a new intermediate result. Finally, output the last result. 10 Source: https://wiki.haskell.org/Fold You should not that all intermediate values have to have the same type b. So to compute a Fold over a list xs :: [a], we need a function f :: a -> b -> b (taking one list element and one intermediate result to compute a new intermediate result) and a starting element e :: b. The result of the Right Fold is then foldr f e xs. It’s almost the same for the Left Fold, but here we need a function f’ :: b -> a -> b (order of the arguments flipped) and the result is foldl f’ e xs. When computing lists as results, foldr is often more natural because you can use cons (’:’) to keep a resulting list in the same order instead of using the more expensive append (’++’). Vice-versa if you want to reverse the list. I know it doesn’t sound that complicated but really understanding how to use folds is a challenge. It pays to practice this thoroughly. Takeaways: • If it’s something you compute by iterating through the list once, you can do it with folds • Folds are part of the exam, you will lose points if they confuse you, so practice Hands-On 4 4.1. Implement the following functions as one-line folds. • and, or • sum, product • concat • length • reverse • map • filter (you can use the function check below) check f x | f x = [ x ] | otherwise = [] 11 1.2 Typing Very often, one part of the exam is about types. The basic goal here is just to go from an expression like foldr (.) to a type like (a -> b) -> [b -> b] -> (a -> b). Of course the expression would also satisfy a more specific type in the same way that the empty list [] :: [a] is also an integer list. But what we really care about is the most general type. This usually involves some amount of type parameters (like the a, b above). The typing exercises show up in two forms: There are exercises about Haskell where it suffices to have an intuitive understanding of what’s happening to find the most general type. And then there are exercises about Mini-Haskell where you have to come up with a derivation tree to justify the most general type. In both cases we also have to check that the expression that we’re typing is well-typed, i.e. if we entered it into GHCi we would not get a type error. Expressions can violate this in two ways: Types can mismatch within the expression (True + 2 would be one example) or types can be infinite. Infinite types are weird and Haskell doesn’t allow them. Infinite types are such that you cannot write them down. Let’s consider an incorrect definition applyTwice f x = f f x - we left out the parentheses by mistake and Haskell interprets this as applyTwice f x = (f f) x. Why is this an issue? Well, you can tell that the first argument is a function because it is applied to itself. So without loss of generality, let’s call its type f :: a -> b. Now what are those new type variables? We see an application so clearly the first argument to the function must be of the same type as the function: a = a -> b. And now we’ve lost because no finite type can satisfy this constraint. No matter how you define a, there’s always one -> b missing. Intuitively: it’s like writing x = x + 1. We can recognize these infinite types as soon as we have a non-trivial recursive constraint like the one above: It constraints a type using the same type and the only time that were be fine would be a constraint like a = a (the trivial constraint). 1.2.1 Typeclasses You might already be familiar with the concept of typeclasses but it’s worth recapping quickly what they are. A typeclasses is the Haskell equivalent of Interfaces in Java: they specify some functionality that must be available for a type. Haskell has a couple typeclasses built in, most notably: • Eq a means that there is a function (==) :: a -> a -> Bool available • Ord a mandates that (<=) :: a -> a -> Bool be available and that a is also a member of the Eq typeclass. The functions (<), (>), (>=) :: a -> a -> Bool are derived automatically. • Show a requires show :: a -> String • Read a requires read :: String -> a • Num a requires membership in both the Show and Eq classes and additionally the functions (+), (-), (*) and a couple more. We’ll talk about how to define these functionalities in Section 1.3.3. For now you should just know that any type like (Num a, Eq b) => a -> b -> c means that the expression has the type a -> b -> c but additionally requires that a be a member of the Num typeclass and b be a member of the Eq class. We call this the class constraints. Takeaways: • Every expression is either well-typed or ill-typed • Ill-typed expressions either force some term to have multiple incompatible types at the same time or they force some term to have an infinite type • Class constraints belong with the type, don’t just forget about them 12 1.2.2 Typing in Haskell Now that we’ve already seen some typing concepts, let’s dive into how you find the most general type. This section will be kind of hand-wavy as it’s best to explain on concrete examples but a good general strategy is the following (I’ll illustrate on the example foldr (.) from earlier): 1. Identify all basic functions (like map, id, fst and so on). Write down the types for each one (don’t worry about naming conflicts yet). Usually this is already done for you in the question. Also add as many parentheses as possible so every arrow has a clear left and right side. foldr :: (a -> b -> b) -> (b -> ([a] -> b)) (.) :: (b -> c) -> ((a -> b) -> (a -> c)) 2. Change any infix function to a prefix function by putting it into parentheses. Also add parentheses until every function application only involves one argument. Here that is not necessary, but (+) . (-) would become ((.) (+)) (-) 3. Starting on the inside, find a basic expression that you can compute the type of, annotate it. Avoid naming conflicts with the definitions. Here, we can fix (.) :: (y -> z) -> (x -> y) -> (x -> z) as there’s no reason to make these more specific (yet). 4. When you get to a complicated term, take the definition of the function in question and explicitly write down the types of the arguments as far as possible, matching them up with the types in the definition. This usually gets a bit messy, try to keep the overview. If there are any conflicts or overlaps, try to resolve them or check if there’s an infinite type involved. In the example, we know the type of foldr’s first argument so we can infer what the a, b in foldr’s type should be: foldr type parameter a b our fixed x,y,z y -> z x -> y x -> z Notice that the b applies to two (possibly different) terms. We can resolve this by adding the constraint y = z, this could lead to a conflict later on but we really have no choice. 5. Now write down the return type of the function application. If there are any types that don’t yet have a definition, pick a new type variable. In the example, this means that foldr (.) :: b -> [a] -> b which we can now instantiate to foldr (.) :: (x -> y) -> [y -> y] -> (x -> y) We used the constraint to eliminate z entirely and if we wanted, we could now rename the variables to get the final type foldr (.) :: (a -> b) -> [b -> b] -> (a -> b) 6. Repeat the steps above until you encounter a type mismatch, an infinite type or until you are done. In this example, we did not have any typeclasses. But as soon as your expression involves ==, comparison operators (<, >) or numbers (0, 1), you will have to carry around the class constraints. Remember, we always gather all class constraints at the beginning of a type in parentheses and we use one => to keep them separate from the ”actual” type. In case you’re unsure, walk through the example \\x f -> filter (0 ==) (f x). Solution below 4. The strategy above works well for me and the renaming helps to combat the confusion that could otherwise arise. These exercises can still be a headache, which is why it’s worthwhile to practice this a couple of times. Just give GHCi some weird random expression and see if you get to the same type by hand. 1.2.3 Typing in Mini-Haskell There’s usually also an exercise about Type Inference in Mini-Haskell. The goal is the same as before - going from expression to most general type - but now we want to justify the type formally. 4\\x f -> filter (0 ==) (f x) :: (Eq a, Num a) => t -> (t -> [a]) -> [a] 13 Also, you won’t need any typeclasses for these exercises as all types you’ll encounter will either be Int, Bool, some tuple or function involving these types or completely arbitrary. For the formal type inference, we use a set of Natural Deduction rules (see Appendix A). You should be familiar with how these work, but we’ll do a full recap in Section 2.2. You don’t need to know them by heart, but you should practice them at least a bit. Any solution proceeds in these steps: 1. Hierarchy Make it clear to yourself what the hierarchy of operations looks like. What is the last/outermost function that happens? What are its arguments? If there are multiple identical operators on the same ”level”, are they right or left-associative? If there are different operators on the same ”level”, which binds stronger? 2. Build the tree Now build a tree using the rules. Start from the expression and decompose it according to the hierarchy you just figured out. If the last operation is a function application, the bottom-most rule in the tree will be the function application rule, etc. Don’t forget the names of the rules. Also leave space for the types for now and omit them. Finish the tree first. 3. Add type variables Now start at the bottom again and add type variables for every unknown type (τ0 for the target expression, then τ1, . . . ). Whenever a rule imposes something on a type variable, add a corresponding constraint (say, τ1 is the result of an abstraction rule then we must have τ1 = τ2 → τ3 for some τ2, τ3). Gather all these constraints in a list. If you’re absolutely sure that there’s only one possibility for a type, feel free to use that type already (e.g. iszero requires Int arguments). 4. Constraint solving At this point your tree should be finished but still with many unknown type variables. That is what we want because we try to solve the exercise in parts. Now we’ll solve the set of constraints. t will stand for any type, τ will stand for any single type variable. (a) If you ever see a constraint like t = t at any point, drop it. I’ll assume you never encounter one in the following. (b) Constraints about type constructs turn into constraints about the parts. e.g. (t0, t1) = (t3, t4) becomes t0 = t3 and t1 = t4. [t0] = [t1] becomes t0 = t1. (c) If constructs don’t match, the expression is not well-typed. Abort. e.g. (t0, t1) = [t3] - tuples cannot be arrays and vice-versa. (d) If you have a constraint with only one type variable on one side: i. That variable is used on the other side as well. The type would have to be infinite and the expression is not well-typed. Abort. e.g. τ0 = (τ0, τ1) leads to an infinite type. ii. Or that variable is not used to construct itself, then we can safely replace all uses of this variable with the other side of the constraint. e.g. τ0 = (τ1, τ2) lets is replace all occurrences of τ0 with (τ1, τ2). (e) Repeat from the beginning until all constraints are solved. 5. Wrapping up Finally gather all the definitions of the type variables next to the tree, make sure it’s clear which type variable is the final result and you’re done. Example. To illustrate here is an example using the expression λx . fst x: 1. Hierarchy: This expression is first and foremost a lambda abstraction. The smaller expression is the application of fst to the variable x. 2. Build the tree: Notice that there are no type variables yet, but the shape of the tree is fixed anyway. Var x : ⊢x :: fst x : ⊢fst x :: Abs ⊢ λx. fst x :: 14 3. Add type variables: We fill in the type variables like so Var x : τ1 ⊢x :: (τ2, τ3) fst x : τ1 ⊢fst x :: τ2 Abs ⊢ λx. fst x :: τ0 And we get the constraints: • τ0 = τ1 → τ2 from the Abs rule • τ1 = (τ2, τ3) from the Var rule 4. Constraint solving: In this example there’s not much to do, we have no constraints that help us narrow down the values for τ2, τ3, so we leave them arbitrary. We simplify τ0 and get: • τ0 = (τ2, τ3) -> τ2 • τ1 = (τ2, τ3) 5. Wrapping up: As τ0 was our original type, we proved that: λx . fst x :: (a, b) -> a We can double check that this matches our intuition and we’re done. That’s all you need to know about typing. It’s time to practice. Hands-On 5 Recall these types. Use them for the first three exercises. 0 :: Num a = > a True :: Bool head :: [ a ] -> a (==) :: Eq a = > a -> a -> Bool (.) :: ( b -> c ) -> ( a -> b ) -> a -> c takeWhile :: ( a -> Bool ) -> [ a ] -> [ a ] [] :: [ a ] (:) :: a -> [ a ] -> [ a ] 5.1. Find the most general type of the following expressions. No need to justify. (a) \\x y z -> x (y z) (b) \\x y z -> y . z . z (c) \\x -> \\y -> x . y . x (d) \\x -> \\y -> (x (y x), (x, y) ) 5.2. Are the following expressions well-typed? If yes, state the most general type. If not, justify why. (a) map map (b) \\x -> [x 0] (c) \\x y z -> ( (x 0, y True == False), x (y z) ) (d) \\x y -> takeWhile (x 0) y 5.3 (Assignment 1b from FS13 - 4 points). For each type here, find an expression with that most general type. (a) (a -> a -> b) -> a -> b (b) (a -> b) -> [a] -> b (c) (Num a, Eq b) => (b -> Bool) -> [b] -> [a] 5.4 (Assignment 1c from FS14 - 4 points). Recall the Mini-Haskell Type Inference rules from above. Use these to formally infer the type of the expression below. Write out the derivation tree and the generated constraint system. λx. x (snd (x 0)) 5.5 (Assignment 1b from FS16 - 6 points). Recall the Mini-Haskell Type Inference rules from above. Formally infer the most general type of the following expression using the Mini-Haskell typing rules. Label every proof step with the rule used. (λy. λx. iszero (y x)) (λx. snd x) 15 1.3 Algebraic Data Types We’ve already covered Lists in Haskell in the very first part of this chapter and I mentioned that they are an example of an Algebraic Data Type. Imagine ADTs 5 as a mix of C-structs and Java-Classes. Allow me to explain: In C, structs have several accessible fields that hold values. ADTs have that too. With structs, you can modify the content of those fields, with ADTs you can build new data structures with these new values. In Java, you can have Inheritance (Poodles, Labradors and Dachshunds are all Dogs and can be treated as a generic Dog). ADTs have that too (but unlike Java, ADTs don’t contain methods). To be precise: An ADT consists of one or more Constructors that take a fixed number of zero or more arguments of arbitrary type each. We define them using the keyword data. Examples: • A very boring ADT would be data Thing = Box. It can only have one value and no fields. • More practical are Enumeration types: data Thing = Box | Desk | Cat. By providing a single value of type Thing we can now distinguish three cases at once. This is what I mean with ”Java- like”. • ”Struct-like” would be a type like: data Thing = Box Int Int Int. While this type has only one constructor (Box) it also has three fields, all of type Int that can hold values. • Often you will see a combination of these features in types like data Expr = Lit Int | Add Expr Expr And there is one more feature ADTs have: they can be generalized. So instead of building a tree type for each type of data we might want to store in it (an Int tree, a Bool tree, etc), we build one generalized ”Tree” ADT that we can then instantiate with any type we like. We call this component type a Type Parameter and we write it after the name of the type and before the equality sign. Let’s see an example of that an let’s elaborate a bit on it: Consider data Tree a = Leaf a | Node (Tree a) (Tree a). A Tree value could either be a Leaf, in which case it contains some value of type a, or it could be a Node in which case it would contain two sub-trees. This is by far not the only way to define a tree (e.g. nodes don’t hold values in this tree, the branching factor is constant at 2, etc) and you’ll see different examples in the exercises. So what can you do with ADTs? First of all, you can pattern match them just like lists. Computing the sum in the integer version of the tree type above could look like: mySum :: Tree Int -> Int mySum ( Leaf x ) = x mySum ( Node l r ) = mySum l + mySum r Note that we gave the fields names and that we put parentheses around the constructors. Also: Construc- tors can not be partially applied. But you could always write a lambda expression like \\l r -> Node l r. 1.3.1 Canonical Fold Type More than that however, we can define Folding on any ADT! Let’s recall the example for Lists to see how it generalizes: Lists are empty or they are some element plus some other list. To define Folding, we defined what our result would be for the empty list (e :: b) and how we would handle the other case - using a function f :: a -> b -> b to produce the new value out of the element and an intermediate result! In general: • There will be one folding function for each constructor of the ADT (one or more). • That function will have the same number of arguments as the constructor (zero or more). 5This script will employ the abbreviation ADT because I’m lazy, but this does not mean Abstract Data Type - that’s something else and not relevant in this course 16 • When defining a Fold for type Thing, we replace all Thing-typed arguments with b, our intermediate result • All other arguments keep their type So, going back to the data Tree a = Leaf a | Node (Tree a) (Tree a) example, we will have two folding functions (just take the constructor type and replace all occurences of Tree a with b): foldLeaf :: a -> b foldNode :: b -> b -> b To get the type of the Canonical Fold (the analog of foldr), we just write a new function and: 1. Take these folding functions together as arguments (same order as in the ADT definition) 2. We double-check to parenthesize when necessary 3. We add an argument of the data type (the ADT value we want to fold, Tree a in the example) 4. We triple-check to parenthesize when necessary 5. And we add the output value (type b) data Tree = Leaf Int | Node ( Tree a ) ( Tree a ) foldTree :: ( a -> b ) -> ( b -> b -> b ) -> ( Tree a ) -> b And this cooking recipe is all you need to guarantee you get 2-3 easy points in the exam. You want more easy points? Let’s define the Fold: 1.3.2 Canonical Fold Definition The recipe is simple: 1. Define the fold function only partially: (a) Take only the folding functions as arguments (b) Output go, a function of type (Tree a) -> b that we have yet to define 2. Define go :: (Tree a) -> b by (a) Pattern matching the constructors (b) Recursively evaluate using go if necessary (c) Before combining all values using the corresponding folding function In our example this would be: data Tree a = Leaf a | Node ( Tree a ) ( Tree a ) foldTree :: ( a -> b ) -> ( b -> b -> b ) -> ( Tree a ) -> b foldTree foldLeaf foldNode = go where go ( Leaf x ) = foldLeaf x go ( Node l r ) = foldNode ( go l ) ( go r ) And that’s it. This usually constitutes one or two parts of an entire assignment and everything that’s needed for the rest is usually just knowing how to effectively program with folds, which you need to practice anyway for Lists. 17 1.3.3 Instantiating typeclasses Besides defining the canonical fold function for any ADT, some exam questions also ask to define an instance of a typeclass for some ADT. We’ve seen typeclasses before and you should remember that for some type to be a member of a typeclass, it really only needs to define a couple of special functions. Here, we quickly look at the syntax so you can define these functions according to the question. Often, you don’t even have to define all functions that typeclasses offer. It’s enough to define a set of functions called the minimal complete definition. For the typeclasses we looked at in Section 1.2.1, these are: • Eq a: (==) :: a -> a -> Bool • Ord a: (<=) :: a -> a -> Bool plus membership in Eq • Show a: show :: a -> String • Read a: read :: String -> a • Num a: is a bit more involved but you can look it up online if you ever need it Luckily, there’s no need to memorize all that because this will be given as part of the question. Now, to define an instance of a typeclass for an ADT, let’s consider an example where we try to define an instance of Show for an ADT defined as data Tree a = Leaf a | Node (Tree a) (Tree a): instance Show ( Tree a ) where show t = \" This ␣ is ␣ a ␣ tree \" Now obviously, that’s quite bad. But it shows the basic parts: saying that you ”instantiate” something, saying which typeclass is instantiated and then giving the definition of the necessary functions. We can do better by reflecting the structure of the tree in our output: instance Show ( Tree a ) where show ( Leaf x ) = \" A ␣ leaf \" show ( Node l r ) = \" [ \" ++ show l ++ \" ,\" ++ show r ++ \" ] \" This will work and it even gives us information about the layout of the tree! But you might be wondering why we’re not printing the values at the leaves. Well, how could we? Without additional information, we cannot know how to print an arbitrary type a - what if we store functions in the tree? So to fix this, we can add class constraints into the instantiation: We assume that a is already a member of the Show typeclass and describe how to ”show” a tree in that case. This also means that we cannot ”show” trees where this is not the case but that’s usually perfectly fine. We simply add the class constraint to the first line an then we can use show on values of type a: instance Show a = > Show ( Tree a ) where show ( Leaf x ) = show x show ( Node l r ) = \" [ \" ++ show l ++ \" ,\" ++ show r ++ \" ] \" And that’s how you can instantiate any typeclass for any ADT. It’s really not hard, but you should remember to use the class constraints when necessary. Takeaways: • Every ADT has a canonical fold function. • You can compute the type of the fold function with the recipe in Section 1.3.1. • You can compute the definition of the fold function with the recipe in Section 1.3.2. • ADTs can be members of a typeclass if the necessary functions for that class have been instantiated for that ADT • You can add class constraints on the type parameters of an ADT to the instantiations if necessary 18 Hands-On 6 6.1 (Assignment 5a from FS14). Consider the following algebraic data type representing a file system entry. data FSEntry = Folder String [ FSEntry ] | File String String A file system entry is either a folder consisting of a name and a (possibly empty) list of file system entries, or a single file consisting of a name and some text. For example, test = Folder \" Home \" [ Folder \" Work \" [ File \" students . txt \" \" Alice , ␣ Bob \" , File \" hint \" \" You ␣ can ␣ use ␣ fFSE ! \" ] , File \" Fun \" \" FMFP \" ] represents a folder Home that contains a subfolder and a file. Define the Haskell function fFSE :: (String -> [b] -> b) -> (String -> String -> b) -> FSEntry -> b or folding values of type FSEntry. 6.2 (Assignment 4b from FS16 - 3 points). Consider the data type data Json a = Val a | Obj [( String , Json a ) ] Implement the canonical fold function foldJson for Json a: (a) Give the function declaration (i.e. its type) (b) Give the function definition Sidenote: the original question only asked to find the definition and already gave the type. 19 1.4 Evaluation in Mini-Haskell We already discussed these two strategies in Section 1.1.5. So we won’t go over what they’re supposed to achieve. During the course you were presented with two sets of rules to evaluate Mini-Haskell expressions step-by-step. We’ll quickly recap them here and go over common pitfalls. There’s no difference in how atomic values, if statements, arithmetic, tuples or built-in functions like fst, snd, iszero work. There’s really only one order in which to evaluate these and if in doubt, we go left-to-right. However, Lambda abstractions and function applications are different. Lazy Evaluation works as follows: • If the expression to be evaluated is a lambda abstraction, don’t evaluate it. Leave it be. Don’t touch it. Functions can be left as their original definition until we actually apply them to something and then the expression would be an application instead. Speaking of: • If the expression is a function application, evaluate the function (remember if it’s a lambda ab- straction, leave it be - but it could also be another application). Now substitute the argument into the function. Don’t even look at the argument and certainly don’t evaluate it. Carry on with the evaluation of the substitution result. Eager Evaluation works as follows: • If the expression to be evaluated is a lambda abstraction, evaluate the result (the t in \\x -> t) as far as possible. • If the expression is a function application, evaluate the function then evaluate the argument. Then substitute the argument into the function and evaluate the result. Hands-On 7 7.1 (Exercise 2 in Sheet 6 in FS22). Practice both evaluation strategies by evaluating the following once using each strategy. Give all intermediate steps. (\\ x -> x (\\ y -> x y ) ) (\\ x -> (\\ y -> y ) x ) 20 1.5 Solutions 1.5.1 Solutions for Hands-On 1 Hands-On 1.1 This should not be hard; we simply combine parts we’ve seen before. main :: IO () main = do putStr \" What ␣ is ␣ your ␣ name ? ␣ \" name <- getLine putStr \" What ␣ is ␣ your ␣ age ? ␣ \" age <- getLine putStr ( name ++ \" ␣ is ␣ \" ++ age ++ \" ␣ years ␣ old \" ) Hands-On 1.2 Now we replace the parenthesized expression above by a new function. This could look like: foo :: String -> String -> String foo name age = name ++ \" ␣ is ␣ \" ++ age ++ \" ␣ years ␣ old \" main :: IO () main = do putStr \" What ␣ is ␣ your ␣ name ? ␣ \" name <- getLine putStr \" What ␣ is ␣ your ␣ age ? ␣ \" age <- getLine putStr ( foo name age ) Hands-On 1.3 This one is a bit more interesting. As you can see above, foo took two String arguments. So before we can do any arithmetic on age, we have to convert it to an Int. We can do this using the function read. Conversely, we later need to show the new integer to convert it back to a string. foo :: String -> Int -> String foo name age = name ++ \" ␣ is ␣ \" ++ show ( age * age ) ++ \" ␣ years ␣ old \" main :: IO () main = do putStr \" What ␣ is ␣ your ␣ name ? ␣ \" name <- getLine putStr \" What ␣ is ␣ your ␣ age ? ␣ \" age <- getLine putStr ( foo name ( read age ) ) This kind of ”Reading” works for many different data types. Sometimes Haskell might get confused, so it’s best to have a type annotation close-by. In this case, Haskell knows to produce an integer because we said so in the definition of foo. Hands-On 1.4 And finally we can use ”guards” to do the equivalent of an if statement. We could change foo to do this for us, or implement only the changed part as a separate function (see below). adjective :: Int -> String adjective age | age < 50 = \" young \" | age >= 50 = \" old \" foo :: String -> Int -> String foo name age = name ++ \" ␣ is ␣ \" ++ adjective age 21 main :: IO () main = do putStr \" What ␣ is ␣ your ␣ name ? ␣ \" name <- getLine putStr \" What ␣ is ␣ your ␣ age ? ␣ \" age <- getLine putStr ( foo name ( read age ) ) 1.5.2 Solutions for Hands-On 2 Hands-On 2.1 You can compare the code below with your solution. Note that we can pattern-match on integers as well and that we can also specify more complicated cases than just ”empty” and ”non-empty” lists. (!!!) :: [ a ] -> Int -> a (!!!) [] i = error \" Index ␣ Out ␣ of ␣ Bounds \" (!!!) ( x : xs ) 0 = x (!!!) ( x : xs ) i = xs !!! (i -1) myHead :: [ a ] -> a myHead [] = error \" Empty ␣ List \" myHead ( x : xs ) = x myTail :: [ a ] -> [ a ] myTail [] = error \" Empty ␣ List \" myTail ( x : xs ) = xs myLast :: [ a ] -> a myLast [] = error \" Empty ␣ List \" myLast [ x ] = x myLast ( x : xs ) = myLast xs myInit :: [ a ] -> [ a ] myInit [] = error \" Empty ␣ List \" myInit [ x ] = [] myInit ( x : xs ) = x : ( myInit xs ) 1.5.3 Solutions for Hands-On 3 Hands-On 3.1 The correct sequence is: fibEva 3 = fst ( aux 3) = fst ( next ( aux (3 -1) ) ) = fst ( next ( aux 2) ) = fst ( next ( next ( aux (2 -1) ) ) ) = fst ( next ( next ( aux 1) ) ) = fst ( next ( next ( next ( aux (1 -1) ) ) ) ) = fst ( next ( next ( next ( aux 0) ) ) ) = fst ( next ( next ( next (0 , 1) ) ) ) = fst ( next ( next (1 , 0+1) ) ) = fst ( next (0+1 , 1+(0+1) ) ) = fst (1+(0+1) , (0+1) +(1+(0+1) ) ) = 1+(0+1) = 1+1 = 2 22 Note that the arguments to aux are evaluated immediately because we need to know which definition to apply. In contrast, the arithmetic expressions in the tuples stay unevaluated because their values are not relevant for the definition of next or fst. Only at the end do we care what integer they evaluate to. 1.5.4 Solutions for Hands-On 4 Hands-On 4.1 This is one possible (very concise) solution. It’s absolutely fine if yours is more verbose as long as you feel like you understand Folding better now. and , or :: [ Bool ] -> Bool and = foldr (&&) True or = foldr (||) False sum , product :: Num a = > [ a ] -> a sum = foldr (+) 0 product = foldr (*) 1 concat :: [[ a ]] -> [ a ] concat = foldr (++) [] length :: [ a ] -> Int length = foldr (\\ x i -> i + 1) 0 reverse1 , reverse2 :: [ a ] -> [ a ] reverse1 = foldr (\\ x xs -> xs ++ [ x ]) [] reverse2 = foldl (\\ xs x -> x : xs ) [] map :: (a - > b ) -> [ a ] -> [ b ] map f = foldr (\\ x xs -> ( f x ) : xs ) [] filter :: (a - > Bool ) -> [ a ] -> [ a ] filter f = foldr (\\ x xs -> ( check f x ) ++ xs ) [] check f x | f x = [ x ] | otherwise = [] 1.5.5 Solutions for Hands-On 5 Hands-On 5.1 The most general types are (a) \\x y z -> x (y z) :: (a -> b) -> (c -> a) -> c -> b (b) \\x y z -> y . z . z :: a -> (b -> c) -> (b -> b) -> b -> c (c) \\x -> \\y -> x . y . x :: (a -> b) -> (b -> a) -> a -> b (d) \\x -> \\y -> (x (y x), (x, y) ) :: (b -> a) -> ((b -> a) -> b) -> (a, (b -> a, (b -> a) -> b)) Hands-On 5.2 (a) map map :: [a -> b] -> [[a] -> [b]] (b) \\x -> [x 0] :: Num b => (b -> a) -> [a] (c) \\x y z -> ( (x 0, y True == False), x (y z) ) is ill-typed because y True == False forces y :: Bool -> Bool and x (y z) then forces x :: Bool -> a for some type a. This conflicts with x 0 because this forces x :: Num b => b -> a but Bool is not part of the Num typeclass. (d) \\x y -> takeWhile (x 0) y :: Num b => (b -> a -> Bool) -> [a] -> [a] Hands-On 5.3 Here are the easiest solutions I could find: 23 (a) \\f x -> f x x (b) \\f l -> f (head l) (c) \\f l -> if f (head l) && l == l then [0] else [1] as soon as you have typeclasses involved it’s often easiest to do it in steps. Here you can first worry about getting (b -> Bool) -> [b] -> [a], e.g. applying the first argument to to the head of the second argument forces the second to be a list and the first to be a function taking the lists’s element type as argument. Then you can enforce that the function outputs a Bool by writing the if and as an added bonus you get to pick the final outputs as well. Then you might have \\f l -> if f (head l) then [0] else [1] and you only need to add the equality constraint. Hands-On 5.4 The proof tree is: Var x : τ1 ⊢ x :: τ3 → τ2 Var x : τ3 → τ2 ⊢ x :: Int → (τ4, τ3) Int x : τ3 → τ2 ⊢ 0 :: Int App x : τ3 → τ2 ⊢ x 0 :: (τ4, τ3) snd x : τ3 → τ2 ⊢ snd (x 0) :: τ3 App x : τ1 ⊢ x (snd (x 0)) :: τ2 Abs ⊢ λx. x (snd (x 0)) :: τ0 And the gathered constraints are: • τ0 = τ1 → τ2 from the rule Abs • τ1 = τ3 → τ2 from the lower Var rule • τ2 = (τ4, τ3) from the upper Var • τ3 = Int from the upper Var and the Int rule We could have also introduced a new type variable instead of directly writing ”Int” but some amount of anticipation helps keep the complexity down. If you wanted, you could have also anticipated further and fixed e.g. τ3 to ”Int” from the start. Just make sure you’re not overwhelmed while also avoiding mistakes. These constraints solve to: • τ0 = (Int -> (τ4, Int)) -> (τ4, Int) • τ1 = Int -> (τ4, Int) • τ2 = (τ4, Int) • τ3 = Int So finally: λx. x (snd (x 0)) :: (Int -> (a, Int)) -> (a, Int) Hands-On 5.5 Similar to the last exercise, we’ll first write down, then look at the gathered constraints and then solve them. The proof tree: Var y : τ1, x : τ4 ⊢ y :: τ4 → Int Var y : τ1, x : τ4 ⊢ x :: τ4 App y : τ1, x : τ4 ⊢ y x :: Int iszero y : τ1, x : τ4 ⊢ iszero (y x) :: Bool Abs y : τ1 ⊢ λx. iszero (y x) :: τ0 Abs ⊢ λy. λx. iszero (y x) :: τ1 → τ0 Var x : τ2 ⊢ x :: (τ5, τ3) snd x : τ2 ⊢ snd x :: τ3 Abs ⊢ λx. snd x :: τ1 App ⊢ (λy. λx. iszero (y x)) (λx. snd x) :: τ0 The constraints: • τ1 = τ2 → τ3 from the Abs rule in the right subtree 24 • τ0 = τ4 → Bool from the second Abs rule in the left subtree • τ1 = τ4 → Int from the upper left Var rule • τ2 = (τ5, τ3) from the Var rule in the right subtree We can use the two constraints on τ1 to get two new constraints τ2 = τ4 and τ3 = Int and all constraints solve to: • τ0 = (τ5, Int) -> Bool • τ1 = (τ5, Int) -> Int • τ2 = (τ5, Int) • τ3 = Int • τ4 = (τ5, Int) So finally: (λy. λx. iszero (y x)) (λx. snd x) :: (a, Int) -> Bool Again, with some anticipation it’s possible to recognize immediately that the y in the function will be instantiated with the function snd and that the x in the function will be the actual parameter to the expression as well as an argument to snd, so therefore a tuple. But you should figure out how much of that you can safely think through and how much you trust yourself to simply compute with the constraints. 1.5.6 Solutions for Hands-On 6 Hands-On 6.1 We use the recipe discussed in Section 1.3.2. The easy part is: fFSE :: ( String -> [ b ] -> b ) -> ( String -> String -> b ) -> FSEntry -> b fFSE fFolder fFile = go where go ( Folder name entries ) = ... go ( File name content ) = ... Now we need to define the folding sensibly, the File case is okay because there is no FSEntry that we would need to apply go recursively to, so we just apply fFile directly. When handling the Folder case however, we need to first call go on every component FSEntry. Since they are packed in a list, we simply use map (we could also use list comprehension) to get a list of results which is what fFolder expects. fFSE :: ( String -> [ b ] -> b ) -> ( String -> String -> b ) -> FSEntry -> b fFSE fFolder fFile = go where go ( Folder name entries ) = fFolder name ( map go entries ) go ( File name content ) = fFile name content Hands-On 6.2 (a) The type of the fold function follows from the definition of Json a. We simply write down the type of the constructors: Val :: a -> Json a Obj :: [( Strong , Json a ) ] -> Json a We replace all occurrences of Json a with b and join the types together with -> to get ( a -> b ) -> ([( Strong , b ) ] -> b ) 25 So the type of the fold function is (add the input and output): foldJson :: (a -> b) -> ([(String, b)] -> b) -> Json a -> b (b) For the definition we follow the same recipe as always. The interesting case here is Obj where you need to get results recursively for only the second element in each tuple and add the string back to the tuple. The final definition is this: foldJson :: ( a -> b ) -> ([( String , b ) ] -> b ) -> Json a -> b foldJson foldVal foldObj = go where go ( Val a ) = foldVal a go ( Obj vals ) = foldObj $ map (\\( s , js ) -> (s , go js ) ) vals 1.5.7 Solutions for Hands-On 6 Hands-On 7.1 Lazy Evaluation works as follows: (\\ x -> x (\\ y -> x y ) ) (\\ x -> (\\ y -> y ) x ) = (\\ x -> (\\ y -> y ) x ) (\\ y -> (\\ x -> (\\ y -> y ) x ) y ) = (\\ y -> y ) (\\ y -> (\\ x -> (\\ y -> y ) x ) y ) = (\\ y -> (\\ x -> (\\ y -> y ) x ) y ) These are 3 steps where the function argument gets substituted without evaluation. Evaluation then stops because all that’s left is one big lambda abstraction. Eager Evaluation gives: (\\ x -> x (\\ y -> x y ) ) (\\ x -> (\\ y -> y ) x ) = (\\ x -> x (\\ y -> x y ) ) (\\ x -> x ) = (\\ x -> x ) (\\ y -> (\\ x -> x ) y ) = (\\ x -> x ) (\\ y -> y ) = \\ y -> y These steps are in order: • argument evaluation (check that it makes sense that we could not evaluate the function any further) • substitution • argument evaluation (and in the argument another argument evaluation) • argument evaluation (and in there a substitution) Evaluation then stops because the expression is fully evaluated. 26 2 Formal Proofs Having covered all the main concepts relating to functional programming and Haskell, we’ll now recap the basics of First Order Logic, learn about Natural Deduction and we will look at how to do simple induction proof with Haskell programs. 2.1 Logic fundamentals Recall that First Order Logic (FOL) can be used to write formulas. Formulas may be the conjunction (and) or the disjunction (or) of other formulas, their negation or a predicate. In addition, we can quantify variables using ∀ (for all) and ∃ (there exists). Predicates take terms as arguments that are variables or functions of variables. 2.1.1 Free variables Variables can be free or bound. In a (sub)formula where some variable x is quantified, x does not appear freely (e.g. ∃x.F, ∀x. F ). All other variables appear freely. As an example, in the formula ∀x. x > 5∨y = 2, x is not a free variable while y is. 2.1.2 Structures Formulas are not true of false by themselves. Take x > 5, this formula could be true or it could be false but we’ll only know once we fix the value of x and the semantics of >. So in order to evaluate a formula, we need a structure consisting of: the universe the variables come from, the values of the variables and the definitions of the predicates and functions. If a formula is true under every possible structure (e.g. x > 5 ∨ ¬(x > 5)) we call the formula valid. If it is not true under any possible structure (e.g. x > 5 ∧ ¬(x > 5)) we call the formula unsatisfiable or a contradiction. Most formulas however (like x > 5) are neither. 2.1.3 Substitution Sometimes we have some formula F and we might want to substitute a term t into a variable x. We call the new formula F [x/t] and intuitively, we want the new formula to behave as if we simply changed the value the variable mapped to - i.e. whenever we would look at the variable definition in the structure we use our new term instead. Meanwhile, we want to change nothing else about the formula. Let’s look at a couple examples. Let’s say we have the formula F ≡ x > 5 and we want to substitute y + 1 into x, we get F [x/y + 1] = (y + 1) > 5. This example was easy because y did not appear freely in F . Say we have G ≡ ∀x. x > 5 and we want to compute G[x/y + 1]. Here, x does not even need to be defined in the structure because it is never used, formally we would say that x does not appear freely in G therefore, we have nothing to do (G[x/y + 1] = G). We might have a mixed case H ≡ x < 2 ∧ ∀x. x > 5 where we would only substitute the free occurrence of x, i.e. H[x/y + 1] = (y + 1) < 2 ∧ ∀x. x > 5. Finally, we might sometimes introduce conflicts between variables if we substitute naively. An example for this case would be computing F ′[x/y + 1] for F ′ ≡ ∀y. x > 5. We cannot simply replace the x by y + 1 because y + 1 would then be bound by the quantifier ∀y. This problem is called Variable Capture and we need to do Alpha Conversion to avoid it. 2.1.4 Alpha Conversion Notice first that the actual name we use for a quantified variable has no effect on the evaluation of the formula: ∀x. x > 5 and ∀y. y > 5 can be evaluated using the exact same structures (in both we don’t need to define x or y, only the universe and >) 27 And since variable capture can only occur if a quantified variable occurs in our substitution term, we can effectively avoid variable capture by renaming the quantified variable first. This is called Alpha Conversion. In the example above, we first alpha-convert F ′ ≡ ∀y. x > 5 to F ′′ ≡ ∀z. x > 5 and we can now safely compute F ′′[x/y + 1] ≡ ∀z. (y + 1) > 5 in the usual way. Note that after substitution, not exactly the same structures might be sufficient to evaluate a formula. In particular, the substituted variable does not need to be defined any longer while all variables and functions in the substitution term must now also be defined. Takeaways: • You need to know how FOL works and what structures and free variables are • When substituting, you produce a new formula (not equivalent to the original) • When substituting, you must always avoid variable capture by using alpha renaming (though you don’t really have to mention that you’re doing it because it’s so standard - but make sure to be careful and avoid mistakes) Hands-On 1 1.1. Determine which variables are free in the following formulas: (a) (p(x)) → ¬(∃y. ¬p(y)) (b) ∀z. r(x, z) ∧ r(y, z) → ∀x. ∀y. x = y (c) ∀x. p(x) ∨ ¬p(x) 1.2 (Assignment 2.A from FS21 - 4 points). For each formula below, determine whether it is valid (satisfied with respect to all structures), contradictory (does not admit any models), or satisfiable but not valid (satisfied with respect to some but not all structures). (a) (∀x. p(x)) → ¬(∃y. ¬p(y)) (b) ∀x. ∀y. ∀z. r(x, z) ∧ r(y, z) → x = y (c) (F → G) ∨ (G → F ) 1.3. Perform the following substitutions: (a) F [x/y] where F = ∀x.p(x) (b) F [x/y] where F = ∀y.p(x) (c) F [x/f (y, z)] where F = p(x) ∧ ∀y. (∀x. r(x, x) ∨ r(x, z)) ∧ r(y, x) 28 2.2 Natural Deduction Natural Deduction describes a way to prove statements in some logic using rules. What kind of logic is modelled depends on what rules are available: In Classical Logic it is valid to argue that ”if A is not true, then A must be false” while in Intuitionistic Logic that axiom (tertium non datur) is not an accepted rule. The course presented several applications of Natural Deduction. All the relevant rules can be found in the Appendices but you don’t need to know them by heart as they will always be provided as part of the question. Here, we will cover how Natural Deduction works and we will go over basic techniques to build proof trees. We will use the example of Natural Deduction for FOL throughout this part as we’ve seen the Mini-Haskell typing rules in action already and the semantic rules will be seen later as well. First we need to make it clear what kinds of statements we’re proving. In the typing case, we were proving statements of the form ”given that these variables have these types, this expression has this type”. Later, we’ll use the semantics rules to prove statements like ”starting in this state and with this program, we can execute it to this next configuration”. But for now, we’ll stick with FOL and we will prove statements like ”this formula is a logical consequence of this set of formulas”, in particular we will often not have a left hand side in the statement we’re interested in - i.e. often we will prove a statement of the form ”this formula is valid”. Note that this set of statements is a true subset of the set of statements I mentioned first. Natural Deduction always proceeds as follows: 1. Write down the statement you want to prove 2. Apply available rules to build a proof tree above the statement (name all the rules) 3. If a rule requires additional statements, these need to proved as well 4. If a rule has ”side conditions”, explicitly check them - it needs to be obvious that you saw the side conditions and that you checked them. Also make sure it’s clear which rule they belong to. 2.2.1 FOL proof strategies Take a quick look at the rules in Appendix B now. You should read ⊥ as ”the left side leads to a contradiction”. Formally, ⊥ stands for any unsatisfiable formula (they are all equivalent). In FOL, the only rule that doesn’t require a continuation of the proof tree is the Ax rule, but to use it you already need the formula A in the left hand side. The goal is therefore to: bring enough formulas to the left side of the ⊢ so you can use them while also applying rules in such a way that you can actually prove the right side of the ⊢. If you recall, the Mini-Haskell rules were much nicer in that regard. There was never a choice between multiple rules that you could use. The expression always dictated the exact shape of the tree (up to swapping rules around). Some of the FOL rules follow the same principle, namely all of the Introduction rules (”-I”). These are easy rules in the sense that they only decompose formulas in the straight-forward way. Other FOL rules correspond to having a ”good idea” for a proof. They don’t just decompose a formula but instead they prove a more complicated formula which can then be used to prove the original one. That is what all the Elimination rules do (”-E”). They’re harder to apply effectively because you need to think ahead and pull some new formula A, B, C, . . . out of thin air. For this reason, it’s important to first see how far you can get with the easy rules. And then thinking through informally how you would finish the proof in words. Get an understanding of why a certain statement is true. Only then, once you have a proof sketch in your head, work on translating it to a proof tree. It’s important to take this extra step as you’ll otherwise risk drowning in rule applications that, while valid, don’t lead anywhere because you’ve lost the overview. 29 For a somewhat less serious but very valid discussion of a sensible order to apply rules in, combinations that are destined to fail, etc. See a list Jonas Fiala graciously provided on the FS22 FMFP Moodle in Appendix C. Takeaways: • Sketch a proof in your head or on paper before you write a proof tree • Use easy decomposition rules before trying to introduce new formulas out of thin air • Avoid variable capture during substitution • Check the side conditions and name the rules Hands-On 2 2.1. For each of the following formulas, justify informally why they are tautologies in FOL then prove that statement using Natural Deduction. (a) (∀y. P (y)) → ∃x. P (x) (b) (∃x. ¬P (x)) → ¬∀y. P (y) 2.2 (Assignment 2 from FS12 - 8 points). Prove ⊢ (∀y. P (y) ∧ R(y)) → ((∃x. Q(x)) → ∃x. Q(x) ∧ R(x)) using natural deduction and label each inference step with the name of the corresponding rule. 2.3 (Assignment 2 from FS15 - 6 points). Prove using the natural deduction rules for FOL that (∃x. P (x)) → ¬(∀x. ¬P (x) ∧ Q) 30 2.3 Induction In this section, we will introduce induction as a proof strategy. The following section will go on to look at one particular use case relating to Haskell. When to use Induction Like any proof strategy, induction can be useful or not depending on the kind of statement you want to prove. In one sentence, induction is used for proving statements over recursively defined objects: Imagine you have a set of objects and you want to prove something for all of them. We would call these objects recursively defined if there are some ”base objects” and all other objects are built on top of those. Take dogs; different dogs don’t really have anything to do with each other and dogs are certainly not made of other dogs, so to prove something ”for all dogs” induction would be useless. In contrast, consider LEGO buildings: All LEGO buildings are made from a (comparatively) small set of building blocks. If you want to prove something for LEGO buildings, induction is your friend because instead of having to consider all the infinitely many buildings, you only have to consider all the finitely many building blocks plus the ways to put them together. To illustrate this a bit more, let’s move to more realistic examples: Natural numbers and Lists. We view natural numbers as being recursively defined in the sense that 0 is the base object and we can build new natural numbers by incrementing another natural number by 1. Lists are recursively defined because [] is the base object and all other lists can be built by appending an element from some universe (the type of list elements) to another list. 0 1 2 3 [] [3] [2, 3] [1, 2, 3] An illustration of how the natural number 3 and the list [1, 2, 3] are built recursively from other objects. Base objects are colored green. How to use Induction So, now that we understand when induction can be useful, let’s look at how induction works. The principle is easy: Take advantage of the recursive structure of the objects and: 1. Prove your statement for all the base objects 2. Then take all the ways new objects can be built and prove that: assuming the objects used in the construction satisfy the statement, the newly constructed object also satisfies the statement If this sounds vague to you, it’s because it is and we’ll get to the details in a bit. But first, notice that such an assumption is really powerful because you don’t need to prove much more than you can already assume but at the same time your proof is so general that it works for any object. This idea is valid because any object is built in a finite way from the base objects (i.e. a finite number of constructions lead to the object in question). You proved the statement for the base object and you proved that the statement will also hold after the first construction, also after the second, and so on until (after finitely many such arguments) the statement must also hold for the object in question. 2.3.1 Weak and Strong Induction Induction comes in two forms: Weak Induction and Strong Induction. I will try to keep the explanation general enough so you can apply it to other structures too but I will illustrate everything using the examples of natural numbers. Let P be the statement we’re trying to prove. Weak Induction is the kind of induction one you should all be familiar with already: 1. Base Case: Prove P for all the base objects (for natural numbers, that’s only 0) 2. Preamble: Let m be any non-base object (any number > 1) - use another variable name if m is already used avoid conflicts. State that your goal is to prove P (m). 31 3. Induction Hypothesis: Define your induction hypothesis (IH), namely that P holds for all the objects directly used in the construction of m (for the number m − 1) 4. Induction step: Prove P (m) under the IH. Weak Induction should be more familiar to you and thus more natural to apply. The only things you have to be careful about is: Not using the IH on negative numbers and not creating a naming conflict when you introduce the induction variable m. Strong Induction looks more powerful (but in reality it’s just less complex for some use-cases) and any proof with Weak Induction can easily be converted into a proof with Strong Induction. The process is: 1. Preamble: Let m be any arbitrary object (again, avoid naming conflicts). State that your goal is to prove P (m). 2. Induction Hypothesis: Define your induction hypothesis (IH), namely that P holds for all the objects directly and indirectly used in the construction of m (for all numbers < m) 3. Proof: Prove P (m) under the IH. Note: It’s not part of the proof strategy but there is a 99% chance that you will have to do this by immediately adding a case distinction on whether or not m is a base object and on the construction used to create m. (is m = 0 or not) As you can see, by moving around the parts a tiny bit, you can easily make a Weak Induction proof into a Strong Induction proof. The reverse way is possible but it requires changing the statement that is proved: If you used Strong Induction to prove ∀m. P (m), then you can use Weak Induction to prove ∀m. ∀n ≤ m. P (n). Because the statement changed you will have to add some paragraphs along the lines of ”and because we can assume P (n) for all n < m, it only remains to prove P (m)”. In conclusion, there’s a bit of a difference in how proofs are presented but the main difference is in what you can assume for the proof. That’s why it’s generally a good strategy to use Strong Induction directly unless the exercise explicitly demands using Weak Induction. 0 1 2 3 0 1 2 3 An illustration of the induction hypotheses in Weak vs Strong Induction. In both cases, we want to prove P (m) for m = 3 and objects that we can assume P for are colored green. Weak Induction is on the left, Strong Induction on the right. To write an induction proof, I recommend you follow the following skeletons to write your proof (using the example of natural numbers again and including the almost always used case distinction): Weak Induction Strong Induction We will prove ∀m. P (m) using Weak Induction . Base Case : We will show P (0). [ Proof for P (0)] Step Case : Let m > 0 be arbitrary . IH : We can assume P (m − 1) We will show P (m). [ Proof for P (m) using IH ] We will prove ∀m. P (m) using Strong Induction . Let m be arbitrary . IH : We can assume ∀n < m. P (n) We will show P (m). Case Distinction : Case m = 0: [ Proof for P (0)] Case m > 0: [ Proof for P (m) using IH ] Takeaways: • Induction is only useful with recursive definitions • Use Strong Induction unless something else is explicitly asked of you • Avoid any naming conflicts 32 • Make it incredibly clear what your goal is at every step of the proof and explicitly write out the induction hypothesis (substitute the definition of P ) Hands-On 3 3.1. Let U be a sequence of integers, defined by U0 = U1 = −1, and, for all n ≥ 0, Un+2 = 5 · Un+1 − 6 · Un. Prove that ∀n. Un = 3 n − 2 n+1 (a) using strong induction (b) using weak induction 3.2. Prove that ∀n. ∑n i=0 2 i = 2 n+1 − 1 using weak induction 3.3. Prove that every natural number n ≥ 2 can be written as a product of finitely many primes 33 2.3.2 Induction as a Proof Rule What we’ve done above in essence described a way to prove a statement of the form ∀m. P (m) using some clever structure in our proof. We can also write this as a proof rule for Natural Deduction. No, this is not just because I love writing proof rules, but you actually have to be able to cite this rule in the exam if the question comes up and you have to be able to write such a rule down for different object structures. The rule for Weak Induction is: Γ ⊢ P (0) Γ, P (n) ⊢ P (n + 1) where n not free in Γ Γ ⊢ ∀n. P (n) Note that the change from P (m − 1) =⇒ P (m) above to P (m) =⇒ P (m + 1) now is just to avoid the m > 0 constraint that I introduced such that we’ll never assume P (−1). It’s only a technicality and I introduced it for the two strategies to be more similar but you should remember the rule as stated here to avoid having to add the constraint. For Strong Induction the rule is: Γ, ∀m < n. P (m) ⊢ P (n) where n not free in Γ and m not free in P (n) Γ ⊢ ∀n. P (n) 2.3.3 Structural Induction The course differentiates between mathematical induction and structural induction. The difference is trivial, we only mention it for completeness: Mathematical Induction is by definition induction over the natural numbers. Structural Induction is induction over anything else (Lists or any ADT). In both cases we can use either strong or weak induction. The only notable difference in terms of the actual proof is that structural induction often has more than one way to construct new objects and more than one base object. Meanwhile mathematical induction only has ”increment” as a construction and 0 as a base object. 34 2.3.4 Induction over any ADT We can also define how induction works with any ADT. You need to be able to write down the Natural Deduction rule for induction over any ADT at the exam. Weak Induction We will examine the more complicated rule for Weak Induction first and we’ll use the example ADT data Expr a = Var a | Const Int | Add (Expr a) (Expr a) | Pow (Expr a) Int to illustrate. To define an the weak induction rule, we proceed as follows: 1. Identify base objects: Find all the constructors that don’t have the ADT type as an argument. These construct all the base objects. Here, these are the Var and the Const constructor. 2. Identify recursive arguments: Look at all other constructors and identify the arguments with ADT type. These will be the objects we can assume our statement on as part of the induction hypothesis. Here, these are all the arguments to the Add and the first argument to the Pow constructor. 3. Proof Rule: Write down the proof rule: (a) Let T be the ADT type. Below the rule is the statement: Γ ⊢ ∀x ∈ T . P (x) In the example, our rule start off as: . . . Γ ⊢ ∀x ∈ Expr a. P (x) (b) For each constructor, the rule has a requirement above. The right side is P of the result of the constructor application. We introduce names for all arguments to the constructor. For each recursive argument, say y, we add P (y) to the left side. In our example, the constructors produce the following statements: Constructor Statement Var Γ ⊢ P (Var s) Const Γ ⊢ P (Const i) Add Γ, P (e0), P (e1) ⊢ P (Add e0 e1) Pow Γ, P (e0) ⊢ P (Pow e0 i) (c) Now just write each of those statements on top of the rule and add the side condition. The side condition is that none of the newly introduced variables may be free in Γ or in P . In our example the final rule for Weak Structural Induction over Expr a is: Γ ⊢ P (Var s) Γ ⊢ P (Const i) Γ, P (e0), P (e1) ⊢ P (Add e0 e1) Γ, P (e0) ⊢ P (Pow e0 i) Γ ⊢ ∀x ∈ Expr a. P (x) Side Condition: where e0, e1, i, s are not free in Γ or P Strong Induction The Strong Induction rule on the other hand is super simple: Let T be the ADT type (including type parameters) then the strong induction rule is: Γ, ∀y < x. P (y) ⊢ P (x) Γ ⊢ ∀x ∈ T. P (x) Side Condition: where x is not free in Γ and y is not free in P (x) 35 Don’t worry about defining <, you usually don’t have to do this. Even if you do, the definition is quite easy to get to recursively (every constructor argument of the ADT type is a subterm and also all of their subterms). The reason this rule is so simple is because we leave the differentiation of which constructor was used up to the prover (who in all likelihood will make a case distinction to that end) and we introduce less variables. Takeaways: • The weak induction rule is given by the recipe above • The strong induction rule is always the same for any ADT (because the interesting things hide in the subterm relation) • Do not forget the side conditions! Hands-On 4 4.1 (Assignment 2b from FS13 - 3 point). Give the induction rule for the following Haskell datatype as a proof rule for natural deduction. State all the necessary side conditions. data Tree a b = Leaf a | Node ( Tree a b ) ( Tree a b ) | List [ b ] ( Tree a b ) 4.2. Give the induction rule for the following Haskell datatype as a proof rule for natural deduction. State all the necessary side conditions. data AList a b = AEnd a | ACons a b ( AList a b ) 36 2.3.5 CYP-style proofs We extensively used the online tool CYP during the course to automatically check induction proofs that involved Haskell statements. A common type of exam question is to do this kind of proof so you should be familiar with this. The proof structure is exactly as before though all the examples we did using CYP used Weak Structural Induction. CYP does not understand Strong Induction, but it’s possible you’ll be asked to use strong induction in the exam. There are usually a couple of Lemmas that you will be allowed to cite and additionally, you can always use the Lemma called arith to justify arithmetic steps. In addition to some small things (see the Takeaways), there’s only one interesting concept to discuss here: Equational Reasoning. All the proofs you will conduct that use Haskell will work by Equational Reasoning. That means that you will construct a sequence of Haskell expressions, you will justify in each step that the expressions evaluate to the same value and you will then be able to conclude that the first and last expression evaluate to the same value - i.e. that they are semantically equivalent given whatever additional assumptions you made. It is common and a good strategy to start constructing this sequence from both ends. So if you want to prove that reverse xs evaluates to the same value as foldl (\\a b -> b:a) [] xs, then it’s a good idea to substitute the definition of both reverse and foldl once and to keep going from there. If you only consider one side, you might get sidetracked and prove something unhelpful instead. That is why equational proofs are often in the form: reverse xs = ... = ... = ... = expr foldl (\\ a b -> b : a ) [] xs = ... = ... = expr for some expression expr. The understanding is that we can construct a complete sequence from reverse xs to foldl ... by reversing the order of steps from foldl ... to expr and concatenating them onto the steps from reverse xs to expr. Takeaways: • Weak Induction is usually enough for these exercises • You must justify every single step with either a Lemma, Definition or with arith. • You must only do a single change in a single place per line. This does not apply to arithmetic. • Definitely practice some examples from the exercises using CYP to get the syntax down and to accustom yourself to doing a single thing at a time. 37 Hands-On 5 5.1 (Assignment 4 from FS12 - 10 point). Consider the following functions. imerge :: [ a ] -> [ a ] -> [ a ] imerge [] ys = ys -- imerge .1 imerge ( x : xs ) ys = x :( imerge ys xs ) -- imerge .2 length :: [ a ] -> Int length [] = 0 -- length .1 length ( x : xs ) = 1 + length xs -- length .2 Prove that for all finite lists xs, ys the equality length(imerge xs ys) = length xs + length ys holds. Note: you might have to perform a case distinction in the step case. Take care that you structure your proof clearly and that you justify every proof step. 5.2 (Assignment 3 from FS13 - 9 point). Consider the following functions. take :: Integer -> [ a ] -> [ a ] take _ [] = [] -- take .1 take 0 _ = [] -- take .2 take n ( x : xs ) = x : take (n -1) xs -- take .3 drop :: Integer -> [ a ] -> [ a ] drop _ [] = [] -- drop .1 drop 0 xs = xs -- drop .2 drop n ( x : xs ) = drop (n -1) xs -- drop .3 (++) :: [ a ] -> [ a ] -> [ a ] [] ++ ys = ys -- app .1 ( x : xs ) ++ ys = x : ( xs ++ ys ) -- app .2 Prove by induction that for all integers n and all finite lists xs the equality (take n xs) + +(drop n xs) = xs holds. Structure your proof clearly and justify every proof step. 38 2.4 Solutions 2.4.1 Solutions for Hands-On 1 Hands-On 1.1 (a) In (p(x)) → ¬(∃y. ¬p(y)), the x is free (b) In ∀z. r(x, z) ∧ r(y, z) → ∀x. ∀y. x = y, the first occurrence of x and y are free (c) ∀x. p(x) ∨ ¬p(x) has no free variables (quantifiers extend as far as possible) Hands-On 1.2 (a) (∀x. p(x)) → ¬(∃y. ¬p(y)) is valid (b) ∀x. ∀y. ∀z. r(x, z) ∧ r(y, z) → x = y is satisfiable (e.g. using r(x, y) = x ≤ y with the universe N) but not valid (e.g. using r(x, y) always true) (c) (F → G) ∨ (G → F ) is valid Hands-On 1.3 (a) F [x/y] where F = ∀x.p(x) stays the same because x is not free anywhere in F (b) F [x/y] where F = ∀y.p(x) becomes ∀z. p(y) because we need to avoid the capture of y (c) F [x/f (y, z)] where F = p(x)∧∀y. (∀x. r(x, x)∨r(x, z))∧r(y, x) becomes p(f (y, z))∧∀y′. (∀x. r(x, x)∨ r(x, z)) ∧ r(y′, f (y, z)) 2.4.2 Solutions for Hands-On 2 Hands-On 2.1 (a) (∀y. P (y)) → ∃x. P (x) This formula is a tautology. We can pick any value for x, because of the left side P (x) must hold, and because P (x) holds for this x, it must hold for some x. Picking x arbitrarily in accordance with Γ (which here has no influence on x) corresponds to ∃ I. Using ∀y. P (y) for this particular x corresponds to ∀ E. As a proof tree, this looks like: Ax (∀y. P (y)) ⊢ ∀y. P (y) ∀ E (∀y. P (y)) ⊢ P (x) ∃ I (∀y. P (y)) ⊢ ∃x. P (x) → I ⊢ (∀y. P (y)) → ∃x. P (x) (b) (∃x. ¬P (x)) → ¬∀y. P (y) Intuitively, this formula is also a tautology because: Assume ∀y. P (y) were true, then pick x such that ¬P (x) holds (possible because we know ∃x. ¬P (x)). Now P (x) must be both true and false, so we have a contradiction. Hence, ∀y. P (y) must be false. As before ”picking x” is done using the ∃ E rule. The ”proof by contradiction” translates into the ¬ I rule, the contradiction itself is produced using the ¬ E rule. As a proof tree, this looks like: Ax Γ ⊢ (∃x. ¬P (x)) Ax Γ, ¬P (x) ⊢ ¬P (x) Ax Γ, ¬P (x) ⊢ ∀y. P (y) ∀ E Γ, ¬P (x) ⊢ P (x) ¬ E Γ, ¬P (x) ⊢ ⊥ ∃ E Γ ⊢ ⊥ ¬ I (∃x. ¬P (x)) ⊢ ¬∀y. P (y) → I ⊢ (∃x. ¬P (x)) → ¬∀y. P (y) with Γ ≡ (∃x. ¬P (x)), (∀y. P (y)). Hands-On 2.2 The following is a valid proof tree: 39 Ax Γ ⊢ ∃x. Q(x) Ax Γ, Q(x) ⊢ Q(x) Ax Γ, Q(x) ⊢ ∀y. P (y) ∧ R(y) ∀ E Γ, Q(x) ⊢ P (x) ∧ R(x) ∧ ER Γ, Q(x) ⊢ R(x) ∧ I Γ, Q(x) ⊢ Q(x) ∧ R(x) ∃ I Γ, Q(x) ⊢ ∃x. Q(x) ∧ R(x) ∃ E Γ ⊢ ∃x. Q(x) ∧ R(x) → I (∀y. P (y) ∧ R(y)) ⊢ (∃x. Q(x)) → ∃x. Q(x) ∧ R(x) → I ⊢ (∀y. P (y) ∧ R(y)) → ((∃x. Q(x)) → ∃x. Q(x) ∧ R(x)) where Γ ≡ (∀y. P (y) ∧ R(y)), (∃x. Q(x)) and we also have to check the side condition (∗∗) which is satisfied because x is not free in Γ. Hands-On 2.3 The following is a valid proof tree: where Γ ≡ (∃x. P (x)), (∀x. ¬P (x) ∧ Q) and we also have to check the side condition (∗∗) which is satisfied because x is not free in Γ. 2.4.3 Solutions for Hands-On 3 Hands-On 3.1 Let U be a sequence of integers, defined by U0 = U1 = −1, and, for all n ≥ 0, Un+2 = 5 · Un+1 − 6 · Un. Prove that ∀n. Un = 3 n − 2n+1 (a) We define P (n) ≡ Un = 3 n − 2n+1 and we prove ∀n. P (n) using strong induction. Let n ≥ 0 be arbitrary, and assume ∀j < n. P (j). We will show P (n). We proceed by case distinction: • Case n = 0: This is trivial because U0 = −1 by definition and −1 = 30 − 2 1 • Case n = 1: This is trivial because U0 = −1 by definition and −1 = 31 − 2 2 • Case n ≥ 2: We know that Un = 5·Un−1−6·Un−2 by definition. We also know Un−1 = 3 n−1−2n and Un−2 = 3 n−2−2 n−1 because we can invoke the induction hypothesis on 0 ≤ n−2, n−1 < n. Now it’s just a matter of arithmetic: Un = 5 · Un−1 − 6 · Un−2 = 5 · (3n−1 − 2 n) − 6 · (3 n−2 − 2 n−1) = 5 · 3 n−1 − 5 · 2 n − 6 · 3n−2 + 6 · 2 n−1 = 5 · 3 n−1 − 5 · 2 n − 2 · 3n−1 + 3 · 2 n = 3 · 3 n−1 − 2 · 2 n = 3 n − 2n+1 40 (b) To use weak induction we need to employ a trick, otherwise we could not use the IH on Un−2 as above. So instead of proving P (n), we prove Q(n) ≡ ∀k ≤ n. P (k). This way, having Q(n − 1) gives us P (n − 1) as well as P (n − 2). The proof of ∀n. Q(n) using weak induction: Base Case: We prove Q(0) ≡ P (0) as above Induction Step: Let n ≥ 1 be arbitrary, we assume Q(n − 1) and we prove Q(n). The only additional thing we need to show is P (n) as Q(n − 1) =⇒ ∀k < n. P (k). We again proceed by case distinction: • Case n = 1: This is trivial because U0 = −1 by definition and −1 = 31 − 22 • Case n ≥ 2: We know that Un = 5·Un−1−6·Un−2 by definition. We also know Un−1 = 3 n−1−2n and Un−2 = 3 n−2 − 2 n−1 because Q(n) =⇒ P (n − 1) ∧ P (n − 2). Now it’s just a matter of arithmetic: Un = 5 · Un−1 − 6 · Un−2 = 5 · (3 n−1 − 2 n) − 6 · (3n−2 − 2 n−1) = 5 · 3 n−1 − 5 · 2 n − 6 · 3n−2 + 6 · 2n−1 = 5 · 3 n−1 − 5 · 2 n − 2 · 3n−1 + 3 · 2n = 3 · 3 n−1 − 2 · 2 n = 3 n − 2 n+1 Notice how parts of the proof were moved only around but we also had to add some additional argumentation (marked in bold). This strategy works in general. Hands-On 3.2 Let P (n) ≡ ∑n i=0 2i = 2 n+1 − 1. We prove ∀n. P (n) using weak induction: Base Case: P (0) holds trivially as 1 = 2 1 − 1. Induction Step: Let n ≥ 1 be arbitrary, we assume P (n − 1) and we prove P (n). We can show this directly using only arithmetic: n∑ i=0 2 i = n−1∑ i=0 2 i + 2n = 2 n − 1 + 2n = 2 · 2n − 1 = 2 n+1 − 1 Hands-On 3.3 Let P (n) denote that n + 2 can be written as a product of primes. We prove ∀n. P (n)6. Let n be arbitrary. We may assume ∀m < n. P (m) and we show P (n) using case distinction: • Case n is prime: We can write n = n · 1 which proves the statement. • Case n is not prime: Then by definition, there exist 2 ≥ a, b < n such that n = a · b. Because both a − 2 and b − 2 are still natural numbers, we know P (a) ∧ P (b), so both of them can be written as a product of primes: a = ∏ ai, b = ∏ bi and therefore we can also write n = a · b as a product of finitely many primes: n = ( ∏ ai) · ( ∏ bi) 6I opted to not add the condition n ≥ 2 but to instead talk about n + 2 with n ≥ 0 by default. Both options are valid. 41 2.4.4 Solutions for Hands-On 4 Hands-On 4.1 We build the rule for the data type data Tree a b = Leaf a | Node ( Tree a b ) ( Tree a b ) | List [ b ] ( Tree a b ) Our goal is to deduce Γ ⊢ ∀x ∈ Tree a b. P , so this will form the bottom of our rule. We have three cases to consider so our rule will have three requirements above. We’re not yet sure which variables we’ll use so we can’t write down the side condition yet. So far: . . . . . . . . . Γ ⊢ ∀x ∈ Tree a b. P Now we construct the three cases where x is substituted with the constructor and the arguments receive names. • For the constructor Leaf a, we’ll prove P [x ↦→ Leaf n] • For the constructor Node (Tree a b) (Tree a b), we’ll prove P [x ↦→ Node l r] • For the constructor List [b] (Tree a b), we’ll prove P [x ↦→ List bs t] Now every time we replaced a Tree a b with a variable, we also get the IH for that variable, and we can also use Γ of course. Now we have the rule Γ ⊢ P [x ↦→ Leaf n] Γ, P [x ↦→ l], P [x ↦→ r] ⊢ P [x ↦→ Node l r] Γ, P [x ↦→ t] ⊢ P [x ↦→ List bs t] Γ ⊢ ∀x ∈ Tree a b. P And the side condition is: ”n, l, r, t, bs not free in Γ or P ” Hands-On 4.2 We build the rule for the data type data AList a b = AEnd a | ACons a b ( AList a b ) in exactly the same way: Our goal is to deduce Γ ⊢ ∀x ∈ AList a b. P . The constructors lead to requirements: • For the constructor AEnd a, we’ll prove P [x ↦→ AEnd n] without any IH • For the constructor ACons a b (AList a b), we’ll prove P [x ↦→ ACons n m l] with the IH about l All in all: Γ ⊢ P [x ↦→ AEnd n] Γ, P [x ↦→ l] ⊢ P [x ↦→ ACons n m l] Γ ⊢ ∀x ∈ AList a b. P with the side condition ”n, m, l not free in Γ or P ” 2.4.5 Solutions for Hands-On 5 Hands-On 5.1 Disclaimer: This solution is copied without change from the VIS community solutions. It was originally written by David Enderlin and can be accessed under https://exams.vis.ethz.ch/exams/9yw6bhmq. pdf#5d56889dab292a0011b54007 Here is a CYP checkable solution. Lemma : length ( imerge xs ys ) .=. length xs + length ys Proof by induction on List xs generalizing ys Case [] For fixed ys Show : length ( imerge [] ys ) .=. length [] + length ys Proof length ( imerge [] ys ) ( by def imerge ) .=. length ys 42 length [] + length ys ( by def length ) .=. 0 + length ys ( by arith ) .=. length ys QED Case x : xs Fix x , xs Assume IH : forall ys : length ( imerge xs ys ) .=. length xs + length ys Then for fixed ys Show : length ( imerge ( x : xs ) ys ) .=. length ( x : xs ) + length ys Proof by case analysis on List ys Case [] Assume empty : ys .=. [] Then Proof length ( imerge ( x : xs ) ys ) ( by def imerge ) .=. length ( x :( imerge ys xs ) ) ( by def length ) .=. 1 + length ( imerge ys xs ) ( by empty ) .=. 1 + length ( imerge [] xs ) ( by def imerge ) .=. 1 + length xs ( by def length ) .=. length ( x : xs ) length ( x : xs ) + length ys ( by empty ) .=. length ( x : xs ) + length [] ( by def length ) .=. length ( x : xs ) + 0 ( by arith ) .=. length ( x : xs ) QED Case z : zs Fix z , zs Assume nonempty : ys .=. z : zs Then Proof length ( imerge ( x : xs ) ys ) ( by def imerge ) .=. length ( x :( imerge ys xs ) ) ( by def length ) .=. 1 + length ( imerge ys xs ) ( by nonempty ) .=. 1 + length ( imerge ( z : zs ) xs ) ( by def imerge ) .=. 1 + length ( z :( imerge xs zs ) ) ( by def length ) .=. 1 + (1 + length ( imerge xs zs ) ) ( by IH ) .=. 1 + (1 + ( length xs + length zs ) ) ( by arith ) .=. 1 + ((1 + length xs ) + length zs ) ( by def length ) .=. 1 + ( length ( x : xs ) + length zs ) ( by arith ) .=. 1 + ( length zs + length ( x : xs ) ) ( by arith ) .=. (1 + length zs ) + length ( x : xs ) ( by def length ) .=. length ( z : zs ) + length ( x : xs ) ( by arith ) .=. length ( x : xs ) + length ( z : zs ) ( by nonempty ) .=. length ( x : xs ) + length ys QED QED QED The needed definitions are: data List a = [] | a : List a length [] = 0 length ( x : xs ) = 1 + length xs imerge [] ys = ys imerge ( x : xs ) ys = x :( imerge ys xs ) 43 axiom arith : 0 + a .=. a axiom arith : a + 0 .=. a axiom arith : a + b .=. b + a axiom arith : ( a + b ) + c .=. a + ( b + c ) goal length ( imerge xs ys ) .=. length xs + length ys Hands-On 5.2 Disclaimer: This solution is copied without change from the VIS community solutions. It was originally written by Davud Evren and can be accessed under https://exams.vis.ethz.ch/exams/jvkpag10. pdf#67ludsikl22i8x5s Here is a CYP checkable solution. Lemma : ( take n xs ) ++ ( drop n xs ) .=. xs Proof by induction on List xs generalizing n Case [] For fixed n Show : ( take n []) ++ ( drop n []) .=. [] Proof ( take n []) ++ ( drop n []) ( by def take ) .=. [] ++ ( drop n []) ( by def drop ) .=. [] ++ [] ( by def ++) .=. [] QED Case x : xs Fix x , xs Assume IH : forall n : ( take n xs ) ++ ( drop n xs ) .=. xs Then for fixed n Show : ( take n ( x : xs ) ) ++ ( drop n ( x : xs ) ) .=. ( x : xs ) Proof ( take n ( x : xs ) ) ++ ( drop n ( x : xs ) ) ( by def take ) .=. ( x : ( take ( n - 1) xs ) ) ++ ( drop n ( x : xs ) ) ( by def drop ) .=. ( x : ( take ( n - 1) xs ) ) ++ ( drop ( n - 1) xs ) ( by def ++) .=. x :(( take ( n - 1) xs ) ++ ( drop ( n - 1) xs ) ) ( by IH ) .=. x : xs QED QED The needed definitions are: data List a = [] | a : List a take n [] = [] take 0 xs = [] take n ( x : xs ) = x : take ( n - 1) xs drop n [] = [] drop 0 xs = xs drop n ( x : xs ) = drop ( n - 1) xs [] ++ ys = ys ( x : xs ) ++ ys = x : ( xs ++ ys ) goal ( take n xs ) ++ ( drop n xs ) .=. xs 44 3 Language Semantics We now move on to the main topics of Part 2 of the course. We will discuss Language Semantics in this section. Language Semantics loosely means ”how is a language works” - i.e. given a program as some syntactic construct with assignments, sequential statements, loops, etc, what actually happens during execution? Language semantics are defined in terms of each basic syntactic construct in the language. So we define how a program works by defining how assignments, sequential composition, loops, etc work in isolation and the language behavior emerges from the interaction of these basic building blocks. We will discuss two approaches in detail: Natural Semantics and Structural Operational Semantics. They both aim to define how programs are executed - given a program and a state, what is the next state in the execution. This is why we call these Operational Semantics. We will also look at Axiomatic Semantics. It does not belong to the operational semantics because it does not define how a program executes. Instead, Axiomatic Semantics helps us to construct proofs about programs - given a program and a logical formula that holds, what is another logical formula that we can deduce for the next execution step. But before we dive right into the two forms of operational semantics, let’s set up some context. 3.1 Background IMP In this entire section, we will work with the IMP language. It’s a very simple language and its syntax is as follows: • There is a set of (integer-valued) program variables we call Var • There is a set of Arithmetic Expressions (Aexp) that are either: – Program variables, – Integer literals, or – The result of a binary operation (+, -, *) between two Arithmetic Expressions • There is a set of Boolean Expressions (Bexp) that are either: – The result of a comparison (=, #, <, <=, >, >=) between two Arithmetic Expressions, – The disjunction (or) of two Boolean Expressions, – The conjunction (and) of two Boolean Expressions, or – The negation (not) of one Boolean Expressions Note that whenever you make a case distinction on one of the types of expressions then you will need to cover exactly these 3 or 4 cases above respectively. IMP Programs are now simply statements and any statement can be: • Skips: skip • Assignments: x := e • Sequential Compositions: s1; s2 • If Statements: if b then s1 else s2 end • While Statements: while b do s end where e ∈ Aexp, b ∈ Bexp, x ∈ Var and s, s1, s2 are statements. Note that these are the 5 cases you would need to distinguish for case distinctions on statements. Also note that a statement like s1; s2; s3 would be understood as (s1; s2); s3 - i.e. sequential composition associates to the left. This is important because it makes sure that there’s only ever one way to split the program into the components (although the choice of left associativity is arbitrary). 45 Syntactic Sugar While not strictly part of the syntax, we will define a couple of short-hands that you might see used in the course. These are not strictly part of the IMP syntax and you do not need to worry about leaving them out in a proof. This paragraph is only here so it’s unambiguously clear how these short-hands could be written in IMP. • true is short for the Boolean Expression 1 = 1 • false is short for the Boolean Expression 0 = 1 • if b then s end is short for the statement if b then s else skip end States To define how an IMP statement executes we need to track the program state. Any mutable state we might need to execute the program should be part of the program state. In our case, this is only the mapping from variables to integer values. We denote states using the Greek letter sigma: σ : Var → Z Note that this implicitly means that all program variables have some value at every point in the execution (there is no ”out of scope”) and that all the values are mathematical integers (there is no overflow). For this to make sense, we use the convention to always start executions in a state σzero where every variable is mapped to 0. Unless of course we explicitly use another initial state. We can modify states by overwriting the value of a variable with another integer, this produces a new state that assigns every variable an integer value by checking7 if it’s the overwritten variable and returning either the new integer or looking the variable up in the original state respectively. Formally, we can write this as follows for an original state σ in which we update the value of the variable x ∈ Var to v ∈ Z. σ[x ↦→ v] is the new state and y ∈ Var is arbitrary: σ[x ↦→ v](y) = {v if y ≡ x σ(y) if y ̸≡ x Also, if you ever need to prove that states are equal, you can only do that by proving that for all variables, both states map the variables to the same value. σ1 = σ2 ⇐⇒ ∀x ∈ Var. σ1(x) = σ2(x) Semantic Functions We also need to define some common functions to help us evaluate expressions. They will be used when we look at concrete semantics further below. To evaluate integer literals, we use the function N . Its inputs are strings, i.e. text and its output is an integer. In the lecture, we called strings that can be parsed to a number Numerals. But important is: Text in, Number out. Its definition is as you would expect. To evaluate arithmetic expressions, we use the function A. Inputs are an expression but also a state (because we might need to look up variables), the output is an integer. Literals are parsed using N , variables are looked up in the state and the operations are executed on results of recursive calls to A. Evaluating Boolean expressions works analogously with a function B. Inputs are the expression and the state, but the outputs are now either tt or f f . The Boolean operations are carried out on results of recursive calls to B. Comparisons are carried out on the results of calls to A. Notation: To denote the result of the evaluation of an arithmetic expression e in a state σ, we write AJeKσ Similarly for the other functions (if the state is not needed it is simply omitted). 7We use ≡ to check if the variable names are equal and = to check if the associated values are equal 46 Configurations and Derivation Sequences Configurations contain all the information about pro- gram and state, so any configuration is either: • A pair of statement and state ⟨s, σ⟩ - meaning we still need to execute s and we should execute it in the program state σ, or • Simply a state σ - meaning there is nothing left to execute and the final program state is σ We call the first kind of configuration a non-terminal configuration and the second kind a terminal configuration. And finally, an Derivation Sequence is a sequence of configurations where every step is justified by the language semantics. How exactly we justify such steps will be defined per semantic. Disclaimer In this course and in this script, we sometimes gloss over executions for non-terminating programs. Whenever we say ”given an execution ⟨s, σ⟩ → · · · → σ′”, we implicitly require that the program terminates at all. If it does not, then there is simply no such execution (each configuration would be non-terminal by definition). This does not mean that you don’t need to understand how non-terminating programs could behave and how they relate to the rest of the material covered. It only means that we will sometimes implicitly assume termination. Further, IMP is deterministic. That means that for any statement and any initial state, there is only a single possible execution. The material we’ll cover below however, allows for non-determinism. Whenever we say ”given an execution ⟨s, σ⟩ → · · · → σ′”, that does not mean that there is only this execution. It means that this is one possible execution that is consistent with the semantics. If the language had non-deterministic constructs like randomness, non-deterministic choice or input/output then we could apply the same definitions as for IMP and a program could simply have multiple executions for the same initial state. 47 3.2 Natural Semantics Natural Semantics (NS), also called Big-Step Semantics works as follows: • Any execution has exactly two configurations: An initial non-terminal one and a final, terminal one • The ”step” relation is called ”→” • The one step in these executions is justified using Natural Deduction and the rules in Appendix E As a consequence, complicated programs produce complicated proof trees when we justify the execution. Non-terminating programs would try to produce a proof tree of infinite height. So we can say that: Using Natural Semantics, proof trees are unbounded in height while executions are bounded in length. The main type of statement that we prove using Natural Semantics is to say that ”there is a proof tree that justifies the execution from ⟨s, σ⟩ to σ′”. We write ⊢ ⟨s, σ⟩ → σ′ or equivalently ∃T. root(T ) = ⟨s, σ⟩ → σ′. We’ll now look at three common questions: • Applying the semantics to a given program or proving that some given execution is possible • Proving a general fact about the semantics (e.g. that IMP is deterministic w.r.t. NS) • Extending the semantics to incorporate some new behavior 3.2.1 Executions with Natural Semantics A typical question of this form usually takes the following shape: You are given a program s and a (partial) description of the initial state that defines the values of all the relevant variables (e.g. ”let σ be a state such that σ(x) = 1, σ(y) = 0”). Your goal is to show that an execution to a specified final state is possible (e.g. to a state σ′ such that σ′(x) = 3). Usually, this is the only possible execution (IMP is deterministic) but sometimes this question is also posed with respect to a variant of IMP that is non-deterministic and that’s where the weird formulation comes from. For NS, you can solve this question easily: 1. Begin with the statement you want to prove, but leave out the final state 2. Figure out what the next rule up in the tree should be, fill it in and leave out the final state again 3. Once you encounter rules without preconditions, add in the final states from there and propagate the final states through the tree until finally you can write down the final state in the statement you wanted to prove all along You can save yourself a lot of writing by defining names for component statements (e.g. let the loop body be sL, etc) and by defining a short-hand for states. But, you have to explain your notation in every question that you use it in! See the below example. Example. Given the statement s as x := 2; y := x*x, let’s build a tree justifying that there is a state σ′ where σ′(y) = 4 such that ⊢ ⟨s, σzero⟩ → σ′. First, we build the tree (which is easy here but in general should be done in steps so you can properly evaluate expressions that might change the control flow) and we replace final or unknown states with . . . : AssN S ⊢ ⟨x := 2, σzero⟩ → . . . AssN S ⊢ ⟨y := x*x, . . . ⟩ → . . . SeqN S ⊢ ⟨s, σzero⟩ → . . . Next, we figure out the states and we propagate them through the tree according to the rules. For this, we’ll define the short-hand [v1, v2] to mean σzero[x ↦→ v1][y ↦→ v2]. The updated tree is: 48 AssN S ⊢ ⟨x := 2, σzero⟩ → [2, 0] AssN S ⊢ ⟨y := x*x, [2, 0]⟩ → [2, 4] SeqN S ⊢ ⟨s, σzero⟩ → [2, 4] And this is a valid tree that justifies the corresponding NS execution. You should double-check that we used all the rules correctly. 3.2.2 Proofs with respect to Natural Semantics Another common type of question is to prove some property of the semantics. Let’s explore this using the example of determinism in IMP. The question might be to ”prove that ∀s, σ, σ′, σ′′. ⊢ ⟨s, σ⟩ → σ′∧ ⊢ ⟨s, σ⟩ → σ′′ =⇒ σ′ = σ′′ First and foremost, these exercises are about understanding this gibberish: Recall ⊢ . . . means ”there exists an execution” and you should be able to see that this reads roughly as ”for any initial configuration, if there are multiple (not necessarily distinct) possible terminal configurations then they must be equal” which we use to express determinism in IMP. Notice that this is a statement about NS because we use ⊢ and →. Now we can solve this exercise. Because these questions are quite varied, there is no concrete recipe but the general steps are always: 1. Understand the question and what it’s asking intuitively 2. Transform the statement into a statement about NS derivation trees 3. Use strong (structural) induction over the shape of the derivation tree to prove the statement We already did the first step above. Now we need to transform this statement into something we can prove using induction. This means that we want to end up with a statement like ∀T. . . . Specifically, this means we have to use the definition of one of the ⊢ symbols. We have to pay attention here because the definition gives us an existential quantification (∃) while we need universal quantification (∀). In our example, we can do the following steps: ∀s, σ, σ′, σ′′. ⊢ ⟨s, σ⟩ → σ′∧ ⊢ ⟨s, σ⟩ → σ′′ =⇒ σ′ = σ′′ ⇐⇒ ∀s, σ, σ′, σ′′. (∃T. root(T ) = ⟨s, σ⟩ → σ′)∧ ⊢ ⟨s, σ⟩ → σ′′ =⇒ σ′ = σ′′ ⇐⇒ ∀s, σ, σ′, σ′′. ∀T. root(T ) = ⟨s, σ⟩ → σ′∧ ⊢ ⟨s, σ⟩ → σ′′ =⇒ σ′ = σ′′ ⇐⇒ ∀T. ∀s, σ, σ′, σ′′. root(T ) = ⟨s, σ⟩ → σ′∧ ⊢ ⟨s, σ⟩ → σ′′ =⇒ σ′ = σ′′ Notice that we used the fact that (∃x. F ) → G ≡ ∀x. (F → G) and recall that in FMFP, quantifiers extend as far to the right as possible. Often, it makes sense to only substitute the definition of ⊢ once here because we basically never want to do induction as part of an induction proof. We can later still use the definition of ⊢ during the proof if we need to. 49 To prove determinism of IMP we can now use strong induction. Our proof would look roughly like (recall section 2.3.1): Let P (T ) = ∀s, σ, σ′, σ′′. root(T ) = ⟨s, σ⟩ → σ′∧ ⊢ ⟨s, σ⟩ → σ′′ =⇒ σ′ = σ′′ . We will prove ∀T. P (T ) using Strong Induction over the shape of T . Let T, s, σ, σ′, σ′′ be arbitrary and assume that root(T ) = ⟨s, σ⟩ → σ′ as well as ⊢ ⟨s, σ⟩ → σ′′ . We now prove that σ′ = σ′′ . IH : We can assume ∀T ′ < T. P (T ′) We will show P (T ). Case Distinction on the last rule in T : Case AssN S : In this case , the statement s needs to be of the form x := e for some x, e and T consists of only that rule . Also , σ′ = σ[x ↦→ AJeKσ]. Let T ′ be a tree with root ⟨s, σ⟩ → σ′′ ( we assumed its existence above ) . Because of the form of s, T ′ also needs to consist only of the AssN S rule and the rule only allows for σ′′ = σ[x ↦→ AJeKσ]. We can conclude that σ′ = σ′′ . Case SeqN S : [...] [...] Notice that we only wrote down the entire statement once as P (T ). We also moved the quantification of the statement into text (”let ... be arbitrary”) and we assumed the left hand side of the implication in text. This way, the rest of the proof can be about just proving the core of the statement (namely σ′ = σ′′). If you want you can add ”because otherwise the statement is trivially true” after the assumptions. The meat of the proof is now up to you to figure out. Some general guidelines: 1. You know the last rule in T , what does this tell you about the statement s and the states involved? 2. You now know more about s and the states, what does this tell you about any additional assump- tions? (in this case, you can learn about the tree T ′) 3. Now use this gathered knowledge to finish the proof for this case. 4. Also, you can skip cases that are impossible (if you’re already given the shape of s then not all rules could be the last rules in T ) 5. If you need to construct a tree as part of the proof (e.g. if the right hand side contains ⊢), clearly label that this is a new construction and make it obvious how it’s built up. 3.2.3 Extensions of Natural Semantics Some questions ask you to introduce a new kind of statement to IMP, to modify the semantics or to explore how some new rule would behave. These are all questions where we don’t exactly use NS but we extend it. This can involve: • New rules that can be used instead of other rules (e.g. allowing for non-determinism) • A new definition of the program state (e.g. tracking the number of executed assignments in addition to σ) • New statements that may or may not be allowed for use by the programmer (e.g. adding a for-loop, or adding division but also an abort statement that is only created when we try to divide by 0) Whenever you need to ”extend the semantics of IMP”, you can do any modifications you want to the above components. The entire definition of IMP from expressions to statements to the rules and state is fair game. It’s common understanding that IMP programs that don’t use the new functionality should behave the same as before - so we only add constructs in practice instead of modifying the existing ones. 50 It’s worth noting however, that if we change e.g. the definition of states then we also need to redefine all the existing rules because they use ”program state” incorrectly or maybe we even want to define new behavior with the extended state. Try to pay attention to not introduce ”stuck” configurations (where we cannot continue execution) or ambiguity in which rule to apply (unless you want non-determinism). 51 Hands-On 1 1.1 (Assignment 6 from FS14 - 3 points). Consider the following IMP program s: while x < y do x := x + 1; y := y - 1 end Let σ be a state such that σ(x) = 2 and σ(y) = 3. Prove using the natural semantics (big-step semantics - see page 35) that there is some state σ′ with σ′(x) = 3 and σ′(y) = 2 such that ⊢ ⟨s, σ⟩ → σ′. Provide the complete derivation tree. Don’t forget to write the names of the rules as part of your derivation tree. If you use any abbreviations in your solution, don’t forget to define them clearly. 1.2 (Assignment 8 from FS14 - 13 points). The following lemma (that you may recall seeing in your exercise sessions) can be understood informally as stating that the execution of a statement can only modify its free variables (see appendix D) ∀s, σ, σ′, x. (⊢ ⟨s, σ⟩ → σ′ ∧ x ̸∈ F V (s) =⇒ σ(x) = σ′(x)) However, we cannot apply this lemma in situations in which the variable x is only read from in the program s. For example, from ⊢ ⟨y:=z, σ⟩ → σ′ we cannot deduce σ(z) = σ′(z) using the lemma directly, because z ∈ F V (y:=z). (a) Define a refinement of F V (s), called wr(s), that represents the set of variables potentially written to by the statement s. For example, it should be the case that: wr(if x > 0 then y := x else z := x end) = {y, z} (b) Prove that the correspondingly-refined lemma holds: ∀s, σ, σ′, x. (⊢ ⟨s, σ⟩ → σ′ ∧ x ̸∈ wr(s) =⇒ σ(x) = σ′(x)) 1.3 (Assignment 7 from FS13 - 8 points). Prove that: ∀v, s, σ, σ′. ⊢ ⟨while b do s end, σ⟩ → σ′ =⇒ BJbKσ′ = ff 1.4 (Assignment 5 from FS21 - 7 points). In this assignment, we consider an extension to IMP adding input and output functionality. This extension makes use of two new statements: • read x reads a number into the variable x. This input should come from the user, which we model by assigning a non-deterministic value to variable x. • write x evaluates the value of the expression e and adds the result to the end of the output log. The output log is a list of numbers which we track as part of the state. Throughout this task, you may use the following notation for lists: • [] to represent an empty list and • vs :: v to represent element v appended to the end of the list vs. You may also use the above syntax to pattern match on values when providing new definitions. (a) Define a suitable extension to IMP states to accommodate a log of output values. Make sure to also define an appropriate initial state, τzero. Then define the big-step semantics for the read and write statements. (b) Provide a new definition for the AssN S rule using the extended states defined in the previous task. (c) Does the following program terminate successfully starting in state τzero? If so, prove termi- nation for the program in big-step semantics. If not, explain why. x := 1; while x > 0 do read x end 52 3.3 Structural Operational Semantics Structural Operational Semantics (SOS), also called Small-Step Semantics works as follows: • Any execution has arbitrarily many configurations: An initial non-terminal one and at some point a final, terminal one • The ”step” relation is called ”→1” • The steps in these sequences are justified using Natural Deduction and the rules in Appendix F • To say ”there is a derivation sequence from ⟨s, σ⟩ to σ′”, we write ⟨s, σ⟩ → ∗ 1 σ′. As a consequence, complicated programs produce long derivation sequences. Non-terminating programs would try to produce an execution of infinite length. So we can say that: Using Structural Operational Semantics, proof trees are bounded in height while derivation sequences are unbounded in length. The main type of statement that we prove using Natural Semantics is to say that ”there is a derivation sequence that justifies the execution from ⟨s, σ⟩ to σ′”. We write ⟨s, σ⟩ → ∗ 1 σ′ or equivalently ∃k. ⟨s, σ⟩ → k 1 σ′. Notice that k = 0 is possible and would correspond to execution where nothing happens (we have to worry about these a bit). This wasn’t a thing for NS because we don’t allow for empty proof trees. 3.3.1 Executions with Structural Operational Semantics Again, the question here is given a program s and a description of the initial state, show that an execution to a specified final state is possible. Usually, this is again the only possible execution (IMP is deterministic) but sometimes this question is also posed with respect to a variant of IMP that is non-deterministic. For SOS, you can solve this question as follows: 1. Begin with the initial configuration and state 2. Figure out what the next rule can be, and write down the next configuration (one small step later) 3. Repeat until you end in a terminal configuration 4. As required by the question, justify none, some or all steps with proof trees Again, you can save yourself a lot of writing by defining names for component statements and by defining a short-hand for states. We’ll look at the example we already used for NS. Example. Given the statement s as x := 2; y := x*x, let’s build a derivation sequence justifying that there is a state σ′ where σ′(y) = 4 such that ⟨s, σzero⟩ → ∗ 1 σ′. Also, let’s justify the first small step with a proof tree. First, we define the short-hand [v1, v2] to mean σzero[x ↦→ v1][y ↦→ v2] and we construct the sequence: ⟨s, [0, 0]⟩ →1⟨y := x ∗ x, [2, 0]⟩ →1[2, 4] And the justification of the first step is: AssSOS ⟨x := 2, [0, 0]⟩ →1 [2, 0] Seq1SOS ⟨s, [0, 0]⟩ →1 ⟨y := x ∗ x, [2, 0]⟩ And assuming that the second step also has a valid proof tree, this shows: ⟨s, σzero⟩ → 2 1 σ′ =⇒ ⟨s, σzero⟩ → ∗ 1 σ′ Which we wanted to show. 53 3.3.2 Proofs with respect to Structural Operational Semantics The main concepts here are almost exactly as for NS. I won’t repeat the overview here, you can refer to section 3.2.2. The main difference is that in NS, ”executions” consisted of only a single tree so we could prove everything using induction over trees. In SOS however, ”executions” are derivation sequences so we have to use induction over the length of the derivation sequence. So: 1. We still use strong induction, but now over k, the length of the derivation sequence 2. Instead of unrolling ⊢ to ∃T. root(T ) = . . . , we now unroll · · · →∗ 1 . . . to ∃k. · · · →k 1 . . . 3. Instead of invoking the IH on smaller trees, we invoke it on shorter sequences 4. Instead of doing a case distinction on the last rule in T , we first check for k = 0 and then look at the last rule in the tree justifying the first small step (this achieves the same effect of telling us something about s) Also, if you need to construct a derivation sequence as part of the proof (e.g. if the right hand side contains →∗ 1), clearly label that this is a new construction and make it obvious how it’s built up. Also justify all the small steps that are not copied from another sequence. Other than that, nothing much changes. The partial proof from before now looks like: Let P (k) = ∀s, σ, σ′, σ′′. ⟨s, σ⟩ → k 1 σ′ ∧ ⟨s, σ⟩ → ∗ 1 σ′′ =⇒ σ′ = σ′′ . We will prove ∀k. P (k) using Strong Mathematical Induction . Let k, s, σ, σ′, σ′′ be arbitrary and assume that ⟨s, σ⟩ → k 1 σ′ as well as ⟨s, σ⟩ → ∗ 1 σ′′ . We now prove that σ′ = σ′′ . IH : We can assume ∀k′ < k. P (k′) We will show P (k). Case Distinction : Case k = 0: This case cannot occur because ⟨s, σ⟩ is a non - terminal configuration while σ′ is terminal , hence they cannot be equal . Case k > 0: There exists a configuration γ ( terminal or non - terminal ) such that ⟨s, σ⟩ →1 γ . Let T be a derivation tree with this root . Case Distinction on the last rule in T : Case AssSOS : In this case , the statement s needs to be of the form x := e for some x, e and T consists of only that rule . Thus , γ = σ′ = σ[x ↦→ AJeKσ].Because σ′′ is also terminal , there must also be a configuration γ′ such that ⟨s, σ⟩ →1 γ′ ( this is the first step of the sequence whose existence we assumed above ) . Let T ′ be a tree with this root . Because of the form of s, T ′ also needs to consist only of the AssSOS rule and the rule only allows for γ′ = σ′′ = σ[x ↦→ AJeKσ]. We can conclude that σ′ = σ′′ . Case SeqSOS : [...] [...] 3.3.3 Extensions of Structural Operational Semantics Extensions for SOS work the same as for NS. Refer to section 3.2.3. 54 Hands-On 2 2.1 (Assignment 6 from FS13 - 4 points). Consider the following IMP program s: while x < y do y := y - 1; if x < y then x := x + 1 else skip end end and a starting state σ in which σ(x) = 0 and σ(y) = 2. Write a structural operational semantics (small-step semantics) derivation sequence to find the state σ′ such that ⟨s, σ⟩ → ∗ 1 σ′ You do not need to provide the derivation trees justifying each step of your sequence. 2.2 (Assignment 7 from FS16 - 10 points). Recall the extension of the small-step semantics of IMP with rules for non-determinism: (ND1SOS) ⟨s[]s ′, σ⟩ →1 ⟨s, σ⟩ (ND2SOS) ⟨s[]s ′, σ⟩ →1 ⟨s ′, σ⟩ We can use non-determinism to write a while loop which ”chooses” an integer value (an idea which we have seen in the context of Promela modelling). For example, consider the following IMP statement s which chooses any integer value for y equal to or higher than its initial value: while x = 0 do y := y + 1 [] x := 1 end Prove, by induction on n, that: ∀n : N at. ∀σ. σ(x) = 0 =⇒ ⟨s, σ⟩ → ∗ 1 ⟨s, σ[y ↦→ σ(y) + n][x ↦→ 1]⟩ You do not need to provide the derivation trees justifying any k-step reduction (→k 1) steps used in your proof. You may use any of the following lemmas without proving them - state clearly where you use them in your solution (if you do): Lemma 1: ∀σ, x. σ[x ↦→ σ(x)] = σ Lemma 2: ∀σ, x, v1, v2. σ[x ↦→ v1][x ↦→ v2] = σ[x ↦→ v2] Lemma 3: ∀σ, x, y, v1, v2. x ̸≡ y =⇒ σ[x ↦→ v1][y ↦→ v2] = σ[y ↦→ v2][x ↦→ v1] 2.3. In this task, we will define small-step semantics rules for a break statement. You may assume that break only occurs inside loop bodies. Ensure that a break in a nested loop only jumps out of the inner loop. Your job is to define a SOS rule for the break statement and to show specifically how (if at all) the rules AssSOS and WhileSOS need to be adapted. Note: You can use the following two IMP programs to manually check your rules: while 1=1 do break end ; while 1=1 do s end while 1=1 do break ; while 1=1 do s end end You may introduce additional statements if you provide appropriate SOS rules for them. 55 3.4 Axiomatic Semantics Axiomatic Semantics (AX) are very similar to NS. Proving facts about the semantics and extending it work exactly the same as in NS (strong induction over the shape of the derivation tree). Axiomatic semantics are also ”big-step” in the sense that they take any non-terminal configuration directly to a terminal one using a single proof tree. What’s different about AX is the kinds of statements we prove. AX is used to prove the validity of an Hoare Triple. A Hoare Triple {P }s{Q} is valid if and only if the following is true: Whenever the formula P holds before the execution of s then the formula Q holds after the execution. AX does this by using adapted NS-rules. They are given in appendix G. We will now focus on some special concepts needed only with AX. If you want to look up anything about how to create proofs or extensions, refer to section 3.2. 3.4.1 Proof Outlines Constructing a proof outline is very similar in spirit to constructing executions in NS (see 3.2.1) but we use different notation. Instead of writing a tree and decomposing the program as part of the tree, we now intersperse the tree into the program. This means we write down the program and before and after each line (atomic statements like assignments and skips) we write down the pre- and post-conditions of the corresponding Hoare Triple. This could look like: {0 = 0 ∧ x ≥ 0 ∧ x = X} y := 0; {y = 0 ∧ x ≥ 0 ∧ x = X} if x >= 1 then {x ≥ 1 ∧ y = 0 ∧ x ≥ 0 ∧ x = X} ⊨ {x = x ∧ x = X} y := x {x = y ∧ x = X} else{¬(x ≥ 1) ∧ y = 0 ∧ x ≥ 0 ∧ x = X} ⊨ {x = 0 ∧ y = 0 ∧ x = X} ⊨ {x = y ∧ x = X} skip {x = y ∧ x = X} end {x = y ∧ x = X} which proves the Hoare Triple {0 = 0 ∧ x ≥ 0 ∧ x = X} y:= 0; if x >= 1 then y := x else skip end {x = y ∧ x = X} You also saw some things in the proof outline above that might seem weird: • We use ⊨ to denote that there would have been an application of the ConsAx rule. • We use capital letters (like X) to ”save” the initial values of variables. In our case, the initial value of y would be inaccessible after the first line, so if we want to use such a value later, we need to use a capital letter to remember it. There are always part of the first line, we don’t introduce them during the proof (except for introducing the variant). A good strategy to write these proofs is: 1. First try to do the proof on some scratch paper or in your head - very informally but figuring out intuitively why the triple should be valid. 2. Now write down the very first pre-condition and the very last post-condition. 56 3. Now work backwards and always ask yourself ”what needs to be true so I can show this” 3.4.2 Partial and Total Correctness AX comes in two flavors: Partial Correctness and Total Correctness. The section up to here only used concepts that are common in both types (and sometimes notation from partial correctness). To see why both flavors exist, recall the definition of a valid Hoare Triple: Whenever the formula P holds before the execution of s then the formula Q holds after the execution. But what do we do if the program does not terminate? Maybe we don’t care because there are no outputs that we can say something about - or maybe we do care because we need to be sure that our program always terminates. Partial Correctness proves that if the statement terminates, then Q holds. Total Correctness proves that the statement will terminate and Q will hold. This has no effect on all rules other than the rule for loops (because that is the only place where non-termination could arise). If we’re doing a proof with Total correctness, all post-conditions have a downwards arrow ⇓ added to the front to make it clear which flavor we’re using. For example: {P } s {⇓ Q} requires a proof with the Total Correctness rules and {0 = 0} x := 0 {⇓ x = 0} would be something (boring) that we can prove this way. 3.4.3 Invariants and Variants One special rule in both flavors of AX is the rule for loops. Using Partial Correctness, a proof about loops requires an invariant. A formula that holds before the loop, after each loop iteration and also after the loop. The proof rule (written as a proof outline) is: {P } while b do {b ∧ P } s {P } end {¬b ∧ P } Here, P is the invariant. Notice: • P must contain all the facts you want to carry from before to after the loop (e.g. initial variable values etc) • After the loop, we can also use ¬b. A common use is to have something like x ≤ N in P when the loop has e.g. b = x < n as a condition. This way, we could conclude x = N with one ConsAx rule after the loop. • The proof in the loop body can additionally use b. It’s worth spending five minutes to figure out the invariant before attempting to write the proof outline. In most questions you can also give the invariant P explicitly for partial points, so it’s worth writing it down first. Usually, you find the invariant in steps: First find the most essential parts (using your intuitive understanding of why the code is correct) then add in the technicalities later (constraints you need during the loop body, other facts you just want to carry through the loop, etc). For Total Correctness, the loop rule is slightly different (let Z be a new capital letter that was not used before): {P } while b do {b ∧ P ∧ e = Z} s {P ∧ e < Z} 57 end {⇓ ¬b ∧ P } and it also has a side condition: b ∧ P ⊨ 0 ≤ e So the new part here is the variant e, in contrast to the invariant P it’s its job to change during the loop body. Specifically, the values of e must be positive during the execution of the loop body and they must decrease strictly in each iteration. This gives us termination because e has to reach the value zero or below sooner or later. Unlike P , e is not a formula but rather just an arithmetic expression. But like P , you should also write it down explicitly before you start the proof. You also have to check the side condition explicitly when you use this rule (best justify it in a footnote). This should give you all the necessary knowledge to write your own proof outlines. Takeaways: • Get an intuitive understanding before starting the proof • Check which flavor of AX is required in the question • Figure out invariant (and maybe also the variant) ahead of time • Write the proof outline and always think backwards (what do I need for this to hold?) 58 Hands-On 3 3.1 (Assignment 7 from FS14 - 11 points). Consider the following IMP program s: while r >= 0 do r := r - d ; q := q + 1 end ; ( r := r + d ; q := q - 1) The program s computes the quotient q and remainder r resulting from the division of a given non-negative integer N (initially stored in the variable r) by a given positive integer D (stored in the variable d). (a) Find an appropriate loop invariant and loop variant for the while loop. The loop invariant must be strong enough to allow you to prove the Hoare triple in part (b) of the question. (b) Construct a (total correctness) proof outline (see Appendix G) to show that: ⊢ {N ≥ 0 ∧ D > 0 ∧ d = D ∧ r = N ∧ q = 0} s {⇓ N = q · D + r ∧ r ≥ 0 ∧ r < D} 3.2 (Assignment 6 from FS19 - 14 points). Consider extending IMP with the statements guess x and assert b, where x is a variable and b is a boolean expression. The statement guess x implements an angelic choice, that is, non-deterministically assigns a value to x such that the rest of the program terminates successfully, if possible. The statement assert b checks whether the condition b holds and fails if it does not. The axiomatic semantics of these two new statements are given by the following derivation rules: (GuessAx) {∃n. P [x ↦→ n]} guess x {P } (AssertAx) {b ∧ P } assert b {P } where, in the first rule, n ranges over numerals and does not occur in P . (a) The statements x := e and guess x; assert x = e are provably equivalent for all variables x and expressions e that satisfy x ̸∈ F V (e). Show that the condition x ̸∈ F V (e) is necessary by appropriately choosing a variable x, an expression e, and some predicates P and Q such that the Hoare triple {P } x := e {Q} is valid but the Hoare triple {P } guess x; assert x = e {Q} is not. It is sufficient to informally justify your answer; a formal proof is not required. (b) Prove that for all variables x and all expressions e with x ̸∈ F V (e) the statement ∀P, Q. ⊢ {P } x := e {Q} =⇒ ⊢ {P } guess x; assert x = e {Q} holds. Note that this proves one direction of the aforementioned equivalence. You may use the following lemma without proof. Lemma 1: ∀x, e, e′. x ̸∈ F V (e) =⇒ e[x ↦→ e′] ≡ e 59 Hands-On 3 3.3 (Assignment 10 from FS14 - 12 points). Recall that a (partial correctness) Hoare triple {P }s{Q} is said to be valid if the following condition holds: ∀σ, σ′. (BJP Kσ = tt∧ ⊢ ⟨s, σ⟩ → σ′ =⇒ BJQKσ′ = tt) In axiomatic semantics, we define a derivation rule to be sound if: for every instance of the rule, if all of the rule premises are valid triples and the rule side-condition (if any) is true, then the rule conclusion is also a valid triple. The assignment rule (AssAx) used in the course (appendix G) is convenient for constructing proof outlines backwards: that is, when we already know the assertion to use after an assignment statement, and need to find a corresponding assertion to use before it. Here, we attempt to formulate an alternative rule (AssFwAx) that could be more convenient for constructing proof outlines forwards: (AssFwAx) {P } x := e {x = e ∧ P } (a) Show that the proposed rule (AssFwAx) is not sound: that is, find a P, x, e, σ and σ′ such that the following is not true: BJP Kσ = tt∧ ⊢ ⟨x := e, σ⟩ → σ′ =⇒ BJx = e ∧ P Kσ′ = tt (b) Write down a side condition which can be added to the proposed rule (AssFwAx), so that the modified version of the rule (with your side condition) is sound. The following derivation should be a possible instance of your modified version of the rule: (AssFwAx) {y > 0} x := y {x = y ∧ y > 0} (c) Prove that your modified rule is sound. That is, prove that for all P, x, e, if your side condition (chosen in part (b)) holds for P, x, e, then: ∀σ, σ′. BJP Kσ = tt∧ ⊢ ⟨x := e, σ⟩ → σ′ =⇒ BJx = e ∧ P Kσ′ = tt You can use the following lemmas (without proving them) in your proof: Lemma 1: ∀σ, x, v, P. BJP Kσ = tt ∧ x ̸∈ F V (P ) =⇒ BJP K(σ[x ↦→ v]) = tt Lemma 1: ∀σ, x, e, v. x ̸∈ F V (E) =⇒ AJeKσ = AJeK(σ[x ↦→ v]) 60 3.5 Solutions 3.5.1 Solutions for Hands-On 1 Hands-On 1.1 Disclaimer: This solution is copied without change from the VIS community solutions. It was originally written by Max Mathys and can be accessed under https://exams.vis.ethz.ch/exams/dhm62tei. pdf#5d4a94bb5731fc000f39f88c We use the abbreviations [v1, v2] := σ[x ↦→ v1][y ↦→ v2] and sbody ≡ x := x + 1; y := y − 1 s ≡ while x < y do sbody end AssN S ⟨x := x + 1, [2, 3]⟩ → [3, 3] AssN S ⟨y := y − 1, [3, 3]⟩ → [3, 2] SeqN S ⟨sbody, [2, 3]⟩ → [3, 2] WhFN S ⟨s, [3, 2]⟩ → [3, 2] WhTN S ⟨s, [2, 3]⟩ → [3, 2] Side condition for WhTN S : BJx < yK[2, 3] = tt Side condition for WhFN S : BJx < yK[3, 2] = ff Hands-On 1.2 (a) The refinement can be defined as follows: wr(skip) = ∅ wr(x := e) = {x} wr(s1; s2) = wr(s1) ∪ wr(s2) wr(if b then s1 else s2 end) = wr(s1) ∪ wr(s2) wr(while b do s end) = wr(s) (b) Disclaimer: This solution is copied without change from the VIS community solutions. It was originally written by Manuel H¨assig and can be accessed under https://exams.vis.ethz.ch/exams/dhm62tei.pdf# 5d5697c6727a21001085df2a We define the predicate P (T ) :≡ ∀s, σ, σ′, x. (root(T ) = ⟨s, σ⟩ → σ′ ∧ x /∈ wr(s) =⇒ σ(x) = σ′(x) and prove ∀T. P (T ) by strong structural induction overt the Natural Semantics derivation tree T using the induction hypothesis ∀T ′ < T. P (T ′), where < is the ”is a subtree of” relation. Let s, σ, σ′, x be arbitrary and assume root(T ) = ⟨s, σ⟩ → σ′ and x /∈ wr(s). We distinguish the cases over the last rule applied in T . • Case SKIPNS: If SKIPNS is the last rule applied, T must have the form ⟨s, σ⟩ → σ′ SKIPNS, where σ = σ′. Therefore, we have σ(x) = σ′(x). • Case ASSNS: If ASSNS is the last rule applied, T must have the form ⟨s, σ⟩ → σ′ ASSNS, where s = y:=e for some variable yˆ≡x and some arithmetic expression e and σ′ = σ[y ↦→ A[[e]]σ]. Since x is not modified we have σ(x) = σ′(x). 61 • Case WHFNS: If WHFNS is the last rule applied, T must have the form ⟨s, σ⟩ → σ′ WHFNS, where s = while b do s ′ end for some Boolean expression b with B[[b]]σ = f f and some statement s ′ and σ = σ′. Therefore, we have σ(x) = σ′(x). • Case SEQNS: If SEQNS is the last rule applied, T must have the form T1 ⟨s1, σ⟩ → σ′′ T2 ⟨s2, σ′′⟩ → σ′ ⟨s, σ⟩ → σ′ SEQNS, where s = s1; s1 for some statements s1, s2, some state σ′′ and T1, T2 < T . From ws(s) = ws(s1; s2) = ws(s1) ∪ ws(s2) it follows x /∈ ws(s1) an thus, by instantiating the quantified s, σ, σ′, x with s1, σ, σ′′, x respectively, we can apply the induction hypothesis P (T1) and get σ(x) = σ′′(x). From ws(s) = ws(s1; s2) = ws(s1) ∪ ws(s2) it follows x /∈ ws(s2) an thus, by instantiating the quantified s, σ, σ′, x with s2, σ′′, σ′, x respectively, we can apply the induction hypothesis P (T2) and get σ′′(x) = σ′(x). By the transitivity of equality we get σ(x) = σ′(x). • Case IFTNS: If IFTNS is the last rule applied, T must have the form T ′ ⟨s1, σ⟩ → σ′ ⟨s, σ⟩ → σ′ IFTNS, where s = if b then s1 else s2 end for some Boolean expression b with B[[b]]σ = tt and some statements s1, s2, and T ′ < T . From ws(s) = ws(s1) ∪ ws(s2) it follows that x /∈ ws(s1) and thus, by instantiating the quantified s, σ, σ′, x with s1, σ, σ′, x respectively, we can apply the induction hypothesis P (T ′) and get σ(x) = σ′(x). • Case IFFNS: If IFFNS is the last rule applied, T must have the form T ′ ⟨s2, σ⟩ → σ′ ⟨s, σ⟩ → σ′ IFFNS, where s = if b then s1 else s2 end for some Boolean expression b with B[[b]]σ = f f and some statements s1, s2, and T ′ < T . From ws(s) = ws(s1) ∪ ws(s2) it follows that x /∈ ws(s2) and thus, by instantiating the quantified s, σ, σ′, x with s2, σ, σ′, x respectively, we can apply the induction hypothesis P (T ′) and get σ(x) = σ′(x). • Case WHTNS: If WHTNS is the last rule applied, T must have the form T1 ⟨s′, σ⟩ → σ′′ T2 ⟨s, σ′′⟩ → σ′ ⟨s, σ⟩ → σ′ WHTNS, where s = while b do s ′ end for some Boolean expression b with B[[b]]σ = tt and some statement s ′, and T1, T2 < T . From ws(s) = ws(s ′) it follows that x /∈ ws(s ′) and thus, by instantiating the quantified s, σ, σ′, x with s ′, σ, σ′′, x respectively, we can apply the induc- tion hypothesis P (T1) and get σ(x) = σ′′(x). By instantiating the quantified s, σ, σ′, x with s, σ′′, σ′, x respectively, we can apply the induction hypothesis P (T2) and get σ′′(x) = σ′(x). By the transitivity of equality we get σ(x) = σ′(x). □ Hands-On 1.3 Disclaimer: This solution is copied without change from the VIS community solutions. It was originally written by Manuel H¨assig and can be accessed under https://exams.vis.ethz.ch/exams/jvkpag10. pdf#5d5572055731fc00124fa5cb We define the predicate P (T ) :≡ ∀b, s, σ, σ′. root(T ) = ⟨while b do s end, σ⟩ → σ′ =⇒ B[[b]]σ′ = f f 62 and prove ∀T. P (T ) by strong structural induction over the shape of the derivation tree T using the induction hypothesis ∀T ′ < T. P (T ′), where < is the ”is a subtree of” relation. Let b, s, σ, σ′ be arbitrary and assume root(T ) = ⟨while b do s end, σ⟩ → σ′. We distinguish two cases over the last rule applied in T. • Case WHFNS: If WHFNS is the last rule applied, T must have the form ⟨while b do s end, σ⟩ → σ′ WHFNS where σ = σ′. From the side condition of WHFNS we learn that B[[b]]σ = B[[b]]σ′ = f f , which proves this case. • Case WHTNS: If WHTNS is the last rule applied, T must have the form T1 ⟨s, σ⟩ → σ′′ T2 ⟨while b do s end, σ′′⟩ → σ′ ⟨while b do s end, σ⟩ → σ′ WHTNS where T1, T2 < T and σ′′ is some state. By instantiating the quantified b, s, σ, σ′ with b, s, σ′′, σ′ respectively we can apply the induction hypothesis for T2 as T2 < T and find that B[[b]]σ′ = f f , which concludes this case. With all possible cases for last rules proven, we have proven the claim. □ Hands-On 1.4 (a) We can extend the state by keeping the output log as a list of integers: State ′ = State × [Int] We define the initial state τzero = (σzero, []) to have an empty list. The rules are (where v ranges over Z): ReadN S ⟨read x, τ ⟩ → τ [x ↦→ v] WriteN S ⟨write e, (σ, ls)⟩ → (σ, ls :: AJeKσ) (b) We just copy the list and change the state according to the old rule: AssN S ⟨x := e, (σ, ls)⟩ → (σ[x ↦→ AJeKσ], ls) (c) I wouldn’t be quite certain how to answer this myself as the program can terminate and it can also run forever. You should probably just write that, justify informally that the program would run forever if read always produces a positive integer. Then you should prove that termination is possible using a derivation tree like this: Let s be defined as x := 1; while x > 0 do read x end. Let s ′ be while x > 0 do read x end. We will also use the abbreviation [v] := σzero[x ↦→ v]. AssN S ⟨x := 1, ([0], [])⟩ → ([1], []) ReadN S ⟨read x, ([1], [])⟩ → ([0], []) WhFN S ⟨s ′, ([0], [])⟩ → ([0], []) WhTN S ⟨s ′, ([1], [])⟩ → ([0], []) SeqN S ⟨s, ([0], [])⟩ → ([0], []) The relevant side conditions are: Side condition for WhTN S : BJx > 0K[1] = tt Side condition for WhFN S : BJx > 0K[0] = ff 3.5.2 Solutions for Hands-On 2 Hands-On 2.1 We will use these abbreviations: [v1, v2] := σ[x ↦→ v1][y ↦→ v2], the statement s ′′ is if x < y then x := x + 1 else skip end 63 and the statement s ′ is y := y - 1; s’’. The derivation sequence is: ⟨s, [0, 2]⟩ →1⟨if x < y then s’; s else skip end, [0, 2]⟩ WhileSOS →1⟨s’; s, [0, 2]⟩ IfTSOS →1⟨s’’; s, [0, 1]⟩ Seq2SOS, Seq1SOS, AssSOS →1⟨x := x + 1; s, [0, 1]⟩ Seq2SOS, IfTSOS →1⟨s, [1, 1]⟩ Seq1SOS, AssSOS →1⟨if x < y then s’; s else skip end, [1, 1]⟩ WhileSOS →1⟨skip, [1, 1]⟩ IfFSOS →1[1, 1] SkipSOS Rules that would justify the step are on the side, the left-most rule is the last rule in the tree. Hands-On 2.2 Disclaimer: This solution is copied without change from the VIS community solutions. It was originally written by Sebastian Haslebacher and can be accessed under https://exams.vis.ethz.ch/exams/ y05fwfbb.pdf#5d26299ef86271000eacd560 P (n) ≡ ∀σ.σ(x) = 0 =⇒ ⟨s, σ⟩ → ∗ 1 ⟨s, σ[y ↦→ σ(y) + n][x ↦→ 1]⟩. to prove: ∀n.P (n). By strong natural induction over n: Fix n and assume that ∀n′ < n.P (n′) (IH). Now prove P (n). Let σ be arbitrary and assume σ(x) = 0. By case distinction over n: Case n = 0: ⟨s, σ⟩ →1 ⟨if x = 0 then y := y + 1 [] x := 1; s, σ⟩ →1 ⟨y := y + 1 [] x := 1;s, σ⟩ →1 ⟨x := 1; s, σ⟩ →1 ⟨s, σ[x ↦→ 1]⟩ Using Lemma 1 we have ⟨s, σ[x ↦→ 1]⟩ = ⟨s, σ[y ↦→ σ(y)][x ↦→ 1]⟩ = ⟨s, σ[y ↦→ σ(y) + 0][x ↦→ 1]⟩. Thus ⟨s, σ⟩ → ∗ 1 ⟨s, σ[y ↦→ σ(y) + n][x ↦→ 1]⟩. Case n > 0: ⟨s, σ⟩ →1 ⟨if x = 0 then y := y + 1 [] x := 1; s, σ⟩ →1 ⟨y := y + 1 [] x := 1;s, σ⟩ →1 ⟨y:= y + 1; s, σ⟩ →1 ⟨s, σ[y ↦→ σ(y) + 1]⟩ →∗ 1 ⟨s, σ[y ↦→ σ(y) + 1][y ↦→ σ[y ↦→ σ(y) + 1](y) + (n − 1)][x ↦→ 1]⟩ In the last step we used IH for n′ = n−1 with appropriate instantiation. Note that σ[y ↦→ σ(y)+1]](x) = 0 because σ(x) = 0. We now have: ⟨s, σ[y ↦→ σ(y) + 1][y ↦→ σ[y ↦→ σ(y) + 1](y) + (n − 1)][x ↦→ 1]⟩ L2 =⟨s, σ[y ↦→ σ[y ↦→ σ(y) + 1](y) + (n − 1)][x ↦→ 1]⟩ =⟨s, σ[y ↦→ σ(y) + 1 + (n − 1)][x ↦→ 1]⟩ =⟨s, σ[y ↦→ σ(y) + n][x ↦→ 1]⟩ Therefore ⟨s, σ⟩ → ∗ 1 ⟨s, σ[y ↦→ σ(y) + n][x ↦→ 1]⟩. 64 All cases hold and the thus ∀n.P (n) holds. Hands-On 2.3 This exercise is a bit tricky. The main problem is to differentiate between nested loops and unrolled loops during the execution: Starting with some initial state σ and after some loop unrolling and branching, the first example program reaches the configuration break; while 1=1 do break end; while 1=1 do s end, σ and the second program reaches break; while 1=1 do s end; while 1=1 do break; while 1=1 do s end end, σ We expect the first program to eventually reach while 1=1 do s end, σ and the second program to eventually reach (no matter what s is) σ But from the configurations above, it’s not at all obvious how we would pick which loop to ”skip” and which to keep. One possible solution is to add an additional statement leave. We assume the programmer does not know about this feature and therefore it is not part of any statement without a rule adding it during execution. We aim to add a leave in front of every loop that we have ”seen before”, i.e. a loop that we have unrolled and entered. Thus, when we encounter a break, we skip loops and make assignments ineffectual until the next leave, after which we continue executing normally. To denote whether or not we are currently ”break-ing”, i.e. skipping loops and assignments until the next leave, we extend the state to incorporate a Boolean flag that is tt during ”break-ing”: State ′ = {tt, ff} × State The rule for break is easy: (BreakSOS) ⟨break, (q, σ)⟩ →1 (tt, σ) The adding of the leave statement (if we’re not already ”break-ing”) is handled by: (WhTSOS) BJbKσ = tt ⟨while b do s end, (ff, σ)⟩ →1 ⟨s; (leave; while b do s end), (ff, σ)⟩ And in case we already encountered a break, we skip nested loops (loops without a leave) and we can combine this rule with the case that the condition is false (because then we would also skip the loop): (WhFSOS) q = tt or BJbKσ = ff ⟨while b do s end, (q, σ)⟩ →1 ⟨(v, σ) Note that these two rules replace the WhileSOS rule. What do we do when we encounter a leave? We check the flag: If we’re ”break-ing”, we stop ”break-ing” and skip the loop, otherwise we simply remove the leave without effect. (LeaveTSOS) ⟨leave; while b do s end, (tt, σ)⟩ →1 ⟨(ff, σ)⟩ (LeaveTSOS) ⟨leave; while b do s end, (ff, σ)⟩ →1 ⟨while b do s end, (ff, σ)⟩ Finally, we should also skip assignments if we’re ”break-ing” so they don’t affect our state and we also have to define the rule for the ”normal” case again, so as not to introduce ambiguity: 65 (AssInBreakSOS) ⟨x := e, (tt, σ)⟩ →1 (tt, σ) (AssSOS) ⟨x := e, (ff, σ)⟩ →1 (ff, σ[x ↦→ AJeKσ]) And that’s it. You wouldn’t be expected to do this on an exam (hence why I reused an exercise from during the semester instead of an exam question; they’re kind of rare). But it’s a good exercise to practice thinking about custom rules in detail. Note also that technically, you would also have to redefine every other rule because we changed the state and we have to describe how the new state behaves, but for all other rules it’s simply a matter of ”modify the state like the regular rule, but keep the flag the same”. 3.5.3 Solutions for Hands-On 2 Hands-On 3.1 (a) Invariant: {(N = q × d + r) ∧ (d = D) ∧ (D > 0) ∧ (r + d ≥ 0)} Variant: r = Z The reasoning is as follows: It’s important to carry along that d = D because we need to be sure that d does not change. D > 0 is needed so we can be sure that r actually decreases. r + d ≥ 0 is needed so we can show that 0 ≤ r < D at the end (together with the negated loop condition). The variant is trivially positive at the start of the loop body because the loop condition implies it. (b) The finished proof outline is as follows: 66 Hands-On 3.2 (a) Disclaimer: This solution is copied without change from the VIS community solutions. It was originally written by Jeremy Bitar and can be accessed under https://exams.vis.ethz.ch/exams/rh0fbq8b.pdf# 605qwx4cjabbw5eh {true} x := x + 1 {true} is valid yet the following is not valid {true} guess x; assert x = x + 1 {true} because the execution cannot even terminate (b) Disclaimer: This solution is loosely based on the VIS community solutions. The original was written by Jonathan Thomm and commented on by Leonardo Galli. It can be accessed under https://exams.vis. ethz.ch/exams/rh0fbq8b.pdf#racn5x3lcrjo2ia4 Define: P (T ) := ∀P, Q, x, e. (root(T ) ≡ {P }x := e{Q} ⇒ ∃T ′ : root(T ′) ≡ {P }guess x; assert x=e{Q}) The exercise is equivalent to proving ∀T. P (T ). Proof by induction over the shape of the derivation tree T. Fix T arbitrary. Assume P(T’) for all T ′ < T . Let P, Q, x, e be arbitrary. Case distinction on the last rule in T : • Case AssAx: Then T is: AssAx {Q[x → e]} x := e {Q} We can build a proof outline as follows (where n ranges over the integers): {Q[x ↦→ e]} ⊨∗ {∃n. n = e ∧ Q[x ↦→ n]} ⊨∗∗ {∃n. (x = e ∧ Q)[x ↦→ n]} guess x; {x = e ∧ Q} assert x = e {Q} * Can be shown by picking n := AJeK ** Works by definition of substitution and uses the fact that x ̸∈ F V (e), hence e[x ↦→ n] ≡ e by the lemma. • Case ConsAx: Then T has the form: T’ {P ′} x := e {Q ′} ConsAx {P } x := e {Q} for some P ′, Q ′, T ′ such that P ⊨ P ′, Q ′ ⊨ Q. It follows by I.H. that P (T ′) holds, therefore we know that there exists a tree T ′′ such that root(T ′′′) ≡ {P ′} guess x; assert x = e {Q ′} We can build a new tree to show {P } guess x; assert x = e {Q} simply as: 67 T” {P ′} guess x; assert x = e {Q ′} ConsAx {P } guess x; assert x = e {Q} and the side conditions P ⊨ P ′, Q ′ ⊨ Q still hold as the conditions did not change. □ Note that there were two applicable rules because the Cons rule is applicable for any statement! Any proof without this rule would be considered incomplete (i.e. big point deduction). Hands-On 3.3 (a) Disclaimer: This solution is copied without change from the VIS community solutions. It was originally written by Thore G¨obel and can be accessed under https://exams.vis.ethz.ch/exams/dhm62tei.pdf# 5d494e305731fc000f39f7ee We can pick: {x = 0} x := 1 {x = 1 ∧ x = 0} (b) Disclaimer: This solution is copied without change from the VIS community solutions. It was originally written by Thore G¨obel and can be accessed under https://exams.vis.ethz.ch/exams/dhm62tei.pdf# 5d494dd35731fc000f39f7e9 We require x ̸∈ F V (P ) ∧ x ̸∈ F V (e), the first part ensures that we don’t add contradictory statements about x. The second part is required to avoid recursive statements like x = x + 1. (c) Our side condition is that x ̸∈ F V (P ) and x ̸∈ F V (e). We therefore directly prove ∀P, x, e, σ, σ′. (((x ̸∈ F V (P )) ∧ (x ̸∈ F V (e)) ∧ (B[[P ]]σ = tt) ∧(⊢ ⟨x := e, σ⟩ → σ′)) ⇒ B[[x = e ∧ P ]]σ′ = tt). Let P, x, e, σ and σ′ be arbitrary such that our LHS holds (by assumption). That is: (1) x ̸∈ F V (P ) (2) x ̸∈ F V (e) (3) B[[P ]]σ = tt (4) ⊢ ⟨x := e, σ⟩ → σ′ We now show the RHS, that is B[[x = e ∧ P ]]σ′ = tt. By the definition of B, we need to show: (i) B[[x = e]]σ′ = tt ⇔ A[[x]]σ′ = A[[e]]σ′ (ii) B[[P ]]σ′ = tt By (4) we know that there is some derivation tree T with root(T ) = ⟨x := e, σ⟩ → σ′. The last rule applied in T must be AssN S. By the form of this rule, we can deduce that σ′ = σ[x ↦→ A[[e]]σ]. We can prove (i) as: A[[x]]σ′ = σ′(x) = A[[e]]σ = A[[e]]σ′ where the first two steps hold by definition and the last step is justified by Lemma 2 and (2). In order to prove (ii), we can directly apply Lemma 1: B[[P ]]σ′ = B[[P ]]σ = tt This concludes our proof. 68 4 Model Checking Modelling is a concept used in formal verification to check if a given idea can achieve a certain property. We could imagine using Modelling to verify the correctness of a synchronization primitive like Dekker’s Algorithm or to check that some train schedule cannot result in deadlocks. We will use the modelling language Promela8 to write models, discover how we can express properties of models and we will quickly go over the guarantees that a model checker like spin9 can give us. 4.1 Promela Promela is the language we’ll use to define models this means that Promela programs aren’t meant to be executed like Java or C, instead the goal is that the possible executions of the program model something we’re interested in. So by analyzing the executions in Promela, we can learn something about the behavior of whatever we’re modelling. Promela focuses heavily on non-determinism and has a bit unconventional semantics that we’ll go over now. Non-determinism is so central to Promela because it allows us to model all possible choices that could be made during execution (destinations the train goes to, inputs the user could give, the order of some operations, etc). Data Types Data types work very similar to Java. We have the primitive types bit, byte, short and int 10. We can also declare arrays like int x [10]; and we can index into them as e.g. x[0] = 2 or y = x[0]. Pre-Process Macros Macros can be used to define constants (just like in C). The syntax is #define name definition or #define name(arg, ..., arg) definition. Just like a pre-process macro in C, all occurences of name will be replaced with definition before the code gets compiled. Methods A simple ”method” can be declared as inline myMethod(argA, argB) { code } which is really just a fancy macro. Also note that the arguments don’t have type annotations (just as in a macro). Processes We can implement concurrency using multiple processes (like threads). These are declared as proctype myThread(int argA, bit argB) { code } and these need type arguments in the definition. We can start a process using the keyword run myThread(2,0) and execution will now produce some interleaving of thread executions from this point on. Atomic Sometimes we might want to ensure that certain interleavings are impossible (e.g. process A cannot start executing before process B was created). For this we can use atomic blocks. Everything in an atomic block is executed together meaning no other process will make any execution step during the atomic block. The syntax is atomic { code }. Control Flow: If This is where the non-determinism of Promela really comes through. Any if state- ment consists of arbitrarily many branches. Each branch has a ”guard” (like in Haskell) determining whether or not that branch can be executed. The basic syntax is: if :: guard1 -> code1 :: guard2 -> code2 :: guard3 -> code3 fi The semantics are: Evaluate all guards. From all the guards that evaluate to true, non-deterministically choose which branch to execute. There are special guards: No guard is equivalent to the true guard, and the else guard evaluates to true only if all other guards evaluate to false. 8A extensive but not too technical description can be found on https://en.wikipedia.org/wiki/Promela 9https://spinroot.com/ 10bool also exists but it’s just a synonym for bit 69 So, you can see that if statements are a mix between regular if statements (which you can still write using one guard and else) and non-deterministic choice (which we could write without any guards). Control Flow: Loops Loops are very similar to if but they are executed repeatedly until a branch calls break. The syntax is: do :: guard1 -> code1 :: guard2 -> code2 :: guard3 -> code3 od Naked expressions Unlike many other languages, Promela allows for expressions to be present as statements in the code, e.g. useSomeMacro () ; x == 0; printf ( \" x ␣ is ␣ zero ! \" ) ; The semantics are that the thread will block execution until the expression evaluates to true. So be careful not to confuse = and == by accident. Init Of course, execution has to start somewhere so we always write an init { code } block that will be executed at the start of the model11. Deadlocks As we saw just now, there can be a lot of situations where Promela programs simply cannot continue execution (a naked expression stays false, an if statement has no executable branches, etc). We would call these program states stuck executions and they are a form of Deadlock. When using spin, these are called invalid end states. 11There is also the active proctype ... syntax to say that a process is running in the initial state already - but I’ve never seen it used in this course 70 4.2 Linear Temporal Logic Now we take a quick detour to talk about linear time properties and linear temporal logic. We will find out how to write LTL formulas, what they mean and how to use them in a promela program. 4.2.1 Transition Systems To start off, we define a Transition System. Let P be a set of atomic propositions (e.g. P = {p, q, r}). A transition system over P consists of the following components: • The finite set of (concrete) states Γ • The transition relation → • A fixed initial state s0 ∈ Γ • And a mapping L : Γ → P(P ). Every state gets assigned a subset of the propositions in P (e.g. L(s0) = {p, q}) and we call the result of this mapping the abstract state. This should remind you of Finite Automata. Only that here, there is no ”accept” state, no end of the computation unless it gets stuck. We will assume that: There is no state which we cannot leave using →. Given this assumption, all the ”computations” (sequences of states, beginning with s0 and following →) are infinite. Note that we do allow for self-loops, so any finite automaton could be made into such a transition system by simply adding some loops where necessary. Any computation in this transition system can be mapped to a trace. Given the computation s0, s1, s2, s1, . . . , the corresponding trace is define as L(s0), L(s1), L(s2), L(s1), . . . . So to make an computation into a trace, we simply list the abstract states instead of the concrete ones. From now on, we will mostly talk only about traces, but it’s important to remember how they are built. Notation: To get the first abstract state of a trace t, we write t[0]. For the second t[1] and so on. To get another (infinite) trace starting after the first 4 abstract states, we can write t(≥4). We call this a suffix of t and all suffixes are also traces (because they are infinite). Prefixes are not also traces because they are finite. 4.2.2 Linear Time Properties Just like formulas in FOL are either true or false depending on the structure, we can define Linear Time Properties that are satisfied or not depending on the trace. When explicitly describing properties, we use set notation, so for example F = P(P ) ω \\ {{p}}ω is a property that holds for all possible traces except the trace {p}, {p}, {p}, . . . . The ω in the exponent is like the ∞ in Discrete maths: Aω is the set of all (semi-)infinite sequences where each element comes from the set A. Make sure that the double parentheses and the power set are necessary to express the property correctly. We can now write t ∈ F to denote that the trace t satisfies the property F and t ̸∈ F for the opposite statement. To say: ”The property F holds for all possible traces in the transition system T ”, we write T ⊨ F . Note that this is not the same as saying that F is a tautology (⊨ F ) because we restrict ourselves to the transition system T and not all traces in P(P ) ω might be possible. 4.2.3 Linear Temporal Logic Because the set notation above can mess with your head really badly (and it’s not nice to express more complicated properties), we instead use Linear Temporal Logic to write formulas. The formulas express Linear Time Properties so we borrow the notation t ∈ ψ, t ̸∈ ψ and T ⊨ ψ for a formula ψ. Let’s see how we can write these formulas now. Atomic Formulas The most basic building block (like the atomic formulas A, B, C in propositional logic) are the atomic propositions in P . With for example P = {p, q, r} we can write the formula p. It is satisfied for all traces beginning with {p, . . . } i.e. p needs to be part of the first abstract state, all other states are irrelevant. 71 Logical Operators What if we want to express that both p and q need to hold in the first abstract state? If p holds in the first state then q also needs to hold in the first state? For this we can use the same operators as in propositional logic (∧, ∨, ⇒, ¬, ⇔ Until To express that the formula ψ holds in some number of suffixes until ϕ holds once, we write ψ ∪ ϕ. Formally, this new formula is satisfied if and only if there exists a k ≥ 0 such that ψ holds in t(≥0), t(≥1), . . . , t(≥k) and ϕ holds in t(≥k+1). Next To express that some formula ψ has to hold for the trace starting in the next state, we write ⃝ψ. This is true iff ψ holds for t(≥1). Derived Constructs The above syntax is enough to express everything we like but for convenience we add some syntactic sugar. • ”true” and ”false” are obvious (we could construct them as p ∨ ¬p and p ∧ ¬p resp.) • The ”eventually” operator is defined as 3ψ ≡ true ∪ ψ • The ”always” operator is defined as □ψ ≡ ¬3¬ψ And finally a quick note about precedence: Unary operators bind stronger than binary ones so 3ϕ ⇒ ψ means (3ϕ) ⇒ ψ. For everything else, use parentheses. Takeaways: • LTL formulas express LT properties • LT properties are true or false depending on the trace • Traces are always infinite in length • You need to remember that whenever we talk about ”the formula is true” we need to know which trace we’re looking at - it might not only be the first abstract state that is relevant 72 Hands-On 1 1.1. Consider a transition system with two states s1, s2, where s1 is the initial state, and with transitions back and forth from s1 to s2 as well as a loop from s2 to itself. Let p be true in and only in state s2. Decide for these two LTL formulas if they hold or don’t hold for any trace in this transition system: (a) □3p (b) 3□p 1.2. Consider a transition system with three states s1, s2, s3, where s1 is the initial state. There are the following transitions: s1 → s2, s1 → s3, s2 → s3, s2 → s2 and s3 → s3. Let p be true in and only in s2. Decide for these two LTL formulas if they hold or don’t hold for any trace in this transition system: (a) ⃝p (b) □p (c) ⃝p ⇒ □p (d) 3p (e) □¬p (f) 3p ∨ □¬p Justify your answers. 1.3 (Assignment 7 from FS09 - 12 points (adjusted)). Consider the following LTL formulas over the set P = {p, q} of atomic propositions: ψ = □((3p) ∧ ⃝ ⃝ q) ϕ = ((¬p) ∪ q) ∨ □¬p (a) Give traces t, t ′ such that t ∈ ψ, t ′ ∈ ϕ (b) Which of the formulas describe safety properties? Justify your answers by using the definition. 1.4 (Assignment 8a from FS17 - 6 points). Consider the following transition system that models a vending machine. Initially (s1), the vending machine is idle. It is then possible to select a beverage (s2) or insert money (s3). The selection can be modified any number of times, and money can be requested back before selecting a beverage. Once a selection and payment are made (s4), the vending machine releases a beverage (s5) and returns to the idle state. The extra labels on each state indicate which of three atomic propositions s (selection), p (payment) and d (beverage) are true in that state. For each of the following LTL formulas, state whether or not the formula defines a property which is valid in the given transition system and justify your answer: if the property is valid then briefly explain why; if it is not valid then provide a counterexample (in the form of a computation). (a) □ ⃝ (s ∨ p) (b) ⃝ ⃝ p ⇒ 3d (c) □(p ⇒ ⃝d) (d) □(⃝d ⇒ p) 73 Hands-On 1 1.5 (Assignment 10a from FS15 - 3 points). Consider the following graph that depicts a transition system. The extra labels on each state indicate which of three atomic propositions p, q and r are true in that state. For each of the following LTL formulas, state whether or not the formula defines a property which is valid in the given transition system and justify your answer : if the property is valid then briefly explain why; if it is not valid then provide a counterexample. (a) 3p (b) 3p ∨ 3q (c) □3r (d) (⃝q) =⇒ □¬p 4.2.4 Safety and Liveness We’ve seen LTL formulas and LT properties above. We’ve also talked about transition systems. We’ll now define two common kinds of properties that are independent of the transition system. So you don’t need a transition system at all and P is implicitly defined as the set of all atomic propositions in a formula. We’ll also show you how to do proofs about them - don’t worry it’s not that bad. Safety Properties ψ is a Safety Property if and only if for each trace t violating ψ, there exists a finite ”bad prefix” t′ ∈ P(P )∗. We call t′ ∈ P(P ) ∗ a bad prefix if and only if for any trace t′′ ∈ P(P )ω, the concatenation t′t′′ does not satisfy ψ. So, intuitively: Safety Properties are broken only because of bad prefixes and these prefixes is ”unfixable”. We talk about ”violations in finite time”. To prove that a formula is a safety property, prove that for any violation you can construct a bad prefix: Let t ̸∈ ψ be arbitrary . [ define t ′ ∈ P(P ) ∗ as a prefix of t] Let t′′ ∈ P(P ) ω be an arbitrary trace . [ prove that t ′t′′ ̸∈ ψ] To prove that a formula is not a safety property, prove that there is a violation without a bad prefix: [ define some t ̸∈ ψ] Let t′ ∈ P(P ) ∗ be an arbitrary prefix of t. [ construct t′′ ∈ P(P )ω ] [ prove that t ′t′′ ∈ ψ] 74 Liveness Properties ψ is a Liveness Property if and only if for each finite sequence t ′ ∈ P(P ) ∗, there is a trace t′′ ∈ P(P )ω such that the concatenation t′t′′ satisfies ψ. Intuitively: This means that there are no bad prefixes. We talk about ”violations in infinite time”. To prove that a formula is a liveness property, prove that no sequence is a bad prefix: Let t′ ∈ P(P ) ∗ be arbitrary . [ construct t′′ ∈ P(P )ω ] [ prove that t ′t′′ ∈ ψ] To prove that a formula is not a liveness property, prove that there is a bad prefix: [ define some t′ ∈ P(P ) ∗ ] Let t′′ ∈ P(P ) ω be an arbitrary trace . [ prove that t ′t′′ ̸∈ ψ] Hands-On 2 2.1 (Assignment 8bc from FS17 - 4 points). This exercise is the continuation of Assignment 8a that appeared in Hands-On 1.4 but because these questions are independent of the transition system, you don’t need to refer back to the first part. (a) Is the formula □ ⃝ (s ∨ p) a safety property? If yes, describe the bad prefixes of the property. If not, explain why. (b) Does the formula ⃝⃝p ⇒ 3d describe a safety property, a liveness property, neither, or both? Justify your answer. 2.2 (Assignment 10b from FS15 - 3 points). This exercise is the continuation of Assignment 10a that appeared in Hands-On 1.5 but because these questions are independent of the transition system, you don’t need to refer back to the first part. Consider the LTL formula □(p′ =⇒ ⃝□¬p′) (in which p ′ is an atomic proposition). Does this formula define a safety or a liveness property? Justify your answer (a clear explanation is enough; you do not need to provide a formal proof). 75 4.3 Model Checking Now equipped with our knowledge about LTL, let’s revisit Promela and spin and conclude this section. Spin is a model checker, meaning that its job is to find out whether or not our model satisfies some formal specification. So how do we specify what the model is supposed to do and not supposed to do? We can use assertions or LTL formulas. An assertion is part of the Promela syntax and written as assert( Bexp ). Writing such an assertion means that ”at this point, this expression must be true”. LTL formulas are written as a global block: ltl { formula }. The operators are written as [], <>, U , ! for always, eventually, until and not respectively. We can also use true and false. Writing an LTL formula means ”for any trace this model allows, this formula must be satisfied”. Now spin comes along and analyzes your model (command line option -a). Spin will consider every possible execution your model allows: all interleavings, all non-deterministic choices, etc. While spin is smart about its resource usage, this is still a huge task and spin might not terminate. Luckily, we never consider this case in this course - you can assume spin always terminates. When spin terminates, it will either have verified the model or it will produce an error caused by: • The violation of an assert, • The violation of a LTL formula, or • A deadlock (”invalid end state”) The Power of Assertions Now, using an LTL formula to verify a model is very powerful - you can think of it a bit like ”cheat mode”. So in order to ensure that you ”git gud”, exams will often ask you to only work with assertions. Common strategies are: • If you need to check whether a point in the program is reachable, add assert(false) - a violation means that it can be reached. • If you need to check whether a certain value of a variable is possible, add corresponding assertions either throughout the program, in the main loop (assuming the variable can only be updated once per iteration) or you can create another process that consists only of the assertion. Because spin enumerates all interleavings, this is the same as checking the assertion at every point. • If you need to check whether X is possible after Y, add a global bit to track Y and check for that when X happens. Important: ”violating an assertion” is not necessarily the same as saying ”the statement is false”. You need to really think through what spin output you would expect if the statement were true. As an example, we would check for ”this line is reachable” using assert(false) but a violation means that it was reachable, so the statement is true. Also: It’s impossible to check liveness properties this way because initial ”violations” can be ”fixed” later. But a single failed assertion leads to a violation for the entire model and the assertion has to decide whether or not to fail given only the execution up to the assertion - it cannot take into account what happens afterwards. 76 Hands-On 3 3.1 (Assignment 8 from FS18 - 14 points). The diagram below shows a train track, consisting of a main loop (on the right), which can be travelled in either direction, and a one-way loop on the left (used for turning the train around). On the far right side, there is a single end-point for the train, from which it cannot travel further; the train only moves forwards. The numbered arrows in the diagram indicate particular positions (and directions) of the train; we will use these to define the states of a model of the system above. The train is initially in state 0. (a) Consider the following incomplete Promela model for the train system: byte pos = 0; /* train position */ init { do :: pos >= 0 && pos <= 5 -> pos = pos + 1; :: pos == 7 -> break ; /* end point */ od } Add appropriate cases to the do loop, to complete the modelling of the system in the diagram. (b) Promela assert statements can be used to check certain properties of the model. When we use a model checker (such as Spin) to check for assertion violations, is the property we are checking of the model a safety property, a liveness property, neither or both? Briefly explain your answer. (c) For each of the following four statements, answer the following questions: can you use a model checker (such as Spin) to verify whether or not this property is true of the train system by modifying the code of the Promela model and checking for assertion violations? If your answer is ”yes”, describe briefly here what you would add or change in the code of the model, and why your approach works. If your answer is ”no”, briefly explain why the property cannot be checked in such a way. (a) ”The train will never reach position 6.” (b) ”The train is guaranteed to eventually reach position 7.” (c) ”It is possible for the train to reach position 7.” (d) ”The train cannot reach position 7 without first visiting position 3 (the left-hand loop).” 77 Hands-On 3 3.2 (Assignment 8 from FS20 - 8 points). A farmer went to the market and is on the way back home, with him he has a fox, a chicken, and some grain. He has to cross a river using a small boat only big enough for him and one other item. If the fox is left alone with the chicken the fox will eat it. If the chicken is left alone with the grain the chicken will eat it. Neither the fox, the chicken, nor the grain can cross the river without the farmer. The goal of this assignment is to model the aforementioned scenario in Promela and use a model checker to find out whether it is possible for the farmer to make it home with all three of his items. Consider the following partial Promela model: bool success = 0 /* success flag */ bool failure = 0 /* failure flag */ inline step () { /* implement me (1) */ } inline check () { /* implement me (2) */ } init { do :: failure -> /* (3) */ :: success -> /* (4) */ :: else -> step () ; check () od } (a) Provide appropriate implementations for the step() and check() macros. You may introduce additional variables and macros to do so. • The step() macro should model all possible steps the farmer can perform, i.e., cross the river with an item or cross the river on his own. Your model should reflect the positions of the farmer and the three items. Briefly explain how you model these positions. • The check() macro should set the success flag to 1 if the farmer made it to the other side of the river with all three items. Moreover, it should set the failure flag to 1 if the fox is left unattended with the chicken or the chicken is left unattended with the grain. (b) Can we use a model checker (such as Spin) to verify whether there is a strategy for the farmer to make it home with all three items? If your answer is ”yes”, briefly explain the necessary additions or changes to the code of the Promela model, and why your approach works. If your answer is ”no”, briefly explain why it is not possible. Note: You may solve this task without having solved Task (a) by assuming appropriate im- plementations for the step() and check() macros. 78 Hands-On 3 3.3 (Assignment 8 from HS21 - 3 points). For your birthday, your friends have prepared a delicious cake. On the cake are five lit candles in a circle, and your mission is to extinguish them all, by blowing on them. Blowing on a lit candle effectively extinguishes this candle, but it also affects the two neighbouring candles: Any neighbouring candle that was lit before becomes extinguished, and any neighbouring candle that was extinguished before relights. Put another way, the candles follow these special rules: • You can only blow on a lit candle. • Blowing on a lit candle extinguishes it. • The two neighbouring candles switch states: If a neighbouring candle was unlit, then it becomes lit, and vice-versa. The goal of this assignment is to use a model checker to find out whether it is possible to extinguish all candles. And if it is possible, to find out the minimum number of times you have to blow to extinguish all candles. Consider the following partial Promela model: # define n 5 bool success = 0 inline blow () { /* implement me in Task 8. A */ } init { do :: success -> /* implement me in Task 8. B */ :: else -> blow () od } Assume an appropriate implementation of the blow() macro that should model a blow on one candle, and then set the success flag to 1 if all candles are extinguished after the blow. (a) Can we use a model checker (such as Spin) to verify whether there is a way to extinguish all the candles? If your answer is ”yes”, briefly explain the necessary additions or changes to the code of the Promela model, and why your approach works. If your answer is ”no”, briefly explain why it is not possible. (b) Assume that there is a way to extinguish all the candles. We would like to find a strategy (potentially invoking the model checker multiple times) that minimizes the number of steps, that is the number of times we have to blow on a candle. Can we use a model checker (such as Spin) to find if extinguishing all candles is possible in at most k steps? If your answer is ”yes”, briefly explain your approach and how you would modify the code of the Promela model. If your answer is ”no”, write ”no” and briefly explain why. 79 4.4 Solutions 4.4.1 Solutions for Hands-On 1 Hands-On 1.1 (a) □3p holds (no matter where in the trace we start, p holds for some abstract state in the future) (b) 3□p does not hold (consider the trace s1, s2, s1, s2, . . . ) Both of these properties imply that p holds infinitely often, but the second one additionally requires that p never be false again after some point. The first formula is however a logical consequence of the second. Hands-On 1.2 (a) ⃝p does not hold. A counterexample is the computation s1, s3, s3, s3, . . . . (b) □p also does not hold with the same trace as a counterexample. (c) ⃝p ⇒ □p is also false. A counterexample is the computation s1, s2, s3, s3, . . . . Notice that even though this looks like ”the left side of the implication is false”, this reasoning only works for a single trace at a time: If some trace does not satisfy ⃝p, then the formula is true. But only because ⃝p is not true for all traces - see a) - does not mean that all traces violate ⃝p (which would be needed to claim that because of this the entire implication is true for all traces). (d) 3p does not hold. A counterexample is the computation s1, s3, s3, s3, . . . . (e) □¬p does not hold. A counterexample is the computation s1, s2, s3, s3, . . . . (f) 3p ∨ □¬p holds! Even though this looks like ”both are false so the disjunction is false”, that again only works for a single trace. And because there is no trace that is simultaneously a counterexample for both formulas, this holds (also in general for any transition system). Hands-On 1.3 (a) t := {p, q}, {p, q}, {p, q}, · · · ∈ ψ, t′ := {}, {}, · · · ∈ ϕ (b) ψ is not a safety property because the trace {}, {}, {q}, {q}, {q}, . . . violates it, yet any prefix of this trace can be made to satisfy ψ by appending infinitely many copies of {p, q} to the end. Hence, there is no bad prefix. ϕ is a safety property because any trace that violates it, must have an abstract state in which p holds and ¬p ∪ q must also be false. Therefore, p must be true in an abstract state before q is first true. This means that a finite sequence consisting only of zero or more {} and then {p} is a prefix of the violating trace. This is also a bad prefix because no matter what abstract states we append, both sub-formulas are violated and cannot be fixed. Hands-On 1.4 (a) □ ⃝ (s ∨ p) does not hold. A counterexample would be s1, s3, s1, s3, . . . . (b) ⃝ ⃝ p ⇒ 3d holds because if we start off the computation in way such that p holds in the third abstract state, then we must have the prefix s1, s2, s4. The only next possible state is s5, so clearly 3d is also true in this trace. (c) □(p ⇒ ⃝d) does not hold. A counterexample would be s1, s3, s1, s3, . . . . (d) □(⃝d ⇒ p) holds because whenever, at any point during the trace, ⃝d holds we must be in s4. And indeed p holds in s4, so the property is valid. Hands-On 1.5 (a) 3p is invalid. Counter example: s1, s4, s3, s4, s3, s4, s3, s4, s3, ... 80 (b) 3p ∨ 3q is valid. Starting at the initial state s1, the next state will be either s2 or s4. This implies that in the second state at least one of q or p holds and makes the formula true for all traces beginning at the initial state. (c) □3r is valid. There is no loop of states that doesn’t include s3. (d) (⃝q) =⇒ □¬p is valid. If ⃝q holds the next state in the trace must be s4, from there s2 is unreachable. 4.4.2 Solutions for Hands-On 2 Hands-On 2.1 (a) □ ⃝ (s ∨ p) is a safety property. Informally, it’s the same as saying ”in every state after the first one, one or both of s and p need to hold”. Whenever the property is violated, there is some state after the first one where neither s nor p hold. The trace up to and including that point is a bad prefix. We can describe all bad prefixes as: the first state is arbitrary, then follow zero or more states in which s or p hold and then there is a state in which neither hold. (b) ⃝ ⃝ p ⇒ 3d is a liveness property. Any finite sequence of abstract states either satisfies ⃝ ⃝ p in which case we can ”fix” the property by extending the sequence with copies of {d} - or it does not satisfy the left side of the implication in which case the property is satisfied with any extension of the sequence. It is not a safety property because only the property that is true for all traces ({P } ω) is both a safety and a liveness property. Hands-On 2.2 It is a safety property. In any trace t violating the formula, there must be a finite prefix t′ where p ′ holds in at least two different states, any infinite trace with prefix t′ will violate the property, hence t′ is a bad prefix and because t was arbitrary, this is a safety property. 4.4.3 Solutions for Hands-On 3 Hands-On 3.1 (a) The model is complete if we add the remaining transitions 2 → 0, 5 → 7 and 6 → 4. byte pos = 0; /* train position */ init { do :: pos >= 0 && pos <= 5 -> pos = pos + 1; :: pos == 7 -> break ; /* end point */ :: pos == 2 -> pos = 0 :: pos == 5 -> pos = 7 :: pos == 6 -> pos = 4 od } (b) Disclaimer: This solution is paraphrased from the VIS community solutions. It was originally written by Anna Maria Eggler and can be accessed under https://exams.vis.ethz.ch/exams/3fiscy9n.pdf# 5d08ba468fc5530013fa18b7 Safety property. We can trap violations during verification. We check that a ”bad thing” did not happen up to that point. If that is violated, the assertion fails. This is exactly the kind of ”bad prefix”-behavior captured by safety properties. Whereas for a liveness property we would need to know what happens afterwards in order to state that the property is violated. (c) Disclaimer: This solutions are copied without change from the VIS community solutions. They were originally written by Christian Ulmann, Anna Maria Eggler, Marc Widmer, Fabian Bossard and Nicolas Kopp. The solutions can be accessed under https://exams.vis.ethz.ch/exams/3fiscy9n.pdf 81 (a) An easy way to test this, is to change the line :: pos == 6 -> pos = 4 to :: pos == 6 -> assert(false) So if position 6 is reachable, an assertion will be violated. (b) This is clearly a liveness property and cannot be checked. There is no bad prefix to this e.g. if state=... then the assertion is violated (as it can still reach position 7 at some later point in the future)12 (c) Replace the line :: pos == 5 -> pos = 7; with :: pos == 5 -> assert(false); pos = 7;. When this assertion is violated (at least once) the statement actually holds and when it is never violated, the statement doesn’t hold. (d) We add :: pos == 3 -> break; and change :: pos == 7 -> break; to :: pos == 7 -> assert(false); as well as change :: pos >= 0 && pos <= 5 -> pos = pos + 1; to :: pos >= 0 && pos <= 2 -> pos = pos + 1; :: pos >= 4 && pos <= 5 -> pos = pos + 1; This way, we can’t continue after visiting 3. If the assertion does not get triggered then there is no way to reach 7 without first visiting 3. Hands-On 3.2 (a) Disclaimer: This solution is copied without change from the VIS community solutions. It was originally written by Jonas Maier and can be accessed under https://exams.vis.ethz.ch/exams/ghigz2p5.pdf# 12uqrd539cx6dfmq An appropriate implementation is the following: bool farmer bool grain bool chicken bool fox inline move ( item ) { item == farmer -> item = ! item } inline step () { if :: move ( grain ) :: move ( chicken ) :: move ( fox ) :: true /* or don ’ t move anything */ fi farmer = ! farmer } inline checkAlone (a , b ) { if :: a == b && a != farmer -> failure = true :: else fi } inline check () { checkAlone ( chicken , grain ) 12spin can still check liveness properties but not only using assertions 82 checkAlone ( chicken , fox ) if :: grain && chicken && fox -> success = true :: else fi } (b) Disclaimer: This solution is copied without change from the VIS community solutions. It was originally written by Robin H¨anni and can be accessed under https://exams.vis.ethz.ch/exams/ghigz2p5.pdf# c9qcq8y0ao8uci9y We can simply add an assert(false) at location (4) and a break at location (3). As the model checker will find all possible traces, it will find one that triggers the assertion violation in location (4) iff there is a winning strategy. Hands-On 3.3 (a) Disclaimer: This solution is copied without change from the VIS community solutions. It was originally written by Alessandro Abbate and can be accessed under https://exams.vis.ethz.ch/exams/kq31s35t. pdf#3nr2q8ymms244j8u Yes, it is. We add :: success -> assert(false)”. The only way the execution of the model fails is, if the all candles are unlit; therefore, we expect an assertion violation if and only if there is a strategy. (b) Disclaimer: This solution is copied without change from the VIS community solutions. It was originally written by Alessandro Abbate and can be accessed under https://exams.vis.ethz.ch/exams/kq31s35t. pdf#ll3kdodp2wft2z8a Yes, it is possible. We add a counter= 0; Before each blow we check if counter ¡ k holds. If yes we continue normally and increment the counter by 1. If not (counter == k), we add a ”break”. Therefore, there can be max k blows. 83 Appendices A Mini-Haskell Typing Rules B First Order Logic Rules 84 C Hierarchy of FOL Rules Credit for the following classification goes to Jonas Fiala (disclaimer: please don’t take this too seriously). Tier Rule Notes S Γ, A ⊢ A Axiom Nicknamed “Ax”, this rule should always be played if possible. A Γ, A ⊢ B Γ ⊢ A → B → -I Harmless since it can be reversed with → -E. To be applied eagerly. Γ, A ⊢ ⊥ Γ ⊢ ¬A ¬-I Same as above if one assumes ¬A ≡ A → ⊥. Γ ⊢ A Γ ⊢ B Γ ⊢ A ∧ B ∧ -I Can’t go wrong with this rule, but consider applying B tier rules beforehand to avoid duplication. Γ ⊢ A Γ ⊢ ∀x. A ∀-I* Hard counters ∀, but only when used correctly. The logician must state and check that “x is not free in Γ”; else use α-renaming as needed. Beware of (∀x. A) → B! B Γ ⊢ A → B Γ ⊢ A Γ ⊢ B → -E Situational rule; only powerful when combined with an appropriate Γ. Γ ⊢ ¬A Γ ⊢ A Γ ⊢ B ¬-E Equivalent to the combo ⊥-E then → -E, this rule is strictly better than ⊥-E (given the current roster of rules). Γ ⊢ A ∨ B Γ, A ⊢ C Γ, B ⊢ C Γ ⊢ C ∨ -E If one can show Γ ⊢ A ∨ B then this becomes A tier, otherwise consider passing on this rule. Beware that ∃x. A ∨ B ̸= (∃x. A) ∨ B Γ ⊢ ∃x. A Γ, A ⊢ B Γ ⊢ B ∃-E** Another situational rule, requiring a suitable Γ. Must state and check “x is not free in Γ or B” 1 (α-rename as needed). C Γ ⊢ ⊥ Γ ⊢ A ⊥-E Not viable in the current meta and should be avoided. Γ ⊢ A ∧ B Γ ⊢ A ∧ -EL Niche rule, generally only used as part of the finishing combo. Γ ⊢ A ∧ B Γ ⊢ B ∧ -ER Same as above. Γ ⊢ A Γ ⊢ A ∨ B ∨ -IL Same as above. Not to be played in combination with ∨-E. Γ ⊢ B Γ ⊢ A ∨ B ∨ -IR Same as above. Γ ⊢ ∀x. A Γ ⊢ A[x ↦→ t] ∀-E Can be strong due to the arbitrary choice of t, though mostly only used as a finisher (e.g. with [x ↦→ x]). To be combined with ∃-I only in extreme cases. Γ ⊢ A[x ↦→ t] Γ ⊢ ∃x. A ∃-I Weak rule in disguise due to its introductory nature; only apply with a well chosen witness t to guarantee victory. 1Substituting in the relevant B (and Γ if not defined)! 85 D IMP Syntax E Big-Step Semantics Rules F Small-Step Semantics Rules 86 G Axiomatic Semantics Rules Rules are as for partial correctness above, except for the following rule, which replaces WhAx where Z is a fresh logical variable (not used in P). 87","libVersion":"0.5.0","langs":""}