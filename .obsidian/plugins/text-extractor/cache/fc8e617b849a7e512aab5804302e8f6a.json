{"path":"sem2a/DDCA/PV/summaries/DDCA-FS19-summary-yamuell.pdf","text":"Yannick M¨uller muelleya student.ethz.ch yajm.ch Computer Science Basispr¨ufung - Block 2 August 2019 Design of Digital Circuits - Lecture Prof. Onur Mutlu Optional Homework is very important. All material we covered in lectures and the labs can be part of the exam. Contents 1 Basics 4 1.1 FPGA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 2 Combinational Logic Design 4 2.1 MOS Transistors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 2.2 Logic Gates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 2.3 Combinational Building Blocks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 2.4 Tri-State Buﬀer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 2.5 Karnaugh Maps (K-maps) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 3 Sequential Logic Design 6 3.1 Storage Elements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 3.2 Finite State Machines (FSM) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 4 Hardware Description Languages and Verilog 7 5 Timing and Veriﬁcation 7 6 The Von Neumann Model 8 6.1 Dataﬂow Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9 7 Instruction Set Architecture 9 7.1 Control Flow Instructions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10 8 Microarchitecture 10 8.1 Architectural State . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10 8.2 Multi-Cycle Design . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11 8.3 Instruction Processing Cycle . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11 9 Pipelining 11 9.1 Five fundamental ways of handling ﬂow dependencies . . . . . . . . . . . . . . . . . 11 10 Reorder Buﬀer 12 11 Out-of-Order Execution 12 12 Superscalar Execution 13 13 Branch Prediction 13 13.1 Static Branch Prediction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13 13.2 Dynamic Branch Prediction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13 13.3 Two Bit Branch Prediction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14 13.4 Global Branch Correlation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14 13.5 Local Branch Correlation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14 13.6 Perceptron Branch Predictor . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14 14 VLIW 14 15 Systolic Arrays 14 15.1 Pipeline-Parallel Programs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 15.2 Decoupled Access / Execute (DAE) . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 15.3 Loop Unrolling to eliminate branches . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 16 SIMD Processors 15 16.1 Gather/Scatter . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16 16.2 Masked Vector Instructions - Density Time Implementation . . . . . . . . . . . . . 16 17 Graphics Processing Units 16 17.1 Fine-Grained Multithreading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16 2 17.1.1 Dynamic Warp Formation/Merging . . . . . . . . . . . . . . . . . . . . . . . 16 18 Memory Organization and Memory Technology 17 18.1 Memory Bank Organization and Operation . . . . . . . . . . . . . . . . . . . . . . . 17 18.2 Memory Hierarchy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17 18.3 Locality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 18.4 Manual or Automatic Memory Management . . . . . . . . . . . . . . . . . . . . . . 18 18.5 Recursive latency equation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 19 Caches 18 19.1 Cache access . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 19.2 Associativity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19 19.3 Handling Writes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19 19.4 Cache misses . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19 19.5 Restructuring Data Access patterns . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19 19.6 Multiprocessor Caching . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20 19.7 Cache Coherence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20 20 Virtual Memory 20 20.1 Cache / Virtual Memory Analogues . . . . . . . . . . . . . . . . . . . . . . . . . . . 20 20.2 How do we translate addresses . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 20.3 Translation Lookaside Buﬀer (TLB) . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 20.4 Page Replacements Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 20.5 Memory Protection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 20.6 Runahead Execution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22 3 Goals of this course • Understand the basics • Understand the principles of design • Understand the precedents The following was presented in the lecture on 28. February 2019. 1 Basics 1.1 FPGA FPGA = Field Programmable Gate Array FPGA is a reconﬁgurable substrate and they ﬁll the gab between software and hardware. It Achieves higher performance than software. But it also maintains more ﬂexibility than hard- ware. It consists of two main building blocks lock-up tables (LUT) and switches. The following was presented in the lecture on 01. March 2019. 2 Combinational Logic Design 2.1 MOS Transistors One gets a MOS transistor by combining conductors (Metal), Insulators (Oxide), Semiconduc- tors. There are two types of transistors: n-type Apply High Voltage to turn it on by connecting the drain and the source with the gate. (0.3V to 3V) If the gate is supplied with 0V the connection ends. This type passes 0 well but 1 poorly. p-type Apply Low Voltage to turn it on by connecting the drain and the source with the gate. the circuit is closed (turned on) by applying 0V. This type passes 1 well but 0 poorly. 2.2 Logic Gates nM OS + pM OS = CM OS (1) Digital circuit: one possible interpretation: 0V interpret it as binary 0 value 4 3V interpret it as binary 1 value https://en.wikipedia.org/wiki/Logic_gate#Symbols Series connections are slower than parallel connections, because there is more resistance. Dynamic Power Consumption: C ⋅ V 2 ⋅ f (2) C = Capacitance of the circuit V = Supply voltage f = charging frequency of the capacitor Static Power Consumption V ⋅ Ileakage (3) The following was presented in the lecture on 07. March 2019. 2.3 Combinational Building Blocks A combinational logic is often grouped into larger building blocks to build more complex sys- tems. A few of them are: Decoders It has n input and 2n outputs. Exactly one of the outputs is 1 and all the rest are 0s. The one output that is logically 1 is the output corresponding to the input pattern that the logic circuit is expected to detect. The decoder is useful in determining how to interpret a bit pattern. Multiplexers Selects one of the n inputs to connect it to the output. One achieves this with connecting all input to AND-gates and only put a one in the second and-input which needs to be selected. After that OR everything. Full adder One has a value a, b and a carry bit. One adds those three values together and if it gives an overﬂow the next carry bit is set to one. Programmable Logic Array (PLA) A PLA is a common building block for implementing any collection of logic functions. The inputs are connected to an array of AND gates followed by an array of OR gates. It is a implementation of the Sum Of Product. Therefore the number of AND gates is the number of possible minterms. Which is 2n where n is the number of input wires. The numbers of OR gates is the number of output columns. Any logic function can be implemented using a PLA. A PLA only consists of AND, OR gates and inverters. 2.4 Tri-State Buﬀer Tri-State Buﬀer enables gating of diﬀerent signals onto a wire. The output could also be a Floating Signal (Z) which is a signal that is not driven by any circuit. 5 2.5 Karnaugh Maps (K-maps) The goal with K-maps is to logically simplify expressions. So that we can reduce the number of gates and inputs. Note 1. Essence of Simpliﬁcation: Find two element subsets of the ON-set where only one variable changes its value. This single varying variable can be eliminated. F = A + B ⋅ D + B ⋅ C ⋅ D (4) Note 2. One can make K-maps even more simpler with don’t cares. The following was presented in the lecture on 08. March 2019. 3 Sequential Logic Design A sequential circuit consists of a Storage Element and Computational Element. Cross-Coupled Inverters consists of two inverters into each other. 3.1 Storage Elements Latches and Flip-Flops Very fast, parallel access, Very expensive (costs tens of transistors) Static RAM (SRAM) Relatively fast, only one data word at a time, expense (one bit costs 6 transistors) Dynamic RAM (DRAM) Slower, one data word at a time, reading destroys content, cheap (one bit costs only one transistor plus one capacitor) Flash Memory, Hard Disk, Tape Much slower, access takes a long time, non-volatile, very cheap (No transistors are involved) 3.2 Finite State Machines (FSM) Moore FSM outputs depends only on the current state Mealy FSM outputs depend on the current state and the inputs 6 A ﬁnite state machine consists of three separate parts: 1. next state logic 2. state register 3. output logic The following was presented in the lecture on 14. March 2019. 4 Hardware Description Languages and Verilog Deﬁnition 1. HDL = Hardware Description Language Deﬁnition 2. Floating Signal: Signal that is not driven by any circuit. Deﬁnition 3. Synchronous Reset, resets everything in respect to the clock. Asynchronous resets independently The following was presented in the lecture on 15. March 2019. 5 Timing and Veriﬁcation Delay is fundamentally caused by capacitance and resistance in a circuit. Contamination Delay Delay until the Output starts changing Propagation Delay Delay until the Output ﬁnishes changing The critical path that we need to worry about is the longest path with the most transistors, as this takes the most amount of time. Transistor delay is dependent on voltage and temperature. Glitch: One input transition causes multiple output transitions. This is caused by bad timing. tcd > thold − tccq (5) tcd Combinational Logic Delay thold Time that FF inputs must be stable after a clock edge 7 tccq Clock-To-q Delay (Contamination/Propagation) Clock Skew is the delay between the ﬁrst transistor and the last transistor sees the clock. Test bench: A module created speciﬁcally to test a design. A golden model represents the ideal circuit behavior. The following was presented in the lecture on 21. March 2019. 6 The Von Neumann Model Little Endian Least Bit is most important Big Endian First bit is most important MAR Memory Address Register MDR Memory Data Register ALU Arithmetic and Logic Unit which executes computations Registers Is in the computational unit, ensures fast access to operands GPR General purpose registers Control Unit It conducts the step-by-step process of executing every instruction in a program. IR instruction register PC program counter IP instruction pointer 8 Note 3. Von Neumann Model is the computer architecture used today Stored Program Instructions stored in a linear memory array Sequential instruction processing One instruction is processed which means fetched exe- cuted and completed at a time. Program Counter identiﬁes the current instructions and ﬁnds the next, Table 1: MIPS 0 rs rt d shamt funct 6.1 Dataﬂow Model An instruction is fetched and executed in data ﬂow order. (Von Neumann model: The instruc- tion is fetched and executed in control ﬂow order) In a dataﬂow model the nodes in the graph are the operations and the connections are values. This means the instruction gets executed as soon as it is ready, this gives a lot more parallelism. The following was presented in the lecture on 22. March .2018. 7 Instruction Set Architecture Note 4. Instruction Processing Cycle: Fetch, Decode, Evaluate Address, Fetch Operands, Execute, Store Result Deﬁnition 4. The ISA is the instruction set architecture is the interface between what the software commands and what the hardware carries out. The ISA speciﬁes: Memory organization Address space, addressability, word- or byte addressable register set 32 registers in MIPS instruction set Opcodes, data types, addressing modes The following ISA support diﬀerent data types: LC3 NOT, AND, ADD Operation MIPS R-type (and, add, xor, nor), I-type (Version of R-type), F-type (ﬂoating numbers) Note 5. An addressing mode is a mechanism for specifying where an operand is located. Four ways are: 1. PC-Relative mode 9 2. Indirect mode 3. Base+oﬀset mode (MIPS) 4. immediate modes (MIPS) Table 2: LC3 Assembly ADD R1 R4 #-2 OP DR SR imm5 Field Code 1 1 4 1 -2 Machine Code 0001 001 100 1 11110 7.1 Control Flow Instructions Control Flow instructions allow a program to execute out of sequence. It is used for in con- ditional branches and jumps. Conditional branches are used to make decisions, like the if-else statement. Jumps are used to implement loops and function calls. The following was presented in the lecture on 28. March 2019. 8 Microarchitecture Deﬁnition 5. A microarchitecture is an implementation of the ISA under speciﬁc design con- straints and goals. The microarchitecture is the level below the ISA. ISA Speciﬁes how the programmer sees the instructions to be executed. Microarchitecture How the underlying implementation actually executes instructions. This means the microarchitecture can execute instructions in any order as long as it obeys the semantics speciﬁed by the ISA. The microarchitecture is responsible for everything done in hardware without exposure to software. The following was presented in the lecture on 29. March 2019. 8.1 Architectural State AS = Architectural State at the beginning of an instruction. (Programmer visible) This AS then gets processed to AS’ AS’ = Architectural state after an instruction is processed (Programmer visible) ISA speciﬁes abstractly what AS’ should be, given an instruction and AS. Microarchitecture implements how AS is transformed to AS’ 10 8.2 Multi-Cycle Design The diﬀerent principles in multi-cycle design are Critical Path Design, Bread and Butter Design and Balanced Design. Critical path design Find and decrease the maximum combinational logic delay. Break a path into multiple cycles if it takes too long. Bread and Butter design Spend time and resources on where it matters most. Common case vs. uncommon case. Balanced design Balance instruction/data ﬂow through hardware components. Design to eliminate bottlenecks: balance the hardware for the work. The beneﬁts are higher clock frequency, simpler instructions run faster, reuse expensive hard- ware across multiple cycles The downsides are that one needs to store the intermediate results. Sequencing overhead paid many times, hardware overhead for storing intermediate results. 8.3 Instruction Processing Cycle Fetch, Decode, Evaluate Address, Fetch Operands, Execute, Store Result The following was presented in the lecture on 04. April 2019. 9 Pipelining All instructions classes must follow the same path and timing through the pipeline stages. Diﬃculties are keeping the pipeline correct, moving and full in the presence of events that dis- rupt the pipeline ﬂow. 9.1 Five fundamental ways of handling ﬂow dependencies Detect and wait until the value is available in register ﬁle Detect and forward/bypass data to dependent instruction Detect and eliminate the dependence at the software level Predict the needed values and execute speculatively and verify Do something else Like ﬁne-grained multithreading 11 The following was presented in the lecture on 05. April 2019. Deﬁnition 6. Interlocking: Detection of dependence between instructions in a pipelined pro- cessor to guarantee correct execution. Deﬁnition 7. Branch misprediction penalty: Number of instructions ﬂushed when branch is taken. The following was presented in the lecture on 11. April 2019. 10 Reorder Buﬀer Deﬁnition 8. The idea behind the reorder buﬀer is to complete instructions out-of-order, but reorder them before making results visible to architectural state Note 6. Output and anti dependencies are not true dependencies. The same register refers to values that have nothing to do with each other. They exist due to lack of register ID’s in the ISA. The register ID is renamed to the reorder buﬀer entry that will hold the register’s value. The following was presented in the lecture on 12. April 2019. 11 Out-of-Order Execution One can save future calculations in a reservation table of the operation and then it executes the operation as soon as both variables are ready. Note 7. 1. What is needed in hardware to perform tag broadcast and value capture? A lot of comparators. 2. Does the tag have to be the ID of the Reservation Station Entry? No. 3. What can potentially become the critical path? Scheduling loop. 4. When is a reservation station entry deallocated? Right after execution. 5. Should the reservation stations be dedicated to each functional unit or global across func- tional? There are trad oﬀs for both. Note 8. Reorder Buﬀer to support in-order retirement of instructions (Programming order) Note 9. 1. Current machines work with FrontEnd-Register Map and Architectural Register Map. 2. An out-of-order engine dynamically builds the dataﬂow graph. 12 3. The dataﬂow graph is limited to the instruction window. (Instruction Window: All de- coded but not yet retired instructions.) 4. Out-of-order execution tolerates the latency of multi-cycle operations by executing inde- pendent operations concurrently. 5. A load’s dependence status is not known until all previous store addresses are available. The following was presented in the lecture on 18. April 2019. 12 Superscalar Execution Idea: Fetch, decode, execute, retire multiple instructions per cycle. N-wide superscaler → N instructions per cycle. 13 Branch Prediction Idea: Predict the next fetch address Three things to be predicted: 1. Whether the fetched instruction is a branch 2. Branch direction 3. Branch target address 13.1 Static Branch Prediction Always taken 65% Accuracy Never taken 35% Accuracy BTFN Predict backward branches as taken and forward branches as not taken. Proﬁle-based Compiler determines likely direction for each branch using a proﬁle run. En- codes that direction as a hint bit in the branch instruction format. Programmer-based The programmer provides the statically-predicted direction. Deﬁnition 9. Pragmas: Keywords that enable a programmer to convey hints to lower levels of the transformation hierarchy. 13.2 Dynamic Branch Prediction Predict branches based on dynamic information. One example is the last time predictor. 13 The following was presented in the lecture on 02. May 2019. 13.3 Two Bit Branch Prediction One way to get more accurate is to have two bits instead of just one. This way one can be more conﬁdent when one wants to switch to the other prediction. One can now switch between four diﬀerent states and not just switch between two states. 13.4 Global Branch Correlation Associate branch outcomes with global Taken/NotTaken history of all branches. Then make a prediction based on the outcome of the branch the last time the same global branch history was encountered. 13.5 Local Branch Correlation One has a per-branch history register. Make a prediction based on the outcome of the branch last time the same local branch history was encountered. 13.6 Perceptron Branch Predictor Linear Machine Learning prediction on hardware level. The following was presented in the lecture on 03. May 2019. Deﬁnition 10. Superscalar: Hardware fetches multiple instructions and checks dependencies between them. 14 VLIW Deﬁnition 11. VLIW: Very Long Instruction Word consists of multiple independent instruc- tions packed together. Software (compiler) packs independent instructions in a larger instruction bundle to be fetched and executed concurrently. Deﬁnition 12. Lock-step execution: If any operation in a VLIW instruction stalls, all instruc- tions stall. 15 Systolic Arrays Goal: Design an accelerator that has simple, regular design, high concurrency, balanced com- putation and I/O bandwidth. 14 Idea: Replace a single processing element with a regular array of processing elements and care- fully orchestrate ﬂow of data between the processing elements. Beneﬁt: Maximizes computation done on a single piece of data element brought from memory. Data ﬂows from the computer memory in a rhythmic fashion, passing through many processing elements before it returns to memory. Note 10. Diﬀerences from pipelining: These are individual processing elements, array structure can be multi-dimensional and multidirectional. Deﬁnition 13. Convolution is used in ﬁltering, pattern matching, correlation, polynomial evaluation. It is calculating the input times the weights and adds it all up. Note 11. It is used in machine learning for matrix multiplications or least square error 15.1 Pipeline-Parallel Programs Loop iterations are divided into code segments called stages. Threads execute stages on diﬀerent cores. In the following example one thread executes all A, another all B, and one all C. 1 loop { 2 Compute Ai 3 Compute Bi 4 Compute Ci 5 } 15.2 Decoupled Access / Execute (DAE) Idea: Decouple operand access and execution via separate instruction streams that communicate via ISA-visible queues. Compiler generates two instruction streams (Access and Execution) and then synchronizes the two upon control ﬂow instructions using branch queues. 15.3 Loop Unrolling to eliminate branches Idea: Replicate loop body multiple times within an iteration The following was presented in the lecture on 09. May 2019. 16 SIMD Processors Deﬁnition 14. Data Parallelism: Concurrency arises from performing the same operation on diﬀerent pieces of data. Deﬁnition 15. Array Processor: Each Processor can do all the operations. The same opera- tions can be done at the same time. Diﬀerent operation at the same space. 15 Deﬁnition 16. Vector Processor: Each Processor can do one operation. The same operation cannot be done at the same time. The same operation are done at the same space. Deﬁnition 17. A vector processor is one whose instructions operate on vectors rather than scalar single data values. Note 12. Vector instructions allow deeper pipelines, because there is no dependency. But memory bandwidth can become a bottleneck. Note 13. A computer needs at least as many banks (to save memory) as the memory latency for loading memory is. 16.1 Gather/Scatter Gather is the loading of the elements and Scatter gives them out again. Gather/Scatter opera- tions often implemented in hardware to handle sparse vectors. 16.2 Masked Vector Instructions - Density Time Implementation Scan mask vector and only execute elements with non-zero masks. Note 14. An issue: If stride and banks are relatively prime we can sustain 1 element/ cycle throughput, otherwise not. The following was presented in the lecture on 10. May 2019. 17 Graphics Processing Units 17.1 Fine-Grained Multithreading Idea: Hardware has multiple thread contexts. Each cycle, fetch engine fetches from a diﬀerent thread. Switch to another thread every cycle such that no two instructions from a thread are in the pipeline concurrently. Deﬁnition 18. SPMD: Single Procedure / Program, multiple Data. This is a programming model rather than computer organization. GPU are SIMD Engines Underneath. The instruction pipeline operates like a SIMD pipeline. However, the programming is done using threads and not SIMD instructions. Deﬁnition 19. Warp: A set of threads that execute the same instruction on diﬀerent data elements 17.1.1 Dynamic Warp Formation/Merging Idea: Dynamically merge threads executing the same instruction after branch divergence. This forms a new warp from the warps that are waiting. 16 The following was presented in the lecture on 16. May 2019. 18 Memory Organization and Memory Technology Good exam questions: Which system is faster given two diﬀerent memory hierarchy systems. The goal is to eﬃciently store large amounts of data. 18.1 Memory Bank Organization and Operation 1. Decode row address and drive word-lines 2. Selected bits drive bit-lines 3. Amplify row data 4. Decode column address, select subset of row and select to output 5. Precharge bit-lines for next access 18.2 Memory Hierarchy We need both fast and large memory, but it is not possible to achieve both, with a single level of memory. That is why one needs multiple levels of storage (progressively bigger and slower as the levels are farther from the processor) and ensure most of the data the processor needs is kept in the faster levels. 17 18.3 Locality Temporal Locality If you just did something, it is likely that you will do the same thing again soon. Spatial Locality If you did something, it is likely you will do something similar or related in space. 18.4 Manual or Automatic Memory Management Manual Programmer manages data movement across levels. This is used in GPUs. Automatic Hardware manages data movement across levels, transparently to the programmer. 18.5 Recursive latency equation Ti = ti + mi ⋅ Ti+1 (6) Ti is the total memory access time, ti is the technology-intrinsic access time and mi is the miss-rate for i Deﬁnition 20. It is called a hit if it is in the cache, else it is a miss and one needs to bring the block to the cache. The following was presented in the lecture on 17. May 2019. 19 Caches Deﬁnition 21. A cache is a structure that memorizes frequently used results to avoid repeating the long-latency operations required to reproduce the results from scratch Deﬁnition 22. A block is a unit of storage in the cache. Note 15. The most important cache parameters are size, set associativity, block size and eviction policy. cache hit rate = # hits # hits + # misses (7) Average memory access time (AMAT) = hit-rate ⋅ hit-latency + miss-rate ⋅ miss-latency (8) Note 16. Memory is logically divided into ﬁxed-size blocks. 19.1 Cache access 1. index into the tag and data stores with index bits in address 2. check valid bit in tag store 18 3. compare tag bits in address with the stored tag in tag store Note 17. Direct-mapped cache means that a block can go to only one location 19.2 Associativity Associative memory within the set accommodates conﬂicts better that means there are fewer conﬂict misses. But it is more complicated and has slower access rate. Which block in the set to replace on a cache miss? Any invalid block ﬁrst. 19.3 Handling Writes When do we write the modiﬁed data in a cache to the next level? Write through: At the time the write happens or write back: At the time the block is evicted. Note 18. First level cache is split and higher level caches are uniﬁed. Note 19. Is power of 2 associativity required? Technically is not required, but it is easier. 19.4 Cache misses Compulsory miss ﬁrst reference to an address always results in a miss. Prefetching can help solve this problem. Capacity miss cache is too small to hold everything needed. To solve this problem one can utilize the cache space better. Conﬂict miss deﬁned as any miss that is neither a compulsory nor a capacity miss. More associativity, better indexing or software hints can solve this problem. 19.5 Restructuring Data Access patterns One can have faster access time with loop interchange. Data is laid-out in a column-major layout so that x[i + 1, j] follows x[i, j] 1 Poor code 2 for i = 1, rows 3 for j = 1, columns 4 sum = sum + x[i,j] 1 Better code 2 for j = 1, columns 3 for i = 1, rows 4 sum = sum + x[i,j] Note 20. Another way to make it faster is blocking. That is to divide loops operating on arrays into computation chunks so that each chunk can hold its data in the cache. It is also called tiling. 19 The following was presented in the lecture on 23. May 2019. 19.6 Multiprocessor Caching Note 21. A private cache belongs to one core and a shared cache is shared by multiple cores. 19.7 Cache Coherence If multiple processors cache the same block, how do they ensure they all see a consistent state? One idea to solve it is to invalidate all other copies of block A when a processor writes to it. Caches observe each other’s write and read operations via a shared bus if a processor writes to a block, all others invalidate the block. We need to guarantee that all processors see a consistent value for the same memory location. 20 Virtual Memory The programmer sees inﬁnite amount of virtual memory. So that the programmer does not need to worry about managing physical memory. Each process has its own mapping form virtual to physical memory. It enables the following: 1. Code and data to be located anywhere in physical memory 2. Isolation of code and data of diﬀerent processes in physical memory 3. Code and data sharing between multiple processors. Deﬁnition 23. Address Translation: The hardware converts virtual addresses into physical addresses via an OS-managed lookup table (page table). Note 22. Virtual address space is divided into pages and physical address space is divided into frames. A virtual page is mapped to a physical frame, if it is in physical memory or to a location in the disk otherwise. 20.1 Cache / Virtual Memory Analogues Table 3: Cache Virtual Memory Block Page Block Size Page Size Block Oﬀset Page Oﬀset Miss Page Fault Tag Virtual Page Number 20 The following was presented in the lecture on 24. May 2019. 20.2 How do we translate addresses Page table has an entry for each virtual page and each page table entry has: Valid bit Whether the virtual page is located in physical memory Physical page number Where the virtual page is located in physical memory Replacement policy, dirty bits Deﬁnition 24. PTBR = PageTableBaseRegister VPN= Virtual Page Number PTE = Page Table Entry P T BR + V P N ⋅ P T ESize (9) 20.3 Translation Lookaside Buﬀer (TLB) Idea: Cache the page table entries (PTEs) in hardware structure in the processor to speed up address translation. The TLB has a small cache of most recently used translations. The TLB has a lot of temporal locality and usually has around 95-99 %. The ﬁrst level TLB cache can accessed in 1 cycle. Deﬁnition 25. The hardware component is called the MMU (memory management unit). It includes the page table base registers, TLBs, page walkers. It is the job of the software to leverage the MMU. Deﬁnition 26. Page table is the tag store for the physical memory data store. It is a mapping table between virtual memory and physical memory. Deﬁnition 27. PTE is the tag store entry for a virtual page in memory. Deﬁnition 28. Page Fault. If a page is not in physical memory but on the disk. Access to such a page triggers a page fault exception. OS trap handler invoked to move data from disk into memory. 20.4 Page Replacements Algorithms Deﬁnition 29. A clock page replacement keeps a circular list of physical frames in memory and keep a pointer to the last-examined frame in the list. 20.5 Memory Protection A process can only access physical pages mapped in its page table and cannot overwrite memory of another process. This provides protection and isolation between processes. Note 23. Virtual memory system serves two functions today address translation and access control. 21 The following was presented in the lecture on 31. May 2019. 20.6 Runahead Execution 22","libVersion":"0.3.2","langs":""}