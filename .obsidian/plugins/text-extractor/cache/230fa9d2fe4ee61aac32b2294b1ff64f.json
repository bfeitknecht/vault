{"path":"sem4/FMFP/PV/FMFP-summary-dcamenisch.pdf","text":"Formal Methods and Functional Programming Summary FS22 Danny Camenisch June, 2022 Contents Functional Programming 3 1. Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 1.1 Basic Concepts of Functional Programming . . . . . . . . . . . . . . . . . 3 1.2 Introduction to Functional Programming . . . . . . . . . . . . . . . . . . 3 2. Natural Deduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 2.1 Propositional logic . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 2.2 First-order logic . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7 2.4 Equality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8 3. Correctness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9 3.1 Termination . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9 3.2 Reasoning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9 4. List and Abstraction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10 4.1 List Type . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10 4.2 Patterns . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10 4.3 Intermezzo - Advice on Recursion . . . . . . . . . . . . . . . . . . . . . . . 10 4.4 List Comprehension . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11 4.5 Induction over Lists . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11 5. Abstraction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11 5.1 Polymorphic Types and Reusability . . . . . . . . . . . . . . . . . . . . . 11 5.2 Higher-Order Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11 5.3 λ - Expressions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12 5.4 Functions as Values . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12 6. Typing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12 6.1 Type Checking . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13 6.2 Mini-Haskell - Syntax . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13 6.3 Typing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13 6.4 Rules for Core λ-Calculus . . . . . . . . . . . . . . . . . . . . . . . . . . . 13 6.5 Type Inference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14 6.6 Type Classes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14 7. Algebraic Data Types . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 7.1 Enumeration Types (disjoint unions) . . . . . . . . . . . . . . . . . . . . . 15 7.2 Product Types . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 7.3 General Definition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16 7.4 Recursive Types . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16 7.5 Correctness for Algebraic Data Types . . . . . . . . . . . . . . . . . . . . 16 8. Lazy Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16 8.1 Evaluation Strategy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16 8.2 Correctness of lazy Programs . . . . . . . . . . . . . . . . . . . . . . . . . 17 Formal Methods 18 1. Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 2. Introduction to Language Semantics . . . . . . . . . . . . . . . . . . . . . . . . . 18 2.1 The Language IMP . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 2.2 Semantics of IMP Expressions . . . . . . . . . . . . . . . . . . . . . . . . . 19 2.3 Properties of the Expression Semantics . . . . . . . . . . . . . . . . . . . . 20 Substitution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20 3. Operational Semantics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20 3.1 Big-Step Semantics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 1 3.2 Small-Step Semantics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 3.3 Equivalence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26 4. Axiomatic Semantics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27 4.1 Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27 4.2 Hoare Logic . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27 4.3 Soundness and Completeness . . . . . . . . . . . . . . . . . . . . . . . . . 28 5. Modeling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29 5.1 Protocol Meta Language Promela . . . . . . . . . . . . . . . . . . . . . . . 29 5.2 Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31 5.3 More on Promela . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32 6. Linear Temporal Logic . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33 6.1 Linear-Time Properties . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33 6.2 Linear Temporal Logic . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34 7. Model Checking . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35 2 Functional Programming 1. Introduction 1.1 Basic Concepts of Functional Programming One important notion in functional programming is that functions have no side effect, i.e. f (x) always returns the same value (for the same x). This allows us to reason as in mathematics, i.e. if f (0) = 2 then f (0) + f (0) = 2 + 2. The above mentioned property is called referential transparency , i.e. an expression evaluates to the same value in every context. Another basic concept is that we use recursion instead of iteration . This can be shown with an easy example of gcd . In Java we might use an iterative function: public static int gcd (int x, int y) { while(x != y) { if (x > y) { x = x - y; } else { y = y - x; } } return x; } whereas in functional programming we strictly use recursion: gcd x y | x == y = x | x > y = gcd (x - y) y | otherwise = gcd x (x - y) In this example we can already see the main property of functional programming - it specifies what should be computes, rather than how. 1.2 Introduction to Functional Programming Expression Evaluation In general, expression evaluation in Haskell works the same way as in mathematics, e.g. for f (x, y) = x − y, if we want to compute f (5, 7) we substitute 5 for x and 7 for y. We differ between two types of evaluation strategies : • Eager evaluation , where we evaluate arguments first (corresponds to the green path in the picture below) • Lazy evaluation , which is used in Haskell, where we evaluate expression from the left and only when needed (corresponds to the blue path in the picture below) 3 Syntax and Types The basic syntax of Haskell consists of the following two rules: - Functions consist of different cases: functionName x1 ... x2 | guard1 = expr1 | guard2 = expr2 ... | guardm = exprm • Programs consist of several definitions: myConstant = 5 aFunction y1 ... ym | guard1 = expr1 | guard2 = expr2 anotherFunction z1 ... zk ... In Haskell, indentation determines separation of definitions: • All function definitions must start at the same indentation level. • If a definition requires n > 1 lines, we indent the lines 2 to n further. Spaces are therefore very important, one should not use tabs! Haskell is a strongly typed language, i.e. we can only assign values to variables of a certain type. This helps us to avoid runtime errors like 3 + True . In Haskell we can use the following types : • Int for integers with at least the range of {−229, ..., 229 − 1} and the functions +, *, ˆ, -, div, mod, abs • Integer for unbounded integers and the same functions as Int • Bool with the values True, False and the binary operators &&, || and the unary operator not • Char for single characters as expected, i.e. 'a', 'b', 'c', ... • String for strings as expected, i.e. \"hello\", \"world\", ... • Doubles for double precision numbers and functions like +, -, *, /, abs, acos, ... We furthermore examine the type tuple more in detail. Tuples are represented in () brackets. We can put as many elements in a tuple as we want, we can even put tuples in tuples. 4 (Type1, Type2, ..., TypeM) Functions can take tuples as arguments and / or return tupled values as shown in the example below: addPair :: (Int, Int) -> Int addPair (x, y) = x + y ----------------------------- ? addPair (3, 4) 7 Pattern and Function Definition Function definitions build from both patterns xi and guards gi. functionName x1 ... x2 | g1 = expr1 | g2 = expr2 ... | gm = exprm Patterns are variables, constants or built from data constructors, while guards are boolean expressions. We need to note that pattern matching forces evaluation. Function Scope Functions have a global scope , i.e. a function can be called from any other function. We can force a local scope with the keywords let and where as shown in the example below: f x = let sq y = y * y in sq x + sq x The keyword where comes directly after a function definition and is used to define bindings over all guards: f p1 p2 ... pm | g1 = e1 | g2 = e2 ... | gk = ek where v1 = r1 v2 = r2 ... 2. Natural Deduction To carry out formal reasoning about systems we need three essential parts: 1. Language 2. Semantics 3. Deductive system for carrying out proofs Natural deduction is a method for proofs. It consists of a set of rules which are used to construct a derivation tree. Finally a proof is a derivation tree whose root has no assumptions. 2.1 Propositional logic Syntax The formal definition is given by: - Let a set V of variables be given. Then LP , the language of propositional logic , is the smallest set where: - - X ∈ LP if X ∈ V - ⊥ ∈ LP . - A ∧ B ∈ LP if A ∈ LP and B ∈ LP . - A ∨ B ∈ LP if A ∈ LP and B ∈ LP . - A → B ∈ LP if A ∈ LP and B ∈ LP . 5 Semantics A valuation σ : V → {True, False} is a function mapping variables to truth values. We furthermore let Valuations be the set of valuations. Satisfiability describes the smallest relation ⊨ ⊆ Valuations × LP such that: • σ ⊨ X if σ(X) = True • σ ⊨ A ∧ B if σ ⊨ A and σ ⊨ B • σ ⊨ A ∨ B if σ ⊨ A or σ ⊨ B • σ ⊨ A → B if whenever σ ⊨ A then σ ⊨ B We note here that σ ⊭ ⊥, for every σ ∈ Valuations. A formula A ∈ LP is satisfiable if σ ⊨ A, for some valuation σ A formula A ∈ LP is valid (a tautology ) if σ ⊨ A, for all valuations σ We furthermore respect semantic entailment , that is, A1, ..., An ⊨ A if for all σ, if σ ⊢ A1, ..., σ ⊨ An, then σ ⊨ A. Requirement for a Deductive System For a deductive system we require that syntactic entailment ⊢ (derivation rules) and semantic entailment ⊨ (truth tables) agree. This requirement has two parts. For H ≡ A1, ..., An some collection of formulae: 1. Soundness : If H ⊢ A can be derived, then H ⊨ A 2. Completeness : If H ⊨ A, then H ⊢ A can be derived Further Decidability is desirable. Natural Deduction for Propositional Formulae We define three keywords for natural deduction: • Sequent : An assertion of the form A1, ..., An ⊢ A where all A, A1, A2, ..., An are propositional formulae • Axiom : A starting point for building derivation trees of the form ..., A, ... ⊢ A axiom • Proof (of A): A derivation tree with root ⊢ A Rules We distinguish between two kinds of rules : • introduce , denoted by −I, which introduces a connective • eliminate , denoted by −E, which eliminates a connective Conjunction Rules Γ ⊢ A Γ ⊢ B Γ ⊢ A ∧ B ∧ −I, Γ ⊢ A ∧ B Γ ⊢ A ∧ −EL, Γ ⊢ A ∧ B Γ ⊢ B ∧ −ER Implication Rules Γ, A ⊢ B Γ ⊢ A → B → −I, Γ ⊢ A → B Γ ⊢ A Γ ⊢ B → −E Disjunction Rules Γ ⊢ A γ ⊢ A ∨ B ∨ −IL, Γ ⊢ B Γ ⊢ A ∨ B ∨ −IR Γ ⊢ A ∨ B Γ, A ⊢ C Γ, B ⊢ C Γ ⊢ C ∨ −E 6 Falsity Rule Γ ⊢ ⊥ Γ ⊢ A ⊥ − E Negation Rule Γ ⊢ ¬A Γ ⊢ A Γ ⊢ B ¬ − E 2.2 First-order logic Syntax There are two syntactic categories: terms and formulae . Furthermore, a signature consists of a set of function symbols F and a set of predicate symbols P and we also denote the set of variables as V. Term, the terms of first-order logic , is the smalles set where: 1. x ∈ Term if x ∈ V, and 2. f n(t1, ..., tn) ∈ Term if f n ∈ F and tj ∈ Term for all 1 ≤ j ≤ n Form, the formulae of first-order logic , is the smallest set where: 1. ⊥ ∈ Form 2. pn(t1, ..., tn) ∈ Form if p n ∈ P and tj ∈ Term, for all 1 ≤ j ≤ n 3. A ◦ B ∈ Form if A ∈ Form, B ∈ Form, and ◦ ∈ {∧, ∨, →} 4. Qx.A ∈ Form if A ∈ Form, x ∈ V, and Q ∈ {∀, ∃} Each occurrence of each variable in a formula is either bound or free . A variable occurrence x in a formula A is bound if x occurs within a subformula B of A of the form ∃x.B or ∀x.B and is said to be free otherwise. α - Conversion Names of bound variables are irrelevant, the just encode the binding structure. Therefore we can rename bound variables at any time (called α -conversion ). Example: ∀x.∃y.p(x, y) ≡ ∀y.∃x.p(y, x) Omitting Parentheses For binary operators we have the following binding strengths: • ∧ binds stronger than ∨ binds stronger than → • → associates to the right, ∧ abd ∨ bind to the left • ¬ binds stronger than any binary operator • Quantifiers extend to the right as far as possible, that is, the end of the line or “)” Semantics A structure is a pair S = ⟨US , IS ⟩ where US is a non-empty set, the universe , and IS is a mapping where: 1. IS (p n) is an n-ary relation on US , for pn ∈ P, and 2. IS (f n) is an n-ary (total) function on US , for f n ∈ F As shorthand, we may also write p S for IS (p) and f S for IS (f ). An interpretation is a pair I = ⟨S, v⟩, where S = ⟨US , IS ⟩ is a structure and v : V → US a valuation. The value of a term t under the interpretation I = ⟨S, v⟩ is written as I(t) and defined by: 1. I(x) = v(x), for x ∈ V, and 2. I(f (t1, ..., tn)) = f S (I(t1), ..., I(tn)) When ⟨S, v⟩ ⊨ A we say A is satisfied with respect to ⟨S, v⟩ or ⟨S, v⟩ is a model of A. When every suitable interpretation is a model, we write ⊨ A and say A is valid . A is satisfiable if there is at least one model for A. 7 Following an example of a suitable model for ∀x.p(x, s(x)) • US = N • pS = {(m, n) | m, n ∈ US and m < n} • s S (x) = x + 1 Substitution Substitution describes replacing in A all occurrences of a free variable x with some term t. We write A[x ↦→ t] to indicate that we substitute x by t in A. Example: A ≡ ∃y.y ∗ x = x ∗ z → A[x ↦→ 2 − 1] ≡ ∃y.y ∗ (2 − 1) = (2 − 1 ∗ z) Rules We will continue to use rules from propositional logic, but now for first-order formulae. Universal Quantification Γ ⊢ A Γ ⊢ ∀x.A ∀ − I ∗, Γ ⊢ ∀x.A Γ ⊢ A[x/t] ∀ − E For the insertion rule, the side condition * denotes that x cannot be free in any assumption Γ. Existential Quantification Γ ⊢ A[x ↦→ t] Γ ⊢ ∃x.A ∃ − I, Γ ⊢ ∃x.A Γ, A ⊢ B Γ ⊢ B ∃ − E∗ For the elimination rule, the side condition * denotes that x is neither free in B nor free in Γ. 2.4 Equality First order logic with equality Equality is logical symbol with associated proof rules. We define it with: • Extended language: t1 = t2 ∈ F orm if t1, t2 ∈ T erm • Extended definition of ⊨ : I ⊨ t1 = t2 if I(t1) = I(t2) Equality Equality is an equivalence rule, shown by the following three rules: Γ ⊢ t = t ref, Γ ⊢ t = s Γ ⊢ s = t sym, Γ ⊢ t = s Γ ⊢ s = r Γ ⊢ t = r trans Equality is also a congruence on terms and all definable relations: Γ ⊢ t1 = s1 · · · Γ ⊢ tn = sn Γ ⊢ f (t1, ..., tn) = f (s1, ..., sn) cong1 Γ ⊢ t1 = s1 · · · Γ ⊢ tn = sn Γ ⊢ p(t1, ..., tn) Γ ⊢ p(s1, ..., sn) cong2 8 3. Correctness Correctness is important for programs. But what does correctness mean? What properties should hold? • Termination : Important for many, but not all programs • Functional behavior : Function should return “correct” values Correctness is rarely obvious, it must be proven! 3.1 Termination If f is defined in terms of functions g1, ..., gk, and each gj terminates, then so does f . If we work with recursion, a sufficient condition for termination is: > Arguments are smaller along a well-founded order of function’s domain. > > - An order > on a set S is well-founded iff. there is no infinite decreasing chain chain x1 > x2 > x3 > · · · , for xi ∈ S > - We write >S to indicate the domain S, i.e. >S ⊆ S × S Well-Founded Relations We can construct new well-founded relations from existing ones the following way: Let R1 and R2 be binary relations on a set S. The composition of R1 and R2 is defined as: R2 ◦ R1 ≡ {(a, c) ∈ S × S | ∃b ∈ S.a R1 b ∧ b R2 c} Remark: For a binary relation R, we write a R b for (a, b) ∈ R. Let R ⊆ S × S. Define: • R1 ≡ R • Rn+1 ≡ R ◦ Rn, for n ≥ 1 • R+ ≡ ⋃ n≥1 Rn So a R+ b iff. a Ri b for some i ≥ 1. Lemma: Let R ⊆ S × S. Let s0, si ∈ S and i ≥ 1. Then s0 Ri si iff- there are s1, .., si−1 ∈ S such that s0 R s1 R ... R si−1 R si. Theorem: If > is a well-founded order on a set S, then >+ is also well-founded on S. 3.2 Reasoning Equational Reasoning Equational reasoning include proofs based on the simple idea, that functions are equa- tions. For example we can have the following simple Haskell program: swap :: (Int, Int) -> (Int, Int) swap (a, b) = (b, a) More formally: ∀a ∈ Z, ∀b ∈ Z, swap(a, b) = (b, a) Reasoning by Cases Consider the following Haskell snippet: maxi :: Int -> Int -> Int maxi n m | n >= m = n | otherwise = m Can we prove that maxi n m ≥ n? 9 • We have that n ≥ m ∨ ¬(n ≥ m) • We now show maxi n m ≥ n for both cases: – Case 1: n ≥ m, then maxi n m = n and n ≥ n – Case 2: ¬(n ≥ m), then maxi n m = m. But m > n, so maxi n m ≥ n Proof by Induction We use a domino principle formulated by induction proof rule. Example: To prove ∀n ∈ N .P • Base case: Prove P [n ↦→ 0] • Step case: For an arbitrary m not free in P , prove P [n ↦→ m + 1] under the assumption P [n ↦→ m]. Well-Founded Induction The induction schema for well-founded induction is given by: • To prove P for all natural numbers n • Well-founded step: For an arbitrary m (not free in P ), prove P [n ↦→ m] under the assumption that P [n ↦→ l] holds, for all l < m (where also l is not free in P ). 4. List and Abstraction 4.1 List Type Lists require a new type constructor: • If T is a type, then [T ] is a type. The elements of [T ] are given by: • Empty list: [] :: [T ] • Non-empty list: (x : xs) :: [T ], if x :: T and xs :: [T ] In Haskell we can use the following shorthand: 1 : (2 : (3 : [])) written as [1, 2, 3]. When writing functions on lists we always have to consider the case of the empty list! 4.2 Patterns Pattern matching has two purposes: 1. checks if an argument has the proper form 2. binds values to variables Example: (x : xs) matches with [2, 3, 4] → x = 2, xs = [3, 4]. Patterns are inductively defined with: • Constants: −2, ′1 ′, True, [], ... • Variables: x, f oo, ... • Wild card: _ • Tuples: (p1, ..., pk), where pi are patterns • Non-empty lists: (p1 : p2), where pi are patterns Moreover, patterns require to be linear , this means that each variable can occur at most once. 4.3 Intermezzo - Advice on Recursion Defining recursive functions can be difficult. We show a 5-step “tool” which should make the process easier. Following an example of defining a function drop which removes the first n elements of a list: 1. Step: Define the type 10 drop :: Int -> [Int] -> [Int] 2. Step: Enumerate the cases drop 0 [] = ... drop 0 (x:xs) = ... drop n [] = ... drop n (x:xs) = ... 3. Step: Define the simple cases drop 0 [] = [] drop 0 (x:xs) = x:xs drop n [] = [] drop n (x:xs) = ... 4. Step: Define the other cases drop 0 [] = [] drop 0 (x:xs) = x:xs drop n [] = [] drop n (x:xs) = drop (n-1) xs 5. Step: Generalize and simplify drop _ [] = [] drop 0 (x:xs) = x:xs drop n (x:xs) = drop (n-1) xs 4.4 List Comprehension We often need an analogous notation to set comprehension in set theory, i.e. an expression of the form {2 · x | x ∈ X}. In Haskell we can use list comprehension to achieve this: [2 * x | x <- xs] 4.5 Induction over Lists If we want to apply induction proofs to a function on lists, we need the following schema; • Proof by Induction: to prove P for all xs in [T ] • Base case: prove P [xs ↦→ []] • Step case: prove ∀y :: T, ys :: [T ]. P [xs ↦→ ys] =⇒ P [xs ↦→ y : ys] – Fix arbitrary y :: T and ys :: [T ] (both not free in P ) – Induction hypothesis: P [xs ↦→ ys] – To prove: P [xs ↦→ y : ys] 5. Abstraction 5.1 Polymorphic Types and Reusability Until now we have only seen monomorphic types. For example: [Int] -> Int, [String] -> Int, ... In Haskell we can define polymorphic types that contain type variables (start with lower case letter). [t] -> Int 5.2 Higher-Order Functions First-order functions take base types or constructor types as arguments. While second-order functions take first-order functions as arguments and so on. One example of such a higher-order functions are map and foldr . 11 map :: (a -> b) -> [a] -> [b] foldr :: (a -> b -> b) -> b -> [a] -> b The advantages of functions as arguments is that we can increase reusability, definitions get easier to understand and correctness is simpler to show. 5.3 λ - Expressions λ- expressions describes the convention to use or λ instead of f (x), i.e., f (x) = x + 3 in conventional notation becomes λx.x + 3 in λ-calculus. We will look at the following two simple functions: times2 x = 2 * x atEnd x xs = xs ++ [x] Haskell provides a notation to write functions like times2 and atEnd in-line: ? map (\\x -> 2 * x) [2, 3, 4] [4, 6, 8] ? foldr (\\x xs -> xs ++ [x]) [] [1, 2, 3, 4] [4, 3, 2, 1] 5.4 Functions as Values Functions can be returned as values. Example ( f . f denotes the composition, i.e. f ◦ f ): twice :: (t -> t) -> (t -> t) twice f = f . f ----------------------------- ? > twice times3 1 9 :: Int 6. Typing Again a friendly reminder that there are always hidden gems in the lecture slides / notes from when lecturers try their best to be funny. 12 6.1 Type Checking Type checking should prevent “dangerous” expressions such as 2 + T rue. The objectives for a type checker are as follows: • quick, decidable, static analysis • permit as much generality as possible • prevents runtime errors We examine here a simplified language: Mini-Haskell 6.2 Mini-Haskell - Syntax Programs are terms t (we assume variables V and integers Z to be given): t ::= V | (λx.t) | (t1, t2) | True | False | (iszero(t)) | Z | (t1 + t2) | (t1 ∗ t2) | (if t0 then t1 else t2) | (t1, t2) | fst(t) | snd(t) We often employ syntactic sugar, like omitted parentheses, to make the syntax more readable. 6.3 Typing Types are terms τ (we assume that VT is a set of types variables: a, b, ...): τ ::= VT | Bool | Int | (τ, τ ) | τ → τ We define a type system notation based on typing judgement: Γ ⊢ t :: τ where: • Γ is a set of bindings. • t is a term • τ is a type Intuitively, one might read x : Int ⊢ x + 2 :: Int as “Given symbol table x : Int (Γ), then term x + 2 (t) has type Int (τ )”. 6.4 Rules for Core λ-Calculus We introduce the first three core rules: Axiom ..., x : τ, ... ⊢ x :: τ Var Abstraction Γ, x : σ ⊢ t :: τ Γ ⊢ (λx.t) :: σ ↦→ τ Abs Application Γ ⊢ t1 :: σ ↦→ τ Γ ⊢ t2 :: σ Γ ⊢ (t1, t2) :: τ App Furthermore, there are some additional typing rules for Mini-Haskell: Base Types Γ ⊢ n :: Int Int Γ ⊢ True :: Bool True Γ ⊢ False :: Bool False Operations (op ∈ {+, ∗}) Γ ⊢ t :: Int Γ ⊢ (iszero t) :: Bool iszero Γ ⊢ t1 :: Int Γ ⊢ t2 :: Int Γ ⊢ (t1 op t2) :: Int BinOp Γ ⊢ t0 :: Bool Γ ⊢ t1 :: τ Γ ⊢ t2 :: τ Γ ⊢ (if t0 then t1 else t2) :: τ if 13 Tuples Γ ⊢ t1 :: τ1 Γ ⊢ t2 :: τ2 Γ ⊢ (t1, t2) :: (τ1, τ2) Tuple Γ ⊢ t :: (τ1, τ2) Γ ⊢ fst(t) :: τ1 fst Γ ⊢ t :: (τ1, τ2) Γ ⊢ snd(t) :: τ2 snd 6.5 Type Inference Syntax-directed typing rules specify the algorithm for computing a type: 1. Start with the judgment ⊢ t :: τ0 with the type variable τ0. 2. Build the derivation tree bottom-up by applying rules. 3. Solve constraint (unification) to get possible types. Example: 6.6 Type Classes Monomorphic versus Polymorphic Some functions are monomorphic . For example, the function XOR, which can only be applied to Bool . In contrast, some functions are polymorphic , for example a function which can be applied to lists of any type. Definition of the Eq Class The equality class is defined as: class Eq a where (==) :: a -> a -> Bool (/=) :: a -> a -> Bool x /= y = not (x == y) The definition includes the class name, the signature (the list of function names and types), and the default implementations. Elements of the class are called instances . Classes allow restricted form of type generalization with class constraints. For example the function elem checks if something is an element of a list. elem :: Eq t => t -> [t] -> Bool 14 elem _ [] = False elem a (x:xs) = (a == x) || elem a xs Instances We can create an instance the following way: instance Eq Bool where True == True = True False == False = True _ == _ = False Now the type Bool is an instance of the Eq class and we can use the functions of the type class. Class Hierarchies Classes can be hierarchically structured. For example, the class Ord is a subclass of the class Eq . class (Eq a) => Ord a where ... Classes can even “inherit” from multiple classes. class (Num a, Ord a) => Real a where ... 7. Algebraic Data Types With algebraic data types we can declare new types tailored to the objects being modeled. For months for example, we declare the type Month with elements January, February,. . . , December. These are new data constructors . 7.1 Enumeration Types (disjoint unions) For example: data Season = Spring | Summer | Fall | Winter data Month = January | February | ... | December For the syntax, the following holds: • Starts with the keyword data • Names different constructors • First letter of each constructor must be upper-case 7.2 Product Types A product type looks as follows: data People = Person Name Age type Name = String type Age = Int An element of type People consists of a name n and an age a e.g., Person \"John\" 30 Enumeration and product types can be combined : data Shape = Circle Double | Rectangle Double Double i.e., we define a Shape as either a circle with a radius or a rectangle with two sides. Functions are definable by pattern matching: area :: Shape -> Double area (Circle r) = pi * r * r area (Rectangle h w) = h * w 15 These new types can again be used with type classes to gain some functionality. In some cases, class instances can be automatically derived. data Foo = D1 | D2 | D3 deriving (Eq, Ord, Enum, Show) 7.3 General Definition The general definition looks as follows: data T = Constr_1 T_11 ... T_1k | Constr_2 T_21 ... T_2k' ... | Constr_n T_n1 ... T_nk'' where Tij are types, possibly also containing T . 7.4 Recursive Types Set of objects are often recursively defined: Expr ::= Int | Expr + Expr | Expr − Expr Formalized as a recursive data type: data Expr = Lit Int | Add Expr Expr | Sub Expr Expr An example of recursive functions over data types would be an interpreter for arithmetic expressions , which could look as follows: eval :: Expr -> Int eval (Lit n) = n eval (Add e1 e2) = (eval e1) + (eval e2) eval (Sub e1 e2) = (eval e1) - (eval e2) 7.5 Correctness for Algebraic Data Types Structural Inducation data Tree t = Leaf | Node t (Tree t) (Tree t) We want to perform induction over the structure of terms. Γ ⊢ P [x ↦→ Leaf] Γ, P [x ↦→ l], P [x ↦→ r] ⊢ P [x ↦→ Node a l r] Γ ⊢ ∀x ∈ Tree t.P a l r not free in Γ, P Proof: We show ∀x ∈ Tree t.P by induction. • Base case: Show P [x ↦→ Leaf] • Step case: Let a ∈ t and l, r ∈ Tree t be arbitrary. – Assume P [x ↦→ l] and P [x ↦→ r]. – Show P [x ↦→ Node a l r] 8. Lazy Evaluation 8.1 Evaluation Strategy Lazy Evaluation Haskell has a lazy evaluation strategy, which means that expressions are evaluated only when necessary. This happens top-down (outermost operator first) and from left to right, depending on operator precedence. 16 Example: f (9 − 3)(f 34 3) = (9 − 3) + (f 34 3) = 6 + (f 34 3) = 6 + (34 + 3) = 6 + 37 = 43 In Haskell, substitution occurs without argument evaluation. A potential problem with lazy evaluation is that computations may be duplicated. However, function arguments are evaluated only when needed and at most once, using sharing , i.e. sharing common sub-terms via pointers. Evaluation - Pattern Matching Arguments are evaluated as far as needed to determine a pattern match. Example: f [] _ = 0 -- (f.1) f _ [] = 0 -- (f.2) f (a:_) (b:_) = a + b -- (f.3) On execution of f [1 .. 3] [4 .. 6] , the following evaluation happens: f [1 .. 3] [4 .. 6] -- Does (f.1) match? = f (1 : [2 .. 3]) [4 .. 6] -- No. Does (f.2) match? = f (1 : [2 .. 3]) (4 : [5 .. 6]) -- No. Does (f.3) match? = 1 + 4 -- Yes! = 5 8.2 Correctness of lazy Programs Induction is only sound for finite, everywhere defined data. Thus we will only consider these type of programs for correctness proofs and not programs that lazyly generate infinite data. 17 Formal Methods Formal methods are mathematical approaches to software and system development which support the rigorous specification, design, and verification of computer systems. 1. Introduction We need to set specifications for programs. We need to use mathematical notations to describe: • System design to accomplish these requirements, e.g. program code • Requirements for the system, i.e. desired properties, e.g. deadlock freedom • Assumptions about the environment, e.g. intruder model We need to use a formal logic for verification to: • Validate specifications by checking consistency • Prove that the design satisfies requirements under given assumptions • Deductive via proof system or algorithmic via model checking The following limitations we see with formal methods: • Incorrect specifications • Technical limitations (undecidable properties, computing resources) • Many applications of formal methods require specialist users • Application of formal methods is expensive 2. Introduction to Language Semantics 2.1 The Language IMP The core language IMP is very simple. It has: • Boolean and arithmetic expressions • Variables which range over integers and are initialized • IMP does not include: – Heap allocation and pointers – Variable declarations – Procedures – Concurrency Concrete Syntax of IMP Characters : Letter = 'A' | ... | 'Z' | 'a' | ... | 'z' Digit = '0' | '1' | ... | '9' Tokens : Ident = Letter { Letter | Digit }* Numeral = Digit | Numeral Digit Var = Ident Arithmetic Expressions : 18 Aexp = '(' Aexp Op Aexp ')' | Var | Numeral Op = '+' | '-' | '*' Boolean Expressions : Bexp = '(' Bexp 'or' Bexp ')' | '(' Bexp 'and' Bexp ')' | 'not' Bexp | Aexp Bexp Aexp Rop = '=' | '#' | '<' | '<=' | '>' | '>=' Note that # is not equal. Statements : STM = 'skip' | Var ':=' Aexp | '(' STM ';' Stm ')' | 'if' Bexp 'then' Stm 'else' Stm 'end' | 'while' Bexp 'do' Stm 'end' Meta-Variables Meta-variables denote an arbitrary element of a syntactic category, e.g. an arbitrary statement. We follow the naming conventions below: • n for numerals ( Numeral ) • x, y, z for variables ( Var ) • e, e′, e1, e2 for arithmetic expressions ( Aexp ) • b, b1, b2 for boolean expressions ( Bexp ) • s, s′, s1, s2 for statements ( Stm ) 2.2 Semantics of IMP Expressions Semantic functions map elements of syntactic categories to elements of semantic cate- gories. For example, we need a function that maps from Numerals to Val. Example: • 101 to 5 (Binary to Decimal) • 101 to 101 (Straight forward, no conversion) Semantic of Numerals The semantic function N : Numeral → Val = Z maps a numeral n to an integer value N [[n]]. Examples: N [[8]] = 8 N [[n 9]] = N [[n]] × 10 + 9 A state is a total function, associating a value with each program variable: State : Var → Val 19 We use σ as a meta-variable for states. We define a designated state σzero, in which all variables have the value 0. States can be updated with a function σ[y → v] : (σ[y → v])(x) = v if x ≡ y, σ(x) else Two states σ1 and σ2 are said to be equal if they are equal functions: σ1 = σ2 ⇐⇒ if ∀x.σ1(x) = σ2(x) Semantics of Arithmetic Expressions The semantic function A : Aexp → State → Val maps an arithmetic expression e and a state σ to a value A[[e]]σ. Example: A[[x]]σ = σ(x)A[[e1 op e2]]σ = A[[e1]]σ op A[[e2]]σ for op ∈ Op Note: op is the operation Val × Val → Val corresponding to op. Semantics of Boolean Expressions The semantic function B : Bexp → State → Bool maps a boolean expression b and a state σ to a truth value B[[b]]σ. Example: B[[e1 op e2]]σ = tt if A[[e1]]σ op A[[e2]]σ, f f otherwise with op ∈ Rop. 2.3 Properties of the Expression Semantics Inductive Definitions The semantics is given by recursive definitions of functions A and B. The values for composite elements are defined inductively in terms of the immediate constituents. Example: Assume a new arithmetic expression: −e. A inductive definition of A[[−e]]σ is given by: A[[−e]]σ = 0 − A[[e]]σ since e is a subterm of −e. The definition A[[−e]]σ = A[[0 − e]]σ is not inductive since 0 − e is not a subterm of −e. Lemma: The equations for N define a total function N : Numeral → Val. Substitution Substitution [x ↦→ e] replaces each free occurrence of variable x by e. We use the following substitution lemma. Lemma: B[[b[x ↦→ e]]]σ ⇐⇒ B[[b]]σ[x ↦→ A[[e]]σ] 3. Operational Semantics Operational semantics describe how the state is modified during the execution of a statement. 20 3.1 Big-Step Semantics Natural Semantics of IMP A transition system is a tuple (Γ, T, →), where • Γ : a set of configurations • T : a set of terminal configurations, T ⊆ Γ • → : a transition relation, →⊆ Γ × Γ Operational semantics includes two types of configurations: 1. ⟨s, σ⟩, which represents that the statement s is to be executed in state σ 2. σ, which represents a final state (terminal configuration) The transition relation → describes how executions take place. We specify transition relations by rules of the form ϕ1 · · · ϕn ψ (Name) ∗ where ϕ1 · · · ϕn and ψ are transitions. The meaning of the rules is: If ϕ1 · · · ϕn are transitions, then ψ is a transition. We use the following terminology: • ϕ1 · · · ϕn are called the premises of the rule. • ψ is called the conclusion of the rule. Big-Step Semantics of IMP Skip ⟨skip, σ⟩ → σ (SKIPN S) Assign ⟨x := e, σ⟩ → σ[x → A[[e]]σ] (ASSN S) Sequential Composition ⟨s, σ⟩ → σ′ ⟨s ′, σ′⟩ → σ′′ ⟨s; s′. σ⟩ → σ′′ (SEQN S) Conditional Statement ⟨s, σ⟩ → σ′ ⟨ if b then s else s′ end, σ⟩ → σ′ (IFTN S) if B[[b]]σ = tt ⟨s ′, σ⟩ → σ′ ⟨ if b then s else s′ end, σ⟩ → σ′ (IFFN S) if B[[b]]σ = f f Loop Statement ⟨s, σ⟩ → σ′ ⟨ while b do s end, σ′⟩ → σ′′ ⟨ while b do s end, σ⟩ → σ′′ (WHTN S) if B[[b]]σ = tt ⟨ while b do s end, σ⟩ → σ (WHFN S) if B[[b]]σ = f f 21 Rule Schemes and Instantiations Inference rule definitions are actually rule schemes . A rule is instantiated when all meta-variables are replaced with syntactic elements. For example, consider the assignment rule scheme ⟨x := e, σ⟩ → σ[x → A[[e]]σ] (ASSN S) and a corresponding assignment rule instance: ⟨v := v + 1, σzero⟩ → σzero[v → 1] (ASSN S) Derivation Trees Rule instances can be combined to derive a transition ⟨s, σ⟩ → σ′. The result of the derivation is a derivation tree T . • The root of T is ⟨s, σ⟩ → σ′. written as root(T ) ≡ ⟨s, σ⟩ → σ′. • The leaves of T are axiom rule instances. The transition system permits a transition ⟨s, σ⟩ → σ′, written as ⊢ ⟨s, σ⟩ → σ′, if and only if there exists a finite derivation tree ending in ⟨s, σ⟩ → σ′. Termination For an IMP statement s we define termination in the context of big-step semantics as follows: The execution of a statement s in state σ • terminates successfully iff there ∃σ′ such that ⊢ ⟨s, σ⟩ → σ′ • fails to terminate iff there is no state σ′ such that ⊢ ⟨s, σ⟩ → σ′ 3.1.2 Proving Properties of the Semantics Semantic Equivalence Definition: Two statements s1 and s2 are semantically equivalent (written as s1 ≃ s2) iff: ∀σ, σ′. ( ⊢ ⟨s1, σ⟩ → σ′ ⇐⇒ ⊢ ⟨s2, σ⟩ → σ′ ) Unfolding Loops in IMP We have the following two lemmas: ∀b, s. (while b do s end ≃ if b then s; while b do s end end) ∀b, s, σ, σ′. ( ⊢ ⟨while b do s end, σ⟩ → σ′ ⇐⇒ ⊢ ⟨if b then s; while b do s end end, σ⟩ → σ′) Deterministic Semantics Lemma: The big-step semantics of IMP is deterministic . ∀s, σ, σ′, σ′′. ( ⊢ ⟨s, σ⟩ → σ′ ∧ ⊢ ⟨s, σ⟩ → σ′′ =⇒ σ′ = σ′′) Induction on Derivation Trees We introduce a new proof technique: induction on the shape of derivation trees . To prove a property P (T ) for all derivation trees T , prove that P (T ) holds for an arbitrary derivation tree T under the assumption (I.H.) that P (T ′) holds for all sub-trees T ′ of T . Such a proof typically proceeds by case distinction on the rule applied at the root of the arbitrary derivation tree. 22 3.1.3 Extensions of IMP Local Variable Declarations A statement var x := e in s end declares a local variable that is visible in the sub-statement of the declaration, s. The following semantics hold: 1. Expression e is evaluated in the initial state 2. Statement s is executed in a state in which x has the value of e 3. After the execution of s, the initial value of x is restored Big-step semantics rule: ⟨s, σ[x → A[[e]]σ]⟩ → σ′ ⟨var x := e in s end, σ⟩ → σ′[x → σ(x)] (LOCNS) Procedure Declarations and Calls We look at statements of the form: procedure(x1, · · · , xn; y1, · · · , ym) begin s end We have the following formal parameters: • x1, · · · , xn are value parameters • y1, · · · , ym are variable parameters (used to assign values back to the procedure caller) Procedures can be declared as part of a source program, and called. We apply the following restrictions: • In a procedure declaration, the formal parameter names x1, · · · , xn, y1, · · · , ym must be distinct from each other • x1, · · · , xn, y1, · · · , ym are the only free variables in s The big-step semantics rule is given by: ⟨s, σzero[ ⃗xi → A[[ei]]σ][ ⃗yj → σ(zj)]⟩ → σ′ ⟨p(⃗ei; ⃗zj), σ⟩ → σ[ ⃗zj → σ′(yj)] (CALLNS) Abort The idea is that the statement abort stops the execution of the complete pro- gram. Aborting is modeled in the operational semantics by ensuring that the configurations ⟨abort, σ⟩ are stuck, that is, that there is no state σ′ such that ⟨abort, σ⟩ → σ′. abort and skip are not semantically equivalent since there is a derivation tree for ⟨skip, σ⟩ → σ, but not for ⟨abort, σ⟩ → σ′. abort and while true do skip end are semantically equivalent however! Non-determinism The idea is that for the statement s[]s ′ either s or s ′ is non- deterministically chosen to be executed. Example: The statement x := 1[](x := 2; x := x + 2) will result in a state in which x either has the value 1 or 4. The following rules apply: ⟨s, σ⟩ → σ′ ⟨s[]s′, σ⟩ → σ′ (ND1NS) ⟨s ′, σ⟩ → σ′ ⟨s[]s′, σ⟩ → σ′ (ND2NS) 23 Parallelism The idea is that for the statement s par s ′ both statements s and s ′ are executed, but execution can be interleaved. Example: The statement x := 1 par (x := 2; x := x + 2) could result in a state in which x has the value 4, 1, or 3. 3.2 Small-Step Semantics 3.2.1 Structural Operational Semantics of IMP Structural Operational Semantics (SOS) Small-step semantics focuses attention on the individual steps of an execution. For example: • Execution of assignments • Execution of if-conditions, while-iterations, etc. Describing small steps of the execution allows one to express the order of execution of individual steps. The configurations are the same as for natural semantics (⟨s, σ⟩ or σ). We may use γ as a meta-variable for terminal or non-terminal configurations. The transition relation →1 can have two forms: • ⟨s, σ⟩ →1 ⟨s ′. σ′⟩ : The execution of s from σ is not completed and the remaining computation is expressed by the intermediate configuration ⟨s ′, σ⟩. • ⟨s, σ⟩ →1 σ′ : The execution of s from σ has terminated and the final state is σ′. A transition of the form ⟨s, σ⟩ →1 γ describes the first step of the execution of s in state σ. Transition System Γ{⟨s, σ⟩ | s ∈ Stm, σ ∈ State} ∪ State T = State →1⊆ {⟨s, σ⟩ | s ∈ Stm, σ ∈ State} × Γ We say that a non-terminal configuration ⟨s, σ⟩ is stuck if there does not exist a configura- tion γ such that ⟨s, σ⟩ →1 γ. We note here that terminal configurations (i.e- final states) σ are never stuck. SOS of IMP We have the following small-step semantics rules: Skip ⟨skip, σ⟩ →1 σ (SKIPSOS) Assign ⟨x := e, σ⟩ →1 σ[x ↦→ A[[e]σ]] (ASSSOS) These rules are analogous to natural semantics. Sequential Composition ⟨s, σ⟩ →1 σ′ ⟨s; s′, σ⟩ →1 ⟨s′, σ′⟩ (SEQ1SOS) ⟨s, σ⟩ →1 ⟨s ′′, σ′⟩ ⟨s; s′, σ⟩ →1 ⟨s′′; s′, σ′⟩ (SEQ2SOS) Conditional Statement The first step of executing if b then s1 else s2 end is to determine the outcome of the test b, and thereby, which branch to select: ⟨if b then s else s′ end, σ⟩ →1 ⟨s, σ⟩ (IFTSOS) if B[[b]]σ = tt 24 ⟨if b then s else s′ end, σ⟩ →1 ⟨s′, σ⟩ (IFFSOS) if B[[b]]σ = f f However, there are alternative rules for conditional statements, where the first step of executing if b then s1 else s2 end is the first step of the branch determined by the outcome of the test b: ⟨s, σ⟩ →1 σ′ ⟨if b then s else s′ end, σ⟩ →1 σ′ (IFT1SOS) if B[[b]]σ = tt ⟨s, σ⟩ →1 ⟨s ′′, σ′⟩ ⟨if b then s else s′ end, σ⟩ →1 ⟨s′′, σ′⟩ (IFT2SOS) if B[[b]]σ = tt Loop Statement ⟨while b do s end, σ⟩ →1 ⟨if b then s; while b do s end else skip end, σ⟩ (WHILESOS) Multi-Step Executions A relation is a k-step execution, written as γ → k 1 γ′. The intuitive meaning of this is that there is an execution from γ to γ′ in exactly k steps. We define the relation γ → k 1 γ′ as follows: • γ →0 1 γ′ if and only if γ = γ′ • For k > 0, γ → k 1 γ′ if and only if there exists γ′′ such that both ⊢ γ →1 γ′′ and γ′′ →k−1 1 γ′ We may use the notation γ →∗ 1 γ′ to denote that there is an execution from γ to γ′ in some finite number of steps. Derivation Sequences A derivation sequence is a non-empty, finite or infinite, se- quence of configurations γ0, γ1, γ2, ... for which: • γi →1 1 γi+1 for each 0 ≤ i such that i + 1 is in the range of the sequence • if the derivation sequence is finite then the last configuration in the sequence is either a terminal configuration or a stuck configuration Note that if γ0, γ1, γ2, ... is a derivation sequence, then, for all i in the range of the sequence, γ0 →i 1 γi. Termination The execution of a statement s in state σ • terminates iff there is a finite derivation sequence starting with ⟨s, σ⟩ • terminates successfully iff ∃σ′.⟨s, σ⟩ → ∗ 1 σ′ • runs forever iff there is an infinite derivation sequence starting with ⟨s, σ⟩ 3.2.2 Proving Properties of the Semantics When reasoning about finite derivation sequences, we usually use strong induction on the length of a derivation sequence. More generally, we reason about a multi-step execution γ →k 1 γ′ by strong induction on the number of steps k : • Define P (k) ≡ “for all executions of k, our property holds • Prove P (k) for an arbitrary k, with the induction hypothesis ∀k′ < k. P (k′) Semantic Equivalence The following lemma holds: Under the small-step semantics, two statements s1 and s2 are semantically equivalent if for all states σ, both: • for all stuck or terminal configurations γ, we have ⟨s1, σ⟩ →∗ 1 γ if and only if ⟨s2, σ⟩ → ∗ 1 γ, and • there is an infinite derivation sequence starting in ⟨s1, σ⟩ if and only if there is one starting in ⟨s2, σ⟩ 25 Determinism The following lemma holds: The small-step semantics of IMP is deterministic . That is, for all s, σ, γ, and γ′ we have that ⊢ ⟨s, σ⟩ →1 γ∧ ⊢ ⟨s, σ⟩ →1 γ′ =⇒ γ = γ′ 3.2.3 Extensions of IMP Local Variable Declarations A local variable declaration is of the form var x := e in s end. The steps are: 1. Assign e to x 2. Execute s 3. Restore the initial value of x The first small step could be easily defined as: ⟨var x := e in s end, σ⟩ →1 ⟨s, σ[x ↦→ A[[e]]σ]⟩ We extend the syntactic category Stm with a restore statement: STM = ... | 'restore' (Var, Val) Now we can use the restore statement to mark the end of the scope of a local variable and remember its original value: ⟨var x := e in s end, σ⟩ →1 ⟨s; restore (x, σ(x)), σ[x ↦→ A[[e]]σ]⟩ (LOCSOS) ⟨restore (x, v), σ⟩ →1 σ[x ↦→ v] (RETSOS) Abort The abort statement stops the execution of the complete program. Aborting is modeled by ensuring that the configurations ⟨abort, σ⟩ are stuck. Non-Determinism For the statement s[]s ′ either s or s ′ is non-deterministically chosen to be executed. We have the following rules: ⟨s[]s′, σ⟩ →1 ⟨s, σ⟩ (ND1SOS) ⟨s[]s′, σ⟩ →1 ⟨s′, σ⟩ (ND2SOS) Parallelism For the statement s par s ′ both statements s and s ′ are executed, but the execution may be interleaved: ⟨s, σ⟩ →1 ⟨s ′′, σ′⟩ ⟨s par s′, σ⟩ →1 ⟨s′′ par s′, σ′⟩ (PAR1SOS) ⟨s, σ⟩ →1 σ′ ⟨s par s′, σ⟩ →1 ⟨s′, σ′⟩ (PAR2SOS) ⟨s ′, σ⟩ →1 ⟨s ′′, σ′⟩ ⟨s par s′, σ⟩ →1 ⟨s par s′′, σ′⟩ (PAR2SOS) ⟨s ′, σ⟩ →1 σ′ ⟨s par s′, σ⟩ →1 ⟨s, σ′⟩ (PAR2SOS) 3.3 Equivalence The following theorem holds: > For every statement s of IMP, we have: > ⊢ ⟨s, σ⟩ → σ′ ⇐⇒ ⟨s, σ⟩ → ∗ 1 σ′ Furthermore, we introduce the following two lemmas: > For every statement s of IMP and states σ and σ′ we have: > ⊢ ⟨s, σ⟩ → σ′ =⇒ ⟨s, σ⟩ → ∗ 1 σ′ For every statement s of IMP, states σ and σ′, and natural number k we have that: ⟨s, σ⟩ → k 1 σ′ =⇒ ⊢ ⟨s, σ⟩ → σ′ 26 4. Axiomatic Semantics 4.1 Motivation We can prove correctness of programs based on formal semantics. The proof would also be possible with small-step semantics, but even more complicated. Such a proof is too detailed to be practical. Axiomatic semantics provides a way of constructing these proofs conveniently. 4.2 Hoare Logic 4.2.1 Hoare Triples and Assertions The properties of programs are specified as Hoare Triples of the form {P } s {Q} where s is a statement and P and Q are assertions about the state. Terminology: • The assertion P is called the precondition • The assertion Q is called the postcondition This means that if P evaluates to true in an initial state σ, and if the execution of s from σ terminates in a state σ′, then Q will evaluate to true in σ′. Logical Variables We allow assertions to contain logical variables : • Logical variables may occur only in assertions • Logical variables are not program variables and may, thus, not be accessed in programs For example: { x = N } y := 1; while not x = 1 do y := y*x; x := x-1 end { y = N! AND N > 0 } 4.2.2 Derivation System We formalize an axiomatic semantics of IMP by describing the valid Hoare triples. This is done by a derivation system : • The derivation rules specify which triples can be derived from each statement • The premises and conclusions of the derivation rules are Hoare triples Similarly to the other derivation systems we have studied, we write ⊢ {P } s {Q} if and only if there exists a finite derivation tree ending in {P } s {Q} Axiomatic Semantics of IMP Skip {P } skip {P } (SKIPAx) Assign {P [x → e]} x := e {P } Sequential Composition {P } s {Q} {Q} s ′ {R} {P } s; s′ {R} (SEQAx) Conditional Statement {b ∧ P } s {Q} {¬b ∧ P } s ′ {Q} {P } if b then s else s′ end {Q} (IFAx) 27 Loop Statement {b ∧ P } s {P } {P } while b do s end {¬b ∧ P } (WHAx) The rules so far manipulate assertions syntactically. Semantic entailment expresses semantic reasoning steps: • We write P ⊨ Q iff “for all states σ, B[[P ]]σ = tt implies B[[Q]]σ = tt. The rule of consequence allows semantic entailments in derivations: {P ′} s {Q ′} {P } s {Q} (CONSAx) if P ⊨ P ′ and Q ′ ⊨ Q 4.2.3 Proving Properties of the Semantics Induction on the Shape of Derivation Trees Properties of the axiomatic semantics are typically proved by induction on the shape of the derivation tree. Those proofs typically proceed by case distinction on the rule applied to the root of the arbitrary derivation tree T . Semantic Equivalence Two statements s1 and s2 are provably equivalent if: ∀P, Q . ⊢ {P } s1 {Q} ⇐⇒ {P } s2 {Q} 4.2.4 Total Correctness (Termination) We introduce an alternative form of Hoare triple: {P } s {⇓ Q} The informal meaning of the above triple is: If P evaluates to true in the initial state σ then the execution of s from σ terminates and Q will evaluate to true in the final state. We do not mix these triples with those of partial correctness. However, all total correctness derivation rules are analogous to those for partial correctness, except for the rule for loops. Loop Variants Termination is proved using loop variants , which is an expression that evaluates to a value in a well-founded set before each iteration. While Rule for Total Correctness The total correctness derivation rule for loops is given by: {b ∧ P ∧ e = Z} s {⇓ P ∧ e < Z} {P } while b do s end {⇓ ¬b ∧ P } (WHTOTAx) if b ∧ P ⊨ 0 < e 4.3 Soundness and Completeness We define the following two terms: • Soundness : If a property can be proved then it does indeed hold. • Completeness : If a property does hold then it can be proved. Soundness and completeness can be proved w.r.t. an operational semantics (here, big-step semantics): The partial correctness tripel {P } s {Q} is valid, written as ⊨ {P } s {Q}, iff: ∀σ, σ′. B[[P ]]σ = tt ∧ ⊢ ⟨s, σ⟩ → σ′ =⇒ B[[Q]]σ′ = tt • Soundness: ⊢ {P } s {Q} =⇒ ⊨ {P } s {Q} • Completeness: ⊨ {P } s {Q} =⇒ ⊢ {P } s {Q} The soundness and completeness theorem of partial correctness is given by: 28 For all partial correctness triples {P } s {Q} of IMP we have ⊢ {P } s {Q} ⇐⇒ ⊨ {P } s {Q} 5. Modeling Model checking is an automated technique that, given a finite-state model of a system and a formal property, systematically checks whether this property holds for (a given state in) that model. Model Checking Process The model checking process consists of three different phases: 1. Modeling phase: Model the system under consideration using the description language of your model checker and formalize the properties to be checked. 2. Running phase: Run the model checker to check the validity of the property in the system model. 3. Analysis phase: If property is satisfied, celebrate. If property is violated, analyze counterexample, reduce, and try again. 5.1 Protocol Meta Language Promela Promela is the input language of the Spin model checker. Main objects are processes, channels, and variables. Promela has a C-like syntax: init { printf(\"Hello World!\\n) } 5.1.1 Promela Programs Constant Declarations #define N 5 mtype = { ack, req }; Structure Declarations typedef vector { int x; int y }; Global Channel Declarations chan buf = [2] of { int }; Global Variable Declarations 29 byte counter; Process Declarations proctype myProc(int p) {...} Promela Process Declarations The simple form of a process is given by: proctype myProc(int p) {...} • The body consists of a sequence of variable declarations, channel declarations, and statements • There are no arrays as parameters An active process can be declared with: active [N] proctype myProc(...) {...} This starts N instances of myProc in the initial state. The init process is started in the initial state. 5.1.2 Promela Types The following primitive types exist in Promela: • bit or bool : 0...1 • byte : 0...255 • short : −2 15...2 15 − 1 • int : −2 31...2 31 − 1 There are no floats or mathematical (unbounded) integers! Furthermore, the following used-defined types are available: • Arrays : int name [4] • Structures • Types of symbolic constants : mytype • Channel type : chan 5.1.3 Promela Variable and Channel Declarations Variable declaration works as follows: byte a, b = 5, c: int d[3], e[4] = 3; mytype msg = ack; vector v; Channel declarations looks like: chan c1 = [2] of { mytype, bit, chan }; chan c2 = [0] of { int }; chan c3; • c1 can store up to two messages. Messages sent via c1 consists of three parts. • c2 models rendez-vous communication. • c3 is uninitialised. 5.1.4 State Space State Space of Sequential Programs The number of states is given by: #program locations × ∏ variable x |dom(x)| where |dom(x)| denotes the number of possible values of variable x. 30 State Space of Concurrent Programs The number of states of P ≡ P1||...||PN is at most #states of P1 × ... × #states of PN = N∏ i=1(#program locationsi × ∏ variable xi |dom(xi)|) State Space of Promela Models The number of states of a system with N processes and K channels is at most N∏ i=1(#program locationsi × ∏ variable xi |dom(xi)|) × K∏ j=1 |dom(cj)| cap(cj ) where |dom(c)| denotes the number of possible messages of channel c and cap(c) is the capacity of channel c. 5.1.5 State Transitions A state transition is made in three steps: 1. Determine all executable statements of all active processes. If no executable statement exists, the transition system gets stuck. 2. Choose non-deterministically one of the executable statements. 3. Change the state according to the chosen statement. 5.1.6 Promela Expressions and Statements Promela expressions include: • Variables, constants, and literals • Structure and array accesses • Unary and binary expressions • Function applications • Conditional expressions (E1 -> E2: E3) Furthermore, we have the following promela statements : • skip : Does not change the state in is always executable • timeout : Does not change the state and is executable if all other statements in the system are blocked. • assert(E) : Aborts execution if expression E evaluates to zero, otherwise it is equivalent to skip. Is always executable. • Assignment: x = E assigns the value of E to variable x. Is always executable. 5.2 Motivation 5.2.1 Example: Verification of Parallel Programs Consider the following simplified Java program: class Cell { int x = 0; static void main(...) { Cell c = new Cell(): Thread t1 = new Even(c); Thread t2 = new Even(c); t1.start(); t2.start(); t1.join(); t2.join(); System.out.println(c.x) } } 31 class Even extends Thread { Cell c; Even(Cell c) { this.c = c; } void run() { c.x = c.x + 1; c.x = c.x + 1; } } Modeling Even.run We can model the Even.run method in Promela as follows: int x; proctype EvenRun() { int y = x; y = y + 1; x = y; y = x; y = y + 1; x = y; } Note that we need to extend the model of c.x = c.x + 1 (instead of simply writing x = x + 1 ), since assignments in Promela are atomic, but in Java they are not. Modeling Cell.main Modelling the main program could look as follows: init { x = 0; run EvenRun(); run EvenRun(); /* Wait for termination */ _nr_pr == 1; printf(\"x: %d\\n\", x); assert x % 2 == 0; } _nr_pr is a predefined global variable that yields the number of active processes. 5.3 More on Promela 5.3.1 Promela Statements Selection Selection in Promela looks as follows: if :: s1 /* option 1 */ :: ... :: sn /* option n */ fi This is executable if at least one of its options is executable. It chooses an option non- deterministically and executes it. Repetition Repetitions look the following way: 32 do :: s1 /* option 1 */ :: ... :: sn /* option n */ od This is executable if at least one of its options is executable. Chooses repeatedly an option non-deterministically and executes it. It terminates when a break or goto is executed. Atomic Basic statements are executed atomically in Promela. This means that there is no interleaving during the execution of statements such as skip, timeout, assert, assignments, etc. Furthermore, atomic { s } executes atomically. It is executable if the first statement of s is executable. 5.3.2 Promela Macros Promela does not contain any procedures. However, a similar effect can often be achieved using macros . A macro just defines a replacement text for a symbolic name, possibly with parameters. Example: inline swap(a, b) { int tmp; tmp = a; a = b; b = tmp; } The inline call swap(a, b) is replaced by the body of the definition. 5.3.3 Promela Channels chan ch = [d] of { t1,..., tn } declares a channel . A channel can buffer up to d messages. Each message is a tuple whose elements have types t1,..., tn . Example: mytype = { req, ack, err }; chan ch = [5] of { mytype, int }; Send and Receive ch ! e1,..., en sends a message. Type of ei must correspond to ti in the channel declaration. ch ? a1,..., an receives messages. Receive is executable iff buffer is not empty and the oldest message in the buffer matches the constants ai. In an unbuffered channel , i.e. a channel of type chan ch = 0 ... , send is executable if there is a receive operation that can be executed simultaneously and vice-versa. 6. Linear Temporal Logic 6.1 Linear-Time Properties Transition Systems Revisited We use a slightly different definition here. A finite transition system is a tuple (Γ, σI , →), where: • Γ : a finite set of configurations • σI : an initial configuration, σI ∈ Γ • → : a transition relation, →⊆ Γ × Γ The key differences are that we have a fixed initial configuration and we omit terminal configurations from the definition. 33 Computations We denote: • Sω is the set of infinite sequences of elements of set S • s[i] denotes the i-th element of the sequence s ∈ Sω Now γ ∈ Γω is a computation of a transition system if: • γ[0] = ωI • γ[i] → γ[i+1] (for all i ≥ 0) C(T S) denotes the set of all computations of a transition system T S. Linear-Time Properties Linear-time properties can be used to specify the permitted computations of a transition system. A linear-time property P over Γ is a subset of Γ ω. A T S satisfies an LT-property P over Γ T S ⊨ P if and only if C(T S) ⊆ P Example: “All opened files must be closed eventually”: P = {γ ∈ Γω | ∀i ≥ 0 : γ[i](o) = 1 =⇒ ∃n > 0 : γ[i+n](o) = 0} From Configurations to Propositions For a transition system T S, we additionally specify a set AP of atomic propositions : • An atomic proposition is a proposition containing no logical connectives • Example: AP = {open, closed} (for files) We must provide a labeling function that maps configurations to sets of atomic proposi- tions from AP : • L : Γ → P(AP ) • Example: L(σ) = {open, } if σ(o) = 1, {closed}, if σ(o) = 0, {} otherwise We call L(σ) an abstract state . Traces A trace is an abstraction of a computation. We observe only the propositions of each state. not the concrete state itself. t ∈ P(AP ) ω is a trace of a transition system T S if t = L(γ[0])L(γ[1])L(γ[2]), ... and γ is a computation of T S. We denote the set of all traces of a transition system T S as T (T S). Safety Properties Intuition: “Something bad is never allowed to happen (and can’t be fixed)”. An LT-property P is a safety property if for all infinite sequences t ∈ P(AP ) ω : • If t /∈ P then there is a finite prefix ˆt of t such that for every infinite sequence t′ with prefix ˆt, t ′ /∈ P . Liveness Properties Intuition: “Something good will happen eventually”. An LT-property P is a liveness property if every finite sequence ˆt ∈ P(AP ) ∗ is a prefix of an infinite sequence t ∈ P . 6.2 Linear Temporal Logic Linear Temporal Logic (LTL) allows us to formalize LT-properties of traces in a convenient and succinct way. We will see syntax and semantics for LTL. Whether or not the traces of a finite transition system satisfy an LTL formula is decidable . 34 LTL: Basic Operators Syntax: ϕ = p | ¬ϕ | ϕ ∧ ϕ | ϕ U ϕ | ⃝ ϕ where p is a proposition from a chosen set of atomic propositions AP ̸= ∅. LTL: Semantics t ⊨ ϕ expresses that trace t ∈ P(AP ) ω satisfies LTL formula ϕ | | | |—|–| | t ⊨ p | iff p ∈ t[0] | | t ⊨ ¬ϕ | iff not t ⊨ ϕ | | t ⊨ ϕ ∧ ψ | iff t ⊨ ϕ and t ⊨ ψ | | t ⊨ ϕU ψ | iff there is a k ≥ 0 with t≥k ⊨ ψ and t≥j ⊨ ϕ for all j such that 0 ≤ j < k | | t ⊨ ⃝ϕ | iff t≥1 ⊨ ϕ | where t(≥i) is the suffix of t starting at ti. Derived Operators • true, f alse, ∨, =⇒ , ⇐⇒ are defined as usual • Eventually: ⋄ϕ ≡ (true U ϕ) • Always: □ϕ ≡ ¬ ⋄ ¬ϕ Useful Specification Patterns • Strong invariant: □ψ – ψ always holds • Monotone invariant: □(ψ =⇒ □ψ) – Once ψ is true, then ψ is always true • Establishing an invariant: ⋄□ψ – Eventually ψ will always hold • Responsiveness: □(ψ =⇒ ⋄ϕ) – Every time that ψ holds, ϕ will eventually hold • Fairness: □ ⋄ ψ – ψ holds infinitely often 7. Model Checking We would like to solve the following LTL model checking problem: Given a finite transition system T S and an LTL formula ϕ, decide whether t ⊨ ϕ for all t ∈ T (T S). Finite Automaton for Finite Prefixes Reminder: an NFA is a tuple (Q, Σ, δ, Q0, F ) where • Q : is a finite set of states • Σ : is a finite alphabet • δ : a transition relation • Q0 : the set of initial states • F : the set of accepting states Given a transition system T S = (Γ, σI , →), we define an NFA F AT S characterizing all finite prefixes Tf in(T S) of the traces of T S. Regular Safety Properties Given an automaton, we can check simple LTL formulas manually. An LTL formula is valid in a transition system, if every trace of the transition system satisfies the formula. A safety property is regular if its bad prefixes are described by a regular language over the alphabet P(AP ). We check regular safety properties in the following way: 1. Describe the finite prefixes Tf in(T S) by finite automaton F AT S 2. Describe bad prefixes of regular safety property P by finite automaton F A ¯P 35 3. Construct finite automaton for product of F AT S and F A ¯P 4. Check if the resulting automaton has any reachable accepting states 1. If not, the property P is never violated in traces of T S 2. If yes, the property P is violated Product Automaton Construct NFA F AT S∩ ¯P that accepts the intersection of the languages accepted by both automatons. Emptiness Check If a product automaton F AT S∩ ¯P accepts a word w then: • w ∈ Tf in(T S) because it is accepted by F AT S and • w is bad prefix because it is accepted by F A ¯P • Therefore, P is not satisfied, and w is a counterexample ω-Regular Languages Regular expressions denote languages of finite words. ω-regular expression denote languages of infinite words. An ω-regular expression G has the form G = E1F ω 1 |...|EnF ω n (1 ≤ n) where Ei and Fi are regular expressions and ϵ /∈ L(Fi). Büchi Automata Büchi automata are similar to finite automata, but accept infinite words. A run of an NBA accepts its inputs if it passes infinitely often through an accepting state. LTL Model Checking 1. Describe traces T (N S) by NBA B AT S 2. For an LTL formula ϕ, construct NBA B A¬ϕ that accepts the traces characterized by ¬ϕ (the bad traces) 3. Construct NBA for product of B AT S and B A¬ϕ 4. Check whether the language accepted by product NBA is empty 1. If no, property ϕ is violated and each word in the language is a counterexample 36","libVersion":"0.5.0","langs":""}