{"path":"sem3/LinAlg/PV/cheatsheets/LinAlg-cheatsheet-bfunk.pdf","text":"Cheat Sheet: Comp Sc BSc, LinAlg - Brian Funk, 21.04.2001 - 22-918-18-957 Complex Numbers Basics Def : z = a + bi ⇔ ℜ(z) = a, Im(z) = b Def : z = a + bi ⇔ ¯z = a − bi ⇔ r · e2π−ϕ Def : z = r · cos(ϕ) + i · sin(ϕ) Def : |z| = r = √ x2 + y2 = √z · ¯z Def : ϕ =    arctan y x 1. Q arctan y x + π 2./3. Q arctan y x + 2pi 4. Q Operations Def : z1 ± z2 : (x1 + x2) ± i(y1 + y2) Def : z1 · z2 : (x1 + i · y1) + (x2 + i · y2) = r1 · r2ei(ϕ1+ϕ2 ) Def : z1 z2 : r1 r2 ei(ϕ1−ϕ2 ) = z1 · ¯z2 |z2 |2 Def : n√a ⇔ a = zn ⇔ |a| · e iϕ = rn · eiωn ⇔ r = n√|a|, ω = ϕ+2kπ n Polynomials The roots of a complex polynomial are pairwise conjugated. Def : z = b± √b2−4ac 2a Def : azn + c = 0 ⇔ z = n√− c a SLE Gauss Algorithm Compability Conditions: br+1 = ... = bm = 0 S 1.1: Ax = b hat min eine L¨osung ⇔ r = m oder r < m + V B dann: r = n ⇔ 1 L¨osung , r < n ⇔ ∞ L¨osungen Cor 1.7: For a quadratic SLE with n equations and n variables we have the following set of equivalence, of wich ONLY one of them can be true; So, EITHER i Rank(A) = n (A is regular) ii for every b there exist at least one solution iii for every b there exists exactly one solution iv the corresponding homogeneous system has only the trivial solution OR the following equivalences hold v Rank(A) < n (A is singular) vi for some b there exits no solution vii for no b a unique solution exists viii for some b infinity many solution exists ix the corresponding homogeneous system has non-trivial solutions Matrices and Vectors Definitions A m × n matrix hat m row (Zeilen)↓ and n columns (Spalten)→ , in which the i,j element gets noted by ai,j or (A)i,j Def nullmatrix: Has in every entry 0 Def diagonalmatrix: Has in every entry 0 except for the diagonal: (D)ij = 0 for i ̸= j one can write Diag(d11, · · · , dnn) Def identity: The identity is written as In = Diag(1, · · · , 1) It holds that AI = IA = A Def upper triangular matrix: We have (R)ij = 0 for i > j (Rechtsdreiecksmatrix) Def lower triangular matrix: We have (R)ij = 0 for i < j (Linksdreiecksmatrix) Def Matrix-set: The set of m × n-matrices is written as: E m×n For vectors we have: E n, where E is R or C Def matrix multiplication: If C = AB then one can write Cij = (AB)ij = ∑n k=1(A)ik(B)kj = ∑n k=1 aikbkj S 2.1: ‹ (αβ)A = α(βA) ‹ (A + B) + C = A + (B + C) ‹ (αA)B = α(AB) ‹ (AB) · C = A · (BC) ‹ (α + β)A = αA + βA ‹ (A + B) · C = AC + BC ‹ A · (B + C) = AB + AC ‹ α(A + B) = αA + αB ‹ A + B = B + A S 2.20: Let A and B be unitary(orthogonal). It holds: ‹ A is regular and A −1 = AH (A T ) ‹ AAH (AAT ) = I ‹ A −1 is unitary (orthogonal) ‹ AB is unitary (orthogonal) Def Zerodiviser: If AB = 0 ⇔ A,B Zerodiviser, Nullteiler Def transposes: (A T )ij = Aji Def conjugate transposed: AH = (A) T = AT Def symmetric: A T = A ⇔A symmetric Def skew-symmetric: AT = −A ⇔A skew-symmetric Def hermitian: A H = A ⇔A hermitian S 2.6: Also accounts for AT instead of AH . α simplifies to α ‹ (AH )H = A ‹ (αA) H = αA H ‹ (A + B)H = A H + BH ‹ (AB) H = BH AH S 2.7: For symmetric matrices A and B it holds that: AB = BA ⇔ AB is symmetric It holds for arbitrary matrix C that: CT C and CCT are symmetric. The same holds for the hermitian case Scalarproduct and Norm Def euclidian scalarproduct: ⟨x, y⟩ = xT y = ∑n k=1 xk · yk in −−→ R ∑n k=1 xk · yk S 2.9: S1 ⟨x, y + z⟩ = ⟨x, y⟩ + ⟨x, z⟩(linear in 2nd factor) S1 ⟨x, αy⟩ = α ⟨x, y⟩ (linear in 2nd factor) S2 for E = R: ⟨x, y⟩ = ⟨y, x⟩ (symmetric) S2’ for E = C: ⟨x, y⟩ = ⟨y, x⟩(hermitian) S3 ⟨x, x⟩ > 0, ⟨x, x⟩ = 0 ⇔ x = 0(positiv definite) Cor 2.10: S4 for E = R: linear in 1st factor ⟨w + x, y⟩ = ⟨w, y⟩ + ⟨x, y⟩ ⟨αx, y⟩ = α ⟨x, y⟩ S4’ for E = C: conjugate-linear in 1st factor ⟨w + x, y⟩ = ⟨w, y⟩ + ⟨x, y⟩ ⟨αx, y⟩ = α ⟨x, y⟩ Def norm: ∥x∥ = √ ⟨x, x⟩ = √xT x = √∑n k=1(|xk|)2 in −−→ R √∑n k=1 x2 k S 2.11: | ⟨x, y⟩ | ≤ ∥x∥ · ∥y∥( Cauchy-Schwarz inequality, ”=” holds when y is a multiple of x or vice verca) Def CBS: CBS is a property of the scalar product: CBS squared yields: | ⟨x, y⟩ |2 ≤ ⟨x, x⟩ ⟨y, y⟩ S 2.12: For the euclidian norm holds: N1 ∥x∥ > 0, ∥x∥ = 0 ⇔ x = 0(positiv definit) N2 ∥αx∥ = α ∥x∥ (homogeneous) N3 ∥x ± y∥ ≤ ∥x∥ + ∥y∥ (Triangle-inequality) Def : Angle ϕ between x, y: , ϕ = arccos Re(⟨x,y⟩) ∥x∥·∥y∥ in −−→ R arccos ⟨x,y⟩ ∥x∥·∥y∥ Def : x, y are orthogonal: ⟨x, y⟩ = 0 ⇔ x ⊥ y S 2.13: ∥x ± y∥2 = ∥x∥2 + ∥y∥2 ⇔ x ⊥ y (Pythagoras) Def p-norm: ∥x∥p = (|x1| p · · · |xn| p) 1 p Outer Product and Projections Def outer product: m-vector x and n-vector y: xyT S 2.14: A m × n-matrix has rank 1 if it is the outer product of an m-vector ̸= 0 and n-vector ̸= 0 S 2.15: The orthogonal projection Pyx of the n-vector x onto y is defined as: Pyx = 1 ∥y∥2 · yyH x = uuH = Pu where u = y ∥y∥ Def projections matrix: Py = 1 ∥y∥2 · yyH It has the properties: P H y = Py (hermitian/symmetric) and P 2 y = Py (idempotent) Inverse Def invertible: ∃A −1 ⇔ A −1 · A = A · A−1 = I S 2.17: A is invertible⇔ ∃X : AX = I ⇔ X is unique ⇔ A is regular S 2.18: If A, B are regular: ‹ A−1 is regular and A −1−1 = A ‹ AB is regular and (AB)−1 = B−1A −1 ‹ AH is regular and (AH )−1 = (A−1)H S 2.19: If A is regular das LGS Ax = b has the unique solution x = A−1b Finding an inverse [A|I] row −−−→ op [I|A −1] if A = (a b c d ) and det(A) ̸= 0 ⇔ A is invertible ⇔ A−1 = 1 ad−bc ( d −b −c a ) A = [ a11 a12 a21 a22 ] ⇔ A−1 [ a −1 11 a −1 12 a −1 21 a −1 22 ] Orthogonal and unitary matrices Def unitary/orthogonal: AAH = I, AA T = I ⇔ A is unitary/orthogonal ⇔ det(A) = ±1 S 2.20: A,B are unitary/orthonormal: ‹ A is regular and A−1 = A H ‹ AAH = In ‹ A−1 is unitary/orthogonal ‹ AB is unitary/orthogonal ‹ columns are orthonormal S 2.21: Images from unitary/orthonormal matrices are conformal (l¨angen-winkeltreu) Def 2d rotation: R(ϕ) = ( cosϕ sinϕ −sinϕ cosϕ ) Def 3d rotation: Rx(ϕ) =   1 0 0 0 cosϕ −sinϕ 0 sinϕ cosϕ   , Ry(ϕ) =   cosϕ 0 sinϕ 0 1 0 −sinϕ 0 cosϕ   , Rz(ϕ) =  cosϕ −sinϕ 0 sinϕ cosϕ 0 0 0 1   LU-Decomposition The LU-decomposition is useful when multiple SLE have the same A ‹ Find P A = LR ‹ solve Lc = P b ‹ solve Rx = c Vectorspaces Def : A vectorspace V over K is a non-empty set, on which vectoraddition and scalarmultiplication is defined Def Axioms: V1 : x + y = y + x V2 : (x + y) + z = x + (y + z) V3 : ∃0 ∈ V : x + 0 = x V4 : ∀x∃ − x : x + (−x) = 0 V5 : α(x + y) = α · x + α · y V6 : (α + β)x = αx + βx V7 : (αβ)x = α(βx) V8 : 1 · x = x S 4.1: i : 0 · x = 0 ii : ·0 = 0 iii : α · x = 0 → x = 0 ∨ α = 0 iv : (−α)x = α(−x) = −(α · x) Def polynomial space: Pn is defined as all polynomials of degree n. Further: P = ⋃∞ n=0 Pn S 4.1: I a vectorspace the following holds for a scalar α and x ∈ V : ‹ 0x = 0 ‹ 0α = 0 ‹ αx = 0 ⇒ α = 0 or x = 0 ‹ (−α)x = α(−x) = −(αx) S 4.12: {b1, · · · , bn} ⊂ V is a basis of V ⇔ every vector x ∈ V can be uniquely represented as: x = ∑n k=1 ξkbk S 4.2: ∀x ∈ V, ∀y ∈ V ∃z ∈ V : x + z = y where z is unique and z = y + (−x) Subspace Def : A subspace (unterraum) U is a non-empty subset of V. It is closed under vector addition and scalar multiplication. U contains the zero-vector S 4.3: Every subspace is a vectorspace Def spanning set: The vectors v1, · · · , vn are a spanning set (erzeugendes System) of V, if ∀w ∈ span{v1, · · · , vn} Linear dependency, basis, dimensions Def linear dependency: Vectors v1, · · · , vn are linearly dependent ⇔ ∑n k=1 αk · vk = 0 → α1 = · · · = αn = 0 Def dimension: the dimension of V is dimV = |spanV | (dim{0} = 0) Lem 4.8: Every set {v1, · · · , vm} ⊂ V with |Bv| < m is linear dependant Cor 4.10: in an finite vectorspace, a set with n independant vectors is basis of V if dim(V ) = n Def : The coefficients ξk are coordinates of x with respect to a basis B ξ = (ξ1, · · · , ξn)T is a coordinate vector Def : Two subspaces U, U ′ ⊂ V are complementary if every v ∈ V has a unique representation in U andU ′. Namely, v = u ∈ U + u ′ ∈ U ′. → V = U ⊕ U ′ Linear Maps Definitions Def linearity: F : V → W is linear: ‹ F (v + w) = F (v) + F (w) ‹ αF (v) = F (αv) Def injective: ∀x, x′ ⊂ X : f (x) = f (x′) ⇔ x = x′ Def surjective: ∀y ⊂ Y, ∃x ⊂ X, f (x) = y Def bijective: surjective and injective⇔ f −1 exists Matrix representation Let F be a linear map X → Y . One can write F (bi) ∈ Y as a linear combination of the basis of Y: F (bi) = ∑m k=1 ak,l · ck Def : The matrix A m×n witht the elements ak,l is a matrix (Abbildungsmatrix) with respect X, Y F (x) = y ⇔ Aξ = η [H] Def isomorphism: F is bijective ⇔ F is an isomorphism Def automorphism: F is isomorphism and X = Y ⇔ F is an automorphism S 5.1: F is isomorphism ⇔ F −1 exists and is an isomorphism and linear Kernel, Image and Rank Def Kern: kerF = {x ∈ X|F (x) = 0} S 5.6: F injective ⇔ kerF = {0} Def Image: ImF = {F (x)|x ∈ X} S 5.6: F surjective ⇔ imF = Y ker A is the solution set of Ax = 0. Im(A) set of all b, such that Ax = b is solvable S 5.7: dimX − dim(kerF ) = dim(imF ) = Rank(F ) Def : The rank F is equal to dim(im(F )) Cor 5.8: ‹ F : X ↦→ Y injective ⇔ Rank F = dim X ‹ F : X ↦→ Y surjective ⇔ Rank F = dim Y ‹ F : X ↦→ Y bijective (isomorphism) ⇔ Rank F = dim X = dim Y ‹ F : X ↦→ Y bijective (automorphism) ⇔ Rank F = dim X, ker F = {0} Cor 5.10: ‹ Rank(G ◦ F ) ≤ min(RankF, RankG) ‹ G is injective Rank(G ◦ F ) = RankF ‹ G is surjective Rank(G ◦ F ) = RankG Matrices as linear mapping Def columnspace: The columnspace (Spaltenraum) of A is the subspace ℜ(A) = im(A) = span{a1, · · · , an} Def nullspace: The nullspace (Nullraum) of A is a subspace N (A) = kerA = L0(Ax = 0) Def : # free variables = dimN (A) S 5.12: Rank A =r: and L0 Solution of Ax = 0 ⇒ dimL0 = dimN (A) = dim(KerA) = n − r S 5.13: Rank A ∈ M m×n: ‹ pivots in Row-echelon-form ‹ dim(im(A)) of A : En ↦→ E m ‹ dimension of the linear independent columns/rows Cor 5.14: RankAT = RankAH = RankA S 5.16: for A ∈ E m×n and BinE p×m: ‹ RankBA ≤ min(RankA, RankB) ‹ RankB = m ≤ p ⇒ RankBA = RankA ‹ RankA = m ≤ n ⇒ RankBA = RankB Cor 5.17: From S.5.16 it follows for quadratic matrices. A ∈ E m×m and BinE m×m ‹ RankBA ≤ min(RankA, RankB) ‹ RankB = m ⇒ RankBA = RankA ‹ RankA = m ⇒ RankBA = RankB S 5.18: For quadratic matrix E n×n the following statements are equivalent: ‹ A is regular ‹ RankA = n ‹ Columns are linearly independent ‹ Rows are linearly independent ‹ kerA = N (A) = {0} ‹ A is invertible ‹ ImA = ℜ(A) = En S 5.19: For Ax = b, b ̸= 0 with the solution x0 and L0 the solutionset is defined by Lb = x0 + L0 and is called affine subspace (not a real subspace since 0 /∈ Lb) dim(Im(A)) = n − dim(ker(A)) = n − (n − r) = r RC Find Basis of Im A= R(A): 1 bring into row echelon form 2 mark rows with pivots 3 marked columns in the normal form are a Basis ex −1 −4 7 3 3 0 −6 0 −3 4 1 −3 1 −4 3 3 −−→ (1) −1 −4 7 3 0 −12 15 9 0 0 0 0 0 0 0 0 −−→ (2) −1 −4 7 3 ↑ −12 15 9 0 ↑ 0 0 0 0 0 0 −−→ (3) span { −1 3 −3 1 −4 0 4 −4 } RC Find Basis of ker A = N (A), A ∈ Em×n: 1 bring into row echelon form 2 create a vector for every row, which does not have a pivot. The dimensions of the vectors are E1×n [3]Solve SLE Ax = 0 with the yielded vectors. 4 Write the solution as vector ex −1 −4 7 3 3 0 −6 0 −3 4 1 −3 1 −4 3 3 −−→ (1) −1 −4 7 3 0 −12 15 9 0 0 0 0 0 0 0 0 −−→ (2) −1 −4 7 3 0 −12 15 9 0 0 ↑ ↑ 0 0 0 0 −−→ (3) x4=α x3 =β x2= 5β+3α 4 x1=2α −−→ (4) span { 2 5 4 1 0 0 3 4 0 1 } Def Maps: Let X, Y be vector spaces with dimX = n, dimY = m ‹ F : X ↦→ Y a linear map ‹ A : E n ↦→ E m′, ξ ↦→ η * ‹ B : E n ↦→ E m, ξ′ ↦→ η′ * ‹ T : En ↦→ E n ′, ξ ↦→ ξ′ ** ‹ S : E m ↦→ E m ′, η ↦→ η′** *Abbildungsmatrix **Transformationsmatrix S 5.20: RankF = r has the mappingmatrix* A = [ Ir 0 0 0 ] Vector spaces with scalar products Definitions Def Norm: A norm is a function | · | : V → R, x → ∥x∥ in a vector space which satisfies: N1 ∥x∥ > 0, ∥x∥ = 0 ⇔ x = 0 (positiv definit) N2 ∥αx∥ = α ∥x∥ (homogenous) N3 ∥x ± y∥ ≤ ∥x∥ + ∥y∥ (Triangle-inequality) A normed vector space has a norm Def scalar product: is a function ⟨·, ·⟩ : V × V → E, x, y ↦→ ⟨x, y⟩, which satisfies: S1 ⟨x, y + z⟩ = ⟨x, y⟩ + ⟨x, z⟩(linear in 2nd factor) S1 ⟨x, αy⟩ = α ⟨x, y⟩ (linear in 2nd factor) S2 ⟨x, y⟩ = ⟨x, y⟩(symmetric, hermitian) S3 ⟨x, x⟩ > 0, ⟨x, x⟩ = 0 ⇔ x = 0(positiv definite)) Def unitsphere: the set {x ∈ V | ∥x∥ = 1} Def induced norm: The length of a vector is defined as: ∥·∥ : V ↦→ R, ∥x∥ ↦→ √⟨x, x⟩ Def angle ϕ: ϕ = ∢(x, y), 0 ≤ ϕ ≤ π is defined by: ϕ = ⟨x,x⟩ ∥x∥·∥y∥ = ℜ⟨x,y⟩ ∥x∥·∥y∥ Def orthogonal vectors: two vectors x, y are orthogonal ⇔ ⟨x, y⟩ = 0 Def orthogonal sets : two sets X, Y are orthogonal ⇔ ∀x ∈ X, ∀y ∈ Y ⟨x, y⟩ = 0 S 6.1: |⟨x, y⟩| 2 ≤ ⟨x, x⟩ ⟨y, y⟩ = ∥x∥2 · ∥y∥2(Cauchy Schwarz Inequality) S 6.2: ∥x ± y∥2 = ∥x∥2 + ∥y∥2 ⇔ x ⊥ y(Pythagoras) Def orthogonal basis: ⇔ ∀i, ∀j, i ̸= j : ⟨bi, bj ⟩ = 0 Def orthonormal basis: ⇔ orthogonal basis with vectors of length 1 S 6.3: A set M of pairwise orthogonal vectors are linearly independant if 0 /∈ M S 6.4: Let {b1, · · · , ¯ n} a orthonormal basis, x ∈ V : x = ∑n k=1 ⟨bk, x⟩ bk → ξk = ⟨bl, x⟩ S 6.5: from ξk = ⟨bl, x⟩v , ηk = ⟨bl, x⟩v follows ⟨x, y⟩v = ∑n k=1 ξnηk = ξH η = ⟨ξ, η⟩En Which implies that if a basis in V is orthonormal the scalar product is valid in V From that follows: ∥x∥v = ∥ξ∥En , ∢(x, y)v = ∢(ξ, η)En , x ⊥ y ⇔ ξ ⊥ η RC Gram-Schmidt: ‹ b1 = a1 ∥a1 ∥v ‹ ̃bk = ak − ∑k−1 j=1 ⟨bj , ak⟩v · bj ‹ bk = ̃bk ∥ ̃bk∥v ex A = 2 3 2 4 1 1 −→ 1 a1 = 2 2 1 / ∥ ∥ ∥ 2 2 1 ∥ ∥ ∥ = 2/3 2/3 1/3 , ̃a2 = 3 4 1 − 〈 2/3 2/3 1/3 , 3 4 1 〉 = −1/3 2/3 −2/3 , a2 = −1/3 2/3 −2/3 / ∥ ∥ ∥ ∥ −1/3 2/3 −2/3 ∥ ∥ ∥ ∥ = −1/3 2/3 −2/3 ↦→ A = 2/3 −1/3 2/3 2/3 2/3 −2/3 S 6.6: After k-steps the set {b1, · · · , bk} is pairwise orthonormal.{b1, · · · , bk} is a basis ⇔ {a1, · · · , ak} is a basis Every vectorspace (̸= ∞) has a orthonormal basis Cor 6.7: To a vector space with scalar product with finite or countably infinite many dimensions a orthonormal basis exists. Def orthogonal complement: U ⊥ is the orthogonal complement of a subspace U . U ⊕ U ⊥ = V S 6.9: For a complex matrix with rankA = r it holds: ‹ N (A) = ℜ(A H ) ⊥ ⊂ E n ‹ N (A H ) = ℜ(A) ⊥ ⊂ E m ‹ N (A) ⊕ ℜ(A H ) = E n ‹ N (A H ) ⊕ ℜ(A) = E m ‹ dimℜ(A) = r ‹ dimℜ(A H ) = r ‹ dimN (A) = n − r ‹ dimN (AH ) = m − r Those are the fundamental subspaces Change of Basis B, B′ are orthonormalbasis. Hence: b′ k = ∑j=1 n Tjkbj Matrix for change of basis T : T −1 = T H since both basis are orthonormal. Therfore it holds that: ‹ ξ = T ξ′ ‹ ξ′ = T −1ξ ‹ B =, B′T ‹ B′ =, BT H S 4.13: Let ξ= (ξ1 · · · ξn)T be a coordinate vector of an arbitrary vector v ∈ V with respect to the old basis . Let ξ’= (ξ′ 1 · · · ξ′ n) T be the new representation of a vector x with respect to the new basis. x = ∑n i=1 ξibi = ∑n k=1 ξ′ kb ′ k All matrices are unitary/orthogonal Cor 6.12: ⟨x, y⟩v = ξH η = ⟨ξ, η⟩v = 〈ξ′, η′〉 v = ξ′H η′ ⇒ T is conformal (l¨angen-winkeltreu) Note Convention: A representation of a vector with respect ot the basis B1 is written as [v]B1 Therefore: [v]B2 = M at(B1)B2 [v]B1 and [v]B1 = M at(B2)B1 [v]B2 where M at(B2)B1 is the matrix of change ob basis from B1 to B1 Hence: M at(B1)B2 = ([b1]B2 |· · · | [bn]B2 ) RC Calculating the matrix of F with respect to Basis B: We have a function F : X ↦→ X and the basis of X := B: 1 calculate for all ∀a ∈ B: F (ba) 2 solve for all F (ba) = αab1 + βab2 · · · γabn 3 write coordinate vectors as: ξa = (αaβa · · · γa) T 4 write matrix as F[B] = ξ1 |··· | ξn RC Compute Basistransformationsmatrix from B to S : Since S is a standardbasis we have: 1 S → B is given by B = (s1|s2| · · · |sn) (Columns of B are the basis vectors of B) 2 Compute inverse of B to get B → S RC Basistransformationsmatrix from 2 × 2 matrices with respect to the standard basis: The standard basis of 2 × 2 matrices is given by: S = {(1 0 0 0 ) , (0 1 0 0 ) , (0 0 1 0 ) , (0 0 0 1 )} and the new Basis is defined by: B ={(a b c e ) , (e f g h ) , ( i j k l ) , (m n o p )} Then the matrix (Abbildungsmatrix) is defined by F =    a e i m b f j n c g k o d h l p    RC Prove that B is Basis: S denotes the standard basis. 1 compute for all ∀a ∈ B: ba = αs1 + βs2 · · · γsn 2 Since span(B) = span(S), B has to be ba- sis. RC Prove that F is a bijective mapping F : X ↦→ Y with the basis X for X and Y for Y : 1 calculate for all ∀a ∈ X : F (xa) 2 solve for all F (xa) = αay1 + βay2 · · · γayn 3 write coordinate vectors as: ξa = (αaβa · · · γa) T 4 write matrix as F[X ] = ξ1 |··· | ξn 5 As F[X ] is quadratic and has full rank we have that dim(X ) = dim(Y) and thus by Cor.5.8 that F is bijective unitary/orthogonal mapping Def unitary: A linear mappingF : X ↦→ Y is unitary/orthogonal if ⟨F (v), F (w)⟩y = ⟨v, w⟩x S 6.13: 1 F is isometric (l¨angentreu): ∥F (v)∥y = ∥v∥X 2 F is conformal (winkeltreu): v ⊥ w ⇔ F (v) ⊥ F (w) 3 kerF = {0}, F is injective ‹ if n = dimX = dimY < ∞ 4 F is isomorphism 5 {b1, · · · , bn} is orthonormal basis of X ⇔ {F (b1), · · · , F (bn)} is a orthonormal basis of Y 6 F −1 is unitary/orthogonal 7 The mapping matrix (Abbildungsmatrix) A is unitary/orthogonal Least Squares Let Ax = b a overdetermined SLE ( Equations ¿ Variables). No exact solution exists. → x∗ = argminx∈En ∥Ax − b∥2 2 ⇒ (Ax − b) ⊥ ℜ(A) Def Pseudoinverse: If Rank A = n: A+ = (AH A) −1A H ⇒ A +A = I Def normalequations: (A T A)x = AT y RC Least Square Method for functions: We assume that ker(A) = {0} and AH A is reg- ular 0 bring problem in a form where every- thing is numerically determined except the coefficients 1 calculate A T y 2 calculate A T A 3 solve the equation (AT A)x = A T y 4 calculate error r = y − Ax ex The equation is given y(t) = x1t + x2t2 We have tn(1, 2, 3, 4) and y(t)n = (13.0, 35.5, 68.0, 110.5) −→ 0 A = 1 1 2 4 3 9 4 16 , y = 13.0 35.5 68.0 110.5 −→ 1 AT y = 730.0 2535.0 −→ 2 A T A = 30 100 100 354 −→ 3 30 100 100 354 ∣ ∣ 730 2535.0 ↦→ x1 = 7.9355, x2 = 4.919 RC Least Square Method for 2D-points: 0 Write X- / Y-coordinate alternately in the form (x, 0), (0, y) in A for every point. Write X- / Y-coordinate alter- nately in y. 1 Rest as usually ex The points P = {(−1, 1), (1, 1), (1, −1), (−1, −1)} should be transformed with respect to the squared distance to the points P ′ = {(0, 2), (1, 3), (0, −2), (−1, −3)}. The transforma- tion is defined as T (P ) = T ( px py ) = ( sx·px sy ·py ) Ax = y ⇒            −1 0 0 1 1 0 0 1 1 0 0 −1 −1 0 0 −1            (sx sy ) =            0 2 1 3 0 −2 −1 −3            RC Least Squares with QR-decomposition: We have the normal equations A H Ax = AT b ⇒ (QR) H (QR)x = (QR)H b ⇒ RH QH QRx = RH QH ⇒ RH Rx = RT Q T b ⇒ Rx = QH b There- fore we have: 1 compute QR-decomposition of A 2 solve Rx = Q T b RC Least Squares with SVD: ∥Ax − b∥2 2 = ∥ ∥ ∥ ∥ ∥ ∥ ∥ ∥ Σ V H x ︸︷︷︸ y − U H b ︸ ︷︷ ︸ c ∥ ∥ ∥ ∥ ∥ ∥ ∥ ∥ 2 2 = ∥Σy − c∥2 2;x∗ = V Σ +U H b ⇒ ∞ solutions, here: smallest 2- norm (y∗ = Σ +U H b) Where Σ + is the pseu- doinverse of Σ, hence it holds Determinants Def : det (a11) = a11, det (a11 a12 a21 a22 ) = a11a22 − a12a21, det  a11 a12 a13 a21 a22 a23 a31 a32 a33   = a11a22a33 +a21a32a13 +a31a12a23 −a13a22a31 −a12a21a33 −a11a23a32 S 8.12: det(A) = ∑n i=1 akiKki = ∑n i=1 ailKil for a fixed k and l. Def cofactor Kki: Kki = (−1)k+idet(A[k,i]) Def A[k,i]: Is defined as the matrix A without the k − th row and i − th column Def : detA = 0 ⇔ A is singular Def : detA ̸= 0 ⇔ A is regular S 8.3: i det(A) is linear in every row ii swapping two rows changes the sign of det(A) iii det(I) = 1 S 8.4: iv if A has a row with 0 ⇒ det(A) = 0 v det(γA) = γndet(A) vi if A has to equal rows ⇒ det(A) = 0 vii adding a multiple of a row to another row doesnt affect the det viii is A a diagonalmatrix: det(A) = ∏n i=1 aii iv is A a triangularmatrix: det(A) = ∏n i=1 aii Cor 8.10: Every statement of Satz 8.3 and 8.4 also holds for columns instead of rows S 8.5: Using Gauss on A results in: det(A) = (−1) v ∏n k=1 rkk where v is # swappings of rows and rkk are the diagonal elements of the row echelon form S 8.7: det(AB) = det(A) · det(B) Cor 8.8: if A is regular ⇒ det(A −1) = 1 det(A) S 8.9: det(AT ) = det(A) and det(AH ) = det(A) Def det of block matrices: det [ A C 0 B ] = det(A) · det(B) Def det of unitary matrices: Let A be unitary/orthogonal |det(A)| = ±1 proof: det(U T U ) = det(I) = (det(U ))2 = 1 ⇒ det(U ) = ±1 Eigenvalues and -vectors Def eigenvector: A number λ ∈ E n is called eigenwert of a linear mapping: F : X if ∃v ∈ V, v ̸= 0 such that F (v) = λv. v is an eigenvector. The set of all eingenvectors, which corespond to λ form a subspace Eλ = {v ∈ V |F (v) = λv} Def spectrum: The set of all eigenvalues of F is called spectrum Def : ξ ∈ E n is a eigenvector of λ ⇔ Aξ = λξ Lem 9.1: A linear map F and its matrix representation have the same eigenvalues and the eigenvectors are connected by the coordinaterepresentation kv Lem 9.2: λ is eigenvalue ⇔ ker(A − λI) is singular (Eλ = ker(A − λI)) Def multiplicity: The geometric multiplicity of λ = dim(Eλ) Def characteristic polynomial: It is defined by XA(λ) = det(A − λI) = 0 Def Trace: tr(A) = ∑n k=1 akk S 9.5: λ ∈ E is eigenvalue of A ⇔ XA(λ) = 0 Lem 9.4: XA(λ) = (−1) n · λ n(−1) n−1 · tr(A) · λ n−1 + · · · det(A)λ 0 = an · n +an−1 · λn−1 + · · · det(A) Lem 9.6: A (quadratic) matrix is singular if and only if it has 0 as an eigenvalue Def algebraic multiplicity: is the multiplicity of an eigenvalue in the char. polynomial. S 9.13: geometric multiplicity ≤ algebraic multiplicity RC Find Eigenvalues and -vectors: 1 find char. polynomial XA(λ) = det(A − λI) 2 find roots of XA 3 for every λk find the solution for (A − λkI)x = 0 S 9.7: for similar matrices C = T −1AT (C and A are similar) holds that tr(A) = tr(C), det(A) = det(C), XA = XC and they have the same eigenvalues S 9.11: Eigenvectors for different Eigenvalues are linearly independant ⇒ max dimV different Eigenvalues Def : λ is eigenvalue for A ⇒ λ q is eigenvalue of Aq Note Trace: trace(A) is equal to the sum of all eigenvalues of A:trace(A) = λ1 + · · · + λn Note Trace: det(A) is equal to the producct of all eigenvalues of A:det(A) = λ1 · · · · · λn Decompositions Spectral-/Eigenvaluedecomposition Def : A = V ΛV −1 (A and Λ are similar) Precondition for diagonalisation : S 9.14: A ∈ C n×n is diagonalisible ⇔ ∀ Eigenvalues (geom. mult. = alg. mult.) S 9.15: If A ∈ C n×n is unitary (A H = A)it holds that: i all eigenvalues are real ii the eigenvectors are pairwise orthogonal iii an orthonormal basis U exists, which consists of all the eigenvectors iv for the unitary matrix U holds that U H AU = Λ Cor 9.16: The previous statements is also valid for real-symmetric matrices RC Eigenvaluedecomposition: 1 find the eigenvalues of A λk and write Λ = diag(λ1 · · · λn) 2 find the according eigenvectors vk of λk write them as V = (v1| · · · |vn) (sorted according to Λ) 3 find inverse V −1 Cor 9.10: If A is diagonisable it can be composed as a sum of 1-rank-matrices : A = ∑n k=1 VkλkwT k with V = (V1 · · · Vn) and V −1 =     wT 1 . . . wT n     from that follows: Avk = vkλk and wT k A = λkwT k : wT is a left eigenvector RC Eigenvaluedecomposition with SVD: The SVD is given by A = U ΣV H 1 Expand U ΣV H ⇒ U IΣV H ⇒ U I1I2ΣV H where U I1 = V and I1I2 = I 2 Calculate (U I1) (I2Σ) V H RC Composition of 1-rank- matrices: 1 write A = V ΛV −1 2 rewrite A = ∑n k=1 VkλkwT k Vk = row (↓) of V , wT k = column (→) of V −1 RC Powers of A: 1 write A = V ΛV −1 2 calculate Am = V Λ mV −1 note: Λ m = diag(am 11, · · · , am nn) Singularvaluedecomposition Def SVD for AH A: Spectral-decomposition exists for every matrix A H A. Since AH A is hermetian (hermetisch) and positive semidefinite.→ A H A has real, non-negative eigenvalues λ ∈ R and λ ≥ 0 Therefore one can rewrite:A H AV = V Λ −−−−→ λ=σ2 A H AVr = VrΣ 2 r ⇒ V H r A H AVr = Σ2 r ⇒ (Σ −1 r V H r A H ) ︸ ︷︷ ︸ =U −1 r (AVrΣ −1 r ) ︸ ︷︷ ︸ =Ur = I Def SVD: SVD exists for every matrix, such that U, V are unitary and Σ is diagonal and positive. A = U ΣV H it follows AAH = U Σ 2 mU H ,AH A = V Σ 2 nV H , A H = V Σ T U H Def A invertible: If A is invertible A −1 = V Σ −1U H S 11.11f: For Rank r it holds that: ‹ {u1, . . . , u1}: Basis of Im(A) = R(A) ‹ {ur+1, . . . , um}: Basis of Ker(AH ) = N (AH ) ‹ {v1, . . . , v1}: Basis of Im(AH ) = R(A H ) ‹ {vr+1, . . . , vm}: Basis of Ker(A) = N (A) Def Automorphism: In a self-image (Selbstabbildung) it holds that A = U ΣV H = V V H U ︸ ︷︷ ︸ R ΣV H = V ︸︷︷︸ 1 R ︸︷︷︸ 2 Σ ︸︷︷︸ 3 V H ︸︷︷︸ 4 1,4 Change to orthonormal basis 2 rotation, mirroring 3 scaling of unit-axes Def singular values: singular values: σi = √λi sorted in descending order σa ≤ σb · · · σr ≤ 0 · · · Def Eigenbasis: V is the orthonormal eigenbasis of AH A such that: AAH = U Σ 2 mU H . Similar for U as eigenbasis of AAH : A H A = V Σ 2 nV H RC SVD of A ∈ E m×nwith AH A: 1 Calculate (AH A) ∈ E n×n 2 find eigenvalue of A H A 3 write Σr = diag( √λ1, · · · , √λn) ∈ E n×n 4 rewrite: Σ ∈ E m×n: Σ = Σr 0 0 0 5 find eigenvectors of AH A ⇒ v1, · · · , vr 6 norm eigenvectors and compute V = ( v1 ∥v1∥ | · · · | vn ∥vn∥ ) ∈ Rn×n 7 solve for U = AV Σ −1 8 if Ur ̸= U : Ur schmidt −−−−−−→ gram U ∈ E m×m 9 write A = U ΣV H RC SVD of A ∈ Em×nwith AAH : 1 Calculate (AAH ) ∈ E m×m 2 find eigenvalue of AAH 3 write Σr = diag(√λ1, · · · , √λn) ∈ E m×m 4 rewrite: Σ ∈ E m×n: Σ = Σr 0 0 0 5 find eigenvectors of AAH ⇒ v1, · · · , vr 6 norm eigenvectors and compute V = ( v1 ∥v1 ∥ | · · · | vn ∥vn ∥ ) ∈ R n×n 7 solve for U = AV Σ −1 8 if Ur ̸= U : Ur schmidt −−−−−−→ gram U ∈ E m×m 9 write A = U ΣV H RC SVD with Spectral decomposition A = V ΛV −1: One has to sort the singular values of Λ ac- cording to their value. Then one can do: 1 We have U = V, Σ = √Λ 2 Rewrite: A = V √ΛV −1 Cor 11.4: If A ∈ E m×n and rank(A) = r then: eiganvalues of A H A ∈ E m×m and AAH ∈ E n×n are the same but the mulitplicity of the eigenvalue 0 is n − r or m − r Def spectral norm: It is defined as: ∥A∥2 = σ1 QR-Decomposition Def QR-Decomposition: A matrix A can be composed as A = QR where Q is ortohogonal and R is an upper triangular matrix. The decomposition is unique if m ≤ n and Rank(A) = n RC QR-Decomposition: 1 Gram Schmidt on the rows (↓) of A → Q 2 solve R = Q T A → R ex A = 2 3 2 4 1 1 −→ 1 q1 = 2/3 2/3 1/3 , q2 = −1/3 2/3 −2/3 ↦→ Q = 2/3 −1/3 2/3 2/3 2/3 −2/3 −→ 2 R = QT A = 3 5 0 1 Definitions Def nullmatrix: Has in every entry 0 ‹ ∀A(A + 0 = 0 + A = A) (S.2.2) Def diagonalmatrix: Has in every entry 0 except for the diagonal: (D)ij = 0 for i ̸= j one can write Diag(d11, · · · , dnn) ‹ is symmetric ‹ det(A) = A11 · · · · Ann(S.8.4) ‹ A−1 = diag( 1 A11 · · · 1 Ann ) ‹ Am = diag(am 11 , · · · am nn) Def identity: The identity is written as In = Diag(1, · · · , 1) ‹ AI = IA = A ‹ A−1 = I Def upper triangular matrix: (R)ij = 0 for i > j ‹ is nilpotent Def lower triangular matrix: (R)ij = 0 for i < j ‹ is nilpotent Def Zerodiviser: If AB = 0 ⇔ A,B Zerodiviser, Nullteiler Def symmetric: AT = A ⇔A symmetric ‹ A and B symmetric ⇒ AB = BA ⇔ AB is symmetric (S.2.7) ‹ if positiv definit ⇒ regular (L3.7) Def skew-symmetric: AT = −A ⇔A skew-symmetric ‹ tra(A) = 0 ‹ if A has odd order – det(A)=0 – do inverse exists – A is singular if A has even order ‹ inverse is skew-symmetric if it exists Def hermitian: AH = A ⇔A hermitian ‹ if positiv definit ⇒ regular (L.3.7) Def unitary/orthogonal: AH A = I ⇔A is unitary/orthogonal ‹ A is regular (S.2.20) ‹ A−1 = AH (S.2.20) ‹ A−1 is unitary (S.2.20) ‹ A and B is unitary ⇒ AB is unitary (S.2.20) ‹ det(A) = ±1 Multiple Choice General C If the solutions of an SLE are x1 = 0, x2 = 0, x3 = 1 the system has infinite many solutions C Let A be a real 2 × 4 matrix with rank 2. Then the SLE Ax = b has a non-trivial solution W If A is invertible it holds: ABA−1 = B C En×n → E, A ↦→ trace(A) is linear W En×n → E, A ↦→ det(A) is linear C Let D ∈ E2×2, dim(KerD) = 2 only if D = 0 0 0 0 C If A2 is invertible, so is A3 det(A2) ̸== 0 ⇒ det(A) ̸= 0 ⇒ det(A3 ) ̸= 0 C If A is regular and A2 = A,then A = I W For linear dependant x, y, z it holds x = αy + βz Only x, y can be dependant C If A and A2 ∈ En×n and A2 is regular, A3 is invertible C ∀x ∈ Rn , ∥Ax∥2 ≤ ∥A∥2 ∥x∥2 W Let f : Rn → R, f (x) := ∥Ax∥2 The function f is a norm in Rn W ∥AB∥2 ≤ ∥A∥2 Given are the orthogonal matrices A and B with the same dimension. Which of the following properties is true? W The matrix product AB is orthogonal, but BA is not orthogonal W The matrix product BA is orthogonal, but AB is not orthogonal. C The matrix product AB and the matrix product BA are orthogonal W The matrix product AB and the matrix product BA are not orthogonal It holds that AT = A−1 BT = B−1 and further (AB)T = BT AT = B−1 A−1 = (AB)−1 and also vice versa (BA)T = (BA)−1 We have Rn with the standard scalar product ⟨·, ·⟩ and 2-norm. Let A be a real n × n matrix. Which statements are correct C ∀x, y ∈ Rn it holds that 〈 x, AT y〉 = ⟨Ax, y⟩ C AT = A−1 ⇒ ∀x, y ∈ Rn ⟨Ax, Ay⟩ = ⟨x, y⟩ C AT = A−1 ⇒ ∀x ∈ Rn ∥Ax∥ = ∥x∥ C Let B be another real n × n matrix. AT = A−1 and BT = B−1 ⇒ the inverse of AB exists and it is: (AB)−1 = (AB)T Given are orthogonal matrices A ∈ Rn×n and B ∈ Rn×n. Which of the following statements are correct? C The matrix AT is orthogonal W The matrix A + B is orthogonal W The matrix A + AT is orthogonal C The matrix AB−1 is orthogonal Given is a lower triangular matrix A ∈ R3×3 whose entries are non-negative integers and whose entries either occur only once or are equal to zero. Which of the following options are possible for the value of the determinant det(A)? W 5 W 7 W −2 C 35 A is triangular⇒ det(A) = a1,1a2,2a3,3 ⇒ so det(A) = 0 or det(A) = a1,1a2,2a3,3 The dimensions of the subspace of all skew-symmetric real 3 × 3 matrices is: W 1 C 3 W 6 W 9 Let A ∈ R2×3 and b ∈ R2 . Assume a solution for Ax = b exists C Ax = b has always ∞ solutions min 1 free variable W The set of the solution (L¨osungsmenge) of Ax = b forms a line in 3D could also be a plance, 1 or 2 free variables C Geometrically Ax = b coresponds to an intersection of two planes in 3D Rank W Let B ∈ E3×1 and C ∈ E1×3: BC can have rank 3. Vectorspaces W Let V be a vector space over R with scalar product ⟨·, ·⟩ and let F : V ↦→ V be a linear map. If it holds that ∀v ∈ V, ⟨v, F (v)⟩ = 0, then F is necessarily the null map, i.e., F (v) = 0 for all v ∈ V . W Given a vector space V with a norm ∥·∥. For all u, v ∈ V , we have ∥v∥ ≤ ∥v + u∥. counterexample: u = −v = (1, 1)T C Let S ⊂ V and W be a subspace of V : S ⊂ W ⇒ span(S) ⊂ W C In a vector space of finite dimension with scalar product, one can complete any set of orthonormal vectors to form an orthonormal basis. C A vector space of finite dimension with scalar product has an orthonormal basis. Corollary 6.7 W Consider the vector space Rn with the Euclidean scalar product. The scalar product of two unit vectors can be arbitrarily large. Cauchy Schwarz, S 6.1: ⟨v, w⟩2 ≤ ⟨v, v⟩ ⟨w, w⟩ = 1 · 1 = 1 W We again consider Rn with the Euclidean scalar product. Can we find any number of pairwise orthogonal unit vectors in this vector space? Let V the standardvectorspace of all 2 × 2 matrices. Which of the following are subspaces of V ?(B = 1 2 3 4 ) W {A ∈ V|A is invertible} W {A ∈ V|A2 = 0 } C {A ∈ V|AT = A } C {A ∈ V|AT B = BA } Consider the vector space F of functions of R ↦→ R with the operations addition(f + g)(x) = f (x) + g(x) and scalar multiplication (λf )(x) = λf (x). This includes the subspace P 2 = {a0 + a1x + a2x2|ai ∈ R} of polynomials of degree ≤ 2. Which of the following statements are correct? C Span{x + 1, x1, x2 + 1, x21} is equal to P2 . W x + 1, x1, x2 + 1, x21 ∈ P2 are linearly independent W x + 1, x1, x2 + 1, x2 1 ∈ P2 form a generating set (spanning set) of F . C x + 1, x1, x2 + 1, x2 1 ∈ P2 form a generating set (spanning set) of P2. W The polynomials x + 1, x1, x2 + 1, x21 ∈ P2 form a basis of P2 Let V, W be finite dimensional vector spaces over a space K. Let F : V ↦→ W be a linear mapping and (v1 , · · · , vn) a basis of V . Then it holds that: W F (v1), · · · , F (vn) are linearly independent if F is surjective C F (v1 ), · · · , F (vn) are linearly independent if F is injective C F (v1 ), · · · , F (vn) form a generating end system if F is surjective W F (v1 ), · · · , F (vn) form a generating end system if F is injective C F (v1 ), · · · , F (vn) form a basis if and only if F is an isomorphism Let V, W be two real vector spaces with scalar products, let B be an orthonormal basis of V and let F : V ↦→ W be an orthogonal mapping. Which of the following statements is true? W F is an isomorphism. F is only an isomorphism if dimV = dimW < ∞ C ∥F (v)∥W = ∥v∥v for all v ∈ V . Follows directly from the definition of the scalar product induced norm and the orthogonality of F. W If dimV, dimW < ∞, it is possible that dimV > dimW With dimV > dimW there is no orthogonal mapping F : V ↦→ W . W F is not injective. C is angle-preserving, i.e., for all v, w ∈ V it holds that ∢(F (v), F (w)) = ∢(v, w). C F is an isomorphism to the image of F . F is injective as shown before and obviously surjective to its image. Since the inverses of linear mappings are also linear, F is therefore an isomorphism W The set of images F (B) is an orthonormal basis of W . Since F is not necessarily an isomorphism, the image space can be smaller than W . C The set of images F (B) is an orthonormal basis of Im(F )This follows from the isomorphism property of F from the question above C If it exists,F −1 : Im(F ) ↦→ V is orthogonal. It exists as seen above. The orthogonality of the inverse follows directly from the definition of orthogonal mappings. Det W Let Q be unitary and A ∈ En×n , det(QA) = det(A) |detQ| = ±1 C if A, B, P ∈ En×n and P is invertible with A = P BP −1 then: det(A) = det(B) Given is a matrix A ∈ Rn×n with entries aij = ij and n > 1. Which statement is correct? W det(A) = 1 C det(A) = 0 W det(A) = (−1)n W det(A) = (−2)n Which of the following statements are not correct for arbitrary n × n-matrices A and B? C det(A + B) = det(A) + det(B) W det(AB) = det(BA) W If A is singular then AB is also singular W det(AAT A) = (det(A))3 Let A, B ∈ Rn×n with AB = −BA C det(AB) = det(−BA) W det(A)det(B)) = −det(A)det(B) n has to be even ⇒S8.4 v W Either A or B has a zero-determinant W A and B have to be singular W ABx = 0 has more than one solution (L¨osungsschar) C ABx = c can have no, one and ∞ many solutions, if c ∈ R2 , c ̸= 0 W It has to be A = 0 or B = 0 Which of the following statements are correct for an arbitrary n × n-matrix A and for arbitrary n? W det(2A) = 2det(A) W det(−A) = det(A) C det(A4) = det(A)4 W Let A be a triangular matrix with the property ai,j = 0f ori + j > n + 1 (so there are zeros at the bottom right). The determinant can be calculated using the formula det(A) = a1,n · a2,n1 · · · an,1 . Let A, P, Q ∈ Rn×n where P is permutation matrix and Q is a unitary matrix. W det(P A) = det(A) C det(P AP ) = det(A) W det(QA) = det(A) Eigenvalue/Eigenvectors C with XA (λ) = (λ − 1)3 + 3 is A ∈ E3×3 invertible 0 isn’t eigenvalue ⇒ A is regular W Let v1 and v2 be eigenvectors of A, so is v1 + v2 an eigenvector C If A ∈ En×n and XA(λ) = (λ − 1)n + 2, A is invertible C Similar matrices have the same eigenvalues W Similar matrices have the same eigenvectors W Every n × n matrix has linear independant eigenvectors W Eigenvectors, which correspond to the same eigenvalue are always linear dependant C If a real matrix has a eigenvector, it follows that the matrix has infinity many eigenvectors C Every rotation in R3 has the eigenvalue λ = 1 W If λ1 with v and λ2 with w, so is (λ1 + λ2) a eigenvalue with eigenvector v + w Let A ∈ R3×3 with eigenvalues λ1, λ2 , λ3 C A is diagonisable if all eigenvalues are different W if A is diagonisable, all eigenvalues have to be different W A is diagonisable if it has 3 eigenvectors They have to bo linearly independant C If λ1 = 2, λ2 = −2, λ3 = 1 and B = A3 − 3A3 then is B diagonisable W If AP = P D and D is a diagonalmatrix then the columns of P are eigenvectors of A Only if the eigenvalues of A are on the diagonal of D Let A ∈ Rn×n be positiv definite and symmetric. Further, let λ1 , · · · , λn be the eigenvalues to the eigenvectors v1, · · · , vn W A2 has at least one eigenvalue with a strictly positive imaginary part C it holds that λj > 0 for all j = 1, · · · , n W A hast at least one eigenvalue which satisfies: geom.mult. < alg.mult W The eigenvalues are pairwise distinct: λj ̸= λi, if j ̸= i C There exist positiv real numbers α > 0 such that vT Av ≥ vT v for all v ∈ Rn Let A ∈ E2×2 with Rank(A) = 1 and T race(A) = 5. What are the eigenvalues W 1 is an eigenvalue C 0 is an eigenvalue W 2 is an eigenvalue W −5 is an eigenvalue C 5 is an eigenvalue Since det(A) = 0 and trace(A) = 5 ⇒ 0 = λ1 · λ2 and 5 = λ1 + λ2 Decompositions C A matrix A ∈ Rn×n with n eigenvalues has 2n normed spectral decompositions since the sign can be changed n-times Let A ∈ Rm×n m ≥ n be a matrix with rank k. Denote the QR-decomposition of A as A = QR, where Q ∈ Rm×k has orthonormal columns, and R ∈ Rk×n is an upper (right) triangular matrix. Which one of the following statements is always true? W Rank A < Rank R W QQT = I W If A has linearly independant columns, we have Rank R = m C If A has linearly independant columns, we have Rank R = n Let A ∈ Rm×n with linearly independant columns and A = Q1R1 = Q2R2, two QR-Decompositions of A C QT 1 Q2 is orthogonal C QT 1 Q2 is a upper and lower triangular matrix and therefore a diagonal matrix W QT 1 Q2 = I W Rank(R1) = m C Rank(R1) = n W Rank(R2) = m C Rank(R2) = n C QT 1 Q2 is regular Let A, B ∈ Rn×m, B is regular and B = QR W f : Rn ↦→ R, x ↦→ ∥Ax∥2, is a norm in Rn C If ̸ ∃k, such that Ak is invertible, so is A not invertible C Vx ∈ Rn, ∥Ax∥2 ≤ ∥A∥2 · ∥x∥2 W det(B) = det(R) C ∥B∥2 = ∥R∥2 C g : Rn ↦→ R, x ↦→ ∥Qx∥2 , is a norm in Rn W AB is regular, but BA is not necessarily W ∥AB∥2 ≤ ∥A∥2 Basis W The transformation matrix of a basis transformation between orthonormal bases is the identity matrix. The transformation matrix of base transformation between orthonormal bases is orthogonal, the identity matrix is only one possibility. C The inverse of the transformation matrix of a base transformation between orthonormal bases is its Hermitian transpose. C A ∈ Rn×n is an orthogonal matrix if and only if its columns form an orthonormal basis of R with respect to the Euclidean scalar product. C The change of basis matrix is unitary (if E = C) or orthonormal (if E = R) if both bases are orthonormal. Procedures W The Gram-Schmidt orthogonalization method can be used to compute an equally large set of linearly independent vectors from a set of linearly dependent vectors. W Let v1, · · · , vn ⊂ Rn be a set of n vectors. Using the Gram-Schmidt process, we can always produce n unit-length and pairwise orthogonal vectors. Gram Schmidts needs linear independant vectors Let A ∈ Rn×n, m < n. Let Ax = b be a system of linear equations and let x be a solution in the least squares sense. Which statement is always correct? W The vector (bAx) is orthogonal to the row space of A. C The vector (bAx) is orthogonal to the column space of A. ⇒ normal equations W x is in the null space of A. W The solution x does not always exist Kernel/Image W If the nullspace of an 8 × 7 matrix is 5-dimensional, the rowspace has dimension 3 n − dim(Ker(A)) = 7 − 5 = 2 = dim(Im(A)) Let A ∈ Rm×n be such that Ax = 0 has only the trivial solution. Then it holds that: C dimIm(A) = n W dimIm(A) = 1 C dimKer(A) = 0 W dimKer(A) = 1 The kernel of A is exactly the solution set of the system of equations Ax = 0. Since Ax = 0 has only the trivial solution, dimKer(A) = 0. Furthermore, it holds that dimKer(A) + dimIm(A) = n. Therefore dimIm(A) = n. Which of the following statements with A ∈ Rn×n is generally true C im(A) = im(2A) C ker(A))ker(2A) W im(A) = im(A2) W im(A) = im(A + I) W im(A) = im(AT ) W ker(A))ker(A2) W ker(A))ker(A + I) W ker(A))ker(AT ) Proofs 1) Prove that AH A and AAH have the same eigenvalues We have that AH A and AAH are similar with T = A and T −1 = AH Therefore by Satz 9.7 they have the same eigenvalues (and also the same trace and det) 2) Let Q ∈ En×n be an orthogonal matrix. Prove that, if n is odd, that at least one of the matrices (Q + I) and (Q − I) singular. Let KQ (x) be the characteristic polynomial of Q. λ is an eigenvalue of Q ⇔ λ is a root of KQ (x) Q is orthogonal ⇒ |λ| = 1 By Lemma 9.2 we have: λ is eigenvalue ⇒ (A − λI) is singular therefore with λ = ±1 at least one of them has to be singular 3) Prove that for an orthogonal matrix Q it holds that ∥Qx∥2 = ∥x∥2 ∥Qx∥2 1 = √ ⟨Qx, Qx⟩ 2 = √ (Qx)T Qx S.2.6 = √xT QT Qx S.2.20 = √xT Ix 2 = √ ⟨x, x⟩ 1 = ∥x∥2 1 = def of norm, 2 def of scalar product 4) Prove that, if λ is an eigenvalue of orthogonal Q then λ = ±1 Qv = λv(1) ⇔ ∥Qv∥ = ∥λv∥ 2 ⇔ ∥v∥ = ∥λv∥ N2 ⇔ ∥v∥ = |λ| ∥v∥ ⇔ |λ| = 1 1 = def eigenvalue, 2 = as proven before, N2 = norm is homogeneous 5) Prove that for a arbitrary matrix A with its eigenvalue λ it holds that (A − λI) is singular Let v be the eigenvector to the coresponding λ in (A − λI) (A − λI)v = (Av − λIv) = Av − λv 1 = λv − λv = 0v As v is eigenvector we have v ̸= 0(2) ⇒ λ = 0 L.9.6 ⇔ (A − λI) is singular 1 = as v is eigenvalue, 2 = def eigenvalue 6) Let A ∈ Rn×n be a real matrix and x ∈ Rn be a vactor.Prove that: ∥Ax∥2 ≥ σmin ∥x∥2. Where σmin is the smallest singular value of A Let A = U ΣV T be the SVD of A ∥Ax∥2 = ∥ ∥ ∥U ΣV T x∥ ∥ ∥2 1 = ∥ ∥ ∥ΣV T x∥ ∥ ∥2 2 ≥ ∥ ∥ ∥Σmin V T x∥ ∥ ∥2 = ∥ ∥ ∥σminIV T x ∥ ∥ ∥2 N2 = |σmin | ∥ ∥ ∥IV T x ∥ ∥ ∥2 = ∥ ∥ ∥V T x ∥ ∥ ∥2 3 = |σmin | ∥x∥2 1 = U is orthogonal and proof 3, 2 = Σmin = diag(σmin · · · ), 3 = V T is orthogonal and proof 3 7) Prove that for a regular matrix A the two SLE Ax = b and AT Ax = AT b yield the same solution AT Ax = AT b 1 ⇒ A−T AT Ax = A−T AT b ⇒ Ax = b 1 = Since A is regular det(A) ̸= 0 S.8.9 ⇒ det(AT ) ̸= 0, hence AT is regular as well 8) For A ∈ Rn×n holds that A = AT . Prove that all eigenvalues of A2k k ∈ N are not negative. Lets define the SVD as follows: A = V ΣV −1 Further we know A is orthogonal A = V ΣV −1 S.2.20 ⇒ A = V ΣV T One has to prove inductively that An = V Σn V T B.C for n = 1 it holds that: A = V Σ1 V T I.H Assume it holds for any k ∈ N I.S k ↦→ k + 1: Ak+1 = AAk I.H = AV Σk V T n=1 = V ΣV T V Σk V T S.2.20 = V ΣΣk V T = V Σk+1V T The eigenvalue for An can be found in the diagonal of Σn. With σq as the original eigenvalues of A, one can write: Σ2k = diag(σ2k 1 · · · σ2k n ) Therefore no eigenvalue can be negative. 9) Prove that AH Ax = AH b has infinitely many solutions. AH Ax = AH b ⇒ AH (Ax − b) = 0 ⇒ AH (Ax − (b⊥ + b∥ )) = AH ((Ax − b⊥ ) − b∥ ) 1 = AH (−b⊥) ⇒ −(AH b⊥ ) 2 = 0 1 as b⊥ ∈ R(A) it follows: Ax − b⊥ = 0 2 b ∈ N (AH ) Therefore at least one solution exists. Since rank(A) < n ⇔ dim(N (A)) > 0 and as every solution of the system is in Sp + αSh for any α. Where Sp is a arbitrary particular solution and Sh is the homogeneous solution. 9) Prove Satz 9.7. Namely, proof for any two similar matrices that the characteristict polynomial and det is equal Assume A and C are similar. Therefore we have that C = T −1 AT . Characterstic Polynomial: XC (λ) 1 = (C − λI) 2 = det(T −1(AT − λT )) S.8.7 = det(T −1)det(AT − λT ) = det(AT − λT )det(T −1) S.8.7 = det((AT − λT )T −1) = det(AT T −1 − λT T −1) = det(A − λI) 1 = XA (λ) 1 = def of characteristic polynomial 2 = since C = T −1AT Det: det(C) 1 = det(T −1AT ) S.8.7 = det(T −1 ) · det(A) · det(T ) = det(T −1 ) · det(T ) · det(A) C.8.8 = 1 · det(A) = det(A) 1 = def of C = T −1AT 10) let V and W be two vectors spaces. Let ϕ : V ↦→ W be a linear mapping. Show, that Im(ϕ) is a subspace of W 1 im(ϕ) isn’t empty: 0 = ϕ(0) since ϕ is linear 2 For x, y ∈ Im(ϕ) holds that x + y ∈ im(ϕ): ∃a, ∃b such that ϕ(a) = x and ϕ(b) = y Therefore, x + y = ϕ(a) + ϕ(b) = ϕ(a + b) ∈ Im(ϕ) since ϕ is linear 3 For x ∈ Im() and α ∈ R holds that αx ∈ Im() : ∃a, such that ϕ(a) = x Therefore, αx = αϕ(a) = ϕ(αa) ∈ Im(ϕ) since ϕ is linear 11) Let B = (1, x, x2) and B′ = (x + 1, x − 1, x2 ).The columns (spalten ) of T are the elements of B′ in the basis B Then TB′ →B =  1 −1 0 1 1 0 0 0 1   by inverting TB→B′ we get TB′ →B =   0.5 0.5 0 −0.5 0.5 0 0 0 1   The mapping matrix D′ is then given by D′ = TB→B′ DT B′ →B =   0.5 0.5 1 −0.5 −0.5 1 0 0 0  ","libVersion":"0.3.2","langs":""}