{"path":"sem3/LinAlg/UE/s/LinAlg-s-u08.pdf","text":"D-INFK Linear Algebra HS 2024 Bernd G¨artner Robert Weismantel Solution for Assignment 8 1. a) For the first datapoint we get t1 = 1 and b1 = 2. Hence, we get the equation α1 + α0 = 2 from f (t1) = b1. We proceed analogously for all i ∈ [5] and obtain the system of linear equations       1 1 2 1 3 1 4 1 5 1       [ α1 α0 ] =       2 3 5 6 8       . b) Since there are 5 equations and only 2 unknows, we generally don’t expect this system to have a solution. In particular, if our data comes from a real world process we cannot expect the data to exactly lie on a line (even if this is a good model for the real world process). c) Let A be the system matrix from the linear system above and let b be the right-hand side. We then get A ⊤A = [1 2 3 4 5 1 1 1 1 1 ]       1 1 2 1 3 1 4 1 5 1       = [55 15 15 5 ] and A⊤b = [1 2 3 4 5 1 1 1 1 1 ]       2 3 5 6 8       = [87 24 ] . Hence, we get the normal equations [ 55 15 15 5 ] [ α1 α0 ] = [87 24 ] . We can now solve this for α1 and α0 using Gauss-elimination or other techniques. One way to do it in this particular case, is to subtract the second row 3 times from the first row to get [10 0 15 5 ] [ α1 α0 ] = [15 24 ] . We then conclude α1 = 3 2 and α0 = 3 10 . In particular, we conclude that the line f (t) = 3 2 t + 3 10 explains our data best (among all lines). 2. a) We want to find α0, α1 such that m∑ k=1 λk(bk − (α0 + α1tk)) 2 is minimized. We rewrite this to m∑ k=1 λk(bk − (α0 + α1tk)) 2 = m∑ k=1(√ λkbk − √ λk(α0 + α1tk)) 2 = ∥ ∥ ∥ ∥ Λ 1 2 b − Λ 1 2 A [ α0 α1 ] ∥ ∥ ∥ ∥ 2 , 1 where Λ 1 2 =       √λ1 0 . . . 0 0 √λ2 . . . ... ... . . . . . . 0 0 . . . 0 √λm       . Using Fact 5.3.1 with b′ := Λ 1 2 b and A′ := Λ 1 2 A, we therefore get that the optimal choice for α0, α1 satisfies (A′) ⊤A′ [ α0 α1 ] = A′⊤b ′. Now observe that A′⊤A′ = A⊤(Λ 1 2 )⊤Λ 1 2 A = A⊤ΛA which is invertible by assumption. Thus, we conclude that [α0 α1 ] = (A⊤ΛA)−1A⊤Λb. b) By Lemma 5.2.4, A⊤ΛA is invertible if and only if Λ 1 2 A has linearly independent columns. Similarly, A⊤A is invertible if and only if A has linearly independent columns. Thus, it suffices to prove that Λ 1 2 A has linearly independent columns if and only if A has linearly independent columns. Observe that Λ 1 2 has inverse Λ− 1 2 =        1√λ1 0 . . . 0 0 1√λ2 . . . ... ... . . . . . . 0 0 . . . 0 1√λm        since λi > 0 for all i ∈ [m]. We can use this to get Ax = 0 ⇐⇒ Ax = Λ− 1 2 0 ⇐⇒ Λ 1 2 Ax = 0 which directly implies that A has linearly independent columns if and only if Λ 1 2 A has lin- early independent columns. 3. The linear transformation given by A = I − 2vv⊤ corresponds to reflection along the hyperplane H = {x ∈ R2 : x · v = 0}. To see this, recall from Theorem 5.2.6 that the projection matrix for projection onto Span(v) is given by B = vv⊤ ||v||2 . Since v is a unit vector, this simplifies to B = vv⊤. In particular, this means that A = I − 2B applied to some vector x ∈ R2 will subtract the projection of x onto Span(v) twice from x itself. In other words, assume we split x into two parts x = x| + x⊥ with x| = Bx and x⊥ · v = 0. Then the transformation given by matrix A maps x to the vector Ax = (I−2B)(x|+x⊥) = x|+x⊥−2Bx|−2Bx⊥ = x|+x⊥−2x|−2vv⊤x⊥ = x⊥−x|−0 = x⊥−x|. A picture of this is provided in Figure 1. 4. a) We obtained the new datapoints from the old one by shifting them along the t-axis. Imagine a line that optimally fits the old datapoints. By shifting this line the same amount in the direction of the t-axis, we should get a line that optimally fits the new datapoints. In other words, it should be possible to obtain the optimal line for the new datapoints from the optimal line for the old datapoints by a shift along the t-axis. This should also work the other way around, i.e. given an optimal line for the new datapoints we should be able to shift the line by −c in the direction of the t-axis to get an optimal line for the old datapoints. 2 vxAxx⊥x|Hx⊥−x| Figure 1: A sketch of the transformation. It remains to think about this shifting operation in terms of the parameters ααα and ααα′. The main insight here is that shifting a line does not change its slope. Hence we expect to have α1 = α′ 1 but not α0 = α′ 0. b) We compute the scalar product and check that it yields zero. Concretely, we have    1 ... 1   ·    t′ 1 ... t′ m    = m∑ k=1 t ′ m = m∑ k=1(tm+c) = m∑ k=1(tk− 1 m m∑ i=1 ti) = m∑ k=1 tk−m 1 m m∑ i=1 ti = m∑ k=1 tk− m∑ i=1 ti = 0 and hence the columns of A′ are indeed orthogonal. c) We prove that for all ααα′ ∈ R2 and ααα = ααα′ + [cα′ 1 0 ], we have ||A′ααα′ − b||2 = ||Aααα − b||2. From this, it follows that ααα′ is optimal if and only if ααα is optimal. Thus, consider an arbitrary ααα′ ∈ R2 and let ααα = ααα′ + [ cα′ 1 0 ]. We compute ||A′ααα′ − b|| 2 = m∑ k=1(bk − (α′ 0 + α′ 1t ′ k)) 2 = m∑ k=1(bk − (α′ 0 + α′ 1(tk + c)))2 (t ′ k = tk + c) = m∑ k=1(bk − (α′ 0 + α′ 1c + α′ 1tk)) 2 = m∑ k=1(bk − (α0 + α′ 1tk)) 2 (α0 = α′ 0 + cα′ 1) = m∑ k=1(bk − (α0 + α1tk)) 2 (α1 = α′ 1) = ||Aααα − b|| 2 and therefore conclude the proof. d) By the closed form solution and subtask b), we get that the optimal ααα′ satisfies [α′ 0 α′ 1 ] = [ 1 m ∑m k=1 bk (∑m k=1 t′ kbk)/(∑m k=1 t′ k2) ] . 3 By subtask c), the optimal ααα is hence given by [α0 α1 ] = [ 1 m ∑m k=1 bk + c(∑m k=1 t′ kbk)/(∑m k=1 t′ k2) ( ∑m k=1 t′ kbk)/( ∑m k=1 t′ k2) ] where c = − 1 m ∑m k=1 tk and t′ k = tk + c. This is a closed form solution because the right- hand side can be directly computed from the datapoints. 5. Consider an arbitrary linear combination n∑ i=1 αiui + m∑ i=1 βiwi = 0 with scalars α1, . . . , αn and β1, . . . , βm. Observe that we must have n∑ i=1 αiui ∈ U and − m∑ i=1 βiwi ∈ W since U and W are vector spaces (Lemma 4.12) and linear combinations taken in a vector space are again contained in that vector space (Lemma 4.14). But we also know that n∑ i=1 αiui = − m∑ i=1 βiwi which, by our assumption U ∩ W = {0}, implies n∑ i=1 αiui = m∑ i=1 βiwi = 0. Since u1, . . . , un are linearly independent, we conclude α1 = · · · = αn = 0. Similarly, we get β1 = · · · = βm = 0. We conclude that the only way of expressing 0 as a linear combination of u1, . . . , un, w1, . . . , wm is the trivial linear combination, and thus these m+n vectors are linearly independent. 6. Using Theorem 5.1.7 and Corollary 5.1.9, we can decompose x = v + w for some v ∈ C(A⊤) and w ∈ N(A). Now observe that Av = Av + Aw = Ax = b since w ∈ N(A). With x∗ = v this proves existence and it remains to argue uniqueness. Indeed, any distinct x, y ∈ C(A⊤) with Ax = b = Ay must satisfy A(x − y) = 0. This means that x − y ∈ N(A). But since x, y ∈ C(A⊤), we also get x − y ∈ C(A⊤) and thus x = y. We conclude that x∗ is unique. 4","libVersion":"0.3.2","langs":""}