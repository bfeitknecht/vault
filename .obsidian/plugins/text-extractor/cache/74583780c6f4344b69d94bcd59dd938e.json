{"path":"sem2/AuW/UE/u/s/AuW-u04-s.pdf","text":"ETH Z¨urich Institute of Theoretical Computer Science Prof. Rasmus Kyng Prof. Angelika Steger FS 2024 Algorithmen und Wahrscheinlichkeit Theorie-Aufgaben 7 L¨osung zu Aufgabe 1 (a) Wir bezeichnen f¨ur jede Strasse ei = {vi−1, vi} f¨ur i = 1, . . . , k mit Xi die Indikatorvariable, dass an dieser Strasse Blumen spriessen. Die Anzahl der Strassen an der Blumen spriessen ist dann X = X1 + . . . + Xk. Die erwartete Anzahl Strassen an denen Blumen spriessen k¨onnen wir mithilfe der Linearit¨at des Erwar- tungswerts berechnen als E[X] = E[X1] + . . . + E[Xk] = k 2 , wobei wir verwendet haben, dass E[Xi] = 1/2 f¨ur alle i = 1, . . . , k. Die Wahrscheinlichkeit, dass mindestens an 3/4 der Strassen Blumen spriessen (also dass der Spaziergang kein Vergn¨ugen ist), l¨asst sich dann mit Markovs Ungleichung wie folgt absch¨atzen: Pr [ X ≥ 3k 4 ] ≤ E[X] 3k/4 = k/2 3k/4 = 2 3 . (b) Um Chebychevs Ungleichung anwenden zu k¨onnen, berechnen wir zun¨achst Var[X]. Da die Xi unabh¨angig sind, folgt: Var[X] = Var[X1] + . . . + Var[Xk] = k 4 , wobei wir verwendet haben, dass Var[Xi] = 1/4, da Xi eine Bernoulli Zufallsvariable mit Parameter 1/2 ist. Wir k¨onnen nun folgende Absh¨atzung vornehmen: Pr [ X ≥ 3k 4 ] = Pr [X − E[X] ≥ k 4 ] ≤ Pr [ |X − E[X]| ≥ k 4 ] ≤ Var[X] (k/4)2 = 4 k . Anmerkung: Wir haben hier die Absch¨atzung Pr [X − E[X] ≥ a] ≤ Pr [|X − E[X]| ≥ a] ver- wendet, die f¨ur alle Zufallsvariablen X zutriﬀt. In unseren Spezialfall ist X ∼ Bin(n, 1/2) symmetrisch, und wir wissen sogar, dass Pr [X − E[X] ≥ a] = 1 2 Pr [|X − E[X]| ≥ a] , womit wir unsere ﬁnale Absch¨atzung sogar um einen Faktor 2 verbessern k¨onnten. (c) Wir k¨ummern uns zun¨achst nur um unseren eigenen Hund und ¨uberlegen uns, wie wahr- scheinlich es ist, dass er schnieft. Das passiert, wenn an allen k Strassen unseres Spaziergangs Blumen spriessen. Das passiert mit Wahrscheinlichkeit ( 1 2 )k, da alle k Strassen entlang unseres Spaziergangs unabh¨angig voneinander Blumen aufweisen. Sei nun Si die Indikatorvariable, dass der Hund unseres i-ten Freundes schnieft und Sn die Indikatorvariable, dass unser Hund schnieft. Die Anzahl schniefender Hunde ist S = S1 + . . . + Sn, dank der Linearit¨at des Erwartungswert ist die erwartete Anzahl schniefender Hunde E[S] = E[S1] + . . . + E[Sn]. Oben haben wir gesehen, dass E[Sn] = ( 1 2 )k. Mit genau dem gleichen Argument sieht mann, dass E[Si] = ( 1 2 )k f¨ur i = 1, . . . , n − 1. Daraus folgt E[S] = n · ( 1 2 )k. Die Wahrscheinlichkeit, dass es einen schniefenden Hund gibt, ist Pr[S ≥ 1], die wir wie folgt mit der Markov Ungleichung absch¨atzen k¨onnen: Pr[S ≥ 1] ≤ E[S] 1 = n · ( 1 2 )k . Falls k ≥ log2 n + 1, dann ist Pr[S ≥ 1] ≤ 1/2. Also ist die Wahrscheinlichkeit, dass kein Hund shnieft mindestens 1/2. (d) Zuerst merken wir an, dass die Aufgabenstellungen f¨ur n = 1 nicht interessant ist (Spa- zierg¨ange der L¨ange k = 0 sind sicher kein Vergn¨ugen) und nehmen deshalb n ≥ 2 an. Wir k¨ummern uns wieder zuerst um unseren eigenen Hund und versuchen eine noch bessere Schranke als in (a) und (b) zu ﬁnden, dass unser Spaziergang kein Vergn¨ugen wird. Daf¨ur verwenden wir die Chernoﬀ-Ungleichung. Zun¨achst bemerken wir dass X eine Summe un- ab¨angiger Bernoulli-Variablen ist, dass wir also Chernoﬀ anwenden k¨onnen. Dann gilt Pr [X ≥ E[X] + k 4 ] = Pr [ X ≥ (1 + 1 2 ) E[X] ] (1) ≤ e− 1 3 · 1 4 E[X] (2) = e−k/24 (3) = e−1000 log2 n/24 (4) ≤ 2−1000 log2 n/24 (5) ≤ n−1000/24 (6) ≤ n−40 (7) Sei Pi die Indikatorvariable, dass der Spaziergang unseres i-ten Freundes kein Vergn¨ugen wird und P = P1 + . . . + Pn. Analog zu (c) erhalten wir Pr[P ≥ 1] ≤ E[P ] ≤ n · E[Pn] ≤ n · n−40 = n−39. Wenn wir nun n ≥ 2 verwenden, erhalten wir Pr[P ≥ 1] ≤ 2−39 ≤ 2−7 ≤ 0.01, dass heisst mit Wahrscheinlichkeit mindestens 0.99 werden alle Spazierg¨ange ein Vergn¨ugen.","libVersion":"0.3.1","langs":""}