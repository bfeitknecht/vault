{"path":"sem3/LinAlg/UE/s/LinAlg-s-u02.pdf","text":"D-INFK Linear Algebra HS 2024 Bernd G¨artner Robert Weismantel Solution for Assignment 2 1. The rank of A is 2 as we will prove with the following argument. Let vi be the i-th column of A, i.e. vi =    a1i ... ami    for all i ∈ [m]. Observe first that v1 ̸= 0, so the matrix has at least rank 1. Next, observe that there is no λ ∈ R with λv1 = v2: Indeed, any such λ would have to satisfy 2λ = λa11 = a12 = 3 by looking at the first coordinates, and 3λ = λa21 = a22 = 4 by looking at the second coordinates. But these two equations contradict each other: The first one implies λ = 3 2 while the second one implies λ = 4 3 . We conclude that v2 is linearly independent from v1, and thus A has rank at least 2. If m = 2, we are done. Thus, assume now that m ≥ 3. It remains to prove that every other column of A is dependent on the first two. For this, note that by definition of A, we have vj − vi =      a1j − a1i a2j − a2i ... amj − ami      =      (1 + j) − (1 + i) (2 + j) − (2 + i) ... (m + j) − (m + i)      =      j − i j − i ... j − i      for all j, i ∈ [m]. Let now i ∈ {3, . . . , m} be arbitrary. We get vi − v1 = (i − 1)(v2 − v1) which implies vi = (2 − i)v1 + (i − 1)v2 and thus proves that vi is dependent. We conclude that the rank of A is exactly 2. 2. a) We can use our decomposition of A. In particular, we want Ax = (vw⊤)x = v(w⊤x) ! = 0. In particular, it suffices to find a vector x with w⊤x = [w1 w2 w3] x = w1x1 + w2x2 + w3x3 = 0. There are now many possibilities for x. One of them is x1 = w2, x2 = −w1, and x3 = 0. This is a non-zero vector because we have w1 ̸= 0. b) Consider the vector w =   w1 w2 w3  . We claim that L is exactly the set of vectors that are orthogonal to w. In other words, we claim L = {x ∈ R3 : x · w = 0}. Note that {x ∈ R3 : x · w = 0} is a hyperplane since w is non-zero. 1 In order to prove our claim, let us first consider an arbitrary vector x ∈ {x ∈ R3 : x·w = 0}. Because we have x · w = 0, we also get Ax =   v1 v2 v3   [ w1 w2 w3] x =   v1 v2 v3   0 = 0. Hence, we get x ∈ L. For the reverse direction, we proceed with an indirect proof: Consider an arbitrary vector x ∈ R3 that is not orthogonal to w, i.e. x · w = c ̸= 0 for some c ∈ R. Then we have Ax =   v1 v2 v3   [ w1 w2 w3] x =   v1 v2 v3   c =   cv1 cv2 cv3   ̸= 0 since both v1 ̸= 0 and c ̸= 0. In particular, x is not in L. This concludes the proof. 3. a) We start by computing the powers of A. We get A2 =   0 1 0 0 3 1 1 0 3   and A3 =   1 0 3 3 1 9 0 3 1  . Plugging this into the equation, we get B := A 3 + xA2 + yA + zI =  z + 1 x 3 + y 3 + y 1 + 3x + z 9 + x + 3y x 3 + y 1 + 3x + z   ! = 0. In particular, we want to choose x, y, z such that all entries will become zero. From b12 = x we can deduce x = 0, from b13 = 3 + y we can deduce y = −3, and from b11 = z + 1 we get z = −1. In fact, with this choice of x, y, z all entries of B become 0 and hence we found the unique solution. b) Note that one could prove this in a very formal way with induction. We opted for a slightly less but still sufficiently formal proof. Let k ∈ N be arbitrary. If k = 0, then we have (AB)k = I = I 2 = AkBk. Thus, assume now k > 0 and consider the expression (AB) k = AB × AB × · · · × AB ︸ ︷︷ ︸ k times = ABAB · · · AB︸ ︷︷ ︸ k repetitions of AB . Consider now the following algorithm: While there is an appearance of BA in the above expression, replace it by AB. Note that this operation does not change the product because matrix multiplication is associative and we are additionally given AB = BA in this exercise. Moreover, the algorithm must eventually terminate as the number of ways to arrange k A’s and k B’s in a string is finite and the algorithm will never consider the same arrangement twice (to see this, note that the sum of indices of the A’s decreases in every step of the algorithm). Once the algorithm terminates, all A’s must be to the left of the B’s (as otherwise we could find an appearance of BA). Hence, we have (AB)k = AkBk. c) According to the previous question, we have (AB)k = AkBk. We compute (AB)k = AkBk = Ak0 = 0. Therefore, AB is nilpotent with degree at most k. Remark: The nilpotent degree is not necessarily equal to k. If A is nilpotent of degree 5 and we choose B = A2, then B is nilpotent of degree 3. But AB = A3 is nilpotent of degree 2. d) Using distributivity of matrix multiplication, we compute: (I − A)(I + A + . . . + Ak−1) = (I + A + . . . + Ak−1) − A(I + A + . . . + A k−1) = I + A + . . . + Ak−1 − A − A2 − . . . − Ak = I − Ak = I (A is nilpotent of degree k) 2 e) As is often the case, there are many ways to prove this. Here, we will argue by induction over k that the first k columns in T k must be zero for all k ∈ N. • Property: The k first columns of T k are all zero. • Base case: For k = 1, the property is true because by our assumptions on T , the first column of T must be zero. • Induction step: Fix a natural number 1 ≤ k < m and assume that the property is true for this k (induction hypothesis). We prove that the property is true for k + 1. In other words, we prove that the first (k + 1) columns of T k+1 are all zero. For this, we start by splitting T k+1 into T k+1 = T kT . By our induction hypothesis, the first k columns of T k are zero. In particular, we have T k =   | | | | | | 0 0 · · · 0 v1 v2 · · · vm−k | | | | | |   for some vectors v1, . . . , vm−k ∈ Rn. Now recall the column picture AB =   | | | Ab1 Ab2 · · · Abm | | |   for the product of two m × m matrices A and B where b1, b2, . . . , bm are the columns of B. Consider splitting T k+1 as T k+1 = T kT =   | | | T kt1 T kt2 · · · T ktm | | |   where t1, t2, · · · , tm are the columns of T . We want to argue that the first (k + 1) columns T kt1, T kt2, . . . , T ktk+1 of this matrix are zero. Let i ∈ {1, . . . k + 1}. By definition of T , only the first i − 1 entries of column ti can be non-zero. Moreover, we know that T kti is a linear combination of the columns of T k where the coefficients are the entries of vector ti. We just observed that at most the first i − 1 coefficients in this linear combination can be non-zero. At the same time, we know that the first k columns of T k are all zero. Hence, we conclude T kti = 0. 4. a) We take a small detour and first consider an arbitrary vector v that is perpendicular to both x and y. By definition, this means that we have v · x = 0 and v · y = 0. Now consider the scalar product v · (λx + µy). By applying Observation 1.10, we have v · (λx + µy) = λ v · x + µ v · y = 0. In other words, any vector v that is perpendicular to both x and y is also perpendicular to (λx + µy). Now consider an arbitrary row ui from A. The vector ui must be perpendicular to both x and y since we have Ax = 0 and Ay = 0. Hence, ui must also be perpendicular to (λx + µy). b) No, L is not a finite set. Take any non-zero vector x ∈ L (which must exist since |L| ≥ 2). Then every row ui of A must be perpendicular to x. Hence, ui must also be perpendicular to λx for any choice of λ ∈ R by subtask a). In other words, λx is perpendicular to all rows of A. This implies that A(λx) = 0 and hence λx ∈ L. For every choice of λ we obtain a different vector in L. The number of choices of λ is infinite/unbounded, hence L is not a finite set. 3 5. a) In order to determine the rank of A, we have to find out how many of its columns are linearly independent. Let us denote the columns of A by v1, v2, v3, i.e. A =   | | | v1 v2 v3 | | |   . By checking the column vectors in order we quickly see that v2 can be obtained from v1 as v2 = −3v1. But if we try to obtain v3 from v1 we fail, since there is no λ ∈ R such that both 1λ = 3 and −2λ = 0. In other words, the set of vectors {v1, v3} is linearly independent. We found a set of two vectors that is linearly independent and also observed that the set of all three vectors is not linearly independent. Therefore, the rank of A is 2. b) We proceed as in subtask a) and check linear independence of the columns of A. Let us again denote the columns of A by v1, v2, v3, i.e. A =   | | | v1 v2 v3 | | |   . We choose to check the columns in reverse order v3, v2, v1. In particular, we first check whether v2 is independent from v3. Indeed, there is no way of obtaining the 1 in the first coordinate of v2 if we only use v3, hence v2 is linearly independent from v3. It remains to check whether v1 can be obtained as linear combination of v2 and v3. Here we observe that there is no way of obtaining the 2 in the second coordinate of v1 since the second entry of both v2 and v3 is 0. We conclude that the three vectors are linearly independent and hence A has rank 3. 6. a) There are many possible solutions here. For example, we could choose A = [ 1 1 0 1 ] , B = [1 0 1 1 ] with (AB) ⊤ = [2 1 1 1 ]⊤ = [2 1 1 1 ] and A⊤B⊤ = [ 1 0 1 1 ] [ 1 1 0 1 ] = [1 1 1 2 ] . b) Yes, it is possible to find examples where both A and B are symmetric. For example, consider the matrices A = [1 1 1 1 ] , B = [2 0 0 0 ] . We have (AB) ⊤ = B⊤A⊤ = BA = [2 0 0 0 ] [ 1 1 1 1 ] = [2 2 0 0 ] but also A⊤B⊤ = AB = [1 1 1 1 ] [ 2 0 0 0 ] = [ 2 0 2 0 ] . Note that any two symmetric 2 × 2 matrices that do not commute (i.e. AB ̸= BA) work as an example here. Multiple choice Let A be an m1 × n1 matrix and let B be an m2 × n2 matrix for natural numbers m1, n1, m2, n2. For each statement, determine whether it is true or not (regardless of what values m1, n1, m2, n2 take). 4 1. If A2 is defined, then A must be square. √ (a) Yes (b) No Explanation: For matrix multiplication to work, the number of columns of the first matrix has to equal the number of rows of the second matrix. In this case, both the first and second matrix are A which implies that A must be square. 2. If A2 = I, then A = I. (a) Yes √ (b) No Explanation: A possible counterexample is A = [ 0 1 1 0 ]. 3. If A3 = 0, then A = 0. (a) Yes √ (b) No Explanation: A possible counterexample is A = [ 0 1 0 0 ]. 4. If A = [ 1 a 0 1 ], then An = [ 1 na 0 1 ] for all n ∈ N. √ (a) Yes (b) No Explanation: A full proof could be obtained via induction. We will provide an intuitive explana- tion. Consider a 2 × 2 matrix B and consider what happens when we multiply A with B. The first row of AB will be the first row of B, plus a times the second row of B. The second row of AB will be copied from the second row of B. Now if the second row of B is [ 0 1] , this means that left-multiplying with A corresponds to adding a to the entry in the first row and second column. 5. If AB = B for some choice of B, then A = I. (a) Yes √ (b) No Explanation: If B = 0 and A ̸= I, we still have AB = B. 5 6. If both products AB and BA are defined, then A and B must be square. (a) Yes √ (b) No Explanation: A could be a 3 × 2 matrix while B is a 2 × 3 matrix. 7. If both products AB and BA are defined, then AB and BA must be square. √ (a) Yes (b) No Explanation: If both products are defined, we must have n1 = m2 and n2 = m1. Observe that AB is an m1 × n2 matrix while BA is an m2 × n1 matrix. Hence, both must be square. 8. If two columns of A are equal and AB is defined, the corresponding columns of AB must also be equal. (a) Yes √ (b) No Explanation: A counterexample would be A = [ 1 1 0 0 ] and B = [1 0 1 0 ] . 9. If two columns of B are equal and AB is defined, the corresponding columns of AB must also be equal. √ (a) Yes (b) No Explanation: This can be seen from the column picture of matrix multiplication (Figure 1.9 in lecture notes). 10. If two rows of A are equal and AB is defined, the corresponding rows of AB must also be equal. √ (a) Yes (b) No Explanation: This can be seen from the row picture of matrix multiplication (Figure 1.8 in lecture notes). 6 11. If two rows of B are equal and AB is defined, the corresponding rows of AB must also be equal. (a) Yes √ (b) No Explanation: A counterexample would be A = [ 1 1 0 0 ] and B = [1 0 1 0 ]. 12. If A and B are symmetric matrices and AB is defined, AB is also symmetric. (a) Yes √ (b) No Explanation: A counterexample would be A = [ 1 0 0 0 ] and B = [0 1 1 0 ]. 7","libVersion":"0.3.2","langs":""}