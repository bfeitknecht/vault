{"path":"sem3/LinAlg/VRL/extra/strang-linear-algebra.pdf","text":"INTRODUCTION TO LINEAR ALGEBRA Sixth Edition GILBERT STRANG Massachusetts Institute of Technology WELLESLEY - CAMBRIDGE PRESS Box 812060 Wellesley MA 02482 Introduction to Linear Algebra, 6th Edition Copyright ©2023 by Gilbert Strang ISBN 978-1-7331466-7-8 All rights reserved. No part of this book may be reproduced or stored or transmitted by any means, including photocopying, without written permission from Wellesley - Cambridge Press. Translation in any language is strictly prohibited — authonzed translations are arranged by the publisher. BTEX typesetting by Ashley C. Fernandes (info@problemsolvingpathway.com) Printed in the United States of America QA184.578_2023 987654321 Other texts from Wellesley - Cambridge Press Linear Algebra for Everyone, 2020, Gilbert Strang ISBN 978-1-7331466-3-0 Linear Algebra and Learning from Data, 2019, Gilbert Strang ISBN 978-0-6921963-8-0 Differential Equations and Linear Algebra, Gilbert Strang ISBN 978-0-9802327-9-0 Lecture Notes for Linear Algebra, SIAM ebook, Gilbert Strang ISBN 978-1-7331466-4-7 Computational Science and Engineering, Gilbert Strang ISBN 978-0-9614088-1-7 Wavelets and Filter Banks, Gilbert Strang and Truong Nguyen ISBN 978-0-9614088-7-9 Introduction to Applied Mathematics, Gilbert Strang ISBN 978-0-9614088-0-0 Calculus Third Edition, Gilbert Strang ISBN 978-0-9802327-5-2 Algorithms for Global Positioning, Kai Borre & Gilbert Strang ISBN 978-0-9802327-3-8 Essavs in Linear Algebra, Gilbert Strang ISBN 978-0-9802327-6-9 An Analysis of the Finite Element Method, 2008 edition, Gilbert Strang and George Fix ISBN 978-0-9802327-0-7 Wellesley - Cambridge Press Gilbert Strang’s page : math.mit.edu/~gs Box 812060. Wellesley MA 02482 USA | For orders : math.mit.eduw/weborder.php www.wellesleycambridge.com Outside US/Canada: www.cambridge.org Contact gilstrang @gmail.com Select books, India: www.wellesleypublishers.com The website for this book (with Solution Manual) is math.mit.edu/linearalgebra 2020 book : Linear Algebra for Everyone (math.mit.edu/everyone) 2019 book : Linear Algebra and Learning from Data (math.mit.edu/learningfromdata) 2014 book : Differential Equations and Linear Algebra (math.mit.edu/dela) Linear Algebra (Math 18.06) is included in MIT’s OpenCourseWare ocw.mit.edu/courses Those videos (including 18.06SC and 18.065) are also on www.youtube.com/mitocw 18.06 Linear Algebra 18.06SC with problem solutions ~ 18.065 Learning from Data The cover design was created by Gail Corbett and Lois Sellers : Isellersdesign.com Kenji Hiranabe contributed The Five Factorizations (back cover) and Matrix World Table of Contents Preface \\ 1 Vectors and Matrices 1 1.1 Vectors and Linear Combinations . . . . . .. ... ............ 2 1.2 Lengths and Angles fromDotProducts. . . . . . ... .......... 9 1.3 Matrices and TheirColumnSpaces . . . . .. ... ............ 18 1.4 Matrix Multiplication ABandCR . . ... ... ............ 27 Solving Linear Equations Az = b 39 2.1 Elimination and Back Substitution . . . . . ... .. ... ... ... .. 40 2.2 Elimination Matrices and Inverse Matrices . . . . . . ... ... ... .. 49 2.3 Matrix Computationsand A=LU .. .................. 57 2.4 Permutations and Transposes . . . . . .. .. .. ... ... .. ..., 64 2.5 Derivatives and Finite Difference Matrices . . . . . ... ... .. .. .. 74 The Four Fundamental Subspaces 84 3.1 Vector Spacesand Subspaces . . . . . ... ... ... .. 000 85 3.2 Computing the Nullspace by EliminationnA=CR . . . . . ... .. .. 93 3.3 The Complete Solutionto Az =b ... .................. 104 3.4 Independence, Basis,and Dimension . . . . . . ... ... ... ..., 115 3.5 Dimensions of the Four Subspaces . . . . . ... ... ... ....... 129 Orthogonality 143 4.1 Orthogonality of Vectors and Subspaces . . . . ... ... ... ..... 144 4.2 Projections onto Lines and Subspaces . . . . . ... ... ........ 151 4.3 Least Squares Approximations . . . . . . . . . ... ..o, 163 4.4 Orthonormal Bases and Gram-Schmude . . . . . ... ... ... ..... 176 4.5 The PseudoinverseofaMatrix . . ... ... ............... 190 Determinants 198 5.1 3byJ3Determinantsand Cofactors . . . . ... .............. 199 5.2 Computing and Using Determinants . . . . . ... ... ... ...... 205 5.3 Areas and Volumes by Determinants . . . . . ... .. .......... 211 Eigenvalues and Eigenvectors 216 6.1 IntroductiontoEigenvalues: Ax=Ax . ................. 217 6.2 DiagonalizingaMatrix . ... ... ... ... ... . .. 0. 0. 232 6.3 Symmetric Positive Definite Matrices . . . . ... ............ 246 6.4 Complex Numbers and Vectorsand Matrices . .. ............ 262 6.5 Solving Linear Differential Equations . . . . ... ............ 270 iv Table of Contents 7 The Singular Value Decomposition (SVD) 7.1 Singular Values and Singular Vectors 7.2 Image Processing by Linear Algebra 7.3 Principal Component Analysis (PCA by the SVD) 8 Linear Transformations 8.1 The Idea of a Linear Transformation 8.2 The Matnx of a Linear Transformation . . . . . ... ... .. .. .. .. 8.3 TheSearchforaGoodBasis . ... .................... 9 Linear Algebra in Optimization 9.1 Minimizing a Multivanable Function 9.2 Backpropagation and Stochastic Gradient Descent 9.3 Constraints, Lagrange Multipliers, Minimum Norms 9.4 Linear Programming, Game Theory, and Duality 10 Learning from Data 10.1 Piecewise Linear Learning Functions 10.2 Creatingand Experimenting . . ... ................... 10.3 Mean, Variance, and Covariance Appendix 1 Appendix 2 Appendix 3 Appendix 4 Appendix 5 Appendix 6 Appendix 7 Appendix 8 Appendix 9 Appendix 10 The Ranks of ABand A + B Matrix Factorizations Counting Parameters in the Basic Factorizations Codes and Algorithms for Numerical Linear Algebra The Jordan Form of a Square Matrix Tensors The Condition Number of a Matrix Problem Markov Matrices and Perron-Frobenius Elimination and Factorization Computer Graphics Index of Equations Index of Notations Index 286 287 297 302 308 309 318 327 335 336 355 364 370 372 381 386 400 401 403 404 405 406 407 410 413 419 422 423 Preface One goal of this Preface can be achieved right away. You need to know about the video lectures for MIT’s Linear Algebra course Math 18.06. Those videos go with this book, and they are part of MIT’s OpenCourseWare. The direct links to linear algebra are https://ocw.mit.edu/courses/1 8-06-linear-algebra-spring-2010/ https://ocw.mit.edu/courses/18-06sc-linear-algebra-fall-2011/ On YouTube those lectures are at https://ocw.mit.edu/1806videos and /1806scvideos The first link brings the original lectures from the dawn of OpenCourseWare. Problem solutions by graduate students (really good) and also a short introduction to linear algebra were added to the new 2011 lectures. And the course today has a new start—the crucial ideas of linear independence and the column space of a matrix have moved near the front. I would like to tell you about those ideas in this Preface. Start with two column vectors a; and a;. They can have three components each, so they correspond to points in 3-dimensional space. The picture needs a center point which locates the zero vector: - - p— - - - 2 1 0 a=\\|3 a=| 4 zero vector = | O 1 |2 | 0 The vectors are drawn on this 2-dimensional page. But we all have practice in visualizing three-dimensional pictures. Here are a,. a2, 2a;. and the vector sum a; + a,. Vi Introduction to Linear Algebra That picture illustrated two basic operations—adding vectors a; + a2 and multiplying a vector by 2. Combining those operations produced a “linear combination” 2a, + a2 : Linear combination = ca; + da; for any numbers c and d Those numbers ¢ and d can be negative. In that case ca, and da, will reverse their direc- tions : they go right to left. Also very important, ¢ and d can involve fractions. Here is a picture with a lot more linear combinations. Eventually we want all vectors ca, + da;. 2a a a; + a; 1+ a2 —a) — az Here is the key ! The combinations ca; + daj fill a whole plane. It is an infinite plane in 3-dimensional space. By using more and more fractions and decimals ¢ and d, we fill in a complete plane. Every point on the plane is a combination of a, and a. Now comes a fundamental idea in linear algebra: a matrix. The matrix A holds n column vectors a,, a,, . . ., @,. At this point our matrix has two columns a; and a2, and those are vectors in 3-dimensional space. So the matrix has three rows and two columns. 3 by 2 matrix [ 1 [2 1] m = 3 rows A=]a a2 | =]|3 4 n = 2 columns I ] 1 2 The combinations of those two columns produced a plane in three-dimensional space. There is a natural name for that plane. It is the column space of the matrix. For any A, the column space of A contains all combinations of the columns. Here are the four ideas introduced so far. You will see them all in Chapter 1. 1. Column vectors a; and a, in three dimensions 2. Linear combinations ca; + da; of those vectors 3. The matrix A contains the columns a; and a; 4. Column space of the matrix = all linear combinations of the columns = plane Preface vii Now we include 2 more columns in A 2 1 3 0 1 . A=|13 4 7 0 The 4 columns are in 3-dimensional space 1 2 3 -1 Linear algebra aims for an understanding of every column space. Let me try this one. Columns 1 and 2 produce the same plane as before (same a; and a;) Column 3 contributes nothing new because a3 is on that plane: a3 = a; + a2 Column 4 is not on the plane : Adding in c4a4 raises or lowers the plane The column space of this matrix A is the whole 3-dimensional space : all points ! You see how we go a column at a time, left to right. Each column can be independent of the previous columns or it can be a combination of those columns. To produce every point in 3-dimensional space, you need three independent columns. Matrix Multiplication A = CR Using the words “linear combination” and “independent columns™ gives a good picture of that 3 by 4 matrix A. Column 3 is a linear combination: column 1 4 column 2. Columns 1, 2,4 are independent. The only way to produce the zero vector as a combination of the independent columns 1.2. 4 is to multiply all those columns by zero. We are so close to a key idea of Chapter 1 that I have to go on. Matrix multiplication is the perfect way to write down what we know. From the 4 columns of 4 we pick out the independent columns a;, @a3. a4 in the column matrix C. Every column of R tells us the combination of a1, a2, a4 in C that produces a column of A. A equals C times R : p— - - - P - 2 1 3 0 2 1 0 1 0 1 0 A=| 3 4 7 0|l=1]3 4 0 0 1 1 0 |=CR 1 2 3 -1 1 2 -1]1060 0 0 1 Column 3 of A is dependent on columns 1 and 2 of A4, and column 3 of R shows how. Add the independent columns 1 and 2 of C' to get column a3z = a; + a2 = (3.7.3) of A. Matrix multiplication : Each column j of C R is C times column j of R Section 1.3 of the book will multiply a matrix times a vector (two ways). Then Section 1.4 will multiply a matrix times a matrix. This is the key operation of linear algebra. It is important that there is more than one good way to do this multiplication. I am going to stop here. The normal purpose of the Preface is to tell you about the big picture. The next pages will give you two ways to organize this subject—especially the first seven chapters that more than fill up most linear algebra courses. Then come optional chapters, leading to the most active topic in applications today : deep learning. viii Introduction to Linear Algebra The Four Fundamental Subspaces You have just seen how the course begins—with the columns of a matrix A. There were two key steps. One step was to take all combinations ca; + da; + eas + fay of the columns. This led to the column space of A. The other step was to factor the matrix into C times R. That matrix C holds a full set of independent columns. I fully recognize that this is only the Preface to the book. You have had zero practice with the column space of a matrix (and even less practice with C' and R). But the good thing is : Those are the right directions to start. Eventually, every matrix will lead to four fundamental spaces. Together with the column space of A comes the row space—all combinations of the rows. When we take all combinations of the n columns and all combinations of the m rows—those combinations fill up “spaces” of vectors. The other two subspaces complete the picture. Suppose the row space is a plane in three dimensions. Then there is one special direction in the 3D picture—that direction is perpendicular to the row space. That perpendicular line is the nullspace of the matrix. We will see that the vectors in the nullspace (perpendicular to all the rows) solve Az = 0: the most basic of linear equations. And if vectors perpendicular to all the rows are important, so are the vectors perpendicular to all the columns. Here is the picture of the Four Fundamental Subspaces. Column space Row space Dimension r Dimension r combinations of the columns combinations of the rows The Big Picture (m rows and n columns) @ m dimensional space n dimensional space to the columns perpendicular to the rows Nullspace of AT Nullspace of A dimensionm — r dimensionn — r The Four Fundamental Subspaces: An m by n matrix with » independent columns. This picture of four subspaces comes in Chapter 3. The idea of perpendicular spaces is developed in Chapter 4. And special “basis vectors” for all four subspaces are discovered in Chapter 7. That step is the final piece in the Fundamental Theorem of Linear Algebra. The theorem includes an amazing fact about any matrix, square or rectangular: The number of independent columns equals the number of independent rows. Preface ) 4 Five Factorizations of a Matrix Here are the organizing principles of linear algebra. When our matrix has a special property, these factorizations will show it. Chapter after chapter, they express the key idea in a direct and useful way. The usefulness increases as you go down the list. Orthogonal matrices are the win- ners in the end, because their columns are perpendicular unit vectors. That is perfection. cos® —siné 2 by 2 Orthogonal Matrix = [ sin 0 cos 0 ] = Rotation by Angle 6 Here are the five factorizations from Chapters 1,2,4,6,7: A=CR = R combines independent columns in C to give all columns of A A=LU = Lower triangular L times Upper triangular U A=QR = Orthogonal matrix Q times Upper triangular R S = QAQT = (Orthogonal Q) (Eigenvalues in A) (Orthogonal QY A =UXVT = (Orthogonal U) (Singular values in ) (Orthogonal V' T) J O = N = May I call your attention to the last one ? It is the Singular Value Decomposition (SVD). It applies to every matrix A. Those factors U and V have perpendicular columns—all of length one. Multiplying any vector by U or V leaves a vector of the same length—so computations don’t blow up or down. And ¥ is a positive diagonal matrix of “singular values”. If you learn about eigenvalues and eigenvectors in Chapter 6, please continue a few pages to singular values in Section 7.1. Deep Learning For a true picture of linear algebra, applications have to be included. Completeness 1s totally impossible. At this moment, the dominating direction of applied mathematics has one special requirement: It cannot be entirely linear ! One name for that direction is “deep learning”. It is an extremely successful approach to a fundamental scientific problem: Learning from data. In many cases the data comes in a matrix. Our goal is to look inside the matrix for the connections between variables. Instead of solving matrix equations or differential equations that express known input-output rules, we have to find those rules. The success of deep learning is to build a function F'(x. v) with inputs & and v of two kinds: The vectors v describes the features of the training data. The matrices x assign weights to those features. The function F(x, v) is close to the correct output for that training data v. When v changes to unseen test data, F'(x, v) stays close to correct. This success comes partly from the form of the learning function F', which allows it to include vast amounts of data. In the end, a linear function F' would be totally inadequate. The favorite choice for F' is piecewise linear. This combines simplicity with generality. X Introduction to Linear Algebra Applications in the Book and on the Website I hope this book will be useful to you long after the linear algebra course is complete. It is all the applications of linear algebra that make this possible. Matrices carry data, and other matrices operate on that data. The goal is to “see into a matrix” by understand- ing its eigenvalues and eigenvectors and singular values and singular vectors. And each application has special matrices—here are four examples : Markov matrices M Each column is a set of probabilities adding to 1. Incidence matrices A Graphs and networks start with a set of nodes. The matrix A tells the connections (edges) between those nodes. Transform matrices ' The Fourier matrix uncovers the frequencies in the data. Covariance matrices C The variance is key information about a random variable. The covariance explains dependence between variables. We included those applications and more in this Sixth Edition. For the crucial computation of matrix weights in deep learning, Chapter 9 presents the ideas of optimization. This is where linear algebra meets calculus : derivative = zero becomes a matrix equation at the minimum point because F'(x) has many variables. Several topics from the Fifth Edition gave up their places but not their importance. Those sections simply moved onto the Web. The website for this new Sixth Edition 1s math.mit.edu/linearalgebra That website includes sample sections from this new edition and solutions to all Problem Sets. These sections (and more) are saved online from the Fifth Edition : Fourier Series Norms and Condition Numbers Iterative Methods and Preconditioners Linear Algebra for Cryptography Here is a small touch of linear algebra—three questions before this course gets serious : 1. Suppose you draw three straight line segments of lengths r and 8 and ¢ on this page. What are the conditions on those three lengths to allow you to make the segments into a triangle ? In this question you can choose the directions of the three lines. 2. Now suppose the directions of three straight lines u, v, w are fixed and different. But you could stretch those lines to au,bv,cw with any numbers a,b,c. Can you always make a closed triangle out of the three vectors au, bv,cw ? 3. Linear algebra doesn’t stay in a plane! Suppose you have four lines u, v, w, 2 in different directions in 3-dimensional space. Can you always choose the numbers a, b, c,d (zeros not allowed) sothatau + bv + cw +dz =07? For typesetting this book, maintaining its website, offering quality textbooks to Indian fans, I am grateful to Ashley C. Fernandes of Wellesley Publishers (www.wellesleypublishers.com) gilstrang @gmail.com Gilbert Strang 1 Vectors and Matrices 1.1 Vectors and Linear Combinations 1.2 Lengths and Angles from Dot Products 1.3 Matrices and Their Column Spaces 14 Matrix Multiplication AB and CR Linear algebra is about vectors v and matrices A. Those are the basic objects that we can add and subtract and multiply (when their shapes match correctly). The first vector v has two components v; = 2 and v = 4. The vector w is also 2-dimensional. o=[n]=[3] we=[n]-[3] ere-[3] The linear combinations of v and w are the vectors cv + dw for all numbers c and d: The linear 2 1] [ 2c+1d combinations c[ 4 ]+ d[ 3 ] = [ dc + 3d ] fill the xy plane The length of that vector w is ||w|| = V10, the square root of w? + w2 = 1 + 9. The dot product of v and wis v - w = vyw; + vowe = (2)(1) + (4)(3) = 14. In Section 1.2, v - w will reveal the angle between those vectors. The big step in Section 1.3 is to introduce a matrix. This matrix A contains our two column vectors. The vectors have two components, so the matrix is 2 by 2: 2 1 e[ #]- (23] When a matrix multiplies a vector, we get a combination cv + dw of its columns: : cl|_ |21 c|_| 2c+1d | _ Atlmes[d]—[4 3][d]—[4c+3d]_w+dw' And when we look at all combinations Ax (with every c and d), those vectors produce the column space of the matrix A. Here that column space is a plane. With three vectors, the new matrix B has columns v, w, 2. In this example 2 is a combination of v and w. So the column space of B is still the zy plane. The vectors v and w are independent but v, w, 2 are dependent. A combination produces zero: oo olfi 3 fomo ] 3]s veeeBl BT The final goal is to understand matrix multiplication AB = A times each column of B. 1 Chapter 1. Vectors and Matrices 2 1.1 Vectors and Linear Combinations ﬁ _ 3w is a linear combination cv + dw of the vectors v and w. \\ 2J [2] 2 L 4 2 Forv = [‘11] and w = [_1] that combination is 2 [l] -3 [_1 3 All combinations ¢ [‘11] +d [ _?] fill the zy plane. They produce every -y - - 1 “ q Fg fill a plane 9 4 The vectors c | 1| + in Yz space. \\ 2] 4 |3 Calculus begins with numbers z and functions f(z). Linear algebra begins with vectors v, w and their linear combinations cv + dw. Immediately this takes you into two or more (possibly many more) dimensions. But linear combinations of vectors v and w are built from just two basic operations: is not on that plane. 2 Multiply a vector v by a number 3v =3 [ 1 J = [ 3 ] 4 6 Add vectors v and w of the same dimension: v + w = [ ? ] + [ 3 } = [ 4 ] Those operations come together in a linear combination cv + dwofvand w: Linear combination Eo — 2w =5 2| |4 ]|_|10|_| 8 |_ 2 c=5add=-2 \" %11 37| 5 6 |~ | -1 That idea of a linear combination opens up two key questions : 1 Describe all the combinations cv + dw. Do they fill a plane or a line ? 2 Find the numbers c and d that produce a specific combination cv + dw = [ (2) ] We can answer those questions here in Section 1.1. But linear algebra is not limited to 2 vectors in 2-dimensional space. As long as we stay linear, the problems can get bigger (more dimensions) and harder (more vectors). The vectors will have m components instead of 2 components. We will have n vectors vy,v2,...,v, instead of 2 vectors. Those n vectors in m-dimensional space will go into the columns of an m by n matrix A: m rows \" ) n columns A=| v, v -+ v, m by n matrix _ J Let me repeat the two key questions using A, and then retreat back tom = 2and n = 2: 1 Describe all the combinations Az = z,v; + z2v2 + - - - + Z,V,, Of the columns 2 Find the numbers x; to x,, that produce a desired output vector ATz = b 1.1. Vectors and Linear Combinations 3 Linear Combinations cv 4+ dw Start from the beginning. A vector v in 2-dimensional space has two components. To draw v and —wv, use an arrow that begins at the zero vector: _ 2 S b i s~~~ 4 + +— o+t eVt W= [O] =w+v —21 _.°° | —_y = a L 7 [_1 ] w = [ 2] —1 The vectors cv (for all numbers c) fill an infinitely long line in the zy plane. If w i1s not on that line, then the vectors dw fill a second line. We aim to see that the linear combinations cv + dw fill the plane. Combining points on the v line and the w line gives all points. Here are four different linear combinations—we can choose any numbers c and d : lv+ 1w = sum of vectors lv — 1w = difference of vectors Ov+ 0w = zerovector cv + 0w = vector cv in the direction of v Solving Two Equations Very often linear algebra offers the choice of a picture or a computation. The picture can start with v + w and w + v (those are equal). Another important vector 1s w — v. going backwards on v. The Preface has a picture with many more combinations—starting to fill a plane. But if we aim for a particular vector like cv + dw = [g] , it will be better to compute the exact numbers c and d. Here are two ways to write down this problem. 2 2] |8 . 2c+2d =8 Solve c[1]+d[_l]—[2]. This means e d=2 The rules for solution are simple but strict. We can multiply equations by numbers (nor zero!) and we can subtract one equation from another equation. Expernience teaches that the key is to produce zeros on the left side of the equations. One zero will be enough'! %(equation l)isc+d=4 . 2c+2d= 8 Subtract this from equation 2 Oc—-—2d =-2 Then c is eliminated = The second equation gives d = 1. Going upwards, the first equation becomes 2c + 2 = 8. Its solution is ¢ = 3. This combination is correct : [1]ea[ 3]-[2) e [3 2](2]-2 4 Chapter 1. Vectors and Matrices Column Way, Row Way, Matrix Way Column way 7 +d wy | | b Linear combination € Vg wy | | by Row way vic+w d = b, Two equations for ¢ and d v2Cc + wad = b, Matrix way V1 W c|_| b 2 by 2 matrix v w3 d| | b If the points v and w and the zero vector O are not on the same line, there is exactly one solution ¢, d. Then the linear combinations of v and w exactly fill the zy plane. The vectors v and w are “linearly independent”. The 2by 2 matrix A = [ v w | is “invertible”. Can Elimination Fail ? Elimination fails to produce a solution only when the equations don’t have a solution in the first place. This can happen when the vectors v and w lie on the same line through the center point (0,0). Those vectors are not independent. The reason is clear: All combinations of v and w will then lie on that same line. If the desired vector b is off the line, then the equations cv + dw = b have no solution: 11 |3 ! lc+3d=1 Example v—[zl w—[ﬁ] b—[o] 2 +6d = 0 Those two equations can’t be solved. To eliminate the 2 in the second equation, we mul- tiply equation 1 by 2. Then elimination subtracts 2¢ + 6d = 2 from the second equation 2c + 6d = 0. The result is 0 = —2: impossible. We not only eliminated ¢, we also eliminated d. With v and w on the same line, combinations of v and w fill that line but not a plane. When b is not on that line, no combination of v and w equals b. The original vectors v and w are “linearly dependent” because w = 3v. Linear combinations of two independent vectors v and w in two-dimensional space can produce any vector b in that plane. Then these equations have a solution : 2 equations _ cvy; +dw; = by 2 unknowns cv+dw=>b cvy + dwe = b, Summary The combinations cv + dw fill the z-y plane unless v is in line with w. Important Here is a different example where elimination seems to be in trouble. But we can easily fix the problem and go on. 0 +d 2| | 2 or 0+2d=2 ‘11 3| |7 c+3d=7 That zero looks dangerous. But we only have to exchange equations to find d and c: c+3d=\"17 0+2d=2 leadsto d=1 and c=4 1.1. Vectors and Linear Combinations 5 Vectors in Three Dimensions Suppose v and w have three components instead of two. Now they are vectors in three- dimensional space (which we will soon call R>). We still think of points in the space and arrows out from the zero vector. And we still have linear combinations of v and w: (2] 1] 3] [ 2c+d | v=| 3 w=1|1 v+w=| 4 cv+dw=| 3c+d _1_ _0_ _1_ Lc+0_ But there is a difference ! The combinations of v and w do not fill the whole 3-dimensional space. If we only have 2 vectors, their combinations can at most fill a 2-dimensional plane. It is not a case of linear dependence, it is just a case of not enough vectors. We need three independent vectors if we want their combinations to fill 3-dimensional space. Here is the simplest choice 2, 7, k for those three independent vectors : (1] 0 ] 0 ] [ c | i=|0 j=11 k=0 ci+dj+ek=|d (1) LO-J 30- Ll.a .e.. Now %, j, k go along the .y, z axes in three-dimensional space R>. We can easily write any vector v in R® as a combination of 1. j, k Vector form u1 1 00 Uy U . v=| vy |=v1t+ 1) + v3k 0 1 O ' = ) Matrix form | v3 | _ 0 0 1 1w e That is the 3 by 3 identity matrix I. Multiplying by I leaves every vector unchanged. I is the matrix analog of the number 1, because /v = v for every v. Notice that we could not take a linear combination of a 2-dimensional vector v and a 3-dimensional vector w. They are not in the same space. How Do We Know It is a Plane ? Suppose v and w are nonzero vectors with three components each. Assume they are independent, so the vectors point in different directions: w is not a multiple cv. Then their linear combinations fill a plane inside 3-dimensional space. The surface is fiat. Here is one way to see that this is true : Look at any two combinations cv + dw and Cv + Dw. Halfway between those points is h = 2(c + C)v + 3(d + D)w. This is another combination of v and w. So our surface has the basic property of a plane: Halfway between any two points on the surface is another point h on the surface. The surface must be flat ! Maybe that reasoning is not complete, even with the exclamation point. We depended on intuition for the properties of a plane. Another proof is coming in Section 1.2. 6 Chapter 1. Vectors and Matrices Problem Set 1.1 a b two equations ¢ = ma and d = mb. By eliminating m, find one equation connecting a. b. c,d. You can assume no zeros in these numbers. Under what conditions on a, b, ¢, d is [ ; ] a multiple m of [ ] ? Start with the Going around a triangle from (0, 0) to (5.0) to (0, 12) to (0, 0), what are those three vectors u, v, w? What is u + v + w? What are their lengths ||u|| and ||v|| and ||w]| ? The length squared of a vector u = (u;, u) is ||u||* = u? + u2. Problems 3-10 are about addition of vectors and linear combinations. 3 10 Descnbe geometrically (line, plane, or all of R3) all linear combinations of 3 1 0 2 0 2 (a) |2| and |6 (b) O] and | 2 (c) Oland | 2] and | 2 3 9 0] 3] 0] (2 3] Draw v = [ ‘11 J and w= [ _g} and v+w and v—w in a single zy plane. fo+w= [ ? ] andv —w = [ ; ‘ compute and draw the vectors v and w. Fromvz[%]andwz[l 9 ] find the components of 3v + w and cv + dw. Compute u + v + w and 2u + 2v + w. How do you know u, v, w lie in a plane? - - s Y These lie in a plane because _ ; _ _:1; — _;2; w = cu + dv. Find c and d w= v _9 v ~1 | g L - - - Every combination of v = (1,-2,1) and w = (0, 1, —1) has components that add to . Find ¢ and d so that cv + dw = (3,3, —6). Why is (3, 3, 6) impossible? In the ry plane mark all nine of these linear combinations: c[?]+d[(l)] with ¢=0,1,2 and d=0,1,2. (Not easy) How could you decide if the vectors u = (1,1,0) and v = (0,1, 1) and w = (a, b, c) are linearly independent or dependent ? 1.1. Vectors and Linear Combinations 7 ,’, Notice the illusion / 1s (0,0,0) at the ....... & top or bottom ? Figure 1.1: Unit cube from i, j, k and twelve clock vectors: all lengths = 1. 11 If three corners of a parallelogram are (1.1), (4.2), and (1. 3), what are all three of the possible fourth corners? Draw those three parallelograms. Problems 12-15 are about special vectors on cubes and clocks in Figure 1.1. 12 Four corners of this unit cube are (0.0.0), (1.0.0), (0.1.0), (0.0.1). What are the other four corners? Find the coordinates of the center point of the cube. The center points of the six faces have coordinates . The cube has how many edges ? 13 Review Question. In ryz space. where is the plane of all linear combinations of 1 =(1,0,0)and 2+ 7 = (1.1,0)? 14 (a) What is the sum V of the twelve vectors that go from the center of a clock to the hours 1:00, 2:00, ..., 12:00? (b) If the 2:00 vector is removed, why do the 11 remaining vectors add to 8:00? (c) The components of that 2:00 vector are v = (cos 6.sin8) ? What is 6 ? 15 Suppose the twelve vectors start from 6:00 at the bottom instead of (0.0) at the center. The vector to 12:00 is doubled to (0, 2). The new twelve vectors add to : 16 Draw vectors u, v, w so that their combinations cu + dv + ew fill only a line. Find vectors u, v, w in 3D so that their combinations cu + dv + ew fill only a plane. 17 What combination c [ ;] +d [::] produces [lg] ? Express this question as two equations for the coefficients ¢ and d in the linear combination. Problems 18-19 go further with linear combinations of v and w (see Figure 1.2a). 18 Figure 1.2a shows 3 v + 5 w. Mark the points 3 v+ fwand v+ 1 wand v + w. Draw the line of all combinations cv + dw that have ¢ + d = 1. 19 Restricted by 0 < ¢ < 1and 0 < d < 1, shade in all the combinations cv + dw. Restricted only by ¢ > 0 and d > 0 draw the *“cone” of all combinations cv + dw. Figure 1.2: Problems 18-19 in a plane Chapter 1. Vectors and Matrices (a) (b) Problems 20-23 in 3-dimensional space Problems 20-23 deal with u, v, w in three-dimensional space (see Figure 1.2b). 20 21 22 23 24 25 26 27 Locate % u+ % v+ % w and % u+ % w in Figure 1.2b. Challenge problem: Under what restrictions on c, d, e, will the combinations cu + dv + ew fill in the dashed triangle? To stay in the triangle, one requirementis ¢ > 0,d > 0,e > 0. The three dashed lines in the triangle are v — u and w — v and u — w. Their sum is _____. Draw the head-to-tail addition around a plane triangle of (3, 1) plus (—1,1) plus (-2, —-2). Shade in the pyramid of combinations cu + dv + ew withc > 0,d > 0, e > 0 and ¢ +d + e < 1. Mark the vector 3 (u + v + w) as inside or outside this pyramid. If you look at all combinations of those u, v, and w, is there any vector that can’t be produced from cu + dv + ew? Different answer if u, v, w are all in Challenge Problems How many comers (+1,£1,£1, £1) does a cube of side 2 have in 4 dimensions ? What is its volume? How many 3D faces? How many edges? Find one edge. Find two different combinations of the three vectors u = (1,3) and v = (2,7) and w = (1,5) that produce b = (0,1). Slightly delicate question: If I take any three vectors u, v, w in the plane, will there always be two different combinations that produce b = (0,1)? The linear combinations of v = (a,b) and w = (c, d) fill the plane unless Find four vectors u, v, w, 2 with four nonzero components each so that their combinations cu + dv + ew + fz produce all vectors in four-dimensional space. Write down three equations for ¢, d, e so that cu + dv + ew = b. Write this also as a matrix equation Az = b. Can you somehow find ¢, d, e for this b ? ol 2 —1 ] 0 1 u=1] -1 v= 2 w=| -1 0 | 0 -1 2 0 1.2. Lengths and Angles from Dot Products 9 1.2 Lengths and Angles from Dot Products (‘I‘he “dot product” of v = [ . ] and w = [ : ] isv-w= (1)) +(2)(6) =4+12= 1A 2 The length squared of v = (1,3,2)isv-v =1+ 9+ 4 = 14. The length is ||v|| = V14. 3 v =(1,3,2) is perpendicular to w = (4, —4, 4) because v - w = 0. ' 1 1 vVew 1 4 The angle @ = 45° between v = [ ]andw= [ ]hascose= = . L 1 ol Tl ~ (1(v2) Schwarz inequality Triangle inequality QAII angles have | cos 6| < 1. All vectors have |y . w|<||v]| llw|||||v+w||< ||v||+||w||/ The most useful multiplication of vectors v and w is their dot product v - w. We multiply the first components v; w; and the second components vow, and so on. Then we add those results to get a single numberv - w: The dot product of v = [ z; ] and w = [ z; ] isv.w=vw +vow.| (1) If the vectors are in n-dimensional space with n components each, then Dot product vew=nw +vwr+- -+ VW =WV (2) The dot product v - v tells us the squared length ||v||? = v? +---+v2 of a vector. In two dimensions, this is the Pythagoras formula a? + b* = c? for a right triangle. The sides have a2 = v? and b?> = v2. The hypotenuse has ||v||? = v? + v = a? + b2, To reach n dimensions, we can add one dimension at a time. Figure 1.2 shows v = (1,2) in two dimensions and w = (1,2, 3) in three dimensions. Now the right triangle has sides (1,2,0) and (0,0, 3). Those vectors add to w. The first side is in the zy plane, the second side goes up the perpendicular 2 axis . For this triangle in 3D with hypotenuse w = (1,2, 3), the law a2 + b = ¢? becomes (12 + 22) + (3%) = 14 = ||w||. ,,11' \"\"\" 0 : , K ©,2) | (1,2) has length v/5 =1 (‘,'2'13) ht;s\\/ﬁ /3 vev =10+ 0] v/ | g +——8=12+2 ! w | 3 3 1 w-w=vw}+ wi+ vl : o ' \"L0) 14=12 422 4 32— !, ~(0.2,0) ’ | - o/ VBN [(1,2,0) has g AL EE DR length v/5 Figure 1.3: The length /v - v = v/5 in a plane and /w - w = v/14 in three dimensions. 10 Chapter 1. Vectors and Matrices The length of a four-dimensional vector would be Vv? + v2 4+ v2 + v2. Thus the vector (1,1,1,1) has length v12+12+12+12 = 2. This is the diagonal through a unit cube in four-dimensional space. That diagonal in n dimensions has length /7. We use the words unit vector when the length of the vector is 1. Divide v by ||v]|]|. v A unit vector u has length ||u|| = 1. If v # 0 then u = —— is a unit vector. |lv]| Example 1 The standard unit vector along the x axis is written 2. In the ry plane, the unit vector that makes an angle “theta” with the r axis is 4 = (cos 8, sin ) : Unit vectors 1t = L and u = c?sﬂ . Notice 2 - u = cos@. 0 sin 6 u = (cos 0, sin 6) is a unit vector because u - u = cos? 0 + sin’ 9 = 1. In four dimensions, one example of a unit vector is u = (l 11 %) Or you could 9 = 100. So v has 212 start with the vector v = (1,5,5,7). Then ||[v||* = 1+ 25+ 25 + length 10 and u = v/10 is a unit vector. 1 2 4 The word “unit” is always indicating that some measurement equals “one”. The unit price is the price for one item. A unit cube has sides of length one. A unit circle 1s a circle with radius one. Now we see that a “unit vector” has length = 1. Perpendicular Vectors Suppose the angle between v and w is 90°. Its cosine is zero. That produces a valuable test v - w = O for perpendicular vectors. Perpendicular vectors have v - w = 0. Then ||v + w||? = ||v]|? + ||w]|%.| ) This is the most important special case. It has brought us back to 90° angles and lengths a? + b%? = 2. The algebra for perpendicular vectors (v-w =0 = w - v) is easy: lv+w|P=@v+w)-v+w)=v-v+v-wtw-v+w-w=|v|]*+|w|? @) Two terms were zero. Please notice that ||v — w||? is also equal to ||v||2 + ||w]|?. Example 2 The vector v = (1, 1) is at a 45° angle with the z axis The vector w = (1, —1) is at a —45° angle with the z axis The sum v + w is (2, 0). The difference v — w is (0, 2). So the angle between (1,1) and (1, —1) is 90°. Their dot productisv-w =1—-1 = 0. This right triangle has ||v||* = 2 and ||w||> = 2 and ||v — w||? = |jv + w||? = 4. 1.2. Lengths and Angles from Dot Products 11 Example 3 The vectors v = (4,2) and w = (—1, 2) have a zero dot product: Dot product is zero [4] [—1] B . . =—-4+4=0. Vectors are perpendicular 2 Put a weight of 4 at the point £ = —1 (left of zero) and a weight of 2 at the point z = 2 (right of zero). The z axis will balance on the center point like a see-saw. The weights balance because the dot productis (4)(—1) + (2)(2) = 0. This example is typical of engineering and science. The vector of weights is (w;, w2) = (4, 2). The vector of distances from the center is (v;, v2) = (—1,2). The weights times the distances, w;v; and wovs, give the “moments”. The equation for the see-saw to balance is w - v = wv; + wave = 0 = zero dot product. Example 4 The unit vectors v = (1,0) and © = (cos#.sinf) have v - u = cos¥é. Now we are connecting the dot product to the angle between vectors. Cosine of the angle & The cosine formula is easy to remember for unit vectors : If ||v|| = 1 and ||u|| = 1, the angle 6 between v and u has cos 8 = v - u. In mathematics, zero is always a special number. For dot products, it means that these two vectors are perpendicular. The angle between them is 90°. The clearest example of perpendicular vectors is ¢ = (1.0) along the x axis and 3 = (0.1) up the y axis. Again the dot productis 2 -+ 3 = 0 + 0 = 0. The cosine of 90° is zero. j=0.1) v=(L1) J cos 6 “= [sine] E < il & | 2 o 0 cos 6 A P Y i =(1,0) Unit vectors 2,7, u Unit circle —Jv Figure 1.4: Left: The coordinate vectors ¢ and j. The unit vector u divides v = (1.1) by its length ||v]| = v/2. Right: The unit vector u = (cos 6. sin ) is at angle 6 with 1. Example 5 Dot products enter in economics and business. We have three goods to buy. Their prices for each unit are (p;, p2, p3)—this is the price vector p. The quantities we buy are (qi, g2, q3). Buying q, units at the price p, brings in q,p,. The total cost adds up quantities g times prices p. This is the dot product q - p in three dimensions : Cost = (q1,q2,93) * (P1,P2,P3) = 11 + q2p2 + q3p3 = dot product. A zero dot product means that “the books balance”. Total sales equal total purchases if q - p = 0. Then p is perpendicular to g (in three-dimensional space). A supermarket with thousands of goods goes quickly into high dimensions. Spreadsheets have become essential in management. They compute linear combina- tions and dot products. What you see on the screen is a matrix. 12 Chapter 1. Vectors and Matrices The Angle Between Two Vectors We know that perpendicular vectors have v - w = 0. The dot product is zero when the angle is 90°. Our next step is to connect all dot products to angles. The dot product v - w finds the angle between any two nonzero vectors v and w. Example 6 The unit vectors v = (cosa,sina) and w = (cos3,sin8) have v - w = cos acos 3 + sinasin 3. In trigonometry this is the formula for cos (3 — a). Figure 1.5 shows that the angle between the unit vectorsvand wis 8 — a . The dot product w - v equals v - w. The order of v and w makes no difference. cosf| _ | cos6 [Si\"ﬂ]—w “= [sinO] cosa | 9 sing | =7 ol u-i =cos@ f=f-«a Figure 1.5: Unit vectors: u - ¢ = cos 6. The angle between the vectors is 6. Suppose v - w is not zero. It may be positive, it may be negative. The sign of v - w immediately tells whether we are below or above a right angle. The angle is less than 90° when v - w is positive. The angle is above 90° when v - w is negative. The borderline is where vectors are perpendicular to v. On that dividing line between plus and minus, w2 = (1, —3) is perpendicular to v = (3, 1). Their dot product is zero. Then w3 goes beyond a 90° angle with v. The test becomes v + w3 < 0: negative. [—1] v-w; > 0 whenf < 90° Figure 1.6: Small angle v - w; > 0. Right angle v - wa = 0. Large angle v - w3 < 0. The dot product reveals the exact angle 8. To repeat: For unit vectors u and U, the dot product u - U is the cosine of 0. This remains true in n dimensions. Remember that cos 8 is never greater than 1. It is never less than —1. The dot product of unit vectors is between —1 and 1. The cosine of 6 is revealed by u - U. What if v and w are not unit vectors? Divide by their lengths to get u = v/||v|| and U = w/||w||. Then the dot product of those unit vectors u and U gives cos 6. COSINE FORMULA If v and w are nonzero vectors then =cosf. | (5) Vw lv]l [lwl] 1.2. Lengths and Angles from Dot Products 13 Whatever the angle, this dot product of v/||v|| with w/||w|| never exceeds one. That is the “Schwarz inequality” |v - w| < ||v|| |w|| for dot products—or more correctly the Cauchy-Schwarz-Bunyakowsky inequality. It was found in France and Germany and Russia (and maybe elsewhere—it is the most important inequality in mathematics). Since | cos @ | never exceeds 1, the cosine formula in (5) gives two great inequalities. SCHWARZ INEQUALITY v-w| < ||Jv]| ||w]| TRIANGLE INEQUALITY |jv +w| < ||v|| + ||Jw] The triangle inequality comes directly from the Schwarz inequality ! lv+w||?P=v-v+v-w+w-v+w-w< ||v]]?+2|v|]|||lw]] + [[w]||®. (6 The square root is ||v + w|| < ||v]| + ||w]|- Side 3 cannot exceed Side 1 + Side 2. Example 7 Find cos @ forv = [ ? ] and w = [ ; } and check both inequalities. The dot product is v - w = 4. Both v and w have length V5. So ||[v]| ||w|| = 5. 9 Vew 4 4 cos @ = = ——— = -. ol flwll V5v5 5 The Schwarz inequality is 4 < 5. By the triangle inequality, side 3 = ||v + w/] is less than side 1 + side 2. With v + w = (3.3) the three side lengths are V18 < V5 + V5. Square this inequality to get 18 < 20. This confirms the triangle inequality. Example 8 The dot product of v = (a.b) and w = (b.a) is 2ab. Their lengths are |lv|| = ||w|| = Va? + b2. The Schwarz inequality v - w < ||v]| ||w]| is 2ab < a? + b>. Any numbers a2 and b2 have geometric mean |ab| < arithmetic mean 1 (a® + b?). The proof is that 1 (a? + b°) —ab= 1(a - b)2 is a perfect square : never negative ! A Plane in 3 Dimensions Suppose 7 is a unit vector with three components n,.ny.n3. Look at all vectors w 1n R® that are perpendicularton (sow -n = 0): The vectors w with w - n = 0 fill a 2-dimensional plane in R® The whole plane is perpendicular to its “normal vector n”’. The equation of a 2-dimensional plane in 3-dimensional space is nyw; + naw2 + nzws = 0. For the “zy plane” the normal vector n going straight upward has components 0,0, 1. So the equation of the xy plane is just w3 = 0 or 2 = 0, which we already knew. 14 Chapter 1. Vectors and Matrices 8 WORKED EXAMPLES = 1.2 A For the vectors v = (3,4) and w = (4, 3) test the Schwarz inequality on v - w and the triangle inequality on ||v + w|. Find cos@ for the angle between v and w. Solution The dot product is v - w = (3)(4) + (4)(3) = 24. The length of v is lvll = v9+ 16 = 5 and also [|w|| = 5. The sum v + w = (7, 7) has length 7v/2 < 10. Schwarz inequality |v.w| <|v||||w] is 24 < 25. Triangle inequality |lv + w| < ||v|| +|Jw| is 7v2<5+5. 24 Cosine of angle cosf = 5 Thin angle from v = (3,4) to w = (4, 3) 1.2B Which v and w give equality |v - w| = ||v|| |w|| and ||v + w| = ||v| + ||w]|? Equality : One vector is a multiple of the other as in w = cv. Then the angle is 0° or 180°. In this case |cosf| = 1 and |v - w| equals ||v]| ||w||. If the angle is 0°, as in w = 2wv, then ||lv + w||=||v|| + ||w|| (both sides give 3||v||). This v,2v,3v triangle is flat. 1.2 C Find a unit vector u in the direction of v = (3,4). Find a unit vector U that is perpendicular to u. There are two possibilities for U. Solution For a unit vector u, divide v by its length ||v|| = 5. For a perpendicular vector V we can choose (—4, 3) or (4, —3). For a unit vector U, divide V by its length || V|| = 5. 12D We want to explain the angle formula v - w = ||v||||w|| cos?¥. This is abcos & in the Law of Cosines ¢Z = a® + b? — 2abcos . Use Pythagoras in the left triangle ! @ N c? = (asinf)? + (b — acosf)? //e,// = a?sin® 0 + b? — 2abcos 6 + a? cos? § 0 = a? + b? — 2abcos b—acosf acosf —b=jw|| — The picture shows the sides a = ||v||,b = ||w||,c = ||v — w|| and two right triangles. Compare with ||v — w||? = ||v||? + ||w||? = 2v - w to match v - w with ||v]| ||w]| cos 6. 1.2. Lengths and Angles from Dot Products 15 Problem Set 1.2 1 Calculate the dot products u - vand u - wand u - (v + w) and w - v: y — —.6 v = 4 w — 1 | .8 13 2] 2 Compute the lengths ||u|| and ||v|| and ||w|| of those vectors. Check the Schwarz inequalities |u - v| < ||u|| ||v]| and |v - w| < ||v|| ||w]|- 3 Find unit vectors in the directions of v and w in Problem 1, and find the cosine of their angle §. Choose vectors a, b, c that make 0°, 90°, and 180° angles with w. 4 For any unit vectors v and w, find the dot products (actual numbers) of (a) vand —v (b) v+wandv —w (c) v—-—2wandv + 2w 5 Find unit vectors u; and u. in the directions of v = (1.3) and w = (2.1, 2). Find unit vectors U, and U, that are perpendicular to u; and u,. 6 (a) Describe every vector w = (w;, wy) that is perpendicularto v = (2. —1). (b) All vectors perpendicularto V = (1,1.1) lieon a in 3 dimensions. (c) The vectors perpendicular to both (1,1.1) and (1.2.3) lieon a : 7 Find the angle 0 (from its cosine) between these pairs of vectors: - 1} [ 2] (2' _[ v _ _ _ (a) v = .‘fjj and w = 0 (b) v= _? and w= |-1 b - - . 1 -1 3 -1 (c) v= L\\/ﬁ. and w = [\\/ﬁl d wv= [1] and w = [—‘ZJ' 8 True or false (give a reason if true or find a counterexample if false): (a) Ifu = (1,1, 1) is perpendicular to v and w, then v is parallel to w. (b) If u is perpendicular to v.and w, then u is perpendicular to v + 2w. (c) If u and v are perpendicular unit vectors then ||u — v||? = 2. Yes! 9 The slopes of the arrows from (0, 0) to (v;, v2) and (w;, w) are vo/v) and wy/uy. Suppose the product vow; /v, w; of those slopes is —1. Show that v - w = 0 and the vectors are perpendicular. (The line y = 4x is perpendicularto y = — %I.) 10 Draw arrows from (0, 0) to the points v = (1,2) and w = (-2, 1). Multiply their slopes. That answer is a signal that v - w = 0 and the arrows are 11 If v - w is negative, what does this say about the angle between v and w? Draw a 3-dimensional vector v (an arrow), and show where to find all w’s withv - w < 0. 16 12 13 14 15 16 Chapter 1. Vectors and Matrices With v = (1,1) and w = (1, 5) choose a number ¢ so that w — cv is perpendicular to v. Then find the formula for c starting from any nonzero v and w. Find nonzero vectors u, v, w that are perpendicular to (1,1, 1, 1) and to each other. The geometric mean of r = 2 and y = 8 is ,/ry = 4. The arithmetic mean is larger: %(x+ y) = . This would come from the Schwarz inequality for v = (v/2, v/8) andw = (\\/§, \\/5). Find cos @ for this v and w. How long is the vector v = (1,1,.. .,1) in 9 dimensions? Find a unit vector u in the same direction as v and a unit vector w that is perpendicular to v. What are the cosines of the angles a, 3, § between the vector (1,0, —1) and the unit vectors 1, j, k along the axes? Check the formula cos® a + cos? 8 + cos? 8 = 1. Problems 17-20 lead to the main facts about lengths and angles in triangles. 17 18 19 20 21 The vectors v = (4.2) and w = (-1, 2) are two sides of a right triangle. Check the Pythagoras formula a® + b2 = ¢ which is for right triangles only : (Iength of v)? + (length of w)? = (length of v + w)?. (Rules for dot products) These equations are simple but useful : Mv-w=w-v Qu-(v+w)=uv-v+u-w 3)(cv) - w=c(v-w) Use(1)and 2) withu =v + w toprove |v + w||? =v-v + 2v-w + w- w. The “Law of Cosines” comes from (v —w) - (v-w)=v-v-2v-w+ w - w: Cosine Law v — w2 = ||v||? - 2||v|| ||w]|| cos 8 + ||w]|?. Draw a triangle with sides v and w and v — w. Which of the angles is 6 ? The triangle inequality says: (length of v + w) < (length of v) + (length of w). Problem 18 found ||[v + w||? = ||v||? + 2v - w + ||w||2%. Increase that v - w to |v|| ||w|| to show that ||side 3|| cannot exceed ||side 1|| + ||side 2||: Triangle 2< 2 inequality lv + wl|® < (lvll + lwll)® or [|lv+w| <|v|| + ||w]| The Schwarz inequality |v - w| < ||v|| ||w|| by algebra instead of trigonometry: (a) Multiply out both sides of (vyw; + vow2)? < (v + v3) (w? + wd). (b) Show that the difference between those two sides equals (vywz — vowy)?. This cannot be negative since it is a square—so the inequality is true. 1.2. Lengths and Angles from Dot Products 17 22 23 24 25 26 27 28 29 One-line proof of the inequality |u - U| < 1 for unit vectors (u,.u2) and (U;,Us): B+ U 3+ U3 2 2 Put (u;,u2) = (.6, .8) and (U;.U;) = (.8, .6) in that whole line and find u - U. = 1. lu-U| < |up| |Uy]| + |uz| |Uz| £ (a) Why is | cos 0| never greater than 1? (b) Find cos @ in an equilateral triangle. Show that the squared diagonal lengths ||v + w||? + ||[v — w]||? in a parallelogram add to the sum of the four squared side lengths 2||v||? + 2||w||?. (Recommended) If ||v|| = 5 and ||w|| = 3, what are the smallest and largest possible values of ||v — w||? What are the smallest and largest possible values of v - w? Challenge Problems Can three vectors in the zy plane have u - v < 0andv-w < 0andu - w < 0? I don’t know how many vectors in ryz space can have all negative dot products. (Four of those vectors in the plane would certainly be impossible . . .). Find 4 perpendicular unit vectors of the form (:t% :t%. j:%. :t%): Choose + or —. Using v = randn(3, 1) in MATLAB, create a random unit vector u = v/||v||. Using V = randn(3, 30) create 30 more random unit vectors U;. What is the average size of the dot products |u - U,|? In calculus, the average is fo\" |cos0|dO/m = 2/x. In the zy plane, when could four vectors v,, v2, v3,v4 not be the four sides of a quadrilateral ? We could not allow them to have any 4 lengths. 18 Chapter 1. Vectors and Matrices 1.3 Matrices and Their Column Spaces 12 1 2 1 A=1]3 4 |isa3by2matrix: m = 3rows and n = 2 columns. Rank 2. o 6 2 The 3 components of Ax are dot products of the 3 rows of A with the vector x : : 1 2] (1-7+2-81 [ 23] Row at a time 3 4 [7]= 3.7+4.8 | = | 53 A times z 56 L%) |57+68] |83 (1 2] . (1] (2 3 Az is also a combination of the columns of A 3 4 [ 8] =713 |+8] 4 5 6 5 6 4 The column space of A contains all combinations Az =z, a;+r2a- of the columns. {Rank one matrices : All columns of A (and all combinations Azx) are on one Iine./ Sections 1.1 and 1.2 explained the mechanics of vectors—linear combinations, dot products, lengths, and angles. We have vectors in R? and R® and every R\". Section 1.3 begins the algebra of m by n matrices: our true goal. A typical matrix A 1s a rectangle of m times n numbers—m rows and n columns. If m equals n then A is a “square matrix”. The examples below are 3 by 3 matrices. 1 0 0 (2 0 0 (2 1 -3 2 1 -3 ] 010 0 4 0 0O 4 7 1 4 7 0 0 1 0 0 5 0 0 5 -3 7 o Identity Diagonal Triangular Symmetric matrix matrix matrix matrix We often think of the columns of A as vectors a,, a;,...,a,. Each of those n vectors is in m-dimensional space. In this example the a’s have m = 3 components each: m = 3 rows i 1 -1 1 0 o0 n = 4 columns A=|a; a; a3 a4 | = 0 -1 1 O 3 by 4 matrix A I ) 0 0 -1 1 This is a “difference matrix” because A times x produces a vector Ax of differences like T2 — z1. How does an m by n matrix A multiply an n by 1 vector *? There are two ways to the same answer—we work with the rows of A or we work with the columns. The row picture of Az will come from dot products of x with the rows of A. The column picture will come from linear combinations of the columns of A. 1.3. Matrices and Their Column Spaces 19 Row picture of Ax Each row of A multiplies the column vector . Those multiplications row times column are dot products ! The first dot product comes from row 1 of A: (row 1) XL = (_17 17 OaO) * (171,272, $3,$4) =2 — I. It takes m times n small multiplications to find the m = 3 dot products that go into Azx. Three rows Three dot products Ax = . -1 0 0 1 -1 0 0 0] 1 0 -1 1] p- e I T2 I3 ﬂ -l rowl.x row2-.x hrow3-a: I — T I3z — T2 bz4—z3d (1) Notice well that each row of A has the same number of components as the vector z. Four columns of A multiply z, to 4. Otherwise multiplying Az would be impossible. Column picture of Ax The matrix A times the vector £ is a combination of the columns of A. The n columns are multiplied by the n numbers in . Then add those column vectors r;a;,...,IZ,a, to find the same vector Ax as in equation (1): (2) Az = z1(column a;) + z2(column a;) + z3(column a3) + z4(column ay4) This combination of n columns involves exactly the same multiplications as dot products of x with the m rows. But it is higher level! We have a vector equation instead of three dot products. You see the same result Az in equation (1) above and equation (3) below. p= - - - - - p- - - - e e -1 1 0 0 Ty — I Combination 2 1 Axr =1, Ol+x2]| -1 ]| +1x5 l1|4+z4]|0|=]x3—x2 | (3) of columns L O OJ - _1 b 1 - - $4 - .’B3 J Let me admit something right away. If I have numbers in A and &, and I want to compute Az, then I tend to use dot products : the row picture. But if I want to understand Az, the column picture is better. “The column vector Az is a combination of the columns of A.” We are aiming for a picture of not just one combination Ax of the columns (from a particular ). What we really want is a picture of all combinations of the columns (from multiplying A by all vectors x). This figure shows one combination 2a, + a2 and then it tries to show the plane of all combinations z,a; + zr2a; (for every r, and x3). az Figure 1.7: A linear combination of @, and a». All linear combinations fill a plane. The next important words are independence, dependence, and column space. 20 Chapter 1. Vectors and Matrices Here is a key point! Columns of A might or might not contribute anything new. They might be combinations of earlier columns (which we already included). Example 1 shows 3 columns that give 3 new directions. Those columns are “independent”: Example 1 (100 Each column gives a new direction. Independent A;=| 2 4 0 . o 3 columns 3 5 6 J Their combinations fill 3D space R™. If we look at all combinations of the columns, we see all vectors (b;. b>. b3) in 3D space. The first column z,(1, 2, 3) allows us to match any number b,. Then r,(0. 4, 5) leaves b, alone and we can match any number b,. Finally z3(0,0,6) doesn’t touch b; and b, and allows us to match any b3. We have found z,, 72, 3 so that A;x = b. Independence means : The only combination of columns that produces Az = (0,0, 0) is £ = (0,0,0). The columns are independent when each new column is a vector that we don’t already have as a combination of previous columns. Independence will be important. Example 2 (12 3 Column 1+ column 2 =column3 ! 1t2=3 Dependent A;=|1 4 5 , . 't fi 1+4=5 columns 6 0 6 J Their combinations don’t fill 3D space 6+0=6 The opposite of independent is “dependent”. These three columns of A, are dependent. Column 3 is in the plane of columns 1 and 2. Nothing new from column 3. I usually test independence going from left to right. The column (1, 1, 6) is no problem. Column 2 is not a multiple of column 1 and (2,4,0) gives a new direction. But column 3 is the sum of columns 1 and 2. The third column vector (3, 5, 6) is not independent of (1,1,6) and (2,4,0). That matrix A, only has two independent columns. If I went from right to left, I would start with independent columns 3 and 2. Then column 1 is a combination (column 3 minus column 2). Either way we find that the three columns are in the same plane : two independent columns produce a plane in 3D. That plane is the column space of this matrix : Plane = all combinations of the columns. Dependent columns in Example 2 column 1 4+ column 2 — column 3 is (0, 0, 0). E le3 A ; Now a; is 3 times a;. And a3 is 4 times a;. xample = P 3 5 15 20 | Every pair of columns is dependent. This example is important. You could call it an extreme case. All three columns of A3 lie on the same line in 3-dimensional space. That line consists of all column vectors (¢, 2¢, 5¢)— all the multiples of (1,2, 5). Notice that ¢ = 0 gives the center point (0, 0, 0). That line in 3D is the column space for this matrix A3. The line contains all vectors Aszz. By allowing every vector x, we fill in the column space of A3—and here we only filled one line. That is almost the smallest possible column space. The column space C(A) contains all vectors Az : All combinations of the columns. 1.3. Matrices and Their Column Spaces 21 Thinking About the Column Space of A “Vector spaces” are a central topic. Examples are coming unusually early. They give you a chance to see what linear algebra is about. The combinations of all columns produce the column space, but you only need independent columns. So we start with column 1, and go from left to right in identifying independent columns. Here are two examples A4 and As. 1 1 1 1 '1100] 011 1 011 0 A“‘0011 As=|9 0 1 1 0 0 0 1 1 0 0 1 A4 has four independent columns. For example, column 4 is not a combination of columns 1,2,3. There are no dependent columns in A45. Triangular matrices like A, are easy provided the main diagonal has no zeros. Here the diagonalis 1,1,1. 1. As is not so easy. Columns 1 and 2 and 3 are independent. The big question is whether column 4 is a combination of columns 1,2,3. To match the final 1 in column 4, that combination will have to start with column 1. To cancel the 1 in the top left corner of A5, we need minus the second column. Then we need plus column 3 so that —1 and +1 in row 2 will also cancel. Now we see what 1s true about this matrix A5 : Only 3 independent columns because Column 4 of A5 = Column 1 — Column 2 4+ Column 3. (4) The next step is to “‘visualize” the column space—all combinations of the four columns. That word is in quotes because the task may be impossible. I don’t think that drawing a 4-dimensional figure would help (possibly this is wrong). Aj; is a good place to start. Do you see why C(Ay) is the whole space R* ? If we look to algebra, we see that every vector v in R? is a combination of the columns. Here is the combination : [ v | (1] 1] (1] (1] v 0 1 1 1 v= vz =(v; — v2) 0 + (vo — v3) 0 + (v3 — vy) 1 + (ty) 1 (5) v 0, 0] 0] 1] This says: Every v is in the column space. We solved the four equations A,z = v! The short useful word “span” describes all the linear combinations of a set of vectors. So the span of the columns of A (independent or not) is the column space. Geometrically, here is one way to look at A4. The first column (1, 0, 0, 0) is responsible for a line in 4-dimensional space. That line contains every vector (¢,,0,0,0). The second column is responsible for another line, containing every vector (c3,c2,0,0). If you add every vector (c;,0,0,0) to every vector (cz, c2,0,0), you get a 2-dimensional plane inside 4-dimensional space. That plane is the span of columns 1 and 2. 22 Chapter 1. Vectors and Matrices That was the first two columns. The main rule of linear algebra is keep going. The last two columns give two more directions in R*. The four columns are independent. At the end, equation (5) shows how every 4-dimensional vector is a combination of the four columns of A4. The column space of A, is all of R®. If we attempt the same plan for the matrix As, the first 3 columns cooperate. But column 4 of As is a combination of columns 1,2,3. Those three columns combine to give a three-dimensional subspace inside R*. Column 4 happens to be in that subspace. That three-dimensional subspace is the whole column space C(As). We can only solve Asx = v when v is in C(A5). The matrix As only has three independent columns. I always write C(A) for the column space of A. When A has m rows, the columns are vectors in m-dimensional space R™. The column space might fill all of R™ or it might not. For m = 3, here are all four possibilities for column spaces in 3-dimensional space : 1. The whole space R® Ahas 3 independent columns 2. A plane in R® going through (0, 0, 0) 2 independent columns 3. Aline in R? going through (0,0, 0) 1 independent column 4. The single point (0.0.0) in R® A is a matrix of zeros ! Here are simple matrices to show those four possibilities for the column space C(A) : 1 0 0 (1 0 0] 1 0 0] 0 0 0 010 010 000 0 0 O 0 0 1] 0 0 0 |0 0 0 0 0 0| C(A)=R’=ryzspace C(A)=ryplane C(A)=xz axis C(A)=one point (0,0, 0) Author’s note The words “column space” did not appear in Chapter 1 of the 5th edition. I thought the idea of a space was too important to come so soon. Now I think that the best way to understand such an important idea is to see it early and often. It is examples more than definitions that make ideas clear—in mathematics as in life. Here is a succession of questions. With practice in the next section 1.4, you will find the keys to the answers. They give a real understanding of any matrix A. 1. How many columns of A are independent ? That number r is the “rank” of A. 2. Which are the first 7 independent columns ? They are a “basis” for the column space. 3. What combinations of those r basic columns produce the remaining n — r columns ? 4. Write any A as an m by r column matrix C times an r by n matrix R : A = CR. 5. (Amazing) The r rows of R are a basis for the row space of A : combinations of rows. Section 1.4 will explain how to multiply those matrices C' and R. The result is A = CR. C contains columns from A. Please notice that the rows of R do not come directly from A. 1.3. Matrices and Their Column Spaces 23 Matrices of Rank One Now we come to the building blocks for all matrices. Every matrix of rank r is the sum of r matrices of rank one. For a rank one matrix, all column vectors lie along the same line. That one line through (0, 0, 0) is the whole column space of the rank one matrix. 1 3 -2 ] Example A¢=| 4 12 -8 | hasrank r = 1. All columns: same direction ! 2 6 -4 Columns 2 and 3 are multiples 3a; and —2a, of the first column a; = (1,4,2). The column space C(Asg) is only a line containing all vectors ca; = (c, 4c, 2c). Here is a wonderful fact about any rank one matrix. You may have noticed the rows of Ag. All the rows are multiples of one row. When the column space is a single line in m-dimensional space, the row space is a single line in n-dimensional space. All rows of this matrix Ag are multiples of An example like Ag raises a basic question. If all columns are in the same direction, why does it happen that all rows are in the same direction? To find an answer, look first at this 2 by 2 matrix. Column 2 is m times column 1: a ma : — 9 A [ b mb ] Is row 2 a multiple of row 17 Yes! The second row (b, mb) is % times the first row (a.ma). If the column rank is 1, then the row rank is 1. To cover every possibility we have to check the case when a = 0. Thenrow 1 = [ 0 0 ] = 0 times row 2. Our 2 by 2 proof is complete. Let me look at this 3 by 3 matrix. Rank = 1 if a # 0. a ma pa Column 2 is m times column 1 A= b mb pb Column 3 is p times column 1 c mc pc Rows 2 and 3 are b/a and c/a times row 1 This matrix does not have two independent columns. Is it the same for the rows of A? Is row 2 in the same direction as row 1 ? Yes. Is row 3 in the same direction as row 1? Yes. The rule still holds. The row rank of this A is also 1 (equal to the column rank). Now jump from rank one matrices to all matrices. At this point we could make a guess : It looks possible that row rank equals column rank for every matrix. If A has r independent columns, then A has r independent rows. A wonderful fact! I believe that this is the first great theorem in linear algebra. So far we have only seen the case of rank one matrices. The next section 1.4 will explain matrix multiplication AB and lead us toward an understanding of “row rank = column rank” for all matrices. 24 Chapter 1. Vectors and Matrices Problem Set 1.3 This section introduced column spaces. But we don’t yet have a computational system to decide independence or dependence of column vectors. So these problems stay with whole numbers and small matrices. 1 Describe the column space of these matrices : a point, a line, a plane, all of 3D. 2 2 A=]11 Ay = 5 6 puaed pummd b oo WD 0 0 Agy=1{0 0 0 0 0 0] 1 1 0 As=12 1 1 1] 1 2 Find a combination of the columns that produces (0,0,0): column space = plane. The trivial combination (zero times every column) is not allowed. Which columns are 1 2 3] (1 4 7] dependent on Ai=14 5 6 Ao=1]12 5 8 earlier columns ? 7 8 9 | | 3 6 9 | 3 Describe the column spaces in R® of Band C': \"1 9 1 - B=1]121 C=|B -B (3 rows, 4 columns) |33, L . 4 Muluply Az and By and Iz using dot products as in (rows of A)-x: 2 1 2][1] 10 0] 4 1 0 0][ 21 Az=|4 2 4||2| By=|110 4 Iz=|0 1 0]]| 2 01 0ff5] 11 1]]10 0 0 1]| 23 | 5 Multiply the same A times x and B times y and I times 2z using combinations of the columns of A and B and I, as in Az = 1(column 1) + 2(column 2) + 5(column 3). 6 InProblem 5, how many independent columns does A have ? How many independent columns in B ? How many independent columnsin A + B ? 7 Canyoufind A and B (both with two independent columns) so that A + B has (a) 1 independentcolumn (b) No independent columns (c) 4 independent columns 8 The “column space” of a matrix contains all combinations of the columns. Describe the column spaces in R® of Aand Band C': 1 0 0] (2 4] 1 0 1 27 A=]101 0 B=|1 2 C=10 2 2 4 00 1 12 4 0 2 2 4 9 Find a 3 by 3 matrix A with 3 independent columns and all nine entries = 1 or 2. What is the maximum possible number of 1’s with independent columns ? 1.3. Matrices and Their Column Spaces 25 10 11 12 13 14 15 16 17 18 Complete A and B so that they are rank one matrices. What are the column spaces of A and B ? What are the row spaces of A and B? c[2a] e[ Suppose A is a 5 by 2 matrix with columns a; and az. We include one more column to produce B (5 by 3). Do A and B have the same column space if (a) the new column is the zero vector ? (b) the new columnis (1,1,1,1,1)? (c) the new column is the difference a; — a; ? Explain this important sentence. It connects column spaces to linear equations. Ax = b has a solution vector x if the vector b is in the column space of A. The equation Ax = b looks for a combination of columns of A that produces b. What vector will solve Ax = b for these right hand sides b ? Mt R N Find two 3 by 3 matrices A and B with the same column space = the plane of all vectors perpendicular to (1, 1,1). What is usually the column space of A + B? Which numbers g would leave A with two independent columns ? - - r - 1 0 2] 1 4 7 1 1 2 A=13 1 9 A=12 5 8 A=1]12 2 4 L50q_ _36q- bOOqi Suppose A times x equals b. If you add b as an extra column of A, explain why the rank r (number of independent columns) stays the same. True or false (a) If the 5 by 2 matrices A and B have independent columns, so does A + B. (b) If the m by n matrix A has n independent columns, then m > n. (c) A random 3 by 3 matnx almost surely has 3 independent columns. If A and B have rank 1, what are the possible ranks of A + B ? Give an example of each possibility. Why is rank 4 impossible ? Find the linear combination 38; + 482 + 583 = b. Then write b as a matrix-vector multiplication Sz, with 3,4, 5 in . Compute the three dot products (row of S) - x: 17 0 0 s1=1]1 so= |1 83 = | 0 | gointo the columnsof S. Ll. -la .IJ 26 19 21 23 24 Chapter 1. Vectors and Matrices If (a, b) is a multiple of (c, d) with abcd # 0, show that (a, c) is a multiple of (b, d). This is surprisingly important; two columns are falling on one line. You could use numbers first to see how a, b, ¢, d are related. The question will lead to: If [ z 3 ] has dependent rows, then it also has dependent columns. Solve these equations Sy = b with 8,, 82, 83 in the columns of the sum matrix S': (1 0 0] [wn] [1° 1 0 0] [wn] [17 1 10 po|l=|1]ad |1 1 O y2 | =14 11 1)|w] |1, 11 1] ]y 9 The sum of the first 3 odd numbers is . The sum of the first 10 1s . Solve these three equations for y;, y2, y3 in terms of ¢;, ¢2, c3: Pl 0 0- Pqu (61- Sy=c 1 1 0]||yp]|=]c _1 1 1.. | Y3 | C3 | Write the solution y as a matrix A times the vector c. A is the “inverse matrix” S~1. Are the columns of S independent or dependent ? The three rows of this square matrix A are dependent. Then linear algebra says that the three columns must also be dependent. Find x # 0 that solves Az = 0: 1 2 3 Row 1 + row 2 = row 3 A=13 6 A has only two independent rows 4 9 Then only two independent columns Which numbers c give dependent columns ? Then a combination of columns is zero. c ¢ c 1 s[5 3 6 § 5 . r 11 3 2 7 4 O =0 3 1 0 ¢ 110 011 J If the columns combine into Az = 0 theneachrow of Ahasrow:.x =0: i 1[z] [0] \"rexz] [O] If |a1 a2 a3 | |z2]| =10 thenbyrows [ rcx | = | 0 L - -I3J .0.4 5r3.z.. -O- The three rows also lie in a plane. Why is that plane perpendicular to z ? 1.4. Matrix Multiplication AB and CR 27 1.4 Matrix Multiplication AB and CR ﬁ To multiply AB we need row length for A = column length for B. \\ 2 The number in row ¢, column j of AB is (row 2 of A) - (column j of B). 3 By columns: A times column j of B produces column j of AB. 4 Usually AB is different from BA. But always (AB)C = A (BC). {If A has r independent columns in C,then A = CR = (m x r) (r x W We know how to multiply a matrix A times a column vector & or b. This section moves to matrix-matrix multiplication : a matrix A times a matrix B. The new rule builds on the old one, when the matrix B has columns by, b,, ..., b,. We just multiply A times each of those p columns of B to find the p columns of AB. Column j of AB equals A times column j of B - - - - (1) IfB=|b;---b,| then AB = | Ab, --- Abp L - - - To see that clearly, start with a 2 by 2 “exchange matrix” for B. So B has two columns b, and b,. We multiply A times each column to produce a column of AB: 1 2{|0 2 1 2(|1 1 1 2(/0 1 2 1 o B B R IR B Bl S H S i For this matrix B, the result of multiplying AB is to exchange the columns of A. There is more to see when we multiply the same A by a full 2 by 2 matrix B: 1 2 5 6 1 2 5 1 2 6 (o ][5 8] pman= 3 T][7 =[5 0] [5] Here is the point. We can multiply Ab, (matrix times vector) the row way or the column way. The row way uses dot products of b; with every row of A: Row way Ab, = 1 25| |rowl:by| |1-5+2-7]_ 1|19 2) Dot products L= 13 4| (7| |row2-b;| [3-5+4-7| |43 The column way uses a combination of the columns of A to find Ab,. Same result: Column way |1 29| _.]1 2] | S 141 |19 Combine columns Abl_[3 4][7]—5[3]+7[4]_[15]+[28]_[43] (3) Both ways use the same 4 multiplications. With numbers like these, I think most people choose the row way. To multiply A B, take the dot product of each row of A with each column of B. When A has 2 rows and B has 2 columns, that means 4 dot products. 28 Chapter 1. Vectors and Matrices When A is m by n and B is n by p, then AB is m by p. So we need mp dot products. Row way _{rowl-coll rowl-col2| (19 22 Rows of A row2.coll row2-col2| |43 50 Now compute AB the column way: combinations of columns of A. This is a vector operation and it produces whole columns of AB. Equation (3) found the first column. Now we find 22 and 50 in the second column of AB from A times b, : Column way (1 2}16( |1 2] | 6 16| |22 for Ab, Ab\"\"[3 4][8]—6[3]+8[4]—[18]+[32] = [50] ©) Equations (3) and (5) gave the same two columns of AB as equation (4). Both ways use the same 8 multiplications; only the order is different. To multiply an m by n matrix A times an n by p matrix B, we can count the small multiplications: AB is m by p. (4) Row way mp dot products in AB, n multiplications each: mnp small multiplications Column way p columns in AB, mn multiplications each : mnp small multiplications The actual speed will depend on how the matrices are stored. I think column storage is usual. Please note that it is faster to move large pieces of a matrix from storage rather than individual numbers. In a big multiplication, matrix-matrix operations using BLAS 3 (Level 3 Basic Linear Algebra Subprograms) are the best. The comparison with Level 1 (vector-vector) and Level 2 (matrix-vector) is online at netlib.org/blas/. So far we have used (row)-(column) dot products and (matrix)(column) Ab; in multiplying AB. The other two ways are (row) (matrix) and (column) (row), coming soon. All four ways use the same mnp multiplications in varying orders to find AB. If A and B are 2 by 2, that means n® = 8 small multiplications for AB.T See below. AB is usually different from B A For B =[(1) (1)],AB exchanged the columns of A. But B A exchanges the rows of A'! 2 1 0 1 1 2 3 4 AB‘[4 3} BA‘[lo][s 4]‘[1 2] (©) Matrix multiplication is not commutative. In general BA # AB. Multiply A on the left for row operations on A, and multiply on the right by B for column operations on A. Question Why does squaring the exchange matrix give B = [ (1) (1) ] [ 0 1 ] = T Strassen noticed that 7 multiplications are enough for 2 by 2 matrices, at the cost of extra additions. For n by n matrices this reduces the multiplication count to n¢, where ¢ = log, 7 instead of the usual ¢ = log, 8 = 3. Hard work has now reduced c even more. Certainly ¢ cannot go below 2, because all of the n? entries in A and B must be used. Finding the smallest exponent c is an extremely tough unsolved problem. 1.4. Matrix Multiplication AB and CR 29 AB times C = A times BC For matrix multiplication, this associative law is true. We are not willing to give up this extremely useful law. We can multiply AB first or we can multiply BC first. The matrices stay in the order A, B, C and their sizes must be right for multiplication : Aismxn Bisnxp Cispxq. ThenAB is mxp and (AB)C is m xq. We can test the law using the exchange matrix B on the rows and the columns of A: 0 111 210 1] [3 4 0 1] 4 3 (BA)B__I 0|3 4][1 0] |1 2”104‘_2 1 0 1]]1 2][0o 1] [o 1 2 1] [4 3 B(AB)—LI 0[[3 4f]l1 0] |1 0”4 3] 121 So row operations on A can come before or after column operations on A. Notice the meaning of (AB)C = A(BC) when C is just a column vector x. If that vector has a single 1 in component j, then the associative law is (AB)x = A(Bx). This tells us how to multiply matrices ! The left side is column 7 of AB. The right side is A times column j of B. So their equality is exactly the rule for matrix multiplication that we saw in equation (1). It is simply the right rule. Let me bring together the important facts about ABC and also A times B + C': Associative (AB)C = A(BC) and Distributive A(B+C)=AB+ AC| (7) Review of AB Dot products (Row 2 of A)+(Col 5 of B)= (AB)z-]- = number in row i.col j of AB Combine columns (Matrix A) (Column b; of B) = vector in column j of AB With numbers (the usual way), mp dot products produce the m by p matrix AB. With vectors (the big picture), p combinations Ab; produce the p columns of AB. For computing by hand, I would use the row way to find each number in AB. I visualize multiplication by columns: The columns Ab; in A B are combinations of columns of A. Rank One Matricesand A = CR All columns of a rank one matrix lie on the same line. That line is the column space of A. Examples in Section 1.3 pointed to a remarkable fact: The rows also lie on a line. When all the columns of A are in the same column direction, then all the rows of A are in the same row direction. Here is a new example of this extreme case: rank r = 1. 1 2 10 100 ] rank one matrix Example 1 A=13 6 30 300 | = oneindependentcolumn | 2 4 20 200 one independent row ! 30 Chapter 1. Vectors and Matrices All columns are multiples of (1,3,2). All rows are multiplesof [ 1 2 10 100 ]. Only one independent row when there is only one independent column. Why is this true ? Another example : Matrix of all 1’s = (Column of 1°s) times (Row of 1°’s). Our approach is through matrix multiplication. We factor A into C times R. For this very special matrix, C has one column and R has one row. CR is (3 X 1) (1 x 4). 1210 100] [1][1 2 10 100 ] A=|3 6 30 30 (|=|3 =CR| (8 2 4 20 200 |2 Rank =1 The dot products (row of C):(column of R) are small multiplications like 1 times 1. The last dot product is 2 times 100. We are following the dot product rule! This is multiplication of thin matrices C R. 12 small multiplications produce the 12 numbers in A. The rows of A are numbers 1,3,2 times the (only) row [ 1 2 10 100 | of R. By factoring this special A into one column times one row, the conclusion jumps out: If the column space of A is a line, the row space of A is also a line. One column in C, one row in R. Our next goal is to allow r columns in C and to find r rows in R. And to see A = CR. That number r is the “rank” of A. C Contains the First » Independent Columns of A Suppose we go from left to right, looking for independent columns in any matrix A: If column 1 of A is not all zero, put it into the matrix C If column 2 of A is not a multiple of column 1, put it into C If column 3 of A is not a combination of columns 1 and 2, put it into C. Continue. At the end C will have r columns taken from A. That number r is the rank of A and C. The n columns of A might be dependent. The r columns of C will surely be independent. Independent No column of C is a combination of previous columns columns No combination of columns gives Cx = 0 except * = all zeros Those r independent columns in C combine to give all n columns in A. Cz = 0 means that z;(column 1of C) + z3(column2of C) + --- = zero vector. With independent columns, Cx = O only happens if all z's are zero. Otherwise we can divide by the last nonzero coefficient £ and that column would be a combination of the earlier columns—which our construction forbids. C' always has independent columns. (2 6 4] (2 4 Example 2 A=14 12 8 leadsto C=| 4 8 Rank r = 2 1 3 5 1 5 Columns 1 and 3 gointo C. Column 2 is 3 times column 1: not independent, not in C. 1.4. Matrix Multiplication AB and CR 31 Matrix Multiplication C times R R tells how to produce all columns of A from the columns of C. Then A = CR. The first column of A is actually in C, so the first column of R just has 1 and 0. The third column of A comes second in C, so the third column of R just has 0 and 1. Notice I (2 6 4] [2 4° 1 7 0 inside R A=CRis | 4 12 8 |=]|4 8 [O o 1]- %) Rank r = 2 1 3 5| [1 5 ' Two columns of A went straight into C, so part of R is the identity matrix. The question marks are in column 2 because column 2 of A is not in C. It is a dependent column. Column 2 of A is 3 times column 1, so that number 3 goes into R. Aism xn 2 6 4 (2 4 ] 1 3 0 Cismxr A=CRis | 4 12 8| =14 8 [O 0 1] (10) Risr xn 1 35 [1 5 That example is typical of A = C R. We review the descriptions of C and R. 1. C contains a full set of r independent columns (chosen left to right) in A 2. R= [ I F ] contains the identity matrix I in the same r columns that held C. 3. The dependent columns of A are combinations C F' of the independent columns in C. That matrix F goes into the other n — r columns of R = [I F]. A = CR becomes A=C[I F]=[C CF]=[indepcolsof A dep colsof A | (in correct order) C has the same column space as A. R has the same row space as A. Here F' = [ - ] 2 Example 3 1 23] [1 2][1 0 -1 of A=CR 5 6 | = 5 01 2 (11) Rank 2 | 7 8 9 | 7 8 When a column of A goes into C, a column of I goes into R. Column j of A = C times column j of R. Row i of A = row t of C times R. If all columns of A are independent, then C = A. What matrixis R? Answer R = I. 32 Chapter 1. Vectors and Matrices Chapter 1 finds C (independent columns of A) before R. Chapter 3 will find R first. Here column 3 of A is the 2nd independent column in C. Then column 3 of R is 1 1 2 3 4 1 3 1 2 0 1 A—[l 9 4 5]—[1 4][0 0 1 1]=CR All three ranks = 2 R tells how to recover all columns of A from the independent columns in C. Here is an informal proof that the row rank of A equals the column rank of A 1. The r columns of C are independent (they are chosen that way from A) 2. Every column of A is a combination of those r columns of C (this is A = CR) 3. The r rows of R are independent (they contain the r by r matrix I) 4. Every row of A is a combination of the r rows of R (thisis A = CR by rows!) How to Find the Matrix R Up to now you have had very little help in discovering the matrix R in A = CR. If you could tell that column 3 of this matrix A is a combination of columns 1 and 2, then the numbers z and y in that combination will go into column 3 of R : 1 3 4° (1] (3] [4° Example4 A= 4 2 zl2|+yld4l=|2]|. (12) |3 7 6 A I G N But even for this small matrix, we can’t immediately see z and y. So we don’t know the rank of A (2 or 3?). There has to be a good way to discover z and y. That good way is elimination. It will be the key algorithm in Chapter 2 for square matrices and again in Chapter 3 for all matrices. We want to introduce it now for this matrix. The idea is to simplify A by “row operations”. That will simplify the equations for x and y. We will eliminate the 2 and 3 in column 1 of A. To do that, subtract 2 times row 1 from row 2 of A and also subtract 3 times row 1 from row 3. The matrix A changes to B. p- - o - - po - 1 3 4 1 3] o[ 4 B=|0 -2 -6 z| 0 |+y| -2 |=] -6 (13) 0 -2 -6 0] L-2] [-6 We only did what is legal. Subtracting an equation from an equation leaves a new equation. The new equation is =2y = -6, so we know y = 3. Then if £ = —35 the top equation becomes —5 + 9 = 4, which is correct. The original equations (12) are solved by —5, 3: L s 17 [3] [4] -5| 21434 ]|=]|2 Column 3 of A is dependent y=+3 .3.J .7J ..6.1 1.4. Matrix Multiplication AB and CR 33 So —5 and 3 are the numbers we needed in column 3 of R. All the ranks are r = 2: 1T T 11 0 -5 01 3|=CR (14) There is more to see in this example. The elimination process that reduced A to B is called row reduction. 1 will complete it from B to U, to make the matrix even simpler. Just subtract row 2 of B from row 3 of B to see a row of zerosin U : B 3 41 1 3 4 upper triangular A>B=|0 -2 -¢6|oU=|l0 -2 -6 |= ppmatﬁxlgj (15) 0 -2 -6 ‘0 0 0 That zero row is a clear signal: the row rank is also 2. Chapter 2 will stop with U. Chapter 3 will eliminate upward to produce more zeros. We end up with Ro and R: 1 3 4] 1 3 4] 1 0 -5 ] U=]|10 -2 -6 -0 1 31 -0 1 3 | =Ro 0 0 0 0 0 0| 0 0 0 All rows of Ry are combinations of the original rows of A That zero row of Ry shows that A has rank r = 2 The 2 by 2 identity matrix shows that columns 1.2 of A are independent (in () Removing the zero row of Ry leaves the desired matrix Rin A = CR Elimination in Chapter 3 will be a systematic way to find R Key facts | The r columns of C are a basis for the column space of A : dimension r A = CR | The r rows of R are a basis for the row space of A : dimension r Those words “basis” and ‘“dimension” will be properly defined later in Section 3.4. Chapter 1 starts with independent columns of A, placed in C. Chapter 3 starts with the rows of A, and combines them into R. We are emphasizing C R because both matrices are so important. C contains r independent columns of A. R tells how to combine those columns to give all columns of A. (R contains I, because r columns of A are already in C.) Chapter 3 will produce R directly from A by elimination, the most used algorithm in computational mathematics. A = CR will be the key to a fundamental problem: Solving linear equations Ax = b. 34 Chapter 1. Vectors and Matrices Columns of A times Rows of B ... Columns of C times Rows of R Before this chapter ends, I want to add this message. There 1s another way to multiply matrices (producing the same matrix AB or C'R as always). This way is not so well known, but it is powerful. The new way multiplies columns of A times rows of B. I AB=|a; - - a, ; =a,b] + azb; +--- + a,b. (16) B | | [— b — columns a; rows b, Add columns a; times rows b Those matrices axb, are called outer products. We recognize that they have rank one: column times row. They are entirely different from dot products (rows times columns). If Ais an m by n matrix and B is an n by p matrix, then columns of A times rows of B adds up to the same answer AB as dot products of rows of A and columns of B. AB involves the same mnp small multiplications but in a new order ! (Row) - (Column) mp dot products, n multiplications each total mnp (Column) (Row) n rank one matrices, mp multiplications each total mnp 17 8 9] [1][7 8 9] [4][10 11 12] 10 11 12|=|2 + |5 3 6 - - - N - 1 4 Columns x Rows 2 5 3 6 7 8 971 T[40 44 48 47 52 57 Rank1 + Rank1=|14 16 18| +/50 55 60| =|64 71 78| = AB 21 24 27 |60 66 72| |81 90 99 This example has mnp = (3) (2) (3) = 18. At the start of the second line you see the 18 multplications (in two 3 by 3 matrices). Then 9 additions give the correct answer AB. As we learned in this section, the rank of AB is 2. Two independent columns, not three. Two independent rows, not three. The next chapter uses different words. AB has no inverse matrix : it is not invertible. And in Chapter S : The determinant of AB is zero. Note about the matrix R We were amazed to learn that the row matrix R in A = CR is already a famous matrix in linear algebra! It is essentially the “reduced row echelon form” of the original A. MATLAB calls it rref (A) and includes m — r zero rows. With the zero rows, we call it Rp. The factorization A = C R is a big step in linear algebra. The Problem Set will look closely at the matrix R, its form is remarkable. R has the identity matrix in 7 columns. Then C multiplies each column of R to produce a column of A. Rg comes in Chapter 3. 1 0 3 Examples A -_ al 0.2 30’1 +4a2 ] = [ al a’2 ] [ 0 1 4 |-cr 1.4. Matrix Multiplication AB and CR 35 Here a; and a; are the independent columns of A. The third column is dependent— a combination of a; and a;. Therefore it is in the plane produced by columns 1 and 2. All three matrices A, C, R have rank r = 2. We can try that new way (columns X rows) to quickly multiply C'R in Example 5: Columns of C times rows of R CR:al[l 0 3]+a2[0 1 4]=[al a: 301+402]=A (Column 3 of A) (Row 7 of B) = Rank 1 Matrix J=1to?2 2 matrices (3by 2)(2by4)=(3 by 4) Four Ways to Multiply AB = C [' T T z] (Row 2 of A) - (Column k of B) = Number Cj; Tz T t=1to3 k=1tod 12 numbers T T [' T T m} A times (Column k of B) = Column k of C T T T k=1to4 4 columns ( [-] (Row ¢z of A) times B = Row i1 of C T T . 1=1t03 3 rows b Dot product way, Column way, Row way, Columns times rows Problem Set 1.4 1 Rewrite this four-way table for AB = C when A4 is m by n and B is n by p. How many dot products and columns and rows and rank one matrices go into AB? In all four cases the total count of small multiplications is mnp. 2 If all columns of A = [ a a a ] contain the same a # 0. what are C and R ? 3 Multiply A times B (3 examples) using dot products: (each row) - (each column). T“Tr 1.0 0 1 0o0] [1 2 3][4] 4 ][1 2 3] 1 10(]-1 10 5 5 11 1] 1 -11 6 | |6 4 Test the truth of the associative law (AB)C = A(BC). (a)[l'l][i][lll] (b)[(l)f”(l)?][ 36 10 11 12 13 14 Chapter 1. Vectors and Matrices Why is it impossible for a matrix A with 7 columns and 4 rows to have 5 independent columns ? This is not a trivial or useless question. Going from left to right, put each column of A into the matrix C if that column is not a combination of earlier columns: (2 -2 1 6 0 [ 2 ] A=]1 -1 0 2 0 C=]1 '3 306 1 3 ] Find R in Problem 6 so that A = CR. If your C has r columns, then R has r rows. The 5 columns of R tell how to produce the 5 columns of A from the columns in C. This matrix A has 3 independent columns. So C has the same 3 columns as A. What is the 3 by 3 matrix R so that A = CR? What is different about B = CR? (2 2 2 (2 2 2] Uppertriangular A=| 0 4 4 B=|0 0 4 10 0 6 | 0 0 6 Suppose A is a random 4 by 4 matrix. The probability is 1 that the columns of A are “independent”. In that case, what are the matrices C and Rin A = CR? Note Random matrix theory has become an important part of applied linear algebra— especially for very large matrices when even multiplication AB is too expensive. An example of “probability 1” is choosing two whole numbers at random. The probability is 1 that they are different. But they could be the same! Problem 10 is another example of this type. Suppose A is a random 4 by 5 matrix. With probability 1, what can you say about C and Rin A = CR? In particular, which columns of A (going into C') are probably independent of previous columns, when you go from left to right ? Create your own example of a 4 by 4 matrix A of rank r = 2. Then factor A into CR = (4by 2) (2 by 4). Factor these matrices into A = CR = (m by r) (r by n) : all ranks equal to 7. {1 2 3 (01 2 3 12 1 3 [1 0 0 4 A“[l 3 4] A\"\"[o 1 3 5] A3‘[6 3 9] A“‘[o 2 2 o] Starting from C' = [;] and R= [2 4] compute CR and RC and CRC and RCR. Complete these 2 by 2 matrices to meet the requirements printed underneath: T e [P s 3 4 -3 rank one orthogonal columns rank 2 A2 =] 1.4. Matrix Multiplication AB and CR 37 15 16 17 18 19 20 Suppose A = CR with independent columns in C and independent rows in R. Explain how each of these logical steps follows from A = CR = (m by r) (r by n). 1. Every column of A is a combination of columns of C. 2. Every row of A is a combination of rows of R. What combination is row 1? 3. The number of columns of C = the number of rows of R (needed for C R). 4 . Column rank equals row rank. The number of independent columns of A equals the number of independent rows in A. (a) The vectors ABx produce the column space of AB. Show why this vector ABz is also in the column space of A. (Is ABx = Ay for some vector y ?) Conclusion : The column space of A contains the column space of AB. (b) Choose nonzero matrices A and B so the column space of AB contains only the zero vector. This is the smallest possible column space. True or false, with a reason (not easy): (a) If 3 by 3 matrices A and B have rank 1, then AB will always have rank 1. (b) If 3 by 3 matrices A and B have rank 3, then AB will always have rank 3. (c) Suppose AB = BA for every 2 by 2 matrix B. Then A = [ 8 (c) ] = cl for some number c. Only those matrices A = cI commute with every B. This section mentioned a special case of the law (AB)C = A(BC). : 0 1 1 2 A—C—exchangematnx[l O} B—[3 4]. (a) First compute AB (row exchange) and also BC' (column exchange). (b) Now compute the double exchanges: (AB)C with rows first and A(BC') with columns first. Verify that those double exchanges produce the same ABC. Test the column-row matrix multiplication in equation (16) to find AB and BA: 1 0 0] ] 1 0 |11 1] [ 1 1 O O = 0 0 1 0 1 1 ek pud b poad poud foud 1 1] 1 1 0 1| 1 1 0 1 0 0 1 l How many small multiplications for (AB)C and A(BC) if those matrices have sizes ABC = (4 x 3) (3 x 2) (2 x 1) ? The two counts are different. 38 Chapter 1. Vectors and Matrices Thoughts on Chapter 1 Most textbooks don’t have a place for the author’s thoughts. But a lot of decisions go into starting a new textbook. This chapter has intentionally jumped right into the subject, with discussion of independence and rank. There are so many good 1deas ahead, and they take time to absorb, so why not get started ? Here are two questions that influenced the writing, What makes this subject easy ? All the equations are linear. What makes this subject hard? So many equations and unknowns and ideas. Book examples are small size. But if we want the temperature at many points of an engine, there is an equation at every point: easily n = 1000 unknowns. I believe the key is to work right away with matrices. Ax = b is a perfect format to accept problems of all sizes. The linearity is built into the symbols Ax and the rule is A(z + y) = Az + Ay. Each of the m equations in Az = b represents a flat surface: 2r + 5y — 42 =6 is a plane in three-dimensional space 2z +5y—42+ 7w =19 isa3D plane (hyperplane?) in four-dimensional space Linearity is on our side, but there is a serious problem in visualizing 10 planes meeting in 11-dimensional space. Hopefully they meet along a line: dimension 11 — 10 = 1. An 11th plane should cut through that line at one point (which solves all 11 equations). What the textbook and the notation must do is to keep the counting simple Here is what we expect for a random m by n matrix A: m < n Probably many solutions to the m equations Ax = b m = n Probably one solution to the n equations Az = b m > n Probably no solution : too many equations with only n unknowns in & But this count is not necessarily what we get! Columns of A can be combinations of previous columns : nothing new. An equation can be a combination of previous equations. The rank r tells us the real size of our problem, from independent columns and rows. The beautiful formulais A = CR = (m x r) (r x n): three matrices of rank r. Notice : The columns of A that go into C must multiply the matrix I inside R. We end with the great associative law (AB) C = A (BC). Suppose C has 1 column: AB has columns Ab,,...,Ab, and then (AB)c equals c; Ab; + --- + c,Abn. Bc has one column ¢, b; + - -+ + c,b,, and then A(Bc) = A(ci1by + -+ + cnbn). Linearity gives equality of those two sums. This proves (AB) c = A (Be). The same is true for every column of C. Therefore (AB) C = A (BC). Notice that over and over—for Az and AB and C R—we write about linear combinations of columns of A or C. Not about dot products with the rows ! 2 Solving Linear Equations Ax = b 2.1 Elimination and Back Substitution 2.2 Elimination Matrices and Inverse Matrices 2.3 Matrix Computations and A = LU 24 Permutations and Transposes 2.5 Derivatives and Finite Difference Matrices The matrices in this chapter are square: n by n. Ax = b gives n equations (one from each row of A). Those equations have n unknowns in the vector . Often but not always there is one solution = for each b. In this case A has an inverse A~ with A714 =T and AA~! = I. Multiplying Az = b by A~! produces the symbolic solution z = A~!b. This chapter aims to find that solution . But we don’t compute A~!. (That would solve Az = b for every possible b.) We go forward column by column, assuming that A has independent columns. We only stop if this proves wrong. At the end Az = b has changed to a triangular system Uz = c, and now the solution x is easy to find. i Az =Db Square Qg back Uz = : -— U R - Tr=c matrix b > > c > - | x —1 .. . z=U\"\"¢c A elimination | 7.r05 substitution r= A-1p Here is an idea that goes back thousands of years (to China). Each step of “elimination” produces a zero in the matrix. The original A changes slowly into an upper triangular U'. We may need row exchanges. This is not exciting, it is just the natural way to simplify A. To describe all the steps we need matrices. This is the point of linear algebra' A simple elimination matrix E;; produces a zero where row ¢ meets column j (¢ > j). Overall, an elimination matrix £ multiplies A to give EA = U. And we multiply U by an inverse matrix L = E~! to come back to A. Here are key matrices in this chapter: Coefficient matrix A Upper tniangular U Lower triangular L Elimination matrix E;; Overall elimination E Inverse matrix A™! Permutation matrix P Transpose matrix AT Symmetric matrix § = ST Our goal is to explain all the steps from A to EA=U to A=E-U=LU to . (If the steps fail, this signals that Az = b has no solution for most b.) Every computer system has a code to find the triangular U and then the solution . Those codes are used so often that elimination adds up to the greatest cost in all of scientific computing. But the codes are highly engineered and we don’t know a better way to solve Ax = b. Section 2.5 introduces difference matrices from Computational Science and Engineering. 39 40 Chapter 2. Solving Linear Equations Az = p 2.1 Elimination and Back Substitution /l Elimination subtracts ¢;; times row j from row ¢, leave a zero in row . W 2 Az = bbecomes Uz = c(orelse Ax = b is proved to have no solution). \\3 Then Uz = cis solved by back substitution because U is upper triangularj This chapter explains a systematic way to solve Ax = b: n equations for n unknowns. The n by n matrix A is given and the n by 1 column vector b is given. There may be no vector ¢ = (1,,Iy,...,Z,) that solves Az = b, or there may be exactly one solution, or there may be infinitely many solution vectors . Our job is to decide among these three possibilities and to find all solutions. Here are the possibilities with n = 2. 1 Exactly one solution to Ax = b. In this case A has independent columns. The rank of A is 2. The only solution to Az = 0is x = 0. A has an inverse matrix A~!. Example with one solution (z,y) = (1,1) 2r+3y=5 2 3 Independent columns (2,4) and (3, 2) 4z +2y =6 4 2 2 No solution to Az = b. In this case b is not a combination of the columns of A. In other words b is not in the column space of A. The rank of A is 1. Example with no solution 2r+3y= 6 2 3 Dependent columns (2, 4) and (3, 6) 4z + 6y = 15 4 6 Subtract 2 times the first equation from the second to get 0 = 3. No solution. 3 There will be infinitely many solutions to AX = 0 when the columns of A are not independent. This is the meaning of dependent columns—many ways to produce the zero vector b = 0. We can multiply X by any number a. If there is one solution to Az = b then we can add any solution to AX = 0. All the vectors £ + aX solve the same equations, so we have many solutions. Forany numbera A(z+aX) = Az+aAX =b+0 = b. (1) Example with infinitely many solutions 2z +3y= 6 2 3 A has dependent columns: bisin C(A) 4z + 6y = 12 4 6 Those equations Az = b are solved by £ = 0,y = 2. But there are more solutions because X = (3,—2) solves AX = 0. Then 2X = (6, —4) also solves A(2X) = 0. All vectors aX can be added to the particular solution £ = (0, 2) to produce more solutions: z +aX = (0 + 3a,2 — 2a) is a line of solutions to our two equations Az = b. This chapter will start with Uz = c: one solution, easy to find. Then we explain how Ax = b leads to Uz = c. When this fails, a row exchange may save it. When row exchanges also fail, A has no inverse matrix A=, Its columns are dependent (cases 2 — 3). 2.1. Elimination and Back Substitution 41 Back Substitution to Solve Ux = ¢ This chapter will give a systematic way to decide between those possibilities 1,2,3: One solution, no solution, infinitely many solutions. This system is called elimination. It simplifies the matrix A without changing any solution x to the equation Ax = b. We do the same operations to both sides of the equation, and those operations are reversible. Elimination keeps all solutions & and creates no new ones. Let me show you the ideal result. Elimination produces an upper triangular matrix. That matrix is called U. Then Ax = b leads to Ux = c, which we easily solve : Apply elimination to Ax = b (next page) 2 3 4] [xz;] [19] The result is Ux = c (here) Ux=cis |0 5 6 ro | =| 17 Back substitution now finds = 0 0 7])|z3_ | 14 | That letter U stands for upper triangular. The matrix has all zeros below its diagonal. Highly important: The “pivots” 2,5,7 on that main diagonal of U are not zero. Then we can solve the equations by going from bottom to top : find 3 then x then x,. Back substitution The last equation 7r3 = 14 gives T3 = 2 Work upwards The next equation Sry + 6(2) = 17 givesxo = 1 Upwards again The first equation 2r; + 3(1) + 4(2) =19 givesx; = 4 Conclusion The only solution to this example Uz = cis|x = (4.1.2). Special note In solving for ;. r2, x3 we needed to divide by the pivots 2.5. 7. These pivots were probably not on the diagonal of the original matrix 4 (which we haven’t seen). The pivots 2,5,7 were discovered when “‘elimination™ produced the lower triangular zeros in U. This crucial step from A to U is still to be explained' We have displayed the final back substitution step, next we explain elimination. Equations Az =>b Elimination to Uz =c Back substitutiontoz=U\"1c=A\"1} Note We would not allow the number zero to be a pivot. That would destroy our plan because an equation like 0z; = 2 or Or2 = 5 or Oxr3 = 8 has no solution. Back substitution will break down with a zero in any pivot position (on the diagonal of U). The test for independent columns in A is n nonzero pivots in U (after possible row exchanges). Every square matrix A with independent columns (full rank) can be reduced to a trian- gular matnix U with nonzero pivots. This is our job. It is possible that we may need to put the equations Az = b in a different order. We start with the usual case when elimination goes from A to U. Then back substitution as above finds the solution vector to Ax = b. 42 Chapter 2. Solving Linear Equations Az = b From A to U and b to c: Elimination in Each Column First comes a matrix A (independent columns) that will require no row exchanges. We will apply elimination matrices E5; then E3; then E3;. A and b will change to U and c. The starting matrix is A 2 3 4 1 [ 19 | The first pivot is 2 A=]411 |4 b= | 55 (2) The right side is b 2 8 17 | 50 E2; multiplies equation 1 by 2 and subtracts from equation 2. You see the new zero. p- p- - - 1 0 0 2 3 4 19 Ex=1|-2 1 0 Ex yA=10 5 6 Ea b= | 17 (3) 0 0 1 2 8 17 | 50 | This produced the desired zero in column 1. It changed equation 2. To produce another zero, we subtract row 1 from row 3 using E3,. This completes elimination in column 1: p- - p- 1 0 0] 2 3 4 19 | Ez;=1 0 1 0 Ey1E5A=10 5 6 17 4) -1 0 1 |0 5 13 | 31 Move now to column 2 and row 2 (the second pivot row). The pivot is 5, on the diagonal. To eliminate the 5 below it, multiply row 2 by the number 1 and subtract from row 3. 1 0 0] (2 3 4] 19 Exx=1| 0 1 0 U=|0 5 6 c=| 17 (5) 0 -1 1 0 0 7 | 14 | E32FE3,Ez; A = U is triangular. £ = (4,1,2) solved Uz = c on page 41 and ¢ = (4,1,2) solves Az = b here. Since U has 2,5, 7 on its diagonal we know that back substitution will succeed. The columns of U are independent (and therefore the columns of the original A were independent, as we will see). The matrices A and U have full rank. We can summarize the elimination steps when no row exchanges are involved. Use the first equation to produce zeros in column 1 below the first pivot. Use the new second equation to clear out column 2 below pivot 2 in row 2. Continue to column 3. The expected result is an upper triangular matrix U. Elimination on A produces U. The same steps were applied to the right hand side b. Those steps produce a new right hand side c. The new equations Uz = ¢ (equivalent to the old equations Az = b) are solved by back substitution (previous page): = = (4, 1, 2). 2.1. Elimination and Back Substitution 43 Possible Breakdown of Elimination Elimination might fail. Zero can appear in a pivot position. Subtracting that zero from lower rows will not clear out the column below the unwanted zero. Here is an example : o . F'2 3 4q \"2 3 4- climination nconmn1 ~ A=|4 6 1[0 0 6|=B 2 8 17| |0 5 13 The cure is simple if it works. Exchange row 2 with the zero for row 3 with the 5. Then the second pivot is 5 and we can clear out the second column below that pivot. Elimination continues to U as normal after the row exchange by the matnix P. - p= - 1 0 012 3 4 2 3 4 PB=]|0 0 1 0 0 6|=]0 5 13 01 0]]|0 5 13 0 0 6 For this small example, the row exchange is all we need. It produced U with nonzero pivots 2, 5, 6. Normally there are more columns and rows to work on, before we reach U. Row exchange Successful Caution! That row exchange was a success. This is what we hope for, to reach U with no zeros on its main diagonal. (The pivots 2, 5, 6 are on the diagonal.) But a slightly different matrix A* would lead to a bad situation: no pivet is available in column 2. Dependent columns (2 3 4 2 3 4] U™ is not invertible - A*=]|14 6 14|10 0 6 |=U\"* (6 A\" is not invertible 2 3 17 0 0 13 | At this point elimination is helpless in column 2. No second pivot. This misfortune tells us that the matrix A* did not have full rank. Column 2 of U* is in the same direction as column 1 of U*. Column 2 of A* is in the same direction as column 1 of A*. You see how dependent columns are systematically identified by elimination. They can’t escape a zero in the pivot. Then there will be nonzero solutions X to A*X = 0. The columns of U* (and A*) are not independent. This example has column 2 = % column 1. The solution vector X is (3, -—2,0). The equation A*x = b may or may not be solvable, depending on b: probably not. Dependent or Independent Columns This A* looks like a failure of elimination: No second pivot. But it was a success because the problem was identified: dependent columns. The beauty of aiming for a triangular matrix U or U* is that the diagonal entries tell us everything. A triangular matrix U has full rank exactly when its main diagonal has no zeros. In that case (square matrix with nonzero pivots) the columns of U are independent. Also the rows are independent. We can see this directly because elimination has simplified the original A to the triangular U. How do we know that a zero on the diagonal of U* leads to dependent columns ? 44 Chapter 2. Solving Linear Equations Az = b \"« * % x| Uppertriangular with an extra zero on its diagonal U* = 0 = * =+ | This matrix is singular (not full rank 4) (no inverse) 10 0 O x| Thefirst three columns are dependent 0 0 0 = The last two rows are dependent The Row Picture and the Column Picture The next pictures will show the three possibilities for Ax = b: No solution or a line of solutions or one solution. There are two ways to see this. We start with the rows of A and we graph the two equations : the row picture. We have trouble if the lines don’t meet. y4r T Figure 2.1: Parallel lines mean no solution. Intersecting lines give one solution. One line twice means a line of solutions. The solution is where the lines meet. If we had three equations for z, y, and z, those two lines would change to three planes. The three planes meet at a single point in 3-dimensional space. This row picture becomes hard to draw. The column picture is much easier in three or more dimensions. The column picture just shows column vectors: columns of A and also the vector b. We are not looking for points where these vectors meet. The goal of Ax = b is to combine the columns of A so as to produce the vector b. This is always possible when the columns of A (n vectors in n-dimensional space) are independent. Then the column space of A contains all vectors b in R\". There is exactly one combination Az of the columns that equals b. Elimination finds that solution x. b Al [l —2] The columns of A are independent | Column1 4+ Column2 =0 1 Then the solutionisz; =1,z =1 1= [1] Construct b from the columns ! i The bottom point is (0, 0) col2=[ Figure 2.2: Column picture. The vector b is a combination Ax of the columns of A. 2.1. Elimination and Back Substitution 45 Examples of Elimination and Permutation This chapter will go on to express the whole process using matrices. An elimination matrix E will act on Az = b. In case zero appears in a pivot position, use a permutation matrix P. The final result i1s an upper triangular U and a new right hand side ¢. Then Uz = c is solved by back substitution. In reality a computer takes those steps (x = A\\b in MATLAB). But it is good to solve a few examples—not too many—by hand. You see the steps to Uz = c¢ and then to the solution x. This page contains a variety of examples, hopefully to show the way. p- - 2 4 =2 2 4 -2 2 4 =2 (2 4 =2 E5q E3 E39 A= 4 9 -3 |— 0 1 1|—|0 1 1|—|10 1 1|=U -2 -3 T -2 -3 7 01 5] 0 0 4] Those elimination steps E5; and F3; and E'32 produced zeros in positions (2.1) and (3, 1) and (3,2). The matrices E have —2 and +1 and —1 in those positions. The same steps E5,, E3;, F32 must be applied to the right hand side b, to keep the equations correct. [ 2] [ 2] 2 (2 b= 8 —)E21b= 4 —)E31E21b= 4 —)E32E31E21b=Eb=C= 4 10 | | 10 | 12 | 8 | There is a simple way to make sure that operations on the matrix A (left side of equations) are also executed on b (right side of equations). The good way is to include b as an extra column with A. The combination [ A b ] is called an augmented matrix. p= - p- - 2 4 -2 2] p[2 4 -2 2 (A b]=| 4 9 -3 8|—=|01 14(|=[U<c]. D -2 -3 7 10 | 00 4 8| Now we include an example that requires a permutation matrix P. It will exchange equations and avoid zero in the pivot. The new matrix A needs P to improve column 2. 1 1 1° 1 1 1] 1 1 1] f;‘;:;\"f:“ a=l223[Eloo0o1|[E]04s5l|=U 0 4 5_ 0 4 5 0 0 1 That permutation P,3 exchanged rows 2 and 3 when it was needed to avoid a zero pivot. But we could have exchanged rows 2 and 3 at the start. Then E3; and E3; change places. In the final description PA = LU of elimination on A, all the E’s will be moved to the right side. Each matrix in F3; F3; F3; is inverted. Those inverses come in reverse order L = E;'E;'E5,'. The overall equation is PA = LU. Often no permutations are needed and elimination produces A = LU : the best equation of all, in Section 2.2. Section 2.4 will return to understand all the possible permutations of n rows. There are n! permutation matrices P, including P = I for no row exchanges. 46 Chapter 2. Solving Linear Equations Az = b Problem Set 2.1 Problems 1-10 are about elimination on 2 by 2 systems. 1 What multiple {2, of equation 1 should be subtracted from equation 2 ? 2r+3y=1 10z + 9y = 11. After elimination, write down the upper triangular system and circle the two pivots. Use back substitution to find £ and y (and check that solution). 2 If equation 1 is added to equation 2, which of these are changed: the lines in the row picture, the vectors in the column picture, the coefficient matrix, the solution? 3 What multiple of equation 1 should be subtracted from equation 2 ? 21 -4y =6 -+ 5y =0. After this elimination step, solve the tniangular system. If the right side changes to (—6,0), what is the new solution? 4 What multiple £ of equation 1 should be subtracted from equation 2 to remove c? az+by=f cxz+dy =g. The first pivot is a (assumed nonzero). Elimination produces what formula for the second pivot ? What is y ? The second pivot is missing when ad = bc: singular. 5 Choose a right side which gives no solution and another right side which gives infinitely many solutions. What are two of those solutions ? Singular system 3z+2y=10 and 6zr+4y= ___ 6 Choose a coefficient b that makes this system singular. Then choose a right side g that makes it solvable. Find two solutions in that singular but solvable case. 2z + by = 16 4z + 8y = g. 7 For which numbers a does elimination break down (1) permanently (2) temporarily ? Solve for x and y after fixing the temporary breakdown by a row exchange. ar + 3y = -3 4z + 6y = 6. 8 For which three numbers k does elimination break down ? Which is fixed by a row exchange ? Is the number of solutions 0 or 1 or co ? Draw the 3 row pictures. 3 pictures from kx+3y= 6 3 particular k's 3z + ky = 6. 2.1. Elimination and Back Substitution 47 9 What test on b; and by decides whether these two equations allow a solution? How many solutions will they have? Draw the column pictures for b = (1, 2) and (1, 0). 3r —-2y=b, 6r — 4y = b2. 10 Drawthelines z+y = 5 and £+ 2y = 6 and the equation y = that comes from elimination. Which line 5z — 4y = c goes through the solution of these equations ? Problems 11-20 study elimination on 3 by 3 systems (and possible failure). 11 (Recommended) A system of linear equations can’t have exactly two solutions. (a) If (z,y, z) and (X, Y, Z) are two solutions, what is another solution? (b) If 25 planes meet at two points, where else do they meet? 12 Reduce to upper triangular form by row operations. Then find z, y. z. 2r +3y+ 2= 8 2x — 3y =3 4z + Ty +52=20 4z —dy+ =2=7 -2y+2z= 0 2r— y—3z=395 13 Which number d forces a row exchange, and what is the triangular system (not sin- gular) for that d? Which d makes this system singular (no third pivot) ? 2r+35y+2=0 4r+dy+z =2 y—2z=3. 14 Which number b leads later to a row exchange? Which b leads to a missing pivot ? In that singular case find a nonzero solution z, y, z. T+ by =0 T-2y—2=0 y+2z=0. 15 (a) Construct a 3 by 3 system that needs 2 row exchanges to become triangular. (b) Construct a system that needs a row exchange and breaks down later. 16 If rows 1 and 2 are the same, how far can you get with elimination (allowing row exchange)? If columns 1 and 2 are the same, which pivot is missing? Equal 2zr-y+2=0 2r +2y+2=0 Equal rows 2z-y+z=0 4r +4y+2=0 columns 4z+y+2=2 6z + 6y + 2 = 2. 17 Construct a 3 by 3 example that has 9 different coefficients on the left side, but rows 2 and 3 become zero in elimination. How many solutions to your system with b = (1,10, 100) and how many with b = (0, 0,0)? 48 18 19 20 21 22 23 24 25 26 Chapter 2. Solving Linear Equations Az = b Which number g makes this system singular and which right side ¢ gives it infinitely many solutions? Find the solution that has z = 1. r+4y-2z=1 r+Ty—62=06 Jy+qz =t. For which two numbers a will elimination failon A = [ Z Z ] ? For which three numbers a will elimination fail to give three pivots? 2 3 A= |a a 4| issingular for three values of a. a a Look for a matrix that has row sums 4 and 8, and column sums 2 and s: The four equations below are solvable only if s = ____ . Find two matrices with the correct row and column sums. Write down the 4 by 4 system Ax = b with x = (a,b, c,d) and make A triangular by elimination. Matrix | © b a+b=4 a+c=2 4 equations c d c+d=8 b+d=s 4 unknowns Create a MATLAB command A(2, : ) = ... for the new row 2, to subtract 3 times row 1 from the existing row 2 if the 3 by 3 matrix A is already known. Find experimentally the average st and 2nd and 3rd pivot sizes from MATLAB’s [L,U] = lu(rand(3)) with random entries between 0 and 1. The average of U(1,1) is above % because lu picks the largest pivot in column 1. If the last corner entry is A(5,5) = 11 and the last pivot of A is U(5,5) = 4, what different entry A(5, 5) would have made A singular ? Suppose elimination takes A to U without row exchanges. Then row j of U is a combination of whichrowsof A? If Az = 0,isUz =0?If Az = b,isUx = b? If A starts out lower triangular, what is the upper triangular U? Start with 100 equations Az = 0 for 100 unknowns * = (x1,...,Z100). Suppose elimination reduces the 100th equation to 0 = 0, so the system is “singular”. (a) Singular systems Az = 0 have infinitely many solutions. This means that some linear combination of the 100 columns of A is (b) Invent a 100 by 100 singular matrix with no zero entries. (c) Describe in words the row picture and column picture of your Az = 0. 49 2.2. Elimination Matrices and Inverse Matrices 2.2 Elimination Matrices and Inverse Matrices 1 O Elimination multiplies A by FE5;,...,E,; then E3z,...,E,2 as A becomes FA = ﬂ In reverse order, the inverses of the E’s multiply U to recover A=E~'U. Thisis A=LU. A 'A=Tand (LU) ' =U\"'L~!. Then Az = bbecomesz = A~'b = U‘lL‘lly All the steps of elimination can be done with matrices. Those steps can also be undone (inverted) with matrices. For a 3 by 3 matrix we can write out each step in detail—almost word for word. But for real applications, matrices are a much better way. The basic elimination step subtracts a multiple £;; of equation ;3 from equation <. We always speak about subtractions as elimination proceeds. If the first pivotis a;; = 3 and below it is ag; = —3, we could just add equation 1 to equation 2. That produces zero. But we stay with subtraction: subtract {2 = —1 times equation 1 from equation 2. Same result. The inverse step is addition. Equation (10) to (11) at the end shows it all. Here is the matrix that subtracts 2 times row 1 from row 3: Rows 1 and 2 stay the same. Elimination matrix E;; = E3; 1 00 . 1o E3) = 01 0 Row 3, column 1, multiplier 2 _2 0 1 If no row exchanges are needed, then three elimination matrices E5, and Ej; and E'35 will produce three zeros below the diagonal. This changes A to the tnangular U : (D) F = FE3,F3,Fq, FE A = U is upper triangular The number ¢33 is affected by the ¢3; and {3, that came first. We subtract {3, times row 2 of U (the final second row, not the original second row of A). This 1s the E3; step that produces zero in row 3, column 2 of U. E3, gives the last step of 3 by 3 elimination. E, and then E3; subtract multiples of row 1 from rows 2 and 3 of A: Example 1 (100‘(100'(310‘ (310’ two new E31E0nA = O 1 0||1 1 0f|-311]=]0 2 1 zeros in (2) -2 0 1J|0 0 1]| 6 8 4] |0 6 4] columnl To produce a zero in column 2, E32 subtracts £32 = 3 times the new row 2 from row 3: 1 0 0][3 1 0] [3 1 O] U has zeros (E32)(E31E21A)= 0 1 0 0 2 1|=|0 2 1|=U below the (3) 0 -3 1|0 6 4] [0 O 1 main diagonal Notice again: E3; is subtracting 3 times the row 0, 2,1 and not the original row of A. At the end, the pivots 3,2, 1 are on the main diagonal of U : zeros below that diagonal. The inverse of each matrix E;; adds back £;;(row j) to row i. This leads to the inverse of their product E = E3;FE31 E2;. That inverse of E is special. We call it L. 50 Chapter 2. Solving Linear Equations Az = b The Facts About Inverse Matrices Suppose A is a square matrix. We look for an “inverse matrix” A~! of the same size, so that A~ times A equals I. Whatever A does, A~! undoes. Their product is the identity matrix—which does nothing to a vector, so A=Az = . But A~! might not exist. The n by n matrix A needs n independent columns to be invertible. Then A7!A = I. What a matrix mostly does is to multiply a vector. Multiplying Az = b by A~} gives A=1Az = A~'b. Thisis ¢ = A~'b. The product A~'A is like multiplying by a number and then dividing by that number. Numbers have inverses if they are not zero. Matrices are more complicated and interesting. The matrix A~! is called “A inverse”. DEFINITION The matrix A is invertible if there exists a matrix A~! that “inverts” A : Two-sidedinverse @A~!A=1 and AA~' =1 4) Not all matrices have inverses. This is the first question we ask about a square matrix: Is A invertible? Its columns must be independent. We don’t mean that we actually calculate A~1. In most problems we never compute it ! Here are seven “notes” about A~!. Note 1 The inverse exists if and only if elimination produces n pivots (row exchanges are allowed). Elimination solves Az = b without explicitly using the matrix A~!, Note 2 The matrix A cannot have two different inverses. Suppose BA = I and also AC = 1. Then B = C, according to this “proof by parentheses” = associative law. B(AC) = (BA)C gives BI=IC or B=C. (S) This shows that a left inverse B (multiplying A from the left) and a right inverse C (multiplying A from the right to give AC' = I) must be the same matrix. Note 3 If A is invertible, the one and only solutionto Az = bisz = A~ ' b: Multiply Az =b by A~). Then z=A\"'Ac=A\"1b. Note 4 (Important) Suppose there is a nonzero vector x such that Ax = 0. Then A has dependent columns. It cannot have an inverse. No matrix can bring 0 back to . If A is invertible, then Az = 0 only has the zero solutionz = A~10 = 0. Note 5 A square matrix is invertible if and only if its columns are independent. Note 6 A 2 by 2 matrix is invertible if and only if the number ad — bc is not zero : -1 a b 1 d -b 2 by 2 Inverse [c d] = [—c a]' (6) This number ad - bc is the determinant of A. A matrix is invertible if its determinant is not zero (Chapter 5). The test for n pivots is usually decided before the determinant appears. 2.2. Elimination Matrices and Inverse Matrices 51 Note 7 A triangular matrix has an inverse provided no diagonal entries d; are zero: (dy X X X | \" 1/d; x X X 1 0 e x X 1 0 o X X If A= g ¢ ¢ x | Wen A7 =1 4 o o «x 0 0 0 dn 0 0 0 1/dn Example 2 The 2 by 2 matrix A = [} 3] 1s not invertible. It fails the test in Note 6, because ad = be. It also fails the test in Note 4, because Ax = 0 when x = (2, -1). It fails to have two pivots as required by Note 1. Its columns are clearly dependent. Elimination turns the second row of this matrix A into a zero row. No pivot. Example 3 Three of these matrices are invertible, and three are singular. Find the inverse when it exists. Give reasons for noninvertibility (zero determinant, too few pivots, nonzero solution to Ax = 0) for the other three. The matrices are inthe order A. B.C.D.S. T : e e e e O8] [ 8 6 8 7 6 0 6 6 111 111 Solution The three matrices with inverses are B,C. S : 1 0 0} 1 7 -3 1 [0 6 -1 _ - -1 _ -1 _ | _ B _4[—8 4] © 36[6 —6] S . ~ (1) A is not invertible because its determinantis 4 -6 — 3-8 = 24 — 24 = 0. D 1s not invertible because it has only one pivot; row 2 becomes zero when row 1 is subtracted. T has two equal rows (and the second column minus the first column is zero). In other words Tx = 0 has the nonzero solution * = (—1.1,0). Not invertible. The Inverse of a Product AB For two nonzero numbers a and b, the sum a + b might or might not be invertible. The numbers a = 3 and b = —3 have inverses % and —%. Their sum a + b = 0 has no inverse. But the product ab = —9 does have an inverse, which is % times —%. For matrices A and B, the situation is similar. Their product AB has an inverse if and only if A and B are separately invertible (and the same size). The important point is that A1 and B! come in reverse order: If A and B are invertible (same size) then the inverse of AB is B~1 A1, (AB)~! = B-1A-! (AB)(B-'A-)=AIA'=AA'=1 () 52 Chapter 2. Solving Linear Equations Az = b We moved parentheses to multiply BB~ first. Similarly B~! A~! times AB equals I. B~'A~! illustrates a basic rule of mathematics: Inverses come in reverse order. It is also common sense: If you put on socks and then shoes, the first to be taken off are the . The same reverse order applies to three or more matrices : Reverse order (ABC)\"' =C-'B~14-1 (8) Example 4 Inverse of an elimination matrix. If E subtracts 5 times row 1 from row 2, then E~! adds 5 times row 1 to row 2: 1 0 0 ‘1 0 O] %s_“}’:\":;;s E=|-5 1 0| ad E''= |5 1 0 0 0 1 0 0 1. Multiply EE~! to get the identity matrix I. Also multiply E~1E to get I. We are adding and subtracting the same 5 times row 1. If AC = I then for square matrices CA = I. For square matrices, an inverse on one side is automatically an inverse on the other side. Example 5 Suppose F subtracts 4 times row 2 from row 3, and F~! adds it back : = - - - 1 0 1 F=1|0 1 0| ad F = 0 -4 1 0 Now multiply F by the matrix E in Example 4 to find FE. Also multiply E~! times F~! to find (FE)~!. Notice the orders FE and E-1 F~!! ot - 1 0 0 ‘1 0 0 FE=]|-5 1 0| isinvertedby E\"'F!=1|5 1 0]. 9) 120 -4 1 04 1 The result is beautiful and correct. The product F E contains “20” but its inverse doesn’t. E subtracts 5 times row 1 from row 2. Then F subtracts 4 times the new row 2 (changed by row 1) from row 3. In this order FE, row 3 feels an effect of size 20 from row 1. In the order E-1F~!, that effect does not happen. First F~1 adds 4 times row 2 to row 3. After that, E~! adds 5 times row 1 to row 2. There is no 20, because row 3 doesn'’t change again. In this order E~'F~1, row 3 feels no effect from row 1. This is why we choose A = LU, to go back from the triangular U to the original A. The multipliers fall into place perfectly in the lower triangular L : Equation (11) below. The elimination order is FE. The inverse orderis L = E~1F~1, The multipliers 5 and 4 fall into place below the diagonal of 1’s in L. 53 2.2. Elimination Matrices and Inverse Matrices L is the Inverse of E E is the product of all the elimination matrices E;;, taking A into its upper triangular form EA = U. We are assuming for now that no row exchanges are involved (P = I). The difficulty with E'is that multiplying all the separate elimination steps E;; does not produce a good formula. But the inverse matrix E~! becomes beautiful when we multiply the inverse steps E 1. Remember that those steps come in the opposite order. With n = 3, the complication for E = E32 E3, E>; is in the bottom left corner: -y 1 1 1 1M 1 1 T 1 E=|0 1 0 1 -l 1 = —€2, 1 . (10) 0 —¥l32 1]|-¥4; 0 1fL O O 1 | (€32€21 — €31) —l32 1] Watch how that confusion disappears for E—! = L. Reverse order is the good way : (1 11 1 171 1 [1 ] E-1=|¢; 1 0 0 1 €2, 1 =L (11) 0 0 1]|€;m 0 1]]10 €32 1] |4a l32 1) All the multipliers £;; appear in their correct positions in L. The next section will show that this remains true for all matrix sizes. Then EA = U becomes A = LU. Equation (11) is the key to this chapter : Each £;; is in its place for E 1= Problem Set 2.2 0 (more questions than needed) If you exchange columns 1 and 2 of an invertible matrix A, what is the effect on A\"1? Problems 1-11 are about elimination matrices. 1 Write down the 3 by 3 matrices that produce these elimination steps : (a) E,; subtracts 5 times row 1 from row 2. (b) E35 subtracts —7 times row 2 from row 3. (c) P exchanges rows 1 and 2, then rows 2 and 3. 2 In Problem 1, applying E5; and then E3; to b = (1,0,0) gives E32E2,b = : Applying E3, before E3; gives Eo1 E32b = . When E3, comes first, row feels no effect from row 3 Which three matrices E5,, E3;, E32 put A into triangular form U ? 1 1 0] A= 4 6 1 and E32E31E21A =FA=U. -2 2 0 Multiply those E’s to get one elimination matrix E. Whatis E-! = L? 54 10 11 12 13 14 15 Chapter 2. Solving Linear Equations Az = b Include b = (1,0, 0) as a fourth column in Problem 3 to produce [A b]. Carry out the elimination steps on this augmented matrix to solve Az = b. Suppose a3z = 7 and the third pivot is 5. If you change a33 to 11, the third pivot is . If you change a33 to , there is no third pivot. If every column of A is a multiple of (1,1,1), then Az is always a multiple of (1,1,1). Do a 3 by 3 example. How many pivots are produced by elimination? Suppose E subtracts 7 times row 1 from row 3. (a) To invert that step you should 7 times row to row (b) What “inverse matrix” E~! takes that reverse step (so E~'E = I)? (c) If the reverse step is applied first (and then E) show that EE~! = I. The determinant of M = [: 3] is det M = ad — bc. Subtract ¢ times row 1 from row 2 to produce a new M*. Show that det M/* = det M for every £. When € = c/a, the product of pivots equals the determinant: (a)(d — ¢b) equals ad — bc. (a) E3; subtracts row 1 from row 2 and then P,3 exchanges rows 2 and 3. What matrix M = P53 E»; does both steps at once? (b) P33 exchanges rows 2 and 3 and then E3; subtracts row 1 from row 3. What matrix M = E3, P»3 does both steps at once? Explain why the A’s are the same but the E’s are different. (a) What matnx adds row 1 to row 3 and at the same time row 3 to row 1 ? (b) What matrix adds row 1 to row 3 and then adds row 3 torow 1? Create a matrix that has a;; = a2 = a3z = 1 but elimination produces two negative pivots without row exchanges. (The first pivot is 1.) For these “permutation matrices” find P~? by trial and error (with 1’s and 0’s) '0011 0 1 0] P=1]0 10| and P=1|0 0 1 100 10 0, Solve for the first column (z, y) and second column (¢, z) of A~1. Check AA~1. <o 3] 8 BEI = B -0 Find an upper triangular U (not diagonal) with U2 = I. ThenU~! = U, (a) If A is invertible and AB = AC, prove quickly that B = C. (b) If A = [11], find two different matrices such that AB = AC. 2.2. Elimination Matrices and Inverse Matrices 55 16 17 18 20 21 22 23 24 25 26 27 28 29 30 (Important) If A has row 1 + row 2 = row 3, show that A is not invertible : (a) Explain why Ax = (0,0, 1) cannot have a solution. Add eqn 1 + eqn 2. (b) Which right sides (b;, b2, b3) might allow a solutionto Ax = b? (c) In the elimination process, what happens to equation 3 ? If A has column 1 4+ column 2 = column 3, show that A is not invertible: (a) Find a nonzero solution & to Ax = 0. The matnx is 3 by 3. (b) Elimination keeps columns 1 + 2 = 3. Explain why there is no third pivot. Suppose A is invertible and you exchange its first two rows to reach B. Is the new matrix B invertible? How would you find B~! from A~1? (a) Find invertible matrices A and B such that A + B is not invertible. (b) Find singular matrices A and B such that A + B is invertible. If the product C = AB is invertible (A and B are square), then A itself is invertible. Find a formula for A~! that involves C~! and B. If the product Al = ABC of three square matrices is invertible, then B is invertible. (So are A and C.) Find a formula for B~! that involves Af/~! and A and C. If you add row 1 of A to row 2 to get B, how do you find B~! from 4-!? Prove that a matrix with a column of zeros cannot have an inverse. Multiply [g 3] times [_g -2]. What is the inverse of each matrix if ad # bc? (a) What 3 by 3 matrix E has the same effect as these three steps? Subtract row 1 from row 2, subtract row 1 from row 3, then subtract row 2 from row 3. (b) What single matrix L has the same effect as these three reverse steps? Add row 2 to row 3, add row 1 to row 3, then add row 1 to row 2. If B is the inverse of A2, show that AB is the inverse of A. Show that A = 4 x eye (4) — ones (4, 4) is not invertible : Multiply A x ones (4, 1). There are sixteen 2 by 2 matrices whose entries are 1’s and 0’s. How many of them are invertible ? Change I into A~! as elimination reduces A to I (the Gauss-Jordan idea). (An)=l3 7 0q) = (4nl=3 501 Could a 4 by 4 matrix A be invertible if every row contains the numbers 0,1, 2,3 in some order? What if every row of B contains 0, 1, 2, —3 in some order ? 56 31 32 33 35 36 37 39 Chapter 2. Solving Linear Equations Az = b Find A~! and B~ (if they exist) by eliminationon (A I]and [B I|: 92 1 1 C 2 -1 -1] A=11 2 1 and B=1|-1 2 -1 11 2 -1 -1 2 Gauss-Jordan elimination acts on [U I'] to find the matrix [I U~']: - - - O - N 1 fU=10 0 b c then U= 1.4 \" - True or false (with a counterexample if false and a reason if true): A is square. (a) A 4 by 4 matrix with a row of zeros is not invertible. (b) Every matrix with 1’s down the main diagonal is invertible. (c) If A is invertible then A~! and A? are invertible. (Recommended) Prove that A is invertible if a # 0 and a # b (find the pivots or A~1). Then find three numbers ¢ so that C is not invertible: a b b 2 ¢ c] A=]a a } C=|c ¢ c a a af _8 7 C This matrix has a remarkable inverse. Find A~! by elimination on [A I]. Extend to a 5 by 5 “alternating matrix” and guess its inverse; then multiply to confirm. - 1 p— 1 Invert A = 1 -1 1 1 1 1 1 -1 and solve Ax = 1 0 1 1 _J -l Suppose the matrices P and Q) have the same rows as I but in any order. They are “permutation matrices”. Show that P — Q is singular by solving (P — Q)= = 0. Find and check the inverses (assuming they exist) of these block matrices : I 0 A 0 0 I C I C D I D]’ How does elimination from A to U on a 3 by 3 matrix tell you if A is invertible ? IfA=1T1-uvTthen A~ =T+ uvT(1 — vTu)~1. Show that AA~! = I except Au =0whenvTu=1 2.3. Matrix Computations and A = LU 57 2.3 Matrix Computationsand A = LU /l The elimination steps from A to U cost %n3 multiplications and subtractions. N 2 Each right side b costs only n? : forward to Uz = c, then back-substitution for x. \\3 Elimination without row exchanges factors A into LU (two proofs of A = LU )/ How would you compute the inverse of an n by n matrix A ? Before answering that ques- tion I have to ask : Do you really want to know A~! ? It is true that the solutionto Az = b (which we do want) is given by £ = A~1b. Computing A~! and multiplying A~ 'b is a very slow way to find &. We should understand A~! even if we don’t use it. Here is a simple idea for A~!. That matrix is the solution to AA~! = I. The identity matrix has n columns e, es,...,e,. Then AA~! = I is really n equations Az, = e, to Az, = e, for the n columns = of A~!. Three equations if the matrices are 3 by 3: 0 0] 0]. () 1- 1 AA- =T A I -Tp|l=]|€1 \" €xn Alxzy = z3|=]0 0 i 11 i ! ] We are solving n equations and they have the same coefficient matrix A. So we can solve them together. This is called “Gauss-Jordan elimination”. Instead of a matrix [ A b ] augmented by one right hand side b, we have a matrix [ A T ] augmented by n nght hand sides (the columns of I). Then as A changes to I, elimination produces [ I A1 ] 1 0 p - - 1 0 0|1 0 0] 1 0 0|1 0 O [A I] =| -1 1 0/01 0f—=|[0 10110 0 -1 1|0 0 1 0 -1 1|0 0 1 | Gauss-Jordan elimination 1 0 0|1 0 0] Solve AX =T= X =A\"\" 10 101 1 0(=[I A!] Slower than solving Az = b | 0 0 1|1 1 1 | The key point is that the elimination steps on A only have to be done once. The same steps are applied to the right hand side—but now AA~! = I has n right hand sides. The n solutions x; to AT; = e; go into the n columns of A~!. Then Gauss-Jordan takes [ A I ] into [ I A} ] Here elimination is equivalent to multiplication by A~!. In this example A subtracts rows and A~! adds. AA~! = I is linear algebra’s version of the Fundamental Theorem of Calculus: Derivative of integral of f(r) equals f(r). The Cost of Elimination A very practical question is cost—or computing time. We can solve 1000 equations on a PC. What if n = 100,000? (Is A dense or sparse?) Large systems come up all the time in scientific computing, where a three-dimensional problem can easily lead to a million unknowns. We can let the calculation run overnight, but we can’t leave it for 100 years. 58 Chapter 2. Solving Linear Equations Ax = b Reducing A to U The first stage of elimination produces zeros below the first pivot in column 1. To find each of the new entries below row 1 requires one multiplication and one subtraction. We will count this first stage as n® multiplications and n? subtractions. The count is actually n? — n, because row 1 does not change. The next stage clears out the second column below the second pivot. The working matrix is now of size n— 1. Estimate this stage by (n—1)? multiplications and subtractions. The matrices are getting smaller as elimination goes forward. The rough count to reach U is the sum of squares n? + (n — 1)2 + ... + 22 4+ 12, There 1s an exact formula %n (n+ %) (n + 1) for this sum of squares. When n is large, the % and the 1 are not important. The number that matters is % n3. The sum of squares is like the integral of z° ! The integral from O to n is £ n3: Reducing A to U requires about %na multiplications and % n3 subtractions. What about the right side b? Going forward, we subtract multiples of b; from the lower components by, . .., by,. This is n — 1 steps. The second stage takes only nn — 2 steps, because b, is not involved. The last stage of forward elimination (b to c) takes one step. Now start back substitution. Computing z,, uses one step (divide by the last pivot). The next unknown uses two steps. When we reach z; it will require n steps (n — 1 substitutions of the other unknowns, then division by the first pivot). The total count on the right side, from b to c to £ — forward to the bottom and back to the top — is exactly n?: (n-1)+(n-2)+---41] + [1+2+ -+ (n—-1)+n] =n (2) To see that sum, pair off (n — 1) with 1 and (n — 2) with 2. The pairings leave n terms, each equal to n. That makes n?. Going from b to ¢ to x costs a lot less than the left side ! Back substitution Each right side needs n? multiplications and n? subtractions. How long does it take to solve Ax = b? For a random matrix of order n = 1000, a typical time on a PC is 1 second. The time is multiplied by about 8 when n is multiplied by 2. For professional codes go to netlib.org. According to this n3 rule, matrices that are 10 times as large (order 10,000) will take a thousand seconds. Matrices of order 100,000 will take a million seconds. This is too expensive without a supercomputer, but remember that these matrices are full. Most large matrices in practice are sparse (many zero entries). In that case A = LU is much faster. Please see “Computing Time” on the website math.mit.edu/linearalgebra. Proving A = LU Elimination is expressed by EA=U and inverted by LU = A. Equation (11) in Section 2.2 showed how the multipliers ¢;; fall exactly into the right positions in E~! which is L. Why should we want to find a proof for every size n? The reason is: A proof means that we have not just seen that pattern and believed it and liked it, but understood it. 2.3. Matrix Computations and A = LU 59 The Great Factorization A = LU Let me review the forward steps of elimination. They start with a matrix A and they end with an upper tnangular matrix U. Every elimination step E;; produces a lower triangu- lar zero. Those steps E;; subtract £;; times equation j from equation ¢ below it. Row exchanges (permutations) are coming soon but not yet. To invert one elimination step E;;, we add instead of subtracting : 1 ] [ 1 ] E3 = 0 1 and L3; = inverse of E3, = 0 1 |l 0 1] 6y 0 1 Equation (10) in Section 2.2 multiplied E33 E3; E5; with a messy result : - - 1 1 1 1 E=|0 1 0 1 —fly 1 -l 1 LO —832 1_4 _—831 0 1- I 0 0 lj _([32(21 — 331) —(732 1_4 Equation (11) showed how inverses in reverse order E,,' E;,' E5,' produced perfection: 1 1 1 1 E-1= ¢, 1 0 1 0 1 = | €2 1 =L 0 0 1j[€ O 1][0 €3 1] [€31 €32 1] Then elimination EA = U becomes A = E~'U = LU if we run it backward from U to A. These pages aim to show the same result for any matrix size n. The formula A = LU 1s one of the great matrix factorizations in linear algebra. Here is one way to understand why L has all the ;; in position, with no mix-up. The key reason why A equals LU: Ask yourself about the pivot rows that are subtracted from lower rows. Are they the original rows of A? No, elimination probably changed them. Are they rows of U? Yes, the pivot rows never change again. When computing the third row of U, we subtract multiples of earlier rows of U (not rows of the original A): Row 3 of U = (Row 3 of A) — €3,(Row 1 of U) — €32(Row 2 of U). (3) Rewrite this equation to see that the row [€3; £32 1] is multiplying the matrix U Row 3 of A = {3,(Row 1 of U) + €32(Row 2 of U) + 1(Row 3 of U). 4) This is exactly row 3 of A = LU. That row of L holds £3,, €32, 1. All rows look like this, whatever the size of A. With no row exchanges, we have A = LU. 60 Chapter 2. Solving Linear Equations Az = b Second Proof of A = LU : Multiply Columns Times Rows I would like to present another proof of A = LU. The idea is to see elimination as removing one column of L times one row of U from A. The problem becomes one size smaller. Elimination begins with pivot row = row 1 of A. We multiply that pivot row by the numbers £;; and {3; and eventually £,,. Then we subtract from row 2 and row 3 and eventually row n of A. By choosing {9, = as;/a1; and €3, = a3;/a;; and eventually £n1 = an1/ay, the subtraction leaves zeros in column 1. 1 (row 1) 0 0 0 O Step 1 removes by (row 1) from A to leave A3 = 0 x X X £3)(row 1) 0 X X X | g (row 1) 0 X X X | Key point: We removed a rank one matrix: column times row. It was the column £, = (1.€9).£31,€4;) times row 1 of A—the first pivot row u,;. Now we face a similar problem for A;. And we take a similar step to reach Ags: (row 2 of Ag) I (l’OW 2 of A2) (row 2 of A;) | {42(row 2 of Aj) ] 0 Step 2 removes [l from A to leave A3 = 32 o O OO o R e R e I e 0 0] 0 O X X |° 00 x x_ Row 2 of A, was the second pivot row = second row us of U. We removed a column €, = (0,1, €3, ¢42) umes that second pivot row. Continuing in the same way, every step removes a column £; of L times a pivot row u; of U. Now add those pieces back together: o T = 'u,l m A=lbiuy+bur+---+lu, = & - L, = LU () columns times rows u, b - b - That last crucial step was column-row multiplication of L times U. This was introduced at the very end of Chapter 1. The Problem Set for this section will review this important way to multiply LU—by adding up rank-one matrices (columns of L times rows of U). Notice that U is upper triangular. The pivot row u; begins with k — 1 zeros. And L is Jower triangular with 1’s on its main diagonal. Column £ also begins with k — 1 zeros. 2.3. Matrix Computations and A = LU 61 Elimination Without Row Exchanges The next section is going to allow row exchanges P;;. They are necessary to move zeros out of the pivot positions. Before we go there, we can answer this basic question: When is A = LU possible with no row exchanges and no zeros in the pivots ? Answer All upper left k by k submatrices of A must be invertible (sizes kK = 1 to n). The reason is that elimination is also factoring every one of those k by k corners of A. Those corner matrices Ay agree with LUy (k by k comers of L and U are invertible). [Ak *]=[L\" 0][Uk :] tells us that Ay = L Ux. (6) x X x % 0 So each corner matrix A must be invertible to reach the final form A = LU. Problem Set 2.3 Problems 1-8 compute the factorization A = LU (and also A = LDU). 1 (Important) Forward elimination changes |1 } |« = bto a triangular [} } |z = e T+ y=3 T+ y=395 1 15 1 1 5] — That step subtracted €2, = times row 1 from row 2. The reverse step adds ¢, times row 1 to row 2. The matrix for that reverse step is L = . Muluply this L times the triangular system [} }]z1 = [§] to get = . In letters, L multiplies Uz = c to give 2 Write down the 2 by 2 triangular systems Lc = b and Uz = ¢ from Problem 1. Check that ¢ = (5, 2) solves the first one. Find x that solves the second one. 3 What matrix E puts A into triangular form EA = U? Multiply by E-! = L to factor A into LU : 2 1 0] A=10 4 : 6 3 9 4 What two elimination matrices E;; and F32 put A into upper triangular form E3;E; A = U? Multiply by EZ,' and E;! to factor Ainto LU = E;'EL'U : 1 1 1] A=1|2 4 5]. 0 4 0 62 10 Chapter 2. Solving Linear Equations Az = b What three elimination matrices E3;, E3;, E3; put A into its upper triangular form E32F31 Fy1 A = U? Muluply by E3'21. E3\"11 and E{ll to factor A into L times U : ﬁ 1 2| and L=E;'E;'E3. 5 A and B are symmetric across the diagonal (because 4 = 4). Find their triple factor- izations L DU and say how U is related to L for these symmetric matrices: s 4 1 4 0 Symmetric A= [4 11] and B=14 12 4 0 4 0 (Recommended) Compute L and U for the symmetric matrix A: O 0 0 O O O O N O O ON a0 oo ! ] Find four conditions on a, b, c,d to get A = LU with four pivots. This nonsymmetric matrix will have the same L as in Problem 7 : a r r T Find L and U for A=|* b 8 8 a b c t a b c dd Find the four conditions on a, b, c,d, 1, s,t to get A = LU with four pivots. Solve the triangular system Lc = bto find ¢. Then solve Uz = cto find x: _I1 0 12 4 | 2 L-[4 l] and U—[O 1] and b—[ll]. For safety multiply LU and solve Az = b as usual. Circle ¢ when you see it. Solve Lc = b to find ¢. Then solve Uz = ¢ to find z. What was A? O bt 1 0 and U =10 1 0 \" 1 and b= |5 1 2.3. Matrix Computations and A = LU 63 11 12 13 14 15 16 17 (a) When you apply the usual elimination steps to L, what matrix do you reach ? 1 0O O L=1]¢; 1 O l3; €32 1 (b) When you apply the same steps to I, what matrix do you get ? (c) When you apply the same steps to LU, what matrix do you get ? If A= LDU and also A = L;D;U, with all factors invertible, then L = L, and D = D; and U = U,. “The three factors are unique.” Derive the equation LI'ILD = D,U,U~!. Are the two sides triangular or diagonal? Deduce L = L, and U = U, (they all have diagonal 1's). Then D = D,. Tridiagonal matrices have zero entries except on the main diagonal and the two ad- jacent diagonals. Factor these into A = LU and A = LDLT: 1 1 0] a4 a 0 | A=11 2 1 and A= ]|a a+b b : LO 1 2J ..0 b b+C_ If A and B have nonzeros in the positions marked by x, which zeros (marked by 0) stay zero in their factors L and U? I T I X r r 0 A= r x 0 B = r r 0 «x 0 z = «x r 0 = «r _O 0 T | _0 r r I Easy but important. If A has pivots 5, 9, 3 with no row exchanges, what are the pivots for the upper left 2 by 2 submatrix A, (without row 3 and column 3 of A4)? The second proof of A = LU removes which three rank 1 matrices addingto A ? A= = £Ll1u; + €rus + €3u3 = LU! Columns multiply rows Multiply LTL and LLT by columns times rows when the 3 by 3 lower triangular L has six 1’s. 64 Chapter 2. Solving Linear Equations Az = b 2.4 Permutations and Transposes 1 A permutation matrix P has the same rows as [ (in any order). There are n ! different orders. 2 Then P times z puts the components z; to T,, in that new order. And PT equals P~1. 3 Columns of A are rows of AT. The transposes of Az and AB are zTAT and BT AT. 4 The ideabehind AT is that Az -y = z - ATy because (Az)Ty = T ATy = 2T (ATy). {A symmetric matrix has ST = S. The products AT A and AAT are always symmetric. / Permutations Permutation matrices have a 1 in every row and a 1 in every column. All other entries are zero. When this matrix P multiplies a vector, it changes the order of its components: P has the rows of I 0 0 1] [z [ xg | Pz = Circular shift of z Pr=|1 0 0 I | = | T1 1,2,3t03,1,2 01 0] [ x3 | | T2 | For this example, P shifts z; to second position. If we repeat, P? shifts x, to third position. Then P? = I and we recover the order z;, T3, z3. P, P2, and I are permutations. Shuffling a deck of cards would be a permutation of size n = 52. Persi Diaconis proved that a new deck is well shuffled after 7 random permutations. There are 3! = 6 permutation matrices of size 3, and 4! = 24 permutations of size 4. Here are specific tasks for these 4 by 4 permutations, when they multiply a vector z: Pz reverses 0 0 0 1° T4 stays fixed 0 01 0 the order to 0 010 Input z,, x2, 3, T4 1 0 00 I4.23.22,) 0100 output r3,I;, T2, T4 0100 P? =] 1000 P3 =1 0 0 01 Circular shift 0 0 0 1 Evens before odds 1 0 0 0 Input z,, 29, 23,24 1 000 To, x2 before 1,3 0 010 output r4,1;,72,z3 |0 1 0 0 Fast Fourier Transform | 0 1 0 0 Pi=1 001 0 Number from O to 3 0 0 01 Half of the n ! permutations of size n are “even” and half are “odd”. An even permutation needs an even number of simple row exchanges to reach the matrix I. The last example (1 exchange) was odd. The first example (exchange 1 and 4, exchange 2 and 3) was even. The rows of any P are 01 0] (0 01] [1 ] the columnsof P~1=PT PTP=| 0 0 1 || 1 0 O |= 1 =1 PT = “transpose of P” 1 00fL01 0] | 1 2.4. Permutations and Transposes 65 Properties of Permutation Matrices The n 1’s appear in n different rows and n different columns of P. The columns of P are orthogonal : dot products between columns are all zero. The product P, P, of permutations is a permutation. So is the inverse of P. h W b= If A is invertible, there is a permutation P to order its rows in advance, so that elimination on P A meets no zeros in the pivot positions. Then PA = LU. The PA = LU Factorization: Row Exchanges from P An example will show how elimination can often succeed, even when a zero appears in the pivot position. Suppose elimination starts with 1 as the first pivot. Subtracting 2 times row 1 produces 0 as an unacceptable second pivot : 1 2 a‘Ezrl 2 a 1‘Prl 2 a A=12 4 b| >S50 0 b—-2a |l =210 1 ¢c—-3a | =U _3 7 c _O 1 c—3aJ _O 0 b—2a_J In spite of that zero, this A can be invertible. To rescue elimination, P exchanged row 2 with row 3. That brings 1 into the second pivot as shown. So we can continue. A is invertible if and only if b — 2a is not zero in the third pivot. If b = 2a. then row 2 of A equals 2 (row 1 of A). In that case, A is surely not invertible. We can exchange rows 2 and 3 first to get PA. Then LU factorization becomes PA = LU. The matrix P A sails through elimination without seeing that zero pivot. r100‘(1 2 a] [V 2a] [YOO][1 2 a ] 00 1f[2 4 b|[=37¢c|=|l310[/01 c-3a 01037 c¢] [24b)] [201]l00 b-2qa, P A PA L U In principle we might need several row exchanges. Then the overall permutation P includes them all, and still produces PA = LU. Daniel Drucker showed me a neat way to keep track of P, by adding a special 1 2 3 column to the matrix A. That column tracks the original row numbers, as rows are exchanged. Here the final permutation Pis 1 3 2. (1 2 a 1] [1 2 a 1] 1 2 a 1 (1 0 0] 2 4 b 2|-|0 0 b-2a 2(={0 1 c-3a 3|.Pgzis{0 0 1 137 ¢ 3] |01 c-3 3] |00 b—2a 2] 01 0 66 Chapter 2. Solving Linear Equations Az = b “Partial Pivoting” to Reduce Roundoff Errors Every good code for elimination allows for extra row exchanges that add safety. Small pivots are unsafe! The code does not blindly accept a small pivot when a larger number appears below it (in the same column). The computation is more stable if we exchange those rows, to produce the largest possible number in the pivot. This example has first pivot equal to 1, but column 1 offers larger numbers 2 and 3. The code will choose the largest number 3 as the first pivot : exchange rows 1 and 3. The order of rows is tracked by the last column * which is not part of the matnx. 12a1] [37¢38) 3 7 ¢ 3] [3 7 ¢ 8] 2 4 b 2|Bl2 4 b6 2|50 -2 b-2c 2|50 -2 b-2c 2 37 c¢3] [12a1l1f |0-!a-lc1] [0 0 a-3b1 A % U * 0 0 1] 1 0 0° Thisexamplehas P=| 0 1 0 |andL = % 1 0| withPA=LU. {l 00 1 1 o 3 2 o All entries of L are < 1 when we make each pivot larger than all the numbers below it. P AQ Has Row Permutation P and Column Permutation Q Start with a 3 by 3 matrix A. Reorder its rows 1,2, 3 by P in the new order 2, 3, 1,: 01 0] [ a2 G a3 | P = 0 01 PA = asy Q32 Aass (l) 1 00 | a1 a2 a13 | The column indices are still in their original order 1,2,3. Now reorder those columns by Q in the order 3, 2, 1. For column permutations Q follows P A to give PAQ : 0 0 1] 'a23 a2 a2i Q=101 0 PAQ = | a33 a32 aa (2) 10 0 a3 ez an Please notice PAQ. It has row subscripts 2,3, 1 and column subscripts 3,2,1. We had 6 row orders P to choose from and 6 column orders . That makes 36 different matrices PAQ. Question : Are those all the possible permutations of the 9 numbers in A? Absolutely not. Nine numbers have 9! = 362, 880 possible orders. Our matrices P AQ are very special, because the first index 2 or 3 or 1 is constant on each row. And the second index 3 or 2 or 1 is constant down each column. For the column spaces, does C(PAQ) equal C(A)? 2.4. Permutations and Transposes 67 The Transpose of A We need one more matrix, and fortunately it is much simpler than the inverse. It is the “transpose” of A, which is denoted by AT. The columns of AT are the rows of A. When A is an m by n matrix, the transpose is n by m. Then 2 by 3 becomes 3 by 2. 1 2 3 10 Transpose If A= then AT =12 0f. 0 0 4 3 4 . - You can write the rows of A into the columns of AT. Or you can write the columns of A into the rows of AT. The matrix “flips over” its main diagonal. The entry in row %, column j of AT comes from row j, column i of the original A: Rows of A become columns of AT (AT);; = Ajs The transpose of a lower triangular matrix is upper triangular. The transpose of AT is A. Note MATLAB’s symbol for AT is A’. Typing [1 2 3] gives a row vector and the column vector is v = [1 2 3]'. The matrix M with second columnw =[45 6 ]/ is M = [ v w ]. Quicker to enter by rows and transpose: M =1 2 3; 4 5 6]’. The rules for transposes are very direct. We can transpose A + B to get (A + B)T. Or we can transpose A and B separately, and then add AT + BT —with the same result. The serious questions are about the transposes of a product AB and an inverse A~ ' : Sum The transposeof A+ B is AT + BT, (1) Product Thetransposeof AB is (AB)T = BTAT. (2) Inverse The transpose of A~! is (A™1H)T = (A4T)\" . 3) Notice especially how BT AT comes in reverse order. For inverses, this reverse order was quick to check: B~'A~! times AB produces I because A~'!A = B™'B = I. To understand (AB)T = BT AT, start with (Az)T = T AT when B is just a vector: Az combines the columns of A while ¥ AT combines the rows of A\" . It is the same combination of the same vectors! In A they are columns, in AT they are rows. So the transpose of the column Az is the row £TAT. That fits our formula (Az)T = £TAT. Now we can prove (AB)T = BT AT, when B has several columns. If B has two columns x, and x5, apply the same idea to each column. The columns of AB are Ax, and Ax,. Their transposes £ AT appear correctly in the rows of BTAT : I ) \"l AT ] Transposing AB= |Ax, Ax; --| gives :B'{AT whichis BTAT . (6) . 4 R 68 Chapter 2. Solving Linear Equations Az = } The right answer BT AT comes out a row at a time. Here are numbers in (AB)T = BT AT. N I P R | The reverse order rule extends to three or more factors: (ABC)T equals CT BT AT, Now apply this product rule by transposing both sides of A=!A = I. On one side, IT is I. We confirm the rule that (A=\")T is the inverse of AT. Transpose of inverse ~ A~'A =1 is transposed to ATA YT =1. () Similarly AA~! = I leads to (A=!)TAT = I. We can invert the transpose or we can transpose the inverse. Notice especially: AT is invertible exactly when A is invertible. : |11r0]. ., [ 10 ..t |1 6 ’I'hemvc:rseofA-[6 1}1sA —[—6 1].ThetransposelsA = [O 1]. (A™))T and (AT)~! areboth equal to (1) —fli The Meaning of Inner Products The dot product (inner product) of and y is the sum of numbers z;y;. Now we have a better way to write z - y, without using that unprofessional dot. Use matrix notation z'y: T isinside The dot product or inner product is =Ty (Ixn)(nx1)=1x1 T is outside The rank one product or outer productis cyT (n x 1)(1 xn) =n x n zTy is a number, zyT is a matrix. Quantum mechanics would write those as < x|y > (inner) and |z ><y| (outer). Possibly our universe is governed by linear algebra? Here are three more examples where the inner product has meaning: From mechanics Work = (Movements) (Forces) = ™ f From circuits Heat loss = (Voltage drops) (Currents) = eTy From economics Income = (Quantities) (Prices) = q*p We are really close to the heart of applied mathematics, and there is one more point to emphasize. It is the deeper connection between inner products and the transpose of A. We defined AT by flipping the matrix across its main diagonal. That’s not mathematics. There is a better definition. AT is the matrix that makes these inner products equal : (Az)Ty = zT(ATy) Inner product of Az with y = Inner product of = with A™y 2.4. Permutations and Transposes 69 -1 1 0 o Example 1 Start with A = T = |z y= |7 0 -1 1 - Y2 On one side we have Az multiplying y to produce (z2 — T1)y1 + (T3 — T2) y2 That is the same as z; (—y;) + =2 (y1 — ¥2) + =3 (y2). Now z is multiplying ATy. T -1 0] \" -y | (Ax)Ty ATy = 1 -1 [y2]— Y1 — Y2 = 0 1 Y2 T (ATy) Example 2 Will you allow a little calculus ? It is important or I wouldn’t leave linear algebra. (This is linear algebra for functions.) Change the matrix to a derivative: A = d/dt. The transpose of d/dt comes from the rule that (Az)Ty = xT(ATy). First, the dot product £ Ty changes from z,y; + - - - + Tnyn to an integral of x(t)y(t). Inner product T. _ . of functions z and y Ty = /:c(t)y(t)dt by definition (8) Transpose rule for functions T Y — y(t)dt = t)| —— | dt (Az)Ty = 27 (ATy) / ar Y / z(t) ( dt) 9 I hope you recognize “integration by parts”. The derivative A moves from the first function x(t) to the second function y(¢). During that move, a minus sign appears. This tells us that the transpose of A = d/dt is AT = —A = —d/dt. The derivative is anti-symmetric. Symmetric matrices have AT = 4, anti-symmetric matrices have AT = — A. In some way, the 2 by 3 difference matrix in Example 1 followed this pattern. The 3 by 2 matrix AT was minus a difference matrix. It produced y; — y in the middle component of ATy instead of the difference y, — y;. Integration by parts is deceptively important and not just a trick. I left out boundary conditions by assuming that £(—o00) and x(+0c) are zero. Symmetric Matrices For a symmetric matrix, transposing A to AT produces no change. In this case AT equals A. Its (j,7) entry across the main diagonal equals its (z, j) entry. In my opinion, these are the most important matrices of all. We give symmetric matrices the special letter S. A symmetric matrix has ST = S. This means that every sj; = 8;;. . . 11 2] ot |11 0 _ A7 Symmetric matrices S—[2 5]—5 and D_[O IOJ—D' The inverse of a symmetric matrix is a symmetric matrix. The transpose of S~! is (ST = (ST)~! = S~1. When S is invertible, this says that S~! is symmetric . 70 Chapter 2. Solving Linear Equations Az = } -2 1 0 0.1 Next we will produce a symmetric matrix S by multiplying any matrix A by AT, Symmetric inverses S‘lz[ 0 —2] and D! = [1 U ] Symmetric Products AY A and AA™T and LDLT Choose any matrix A, probably rectangular. Multiply AT times A. Then the product S = AT A is automatically a square symmetric matrix : The transposeof ATA is AT(AT)T whichis ATA again. (10) The matrix AAT is also symmetric. (The shapes of A and AT allow multiplication.) But AAT is a different matrix from AT A. In our experience, most scientific problems that start with a rectangular matrix A end up with ATA or AAT or both. As in least squares. -1 0] Example3 Multiply A = [-(l) _i (1)] times AT=| 1 —1] inboth orders. - O 1.1 9 _1 1 -1 0] AAT = and ATA=|-1 2 —1| areboth symmetric matrices. ool 0 -1 1 B ) The product AAT is m by m. In the opposite order, AT A is n by n. Both are symmetric, with positive diagonal (why ?). But even if m = n, it is very likely that AT A # AAT. Symmetric matrices in elimination ST = S makes elimination twice as fast. We can work with half the matrix (plus the diagonal). The symmetry is in the triple product S = LDLT. The diagonal matrix D of pivots can be divided out, to leave U = LT. 1 2] [t o][1 2 L U misses the symmetry of S 0 3 Divide the pivots 1, 3 out of U 2 772 1 1 2] 1 0] 2 7 |21 Example 4 For arectangular A, this saddle-point matrix S is symmetric and important: Block matrix S = I A from least squares T 1AT 0 Apply block elimination to find a block factorization S= LD LT. Then test invertibility: Block elimination I A I A o Subtract AT(row 1) §= [AT 0] goes to [0 _ATA] . Thisis U. The block pivot matrix D contains I and — AT A. Then L and LT contain AT and A: 1 0][1 2] 8 = LDLT captures the symmetry 0 3] (0 1] PivotsinD. Now U is the transpose of L J = ST has size m + n. e I 0] |I 0 I A Block factorization S=LDLT=[AT I] [O -ATA] [O I]' S isinvertible <= ATAisinvertible <= Az # 0 wheneverx #0 2.4. Permutations and Transposes 71 Problem Set 2.4 Questions 1-7 are about the rules for transpose matrices. 1 Find AT and A=! and (A~!)T and (AT)~! for A = [; g] and [(1: S] . 2 Verify that (AB)T equals BT AT but those are different from AT BT . A T ] Show also that AAT is different from AT A. But both of those matrices are 3 (a) The matrix ((AB)~!)T comes from (A~!)T and (B~!)T. In what order? (b) If U is upper triangular then (U~1)7T is triangular. 4 Show that A2 = 0 is possible but AT A = 0 is not possible (unless A = zero matrix). 1 2 3 (0- 5 (a) Compute the number T Ay = [0 1] [4 \" 6] 1| = 0] (b) This is therow xTA = times the column y = (0. 1.0). (c) Thisistherow ! = [0 1] times the column Ay = 6 The transpose of a block matrix M = [AB]is MT = if A. D are square. Under what conditions on A, B, C, D is this block matrix symmetric ? 7 True or false : (a) The block matnix [R ‘3] is automatically symmetnc. (b) If A and B are symmetric then their product AB is symmetric. (c) If A is not symmetric then A~! is not symmetric. (d) When A, B, C are symmetric, the transpose of ABC is C BA. Questions 8-15 are about permutation matrices. 8 Why are there n! permutation matrices of order n? 9 If P, and P, are permutation matrices, so is P, P,. This still has the rows of [ in some order. Give examples with P, P, # P, P, and P3Py = P P;. 10 There are 12 “even” permutations of (1, 2, 3, 4), with an even number of exchanges. Two of them are (1, 2, 3,4) with zero exchanges and (4, 3,2, 1) with 2 exchanges. List the other ten. Instead of writing each 4 by 4 matrix, just order the numbers. 11 If P has I’s on the antidiagonal from (1, n) to (n, 1), describe PAP. Note P = PT, 72 12 13 14 15 Chapter 2. Solving Linear Equations Ax = b Explain why the dot product of and y equals the dot product of Pz and Py. Then (Pz)T(Py) = x'y tells us that PTP = I for any permutation. With z=(1,2,3) and y = (1,4, 2) choose P to show that Px - y is not always x - Py. Which permutation makes P A upper triangular? Which permutations make P, AP, lower triangular? Multiplying A on the right by P, exchanges the of A. 0 A=|1 0 Find a 3 by 3 permutation matrix with P3 = | (butnot P = I). Why can’t P simply exchange two rows ? Find a 4 by 4 permutation P with P* # I. All row exchange matrices are symmetric: PT = P. Then PTP = I becomes P? = I. Other permutation matrices may or may not be symmetric. (a) If P sends row 1 to row 4, then PT sends row to row When PT = P the row exchanges come in pairs with no overlap. (b) Find a 4 by 4 example with PT = P that moves all four rows. Questions 16-18 are about symmetric matrices and their factorizations. 16 17 18 19 If A= AT and B = BT, which of these matrices are certainly symmetric? (a) A%> - B? (b) (A+ B)(A- B) (c) ABA (d) ABAB. (a) How many entries of S can be chosen independently, if S = ST is 5 by 5? (b) How do L and D (still 5 by 5) give the same number of choices in LDLT? (c) How many entries can be chosen if A is skew-symmetric ? (AT = —A). (d) Why does AT A have o negative numbers on its diagonal ? Factor these symmetric matrices into S = LDLT. The pivot matrix D is diagonal: 2 -1 0] s:[; 3] and s:[}, ’;] and S=|-1 2 -1 0 -1 2 Find the PA = L U factonzations (and check them) for 0 1 1] 1 2 0 A=|1 0 1| and A=|2 4 1 2 3 4 11 1 Find a 4 by 4 permutation matrix (call it A) that needs 3 row exchanges to reach the end of elimination. For this matrix, what are its factors P, L, and U ? 24 21 22 23 24 25 26 27 28 29 30 . Permutations and Transposes 73 Prove that the identity matrix cannot be the product of three row exchanges (or five). It can be the product of two exchanges (or four). If every row of a 4 by 4 matrix contains the numbers 0, 1, 2, 3 in some order, can the matrix be symmetric? Start with 9 entries in a 3 by 3 matrix A. Prove that no reordering of rows and reordering of columns can produce AT. (Watch the diagonal entries.) Wires go between Boston, Chicago, and Seattle. Those cities are at voltages g, ¢, xs. With unit resistances between cities, the currents between cities are in y: ‘ypc| [1 -1 0] [zB] y=Ax is yes| =10 1 -1 IcC | YBS | 1 0 -1] |zs_ (a) Find the total currents ATy out of the three cities. (b) Verify that (Az)Ty agrees with T (AT y)—six terms in both. The matrix P that multiplies (z,y, z) to give (z,z,y) is also a rotation matrix. Find P and P3. The rotation axis @ = (1,1,1) doesn’t move, it equals Pa. What is the angle of rotation from v = (2,3, —5) to Pv = (-5, 2, 3)? Here is a new factorization A = LS = triangular times symmetric : Start from A = LDU. Then A equals L (UT)™! times S = UTDU. Why is L (UT)! triangular ? Why is UT DU symmetric ? In algebra, a group of matrices includes AB and A~! if it includes A and B. “Products and inverses stay in the group.” Which of these sets are groups? Lower triangular matrices L with 1’s on the diagonal, symmetric matrices S, positive matrices M, diagonal invertible matrices D, permutation matrices P, orthogonal matrices with QT = Q1. Invent two more matrix groups. Challenge Problems If you take powers of a permutation matrix, why is some P* eventually equal to I ? Find a 5 by 5 permutation P so that the smallest power to equal I is P®. (a) Write down any 3 by 3 matrix M. Split M into S + A where S = ST is symmetric and A = — AT is anti-symmetric. (b) Find formulas for S and A involving M and MT. We want M = S + A. Suppose QT equals Q1,50 QTQ = I. Then Q is called an orthogonal matrix. (a) Show that the columns q,, ..., q,, are unit vectors: ||q,||* = 1. (b) Show that every two columns of Q are perpendicular: ¢ g, = 0. (c) Find a 2 by 2 example with first entry q;; = cos#. 74 Chapter 2. Solving Linear Equations Az = b 2.5 Derivatives and Finite Difference Matrices This section aims to connect key points of calculus and linear algebra. The matrices imitate denivatives (as nearly as possible). Derivatives tell us what is happening at one point z of space or at one moment ¢ in time : d d -&% > 0: The graph of y goes upward d_tt‘ > 0: The function u is increasing 2 2 d d E% > 0: The graph of y bends upward -dt_: > 0: The growth rate d—l; 1S increasing It would be hard to write down ideas about y(r) and u(t) more important than those. Increasing or decreasing is different from bending up (accelerating) or bending down. The parabola y = z?2 has derivatives dy/dr = 2z and d?y/dz? = 2. Its graph goes downward (decreasing) when z is negative and upward when r is positive. Everywhere the graph of z2 is bending up. Its second derivative is positive (+2) so its first derivative 2z 1s increasing. Increasing the slope makes the graph bend upwards. Now come differences instead of derivatives: y at two points and u at two times. The two points are z and + h. Previously we looked at derivatives (one point only). Now we look at differences between those points. Here are three important statements : d 1. The difference Ay = y(z + h) — y(z) is approximately h times the slope Eg at T x 1 d? 2. A better approximation to Ay adds on Eh’ times the second derivative d_z atz T 3. The exact Ay is y(z + h) — y(z) = the integral of % fromztox + h Statements 1 and 2 are connecting the change y(z + h) — y(z) to the derivatives of y at one point. The first derivative gives “first order” approximation—good but not great. By including the second derivative (times %h\"') we get a “second order’” approximation— much better. Compare those options for y = z? with dy/dz = 2z and d%y/dz? = 2. The exact Ay = (z + h)? — 22 is 1% + 2hz + h? - 2. dy 1 . d%y Ay = 2h h? = h — — h? - Z Yy * t iz T 2\" 4z first order second order tangent line parabola The derivative dy/dz multiplied the step length h = Az. That first approximation follows the tangent line to the graph. The second derivative multiplies 3 h? to give a valuable correction. The second order approximation produces the tangent parabola. 2.5. Derivatives and Finite Difference Matrices 75 For the special example y = z?, the tangent parabola is the actual parabola. If we want to estimate the error from the tangent parabola, we can go onward to the function y = 3. The graphsof y = x? and y = 3 fromz = 1to z = 1 + h = 2 show how the tangent parabola improves on the tangent line. The formula for y(z + h) — y(x) that includes all the derivatives of y is the Taylor Series. It starts with h times dy/dx and 3h? times the second derivative d®y/dx?. The next term is lh3 times the third denvatnvc Every term is A\" /n! times the nth derivative. Here are two popular ways to write the Taylor series. First, we use derivatives at z = 0 to predict y(x) at other points x. Second, we use derivatives at a point r to predict y(z + h) at a different point x + h. . dy dQ 1 d'y Taylor series for y(x) y(0) + :z:d—(O) + 5 d_(O) + -+ R e (0) : ' 1o 1 .d\"y Taylor series for y(x + h) y(z) + hy '(z) + §h y (r)+---+ mhndr\" (r) + These are infinite series! For famous functions like y(x) = e*, the series converges at all points. (That makes e* an entire analytic function.) For the function y(z) = 1/(1 — ), the series 1 + = + 2 + - - - will only converge for x| < 1. So 1/(1 — r) is analytic up to that point but r = 1 is a “singularity” called a “pole” of 1/(1 — ). Turning the Formulas Around : Derivatives from Differences Our starting point is still the important approximation by a tangent parabola: dy 1 . d%y y($+h)~y(1‘)+ha($)+§h2@($) (1) Our questions are now in the opposite direction. If we know y(x) and y(r + h), how do we estimate dy/dz ? If we also know y(x — h), how do we estimate dy/dx (improved!) and also d?y/dz?? With three values y(z — h),y(z),y(z + h) we can get the most important approximations in scientific computing. We must do this, and it is not difficult. Write the approximation (1) at £ + h and then write it again at x — h (change h to —h): d 1., d% y(z + h) —y(z) = hd—y + 2 a? (2a) dy 1. . d? y(a:—h)—y()~—hd +2h2dxg (2b) Subtract (2b) from (2a) and divide by 2h. This gives an improved approximation to dy/dx : y(z+h)—y(@-—h) d Centered difference N — 2h dx 3) 76 Chapter 2. Solving Linear Equations Az = b Add (2b) to (2a) and divide by h? for an excellent approximation to the second derivative ! Yz +h) - 2y() +y—h) _ d h? ~ dx? Second difference (4) Those are the two formulas we need for the big step we can now take—approximating a differential equation. Many important differential equations are “second order”. They include first and second derivatives as in dy dy T2 tao-+by=f(z) (5) At a typical point z, a good finite difference approximation is y(zr +h) - 2y(z) + y(z - h) +ay(r+h) —y(z — h) h2 2h +by(x) = f(x)| (6) That step went from derivatives in (5) to differences in (6), using the approximations (3) and (4). You can compare (3) with these uncentered differences, only first order accurate: dy _y@th)=y(m) | oo : y F d diff — orward difference 2 P y(z) —y(z — h) h (7) Backward difference % = + O(h) error When we average forward and backward in (7), we get centered in (3). The leading O(h) error term cancels out. In that way the centered difference has second order accuracy. Second Difference Matrices K, T, B This section introduces three important second difference matrices K, T, B. They all have the 1, -2, 1 pattern that equation (4) found in approximating d®y/dxz? (bending of a graph and acceleration of motion). The actual numbers in those matrices will reverse signs to —1,2, —1. Now we are approximating —d’y/dz?. By extending the pattern from a second difference at one point to second differences at many points, this section takes the step to matrices—crucially important. The big step is to approximate a differential equation by N difference equations at N meshpoints. We normally use equally spaced points z; = h,zo = 2h,...,zy = Nh. The spacing h is the meshwidth. The boundary points are o = 0 and zn4; = 1. Thus (N + 1)h equals the total length 1. The meshwidth is h = 1/(IN 4 1). That number h will appear in first differences, and h? appears in second differences. Start with the equation —~d?u/dz? = f(z). Suppose the boundary points £ = 0 and z = 1 are both fixed at zero. This means u(0) = 0 and also u(1) = 0. For the difference 2.5. Derivatives and Finite Difference Matrices 77 equation those boundary conditions translate into uo = 0 and un4; = 0. The unknown numbers u;,us,...,uy will be approximations to the values u(h), u(2h),...,u(Nh) of the exact solution of the differential equation between z = 0 and z = 1. Our unknown is the vector U = (u;,...,un). We need N difference equations, one at each meshpoint. We plan to express those /N equations as one matrix equation. Here is that matrix equation (1/h?)KU = F for N =4and h = 3 : . 2 -1 0 0)[w] [ fh)° ~— =f() 11-1 2 =1 0] |u]|_[/f(2h) ) h?2 0 -1 2 -1 u3 f(3h) becomes KU/h? = F | 0 0 -1 2] |us| | f(4h) Equation (4) said that d%u/dz? is close to u(x + h) — 2u(z) + u(x — h) divided by h*. This matrix K gives a natural approximation to —d?u/dr? with fixed-fixed boundary conditions ug = 0and un 4+, = 0. The key point is: N = 4 equations went into an N by N matrix. The usual row times column rule for multiplying K'U reproduces our NV difference equations. Row 1 times U iIs 2u; — uz = dot product of the row (2. —1,0.0) and the column (u;. us.u3.uy). For this first row—and also for the last row—the matrix K has built in the fixed boundary conditions ug = 0 and us = 0. A typical row is (—u; + 2uz — u3)/h? = f(h). The division by h? makes K /h? a second difference matrix, replacing —d?u /dz?. Properties of K At this point I would like to ask : What are the important patterns in the matrix K ? When I ask my class that question, I usually get three answers—sometimes four. Let me put those answers in the order that many students have given. Here is the matrix A for NV = 1. 2 -1 0 0] Second difference matrix K, = -1 2 -1 0 g Fixed-fixed endpoints 4= 0 -1 2 -1 (%) 0 0 -1 2 1. K is symmetric. The number in row i, column j is also in row j, column i. Thus K;; = Kji, on opposite sides of the main diagonal. “K transpose equals K.\" So the matrix K is symmetric across the diagonal. The simplest way to express symmetry will be KT = K. That transpose puts the rows of any matrix A into the columns of AT. In our case KT stays the same as K. [ 1 A= AN — has AT =[ ] and (AT)T = A and (ATA)T = AT4. 1 3 5 2 4 6 3 B 78 Chapter 2. Solving Linear Equations Ax = b 2. K is banded. All the nonzeros in K lie in a “band” around the main diagonal. The band has only three diagonals, so K is a tridiagonal matrix. This means that KU/h? = F can be quickly solved. Elimination is easy and fast. A matrix with a narrow band 1s sparse—mostly zeros. We don’t see this well for N = 4 (small matrix). But we see it clearly for N = 100. In that case K has 100 + 99 + 99 nonzeros out of (100)> = 10000 entries : less than 3% nonzeros. And N = 100 is by no means a large matrix. 3. K has constant diagonals. A diagonal of —1’s, then a diagonal of 2’s, then a diagonal of —1’s. The matrix is “shift-invariant”. This is because the underlying differential equation had a constant coefficient — 1. The approximation to —d?u/dx? has the same numbers —1,2, -1 at every z. (Strictly speaking the shift-invariance fails at the boundaries—always an awkward problem.) Right away that constant-diagonal property wakes up Fourier. The whole world of Founer transforms is linked to constant-diagonal matrices. We also call them filters or convolution matrices. The mathematical name is Toeplitz matrix : This K is certainly one of the most important matrices in scientific computing. The command K = toeplitz[2, —1,0, 0] constructs K = KT from its first row. That completes three properties of K that are directly visible : symmetry, sparsity, and shift-invariance. All the matrices K, have a fourth property that is equally important. But this fact is not directly obvious: 4. K is invertible 4 3 2 1] : -1 1|13 6 4 2 . -1 1 __ - It has an inverse matrix K K, = 1o 4 6 3 9) Then K'!K =ITand KK 1 =1 1 2 3 4] This inverse matrix K~! is also symmetric, but it is not tridiagonal. It is a dense matrix (in fact no entries are zero). Numerically we don’t need to know K~ !, but we do need to know that the inverse exists. Invertibility is not easy to decide from a quick look at the matrix. Theoretically, we can compute the determinant. (Here it was 5.) Since we have to divide by it, the determinant must not be zero. Then the matrix has an inverse—but computing the determinant or K ~? is almost never done in practice ! It is a very poor way to find the solution vector K ! F. What we actually do is to go ahead with the elimination steps that solve KU = F. The nonzero pivots on the main diagonal of the triangular matrix show that K is invertible. The backslash command U = K\\ F produces the fastest solution in MATLAB. 5. I am going to add one more property: The symmetric matrices K,, are positive definite. A goal of Chapter 6 is to explain what this means. We plan to contrast K and T (positive definite) with the matrix B (positive semidefinite). 79 2.5. Dernivatives and Finite Difference Matrices All depends on the difference between “positive” and “nonnegative”: > 0 and > 0. An invertible matrix has n nonzero pivots Pivots A positive definite symmetric matrix has n positive pivots A positive semidefinite symmetric matrix has n nonnegative pivots The Free-Fixed Matrices T,, To complete this section, we will describe T}, and B,,. They are variations on K,. Those variations come from *“‘changing the boundary conditions”. We are thinking of an elastic bar that is fixed at two ends or only one end or entirely free. This model problem will be described after we see the matrices. The second difference matrices T,, are symmetric and tridiagonal like K,. But the (1,1) entry in T, is changed from 2 to 1. The first row 1, —1 represents a free boundary condition du/dr = 0, whose meaning we will soon understand. 1 -1 0 O Free-fixed boundary conditions -1 2 =1 0 Ty = (10) Still positive definite 0 -1 2 -1 0 0 -1 2| T is no longer Toeplitz, because its main diagonal is not constant. But it does have a simpler factorization than K (because every pivot of T equals 1). ( 1 1T -1 0 o0 -1 1 1 -1 0} _ =190 21 1 1 -1 | =LY (b 0 0 -1 1]¢{ 1 T comes from multiplying a backward difference L times its transpose: U = LT. Notice that U is minus a forward difference. That gives the minus sign we need, because T is minus a second difference. That minus sign produces a positive definite matrix T = LLT! Please notice that L,U, and T = LU all have beautiful inverses. 1111 11 L= T-1=py-1[- 1= 1 U-1l= . (12) 1 1 1 - The Free-Free Matrices B,, are Singular For a square matrix B, “singular” means “not invertible”. One test is determinant = zero. The inverse of B includes a division by its determinant D, so we need D # 0. A better test (since computing D is not efficient) is to find a nonzero vector such that Bx = 0. 80 Chapter 2. Solving Linear Equations Az = b If B multiplies a nonzero vector x to produce Bx = 0, then B cannot be invertible. The inverse would give B~! Bz = z. But this would mean B~10 # 0. Not possible. Example The free-free matrix B; has 1 (not 2) in its (1, 1) and B; -1 1 -1 0] 2 -1 0 -1 IJ has B3.’B = I - 1 -1 0 -1 2 -1 -1 1 - - 1 1 -1.4 (3,3) corners: : (0 | 0 |0 (13) The all-ones vector z = (1,1, 1) solves Bsx = 0. That vector is in the nullspace of Bs. Any constant vector £ = (c, ¢, ¢) will be in that nullspace, with Bsx = 0. This is because all the rows of B3 add to zero. Such a matrix cannot be invertible. To see why Bj is singular when K3 and T3 are invertible, look at these models. They have three masses connected by springs. The differences are at the ends (the boundaries). For K3, both ends are connected to fixed supports. For B3, both of the ends are free: no supports. In between, T3 is free at the top and fixed at the bottom. fixed free free w w w K 3 T3 BS free fixed fixed Figure 2.3: K is fixed-fixed, T is free-fixed, B is free-free (singular, no inverse). If you add a weight W in the middle box, the B3 springs cannot adjust because they have no support. But the K3 and T3 springs will adjust to balance that weight : - - Masses and springs lead to matrices. Elastic bars lead to differential equations. 2 -1 Qfwe] [0] C 1 -1 11 2w 0 -1 2 -1|| W |=|W]ad |-1 2 -1]||2W |=|W |.(4 L -1 2]|wW/2] |0 -1 2| W | 0 2.5. Denvatives and Finite Difference Matrices 81 Problem Set 2.5 1 10 11 Find the first order and second order terms in Ay (involving h and h?) for the function y(z) = z3 with Ay = (z + h)? — z3. Multiply out (z + h)3 and show that your answers match h dy/dz and 3 h? d®y/dz?. For the function y = e*, what is the slope of the tangent line at the point r = 0, y = 1? What is the equation y = - - - for that tangent line ? Continuing with y = e*, what is the second derivative at r = 0 and what is the equation for the tangent parabola? This is the second order approximation to e” : y(h) = y(0) + hdy/dz + § h? d®y/dz?. Now find all the derivatives d\"y/dz™ of y(x) =e€* at £ =0. Write the Taylor Series ! h™ d\" 2(0) + n! drn y(h)=e\"=y(0)+h3—:(o)+...+ I believe that this is the most important infinite series in mathematics. For y(z) = sin x, what order of accuracy is the approximation sin h = h ? Euler’s great formula e*® = cos x + © sinx can come by comparing Taylor se- ries ! What are all the derivativesof y = e** atx = 0? . 1 1 1 . 1 e“\"=l+z’x+-2-(ix)2+6(i:r)3+ SR (1—§r2+--)+z(1—6.r3+--) For y(z) = z3 and h = 1/10, compute these approximations to dy/dr at r = 0: (h) = y(=h) Forward y(h) — y(0) Backward y(0) — y(—h) 2h h h We know that y(h) =~ y(0) + hdy/dz (0) + 1h? d°y/dz? (0). Substitute into the three formulas in the previous problem. Up to this second order h? term, which formula is exact ? What are the errors in the other two formulas ? Centered J The derivative of y = e* at z = 0 is dy/dr = 1. Find the centered-forward- backward approximations to dy/dz (0) using h = 1. Which is closest to 17? Fory = e and h = 1 how close is (e — 2 + e~!)/1? to the true second derivative ofe*atx =07? When a first difference has coefficients 1, —1 and a second difference has coefficients 1, —2, 1, what four numbers do you think appear in a third difference ? (You would divide by h3 to approximate d3y/dy3. Third differences are pretty rare, but try yours on the functions y3 and y?.) 82 12 13 14 15 16 17 18 Chapter 2. Solving Linear Equations Az = b The transpose of the centered difference Ag is —Ag (antisymmetric). That is like the minus sign in integration by parts, when f(z)g(z) drops to zero at £oc: -4 > d Integration by parts / f(z) Eg dr = — / p (r)dr. Verify the summation by parts ) \" f; (gi41 - gi—1) = — ) _(fit1 — fi—1) g:. Show that four samples of u can give fourth-order accuracy for du/dr at the center: ~uy+8u; —8u_1+u_y du 4 d°u Test u = z2 2% \"t @t ad u=2f The inverses of Kz and K4 have : = : and 1' 3 4 determinant 4 5 - - 4 3 2 1] 3 2 1 _ 1 113 6 4 2 1_ 1 -1_ 1 K3—4f;§ and K, 512 4 6 3 - . _1 2 3 4J First guess the determinant of K = Ks. Challenge: Guess the inverse of K. Then compute det( K )+ inv( K )—any software is allowed. The matnices BT, K come from Ao, Ay, A2 with 0,1,2 boundary conditions. -1 1 00 A=|0 -1 10 , . . 0 0 -1 1 B = Aj Ao is a second difference matrix Which column of Ay would you remove to produce A; with T = AT A;? Which column would you remove next to produce A, with K = AT A,? Ay is a forward difference matrix Show that D, D gives a second difference. Is D, invertible ? - - 1 -1 0 0 1 0 0 -1 _ 0 1 -1 0 . r | -1 1 0 O D, = 0 0 1 -1 times D; = 0 —1 1 0 equals Cjy -1 0 0 1] 0 0 -1 1 j Solve —u” = cos4rz with fixed-fixed conditions u(0) = u(1) = 0. Use K4 and Kg to compute u;,. .., u, and plot on the same graph with u(z) : Uip) —2u; + Ui = h? cosdmih with wug = Un+1 = 0. The centered first difference matrix might not be invertible ! 0 1 0 Test Az;=|-1 0 1 and A4 = QO O 2.5. Derivatives and Finite Difference Matrices 83 Thoughts on Chapter 2 I think of Chapter 2 as useful and practical. It solves Ax = b for a square invertible matrix. The properties of A~! are important to know: (AB)™! = B~ '!A-!and x = A~!b. But we seldom compute inverse matrices. It is expensive and unnecessary to find A~! and multiply A~! times b. The solution is much easier for an upper triangular matrix U. All entries below the main diagonal are zero. Then the nth equation of Uz = c is just up,r, = c,. Dividing by u,, produces r,. Now the equation above is also easy: It tells us r,_;. The whole problem Uz = c is solved from last equation to first equation: this is back substitution. The goal now is to change Az = b into Uz = c. In principle this is easy to do. For equations i = 2 to i = n, subtract a;;/a)) times equation 1 from equation i. That leaves n — 1 equations with unknowns z2 to r,. Repeat on those equations, to find n — 2 equations with unknowns z3 to r,,. After n>/3 steps, you have Uz = ¢ with an upper triangular matrix U—ready for back substitution: £ = U~ 'c = A7 'b. The beautiful formulais A = LU. L is the lower triangular matrix of multipliers (like ¢;; = a;1/ay; in the first column). U is the upper triangular matrix that appears when elimination produces Uz = c. The only unhappy note is when a diagonal entry u;; i1s zero or small—in which case a later number £;; will be infinite or large. Then we exchange equation z with equation below it, to make the pivot u;; as large as possible. The Matrices in Chapter 2 We won't see much more of A = LU or PA = LU. The equations Az = b that they solve are the nice ones—and mathematics is not so much about “nice ones”. The fun 1s to work harder and climb higher. What to do if A is not invertible ? What to do if A is not square ? What to do if A has very many columns and/or very many rows ? Those are real questions and the answers will fill this book. What we carry forward from Chapter 2 is the memory of useful matrices—and of 1deas which Chapter 3 will organize and extend. Here are the “matrix ideas” from Chapter 2: Inverse matrix Singular matrix Finite difference matnx Upper triangular U Lower triangular L Centered, Forward, Backward Elimination matrix E Permutation matrix P Second difference matrix Transpose matrix AT Symmetric matrix S = ST K,T,B Factorization A = LU Factorization S = LDLT Fixed ends or free ends Invertible n by n matrices have column space R\" and row space R\". The columns of A are independent and the rows of A are independent. Chapter 3 will allow every matrix, and it will need new ideas. 3 The Four Fundamental Subspaces 3.1 Vector Spaces and Subspaces 3.2 The Nullspace of A: Solving Az =0 3.3 The Complete Solution to Az = b 34 Independence, Basis, and Dimension 3.5 Dimensions of the Four Subspaces Section 3.1 opens with a pure algebra question. How do we define a “vector space”? Looking at R, the key operations are v + w and cv. They are connected by simple laws like ¢ (v + w) = cv + cw. We must be able to add v + w, and multiply by c. Section 3.1 will give eight rules that the vectors v and the scalars ¢ must satisfy. Notice that v and w could be matrices ! We can add matrices and we can add functions (and multiply by c). So we can have matrix spaces and function spaces. And inside R\", we could allow only vectors z that satisfy Az = 0. That produces the “nullspace of A”. All combinations of solutions to Az = 0 are also solutions : The nullspace is a subspace. Linear algebra gives us a way to solve Az = 0. The best system is elimination: Simplify Az = 0to Rz = 0. Then A = CR tells us how each dependent column of A is a combination of the first r independent columns in C. One column is a combination of other columns! Right there we have a special solution to Ax = 0. Finally comes the idea of a basis : A set of vectors that perfectly describes the space. Their combinations give one and only one way to produce every vector in the space. The r independent columns of A are a basis for C(A). The n — r special solutions to Az = 0 are a basis for N(A). Those subspaces C and N have “dimensions” r and n — r. Chapter 2 was about square invertible matrices. All four of the matrices in PA = LU had full rank r = m = n. The column space and row space of A were the full space R\". The nullspaces of A and AT contained only the zero vector. Chapter 3 moves to a higher level ! It may be the most important chapter in the book. Every m by n matrix is allowed, and there will surely be nonzero solutions to Az = 0 if n > m. Notice that this nullspace of A is not like the column space and row space. The nullspace starts with equations Az = 0, not with columns or rows from A. The prime goal of this chapter is the “Fundamental Theorem of Linear Algebra”. In Section 3.5 this connects the four subspaces and their dimensions r,*,n — r,m — r. Column space of A, row space of A, nullspace of A, nullspace of AT. I hope the picture that goes with it makes the Fundamental Theorem easy to remember. 84 3.1. Vector Spaces and Subspaces 85 3.1 Vector Spaces and Subspaces /1 Requirement: All linear combinations cv + dw must stay in the vector space. \\ 2 The row space of A is “spanned” by the rows of A. The columns of A span C(A). \\3 Matrices M, to My and functions f; to fx span matrix spaces and function spacey Start with the vector spaces R', R%, R, . .. The space R\" contains all column vectors v of length n. The components v, to v, are real numbers. When complex numbers like v1 = 2 + 3: are allowed, the spaces become C!, C2%2 C3,. ... We know how to add vectors v and w in R™. We know how to multiply a vector by a number c or d to get cv or dw. So we can find linear combinations cv + dw in the vector space R\". This operation of “linear combinations” is fundamental for any vector space. It must satisfy eight rules. Those eight rules are listed at the start of Problem Set 3.1 — they begin with v + w = w + v and they are easy to check in R\". They don’t need to be memorized ' One important requirement : All linear combinations cv + dw stay in the vector space. The set of all positive vectors (vy,...,v,) with every v; > 0 is not a vector space. The set of solutions to Az = (1,1,...,1) is not a vector space. A line in R\" is not a vector space unless it goes through the center point (0,0, ...,0). If the line does go through 0, we can multiply points on the line by any number c. And we can add points on the line—without leaving the line. That line through 0 in R\" shows the idea of a subspace : A vector space inside another vector space. Examples of Vector Spaces This book is mainly about the vector spaces R™ and their subspaces like lines and planes. The space Z that only contains the zero vector 0 = (0,0, ...,0) counts as a subspace ! Combinations c0 + d 0 are still O (inside the subspace). Z is the smallest vector space. We often see Z as the nullspace of an invertible matrix : If the only solution to Az = 0 is the zero vector = 0, then the columns are independent and the nullspace of A is Z. We can certainly accept vector spaces of matrices. The space R3 * 3 contains all 3 by 3 matrices. We can take combinations cA + dB of those matrices. They easily satisfy the eight rules. One subspace would be the 3 by 3 matrices with all 9 entnes equal— a “line of matrices”. S = all symmetric 3 by 3 matrices is also a subspace: A + B stays symmetric. But the invertible matrices are not a subspace. Why not ? We can also accept vector spaces of functions. The line of functions y = ce® (any c) would be a “line in function space”. That line contains all the solutions to the differential equation dy/dx = y. Another function space contains all quadratics y = a + bx + cx?. Those are the solutions to d3y/dz3 = 0. You see how linear differential equations replace linear algebraic equations Ax = 0 when we move into function space. In some way the space of all 3 by 3 matrices is essentially the same space as R®. The space of functions f(z) = a + bx + cz? is essentially the same as R®. 86 Chapter 3. The Four Fundamental Subspaces Linear combinations of the matrices and functions are safely in those spaces. This book will stay almost entirely with ordinary column vectors and not functions. To repeat: The word “space” means that all linear combinations of the vectors or matrices or functions stay inside the space. Subspaces of Vector Spaces At different times, we will ask you to think of matrices and functions as vectors. But at all times, the vectors that we need most are ordinary column vectors. They are vectors with n components—but maybe not all of the vectors with n components. There are important vector spaces inside R\". Those are subspaces of R™. Start with the usual three-dimensional space R3. Choose a plane through the origin (0,0,0). That plane is a vector space in its own right. If we add two vectors in the plane, their sum is in the plane. If we multiply an in-plane vector by 2 or —9, it stays in the plane. A plane in three-dimensional space is not R? (even if it looks like R?). The vectors have three components and they belong to R3. The plane is a vector space inside R. This illustrates one of the most fundamental ideas in linear algebra. The plane going through (0, 0, 0) is a subspace of the full vector space R3. DEFINITION A subspace of a vector space is a set of vectors (including 0) that satisfies two requirements: If v and w are vectors in the subspace and c is any scalar, then (i) v+ wis in the subspace (ii) cvisin the subspace In other words, the set of vectors is “closed” under addition v + w and multiplication cv (and dw). Those operations leave us in the subspace. We can also subtract, because —w is in the subspace and its sum with v is v — w. All linear combinations stay in the subspace. These operations follow the rules of the host space, so the eight required conditions are automatic. We just have to check that cv + dw stays in the subspace. First fact: Every subspace contains the zero vector. The plane in R3 has to go through (0,0, 0). We mention this separately, for extra emphasis, but it follows directly from rule (ii). Choose ¢ = 0, and that rule requires Qv to be in the subspace. Planes that don’t contain the origin fail those tests. Those planes are not subspaces. Lines through the origin are also subspaces. When we multiply by 5, or add two vectors on the line, then we do stay on the line. But the line must go through (0, 0, 0). Another subspace is all of R3. The whole space is a subspace (of itself). Here is a list of all the possible subspaces of R3: (L) Any line through (0,0, 0) (R3) The whole space (P) Any plane through (0,0,0) (Z) The single vector (0,0, 0) If we try to keep only part of a plane or line, the requirements for a subspace don’t hold. Look at these examples in RZ—they are not subspaces. 3.1. Vector Spaces and Subspaces 87 Example 1 Keep only the vectors (z,y) whose components are positive or zero (this is a quarter-plane). The vector (2, 3) is included but (—2, —3) is not. So rule (ii) is violated when we try to multiply by ¢ = —1. The quarter-plane is not a subspace. Example 2 Include also the vectors whose components are both negative. Now we have two quarter-planes. Requirement (ii) is satisfied; we can multiply by any c. But rule (i) now fails. The sum of v = (2,3) and w = (-3,-2) is (—1, 1), which is outside both quarter-planes. Two quarter-planes don’t make a subspace. Rules (i) and (ii) involve vector addition v + w and multiplication by scalars ¢ and d. The rules can be combined into a single requirement—the rule for subspaces: A subspace containing v and w must contain all linear combinations cv + dw. Example 3 Inside the vector space M of all 2 by 2 matrices, here are two subspaces: : : a b : : a 0 (U) All upper triangular matrices 0 d (D) All diagonal matrices 0 dl- Add any upper triangular matrices in U, and the sum is in U. Add diagonal matnices, and the sum is diagonal. In this case D is also a subspace of U! Of course the zero matrix is in these subspaces, when a, b, and d all equal zero. Z is always a subspace with one vector. Multiples of the identity matrix also form a subspace of M. Those matrices cI form a “line of matrices” inside M. It is also inside U and D. Is the matrix I a subspace by itself? Certainly not. Only the zero matrix is. Your mind will invent more subspaces of 2 by 2 matrices—write them down for Problem 5. The Column Space of A The most important subspaces are tied directly to a matrix A. We are trying to solve Ax = b. We want to describe the good right sides b—the vectors that can be written as A times some vector x. Those b’s form the *“column space” of A. Remember that Ax is a combination of the columns of A. To get every possible b, we use every possible . Start with the columns of A and take all their linear combinations. This produces the column space of A. 1t is a vector space made up of column vectors. DEFINITION The column space consists of all linear combinations of the columns. Those combinations are all possible vectors Ax. They fill the column space C(A). This column space is crucial to the whole book, and here is why. To solve AT = bis to express b as a combination of the columns. The right side b has to be in the column space produced by A, or Az = b has no solution ! The equations Ax = b are solvable if and only if b is in the column space of A. When b is in the column space C(A), it is a combination of the columns of A. The coefficients in that combination will solve Ax = b. The word “space” is justified by taking all combinations of the columns. This includes the zero vector in R™. 88 Chapter 3. The Four Fundamental Subspaces Caution: The columns of A do not form a subspace ! The invertible matrices do not form a subspace. The singular matrices do not form a subspace. You have to include all linear combinations. The columns of A “span” a subspace when we take their combinations. The Row Space of A The rows of A are the columns of AT, the n by m transpose matrix. Since we prefer to work with column vectors, we welcome AT—it contains rows of A. The row space of A is the column space C(AT) of the transpose matrix AT This row space is a subspace of R\". It contains m column vectors from AT and all their combinations. The equations ATy = c are solvable exactly when the vector c is in the subspace C(AT) = row space of A. Chapter 1 explained why C(A) and C(AT) both contain r independent vectors and no more. Then r = rank of A = rank of AT. A new proof is in Section 3.5. Example The row space of the rank 1 matrix A = uw? is the line of all column vectors cv. This is because every column of AT = vuT is a multiple of v. One vector v spans the row space of A, one vector u spans the column space. Those spaces are lines through 0. The Columns of A Span the Vector Space C(A) One useful new word: “Span”. Suppose we start with a set S of vectors in R™. If § contains only N vectors, it is certainly not a subspace. But if we include all combinations of the vectors in S, then we have a vector space V. In this case the set S spans V. In fact V is the smallest vector space containing S (because we are forced to include all combinations to produce a vector space). This is exactly what we did for the columns of A. Those n columns span the column space C(A) = all combinations of the columns. Independence is not required by the word span. In the same way, the m columns of AT span the row space C(AT). Test question. Show that the invertible 2 by 2 matrices span R**? = all 2 by 2’s. Examples If the n by n matrix A is invertible, then its columns span R\". The invertible 3 by 3 matrices span the whole matrix space R3*3, The singular (not invertible) 3 by 3 matrices also span R3%3, Next comes the nullspace N(A) and that requires new ideas. We start with Ax = 0 (m equations and not n vectors). The solutions & to those equations give the nullspace. It is a vector space because Az = 0 and Ay = 0 lead to A(cx + dy) = 0. But we have to work to find those solutions z and y. Test question: When do ten vectors span R® ? This is very possible. But not any ten. 3.1. Vector Spaces and Subspaces 89 Problem Set 3.1 The first problems 1-7 are about vector spaces in general. The vectors in those spaces are not necessarily column vectors. In the definition of a vector space, vector addition T + y and scalar multiplication cx must obey the following eight rules : Hhzty=y+zx 2 z+(y+z)=(r+y)+z (3) There is a unique *“zero vector” such that x + 0 = x for all (4) For each x there is a unique vector —x suchthatx + (—x) =0 (5) 1 times x equals x (6) (cic2)x = c1(ca2) rules (1) to (4) aboutx + y (7) c(x+y)=cx+cy rules (5) to (6) about cx (8) (c1 + c2)x = c1x + cox. rules (7) to (8) connect them 1 Suppose (x1,x2) + (y1,y2) is defined to be (r; + y2.x2 + y;). With the usual multiplication cx = (cx;,cx2), which of the eight conditions are not satisfied ? 2 Suppose the multiplication cx is defined to produce (cr;.0) instead of (cr;.cxsy). With the usual addition in R?, are the eight conditions satisfied ? 3 (a) Which rules are broken if we keep only the positive numbers r > 0 in R!? Every ¢ must be allowed. This half-line is not a subspace. (b) The positive numbers with & + y and cx redefined to equal the usual ry and x€ do satisfy the eight rules. Test rule 7 whenc = 3,z = 2.y = 1. (Then T + y = 2 and cx = 8.) Which number acts as the “zero vector™ in this space ? 4 The matrix A = [g :3] is a “vector” in the space M of all 2 by 2 matrices. Wnite down the zero vector in this space, the vector %A, and the vector — A. What matrices are in the smallest subspace containing A (the subspace spanned by A)? 5 (a) Describe a subspace of M that contains A = [3 §] butnot B = [g _7]. (b) If a subspace of M does contain A and B, must it contain the identity matnix ? (c) Describe a subspace of M that contains no nonzero diagonal matrices. 90 Chapter 3. The Four Fundamental Subspaces 6 The functions f(z) = 2 and g(z) = 5z are “vectors” in F. This is the vector space of all real functions. (The functions are defined for —oc < r < 00.) The combination 3 f(z) — 4¢g(z) is the function h(z) = 7 Which rule is broken if multiplying f(z) by c gives the function f(cr)? Keep the usual addition f(z) + g(x). Questions 8-15 are about the “subspace requirements”: x + y and cx (and then all linear combinations cx + dy) stay in the subspace. 8 One subspace requirement can be met while the other fails. Show this by finding (a) A set of vectors in R? for which = + y stays in the set but %:1: may be outside. (b) A set of vectors in R? (other than two quarter-planes) for which every cz stays in the set but £ + y may be outside. 9 Which of these subsets of R3 are actually subspaces ? They all span subspaces ! (a) The plane of vectors (b;, b, b3) with by = bo. (b) The plane of vectors with b, = 1. (c) The vectors with b;bob3 = 0. (d) All linear combinations of v = (1,4,0) and w = (2, 2, 2). (e) All vectors that satisfy by + by + b3 = 0. () All vectors with b; < b, < bs. 10 Describe the smallest subspace of the matrix space M that contains 1 0 0 1 1 1 1 1 1 0 @ o ajmloo] oo @ a]mls 3] 11 Let P be the plane in R? with equation z + y — 2z = 4. The origin (0, 0, 0) is not in P! Find two vectors in P and check that their sum is not in P. 12 Let Py be the plane through (0,0, 0) parallel to the previous plane P. What is the equation for Py? Find two vectors in Pg and check that their sum is in Py. 13 Suppose P is a plane through (0,0,0) and L is a line through (0, 0, 0). The smallest vector space containing both P and L is either or : 14 (a) Show that the set of invertible matrices in M is not a subspace. (b) Show that the set of singular matrices in M is not a subspace. 3.1. Vector Spaces and Subspaces 91 15 True or false (check addition in each case by an example): (a) The symmetric matrices in M (with AT = A) form a subspace. (b) The skew-symmetric matrices in M (with AT = — A) form a subspace. (c) The unsymmetric matrices in M (with AT # A) span a subspace. Questions 16-26 are about column spaces C(A) and the equation Az = b. 16 Describe the column spaces (lines or planes) of these particular matrices: (1 2] (1 0] (1 A=10 0 and B=|0 2 and C= |2 0 0] 0 0 0 ﬂ 0 of. Od 17 For which right sides (find a condition on b, b,, b3) are these systems solvable? i 1 4 2- r.’L‘lq Pbl- r 1 4- T -blq (a) 2 8 4| |z = |bs (b) 2 9 [IIJ = | by -1 -4 -2 [z3] [bs -1 —4 L7 b3 | 18 Adding row 1 of A to row 2 produces B. Adding column 1 to column 2 produces C'. A combination of the columns of (B or C ?) is also a combination of the columns of A. Which two matrices have the same column ? 19 For which (b, b2, b3) do these systems have a solution ? Then b is in C(A). 1 1 1] [z [& 1 1 1] [z1] [bi] 0 1 1 2| = | by and 0 1 1 o | = | by _0 0 IJ _:L‘;;-i _b3J _0 0 0d _1‘3J _ng (1 1 1] [z] [&n] and 0 0 1 T2 | = b2 0 0 1] [z |bs. 20 (Recommended) If we add an extra column b to a matrix A, then the column space gets larger unless _____ . Give an example where the column space gets larger and an example where it doesn’t. Why is Ax = b solvable exactly when the column space doesn’t get larger ? Then that space is the same for A and [ A b] : 21 The columns of AB are combinations of the columns of A. This means: The column space of AB is contained in (possibly equal to) the column space of A. Give an example where the column spaces of A and AB are not equal. 92 23 24 25 26 27 28 30 31 Chapter 3. The Four Fundamental Subspaces Suppose Az = b and Ay = b° are both solvable. Then Az = b + b\" is solvable. What is z? This translates into: If b and b° are in the column space C(A), then b+ b’ isin C(A). That is a requirement for a vector space. If Ais any S by S invertible matrix, then its column space 1s . Why? True or false (with a counterexample if false): (a) The vectors b that are not in the column space C(A) form a subspace. (b) If C(A) contains only the zero vector, then A is the zero matrix. (c) The column space of 2A equals the column space of A. (d) The column space of A — I equals the column space of A (test this). Construct a 3 by 3 matrix whose column space contains (1,1,0) and (1,0, 1) but not (1,1,1). Construct a 3 by 3 matrix whose column space is only a line. If the 9 by 12 system Az = b is solvable for every b, then C(A) = Challenge Problems Suppose S and T are two subspaces of a vector space V. (a) Definition: The sum S 4 T contains all sums 8 + ¢ of a vector 8 in S and a vector ¢ in T. Show that S + T satisfies the requirements for a vector space. Addition and scalar multiplication stay inside S + T . (b) If S and T are lines in R™, what is the difference between S + T and S U T? That union contains all vectors from S or T or both. Explain this statement: The spanof SUT is S 4+ T. (Section 3.4 returns to this word “span”.) If S is the column space of A and T is C(B), then S + T is the column space of what matrix M ? The columns of A and B and M are all in R™. I don’t think A+ B is always a correct M. We want the columns of M to span S + T. Show that the matrices A and [ A AB | (with extra columns) have the same column space. But find a square matrix with C(A?) smaller than C(A). Important point : An n by n matrix has C(A) = R\" exactly when A is an matrix. Find another independent solution (after y = €¥) to the second order differential equation d*y/dz? = y. Find two independent solutions to d®y/dx? = —y. Then the 2-dimensional solution space contains all linear combinations y = Suppose V and W are two subspaces of R\". Their “intersection” V' N W contains the vectors that are in both subspaces. (Notice that the zero vector is in V and W.) Show that VNW is a subspace by testing the requirement : If z and y are in VN W, whyiscz +dyinVNW? 3.2. Computing the Nullspace by Elimination: A=CR 93 3.2 Computing the Nullspace by Elimination: A=CR ﬁhe nullspace N(A) in R\" contains all solutions to Az = 0. This includes £ = O. \\ 2 Elimination from A to Ry to R does not change the nullspace: N(A) = N(Rp) = N(R). 3 The reduced row echelon form Ry = rref(A) has I in r columns and F in n — r columns. 4 If column j is dependent on previous columns, Az = 0 has a “special solution™ with ; = 1. S5 The n — r special solutions to Ax = 0 contain — F' and I (page 97). {Every short wide matrix with m < n has nonzero solutions to Ax = 0 in its nullspace. / This section finds all solutions to Ax = 0. When A is a square invertible matrix (in this case its rank is » = n), the only solution is & = 0. Then the nullspace only contains the zero vector: the columns of A are independent. But in general A has r independent columns (r = rank). The other n — r columns of A are combinations of those independent columns. We will find n — r vectors in the nullspace of A—special solutions to Ax = 0. With square invertible matrices, Chapter 2 simplified A to an upper tnangular U. For matrices of all shapes, elimination will simplify Az = 0 to an “echelon form” Rx = 0. (Actually R = I when A is invertible, so elimination is now going further than L and U : as far as it can.) We begin with two examples of R, to show where we are going. Here is a matrix Rof rank r = 2. It has n = 4 columns so we look forn — r = 4 — 2 = 2 independent solutions to Rx = 0. The nullspace N(R) will have dimension 2. 1 0 3 o 01 4 6 Iy +3rz3+ory =0 Examplel R=[I F]Pz[ ry +4r3+6r3 =0 ] Rx=01s Two *“special solutions™ are easy to find, when z3 and z4 are 1 and 0 or 0 and 1. Setrx3 =1andzy =0. Equationl givesr; = —3. Equation 2 gives 1o = —4. Setr3 =0andxy =1. Equationl giveszz = —5. Equation 2 gives ro = —6. These two special solutions 8; = (—3,—4,1,0) and 8 = (—5,—6.0,1) are in the nullspace of R. They give Rs; = 0 and Rs; = 0. Any combination of those two solutions will also be in the nullspace. The matrix R times the vector £ = c;8; + c282 produces zero. Soon we will call those special solutions 8, and s, a basis for the nullspace. That matrix R was easy to work with. Its first two columns contained the identity matrix (P was just I). R is an example of a matrix in “reduced row echelon form”. We will give one more example to show a variation Ry that is still in reduced row echelon form and still simple. The subscript in Ry indicates that there can also be rows of zeros. 94 Chapter 3. The Four Fundamental Subspaces 1 70 8] Ty + 7ry +0r3 + 8z4 =0 Example2 Ry=|0 01 9 Roxz =0 is r3 +9r4 =0 000 0. 0=0 Now the identity matrix is inside columns 1 and 3. And row 3 is all zero. This still counts as a reduced row echelon form—elimination can’t make it simpler. The 1's in the identity matrix are still the first nonzeros in their ows. The word “echelon” refers to the “staircase” of 1's. Any zero rows always come last in Rj. The special solutions still have 1 and 0 for the “free variables”—which are x2 and z4. Setzz =1landzg =0. Equationlgivesx; = —7. Equation2 givesxz = 0. Setzz =0andzg4 =1. Equationlgivesx; = —8. Equation 2 gives x3 = —9. Those special solutions are now 8; = (—7,1,0,0) and s, = (—8.0,—9.1). For the free vanables r; and z4, we freely choose 1,0 and then 0, 1. Then the equations Rox = 0 tell us z; and r3. If we want to keep I in columns 1 and 2, we need a permutation P. Here is the plan for this section of the book. We start with any m by n matrix A. We apply elimination (to be explained). That changes A into its reduced row echelon form Ry = rref (A). Our two examples showed the simplest form Ry = R, and then the most general form when Ry may have zero rows. Removing all zero rows of Ry leaves R. r.m,n=2,24 | Simplest case R=[I F] as in OO M OM oo N =O OO W S ©o S Wm J r.m,n =234 | General case Ro=[£ F(;]P as in I and F have r rows. The reduced matrix Ry and the original A have m rows. So Ry has m — r rows of zeros. When we remove those zero rows, wehave R=[ I F | P. Summary In Chapter 2, A was square and invertible. Elimination on Ax = b stopped at Uz = c (upper triangular U). Then back-substitution solved for z. Here we allow any matrix A of rank r. Elimination continues until we reach an r by r identity matrix I, as in Chapter 1. (We didn’t know about elimination then, but we somehow produced the first r independent columns of A in C.) Now we go from A to C and R. Key question for each new column: Is it dependent on the previous columns? In this section, Az = 0 becomes Rz = 0 and we find the nullspace. In Section 3.3, Az = b becomes Rx = d and we find all solutions. Our examples showed R after elimination has done all it can to simplify A. We will soon see those steps to R. Appendix 9 summarizes rref (A) as the key step to A = CR and solving Az = 0. 3.2. Computing the Nullspace by Elimination: A=CR 95 Elimination from A to rref( A) : Reduced Row Echelon Form How does elimination work ? In any order, we may execute these three different steps : 1. Subtract a multiple of one row from another row (above or below !) 2. Multiply a row by any nonzero number 3. Exchange any rows. Let me stay with Examples 1 and 2, the simplest case R and the general case Fj. Here is a 2 by 4 matrix A that elimination reduces to our 2 by 4 example R = [ I F ] _121117_)121117 1035_R |3 7 37 57 01 4 6 01 4 6| Elimination starts with column 1. It subtracts 3 times row 1 from row 2. That produces the zero in the middle matrix. Now column 1 is set (the corner pivot was A;; = 1 which is what we want). Moving to column 2, we subtract 2 times the new row 2 from row 1. Upward elimination produces the second zero in R. Now R starts with the r by r identity matrix I. The rank is » = 2 and elimination on this matrix A is complete. What did elimination actually do ? It inverted the leading 2 by 2 matrix W = [; 3] | The matrix W at the start of A became I at the start of R : Multiply WA =W~ W H |toproduceR=(I W™'H |=[1 F |. We always knew that the dependent columns of A (in H) would be some combination of the independent columns (in W). Now we see that H = W F'. The matrix F is telling us how to combine the independent columns of A to produce the dependent columns. We can understand the echelon form R and the role of F'! Dependent 11 17 | I I T 3 5 columns [37 57]‘WF“[3 7]\"\"‘“‘[4 6]' However you compute R from A, you always reach the same R. R is completely determined by A (even if there are different elimination steps that lead from A to R). A 1 The first r independent columns of A locate the columns of R containing [ The permutation P puts those columns into their correct places in A. 2 The remaining columns F' in R are determined by the equation H = W F': (Dependent columns of A) = (Independent columns of A) times F' 3 The last m — r rows of Ry are rows of zeros. Example 2 Come back to the matrix A that leads to our second reduced echelon form Ry. Both A and Ry are 3 by 4 matrices of rank r = 2. Watch each step of elimination : 1 7 3 3] [1 73 3] [170 8] [170 8 A=|2 14 6 70|>|/0 0 0 O0|—=|0 0 0 0|—=]|0 0 1 9|=R, 2 14 9 97] |0 0 3 27 |0 O 3 27| |0 O O O 96 Chapter 3. The Four Fundamental Subspaces Elimination Column by Column : The Steps from A to R, After those examples, here is an algorithm ! Elimination goes a column at a time, from left to right. After k columns, that part of the matnx is in its own rref form and we are ready for column k + 1. This new column has an upper part u and a lower part £. First k + 1 columns [ I(;‘ FOk ] P, followed by [ t; J : The big question is : Does this new column k + 1 join with Iy or Fj ? If £ is all zeros, the new column is dependent on the first k columns. Then u joins with Fy to produce Fy. in the next step to column k + 2. If £ is not all zero, the new column is independent of the first k columns. Pick any nonzero in £ (preferably the largest) as the pivot. Move that row of A up into row k + 1. Then subtract multiples of that pivot row to zero out (by standard elimination) all the rest of column k + 1. (That step is expected to change the columns after k + 1.) Column k + 1 joins with Iy to produce I ;. We are ready for column k + 2. At the end of elimination, we have a most desirable list of column numbers. They tell us the first r independent columns of A. Those are the columns of C. They led to the identity matrix I, vy » in the row factor R of A = CR. Example 2 showed the three allowed row operations in elimination from A to Ry : 1) Subtract a multiple of one row from another row (below or above) 2) Divide arow like [ 0 0 3 27 ]by its first nonzero entry (to reach pivot = 1) 3) Exchange rows (to move all zero rows to the bottom of Ry) A different series of steps could reach the same Ry. But that result Ry = rref(A) can’t change. The pieces of Ry are all fully determined by the original matrix A. Ry had a zero row in Example 2 because A has rank r = 2 I is in columns 1 and 3 of Ry because those are the first independent columns of A F in columns 2, 4 of R combines columns 1, 3 of A to give its dependent columns 2, 4 117 8 \" 7 35] dependent 0 9|=1]14 70 | = columns B! C times F=| 2 | 2 14 97 2and4of A = R = L) [ 1 We could never have seen in Chapter 1 that (35, 70,97) combines columns 1 and 3 of A. Please remember how the matrix R shows us the nullspace of A. To solve Ax = 0 we just have to solve Rz = 0. This is easy because of the identity matrix inside R. 3.2. Computing the Nullspace by Elimination: A=CR 97 The Matrix Factorization A = C R and the Nullspace This is our chance to complete Chapter 1. That chapter introduced the factorization A = CR by small examples: We learned the meaning of independent columns, but we had no systematic way to find them. Now we have a way : Apply elimination to reduce A to Ro. Then I in Ry locates the matrix C of independent columns in A. And removing any zero rows from Ry produces the row matrix Rin A = CR. We find two special solutions 8; and 8;—one solution for every column of F in R. Example 2 again 2 Rsy = 0 [1 70 8] 1 =[0] Put1and 0 0 01 9 0 0 in positions 2 and 4 L 0. -8 Rsy = 0 [1708] 0 =[0} ~ PutOand1 0 019 -9 0 in positions 2 and 4 1 I think s, and s, are easiest to see using the matrices —F and I and PT. The two special solutions to [/ F |« = 0 are the columns of [_FI‘] in Example 1 The special solutionsto [I F| Pz =0 are the columns of PT [_}; ] in Example 2 The first one is easy because the permutation is P = I. The second one is correct because PPT is the identity matrix for any permutation matrix P : Rr=0 [I F]PtimesPT[—I;]reducesto[] F][_€]=[O] Review Suppose the m by n matrix A has rank r. To find the n — r special solutions to Az = 0, compute the reduced row echelon form Ry of A. Remove the m — r zero rows of Rotoproduce R=[ I F ] P and A = CR. Then the special solutions to Az = 0 are the n — r columns of PT [ —I; ] Please see the last page of Chapter 3. Example 3 Elimination on A gives Rg and R. Then R reveals the nullspace of A. 1’ [ 1 2 0] 3| =210 0 1| =Ry withrank2 6 10 0 0 OO N 1 2 1 B! A=12 4 5|10 3 6 9 0 and the independent columns of A and Ry and R are 1 and 3. z = =) Il — O = o N - O — 98 Chapter 3. The Four Fundamental Subspaces To solve Az = 0and Rx = 0, set 2 = 1. Solve for z; = —2 and r3 = 0. Special solution 8 = (-2,1,0). All solutions z = (—2c, c,0). And here is A = C'R. 1 [~ n ; —CR= ; 51) [ (1) (2) (1) ]_ (column basis) (row basis) 9 39 B! A= g inC in R (=, I~ N Can you write each row of A as a combination of the rows of R ? Yes. Just use A = CR. The three rows of A are the three rows of C' times the two rows of R. For many matrices, the only solution to Az = 0 is * = 0. The columns of A are independent. The nullspace N(A) contains only the zero vector: no special solutions. The only combination of the columns that produces Ax =0 is the zero combination =0. This case of a zero nullspace Z is of the greatest importance. It says that the columns of A are independent. No combination of columns gives the zero vector (except = 0). But this can’t happen if n > m. We can’t have n independent columns in R™. Important Suppose A has more columns than rows. With n > m there is at least one free variable. The system Ax = 0 has at least one nonzero solution. Suppose Az = 0 has more unknowns than equations (n > m). There must be at least n — m free columns. Az = 0 must have nonzero solutions in N(A). The nullspace is a subspace. Its “dimension” is the number of free variables. This central idea—the dimension of a subspace—is explained in Section 3.5 of this chapter. Example 4 Find the nullspaces of A, B, M and the two special solutions to M x = 0. 1 2] 1 2 A 3 8 1 2 2 4 A“[a 8] B‘[u]‘- 2 4 M=[A 2A]“[3 8 6 16]' 6 16 Solution The equation Az = 0 has only the zero solution € = 0. The nullspace is Z. It contains only the single point z = 0 in R?. This fact comes from elimination : 1 2 1 2 1 0 . A:r:—[3 8]_’[0 2]%[0 l]—R=I No free variables, no F' A is invertible. There are no special solutions. Both columns of this matrix have pivots. The rectangular matrix B has the same nullspace Z. The first two equations in Bx = 0 again require T = 0. The last two equations would also force £ = 0. When we add extra equations (giving extra rows), the nullspace certainly cannot become larger. Extra rows impose more conditions on the vectors Z in the nullspace. The rectangular matrix M is different. It has extra columns instead of extra rows. The solution vector has four components. Elimination will produce pivots in the first two columns of M. The last two columns of M are “free”. They don’t have pivots. 3.2. Computing the Nullspace by Elimination: A=CR 99 1 2 2 4 1 0 2 0] _ M-l s sl R=|5 1 |-l F For the free variables 3 and x4, we make special choices of ones and zeros. First 3 = 1, x4 = 0 and second 3 = 0, z4 = 1. The pivot variables z, and z, are determined by the equation Rz = 0. We get two special solutions in the nullspace of M. This is also the nullspace of R : elimination doesn’t change solutions. Special solutions to Mx = 0 [ —2] \" 0] « 2 pivot 1 0 2 0 1 0 _ | 2| « variables B=1o01 0 2] s1=| g ads2=1 o] o 2free Rs; =0 Rs;=0 0] | 1] « variables Block Elimination in Three Steps : Final Thoughts The special value of matrix notation is to show the big picture. So far we have described elimination as it is usually executed, a column at a time. But if we work with blocks of the original A, then block elimination can be described in three quick steps. A has rank r. Step 1 Exchange columns of A by Pc and exchange rows of A by Pg to put the r independent columns first and r independent rows first in PR APc. WH] C PrAPc = [ J K = [ ‘}/ ]and B = [ W H ]have full rank r Step 2 Multiply the r top rows by W~! to produce W~!B=[I W 'H|=[I F] Step 3 SubtractJ [I W~'H] fromthe m —r lowerrows [ J K] toproduce [0 O] The result of Steps 1, 2, 3 is the reduced row echelon form Ry J K J K o o |“R| D PRAPc=[W H] [I W“H]_)[I W_IH] There are two facts that need explanation. They led to Step 2 and Step 3: 1. The r by r matrix W is invertible 2. The blocks satisfy JW ' H = K. 1. For the invertibility of W, we look back to the factorization A = CR. Focusing on the r independent rows of A that go into B, this is B = W R. Since B and R have rank r and W is r by 7, W must have rank 7 and must be invertible. 2. We know that the first 7 rows [ I W~!H | are linearly independent. Since A has rank r, the lower rows [ J K ] must be combinations of those upper rows. The combinations must be given by J to get the first r columns correct: JI = J. Then J times W~ H must equal K to make the last columns correct. [v}f]w-l[w H | The conclusion is that PRAPc = =CW-~-1B We need that middle factor W —! because the columns C and the rows B both contain W'. 100 Chapter 3. The Four Fundamental Subspaces Problem Set 3.2 1 Whydo A and R = E'A have the same nullspace ? We know that E is invertible. 2 Find the row reduced form R and the rank r of A and B (those depend on c). Which are the pivot columns ? Find the special solutions to Az = 0 and Bx = 0. 1 2 1] Find special solutions @ A=|3 6 3 | and Bz[z z] L4 8 C_ 3 Create a 2 by 4 matrix R whose special solutions to Rz = 0 are 8; and s : _i _(2) 1 pivot columns 1 and 3 8 = 0 and 8= _ 6 free vanables r, and x4 zoand x4 are 1,0and 0, 1 L 0 - L 1 - Describe all 2 by 4 matrices with this nullspace N(A) spanned by s; and s,. 4 Reduce A and B to their echelon forms R. Factor A and B into C times R. 1 2 2 4 6] '242] @ A=1]1 2 3 6 9 ) B=10 4 4. 00123 0 8 8 5 For the matrix A in Problem 4, find a special solution to Rz = 0 for each free var- able. Set the free variable to 1. Set the other free variables to zero. Solve Rx = 0. 6 True or false (with reason if true or example to show it is false) : (a) A square matrix has no free variables (its columns are independent). (b) An invertible matrix has no free varables. (c) An m by n matrix has no more than n pivot variables. (d) An m by n matrix has no more than m pivot vanables. 7 Cisanr by rinvertible matrix and A = [ C C ] Factor A into CR and find r independent solutions to Az = 0: a basis for the nullspace of A. 8 Put as many 1’s as possible in a 4 by 8 reduced echelon matrix R so that the free columns (dependent on previous columns) are (a) 2,4,5,6 or (b)1,3,6,7,8. 9 Suppose column 4 of a 3 by 5 matrix is all zero. Then x4 is certainly a variable. The special solution for this variable is the vector © = 10 Suppose the first and last columns of a 3 by 4 matrix are the same (not zero). Then is a free variable. Find the special solution for this free variable. 11 The nullspace of a § by 5 matrix contains only £ = 0 when the matrix has pivots. In that case the column space is R°. Explain why. 3.2. Computing the Nullspace by Elimination. A=CR 101 12 13 14 Suppose an m by n matrix has r pivots. The number of special solutions is by the Counting Theorem. The nullspace contains only = 0 when r = The column space is all of R™ when the rank is r = (Recommended) The plane £ — 3y — 2 = 12 is parallel toz — 3y — 2z = 0. One particular point on this plane is (12,0, 0). All points on the plane have the form p - ~ - p- - p- - T y|=10+y (1] +2]|0 |2 0] 0] 1] Suppose column 1 + column 3 + column 5 = 0 in a 4 by 5 matrix with four pivots. Which column has no pivot? What is the special solution? Describe N(A). Questions 15-20 ask for matrices (if possible) with specific properties. 15 16 17 18 20 21 22 23 24 25 26 Construct a matrix for which N(A) = all combinations of (2,2,1,0) and (3.1,0.1). Construct A so that N(A) = all multiples of (4, 3,2, 1). Its rank is Construct a matrix whose column space contains (1,1,5) and (0. 3.1) and whose nullspace contains (1,1, 2). Construct a matrix whose column space contains (1,1.0) and (0, 1,1) and whose nullspace contains (1,0, 1). Construct a 2 by 2 matrix whose nullspace equals its column space. This is possible. Why does no 3 by 3 matrix have a nullspace that equals its column space? If AB = 0 then the column space of B is contained in the of A. Why? The reduced form R of a 3 by 3 matrix with randomly chosen entries 1s almost sure to be . What R is virtually certain if the random A is 4 by 3? If N(A) = all multiples of z = (2,1,0, 1), what is R and what is its rank? If the special solutions to Rz = 0 are in the columns of these nullspace matrices [V, go backward to find the nonzero rows of the reduced matrices R: g 3\" - - - N=1]10 and N=|0 and N = (empty 3 by 1). .O 1. le N - (a) What are the five 2 by 2 reduced matrices R whose entnies are all 0’s and 1's? (b) What are the eight 1 by 3 matrices containing only 0’s and 1’s? Are all eight of them reduced echelon matrices R ? If A is 4 by 4 and invertible, describe the nullspace of the 4 by 8 matrix B = [A A]. Explain why A and — A always have the same reduced echelon form R. 102 Chapter 3. The Four Fundamental Subspaces 27 How is the nullspace N(C) related to the spaces N(A) and N(B), if C' = [ g ] ? 28 Find the reduced Ry and R for each of these matrices : 0 00 A A A A R B ] IR 29 Suppose the 2 pivot variables come last instead of first. Describe the reduced matrix R (3 columns) and the nullspace matrix N containing the special solutions. 30 If A has r pivot columns, how do you know that AT has r pivot columns? Give a 3 by 3 example with different pivot column numbers for A and AT 31 Fill out these matrices so that they have rank 1 : a b c] 9 ] 4 b A=|d and B=1|1 and Mz[c ] g ] L2 6 -3 32 If Ais arank one matrix, the second row of R is . Do an example. 33 If A has rank r, then it has an r by r submatrix S that is invertible. Remove m — r rows and n — r columns to find an invertible submatrix S inside A, B, and C. You could keep the pivot rows and pivot columns : 1 2 3 1 2 3 A B PR o O O 1 O 0 0]. 0 1 34 Suppose A and B have the same reduced row echelon form R. (a) Show that A and B have the same nullspace and the same row space. (b) We know E}A = Rand E>B = R. So A equals an matrix times B. 35 Kirchhoff's Current Law ATy = 0 says that current in = current out at every node. At node 1 this is y3 = y; + y4 (arrows show the positive direction of currents). Reduce AT to R (3 rows) and find three special solutions in the nullspace of AT. \" 14 > .2 Ya Ys _ . 1 0 1 -1 0 0 1 -1 0 0 -1 0 T _ Usy: Y2 A'=1 9 1 -1 0o 0 1 0 0 0 1 1 1. 3.2. Computing the Nullspace by Elimination: A=CR 103 36 37 38 39 40 41 42 43 45 46 C contains the r pivot columns of A. Find the r pivot columns of CT (r by m). Transpose back to find an r by 7 invertible submatrix S inside A: For A = find C (3 by 2) and then the invertible S (2 by 2). 2 4 6 2 4 7, Why is the column space C(AB) a subspace of C(A) ? Then rank(AB) < rank(A). Suppose column j of B is a combination of previous columns of B. Show that column j of AB is the same combination of previous columns of AB. Then AB cannot have new pivot columns, so rank(AB) < rank(B). (Important) Suppose A and B are n by n matrices, and AB = I. Prove from rank(AB) < rank(A) that the rank of A is n. So A is invertible and B must be its inverse. Therefore BA = I (which is not so obvious!). If Ais2by3and Bis3by2and AB = I, show from its rank that BA # I. Give an example of A and B with AB = I. For m < n, aright inverse is not a left inverse. What is the nullspace matrix N (containing the special solutions) for A, B.C'? I 1 0 0 Suppose A is an m by n matrix of rank r. Its reduced echelon form (including any zero rows) i1s Ry. Describe exactly the matrix Z (its shape and all its entries) that comes from transposing the reduced row echelon form of R} : Z = (rref(AT))T. 2by2blocks A=[I I] and Bz[ ] and C=[I I I]. (Recommended) Suppose Ry = [(I) g] is m by n of rank r. Pivot columns first: (a) What are the shapes of those four blocks, based on m and n and r ? (b) Find a right inverse B with RygB = I if r = m. The zero blocks are gone. (c) Find a left inverse C with CRy = I if r = n. The F and 0 column is gone. (d) What is the reduced row echelon form of R;,r (with shapes)? (e) What is the reduced row echelon form of Rl Ry (with shapes)? Suppose you allow elementary column operations on A as well as elementary row operations (which get to Rp). What is the “row-and-column reduced form™ for an m by n matrix A of rank 7 ? Find the factorizations A = CR = CW 1B for 1 4 7] A= 2 5 8 '3 6 9 What multiple of block row 1 will equal block row 2 of this matrix ? [w][w-l][w H]_ [w H J J JW-IH 104 Chapter 3. The Four Fundamental Subspaces 3.3 The Complete Solutionto Ax = b ﬁ Complete solutionto Az = b: x = (one particular solution ,) + (any x,, in the nullspace). 2 Elimination on Az = b leads to Ryx = d: Solvable when zero rows of R, have zeros in d. 3 When Ryz = d s solvable, one very particular solution &, has all free variables equal to zero. 4 A has full column rank r = n when its nullspace N(A) = zero vector: no free variables. {A has full row rank r = m when its column space C(A) is R™ : Az = b is always solvaty The last section totally solved Az = 0. Elimination converted the problem to Rox = 0. The free vanables were given special values (one and zero). Then the pivot variables were found by back substitution. We paid no attention to the right side b because it stayed at zero. Then zero rows in Ry were no problem. Now b is not zero. Row operations on the left side must act also on the right side. Az = b is reduced to a simpler system Rox = d with the same solutions (if any). One way to organize that is to add b as an extra column of the matrix. 1 will “augment” A with the right side (b;, b3, b3) = (1,6, 7) to produce the augmented matrix [A b]: : AR 130 2]|7t [1] hasthe 1302 1 00 1 4| =|6 augmented (0 0 1 4 6|=[A b]. 131 6/({° |7] maix 1 3167 NEA ] . When we apply the usual elimination steps to A, reaching Ry, b turns into d. In this example we subtract row 1 and row 2 from row 3. Elimination produces a row of zeros in Ry, and it changes b = (1,6, 7) to a new right side d = (1,6,0) : 3 2] 1 3 0 2 . ‘1] has the 1 3 0 2 1] 001 4\" =6]| augmented |0 0 1 4 6| =[Ro dJ. LOOOOL;' 0) marix {|0 0 0 0 O - 4J - - That very last row is crucial. The third equation has become 0 = 0. So the equations can be solved. In the original matrix A, the first row plus the second row equals the third row. To solve Az = b, we need b; + b2 = b3 on the right side too. The all-important property of bwas 1+ 6 = 7. That led to 0 = 0 in the third equation. This was essential. Here are the same augmented matrices for a general b = (b;, b2, b3) : 13020b] [1302 b 1 ([Ab]=(0 01 4 b —|[00 1 4 b = [Ro d] 1316 b] (000 0 b3—b—by] Now we get 0 = 0 in the third equation only if b3 — by — bz = 0. This is by + by = bs. 3.3. The Complete Solutionto Ax = b 105 One Particular Solution Az, = b For an easy solution &,,, choose the free variables to be zero: o = x4 = 0. Then the two nonzero equations give the two pivot variables £, = 1 and 3 = 6. Our particular solution to Axz = b (and also Rox = d) is ¢, = (1,0,6,0). This particular solution is my favorite: free variables = zero, pivot variables from d. For a solution to exist, zero rows in Ry must also be zero in d. Since I is in the pivot rows and pivot columns of Ry, the pivot variables in Tparticular €OMe fromd: ( ) Pivot variables 1,6 =d Free variables 0, 0 Tparticular = (1,0,6,0). | O O 1 Roxy = | O 0 O OO - - Notice how we choose the free variables (as zero) and then solve for the pivot vanables. After the row reduction to Ry, those steps are quick. When the free vanables are zero, the pivot variables for x, are already seen in the right side vector d. One Tparticular The particular solution solves Az, =b All = The n — r special solutions solve Az, =0 nullspace That particular solution is (1,0,6.0). The 4 — 2 special (nullspace) solutions came in Section 3.3 from the two free columns of Ry, by reversing signs of 3.2. and 4. Please notice how the complete solution x, + T, to Ax = bincludes all x, : - - Complete solution 1 -3 —2 one r,+many T, _ _ 10 1 0 particular z, T=TpFIn =g TT2| o ¥T4_ nullspace vectors x, 0] 0] 1) Question Suppose A is a square invertible matrix, m = n = r. What are £, and x,,? Answer The particular solution is the one and only solution x, = A~'b. There are no special solutions or free variables. Ry = I has no zero rows. The only vector in the nullspace is £, = 0. The complete solutionis € = x, + *, = A\"'b + 0. We didn’t mention the nullspace in Chapter 2, because A was invertible. 4 was reduced all the way to 1. [A b] went to [I A‘lb]. Then Az = bbecamex = A~'b which is d. This is a special case here, but square invertible matrices are the best. So they got their own chapter before this one. For small examples we can reduce [A b| to [Ry d]. For a large matrix, MATLAB does it better. One particular solution (not necessarily ours) is £ = A\\b from backslash. Here is an example with full column rank. Both columns have pivots. Example 1 Find the condition on (b, b2, b3) for Ax = b to be solvable, if 1 1 by A= 1 2| and b= b2 . -2 -3 bs. 106 Chapter 3. The Four Fundamental Subspaces Solution Elimination uses the augmented matrix [A b] with its extra column b, Subtractrow 1 of [ A b] from row 2. Then add 2 times row 1 to row 3 to reach | Ry d|: - 1 1] 1 16 ] [1 0 2-b 1 2 b2 - |0 1 bz-bl =10 1 b2—b1 = [R() d] L—? -3 b3‘ LO -1 b3+ 2b1_ L0 0 bs+b; + bz_ Equation 3 is 0 = 0 provided b3 + b; + b, = 0. This is the condition to put b in the column space. Then Az = b will be solvable. The rows of A add to the zero row. So for consistency (these are equations') the entries of b must also add to zero. This example has no free vanables since n — r = 2 — 2. Therefore no special solutions. The nullspace solution is x, = 0. The particular solution to Az = b and Rox = d is at the top of the final column d: _ : _ _ _12b1 = by 0 r=n OnesolutiontoAz=b z=z,+z,= [b2 _ b, } + [0] : If b3 + by + by is not zero, there is no solution to AT = b (x, and = don’t exist). This example is typical of an extremely important case: A has full column rank. Every column has a pivot. The rank is r = n. The matrix is tall and thin (m > n). Row reduction puts R = I at the top, when A is reduced to Ry with rank n : (1) Full columnrankr =n Ry = [I] — [\" by n identity matnx] 0 m — n rows of zeros This matrix A has independent columns. The nullspace of A is Z = {zero vector}. We will collect together the different ways of recognizing this type of matrix. Every matrix A with full column rank (r = n) has all these properties : 1. All columns of A are pivot columns (independent). No free variables. 2. The nullspace N(A) contains only the zero vector = 0. 3. If Az = b has a solution (it might not) then it has only one solution. Az = 0 only happens when £ = 0. Later we will add one more fact to the list above: The square matrix AT A is invertible when the rank of A is n. A may have many rows. In this case the nullspace of A has shrunk to the zero vector. The solution to Az = bis unique (if there is a solution). There will be m — n zero rows in Rg. So there are m — n conditions on b in order to have 0 = 0 in those rows, and b in the column space. With full column rank, Az = b will have one solution or no solution. 3.3. The Complete Solutionto Az = b 107 Full Row Rank and the Complete Solution The other extreme case is full row rank. Now Ax = b has one or infinitely many solutions. In this case A must be short and wide (m < n). A matrix has full row rank if r = m. “The rows are independent.” Every row has a pivot, and here is an example. Example 2 This system Ax = b has n = 3 unknowns but only m = 2 equations: r+ y+z2=3 T+ —z=4 (rank r = m = 2) Full row rank These are two planes in zyz space. The planes are not parallel so they intersect in a line. This line of solutions is exactly what elimination will find. The particular solution will be one point on the line. Adding the nullspace vectors ., will move us along the line in Figure 3.1. Then x = x,+ all T, gives the whole line of solutions. m:’-xp‘\\-wn ytions ATn — Line of so! Figure 3.1: Complete solution = one particular solution x,+ all nullspace solutions T .. We find @, and z,, by elimination downwards and then upwards on [ A b] : 1 1 1 3 1 1 1 3 1 0 3 2 [1 2 -1 4]_’[0 1 -2 1]_’[0 1 -2 1]‘[R d]. The particular solution (2,1,0) has free variable x3 = 0. It comes directly from d. The special solution 8 has 3 = 1. Then —x, and —x3 come from the free column of R. It is wise to check that x, and 8 satisfy the original equations Az, = band As = 0: 2+1 = 3 -3+2+1 = 0 2+2 = 4 -3+4-1 = 0 The nullspace solution x,, is any multiple of 8. It moves along the line of solutions, starting at Tparticular- Please notice again how to write the answer : Complete solution N ~ f -3 Particular 4 nullspace T=Tp+&Tn= ) + I3 ; Y | 4] This line of solutions is drawn in Figure 3.1. Any point on the line could have been chosen as the particular solution. We chose x,, as the point with z3 = 0. The particular solution is not multiplied by an arbitrary constant ! 108 Chapter 3. The Four Fundamental Subspaces Every matrix A with full row rank (r=m) has all these properties : 1. All rows have pivots, and Ry has no zero rows: Ry = R. 2. Az = b has a solution for every right side b. 3. The column space of A is the whole space R™. 4. If m < n the equation Ax = b is underdetermined (many solutions). In this case with m pivots, the m rows are linearly independent. So the columns of AT are linearly independent. The nullspace of AT contains only the zero vector. The four possibilities for linear equations depend on the rank r r=m and r=mn Squareandinvertible Ax =b has 1 solution r=m and r <n Shortand wide Ax = b has oc solutions r<m and r=n Tallandthin Ax = b has 0or 1 solution r<m and r<n Notfull rank Ax = b has 0 or o solutions The reduced R, will fall in the same category as the matrix A. In case the pivot columns happen to come first, we can display these four possibilities. For Rox = d and Az = b to be solvable, d must end in m — r zeros. F is the free part of Ry. I I F Four types for R [I] (I F| [Ol [O O] Their ranks r=m=n r=m<n r=n<m r<mr<n Cases 1 and 2 have full row rank r = m. Cases 1 aind 3 have full column rank r = n. To end this important section of the book, here is a note about computational linear algebra. Linear equations Az = b are obviously fundamental. In practice, the steps of elimination are reordered for the sake of speed and numerical stability. We can solve systems of order 1000 on a laptop (allowing roundoff errors in single precision or double precision). Supercomputers can solve much larger systems. But there is a limit on the matrix size. What to do beyond that limit ? The surprising answer is randomized linear algebra. We sample the columns of A. We accept the errors involved. In practice matrices are not completely random, and the final results are remarkably good. Often the approximation to A is expressed in the 3-factor form A ~ CM R. C comes from sampling the columns of A and R comes from the rows of A. The smaller mixing matrix M (sometimes called U) is constructed as we go. With high probability, the approximate solution iIs surprisingly accurate. Linear algebra is alive. The demands of computation (speed and accuracy) lead to new ideas. The same will be true for eigenvalues and singular values—later in this book. 3.3. The Complete Solutionto Az = b 109 ® WORKED EXAMPLES = 3.3 A This question connects elimination (pivot columns and back substitution) to column space-nullspace-rank-solvability (the higher level picture). A has rank 2: NE PN~ 6. T, + 2z + 33+ 54 = by 1 2 3 5 ] Ax =b is 2z +4xz9+ 8x3+ 1214 = b, A=1]12 4 8 12 3z, +6x2+ 7Tx3+ 1324 = b3 _3 6 7 13- Reduce [A b]to[U c], sothat Az = b becomes a triangular system Uz = c. Find the condition on b;, by, b3 for Ax = b to have a solution. Describe the column space of A. Which plane in R3? Describe the nullspace of A. Which special solutions in R4 2 Reduce [U c]to[Rp d]: Special solutions from Ry, particular solution from d. Find a particular solution to Az = (0,6, —6) and then the complete solution. Solution 1. The multipliers in elimination are 2 and 3 and —1. They take [A b]into [U c]. 23 5 b (1 2 3 5|Db; | (12351)1 4 812b|—|0 O 2 2 b2—2b1 —10 0 2 2| bs—-2b, 6 713 b3J _O 0 -2 -2 b3—3blj _0 0 0 0| bg+by—5b; . The last equation shows the solvability condition b3 + by — 5b; = 0. Then 0 = 0. . First description : The column space is the plane containing all combinations of the pivot columns (1,2, 3) and (3,8, 7). The pivots are in columns 1 and 3. Second description: The column space contains all vectors with b3 + b2 — 5b; = 0. That makes Az = b solvable, so b is in the column space. All columns of A pass this test bz + by — 5b; = 0. This is the equation for the plane in the first description ! . The special solutions have free variables 2 = 1,z4 =0andthenz; = 0,74 = 1: Special solutions to Az = 0 [ -2 ] i —3 1 Back substitutionin Uz =0 8 = 0 s2=| _, or change signs of 2,2,1 in R 0 1 The nullspace N(A) in R* contains all z,, = ¢;8; + c282. In the reduced form Ry, the third column changes from (3,2,0) in U to (0.1.0). The right side ¢ = (0, 6,0) becomes d = (-9, 3,0) showing —9 and 3 in x;: 1 2350 1 2 0 2 -9] (Ue¢]=]0 02 2 6| —[Rd|=]0011 3 0 000U O 0 00 0 O] x, = (—9,0,3,0) is the very particular solution with free variables = zero. The complete solution to Az = (0,6, —6) is = *, + Tn = Tp + €181 + C282. 110 Chapter 3. The Four Fundamental Subspaces 3.3B Suppose you have this information about the solutions to Ax = b for a specific b. What does that tell you about m and n and r (and A itself)? And possibly about b. 1. There is exactly one solution. 2. All solutions to Az = bhave theformz = [3] + ¢[}]. 3. There are no solutions. . . 4. All solutions to Az = b have the form ¢ = [(1)] +c [(1)] Solution In case 1, with exactly one solution, A must have full column rank r = n. The nullspace of A contains only the zero vector. And b is in the column space. In case 2, A must have n = 2 columns (and m is arbitrary). With [ } ] in the nullspace of A, column 2 is the negative of column 1. Also A # 0: the rank is 1. Withz = [?] as a solution, b = 2(column 1) + (column 2). My choice for £, would be (1, 0). In case 3 we only know that b is not in the column space of A. The rank of A must be less than m. I guess we know b # 0, otherwise £ = 0 would be a solution. In case 4, A must have n = 3 columns. With (1,0, 1) in the nullspace of A, column 3 of Ais —(column 1). Therankis 3 —1 = 2 and b is column 1 + column 2. 3.3C Find the complete solution £ = Tp + Tn by forward elimination on [A b]: Solution Elimination leads us to Ryz = d 1 210 4] 1 210 4 (120—4 7 2 4 48 2}|—>|0028 -6|—1]10D01 4 -3|. 4 8 6 8 10 (0000 O] 000 0 O] For the nullspace part xy, with b = 0, set the free variables r2,z4to 1,0 and also 0, 1: Special solutions 8;=(-2,1,0,0) and 82=(4,0,-4,1) Particular z,=(7,0,-3,0) Then the complete solutionto Az = b is Tcomplete = Tp + €181 + C282. The rows of A produced the zero row from 2(row 1) + (row 2) — (row 3) = (0, 0, 0, 0). The same combination for b = (4, 2, 10) gives 2(4) + (2) — (10) = 0. If a combination of the rows of A (on the left side) gives the zero row, then the same combination must give zero on the right side. Of course! Otherwise no solution. We could say this again in different words: If every column of A is perpendicular to ¥ = (2,1, —1), then any combination b of those columns must also be perpendicular to y. Otherwise b is not in the column space and Az = b is not solvable. And again: If y is in the nullspace of AT then y must be perpendicular to every b in the column space of A. 3.3. The Complete Solution to Az = b 111 Problem Set 3.3 1 (Recommended) Execute the six steps of Worked Example 3.3 A to describe the column space and nullspace of A and the complete solution to Az = b: (2 4 6 4 by ] [4] A=\\|2 5 7 6 b=1b | =13 2 35 2 b3 | [ 5 Carry out the same six steps for this matrix A with rank one. You will find two conditions on by, b,, b3 for Ax = b to be solvable. Together these two conditions put b into the space (two planes give a line): 1 ] 2 1 3 [ by | 10 | a=|3| 2131 _|g 39 b= | b, =(30 2 4 2 6 | b3 | | 20 | Questions 3-15 are about the solution of Ax = b. Follow the steps in the text to x, and x,,. Start from the augmented matrix with last column b. 3 Write the complete solution as T, plus any multiple of 8 in the nullspace : _ r+3y+3z2=1 Zi:gyzlz 2r +6y+92=5 y= —r—-3y+32=5 Find the complete solution £ = &,+ any T, (also called the general solution) to ﬁ f'l‘ 1 e - ~ @8 Under what conditions on b, , bs, b3 are these systems solvable ? Include b as a fourth column in elimination. Find all solutions when that condition holds: T+2y—2z=b 2r + 2z2=0b, 2z + 5y — 42 =b, 4 + 4y =by 4z 4+ 9y — 8z =b3 8T + 8y =b3 What conditions on b, b2, b3, by make each system solvable? Find x in that case: 1 2] by (1 2 3] . 1 [b] 2 4| [z b, 2 4 6| [T _ |b 25[:52_63 2 5 7| |27 [bs 3 9. ba 3 9 12| LB |, 112 14 15 Chapter 3. The Four Fundamental Subspaces Show by elimination that (b;, b2, b3) is in the column space if b3 — 2b2 + 4b; = (. The rank is # = 2. What combination of the rows of A gives the zero row ? B 9 1 2. 0] Which vectors (b;. by. b3) are in the column space of A? Which combinations of the rows of A give zero? 1 2 1] 1 6 3 (b) 1 2 5 (a) A= A= 1 1] 2 4. 4 8 2 bO I-2 Find the complete solution in the form &, + &, to these full rank systems : r+y+z2=4 +y+z=4 (@ @ rryts O rytz=4 Construct a 2 by 3 system Az = b with particular solution &, = (2.4,0) and homogeneous solution z,, = any multiple of (1,1, 1). Why can’ta 1 by 3 system have z, = (2,4,0) and ,, = any multiple of (1,1,1)? (a) If Az = b has two solutions z, and z,, find two solutions to Ax = 0. (b) Then find another solution to Az = 0 and another solution to Ax = b. Explain why these are all false: (a) The complete solution is any linear combination of &, and x,,. (b) A system Az = b has at most one particular solution. This is true if A is (c) The solution z, with all free variables zero is the shortest solution (minimum length ||z]|). Find a 2 by 2 counterexample. (d) If Ais invertible there is no solution x,, in the nullspace. Suppose column 5 of U has no pivot. Then z5 is a variable. The zero vector (is) (is not) the only solution to Az = 0. If Az = b has a solution, then it has solutions. Suppose row 3 of U has no pivot. Then that row is . The equation Uz = € is only solvable provided . The equation Az = b (is) (is not) (might not be) solvable. 3.3. The Complete Solution to Az = b 16 17 18 19 20 21 22 113 The largest possible rank of a 3 by S matrix is . Then there is a pivot in every of U and R. The solution to Ax = b (always exists) (is unique). The column space of A is . An example is A = The largest possible rank of a 6 by 4 matrix is . Then there is a pivot in every of U and R. The solution to Ax = b (always exists) (is unique). The nullspace of A is . An exampleis A = Find by elimination the rank of A and also the rank of AT: p— - 1 4 0 1 0 1] A= 2 11 5| and A= |1 1 2| (rank dependson q). -1 2 10| 1 1 q] If Az = b has infinitely many solutions, why is it impossible for Ax = B (new right side) to have only one solution? Could Az = B have no solution ? Choose the number q so that (if possible) the ranks are (a) 1 (b)2 (c) 3: 6 4 2 -3 -2 -1 9 6 ¢ A= and Bz[3 I 3]. q 2 ¢ Give examples of matrices A for which the number of solutions to Ax = b is (a) Oor 1, depending on b (b) oc, regardless of b (c) 0 or oo, depending on b (d) 1, regardless of b. Write down all known relations between r and m and n if Az = b has (a) no solution for some b (b) one solution for some b, no solution for other b (c) infinitely many solutions for every b (d) exactly one solution for every b. Questions 23-27 are about the reduced echelon matrices Ry and R. 23 24 25 Divide rows by pivots. Then produce zeros above those pivots to reach Ry and R. 2 4 4 2 4 4] 00 4 U=10 3 6 and U=10 3 6 and U=[O 10]. 0 0 0 0 0 5 If A is a triangular matrix, when is Ry = rref(A) equal to I ? Apply elimination to Uz = 0 and Uz = ¢. Reach Ryx = 0 and Ryx = d: vol=[s 5 %o m wel=p i Solve Ryx = 0 to find x,, with z2 = 1. Solve Rox = d to find -, with 2 = 0. 114 Chapter 3. The Four Fundamental Subspaces 26 ReduceUz =0and Uz = cto Ryz = 0 and Ryx = d. What are the solutions to Rox =d? [ 1 [3 6 0 3 U 0]=10 0 0] and |U c| =10 0 2 0 0 - L o - L J - - 27 Reduce to Uz = c (Gaussian elimination) and then Rox = d. Find a particular solution &, and all homogeneous solutions Z,. 10 2 3 i‘ \" 9] Az =11 3 2 0 I2= 5| =b 2 0 4 9|7 10 --14- . |0 - 28 Find matrices A and B with the given property or explain why you can’t: \" 1 “ . [ (a) Theonlysolutionof Az =] 2 |isz = (1) : I 3 | L J 0 C 1 ] (b) The only solution of Bx = 1 isz=| 2 i 3 | 29 Find the LU factorization of A and all solutionsto Az = b :; 1 3 1] (1] (1 1 2 3 3 0 A= 2 4 6 and b= 6 andthen b = 0 115 J 0] 30 The complete solution to Az = [ ; ] ST = [ (1) ] +c[ (1) ] Find A. 31 (Recommended') Suppose you know that the 3 by 4 matrix A has the vector 8 = (2, 3,1,0) as the only special solution to Az = 0. (a) What is the rank of A and the complete solution to Az = 07? (b) What is the exact row reduced echelon form Ry of A? (c) How do you know that Az = b can be solved for all b? 32 Suppose Az = band Cz = b have the same (complete) solutions for every b. Is it true that A equals C ? 33 Describe the column space of a reduced row echelon matrix Ry with rank r. Removing any zero rows, describe the column space of R. 115 3.4. Independence, Basis, and Dimension 3.4 Independence, Basis, and Dimension ﬁidependent vectors : The only zero combination c;v; + - - +cxvx = 0Ohasall c’s =R 2 The vectors vy, ..., v, span the space S if S = all combinations of the v’s. 3 The vectors vy, ..., v are a basis for S if (1) they are independent and (2) they span S. {The dimension of a vector space S is the number k of vectors in every basis for S. / This important section is about the true size of a subspace. There are n columns in an m by n matrix. But the true “dimension” of the column space is not necessarily n. The dimension of C(A) is measured by counting independent columns. We will see again that the true dimension of the column space is the rank r. This is partly review. The idea of independence applies to any vectors vy, ..., v, In any vector space. Most of this section concentrates on the subspaces that we know and use—especially the column space and the nullspace of A. In the last part we also study “vectors” that are not column vectors. They can be matrices and functions; they can be linearly independent or dependent. First come the key examples using column vectors. The goal is to understand a basis : independent vectors that “‘span the space’. Every vector in the space is a unique combination of the basis vectors. We are at the heart of our subject, and we cannot go on without a basis. The four essential ideas in this section (with first hints at their meaning) are : 1. Independent vectors (no extra vectors) 2. Spanning a space (enough vectors to produce the rest) 3. Basis for a space (not too many and not too few) 4. Dimension of a space (the number of vectors in every basis) Linear Independence Our first definition of independence is not so conventional, but you are ready for it. DEFINITION The columns of A are linearly independent when the only solution to Ax = 0is * = 0. No other combination Ax of the columns gives the zero vector. The columns are independent when the nullspace N(A) contains only the zero vector. Let me illustrate linear independence (and dependence) with three vectors in R®. 116 Chapter 3. The Four Fundamental Subspaces 1. If three vectors in R are nor in the same plane, those vectors are independent. No combination of v;, vz, v3 in Figure 3.2 gives zero except Ov; + Ov; + Ovs. 2. If three vectors w), w5, w3 are in the same plane in R3, they are dependent. 4 V1 In a plane Not in a plane T‘ V2 0 » W3 A ,I ’ ’ v3.. Koo wo w) Figure 3.2: Independent : Only Ov, +0v;+0v3 gives 0. Dependent : w, —w,+w3 = 0. This idea of independence applies to 7 vectors in 12-dimensional space. If they are the columns of A, and independent, the nullspace of the 12 by 7 matrix only contains * = 0. None of the column vectors is a combination of the other six vectors. Now we choose different words to express the same idea in any vector space. DEFINITION The sequence of vectors v,,...,v, is linearly independent if the only combination that gives the zero vector is Ov; + Qv + - - - + Ov,,. Linear independence I)v) + T2v2 + - - -+ ITov, = 0 only happens when all z’s are zero. (D If a combination gives 0, when the z’s are not all zero, the vectors are dependent. Correct language: “The sequence of vectors is linearly independent.” Acceptable shortcut: “The vectors are independent.” Unacceptable: “The matrix is independent.” A sequence of vectors is either dependent or independent. They can be combined to give the zero vector (with nonzero z’s) or they can’t. So the key question is: Which com- binations of the vectors give zero? We begin with some small examples in R: (a) The vectors (1,0) and (1,0.00001) are independent. (b) The vectors (1,1) and (-1, —1) are dependent. (c) The vectors (1,1) and (0, 0) are dependent because of the zero vector. (d) In R2, any three vectors (a, b) and (c, d) and (e, f) are dependent. Dependent columns _[o _ ~ z # 0 in the nullspace [ ][ ]-[0] forzy =1and 2 = 1. Three vectors in R? cannot be independent! One way to see this : the matrix A with those three columns must have a free variable and then a special solution to Az = 0. Now move to three vectors in R3. If one of them is a multiple of another one, these vectors are dependent. But the complete test involves all three vectors at once. We put them in a matrix and try to solve Az = 0. 3.4. Independence, Basis, and Dimension 117 Example 1 The columns of this A are dependent. Az = 0 has a nonzero solution : 1 0 3] [-3] 1 0 3] [0 Az =12 1 5 1 is =312l +11|11+1]|5|=10]. 10 3[[1 1] o] 3] |o The rank is only r = 2. Independent columns produce full column rank r = n = 3. For a square matrix, dependent columns imply dependent rows and vice versa. Question How to find that solution to Ax = 0? The systematic way is elimination. 0 3 1 5| reducessto Rg=|0 1 -1}. Thean[_g} andx = 1 0 3 1 Full column rank The columns of A are independent exactly when the rank is 7 = n. There are n pivots and no free variables and A = C. Only & = 0 is in the nullspace. One case is of special importance because it is clear from the start. Suppose seven columns have five components each (7m = 5 is less than n = 7). Then the columns must be dependent. Any seven vectors from R® are dependent. The rank of A cannot be larger than 5. There cannot be more than five pivots in five rows. Az = Q has atleast 7 — 5 = 2 free variables, so it has nonzero solutions—which means that the columns are dependent. Any set of n vectors in R™ must be linearly dependent if n > m. This type of matrix has more columns than rows—it is short and wide. The columns are certainly dependent if n > m, because Az = 0 has a nonzero solution. The columns might be dependent or might be independent if n < m. Elimination will reveal the r pivot columns. It is those T pivot columns that are independent in C. Note Another way to describe linear dependence is this: “One vector is a combination of the other vectors.” That sounds clear. Why don’t we say this from the start? Our definition was longer: “Some combination gives the zero vector, other than the trivial combination with every x = 0.” We must rule out the easy way to get the zero vector. The point is, our definition doesn’t pick out one particular vector as guilty. All columns of A are treated the same. We look at Az = 0, and it has a nonzero solution or it hasn’t. 118 Chapter 3. The Four Fundamental Subspaces Vectors that Span a Subspace The first subspace in this book was the column space. Starting with columns v,, ..., v, C(A) includes all combinations ,v; + --- + T,v,. The column space consists of all combinations Az of the columns. The single word “span” describes C(A). The columns of a matrix span its column space. They might be dependent. Example 2 Describe the column space and the row space of A. 1 4 m=3 ,_ 19 7| amdaT=|! 2 3| n =2 3 5 4 7 5 The column space of A is the plane in R3 spanned by the two columns of A. The row space of A is spanned by the three rows of A (which are columns of AT). This row space is all of R%. Remember: The rows are in R\" spanning the row space. The columns are in R™ spanning the column space. Same numbers, different vectors, different spaces. A Basis for a Vector Space Two vectors can’t span all of R?, even if they are independent. Four vectors can’t be independent, even if they span R3. We want enough independent vectors to span the space (and not more). A “basis” is just right. DEFINITION A basis for a vector space is a sequence of vectors with two properties: The basis vectors are linearly independent and they span the space. This combination of properties is fundamental to linear algebra. Every vector v in the space is a combination of the basis vectors, because they span the space. More than that, the com- bination that produces v is unique, because the basis vectors v, . .., v, are independent: There is one and only one way to write v as a combination of the basis vectors. Reason: Suppose v = a;v; +- - -+an v, and also v = byv; +- - -+ b, v,,. By subtraction (a1 = b1)vy + -+ + (an — bn)vy, is the zero vector. From the independence of the v’s, each a; — b; = 0. Hence a; = b;, and there are not two ways to produce v. Example 3 The columns of I = [ (1) (l)] produce the “standard basis” for R The basis vectors & = [(1)] and j = [O 1] are independent. They span R%. Everybody thinks of this basis first. The vector 4 goes across and j goes straight up. The columns of the n by n identity matrix give the “standard basis” for R\". 3.4. Independence, Basis, and Dimension 119 Now we find many other bases (infinitely many). The basis is not unique! Example4 (Important) The columns of every invertible n by n matrix give a basis for R\" : Invertible matrix 1 0 0 Singular matrix 1 0 1\" Independentcolumns A= |1 1 0 Dependentcolumns B = {1 1 2}. Column space = R3 1 1 1] Column space # R? {_1 1 2 The only solution to Az = 0is £ = A~'0 = 0. The columns are independent. They span the whole space R\"—because every vector b is a combination of the columns. Ax = b can always be solved by £ = A~!b. Do you see how everything comes together for invertible matrices? Here it is in one sentence : The vectors v, ..., v, are a basis for R™ exactly when they are the columns of an n by n invertible matrix. R\" has infinitely many different bases. When the columns are dependent, we keep only the pivot columns—the first two columns of B above. Those two columns are independent and they span the column space. Every set of independent vectors can be extended to a basis. Every spanning set of vectors can be reduced to a basis. Example 5 This matrix is not invertible. Its columns are not a basis for anything ! One pivot column 12 4 12 One pivot row (r = 1) A= [3 6] reduces to Ko = [O O] ' Example 6 Find bases for the column and row spaces of this rank two matrix : 1 2 0 3] Ro=1(0 0 1 4}. 0 0 0 0] Columns 1 and 3 are the pivot columns. They are a basis for the column space of Ry. The column space is the “zy plane” inside ryz space R>. That plane is not R, itis a subspace of R®. Columns 2 and 3 are also a basis for the same column space. Which pair of columns of Ry is not a basis for its column space ? The row space is a subspace of R*. The simplest basis for that row space is the two nonzero rows of Ry. The zero vector is never in a basis. Question Given five vectors in R’, how do you find a basis for the space they span? First answer Make them the rows of A, and eliminate to find the nonzero rows in R. Second answer Put the five vectors into the columns of A. Eliminate to find the pivot columns. Those pivot columns in C are a basis for the column space. Could another basis have more vectors, or fewer? This is a crucial question with a good answer : No. All bases for a vector space contain the same number of vectors. The number of vectors in any and every basis is the “dimension” of the space. 120 Chapter 3. The Four Fundamental Subspaces Dimension of a Vector Space We have to prove what was just stated. There are many choices for the basis vectors, but the number of basis vectors doesn’t change. If v;....,v, and w,,..., w, are both bases for the same vector space, then m = n. Proof Suppose that there are more w’s than v’s. From n > m we want to reach a contradiction. The v’s are a basis, so each w must be a combination of the v’s. If w, equals a; vy + - - - + @1 U, this is the first column of a matrix multiplication VA = W : - - Eachwisa [ a A1n combination W= |w, wy ... w,| =|v; ... U : : = V A. ’ of the v’s I ] 1 J Lam mn | We don’t know each a,;, but we know the shape of A (it is m by n). The second vector w-, is also a combination of the v’s. The coefficients in that combination fill the second column of A. The key is that A has a row for every v and a column for every w. Aisa short wide matrix, since we assumed n > m. So Ax = 0 has a nonzero solution. Az = 0 gives VAz = 0 which is Wz = 0. A combination of the w's gives zero! Then the w'’s could not be a basis—our assumption n > m is not possible for two bases. If m > n we exchange the v’s and w’s and repeat the same steps. The only way to avoid a contradiction is to have m = n. This completes the proof that m = n. The number of basis vectors is the dimension. So the dimension of R™ is n. We now define the important word dimension. DEFINITION The dimension of a space is the number of vectors in every basis. The dimension matches our intuition. The line through v = (1, 5,2) has dimension one. It is a subspace with this one vector v in its basis. Perpendicular to that line is the plane £ + 5y + 2z = 0. This plane has dimension 2. To prove it, we find a basis (-5,1,0) and (-2, 0, 1). The dimension is 2 because the basis contains two vectors. The plane is the nullspace of the matrix A = [1 5 2 ] , which has two free variables. Our basis vectors (—5,1,0) and (-2,0,1) are the “special solutions” to Ax = O. The n — r special solutions always give a basis for the nullspace: dimension n — r. Note about the language of linear algebra We would never say “the rank of a space” or “the dimension of a basis” or “the basis of a matrix”. Those terms have no meaning. It is the dimension of the column space that equals the rank of the matrix. 3.4. Independence, Basis, and Dimension 121 Bases for Matrix Spaces and Function Spaces The words “independence” and “basis” and “dimension” are not limited to column vectors. We can ask whether three matrices A;, A;, A3 are independent. When they are 3 by 4 matrices, some combination might give the zero matrix. We can also ask the dimension of the full 3 by 4 matrix space. (That dimension is 12. Twelve matrices in every basis.) In differential equations, d%y/dz? = y has a space of solutions. One basis is y = €~ and y = e~ *. Counting the basis functions gives the dimension 2 for this solution space. (The dimension is 2 because the linear equation starts with the second derivative.) Matrix spaces The vector space M contains all 2 by 2 matrices. Its dimension 1s 4. One basisis A;, Ay, A3, A4 = [(1) 8] : [8 (1)] : [(1) 8] : [g (1)] : Those matrices are linearly independent. We are not looking at their columns, but at the whole matrix. Combinations of those four matrices can produce any matrix in M. Every A combines Cc1 Co A A Ay = = A the basis matrices c1Ay + c2Az + c3A3 + cadly [63 c4] A is zero only if the c’s are all zero—this proves independence of A;, A2, A3, As. The three matrices A, A;. A4 are a basis for a subspace—the upper tnangular matrices. Its dimension is 3. A; and A4 are a basis for the diagonal matnces. What is a basis for the symmetric matrices? Keep A; and Ay, and throw in A; + A3. The dimension of the whole n by n matrix space is n?. The dimension of the subspace of upper triangular matrices is %nz + %n. The dimension of the subspace of diagonal matrices is n. The dimension of the subspace of symmetric matrices is %n\"’ + -;-n (why ?). Function spaces The equations d?y/dz? = 0 and d’y/dz® = —y and d*y/dr* = y involve the second derivative. In calculus we solve to find the functions y(x): y” =0 issolved by any linear functiony = cz + d y\"”\" = —y is solved by any combination y = csinz + dcosz y” =y issolved by any combination y = ce* + de™*. That solution space for y”” = —y has two basis functions: sinz and cos x. The space for y\" = 0 has z and 1. It is the “nullspace” of the second derivative! The dimension is 2 in each case (these are second-order equations). The solutions of y” = 2 don’t form a subspace—the right side b = 2 is not zero. A particular solution is y(z) = z2. The complete solution is y(z) = z° + cx + d. All those functions satisfy y”” = 2. Notice the particular solution plus any function cx + d in the nullspace. A linear differential equation is like a linear matrix equation Ax = b. We end here with the space Z that contains only the zero vector. The dimension of this space is zero. The empty set (containing no vectors) is a basis for Z. We can never allow the zero vector into a basis, because then linear independence is lost. 122 Chapter 3. The Four Fundamental Subspaces The key words in this section were independence, span, basis, dimension. 1. The columns of A are independent if x = 0 is the only solution to Ax = 0. 2. The vectors vy, ..., v, span a space if their combinations fill that space. 3. A basis consists of linearly independent vectors that span the space. Every vector in the space is a unique combination of the basis vectors. 4. All bases for a space have the same number of vectors. This number of vectors in a basis is the dimension of the space. 5. The pivot columns are one basis for the column space. The dimension of C(A) is . 8 WORKED EXAMPLES = 3.4A (Important example) Supposevy,...,v, is abasis for R™ and the n by n matrix A is invertible. Show that Av,,..., Av, is also a basis for R™. Solution In matrix language: Put the basis vectors vy, ..., v, in the columns of an invertible (') matrix V. Then Av,, ..., Av, are the columns of AV. Since A is invertible, so is AV'. Its columns give a basis. In vector language: Suppose c;Avy + -+ + chAv, = 0. This is Av = 0 with v = c;v;+- - -+CnVp,. Multiply by A~! to reach v = 0. By linear independence of the v’s, all ¢; = 0. This shows that the Av’s are independent. To show that the Av’s span R™, solve ¢; Av; + - - - + ¢, Av, = b which is the same as c1v1 + -+ + cav, = A71b. Since the v’s are a basis, this must be solvable. 3.4 B Start with the vectors v; = (1,2,0) and v, = (2,3,0). (a) Are they linearly independent? (b) Are they a basis for any space V' ? (c) What is the dimension of V? (d) Which matrices A have V as their column space ? (¢) Which matrices have V as their nullspace? (f) Describe all vectors v3 that complete a basis v, v2, v3 for R3. Solution (a) v; and v, are independent—the only combination to give 0 is Ov, + Ova. (b) Yes, they are a basis for the space V that they span: All vectors (z,y,0). (c¢) The dimension of V is 2 since the basis contains two vectors. (d) This V is the column space of any 3 by n matrix A of rank 2, if every column is 8 combination of v; and v,. In particular A could just have columns v; and v2. (e) This V is the nullspace of any m by 3 matrix B of rank 1, if every row is a multiple of (0,0, 1). In particular take B = [0 0 1). Then Bv; = 0 and By, = 0. (f) Any third vector v3 = (a, b, ¢) will complete a basis for R3 provided c # 0. 3.4. Independence, Basis, and Dimension 123 3.4 C Start with three independent vectors w;, w2, w3. Take combinations of those vectors to produce v,, v2, v3. Write the combinations in matrix formas V = W B: v, = w; + w2 T 1 T 11 1 o0 v = w; + 2w+ w3z whichis |v; v v3| = |lw; w; w; 1 2 1 U3 = w2 + cwsy I | I JLO 1 ¢ What is the test on B to see if V = W B has independent columns? If ¢ # 1 show that v, v2, v3 are linearly independent. If ¢ = 1 show that the v’s are linearly dependent. Solution For independent columns, the nullspace of V' must contain only the zero vector. Vx = 0 requires = (0,0, 0). If ¢ = 1 in our problem, we can see dependence in two ways. First, v; + v3 will be the same as v3. Then v, — v2 + v3 = 0—which says that the v’s are dependent. The other way is to look at the nullspace of B. If ¢ = 1, the vector £ = (1. -1.1) is in that nullspace, and Bx = 0. Then certainly W Bx = 0 which is the same as Vx = 0. So the v’s are dependent: v; — v, + v3 = 0. Now suppose ¢ 7 1. Then the matrix B is invertible. So if x is any nonzero vector we know that Bz is nonzero. Since the w’s are given as independent, we further know that W Bz is nonzero. Since V = W B, this says that & is not in the nullspace of V. In other words v, v2, v3 are independent. The general rule is “independent v’s from independent w’s when B is invertible”. And if these vectors are in R3, they are not only independent—they are a basis for R3. “Basis of v’s from basis of w’s when the change of basis matrix B is invertible.” 124 Chapter 3. The Four Fundamental Subspaces Problem Set 3.4 Questions 1-10 are about linear independence and linear dependence. 1 Show that vy, v,. v3 are independent but v, , v,, v3, v4 are dependent: -ﬂ Pq sk o v;=|0|] vy=]1 v3= |1 vy=|3]. 0] 0 1 4 Solve ¢,v; + cv2 + c3v3 + c4v4 = 0 or Az = 0. The v’s go in the columns of A. 2 (Recommended) Find the largest possible number of independent vectors among 1] 1] 1] 0] [ 0] \" 0] -1 0 0 1 1 0 vV = 0 V) = -1 V3 = 0 V4 = _1 V5 = 0 Ve = 1 R I U B Y R R o T 3 Provethatifa =0o0rd =0or f = 0(3 cases), the columns of U are dependent: a U=10 0 4 Ifa.d. finQuestion 3 are nonzero, show that the only solutionto Uz = 0Qisx = 0. An upper triangular U with no diagonal zeros has independent columns. 5 Decide the dependence or independence of (a) the vectors (1,3,2) and (2,1,3) and (3,2,1) (b) the vectors (1,-3,2) and (2,1,-3) and (-3,2,1). 6 Choose three independent columns of U. Then make two other choices. Do the same for A. and A= 7 If w;, w2, w3 are independent vectors, show that the differences v; = w4 — w3 and vy = w; — w3 and v3 = w; — w; are dependent. Find a combination of the v’s that gives zero. Which matrix Ain [v; v v3] =[w; w; w3 ] A is singular? 8 If w,, w2, w3 are independent vectors, show that the sums v; = w; + w3 and v = w) + w3 and v3 = w) + w; are independent. (Write c; v + c2v2 +czvz =0 in terms of the w’s. Find and solve equations for the c’s, to show they are zero.) 3.4. Independence, Basis, and Dimension 125 9 Suppose v, v3, v3, V4 are vectors in R3. (a) These four vectors are dependent because (b) The two vectors v, and v, will be dependent if (c) The vectors v, and (0, 0, 0) are dependent because 10 Find two independent vectors on the plane £+ 2y —3z—t = 0in R*®. Then find three independent vectors. Why not four? This plane is the nullspace of what matrix? Questions 11-14 are about the space spanned by a set of vectors. Take all linear com- binations of the vectors. 11 Describe the subspace of R? (is it a line or plane or R* ?) spanned by (a) the two vectors (1,1,—1)and (—1,-1,1) (b) the three vectors (0,1,1) and (1, 1,0) and (0,0, 0) (c) all vectors in R3 with whole number components (d) all vectors with positive components. 12 The vector b is in the subspace spanned by the columns of A when has a solution. The vector c is in the row space of A when has a solution. True or false: If the zero vector is in the row space, the rows are dependent. 13 Find the dimensions of these 4 spaces. Which two of the spaces are the same? (a) col- umn space of A, (b) column space of U, (c) row space of A, (d) row space of L’: 1 1 0] 1 1 0 A=1]1 3 1 and U=10 2 1 3 1 -1] 0 0 0 14 v + w and v — w are combinations of v and w. Write v and w as combinations of v + w and v — w. The two pairs of vectors the same space. When are they a basis for the same space? Questions 15-25 are about the requirements for a basis. 15 If vy,...,v, are linearly independent, the space they span has dimension These vectors are a for that space. If the vectors are the columns of an m by n matrix, then m is than n. If m = n, that matrix is 16 Find a basis for each of these subspaces of R*: (a) All vectors whose components are equal. (b) All vectors whose components add to zero. (c) All vectors that are perpendicular to (1,1,0,0) and (1,0,1,1). (d) The column space and the nullspace of I (4 by 4). 126 Chapter 3. The Four Fundamental Subspaces 17 Find three different bases for the column space of U = [3 919 11, Then find two different bases for the row space of U. 18 Suppose v;, v, ..., Ug are six vectors in R’ (a) Those vectors (do)(do not)(might not) span RY. (b) Those vectors (are)(are not)(might be) linearly independent. (c) Any four of those vectors (are)(are not)(might be) a basis for RY. 19 The columns of A are n vectors from R™. If they are linearly independent, what is the rank of A? If they span R™, what is the rank? If they are a basis for R™, what then? Looking ahead: The rank r counts the number of columns. 20 Find a basis for the plane —2y+3z = 0in R>. Then find a basis for the intersection of that plane with the zy plane. Then find a basis for all vectors perpendicular to the plane. 21 Suppose the columns of a 5 by 5 matrix A are a basis for R°. (a) The equation Az = 0 has only the solution £ = 0 because (b) If bis in R’ then Az = b is solvable because the basis vectors R°. Conclusion: A is invertible. Its rank is 5. Its rows are also a basis for R>. 22 Suppose S is a 5-dimensional subspace of R®. True or false (example if false): (a) Every basis for S can be extended to a basis for R® by adding one more vector. (b) Every basis for R® can be reduced to a basis for S by removing one vector. 23 U comes from A by subtracting row 1 from row 3: 3 2 1 1 1 and U=1|0 3 2 1 A=10 1 0 0 . 1]. Od Find bases for the two column spaces. Find bases for the two row spaces. Find bases for the two nullspaces. Which spaces stay fixed in elimination? 24 True or false (give a good reason): (a) If the columns of a matrix are dependent, so are the rows. (b) The column space of a 2 by 2 matrix is the same as its row space. (c) The column space of a 2 by 2 matrix has the same dimension as its row space. (d) The columns of a matrix are a basis for the column space. 25 Suppose vi,...,v; span R\". How would you reduce this set to a basis? Suppose vy,...,v; are independent in R”. How would you add vectors to reach a basis ? 3.4. Independence, Basis, and Dimension 127 26 For which numbers ¢ and d do these matrices have rank 2? 1 2 5 0 5] ] A=1]10 0 and B=[§ d. 0 0 Questions 27-31 are about spaces where the ‘“vectors” are matrices. 27 Find a basis (and the dimension) for each of these subspaces of 3 by 3 matrices: (a) All diagonal matrices. (b) All symmetric matrices (AT = A). (c) All skew-symmetric matrices (AT = —A). 28 Construct six linearly independent 3 by 3 echelon matrices U}, .. ., Us. 29 Find a basis for the space of all 2 by 3 matrices whose columns add to zero. Find a basis for the subspace whose rows also add to zero. 30 What subspace of 3 by 3 matrices is spanned (take all combinations) by (a) the invertible matrices? (b) the rank one matrices? (c) the identity matrix? 31 Find a basis for the space of 2 by 3 matrices whose nullspace contains (2.1.1). Questions 32-36 are about spaces where the ‘“vectors’ are functions. 32 (a) Find all functions that satisfy 3¢ = 0. (b) Choose a particular function that satisfies % = 3. (c) Find all functions that satisfy % = 3. 33 The cosine space F3 contains all combinations y(r) = A cosz+ B cos 2r+C cos 3. Find a basis for the subspace with y(0) = 0. 34 Find a basis for the space of functions that satisfy @ £ -2y=0 d (b) £ -1 =0. 35 Suppose y;(x), y2(x), y3(x) are three different functions of x. The vector space they span could have dimension 1, 2, or 3. Give an example of y;, y2, y3 to show each possibility. 36 Find a basis for the space of polynomials p(x) of degree < 3. Find a basis for the subspace with p(1) = 0. 37 Find a basis for the space S of vectors (a, b, ¢, d) with a + ¢ + d = 0 and also for the space T with a + b = 0 and ¢ = 2d. What is the dimension of the intersection SNT? 128 Chapter 3. The Four Fundamental Subspaces 38 Suppose A is 5 by 4 with rank 4. Show that Az = b has no solution when the 5 by 5 matrix [A b] is invertible. Show that Az = b is solvable when [ A b] is singular. 39 Find bases for all solutions to d*y/dz? = y(z) and then d®y/dr? = —y(x). Challenge Problems 40 Write the 3 by 3 identity matrix as a combination of the other five permutation matrices! Then show that those five matrices are linearly independent. This is a basis for the subspace of 3 by 3 matrices with row and column sums all equal. 41 Choose x = (z,,72,73,Z4) in R%. It has 24 rearrangements like (z2,x,.13,14) and (z4,23,71,Z2). Those 24 vectors, including itself, span a subspace S. Find specific vectors T so that the dimension of S is: (a) zero, (b) one, (c) three, (d) four. 42 Intersections and sums have dim(V) + dim(W) = dim(V N W) + dim(V+W). Start with a basis u,, ..., u, for the intersection VN W. Extend with v,,..., v, to a basis for V, and separately w, ..., w, to a basis for W. Prove that the u’s, v’s and w’s together are independent : dimensions (r+8)+ (r+t) = (r)+(r+s8+t). 43 Inside R\", suppose dimension (V) + dimension (W) > n. Show that some nonzero vectoris in both V and W. Hint ; Put a basis for V' and a basis for IV into the columns of a matrix A, and solve Az = 0. 44 Suppose A is 10 by 10 and A% = 0 (zero matrix). So A multiplies each column of A to give the zero vector. Then the column space of A is contained in the If A has rank r, those subspaces have dimension r < 10 — r. So the rank is r < 5. 3.5. Dimensions of the Four Subspaces 129 3.5 Dimensions of the Four Subspaces /1 The column space C(A) and the row space C(A™) both have dimension r (the rank of A). \\ 2 The nullspace N(A) has dimension n — r. The left nullspace N(AT) has dimension m — r. 3 Elimination from A to Ry changes C(A) and N(AT) (but their dimensions don’t change). / The main theorem in this chapter connects rank and dimension. The rank of a matnx counts independent columns. The dimension of a subspace is the number of vectors in a basis. We can count pivots or basis vectors. The rank of A reveals the dimensions of all four fundamental subspaces. Here are the subspaces, including the new one. Two subspaces come directly from A, and the other two come from AT. Four Fundamental Subspaces Dimensions 1. The row space is C(AT), a subspace of R\". r 2. The column space is C(A), a subspace of R™. T 3. The nullspace is N(A), a subspace of R\". n—r 4. The left nullspace is N(AT), a subspace of R™. m-—r We know C(A) and N(A) pretty well. Now C(AT) and N(AT) come forward. The row space contains all combinations of the rows. This row space is the column space of AT. For the left nullspace we solve ATy = 0—that system is n by m. In Example 2 this produces one of the great equations of applied mathematics—Kirchhoff’s Current Law. The currents flow around a network, and they can’t pile up at the nodes. The matnx A is the incidence matrix of a graph. Its four subspaces come from nodes and edges and loops and trees. Those subspaces are connected in an absolutely beautiful way. Part 1 of the Fundamental Theorem finds the dimensions of the four subspaces. One fact stands out: The row space and column space have the same dimension r. This number r i1s the rank of A (Chapter 1). The other important fact involves the two nullspaces : N(A) and N(AT) have dimensions n — r and m — r, to make up the full n and m. Part 2 of the Fundamental Theorem will describe how the four subspaces fit together : Nullspace perpendicular to row space, and N(AT) perpendicular to C(A). That completes the “right way” to understand Ax = b. Stay with it—you are doing real mathematics. 130 Chapter 3. The Four Fundamental Subspaces The Four Subspaces for R, Suppose A is reduced to its row echelon form Ry. For that special form, the four subspaces are easy to identify. We will find a basis for each subspace and check its dimension. Then we watch how the subspaces change (two of them don’t change!) as we look back at A. The main point is that the four dimensions are the same for A and R,. For A and R, one of the four subspaces can have different dimensions—because zero rows are removed in R, which changes m. As a specific 3 by 5 example, look at the four subspaces for this echelon matrix Ry : pivot rows 1 and 2 m=3 1 n=39% Ro= 10 r=2 0 pivot columns 1 and 4 The rank of this matnix is r = 2 (two pivots). Take the four subspaces in order. 1. The row space has dimension 2, matching the rank. Reason: The first two rows are a basis. The row space contains combinations of all three rows, but the third row (the zero row) adds nothing to the row space. The pivot rows 1 and 2 are independent. That is obvious for this example, and it is always true. If we look only at the pivot columns, we see the r by r identity matrix. There is no way to combine its rows to give the zero row (except by the combination with all coefficients zero). So the r pivot rows (the rows of R) are a basis for the row space. The dimension of the row space is the rank r. The nonzero rows of Rg form a basis. 2. The column space of Ry also has dimension r = 2. Reason : The pivot columns 1 and 4 form a basis. They are independent because they contain the r by r identity matrix. No combination of those pivot columns can give the zero column (except the combination with all coefficients zero). And they also span the column space. Every other (free) column is a combination of the pivot columns. Actually the combinations we need are the three special solutions ! Column 2 is 3 (column 1). The special solution is (-3, 1,0,0,0). Column 3 is 5 (column 1). The special solution is (-5,0,1,0,0, ). Column 5 is 7 (column 1) + 2 (column 4). That solutionis (—7,0,0,-2,1). The pivot columns are independent, and they span C(Ry), so they are a basis for C(Rp). The dimension of the column space is the rank r. The pivot columns form a basis. 3.5. Dimensions of the Four Subspaces 131 3. The nullspace of Ry has dimension n — r = 5 — 2. The 3 free variables give 3 special solutions to Rox = 0. Set the free variables to 1 and 0 and 0. e - - :: g g Rox = 0 has the complete solution 0 0 -2 T = I282 +I383 + I58;5 0 0 1 J The nullspace has dimension 3. | Y | U] I Reason : There is a special solution for each free variable. With n variables and r pivots, that leaves n — r free variables and special solutions. The special solutions are independent, because you can see the identity matrix in rows 2, 3, 5. The nullspace N(A) has dimension n — r. The special solutions form a basis. 4. The nullspace of Rg (left nullspace of Ry) has dimension m — r =3 — 2. Reason: R, has r independent rows and m — r zero rows. Then R} has r independent columns and m — 7 zero columns. So y in the nullspace of R} can have nonzeros in its last m — r entries. The example has m — r = 1 zero column in R} and 1 nonzero in y. \"1 0 0 (o' 3 00|[wm] |O [ 0 ] Riy=|5 00 y2 | =10 issolvedbyy=| 0 |. (1) 01 0 _ygj 0 _y:;_l 7 2 0 0 Because of zero rows in Ry and zero columns in R}, it is easy to see the dimension (and even a basis) for this fourth fundamental subspace : If Ry has m — r zero rows, its left nullspace has dimension m — r. Why is this a “left nullspace”? Because we can transpose Ry = 0 to yT Ry = OT. Now y7 is a row vector to the left of R. This subspace came fourth, and some linear algebra books omit it—but that misses the beauty of the whole subject. In R™ the row space and nullspace have dimensions r and n — r (adding to n). In R™ the column space and left nullspace have dimensions r and m — r (total m). We have a job still to do. The four subspace dimensions for A are the same as for R,. The job is to explain why. A is now any matrix that reduces to Ry = rref(A). 1 3 5 0 7] Samerow space as R, This A reduces to R A= 0 0 0 1 2 Different column space 1 351 9 But same dimension ! 132 Chapter 3. The Four Fundamental Subspaces C(AT) C(4) column space row space all ATy all Az dimension r dimension r The big picture left nullspace nullspace ATy =0 N(AT) dimensionm — r dimensionn — r Figure 3.3: The dimensions of the Four Fundamental Subspaces (for Ry and for A). The Four Subspaces for A 1 A has the same row space as Ryo and R. Same dimension r and same basis. Reason: Every row of A is a combination of the rows of Ry. Also every row of Ry is a combination of the rows of A. Elimination changes rows, but not row spaces. Since A has the same row space as Ry, the first r rows of Ry are still a basis. Or we could choose r suitable rows of the original A. They might not always be the first r rows of A, because those could be dependent. The good r rows of A are the ones that end up as pivot rows in Ry and R. 2 The column space of A has dimension r. The column rank equals the row rank. The number of independent columns = the number of independent rows. Wrong reason: “A and Ry have the same column space.” This is false. The columns of Ry often end in zeros. The columns of A don’t often end in zeros. Then C(A) is not C(Ry). Right reason: The same combinations of the columns are zero (or not) for A and Ry. Dependentin A < dependentin Ry. Say that another way: Az =0 exactly when Rox = 0. The column spaces are different, but their dimensions are the same—equal to the rank . Conclusion The r pivot columns of A are a basis for its column space C(A). 3.5. Dimensions of the Four Subspaces 133 3 A has the same nullspace as Ro. Same dimension n — r and same basis. Reason: Elimination doesn’t change the solutions to Az = 0. The special solutions are a basis for this nullspace (as we always knew). There are n—r free variables, so the nullspace dimension is n — r. This is the Counting Theorem: r 4+ (n — r) equals n. (dimension of column space)+ (dimension of nullspace) = dimension of R™. 4 The left nullspace of A (the nullspace of AT) has dimension m — . Reason: AT is just as good a matrix as A. When we know the dimensions for every A, we also know them for AT. Its column space was proved to have dimension 7. Since AT is n by m, the “whole space” is now R™. The counting rule for A wasr+(n—r) = n. The counting rule for AT is r 4+ (m — r) = m. We have all details of a big theorem: Fundamental Theorem of Linear Algebra, Part 1 The column space and row space both have dimension r. The nullspaces have dimensions n — r and m — r. By concentrating on spaces of vectors, not on individual numbers or vectors, we get these clean rules. You will soon take them for granted—eventually they begin to look obvious. But if you write down an 11 by 17 matrix with 187 nonzero entries, I don’t think most people would see why these facts are true: dimension of C(A) = dimension of C(AT) = rank of 4 Two key facts dimension of C(A) + dimension of N(A) = 17. Every vector Az = b in the column space comes from exactly one & in the row space ! (If we also have Ay = b then A(z —y) = b—b = 0. So x — y is in the nullspace as well as the row space, which forces £ = y.) From its row space to its column space, A is like an r by r invertible matrix. It is the nullspaces that force us to define a “pseudoinverse of A” in Section 4.5. Example 1 A=[; i g] has m =2 with n =3. Therankisr = 1. The row space is the line through (1, 2, 3). The nullspace is the plane x, + 2x2 + 3z3 = 0. The line and plane dimensions still add to 1 + 2 = 3. The column space and left nullspace are perpendicular lines in R2. Dimensions 1 + 1 = 2. Column space = line through [;] Left nullspace = line through [ f] : Final point: The y’s in the left nullspace combine the rows of A to give the zero row. 134 Chapter 3. The Four Fundamental Subspaces Example 2 You have nearly finished three chapters with made-up equations, and this can’t continue forever. Here is a better example of five equations (one equation for every edge in Figure 3.4). The five equations have four unknowns (one for every node). The important matrix in Az = b is an incidence matrix. It has 1 and —1 on every row. Differences Az = b —I1 tI = b across edges 1,2,3,4,5 | ! +T3 = b, between nodes 1, 2, 3, 4 —I2 FI3 = b (2) m=5andn = 4 —T2 trs =by —r3 414 = by If you understand the four fundamental subspaces for this matrix (the column spaces and the nullspaces for A and AT) you have captured a central idea of linear algebra. nodes I, T2 I3 x4 edges -1 1 | 1 -1 1 2 A= -1 1 3 -1 1 4 i -1 1 5 Figure 3.4: A “graph” with 5 edges and 4 nodes. A is its 5 by 4 incidence matrix. The nullspace N(A) To find the nullspace we set b = 0. Then the first equation says r1 = I2. The second equation is 3 = z,. Equation 4 is o = x4. All four unknowns T1,Z2,Z3,Z4 have the same value c. The vectors T = (c, ¢, ¢, ¢) fill the nullspace of A. That nullspace is a line in R*. The special solution z = (1,1,1,1) is a basis for N(A). The dimension of N(A) is 1 (one vector in the basis). The rank of A must be 3, sincen —r =4 — 3 = 1. We now know the dimensions of all four subspaces. The column space C(A) There must be r = 3 independent columns. The fast way 1s to look at the first 3 columns of A. The systematic way is to find Ry = rref( A). -1 1 0 1 0 0 -1° Columns -1 0 1 reduced row 01 0 -1 1,2,3 0 -1 1 Ro = echelonform =| 0 0 1 -1 of A 0 -1 O of A O 00 O 0O 0 -1 00 0 0 From Ry we see again the special solution £ = (1,1, 1, 1). The first 3 columns are basic, the fourth column is free. To produce a basis for C(A) and not C(Rp), we must go back to columns 1,2, 3 of A. The column space has dimension 7 = 3. 3.5. Dimensions of the Four Subspaces 135 The row space C(AT) The dimension must again be r = 3. But the first 3 rows of A are not independent: row 3 = row 2 — row 1. So row 3 became zero in elimination, and row 3 was exchanged with row 4. The first three independent rows are rows 1,2, 4. Those three rows are a basis (one possible basis) for the row space. Edges 1,2, 3 form a loop in the graph: Dependent rows 1, 2, 3. Edges 1, 2,4 form a tree. Trees have no loops! Independentrows 1,2, 4. The left nullspace N(AT) Now we solve ATy = 0. Combinations of the rows give zero. We already noticed that row 3 = row 2 — row 1, so one solution is y = (1,-1,1,0,0). I would say: That y comes from following the upper loop in the graph. Another y comes from going around the lower loop and it is y = (0.0.—1.1,—1): row 3 = row 4 — row 5. Those two y’s are independent, they solve ATy = 0, and the dimension of N(AT) is m — r = 5 — 3 = 2. So we have a basis for the left nullspace. You may ask how “loops” and “trees” got into this problem. That didn’t have to happen. We could have used elimination to solve ATy = 0. The 4 by 5 matrix AT would have three pivot columns 1, 2,4 and two free columns 3,5. There are two special solutions and the nullspace of AT has dimension two: m —r = 5 — 3 = 2. But loops and trees identify dependent rows and independent rows in a beautiful way for every incidence matnx. The equations Ax = b give *“voltages” x,, T7. I3, 14 at the four nodes. The equations ATy = 0 give “currents” y;.y2, Y3, Y4, ys on the five edges. These two equations are Kirchhoff’s Voltage Law and Kirchhoff’s Current Law. Those laws apply to an electrical network. But the ideas behind the words apply all over engineering and science and economics and business. Linear algebra connects the laws to the four subspaces. Graphs are the most important model in discrete applied mathematics. You see graphs everywhere: roads, pipelines, blood flow, the brain, the Web, the economy of a country or the world. We can understand their matrices A and AT. Here is a summary. The incidence matrix A comes from a connected graph with n nodes and m edges. The row space and column space have dimensions # = n — 1. The nullspaces of A and AT have dimensions 1 andm —n + 1: N(A) The constant vectors (c,c,...,c) make up the nullspace of A : dim = 1. C(AT) The edges of any tree give r independentrowsof A: r =n — 1. C(A) Voltage Law: The components of Ax add to zero around all loops: dim = n — 1. N(AT) Current Law: ATy = (flow in)—(flow out) = 0 is solved by loop currents. Thereare m — r = m — n + 1 independent small loops in the graph. For every graph in a plane, linear algebra yields Euler’s formula : Theorem 1 in topology ! (nodes) — (edges) + (small loops) =(n) — (m)+ (m—n+1) =1 136 Chapter 3. The Four Fundamental Subspaces Rank Two Matrices = Rank One plus Rank One Rank one matrices have the form uv™. Here is a matrix A of rank r = 2. We can’t see r immediately from A. So we reduce the matrix by row operations to Ry. Ry has the same row space as A. Throw away its zero row to find R—also with the same row space. 1031 [10 _4220_ _4 2d Now look at columns. The pivot columns of R are clearly (1,0) and (0,1). Then the pivot columns of A are also in columns 1 and 2: u; = (1,1,4) and u, = (0, 1,2). Notice that C has those same first two columns! That was guaranteed since multiplying by two columns of the identity matrix (in R) won’t change the pivot columns u; and u,. When you put in letters for the columns and rows, you see rank 2 = rank 1 + rank 1. _ . . v'lr - Matrix A A _ _ T T Rank two = | u; u2 U3 v, = U1V; + U2v, i i Zero row Columns of C times rows of R. Every rank r matrix is a sum of r rank one matrices 8 WORKED EXAMPLES = 3.5A Putfour1’sintoa 5 by 6 matrix of zeros, keeping the dimension of its row space as small as possible. Describe all the ways to make the dimension of its column space as small as possible. Then describe all the ways to make the dimension of its nullspace as small as possible. How to make the sum of the dimensions of all four subspaces small? Solution The rank is 1 if the four 1’s go into the same row, or into the same column. They can also go into two rows and two columns (so ai; = a;j; = aj; = a;; = 1). Since the column space and row space always have the same dimensions, this answers the first two questions: Dimension 1. The nullspace has its smallest possible dimension 6 — 4 = 2 when the rank is r = 4. To achieve rank 4, the 1’s must go into four different rows and four different columns. You can’t do anything about the sumr + (n—r)+r+(m—7) = n + m. It will be 6 + 5 = 11 no matter how the 1’s are placed. The sum is 11 even if there aren’t any 1’s... If all the other entries of A are 2’s instead of 0’s, how do these answers change ? 3.5. Dimensions of the Four Subspaces 137 3.5B All the rows of AB are combinations of the rows of B. So the row space of AB is contained in (possibly equal to) the row space of B. Rank (AB) < rank (B). All columns of AB are combinations of the columns of A. So the column space of AB is contained in (possibly equal to) the column space of A. Rank (AB) < rank (A). If we multiply A by an invertible matrix B, the rank will not change. The rank can’t drop, because when we multiply by the inverse matrix the rank can’t jump back up. Appendix 1 collects the key facts about the ranks of matrices. Problem Set 3.5 1 (a) If a 7 by 9 matrix has rank 5, what are the dimensions of the four subspaces? What is the sum of all four dimensions? (b) If a 3 by 4 matrix has rank 3, what are its column space and left nullspace? 2 Find bases and dimensions for the four subspaces associated with A and B': 1 2 4 1 2 4 A‘[z 4 8] and B‘[z 5 8]' 3 Find a basis for each of the four subspaces associated with A: 0 1 2 3 4] [1r o0 o0]f0o1 2 3 4 A=10 1 2 4 6/=1]1 1 0]{0 O O 1 2 00012 [011/]|0o000O0O0 4 Construct a matrix with the required property or explain why this is impossible: (a) Column space contains [({)] : [g] , TOW space contains [%] : [g] (b) Column space has basis [i] , nullspace has basis [?] . (c) Dimension of nullspace = 1 + dimension of left nullspace. (d) Nullspace contains [ § |, column space contains 3 ]. (e) Row space = column space, nullspace # left nullspace. 5 If V is the subspace spanned by (1,1,1) and (2,1,0), find a matrix A that has V as its row space. Find a matrix B that has V as its nullspace. Multiply AB. 6 Without using elimination, find dimensions and bases for the four subspaces for 0 3 3 3] 1] A=10 0 0 0| and B=|4]. 01 01 5 7 Suppose the 3 by 3 matrix A is invertible. Write down bases for the four subspaces for A, and also for the 3 by 6 matrix B = [A A|]. (The basis for Z is empty.) 138 Chapter 3. The Four Fundamental Subspaces 8 What are the dimensions of the four subspaces for A, B, and C, if I is the 3 by 3 identity matrix and 0 is the 3 by 2 zero matrix ? I 1 OT OT] and C = [0] 9 Which subspaces are the same for these matrices of different sizes? i o ({4 ] Prove that all three of those matrices have the same rank r. A=[I0] and B=[ 10 If the entries of a 3 by 3 matrix are chosen randomly between 0 and 1, what are the most likely dimensions of the four subspaces? What if the random matrix is 3 by 5? 11 (Important) A is an m by n matrix of rank r. Suppose there are right sides b for which Az = b has no solution. (a) What are all inequalities (< or <) that must be true between m, n, and r? (b) How do you know that ATy = 0 has solutions other than y = 0? 12 Construct a matrix with (1,0,1) and (1,2,0) as a basis for its row space and its column space. Why can’t this be a basis for the row space and nullspace ? 13 True or false (with a reason or a counterexample): (a) If m = n then the row space of A equals the column space. (b) The matrices A and — A share the same four subspaces. (c) If A and B share the same four subspaces then A is a multiple of B. 14 Without computing A, find bases for its four fundamental subspaces : 1 0 0] 1 2 3 4 A=16 1 0|0 1 2 3]. 9 8 1/|0 0 1 2 15 If you exchange the first two rows of A, which of the four subspaces stay the same? If v = (1,2, 3,4)isin the left nullspace of A, write down a vector in the left nullspace of the new matrix after the row exchange. 16 Explain why v = (1,0,—1) cannot be a row of A and also in the nullspace of A. 17 Describe the four subspaces of R3 associated with 0 1 0] 1 1 0] A=1|0 0 1| and I+A=|0 1 1]. 0 0 0 0 0 1 18 Can tic-tac-toe be completed (5 ones and 4 zeros in A) so that rank (4) = 2 but neither side passed up a winning move ? 3.5. Dimensions of the Four Subspaces 139 19 20 21 22 23 24 25 26 (Left nullspace) Add the extra column b and reduce A to echelon form: 1 2 3 b 1 2 3 b ] (A b]=[4 56 b|] — |0 -3 —6 b —4ab L7 8 9 b3- L0 0 0 b3—2b2+b1d A combination of the rows of A has produced the zero row. What combination is it? (Look at b3 — 2b, + b; on the right side.) Which vectors are in the nullspace of AT and which vectors are in the nullspace of A? (Patience needed) Describe the row operations that reduce a matrix A to its echelon form R,. Suppose A is the sum of two matrices of rank one: A = uv! + wzT. (a) Which vectors span the column space of A? (b) Which vectors span the row space of A? (c) Therankislessthan2if _ orif (d) Compute A and its rank if u = z = (1,0,0) and v = w = (0.0, 1). Construct A = uvT + wzT whose column space has basis (1.2.4).(2.2.1) and whose row space has basis (1,0), (1,1). Write A as (3 by 2) times (2 by 2). Without multiplying matrices, find bases for the row and column spaces of A: 3 0 3 1 1 2|° How do you know from these shapes that A cannot be invertible? -y 1 2 A=14 5 |-2 7-1 (Important) ATy = d is solvable when d is in which of the four subspaces? The solution vy is unique when the contains only the zero vector. True or false (with a reason or a counterexample): (a) A and AT have the same number of pivots. (b) A and AT have the same left nullspace. (c) If the row space equals the column space then AT = A. (d) If AT = — A then the row space of A equals the column space. If a, b, c are given with a # 0, how would you choose d so that [g 3] has rank 1? Find a basis for the row space and nullspace. Show they are perpendicular! 140 27 28 30 31 32 33 35 Chapter 3. The Four Fundamental Subspaces Challenge Problems Find the ranks of the 8 by 8 checkerboard matrix B and the chess matrix C': 1010101 0] r nb g k b n r 01010101 P ppPppPPpPDPPUDPD B=|10101010| ad C= four zero rows ° ° ° . ° ° . . p p p p p p p p 0101010 1] rnb gk b nr The numbers r,n, b, ¢, k, p are all different. Find bases for the row space and left nullspace of B and C. Find a basis for the nullspace of C. If A = uvT is a 2 by 2 matrix of rank 1, redraw Figure 3.5 to show clearly the Four Fundamental Subspaces. If B produces those same four subspaces, what is the exact relation of Bto A? M is the space of 3 by 3 matrices. Multiply every matrix X in M by A : [ 1 0 -1 1] [0] A=]-1 1 O0]. Notice: A|1]| =10 0 -1 1 | 1 0] (a) Which matrices X lead to AX = zero matrix ? (b) Which matnces have the form AX for some matrix X ? (a) finds the “nullspace” of that operation AX and (b) finds the “column space”. What are the dimensions of those two subspaces of M ? Why do they add to 9? Suppose the m by n matrices A and B have the same four subspaces. If they are both in row reduced echelon form, is it true that F' must equal G ? N 0 0 0 0 I Find the incidence matrix and its rank and one vector in each subspace for this b, & + b complete graph—all six edges included. 7, I3 b4 b5 (Review) (a) Is N(AB) or N(BA) containedin N(A)? T4 (b) Is C(AB) or C(BA) contained in C(A) ? Suppose Aismbynand Bis Mbynand T = [ g ] (a) What are the relations between the nullspaces of A and B and T ? (b) What are the relations between the row spaces of A and B and T ? Suppose A is m by n. What can you say about each of the four fundamental sub- spaces for the matrices AandW = A A ]? IanndBarcnbyn,isitalwaystructhatrank[ g ] =rank[ A B ]? 3.5. Dimensions of the Four Subspaces 141 Thoughts on Chapter 3 : The Big Picture of Elimination This page explains elimination at the vector level and subspace level, when A is reduced to R. You know the steps and I won’t repeat them. Elimination starts with the first pivot. It moves a column at a time (left to right) and a row at a time (top to bottom) for U. Continuing elimination upward produces Rg and R. Elimination answers two questions : Question 1 Is this column a combination of previous columns? If the column contains a pivot, the answer is no. Pivot columns are “independent” of previous columns. If column 4 has no pivot, it is a combination of columns 1, 2, 3. Question 2 Is this row a combination of previous rows? If the row contains a pivot, the answer is no. Pivot rows are independent of previous rows, and their first nonzero is 1 from I. Rows that are all zero in Ry were not and are not independent. Those zero rows disappear in R. That matrix is r by n. It is amazing to me that one pass through the matrix answers both questions 1 and 2. Elimination acts on the rows but the result tells us about the columns ! The identity matrnx in R locates the first r independent columns in A. Then the free columns F' in R tell us the combinations of those independent columns that produce the dependent columns in A. This is easy to miss without seeing the factorization A = CR. R tells us the special solutions to Az = 0. We could reach R from A by different row exchanges and elimination steps, but it will always be the same R. (This is because the special solutions are fully decided by A. The formula comes before Problem Set 3.2.) In the language coming soon, R reveals a “basis” for three of the fundamental subspaces : The column space of A—choose the r columns of A that produce pivots in R. The row space of A—choose the r rows of R as a basis. The nullspace of A—choose the n — r special solutions to Rz = 0 (and Az = 0). For the left nullspace N(AT), we look at the elimination step EA = Ry. The last m — r rows of Ry are zero. The last m — r rows of E are a basis for the left nullspace! In reducing the extended matrix [A I]to [Ry E], the matrix E keeps a record of elimination that is otherwise lost. Suppose we fix C and B (m by r and r by n, both rank 7). Choose any invertible r by r mixing matrix M. All the matrices C M B (and only those) have the same four fundamental subspaces. Note This is the first textbook to express the result of elimination in its matrix form A=CR=C I F| P. Elimination reveals C and Fand Pand A = [C CF|P. A= [Independent columns in C Dependent columns in CF] Permute columns 142 Chapter 3. The Four Fundamental Subspaces Row Operations on an m by n Matrix A : Review (i) Subtract a multiple of one row from another row (1) Exchange two rows (i) Multiply a row by any nonzero constant The important point is : Those row operations are reversible (invertible). (i) Add back the multiple of one row to the other row (i) Exchange the rows again (ii1)) Divide the row by that nonzero constant Total effect of those row operations: An m by m invertible matrix E multiplies A. The nullspace is not changedby E: Az = 0= EAz =0= Az = 0. A and E A have different rows but the same row space and nullspace. I F 0 O The identity I is r by r, F'is r by n — r, P puts the n columns in correct order. Reduced Row Echelon Form: E can produce EA = Ry = [ } P = rref(A) Factorization: A = CR = [First r independent columns] [ I F | P A= C CF | P = Independent cols Dependent cols | Reorder columns Nullspace of A : Each column of F leads to one of the n—r “special solutions” to Ax = 0: Special solution page 88, Example | — column k of F(rbyn —r) _ pT % =F +columnkof I(n—rbyn —r) That permutation PT puts the n components of the solution 8y in the right order. Example Special solution 8; to Az = 0and Rz = O with P = I andrank r = 3 1 0 0 3] j R = 010 4 81 = R81=0 A81=CR81=0 -9 0015 B 4 Orthogonality 4.1 Orthogonality of Vectors and Subspaces 4.2 Projections onto Lines and Subspaces 4.3 Least Squares Approximations 44 Orthogonal Matrices and Gram-Schmidt 4.5 The Pseudoinverse of a Matrix Two vectors are orthogonal when their dot product is zero: v - w = vTw = 0. This chapter moves to orthogonal subspaces and orthogonal bases and orthogonal matrices. The vectors in two subspaces, and the vectors in a basis, and the column vectors in Q, all pairs will be orthogonal. Think of a® + b% = ¢? for a right triangle with sides v and w. T w=0 and lv]|2 + ||lw||* = ||v + w||?. Orthogonal vectors v The right side is (v + w)T (v + w). This equals vTv + wTw when vTw = wTv = 0. Subspaces entered Chapter 3 to throw light on Az = b. Right away we needed the column space and the nullspace. Then the light turned onto AT, uncovering two more subspaces. Those four fundamental subspaces reveal what a matrix really does. A matrix multiplies a vector: A times x. At the first level this is only numbers. At the second level Az is a combination of column vectors. The third level shows subspaces. But I don’t think you have seen the whole picture until you study Figure 4.1. Those fundamental subspaces are orthogonal : The nullspace N(A) contains all vectors orthogonal to the row space C(AT). The nullspace N(AT) contains all vectors orthogonal to the column space C(A). Az = 0 makes z orthogonal to each row. ATy = 0 makes y orthogonal to each column. A key idea in this chapter is projection: If b is outside the column space of A, find the closest point p that is inside. The line from b to p shows the error e. That line is perpendicular to the column space. The least squares equation ATAZ = ATb produces the closest p = AZ and smallest possible error e. It gives the best £ when Ax = b is unsolvable. That best Z makes ||AZ — b||? as small as possible: Least squares. The equation AT AZ = ATbis easy when AT A = I. Then A has orthonormal columns:: perpendicular unit vectors. That won’t happen by accident but we can make it happen. Orthogonalizing the columns a; to a, in 4.4 produces q, to g, with q,qu = 0. The matrix Q has QTQ = I and QR = A. This R is upper triangular. Orthogonal matrices are perfect for computations. ); times @2 is still orthogonal. In many ways A = QR is better than A = LU, and this chapter shows why. 143 144 Chapter 4. Orthogonality 4.1 Orthogonality of Vectors and Subspaces 1 Orthogonal vectors have vTw = 0. Then ||v||? + ||w||? = ||v + w||? asin a? + b% = 3.\\ W. 2 Subspaces V and W are orthogonal when vTw = 0 for every v in V and every w in 3 The row space of A is orthogonal to the nullspace. The column space is orthogonal to N(AT). 4 The dimensions addtor + (n —r) = nand r + (m — r) = m: Orthogonal complements. {If n vectors in R\" are independent, they span R\". If n vectors span R\", they are independely Chapter 1 connected dot products v T w to the angle between v and w. For 90° angles we have vTw = 0. The vectors v, w and v + w produce a right triangle. The side lengths a.b,cor||v|,||w]|.||v + w|| satisfy the famous rule a® + b*> = ¢? of Pythagoras: Orthogonal vectors v*w =0 and ||v|2 + ||w]|? = ||v + w]||? The right side is (v+w)T(v+w) = vTv+wTw+vTw+wTv = vTv+wTw+0+0. These vectors can be in a plane or in n-dimensional space R\". We still see a right triangle, and orthogonal means perpendicular. A line (1-dimensional subspace) is perpendicular to a plane (2-dimensional subspace) when every vector on the line is perpendicular to every vector in the plane. Now we are in 3 or more dimensions. The test for perpendicular subspaces is passed by the row space and nullspace of any matrix A: The nullspace of A is orthogonal to the row space of A. Look at Az = 0. ‘rowlof A [ | 0 «— (rowl)-xiszero (1) ‘rowmof A - 0f «— (rowm)-ziszero This says : Every row has a zero dot product with &. Then every combination of the rows is perpendicular to . The whole row space C(A™) is orthogonal to the whole nullspace. Remember our convention—to stay with column vectors. The rows of A become the columns of AT. The vectors in the row space become combinations ATy of those columns. Then here is the official short proof of perpendicular subspaces : z in nullspace orthogonal to ATy in row space 2\" (A y)=(Az)Ty=0Ty=0. () We like the first proof in (1). You can see each row of A multiplying x to give zero. Then all linear combinations of the rows multiply z to give zero. 4.1.. Orthogonality of Vectors and Subspaces 145 Important There is another pair of fundamental subspaces: The column space C(A) and N(AT) = nullspace of AT. Those are perpendicular inside m-dimensional space R™. We could just apply the original proof to AT instead of A. The row space of AT is the column space of A. Then row space perpendicular to nullspace (applied to AT) becomes column space C(A) perpendicular to N(AT). Example 1 The two rows of A are perpendicular to in the nullspace of A: row space 11 -2 1 i 10 Dot product1 -2+1=0 nullspace 11 0 -1 i |0 Dot product1 +0—-1=0 And the three columns of A (rows of AT) are perpendicularto y = 0in N(AT): 1 1 0 column space T,=| —2 0 0 _ 0 left nullspace - 1 —1 0 0 That was an extreme case with C(A) = all of R? and nullspace of AT = zero vector. The row space — nullspace part had dimensions 2 + 1 = 3. Those subspaces accounted for all vectors in R®> = R™. The column space — left nullspace part had dimensions 2 + 0 = 2. Those subspaces accounted for all vectors in R? = R™. There is a very important restriction on the dimensions of any two orthogonal subspaces : If V and W are orthogonal subspaces in R\" thendimV + dim W < n. That seems so clear and natural. But look what it tells us about a wall of your room and the floor of your room. Those are 2-dimensional subspaces in R®. They can’t be orthogonal subspaces! 2 + 2 is more than 3. Some vector must lie in both the wall and the floor. Yes, the line where wall meets floor is in both subspaces. Two orthogonal subspaces that account for the whole space have a special name: orthogonal complements. The orthogonal complement V1 of V contains all vectors orthogonal to V. So the two pairs of subspaces in the big picture of linear algebra are actually orthogonal complements : Orthogonal Row space and nullspace r+(n—r)=n complements Column space and left nullspace r+ (m —r) =m Any vector x in R\" is the sum £ = Trow + Tpul) Of its row space component and its [ m . . nullspace component.. Any vector Y in R TlS the sum y = y.o] + Ypujp ©f its column space component and its componentin N(A\"). 146 Chapter 4. Orthogonality Fundamental Theorem of Linear Algebra, Part 2 N(A) is the orthogonal complement of the row space C(AT) (in R™). N(AT) s the orthogonal complement of the column space C(A) (in R™). Part 1 gave the dimensions of the subspaces. Part 2 gives the 90° angles between them. Every z can be split into a row space component Ty and a nullspace component z,,. When A multiplies £ = zr + x,,, Figure 4.1 shows Az, = 0 and Az, = Azx. dimr dimr T all combination c4%) all combinations of the C4) of the rows b columns Figure 4.1: Az, = b is in the column space and Ax,, = 0. The complete solution to Az = bisz = one .+ any x,. Then ||z||? = ||z,||? + ||Zn||? and the minimum norm solution to Az = bis = z, in the row space plus x,, = 0 from N(A). Every vector Az is in the column space! Multiplying by A cannot do anything else. More than that: Every vector b in the column space comes from exactly one vector x, in the row space. Proof: If Azy = Ax;, the difference £ — 7. is in the nullspace. It is also in the row space, where zr and 7. came from. This difference must be the zero vector, because the nullspace and row space are perpendicular. Therefore z = /.. There is an r by r invertible matrix hiding inside A, if we throw away the two nullspaces. A is invertible from row space to column space. The pseudoinverse A+ in Section 4.5 will invert that part of A: column space back to row space. Example 2 Every matrix of rank r has an r by r invertible submatrix. A has rank 2: 1 2 3 4 5° A=\\|1 2 4 5 6 | contains [ 13 ] in the pivot rows and pivot columns. 1 4 1 2 45 6 Proof for any A: Its submatrix C has r independent columns (Chapter 1). Then C and CT have rank r. So C7T has r independent columns—in other words C has r independent rows. This locates an r by r invertible submatrix of A. 4.1. Orthogonality of Vectors and Subspaces 147 Let me repeat : The only vector in two orthogonal subspaces is the zero vector. Combining Bases from Subspaces A basis contains linearly independent vectors that span the space. Normally we have to check both properties of a basis. When the count is right, one property implies the other: Every vector is a combination of the basis vectors in exactly one way. Any n independent vectors in R™ must span R™. So they are a basis. Any n vectors that span R™ must be independent. So they are a basis. This is true in any vector space, but we care most about R™. When the vectors go into the columns of an n by n square matrix A, here are the same two facts : If the n columns of A are independent, they span R\"™. So Az = b is solvable. If the n columns span R™, they are independent. So Az = b has only one solution. If AB = I for square matrices A and B, then also BA = I. Uniqueness implies existence and existence implies uniqueness. Then A is invertible. If there are no free variables, the solution & is unique. There must be n pivot columns. Then back substitution solves Ax = b (the solution exists). Starting in the opposite direction, suppose that Az = b can be solved for every b (existence of solutions). Then elimination produced no zero rows. There are n pivots and no free variables. The nullspace contains only £ = 0 (uniqueness of solutions). With bases for the row space and the nullspace, we have r + (n — r) = n vectors. This is the right number. Those n vectors are independent.! Therefore they span R™. Each z is the sum = + T, of a row space vector £ and a nullspace vector x ;. The splitting x, + x, in Figure 4.1 shows the key point of orthogonal complements— the dimensions add to n and all vectors are fully accounted for. 1 2 : 4 |. | 2 2 Example 3 ForA=[3 6]splltz—[3]mtomr+zn—[4]+[_l ] The vector (2, 4) is in the row space. The orthogonal vector (2, —1) is in the nullspace. The next section will compute this splitting by a projection matrix P. Example 4 Suppose S is a six-dimensional subspace of nine-dimensional space R°. (a) What are the possible dimensions of subspaces orthogonal to S ? 0,1,2,3 (b) What are the possible dimensions of the orthogonal complement St of S? 3 (c) What is the smallest possible size of a matrix A that has row space S ? 6by9 (d) What is the smallest possible size of a matrix B that has nullspace S* ? 6by9 \"'If a combination of all n vectors gives Tr + Tn = 0, then &r = —on 1s in both subspaces. So xr = n = 0. All coefficients of the row space basis and of the nullspace basis must be zero. This proves independence of the n vectors together. 148 Chapter 4. Orthogonality Problem Set 4.1 1 Construct any 2 by 3 matrix of rank one. Copy Figure 4.1 and put one vector in each subspace (and put two in the nullspace). Which vectors are orthogonal? 2 Redraw Figure 4.1 for a 3 by 2 matrix of rank » = 2. Which subspace is Z (zero vector only)? The nullspace part of any vector x in R? is z, = : 3 Construct a matrix with the required property or say why that is impossible: 1 2 (a) Column space contains [ 2] and [_:] , nullspace contains [ i ] 1 2 (b) Row space contains [ g] and [—g], nullspace contains [i] (c) Az = [i] has a solution and AT [é] = [g] (d) Every row is orthogonal to every column (A is not the zero matrix) (e) Columns add up to a column of zeros, rows add to a row of 1°s. 4 If AB = 0 then the columns of B are in the of A. The rows of A are in the of B. With AB = 0, why can’t A and B be 3 by 3 matrices of rank 2? 5 (a) If Az = bhasasolutionand ATy = 0, is (yTx = 0) or (yTb = 0)? (b) If ATy = (1,1,1) has a solution and Az = 0, then 6 Any system of equations Az = b with no solution can be combined into 0 = 1. z+2y+2z = 5 2r+2y+3z = 3z + 4y + 52 Find numbers y,, y2, y3 to multiply the equations so they add to 0 = 1. You have found a vector y in which subspace? Its dot product yTb is 1, so no solution . 7 Every system Az = b with no solution is like the one in Problem 6. There are num- bers y1,. .., Ym that multiply the m equations so they add up to 0 = 1. This is called Fredholm’s Alternative: If b is not in C(A), then part of b is in N(AT). Exactly one problem has a solution: Az =b OR ATy =0 with yTb=1. Multiply the equations £; — z2 = land 22 — 3 = 1 and £; — £3 = 1 by numbers Y1, Y2, Y3 chosen so the equations add up to 0 = 1. The equations are unsolvable. ] 8 InFigure4.1, how do we know that Az is equal to Az? How do we know that this vector Az is in the column space? If A= [} 1] and z = [} ] what is =,.? 4.1. Orthogonality of Vectors and Subspaces 149 10 11 12 if ATAxz = 0 then Az = 0. Reason: Az is in the nullspace of AT and also in the __ of A and those spaces are . Conclusion: Ax = 0 and therefore AT A has the same nullspace as A. This key fact will be repeated when we need it. Suppose A is a symmetric matrix (AT = A). (a) Why is its column space perpendicular to its nullspace? (b) If Ax = 0 and Az = 5z, which subspaces contain these “eigenvectors” z and z? Symmetric matrices have perpendicular eigenvectors xTz = 0. 1 1 Draw Figure 4.1 to show each subspace correctly for A = [3 2} and B = [ 3 8] : : 1 -1 2 Find z; and x5, and draw Figure 4.1 properly if A = [O 0] and x = [ 0] : Questions 13-23 are about orthogonal subspaces. 13 14 15 16 17 18 19 Put bases for the subspaces V and W into the columns of matrices V and IV'. Explain why the test for orthogonal subspaces can be written VTW = zero matrix. This matches vTw = 0 for orthogonal vectors. The floor V and the wall W are not orthogonal subspaces, because they share a nonzero vector (along the line where they meet). No planes V and W in R can be orthogonal! Find a vector in the column spaces of both matrices: 1 2] (5 4] A=1|1 3 and B=16 3 1 2] 5 1 This will be a vector Az and also BZ. Think 3 by 4 with the matrix [A B]. Extend Problem 14 to a p-dimensional subspace V and a g-dimensional subspace W of R™. What inequality on p+ g guarantees that V intersects W in a nonzero vector? These subspaces cannot be orthogonal. Prove that every y in N(AT) is perpendicular to every Az in the column space, using the matrix shorthand of equation (2). Start from ATy =0. If S is the subspace of R3 containing only the zero vector, what is St? IfSis spanned by (1,1, 1), what is S 7 If S is spanned by (1,1,1) and (1,1, —1), what is a basis for S ? Suppose S only contains two vectors (1,5,1) and (2, 2,2) (not a subspace). Then S+ is the nullspace of the matrix A = . §1 is a subspace even if S is not. Suppose L is a one-dimensional subspace (a line) in R3. Its orthogonal complement L+ is the perpendicular to L. Then (L1)1 is a perpendicular to L. In fact (L1)* is the same as 150 20 21 22 23 Chapter 4. Orthogonality Suppose V is the whole space R*. Then V- contains only the vector _____. Then (V4)t is . So (V1)4 is the same as Suppose S is spanned by the vectors (1.2, 2, 3) and (1, 3, 3. 2). Find two vectors that span S*. This is the same as solving Az = 0 for which A? If P is the plane of vectors in R* satisfying £, + 2 + 13 + x4 = 0, write a basis ‘for P*. Construct a matrix that has P as its nullspace. If a subspace S is contained in a subspace V, explain why S* contains V*. Questions 24-28 are about perpendicular columns and rows. 24 25 26 27 28 30 31 32 33 Suppose an n by n matrix is invertible: AA~! = I. Then the first column of A~! is orthogonal to the space spanned by which rows of A ? Find AT A if the columns of A are unit vectors, perpendicular to each other. Construct a 3 by 3 matrix A with no zero entries whose columns are mutually per- pendicular. Compute AT A. Why is it a diagonal matrix? The lines 3r +y = b, and 6z + 2y = b, are . They are the same line if : In that case (b;, b9) is perpendicular to the vector . The nullspace of the matrix i1s the line 3z + y = . One particular vector in that nullspace is Why is each of these statements false? (a) (1,1,1)is perpendicularto (1,1,—2) sothe planesz +y+ 2z = O0andz + y — 22 = ( are orthogonal subspaces. (b) The subspace spanned by (1,1,0,0,0) and (0, 0.0, 1, 1) is the orthogonal com- plement of the subspace spanned by (1, -1,0,0,0) and (2, —2, 3, 4, —4). (c) Two subspaces that meet only in the zero vector are orthogonal. Find a matrix A with v = (1,2, 3) in the row space and column space. Find B with v in the nullspace and column space. Which pairs of subspaces can’t share v ? Suppose Ais 3 by 4 and Bis 4 by 5 and AB = 0. So N(A) contains C(B). Prove from the dimensions of N(A) and C(B) that rank(A) + rank(B) < 4. Suppose the command N = null(A) will produce a basis for the nullspace of A. Then the command B = null(N'T) will produce a basis for the of A. What are the conditions for nonzero vectors 7, n, ¢, in R? to be bases for the four fundamental subspaces C(AT),N(A), C(A),N(AT) of a 2 by 2 matrix ? When can the 8 vectors 7, 72,1, n2,C1,¢2,01,12 in R* be bases for the four fundamental subspaces of a 4 by 4 matrix ? What is one possible matrix A ? 4.2. Projections onto Lines and Subspaces 151 4.2 Projections onto Lines and Subspaces ﬁhe projection of b onto the line through a is the closest pointtob: p = a(a™b/aTa). \\ 2 The error e = b — p is perpendicular to @ : Right triangle b p e has ||p||2 + ||e||2 = ||b]|2. 3 The projection of b onto a subspace S is the closest vector pin S; b — p is orthogonal to S. 4 AT Ais invertible (and symmetric) when A has independent columns: N(AT A) = N(A). 5§ Then the projection of b onto the column space of C(A) is the vector p = A(ATA)\"1ATb. QThe projection matrix onto C(A) is|P=A(ATA) ' AT.|Ithasp=Pband P2=P = PT/ Projections on a subspace S are easy to visualize : Each vector b goes to the closest point p in S. Then the error vector e = b—p is perpendicular to the subspace. If A has independent columns, the projection of b onto C(A) is p = A(ATA)~1ATb. That projection matrix P = A(ATA)~1 AT is symmetric. Its special property is P? = P—a second projection changes nothing, because p projects to p. For specially nice subspaces you can see their projection matrices P : 1 What are the projections of b = (2, 3, 4) onto the z axis and the ry plane? 2 What matrices P; and P, produce those projections onto a line and a plane? When b is projected onto a line, its projection p is the part of b along that line. If b is projected onto a plane, p is the part in that plane. The projection p is Pb. The projection onto the z axis we call p,. The projection p, drops straight down to the ry plane. Start with b = (2, 3, 4). The projection across gives p, = (0.0.4). The projection down gives p, = (2, 3,0). Those are the parts of b along the z axis and in the ry plane (Figure 4.2). The projection matrices P, and P, are 3 by 3. They multiply b with 3 components to produce p with 3 components. Projection onto a line comes from a rank one matnx. Projection onto a plane comes from a rank two matnx: Projection matrix g . P, = Onto the z axis: 1 0 0 1 0 0 0 Ontothe zy plane: P, = |0 1 0 1! 0 0 L P, picks out the z component of every vector. P picks out the z and y components. To find the projections p, and p, of b, multiply b by P, and P, (small p for the vector, capital P for the matrix that produces it): 0 0 0] [z 1 0 p,=Pb=10 0 0| |y|=]0| p,=Pb=|0 1 00 1 |z] |z 0 0 152 Chapter 4. Orthogonality In this case the projections p; and p; are perpendicular. The ry plane and the z axis are orthogonal subspaces, like the floor of a room and the line straight upward. 0] AP\\\\ Projectionp, = P,b=|0 RN z | - -zd |b= y = - 0 0 0 | |z | 1 0 0 P=|0 00 ' P,=10 10 001 ' 0 0 O - l - - | T | p,=Pb=|y bO- Figure 4.2: Projections p, = Piband p, = P,bwith P, + P, = I and P, P, = 0. More than just orthogonal, the line and plane are orthogonal complements. Their dimensions add to 1 + 2 = 3. Every vector b in the whole space is the sum of its parts in the two subspaces. The projections (0,0, z) and (z, y, 0) are exactly those two parts of b: The vectors give p; + p, =b. The matrices give P, + P, = I and PP, = 0. (1) This is perfect. Our goal is reached—for this example. We have the same goal for any line and any plane and any n-dimensional subspace in m dimensions. The object is to find the part p in each subspace, and the projection matrix P that produces that part p = Pb. Every subspace of R™ has its own m by m projection matrix P. The best description of a subspace is a basis. We put the basis vectors into the columns of A. Now we are projecting onto the column space of A! Certainly the z axis is the column space of the 3 by 1 matrix A;. The zy plane is the column space of A,. That plane is also the column space of A3 (a subspace has many bases). So p, = p; and P, = Ps. ro- g n - - A1= 0 and A2= e o L 0 1 2 1] and A3=12 3 0 0 0 - - - 1 0 Our problem is to project any b onto the column space of any m by n matrix. Start with a line (dimension n = 1). The matrix A will have only one column. Call it a. Projection Onto a Line A line goes through the origin in the direction of @ = (a;,. . .,an). Along that line, we want the point p closest to b = (by,. . .,bn). The key to projection is orthogonality: The line from b to p is perpendicular to the vector a. This is the dotted line marked e = b — p for the error on the left side of Figure 4.3. We now compute p by algebra. 4.2. Projections onto Lines and Subspaces 153 The projection p will be some multiple of a. Call it p = Ta = “z hat” times a. Computing this number T will give the vector p. Then from the formula for p, we will read off the projection matrix P. These three steps will lead to all projection matrices : First find , then find the vector p, then find the matrix P. The dashed line b — p is the “error” e = b — Za. It is perpendicular to a—this will determine Z. Use the fact that b — Ta is perpendicular to a : their dot product is zero. Projecting b onto a with errore = b — Ta - a-b aTb 2 r=——=——. a-(b—xa)=0 or a-b—za-a=0 a-a a'a The multiplication a®d is the same as a - b. Using the transpose is better, because it applies also to matrices. Our formula £ = aTb/aa gives the projection p = Za. b b \\ \\ eIror pzAi ‘e=b-p = A(ATA)—IATb \\ a = Pb P aTh 6 D p—ma—ma Figure 4.3: The projection p of a vector b onto a line and onto S = column space of A. T The projection of b onto the line through a is the vector p = Ta = —a—,-rﬁ a. a‘a Special case 1: If b = a then T = 1. The projection of a onto a is itself. Pa = a. Special case 2: If bis perpendicular to a then a™b = 0. The projection is p = 0. p- - - - 1 ontoa= | 2 | tofind p = Za in Figure 4.3. | 2 - . Example 1 Projectb = puud pumd pumad - : C 3 Solution The number Z is the ratioof aTh = 5to aTa = 9. So the projection is p = —a. The error vector between b and p is e = b — p. Those vectors p and e will add to b=(1,1,1): _5_§1_919) and e=b-p=[2-L_1 P=9% 9979 —PTPT 9T Ty The error e should be perpendicularto a = (1,2,2) anditis: eTa =3 - 2 - £ = 0. 154 Chapter 4. Orthogonality Look at the night triangle of b, p, and e. The vector b is split into two parts—its component along the line is p, its perpendicular part is e. Those two sides p and e have length ||p|| = ||b]| cos @ and ||e|| = ||b]| sin 8. Trigonometry matches the dot product: T ab a|l ||b|| cos @ p=——a haslength ||p||= | ”|l||a||2 ala The dot product is a lot simpler than getting involved with cosf and the length of b. The example has square roots in cos§ = 5/3v/3 and ||b]| = /3. There are no square roots in the projection p = 5a/9. The good way to 5/9is a'b/aa. lall =||bl| cos 6. 3) Now comes the projection matrix. In the formula for p, what matrix is multiplying b? You can see the matrix better if the number Z is on the right side of a: Projection a’d aaT matrix P p=azr= am = Pb when the matrixis P = Tg P is a column times a row! The column is a, the row is a*. Then divide by the number aTa. The projection matrix P is m by m, but its rank is one. We are projecting onto a one-dimensional subspace, the line through a. That line is the column space of P. T 1 Example 2 Find the projection matrix P = %%; onto the line througha = | 2 Solution Multiply column a times row a® and divide by aTa = 9: T [ (1 2 2 Projection matrix p=2 \"1 (122]=-(2 4 4 ala 9 9 2 2 4 4 This matrix projects any vector b onto a. Check p = Pbforb = (1,1,1) in Example 1: [1 2 2)[1] [ 8 1] (5] ([ ¢ p=Pb=§ 2 4 4|1 =3 10| witherrore=b-p=|1 -9 10|=-|-1]. 2 4 4)[1] 7|10 1] 7lw0] 9[-1 If the vector a is doubled, the matrix P stays the same ! It still projects onto the same line. If the matrix is squared, P? equals P. Projecting a second time doesn’t change anything, so P? = P. The diagonal entries of P addupto 3(1+4+4) = 1. Ande'p = 0. The matrix I — P should be a projection too. It produces the other side e of the triangle. When P projects onto one subspace, I — P projects onto the perpendicular subspace. Then I — P projects onto the plane perpendicular to a—the orthogonal complement. Now we move beyond projection onto a line. Projecting onto an n-dimensional subspace of R™ takes more effort. The crucial formulas will be collected in equations (5)~6)7). Basically you need to remember those three equations. 4.2. Projections onto Lines and Subspaces 155 Projection Onto a Subspace Start with n vectors ay,...,a, in R™. Assume that these a’s are linearly independent. Problem: Find the combination p = T,a, + --- + T,a, closest to a given vector b. We are projecting each b in R™ onto the n-dimensional subspace spanned by the a’s. With n = 1 (one vector a,) this is projection onto a line. The line is the column space of A, which has just one column. In general the matrix A has n columns a,,...,a,. The combinations in R™ are the vectors Az in the column space. We are looking for the particular combination p = AZ (the projection) that is closest to b. The hat over indicates the best choice Z, to give the closest vector in the column space. That choice is T =a'/aTawhenn = 1. Forn > 1, the best £ = (Z,,..., Zn) is to be found now. We compute projections onto n-dimensional subspaces in three steps as before: Find the vector T in (S), the projection p = AZ in (C), and the projection matrix P in (T). The key is in the geometry! The dotted line in Figure 4.3 went from b to the nearest point AZ in the subspace. This error vector b — AZ is perpendicular to the subspace. The error b — AT makes a right angle with all the vectors a,....a, In the subspace. Those n right angles give n equations for T : aT(b—Ai)zo '_a'lr_'r 1 I or : b—Az|=10| or ATAZ=A\"b. () ay(b- Az)=0 —a) — J L e The matrix with those rows aT is AT. The n equations are exactly AT(b — AZ) = 0. Rewrite AT(b — AZ) = 0 in its famous form ATAZ = A\"b. This is the equation for Z, and the coefficient matrix is AT A. Now we can find Z and p and P, in that order. The next three equations (5), (6), (7) are the keys to projection. The combination p = 1@, + - - - + Znan = AZ that is closest to b comes from T : Findz(nx1) AT(b—AZ)=0 or ATAZ = A\"b. (5) This symmetric matrix ATA is n by n. It is invertible if the a’s are independent. The solution is Z = (AT A)~1 ATb. The projection of b onto the subspace is p: Find p (m x 1) p=AZ = A(ATA)\"1ATb. (6) The projection matrix P is multiplying b in (6). It has four A’s: Find P (m x m) P=AATA) AT (7) Compare with projection onto a line, when A has only one column: ATA=aTa is 1 by 1. 156 Chapter 4. Orthogonality . a'b a'b aa’ Forn=1 r=— ad p=a—— and P=—F. a'a a'a a'a (8) Those formulas are identical with (5) and (6) and (7). The number a®a becomes the matrix ATA. When it is a number, we divide by it. When it is a matrix, we invert it. The linear independence of the columns a,,...,a, guarantees that AT A is invertible. The key step was AT(b — AZ) = 0. We used geometry (e is orthogonal to each a). Linear algebra gives this “normal equation” too, in a very quick and beautiful way: 1. Our subspace C(A) is the column space of A. 2. The error e = b — AZ is in the perpendicular subspace N(AT). This means AT (b — AZ) = 0. The left nullspace N(AT) is important in projections. That nullspace contains the error vector e = b — AZ. The vector b is split into the projection p and the error e = b — p. Projection produces a right triangle with sides p, e, and b. (1 0 KRN Example3 IfA=|1 1 |andb=] 0 | findZ and p and P. -1 2d -0- Solution Compute the square matrix AT A and also the vector AT b: 1 0] 6 T, |1 11 13 3 . |1 1 1 _ 16 AA‘[Olz]i;“Lsa“d’“\"0128‘0 Now solve the normal equation ATAZ = ATb to find Z: B R 1 4 FR The combination p = AT is the projection of b onto the column space of A: 1] [o] [ 5 1 p=5|1|-3|1|=]| 2|. Theerroris e=b—p= |-2]. (10) 1) 2] |1 L L Two checks on the calculation. First, the error e = (1, —2, 1) is perpendicular to both columns (1,1,1) and (0, 1,2). Second, the matrix P times b = (6,0, 0) correctly gives p = (5,2, —1). That solves the problem for one particular b, as soon as we find P. The projection matrix is P = A(AT A)~! AT. The determinant of ATA is 15— 9 = 6; then its 2 by 2 inverse is easy. Multiply A times (AT A)~! times AT to reach P: 5 2 —1] (A\"'A)“=l o =31 and p=1] 1 2 2]. (11) 61-3 3 6l1-1 2 s We must have P? = P, because a second projection doesn’t change the first projection. 4.2. Projections onto Lines and Subspaces 157 Warning The matrix P = A(AT A)~! AT is deceptive. You might try to split (AT 4)~1 into A~! times (AT)~1. If you make that mistake, and substitute it into P, you will find P = AA~1(AT)~1AT. Apparently everything cancels to produce P = I. We want to say why this is wrong. The matrix A is rectangular. It has no inverse matrix. We cannot split (AT A)~! into A~ ! times (AT)~! because there is no A~! in the first place, when m > n. In our experience, a problem that involves a rectangular matrix almost always leads to ATA. When A (m by n) has independent columns, AT A is invertible. This fact is so crucial that we state it and prove it again (Section 3.5 proved it first). AT A is invertible if and only if A has linearly independent columns. Proof AT A is a square matrix (n by n). For every matrix A, we will now show that AT A has the same nullspace as A. When the columns of A are linearly independent, its nullspace contains only the zero vector. Then AT A, with this same nullspace, is invertible. Let A be any matrix. If z is in its nullspace, then Az = 0. Multiplying by AT gives AT Az = 0. So « is also in the nullspace of AT A. Now start with the nullspace of AT A. From ATAz = 0 we must prove Az = 0. We can’t multiply by (AT)~!, which generally doesn’t exist. Just multiply by = T: (xT)ATAz =0 or (Az)T(Az)=0 or |Az|*=0. (12) We have shown: If AT Az = 0 then Ax has length zero. Therefore Az = 0. Every vector x in one nullspace is in the other nullspace. If AT A has dependent columns, so has A. If ATA has independent columns, so has A. This is the good case: AT 4 is invertible. When A has independent columns, AT A is square, symmetric, and invertible. To repeat for emphasis: AT A is (n by m) times (m by n). Then AT A is square (n by n). It is symmetric, because its transpose is (AT A)T = AT(AT)T which equals AT A. We just proved that AT A is invertible—provided A has independent columns. Watch the difference between dependent and independent columns: AT A ATA AT A ATA L R [110]13_[24] [110]13_[24] 220-00J48 221.”% 19 dependent singular indep. invertible very brief summary To find the projection p = @1+ - - + Tn@n, solve ATAZ = ATb. This gives . The projection is p = AT and the errorise = b— p = b — Az. The projection matrix P = A(ATA)~! AT gives p = Pb. P is invertible only if P = I. This matrix satisfies P> = P. The distance from b to the subspace C(A) is ||e]|. 158 Chapter 4. Orthogonality ® REVIEW OF THE KEY IDEAS = 1. The projection of b onto the line through a is p = aZ = a(a'b/aTa). 2. The rank one projection matrix P = aa’/aTa multiplies b to produce p. 3. Projecting b onto a subspace leaves e = b — p perpendicular to the subspace. 4. When A has full rank n, the equation ATAZ = ATbleads to T and p = AZ. 5. The projection matrix P = A(ATA)\"' AT has PT = P and P? = P and Pb = p. ® WORKED EXAMPLES = 4.2 A Project the vector b = (3,4,4) onto the line through a = (2,2,1) and then onto the plane that also contains a* = (1,0,0). Check that the first error vector b — p is perpendicular to a, and the second error vector e* = b — p* is also perpendicular to a*. Find the 3 by 3 projection matrix P onto that plane of a and a*. Find a vector whose projection onto the plane is p = 0. Solution The projection of b = (3, 4, 4) onto the line througha = (2,2.1) is p = 2a: , a™ 18 Onto a line P=_7,0= 3(2,2, 1) =(4,4,2) = 2a. The error vectore = b — p = (-1, 0, 2) is perpendicular to a = (2,2, 1). So p is correct. The plane of @ = (2,2,1) and a* = (1,0, 0) is the column space of A = [a a*]: 2 1 ‘1 0 0] 1 - A=1|2 0 ATA=[3 f] (ATA)“=5[_; g] P=]|0 8 4 1 0] 0 4 2 Now p* = Pb = (3,4.8,2.4). The errore®* = b — p* = (0, —.8, 1.6) is perpendicular to a and a*. This e is in the nullspace of P. Its projection is zero! Note P2 = P = PT. 4.2 B Suppose your pulse is measured at 70 beats per minute, then at z = 80, then at z = 120. Those three equations Az = b in one unknown have AT = [1 1 1] and b = (70,80, 120). The best x is the of 70,80, 120. Use calculus and projection: 1. Minimize E = (z — 70)? + (z — 80)? + (z — 120)? by solving dE'/dz = 0. 2. Project b = (70,80,120) onto @ = (1,1, 1) to find T = average = 90. 4.2C Suppose you know the average T4 of by, bo, ..., bgge. When bjog0 arrives, check that the new average is a combination of Z,)4 and the mismatch by000 — Zolq : ~ bi+---+booo b1+ + bogg 1 (b b1+\"°+5999) = 1000 — . Tnew = —050 999 T 1000 999 This is a “Kalman filter” Zpew = Zold + 1o65 (01000 — Told) With gain matrix 5. 4.2. Projections onto Lines and Subspaces 159 Problem Set 4.2 0 A projection satisfies P2 = P, The extra symmetry condition PT = P makes it an “‘orthogonal projection”. Examples will show the meaning of those words. 1 (a) Verify P2=Pand (I—P)2 =I—PforP=[g l]andI_Pz[é _(1)] , (b) Find vectors v and w in the column spaces of P and I — P. Are those spaces orthogonal ? (c) Show that if P2 = P and PT = P then the dot product of Pv with (I — P)w is zero: v PT(I — P)w = 0. Now the projection is orthogonal. Summary For the projection p = Pb to be perpendicular to the errore = b — p, we need P2 = P = PT, Questions 1-9 ask for projections p onto lines. Also errors e = b — p and matrices P. 1 Project the vector b onto the line through a. Check that e is perpendicular to a: il i i 1 (@) b= ]2 and a=]1 b) b={3 and a=|-3 LQJ .1. .IJ L_ld Draw the projection of b onto a and also compute it from p = Za: 1 1 (@A) b= [Z?r?g] and a = [(1)] (b) b= [1] and a = [_1] : In Problem 1, find the projection matrix P = aa’/a”a onto the line through each vector a. Verify in both cases that P? = P. Multiply Pb in each case to compute the projection p. Construct the projection matrices P; and P, onto the lines through the a’s in Prob- lem 2. Is it true that (P, + P,)? = P, + P,? This would be true if P, P, = 0. Compute the projection matrices aa”/ aTa onto the lines through a; = (—1.2,2) and @y = (2,2, —1). Multiply those projection matrices and explain why their prod- uct P, P, is what it is. Project b = (1,0,0) onto the lines through @, and a; in Problem 5 and also onto a3 = (2, —1,2). Add up the three projections p, + p, + P3. Continuing Problems 5-6, find the projection matrix P3 onto a3z = (2, —1, 2). Verify that P; + P, + P3 = I. This is because the basis a;, a2, a3 is orthogonal! Project the vector b = (1,1) onto the lines through a; = (1,0) and a2 = (1,2). Draw the projections p, and p, and add p, + p,. The projections do not add to b because the a’s are not orthogonal. 160 Chapter 4. Orthogonality Questions 5-6-7: orthogonal a’s Questions 8-9-10: not orthogonal 9 In Problem 8, the projection of b onto the plane of a, and a; will equal b. Find P=A(ATA)\"'ATfor A=[a; az]=[}1] = invertible matrix. 10 Project a; = (1,0) onto a; = (1,2). Then project the result back onto a,. Draw these projections and multiply the projection matrices P, P»: Is this a projection? Questions 11-21 ask for projections, and projection matrices, onto subspaces. 11 If P (m by n) projects R™ onto a subspace S, what are the four fundamental subspaces for the matrix P ? 12 Project b onto the column space of A by solving ATAZ = ATb and p = AZ: 1 1] (2] 1 1 4] (@ A=1]0 1| and b= |3 b)) A=1]1 1| and b= |4 0 0 4 0 1 6 Find e = b — p. It should be perpendicular to the columns of A. 13 Compute the projection matrices P; and P, onto the column spaces in Problem 12. Verify that P, b gives the first projection p,. Also verify P = P;. 14 (Quick and Recommended) Suppose A is the 4 by 4 identity matrix with its last column removed. A is 4 by 3. Project b = (1,2, 3,4) onto the column space of A. What shape is the projection matrix P and what is P? 15 Suppose b equals 2 times the first column of A. What is the projection of b onto the column space of A? Is P = I for sure in this case? Compute p and P when b = (0,2,4) and the columns of A are (0,1, 2) and (1, 2,0). 16 If Ais doubled, then P = 2A(4ATA)~12AT. This is the same as A(AT A)~1AT. The column space of 24 is the same as . Is Z the same for A and 2A? 4.2. Projections onto Lines and Subspaces 161 17 18 19 20 21 What linear combination of (1,2, —1) and (1,0, 1) is closest to b = (2,1,1)? (Important) If P2 = P show that (I — P)?2 = I — P.When P projects onto the column space of A, I — P projects onto the (a) If P is the 2 by 2 projection matrix onto the line through (1,1),thenI — P is the projection matrix onto (b) If P is the 3 by 3 projection matrix onto the line through (1,1,1), then I — P is the projection matrix onto To find the projection matrix onto the plane r — y — 22 = 0, choose two vectors in that plane and make them the columns of A. The plane will be the column space of A! Then compute P = A(ATA)~ 14T, To find the projection matrix P onto the same plane r — y — 2z = 0, write down a vector e that is perpendicular to that plane. Compute the projection Q = ee’/ele andthen P =1 — Q). Questions 22-27 show that projection matrices satisfy P? = Pand PT = P. 22 23 24 25 27 28 29 30 Multiply the matrix P = A(ATA)~1AT by itself. Cancel to prove that P2 = P. Explain why P(Pb) = projection of Pb always equals Pb. Prove that P = A(ATA)~'AT is symmetric by computing PT. Remember that the inverse of a symmetric matrix is symmetric. If A is square and invertible, the warning against splitting (AT A)~! does not apply. It becomes true that AA='(AT)\"1AT = I. When A is invertible, why is P = I? What is the error e? The nullspace of AT is to the column space C(A4). So if ATb = 0, the projection of b onto C(A) should be p = . Check that A(ATA)\"1ATY = p. The projection matrix P onto an n-dimensional subspace of R™ has rank r = n. Reason : The projections Pb fill the subspace S. So Sisthe ____ of P. If an m by m matrix has A2 = A and its rank is m, how do you know that A = I ? The important fact that ends the section is this: If ATAz = O then Az = 0. New Proof: The vector Az is in the nullspace of . Az is always in the column Space of ____ . To be in both of those perpendicular spaces, Az must be zero. Use PT = Pand P2 = Pto prove that the length squared of column 2 of P always €quals the diagonal entry Py,. Give an example. If B has rank m (full row rank, independent rows) show that BBT is invertible. 162 Chapter 4. Orthogonality Challenge Problems 31 (a) Find the projection matrix Pc onto the column space of this matrix A: 3 6 6 A_[488] (b) Find the 3 by 3 projection matrix Pg onto the row space of A. Multiply B = Pc APg. Your answer B should be a little surprising—can you explain it? 32 InR™, suppose I give you b and also a combination p of a,,...,a,. How would you test to see if p is the projection of b onto the subspace spanned by the a’s? 33 Suppose P, is the projection matrix onto the 1-dimensional subspace spanned by the first column of A. Suppose P; is the projection matrix onto the 2-dimensional column space of A. After thinking a little, compute the product P, P;. (1 A= 0 - 1. l -l 2 0 34 Suppose P, and P; are projection matrices (P2 = P; = PT). Prove this fact : Py P, is a projection matrix if and only if P, P, = P, P;. 4.3. Least Squares Approximations 163 4.3 Least Squares Approximations 1 Solving| ATAZ = ATb| gives the projection p = AZ of b onto the column space of A. \\ 2 When Az = b has no solution, T is the “least-squares solution” : ||AZ — b||?> = minimum. 3 Setting derivatives of E = ||Az — b||? to zero (S—E = O) also produces ATAT = ATb. Ty 4 Tofitpoints (¢1,b1),. .., (tm,bm) by astraight line, A has columns (1,...,1) and (¢,...,tm). T . . m Ett T : zbt (In that case A\" A is the 2 by 2 matrix [ st T ] and A\" b is the vector[ Sth | J It often happens that Ax = b has no solution. The usual reason is : 00 many equations. The matrix A has more rows than columns. There are more equations than unknowns (m is greater than n). The n columns span a small part of m-dimensional space. Unless all measurements are perfect, b is outside that column space of A. Elimination reaches an impossible equation and stops. But we can’t just stop when measurements include noise ! To repeat: We cannot always get the error e = b — Ax down to zero. When e is zero, x 1s an exact solution to Ax = b. When the length of e is as small as possible, so that ||b— AZ||? reaches its minimum, then T is a least squares solution. Our goal in this section is to compute T and use it. These are real problems and they need an answer. The previous section emphasized p (the projection). This section emphasizes T (the least squares solution). They are connected by p = Ax. The fundamental equation is still ATAZ = ATb. Here is a short unofficial way to reach this “normal equation” When Az = b has no solution, multiply by AT and solve ATAZ = ATb. Example 1 A crucial application of least squares is fitting a straight line to m points. Start with three points: Find the closest line to the points (0,6), (1,0), and (2, 0). No straight line b = C + Dt goes through those three points. We are asking for two numbers C and D that satisfy three equations: n = 2 and m = 3. Here are the three equations at t = 0, 1, 2 to match the given values b = 6,0, 0: t=20 The first point is on the line b = C + Dt if C+D-0=6 t=1 The second pointisonthelineb=C+ Dtif C+D-1=0 t =2 The third point is on the line b = C + Dt if C+D-2=0. 164 Chapter 4. Orthogonality This 3 by 2 system has no solution: b = (6,0,0) is not a combination of the columns (1,1,1) and (0,1, 2). Read off A, z, and b from those equations: - - - T = [C] b= 10 Az = b is not solvable. 1 L D N = O 12 The same numbers were in Example 3 in the last section. We computed £ = (5, -3). Those numbers are the best C and D, so 5 — 3t will be the best line for the 3 points. We must connect projections to least squares, by explaining why AT Az = ATb. In practical problems, there could easily be m = 100 points instead of m = 3. They don’t exactly match any straight line C + Dt. Our numbers 6, 0, 0 exaggerate the error so you can see €, €3, and ej in Figure 4.6. Minimizing the Error How do we make the error e = b — Ax as small as possible? This is an important question with a beautiful answer. The best z (called ) can be found by geometry (the error e meets the column space of A at 90° ). The key comes from algebra: ATAZ = ATb. Calculus gives the same Z : the derivative of the error | Az — b||° is zero at Z. By geometry Every Az lies in the plane of the columns (1,1,1) and (0, 1,2). In that plane, we look for the point closest to b. The nearest point is the projection p. The best choice for AT is p. The smallest possible error is e = b — p, perpendicular to the columns of A. The three points at heights (pi, p2, p3) do lie on a line, because p is in the column space of A. In fitting a straight line, = (C, D) is the best choice. By algebra Every vector b splits into two parts. The part in the column space is p. The perpendicular part is e. There is an equation we cannot solve (Ax = b). There is an equation AZ = p we can and do solve (by removing e and solving ATAZ = ATb): Az =b=p+e isimpossible @ AZ =p issolvable z is (ATA)\"1ATb. (1) The solution to AZ = p leaves the least possible error (which is e): Squared error for any x |Az - b||? = || Az - p||® + ||| (2) This is the law ¢ = a? + b? for a right triangle. The vector Az — p in the column space is perpendicular to e in the left nullspace. We reduce Ax — p to zero by choosing = = Z. That leaves the smallest possible error e = (e), €2, e3) which we can’t reduce. Notice what “smallest” means. The squared length of Az — b is minimized: The least squares solution T makes E = || Ax — b||? as small as possible. Figure 4.6a shows the closest line. It misses by distances e;,ez,e3 = 1,-2,1. Those are vertical distances. That least squares line minimizes E = €2 + €2 + €2. 4.3. Least Squares Approximations 165 Figure 4.6b shows the same problem in 3-dimensional space (b p e space). The vector b is not in the column space of A. That is why we could not solve Az = b. No line goes through the three points. The smallest possible error is the perpendicular vector e. This is e = b — Az, the vector of errors (1, —2, 1) in the three equations. Those are the distances from the best line. Behind both figures is the fundamental equation AT AZ = ATb. best line b =5- 3¢ b, p, e at 3 points p2 =2 b A €2 = —2 b3 =0 b= > 4183 = |1 t by=0 p3 = -1 errors = vertical distances to line errors e = (1,-2,1)=b—p Figure 4.6: Best line and projection: Two pictures, same problem. The line has heights p = (5,2, —1)witherrorse = (1, -2, 1). The equations ATAZ = ATbgive T = (5. -3). Same answer! The best line is b = 5 — 3t and the closest pointis p = 5a; — 3a>. Notice that the errors 1, —2,1 add to zero. Reason: The error e = (e;,€2.€3) 1s perpendicular to the first column (1,1,1) in A. The dot product gives e, + €2 + ez = 0. By calculus Most functions are minimized by calculus! The graph bottoms out and the derivative in every direction is zero. Here the error E to be minimized is a sum of squares e + e2 + €3 (the squares of the errors in three equations): E=|Az-b|>?=(C+D-0-62+(C+D-1)*+(C+D-2)>° (3) The unknowns are C and D. With two unknowns there are two derivatives—both zero at the minimum. They are “partial derivatives” because OE'/9C treats D as constant and OF /0D treats C as constant. Here are the derivatives of E in (3): dE/8C =2(C+D-0-6) +2(C+D-1) +2(C+D-2) =0 8E/8D =2(C + D -0-6)(0) +2(C+ D-1)(1) +2(C + D -2)(2) = 0. OFE /0D contains the extra factors 0,1, 2 from the chain rule. (The last derivative from (C + 2D)? was 2 times C + 2D times that extra 2.) Those factors are not in OE/9C. 166 Chapter 4. Orthogonality It is no accident that those factors 1, 1, 1 and 0, 1, 2 in the derivatives of || Az — b||? are the columns of A. Now cancel 2 from every term and collect all C’s and all D’s: The C denvative is zero: 3C +3D =6 . : 3 3. T The D derivative is zero: 3C + 5D =0 This matrix [ 3 5 ] sA°A @) Those two equations are ATAT = ATb. The best C and D are the components of Z. The equations from calculus are the same as the “normal equations™ from linear algebra. These are the equations to minimize ||Az — b||2 = zTAT Az — 22TATb + b'b: The partial derivatives of ||Azx — b||? are zero when AT AZ = ATb. The solutionis C = 5 and D = —3. Therefore b = 5 — 3t is the best line—it comes closest to the three points. Att = 0, 1, 2 this line goes through p = 3, 2, —1. It could not go through b = 6, 0, 0. The errors are 1, —2, 1. This is the vector e! The Big Picture for Least Squares The key figure of this book shows the four subspaces and the true action of a matrix A. The vector x on the left side of Figure 4.1 went to b = Ax on the right side. In that figure every x was split into &, + ,,. There were many solutions to Az = b. In this section the situation is just the opposite. There are no solutions to Ax = b. Instead of splitting up x we are splitting up b = p + e. Figure 4.7 shows the big picture for least squares. Instead of Az = b we solve Az = p. The error e = b— p is unavoidable. column space solvable for p inside R\"* p is in the column space row space is all of R™ best Z AZ=0p p= Pbis nearestto b not solvable for b _(\\b =p+e b is not in the column space \\\\ m>n 0 \\ i € = minimum error nullspace -~ of AT Independent columns in A Nullspace = zero vector only Figure 4.7: The projection p = AZ is closest to b, so Z minimizes E = ||b — Az]*. Notice how the nullspace N(A) is very small—just one point. With independent columns, the only solution to Az = 0is 2 = 0. Then AT A is invertible. The equation ATAZ = ATb fully determines the best vector Z. The error e = b — p has ATe = 0. 167 4.3. Least Squares Approximations Chapter 7 will have the complete picture—all four subspaces included. Every x splits into Tr + &n, and every b splits into p + e. The best solution is T = Zr in the row space. We can’t help e and we don’t want x5, from the nullspace. This leaves AT = p. Fitting a Straight Line Fitting a line is the clearest application of least squares. It starts with m > 2 points, hopefully near a straight line. At times t,,...,t,, those m points are at heights bi,...,b,n. The best line C + Dt misses the points by vertical distances e, .. No line is perfect, and the least squares line minimizes E = % + - - - + €2,. The first example in this section had three points in Figure 4.6. Now we allow m points (and m can be large). The two components of Z are still C and D. A line goes through the m points when we exactly solve Az = b. Generally we can’t do it. Two unknowns C and D determine a line, so A has only n = 2 columns. To fit the m points, we are trying to solve m equations (and we only have two unknowns!). .y €m. C+Dt1=b1 1 tl. _ 1t Az =b is C+l?t2—b2 with A= N E (3) C + Dty = b 1t The column space is so thin that almost certainly b is outside of it. When b happens to lie in the column space, the points happen to lie on a line. In that case b = p. Then Az = b is solvable and the errors are e = (0, ...,0). The closest line C + Dt has heights py,...,Ppm with errors e;,...,€em. Solve ATAzZ = ATb forz = (C, D). The errorsare e; = b; — C — Dt,. Fitting points by a straight line is so important that we give the two equations ATAZ = ATb, once and for all. The two columns of A are independent (unless all times ¢; are the same). So we turn to least squares and solve ATAZ = ATb. 1 ¢ Zt 1 ... 1 m ‘ ; . TA — . . = . (6) Dot-product matrix A* A [tl tm] 1 t. [Zti Zt?J b md On the right side of the normal equation is the 2 by 1 vector ATb: D ATA [C] = ATAZ =ATb = [ 1 th 1 tm | bm - by >_bi Y t:b ]. o In a specific problem, these numbers are given. The best Z = (C, D) is (ATA)~1ATb. 168 Chapter 4. Orthogonality The line C + Dt minimizes e? + - -- + €2, = || Az — b||2 when ATAZ = ATb: 5)-[25) Y _t.b; The vertical errors at the m points on the line are the components of e = b — p. This error vector (the residual b — AZ) is perpendicular to the columns of A (geometry). The error is in the nullspace of AT (linear algebra). The best Z = (C, D) minimizes the total error E|, the sum of squares (calculus): E(z) = ||Az - b||>=(C+ Dt; - b))+ --- + (C + Dt,, — b)>. Calculus sets the derivatives 3E/0C and 0E /0D to zero, and recovers ATAZ = ATb. Other least squares problems have more than two unknowns. Fitting by the best parabola has n = 3 coefficients C, D, E (see below). In general we are fitting m data points by n parameters 1,,...,Z,. The matrix A has n columns and n < m. The dernivatives of || Az — b||? give the n equations ATAZ = ATb. The derivative of a square is linear. This is one reason why the method of least squares is so popular. m Zt,’ St T ® ATAZ = ATb [ Example 2 A has orthogonal columns when the measurement times ¢; add to zero. Suppose b = 1,2,4 attimes t = —2,0, 2. Those times add to zero. The columns of A have zero dot product: (1,1,1) is orthogonal to (-2, 0, 2): - p - C+D(-2)=1 1 -2 C C+ D(0)=2 or Ax=|1 O [D]= C+ D(2) =4 _1 2_ L4- When the columns of A are orthogonal, AT A will be a diagonal matrix : T 4a _ AT : 3 01[C] |7 A\"AZ=A'b is [0 sl 1pl=lel 9) Main point: Since AT A is diagonal, we can solve separately for C = £ and D = . The zeros in AT A are dot products of perpendicular columns in A. The diagonal matrix AT A, with entries m = 3 and t2 + t3 + t5 = 8, is virtually as simple as the identity matrix. Orthogonal columns are so helpful that it can be worth shifting the times by subtracting the average time t = (t; + --- + tm)/m._If the original times were 1,3,5 then their average is t = 3. The shiftedtimes T =t —t =t — 3 add up to zero! ' Ty=1-3=-2 (1 T, 3 0 T,=3-3= 0 Aew =11 T, AIewAnew=[0 8]. T3=5-3= 2 1 T3 Now C and D come from the easy equation (9). Then the best straight line uses C + DT whichis C + D(t -t ) = C + D(t — 3). Problem 30 even gives a formula for C and D. 4.3. Least Squares Approximations 169 That was a perfect example of the “Gram-Schmidt idea” coming in the next section: Make the columns orthogonal in advance. Then Al A, is diagonal and Zw is easy. Dependent Columns in A: What is T ? From the start, this chapter has assumed independent columns in A. Then AT A is invert- ible. Then AT AZ = ATb produces the least squares solution Z to Ax = b. Which Z is best if A has dependent columns ? Here is a specific example. 1 1] [z 3 1 1] [z 2 b1=3-‘t\\-, HE R RO L E R H R R s b2=1\" g ; b The measurements b; = 3 and b, = 1 are at the same time T'. A straight line C' + Dt cannot go through both points. I think we are right to project b = (3,1) to p = (2,2) in the column space of A. That changes the equation Az = b to the equation Az = p. An equation with no solution has become an equation with infinitely many solutions. The problem is that A has dependent columns and (1, —1) is in its nullspace. Which solution Z should we choose? All the dashed lines in the figure have the same two errors 1 and —1 at time T. Those errors (1, —1) = e = b — p are as small as possible. But this doesn’t tell us which dashed line is best. My instinct is to go for the horizontal line at height 2. If the equation for the best line is b = C + Dt, then my choice will have ; = C = 2and T, = D = 0. But what if the line had been written as b = ct + d? This is equally correct (just reversing C' and D). Now the horizontal line has Z; = ¢ = 0 and Z; = d = 2. I don’t see any way out. In Section 4.5, the “pseudoinverse” of A finds the shortest solution to Az = p. Here, that shortest solution will be ¥ = (1, 1). This is the particular solution in the row space of A, and t has length v/2. (Both solutions Z = (2,0) and (0, 2) have length 2.) We are choosing the nullspace component of the solution x* to be zero. If A has independent columns, the £+ pseudoinverse is our usual left inverse (AT A)~1AT. When I write it that way, the pseudoinverse sounds like the best way to choose Z. Comment MATLAB experiments with singular matrices produced either Inf or NaN (Not a Number) or 102¢ (a bad number). There is a warning in every case! I believe that Inf and NaN and 10'® come from the possibilities 0x = b and 0z = 0 and 10~z = 1. Those are three small examples of three big difficulties: singular with no solution, singular with many solutions, and very very close to singular. 170 Chapter 4. Orthogonality Fitting by a Parabola If we throw a ball, it would be crazy to fit the path by a straight line. A parabola b = C + Dt + Et? allows the ball to go up and come down again (b is the height at time ¢). The actual path is not a perfect parabola, but the whole theory of projectiles starts with that approximation. When Galileo dropped a stone from the Leaning Tower of Pisa, it accelerated. The distance contains a quadratic term % gt2. (Galileo’s point was that the stone’s mass is not involved.) Without that t? term we could never send a satellite into its orbit. But even with a nonlinear function like ¢2, the unknowns C, D, E still appear linearly! Fitting points by the best parabola is still a problem in linear algebra. Problem Try to fit heights by, ..., by, at times t,...,t, by a parabola C + Dt + Et2. Solution With m > 3 points, the m equations for an exact fit are generally unsolvable: C+Dt1+Et2=b1 - 2 : l is Az = b with 1 t.l t.l . A=|: : 1. (10) ' the m by 3 matnx 1 ¢+ C + Dt + Et2, = by L tm Uy Least squares The closest parabola C + Dt + Et? chooses £ = (C,D,E) to satisfy the three normal equations AT AZ = ATb. May I ask you to convert this to a problem of projection? The column space of A has dimension . The projection of bis p = AZ, which combines the three columns using the coefficients C, D, E. The error at the first data pointis ¢, = b — C — Dt, — Et3. If you prefer to minimize by calculus, take the partial derivatives of e? +---+e2 with respect to , \\ . These three derivatives will be zero when = (C, D, E) solves the 3 by 3 system of equations AT AZ = ATb. Fourier series is least squares in infinite dimensions—approximating functions instead of vectors. The function to be minimized changes from a sum of squared errors to an integral of the squared error. Example 3 Foraparabolab = C + Dt + Et? to go through the three heights b = 6,0,0 when t = 0, 1, 2, the equations for C, D, E are ' C+D-0+E-0*=6 C+D-1+E-1*=0 (11) C+D-2+E.2*=0. This is Az = b. We can solve it exactly. Three data points give three equations and a square matrix. The solution is £ = (C, D, E) = (8, —9, 3). The parabola through the three points is b = 6 — 9t + 3t3. 4.3. Least Squares Approximations 171 ® WORKED EXAMPLES = 4.3 A Start with nine measurements b; to by, all zero, at timest = 1,...,9. The tenth measurement by;g = 40 is an outlier. Find the best horizontal line y = C to fit the ten points (1,0),(2,0),...,(9,0),(10,40) using three options for the error E' (1) Least squares E; = e + --- + €2, (then the normal equation for C is linear) (2) Least maximum error E = |emax| (3) Least sum of errors E} = |e1| + - - - + |ejo]- Solution (1) The least squares fit to 0,0,...,0,40 by a horizontal line is C = 4: A =columnofl’s ATA=10 ATb=sumofb; =40. So10C = 40. (2) The least maximum error requires C' = 20, halfway between 0 and 40. (3) The least sum requires C = 0 (!!). The sum of errors 9|C| + |40 — C| would increase if C' moves up from zero. The least sum comes from the median measurement (the median of 0. . . ., 0,40 is zero). Many statisticians feel that the least squares solution is too heavily influenced by outliers like b;o = 40, and they prefer least sum. But the equations become nonlinear. Now find the least squares line C + Dt through those ten points (1.0) to (10. 40): o[ BB ) w[E) [ Those come from equation (8). Then ATAZ = ATb gives C = —8 and D = 24/11. What happens to C and D if you multiply b = (0,0,...,40) by 3 and then add 30 to get bpew = (30,30, ...,150)? Linearity allows us to rescale b. Multiplying b by 3 will multiply C and D by 3. Adding 30 to all b; will add 30 to C. 4.3 B Find the parabola C + Dt + Et? that comes closest (least squares error) to the values b = (0,0,1,0,0) atthe timest = —2, —1,0, 1, 2. First write down the five equations Az = b in three unknowns x = (C, D, E) for a parabola to go through the five points. No solution because no such parabola exists. Solve AT AZ = ATb. I would predict D = 0. Why should the best parabola be symmetric around t = 0? In ATAZ = ATb, equation 2 for D should uncouple from equations 1 and 3. Solution The five equations Az = b have a rectangular Vandermonde matrix A : C+D(-2)+E(-2?=0 (1 -2 4] C+D(-1)+ E(-1)!=0 1 -1 1 5 0 10 ] C+D (0O+E (0=1 A=|1 00| ATA=| 0 10 O C+D (1)+E (1))=0 1 11 |10 0 34 C+D (2+FE (2°*=0 1 2 4] 172 Chapter 4. Orthogonality Those zeros in AT A mean that column 2 of A is orthogonal to columns 1 and 3. We see this directly in A (the times —2,-1.0, 1, 2 are symmetric). The best C, D, E' in the parabola C + Dt + Et? come from ATAZ = ATb, and D is uncoupled from C and E: 5 0 10][C] [1] C =31/70 0 10 0 D|=1]0 leadsto D =0 as predicted 10 0 M4 || E] [0 E=-10/70 Problem Set 4.3 Problems 1-11 use four data points b = (0, 8, 8, 20) to bring out the key ideas. b,=20 T €, b = (0,8,8.20) <+ P4 \\\\\\ e \\ b= Dt \\\\ C+ P3 p=Ca; + Da, T “ b2=b3=8\"\" € T Py a; =(0,1,3,4) Py e, a=(,1,11) b,=0 } } { 5 l 6h=0 =1 =3 1,=4 Figure 4.8: Problems 1-11: The closest line C + Dt matches Ca; + Da, in R, 1 With b = 0,8.8,20att = 0,1,3,4, set up and solve the normal equations ATAZ = ATb. For the best straight line in Figure 4.9a, find its four heights p; and four errors e;. What is the minimum value E = €? + €2 + €2 + €2? 2 (Line C + Dt does go through p’s) With b = 0,8,8,20 at times t = 0,1, 3,4, write down the four equations Az = b (unsolvable). Change the measurements to p=1,5,13,17 and find an exact solution to AT = p. 3 Checkthate = b-p = (-1,3,-5,3) is perpendicular to both columns of the same matrix A. What is the shortest distance ||e|| from b to the column space of A? 4 (By calculus) Write down E = | Az — b||? as a sum of four squares—the last one is (C + 4D — 20)2. Find the derivative equations JE/8C = 0 and OE /8D = 0. Divide by 2 to obtain the normal equations AT AZ = ATb. 4.3. Least Squares Approximations 173 10 11 Find the height C of the best horizontal line to fit b = (0,8, 8,20). An exact fit would solve the unsolvable equations C = 0, C = 8, C = 8, C = 20. Find the 4 by 1 matrix A in these equations and solve ATAZ = ATb. Draw the horizontal line at height £ = C and the four errors in e. Project b = (0, 8, 8, 20) onto the line througha = (1,1,1,1). Find T = a'b/aTa and the projection p = Ta. Check that e = b — p is perpendicular to a, and find the shortest distance ||e|| from b to the line through a. Find the closest line b = Dt, through the origin, to the same four points. An exact fit wouldsolve D-0=0,D-1=8,D-3=8,D -4 = 20. Find the 4 by 1 matnx and solve AT AT = ATb. Redraw Figure 4.9a showing the best line b = Dt and the e’s. Project b = (0, 8, 8.20) onto the line through @ = (0,1,3,4). Find T = D and p = Ta. The best C in Problems 5-6 and the best D in Problems 7-8 do not agree with the best (C, D) in Problems 1-4. That is because (1,1,1,1) and (0.1.3.4) are _____ perpendicular. For the closest parabola b = C + Dt + Et? to the same four points, write down the unsolvable equations Az = b in three unknowns £ = (C, D. E). Set up the three normal equations AT AZ = ATb (solution not required). In Figure 4.9a you are now fitting a parabola to 4 points—what is happening in Figure 4.9b? For the closest cubic b = C + Dt + Et? + Ft3 to the same four points, write down the four equations Az = b. Solve them by elimination. In Figure 4.9a this cubic now goes exactly through the points. What are p and e? The average of the four times is t = %(O + 1+ 3+ 4) = 2. The average of the four b’s is b = $(0+8+8+20)=09. (a) Verify that the best line goes through the center point ('t\\, 3) = (2.9). (b) Explain why C + Dt = b comes from the first equation in ATAZ = ATb. Questions 12-16 introduce basic ideas of statistics—the foundation for least squares. 12 13 (Recommended) This problem projects b = (by, ..., bx) onto the line through a = (1,...,1). We solve m equations az = b in 1 unknown (by least squares). (a) Solve aTaZ = aTb to show that T is the mean (the average) of the b's. (b) Find e = b — a7 and the variance ||e||? and the standard deviation | e||. (c) The horizontal line b = 3 is closest to b = (1,2,6). Check that p = (3,3,3) is perpendicular to e and find the 3 by 3 projection matrix P. First assumption behind least squares: Ax = b— (noise e with mean zero). Multiply the error vectors e = b — Ax by (ATA)\"1AT to get T — x on the right. The estimation errors T — & also average to zero. The estimate Z is unbiased. 174 14 15 Chapter 4. Orthogonality Second assumption behind least squares: The m errors e; are independent with vari- ance o2, so the average of (b — Az)(b — Ax)T is ?1. Multiply on the left by (ATA)~1AT and on the right by A(ATA)~! to show that the average matrix (Z — z)(Z — )T is 0®(AT A)~!. This is the covariance matrix W in Section 10.2. A doctor takes 4 readings of your heart rate. The best solutiontor = by,...,r = b, is the average T of b;,....bs. The matrix A is a column of 1’s. Problem 14 gives the expected error (T — z)? as 02(ATA)~! = . By averaging, the variance drops from a? to 0% /4. If you know the average Tg of 9 numbers b, ..., bg, how can you quickly find the average I,o with one more number b;o ? The idea of recursive least squares is to avoid adding 10 numbers. What number multiplies Tg in computing T ? ~ _ 1 T10 = j5b10 + Tg = ll—o(bl +---+byo) asin Worked Example 4.2 C. Questions 17-24 give more practice with and p and e. 17 18 19 20 21 22 23 24 Write down three equations for the line b = C + Dt to go through b = 7 att = —1, b=7Tatt=1,and b= 21att = 2. Find the least squares solution £ = (C, D) and draw the closest line. Find the projection p = AZ in Problem 17. This gives the three heights of the closest line. Show that the error vector is e = (2, —6,4). Why is Pe = 0? Suppose the measurements at t = —1, 1,2 are the errors 2, —6,4 in Problem 18. Compute Z and the closest line to these new measurements. Explain the answer: b = (2,-6,4) is perpendicular to so the projectionis p = 0. Suppose the measurements at t = —1,1,2 are b = (5,13, 17). Compute T and the closest line and e. The error is e = 0 because this b is Which of the four subspaces contains the error vector e? Which contains p? Which contains £? What is the nullspace of A? Find the bestline C + Dt to fitb =4,2,-1,0,0 at times t = —2,—1,0,1, 2. Is the error vector e orthogonal to b or p or e or Z? Show that ||e||? equals e™b which equals b'b — pTb. This is the smallest total error E. The partial derivatives of | Az||? with respect to z1,. .., T, fill the vector 2AT Az. The derivatives of 2b\" Az fill the vector 2ATb. So the derivatives of || Az — b]|? are zerowhen 4.3. Least Squares Approximations 25 26 27 28 29 30 175 Challenge Problems What condition on (t1,b1), (t2,b2), (t3,b3) puts those three points onto a straight line ? A column space answer is: (b;, b2, b3) must be a combination of (1, 1, 1) and (t1, t2, t3). Try to reach a specific equation connecting the t’s and b’s. I should have thought of this question sooner! Find the plane that gives the best fit to the 4 values b = (0, 1, 3,4) at the corners (1,0) and (0, 1) and (—1,0) and (0, —1) of a square. The equations C + Dz + Ey = b at those 4 points are Az = b with 3 unknowns * = (C,D,E). What is A? At the center (0, 0) of the square, show that C' + Dz + E'y = average of the b’s. (Distance between lines) The points P = (z,z,z) and Q = (y, 3y, —1) are on two lines in space that don’t meet. Choose z and y to minimize the squared distance | P — Q||?. The line connecting the closest P and () is perpendicular to Suppose the columns of A are not independent. How could you find a matrix B so that P = B(BT B)~! BT does give the projection onto the column space of A? (The usual formula will fail when AT A is not invertible.) Usually there will be exactly one hyperplane in R™ that contains the n given points x = 0,aq,...,a,_,. (Example for n = 3: There will be one plane containing 0,a;, a; unless .) What is the test to have exactly one plane in R™? Example 2 shifted the times ¢; to make them add to zero. We subtracted away the average timet = (¢; + --- + t,n)/mto get T; = t; — t. Those T; add to zero. With the columns (1,...,1) and (T3, ..., Trm) now orthogonal, AT A is diagonal. Its entries are m and T2 + - - - + T2. Show that the best C and D have direct formulas: by +---+bm hT,+ - +bnThm . _A — d — Tist —t C — an Tf+---+T3, The best line is C + DT or C + D(t — t ). The time shift that makes AT A diagonal is an example of the Gram-Schmidt process: orthogonalize the columns of A in advance. This is the subject of the next Section 4.4. = REVIEW OF THE KEY IDEAS = The least squares solution Z minimizes ||Az — b]|? = xTATAz — 2cTATb+ b b. This is E, the sum of squares of the errors e, to e, in the m equations (m > n). 2. The best Z comes from the normal equations ATAZ = ATb. E is a minimum. 3. To fit m points by a line b = C + Dt, the normal equations give C' and D. 4. The heights of the best line are p = (py, ..., pm). The vertical distances to the data points are the errors e = (e, ..., en). A key equation is ATe = 0. If we try to fit m points by a combination of n < m functions, the m equations Ax = b are generally unsolvable. The n equations ATAZ = ATb give the least squares solution—the combination with smallest MSE (mean square error). 176 Chapter 4. Orthogonality 4.4 Orthonormal Bases and Gram-Schmidt | \"y \\ (Thccolumnsql,...,q,.areorthonormal if g q; ={ 0fori# } Then| QTQ=1. | 1fori=) 2 If Q is also square, then QQT = Tand| QT = Q! [. Now Q is an “orthogonal matrix”. 3 The least squares solution to Qz = bis £ = QTb. Projectionof b: p = QQTb = Pb. 4 The Gram-Schmidt process takes independent a; to orthonormal g,. Start with ¢, =a,/ ||a: B § q, is (a; — its projection p,) / ||a; — p,||; projection p, = (a;rql)ql +---+ (a;rq,-_l)q,-_l. {Each a; will be a combination of q, to ¢;. Then A = QR : orthogonal Q and triangulw This section has two goals, why and how. The first is to see why orthogonal columns in A are good. Dot products are zero, so AT A will be diagonal. It becomes so easy to find Z and p = AZ. The second goal is to construct orthogonal vectors q;. You will see how Gram-Schmidt combines the original basis vectors a; to produce right angles for the g;. Those a; are the columns of A, probably not orthogonal. The orthonormal basis vectors q; will be the columns of a new matrix Q. From Chapter 3, a basis consists of independent vectors that span the space. The basis vectors could meet at any angle (except 0° and 180°). But every time we visu- alize axes, they are perpendicular. In our imagination, the coordinate axes are practically always orthogonal. This simplifies the picture and it greatly simplifies the computations. The vectors q,, ..., q,, are orthogonal when their dot products g, - g; are zero. More exactly q;rqj = 0 whenever i # j. With one more step—just divide each vector by its length—the vectors become orthogonal unit vectors. Their lengths are all 1 (normal). Then the basis is called orthonormal. DEFINITION The n vectors q,,...,q,, are orthonormal if Tq. = 0 wheni#j (orthogonal vectors) \"7 |1 wheni=j (unitvectors: |g;|| = 1) A matrix Q with orthonormal columns has QTQ = I. Typically m > n. The matrix Q is easy to work with because QT Q = I. This repeats in matrix language that the columns q,...,q,, are orthonormal. Q is not required to be square: QQT # I. 4.4. Orthonormal Bases and Gram-Schmidt 177 A matrix Q with orthonormal columns satisfies QTQ = I : (_q'lr_ - _ r']. 0o ... ()q . qT— I 01 -~ 0 T I I . . - . L = oo - 1] When row i of QT multiplies column j of Q, the dot product is q;rqj. Off the diagonal (¢ # j) that dot product is zero by orthogonality. On the diagonal (i = j) the unit vectors give ¢ q; = ||q;]|> = 1. Often Q is rectangular (m > n). Sometimes m = n. When Q is square, Q' Q = I means that QT= Q~: transpose = inverse. If the columns are only orthogonal (not unit vectors), dot products still give a diagonal matrix (not the identity matrix). This diagonal matrix is almost as good as I. The important thing is orthogonality—then it is easy to produce unit vectors. To repeat: QTQ = I even when Q is rectangular. Then Q7 is a 1-sided inverse from the left. For square matrices we also have QQT = I. Then Q7 is the two-sided inverse of Q. The rows of a square Q are orthonormal like the columns. The inverse is the transpose Q' . In this square case we call Q an orthogonal matrix.' Here are three examples of orthogonal matrices—rotation and permutation and reflec- tion. Those are important matrices ! The quickest test is to check QTQ = I. Example 1 (Rotation) Q rotates every vector in the plane by the angle 6: 0 = [cosﬂ —sinO] and QT=Q'= [ cos 6 sm0]. sinf cosf@ —sinf@ cosé The columns of Q are orthogonal (take their dot product). They are unit vectors because sin? @ + cos? @ = 1. Those columns give an orthonormal basis for the plane R®. The standard basis vectors ¢ and j are rotated through 6 in Figure 4.10a. Then Q! rotates vectors back through —8. It agrees with QT, because the cosine of —6 equals the cosine of 6, and sin(—6) = —sinf. We have QTQ =T and QQ\" = I. Example 2 (Permutation) These matrices change the order to (y. 2, z) and (y. z): 0] |z Fy 1 yl =12 and [(1) (l)] [;} = [:] 0 4 IJ b -J 3 - - 0 1 ! “Orthonormal matrix” would have been a better name for Q, but it's not used. Any matrix with orthonormal columns has the letter Q. But we only call it an orthogonal matrix when it is square. 178 Chapter 4. Orthogonality All columns of these Q’s are unit vectors (their lengths are obviously 1). They are also orthogonal (the 1's appear in different places). The inverse of a permutation matrix is its transpose: Q! = Q7. The inverse puts the components back into their original order: o - - - - | F]- 2] = Bl ) T y A b - e - 0 0 Inverse = transpose: 1 0 0 1 e Every permutation matrix is an orthogonal matrix. Example 3 (Reflection) If u is any unit vector, set Q = I — 2uuT. Notice that uu’ is a matrix while uT u is the number ||u||? = 1. Then QT and Q! both equal Q: QT=I1-2uuT=Q| and QTQ=1-4uu” +4uuTuuT =1 (2) Reflection matrices I — 2uuT are symmetric and also orthogonal. If you square them, you get the identity matrix: Q? = QTQ = I. Reflecting twice through a mirror brings back the original, like (—1)? = 1. Notice uTu = 1 inside 4uuTuuT in equation (2). J Qi=j A . _|—sinf p : Qi = [ cosO] = cos 0 “ ) ,H/lll'fOI' sin @ ,\\ Rotate by 60 Reflect 0 . i U to —u» > Qj=i / Figure 4.9: Rotation by Q) = [g -g] and reflection across 45° by ) = [‘1’ (1,] As example choose the direction u = (—1/v/2,1/v/2). Compute 2uuT (column times row) and subtract from I to get the reflection matrix Q : : 7 S =5 101 0 1]|(z] |y Reflection Q =1 -2 [_.5 .5] = [l 0] and [l 0] [y] = [:c] : When (z,y) goes to (y, T), a vector like (3,3) doesn’t move. It is on the mirror line. Rotations preserve the length of every vector. So do reflections. So do permutations. So does multiplication by any orthogonal matrix )Q—lengths and angles don’t change. Proof ||Qz||? equals ||z||* because (Qz)T(Qz) = 2TQTQz = =TIz = Tz If Q has orthonormal columns (Q™Q = I), it leaves lengths unchanged : Same length for Qz ||Qz|| = ||z|| for every vector . 3) Q also preserves dot products: (Qz)T(Qy) = zTQTQy = zTy. Justuse QTQ=1\"! 4.4. Orthonormal Bases and Gram-Schmidt 179 Projections QQ™ Using Orthonormal Bases: Q Replaces A Orthogonal matrices are excellent for computations—numbers can never grow too large when lengths of vectors are fixed. Stable computer codes use (’s as much as possible. For projections onto subspaces, all formulas involve AT A. The entries of AT A are the dot products a;raj of the basis vectors a,,...,a,. Suppose the basis vectors are actually orthonormal. The a’s become the g’s. Then AT A simplifies to QTQ = I. Look at the improvements in Z and p and P. Z=QT and p=Qz and P=QQ\"T. 4) The least squares solution of Qr = bis T = QTb. The projection matrix is QQ™. There are no matrices to invert. This is the point of an orthonormal basis. The best T = Qb just has dot products of q,,...,q, with b. We have 1-dimensional projections’ The “coupling matrix” or “correlation matrix” ATA is now QTQ = I. There is no coupling. When A is Q, with orthonormal columns, here is p = QT = QQ'b: | 1] [art Projection _ o ] T N . . onto q’s - qll qln = qy(q b)) +---+4q,(q,b)- | O) - 1 1q.b. Important case: When () is square and m = n, the subspace is the whole space. Then QT = Q7! and T = Q\" bis the same as x = Q~'b. The solution is exact! The projection of b onto the whole space is b itself. In this case p = band P = QQT = I. You may think that projection onto the whole space is not worth mentioning. But when p = b, our formula assembles b out of its 1-dimensional projections. If q,,....q, 1s an orthonormal basis for the whole space, then Q is square. Every b = QQ7Tb is the sum of its components along the q’s b=q,(q7b)+4q,(q3b) + - +q,(q,b). 6) Transforms QQT = I is the foundation of Fourier series and all the great “transforms” of applied mathematics. They break vectors b or functions f(x) into perpendicular pieces. Then by adding the pieces in (6), the inverse transform puts b and f(r) back together. gExample 4 The columns of this orthogonal () are orthonormal vectors q, . q,, q5: lf-l 2 2 m=n=3 Q:5 2 -1 2| has QTQ=QQT=1. 2 2 -1 The separate projections of b = (0,0, 1) onto q, and g, and @; are p, and p, and p;: 01(0?5) = %‘h and Qz(ng) = %Qz and ‘13(‘1’:{”) = ‘313\"13- 180 Chapter 4. Orthogonality The sum of the first two is the projection of b onto the plane of q, and q,. The sum of all three is the projection of b onto the whole space—which is p; + p, + p; = b itself: I-— _2q - - Reconstruct b 2 2 1 1 2+4 B ~Pi TP TP C4+4+1] 1] The Gram-Schmidt Process The point of this section is that “orthogonal is good”. Projections and least squares always involve AT A. When this matrix becomes QTQ = I, the inverse is no problem. The one-dimensional projections are uncoupled. The best Z is QT b (just n separate dot products). For this to be true, we had to say “If the vectors are orthonormal”. Now we explain the “Gram-Schmidt way” to create orthonormal vectors. Start with three independent vectors a, b, c. We intend to construct three orthogonal vectors A. B.C. Then (at the end or as we go) we divide A, B, C by their lengths. That produces three orthonormal vectors q, = A/||A|l, g, = B/||B||, q; = C/||C]||. Gram-Schmidt Begin by choosing A = a. This first direction is accepted as it comes. The next direction B must be perpendicular to A. Start with b and subtract its projection along A. This leaves the perpendicular part, which is the orthogonal vector B: First Gram-Schmidt step ATb B=b- B is perpendicular to A AT A A. (7 A and B are orthogonal in Figure 4.11. Multiply equation (7) by AT to verify that A\"B = ATb - A™b = 0. This vector B is what we have called the error vector e, perpendicular to A. Notice that B in equation (7) is not zero (otherwise a and b would be dependent). The directions A and B are now set. The third direction starts with ¢. This is not a combination of A and B (because c is not a combination of a and b). But most likely ¢ is not perpendicular to A and B. So subtract off its components in those two directions to get a perpendicular direction C': Next Gram-Schmidt step ATe B'e C is perpendicular to A and B C=C—ATAA— BTBB' (8) This is the one and only idea of the Gram-Schmidt process. Subtract from every new vector its projections in the directions already set. That idea is repeated at every step.’ If we had a fourth vector d, we would subtract three projections onto A, B, C to get D. At the end, or immediately when each one is found, divide the orthogonal vectors A, B, C, D by their lengths. The resulting vectors q,, g, g3, g4 are orthonormal. 2 [ think Gram had the idea. I don't really know where Schmidt came in. 4.4. Orthonormal Bases and Gram-Schmidt 181 $ C =C—p . C Subtract the ‘.c p9s = II—CII projection p toget B=b—1p Unit vectors N\\ N A projectionp F91 = TAl N A=a b Figure 4.10: First project b onto the line through a and find the orthogonal B = b — p. Then project ¢ to p onthe AB plane and find C = ¢ — p. Divide by | A||, || B|. ||C]|. Example of Gram-Schmidt Suppose the independent non-orthogonal vectors a, b, ¢ are \" 1] (2 - 3] a=|—-1 and b= 0 and c= |-3 ! 0 .—2J . 3. Then A = a has AT A = 2 and ATb = 2. Subtract from b its projection p along A: 1 ATb 2 FirStStep sz—ATAAzb—§A= 1 -2 Check: AT B = 0 as required. Now subtract the projections of c on A and B to get C: Two projections to subtract for C. rl- ATc BTc 6 6 Next step C=c A +—B=c 5 + 5 ! Check: C = (1,1,1) is perpendicular to both A and B. Finally convert A, B.C to unit vectors (length 1, orthonormal). The lengths of A, B, C are v2 and v/6 and V/3. Divide by those lengths, for an orthonormal basis q,, g5, 3. 1 1 g;=—|-1] and go=—7=| 1| and g3=—4|1 V2 | o V6 | V3 (1 Usually A, B, C contain fractions. Almost always q,, g5, g3 Will contain square roots. That is the price we pay for unit vectors and QTQ = I. 182 Chapter 4. Orthogonality The Factorization A = QR We started with a matrix A, whose columns were a,b,c. We ended with a matrix Q, whose columns are q,, q,,q;. How are those matrices related? Since the vectors a, b, ¢ are combinations of the g’s (and vice versa), there must be a third matrix connecting A to Q. This third matrix is the triangular R in A = QR. (Not the R of Chapter 1.) The first step was ¢, = a/||a|| (other vectors not involved). The second step was equation (7), where b is a combination of A and B. At that stage C and g5 were not involved. This non-involvement of later vectors is the key point of Gram-Schmidt: * The vectors a and A and q, are all along a single line. * The vectors a,band A, B and q,, g, are all in the same plane. * The vectors @,b,cand A, B,C and q,, q,, g5 are in one subspace (dimension 3). At every step a,...,ax are combinations of q,,...,q,. Later q’s are not involved. The connecting matrix R is triangular, and we have A = Q times R. ' 1 T 1[gia 4afb gfc] a bcl=|q aq g g;b gic| or A=QR. (9 L It J{ gsc A = QR is Gram-Schmidt in a nutshell. Multiply by QT to see R = QT A in (9). (Gram-Schmidt) From independent vectors a;, .. ., a,,, Gram-Schmidt constructs orthonormal vectors q;, .. ., q,,. The matrices with these columns satisfy A = QR. Then R = Q™ A is upper triangular because later g’s are orthogonal to earlier a’s. Here are the original a’s and the final ¢’s from the example. The %, j entry of R = QT A is row i of QT times column j of A. The dot products gT a; go into R. Then A = QR: 1 2 3] [1V2 Yve 1/VB][vVZ V2 Vi8] A=|-1 0 -3|=|-1/v2 1/v6 1/V3|| 0 6 -v6|=QR. ! 0 -2 3. 0 -2/\\/6 1/\\/3: 0 0 \\/54 o Look closely at Q and R. The lengths of A, B, C are /2, v/6, v/3 on the diagonal of R. The columns of Q are orthonormal. Because of the square roots, Q R might look harder than LU. Both factorizations are absolutely central to calculations in linear algebra. Any m by n matrix A with independent columns can be factored into A = QR. The m by n matrix Q) has orthonormal columns, and the square matrix R is upper triangular with positive diagonal. We must not forget why this is useful for least squares: ATA = (QR)TQR = RTQTQR = RTR. Then ATAZ = ATb simplifies to RTRz = RTQTb. Finally RZ = QT b with triangular R. 4.4. Orthonormal Bases and Gram-Schmidt 183 Least squares R\"RZ =RTQ™ or R2=QT™b or Z = R'1QTb (10) Instead of solving Az = b, which is impossible, we solve RZ = QTb by back substitu- tion—which is very fast. The real cost is the mn? multiplications for Gram-Schmidt, which are needed to construct the orthogonal () and the triangular R with A = QR. Below is an informal code. It executes equations (11) for 7 = 1 then 7 = 2 and eventually j = n. The important lines 4-5 subtract from v = a; its projection onto each q;,: < j. The last line of that code normalizes v (divides by r;; = ||v]|) to get the unit vector g; : m : m 1/2 . Tkj =Zqikv,—j and v;; = v;j—QikTk; and rj; = (Z v?j) and gq;; = —. (11) 1=1 1=1 Starting from a, b, ¢ = a1, a2, a3 this code will construct q,, then B, g,, then C, q5: q, = a:/|a|| B=a;-(q/a2)q; g,=B/|B]| C'=a3-(qfas)q, C=C\"-(q;C\")q; a3=C/|C| Equation (11) subtracts one projection at a time as in C” and C. That change is called modified Gram-Schmidt. This code is numerically more stable than equation (8) which subtracts all projections at once. We are using MATLAB notation. forj=1:n % modified Gram-Schmidt in n loops v=A(:7); % v begins as column j of the oniginal A fori=1:7-1 % columns g, to q;_, are already settled in Q R(i,j) = Q(:,i)'*v; % compute R;; = q; a; whichis g, v v =v—R(i,j)*Q(:,i); % subtract the projection (g, v)q, end % v is now perpendicularto all of qy,...,q,_, R(j,7) = norm(v); % the diagonal entries Rj; in R are lengths Q(:,7) =v/R(J,7); % divide v by its length to get the next q; end % for j = 1 : n produces all of the q; To recover column j of A, undo the last step and the middle steps of the code: j-1 R(j,7)q; = (v minus its projections) = (column j of A) — Z R(%,7)q; . (12) Moving the sum to the far left, equation (12) is column j in the multiplication QR = A. 184 Chapter 4. Orthogonality Confession Good software like LAPACK, used in good systems like MATLAB and Julia and Python, might not use this Gram-Schmidt code. There is now another way. “Householder reflections™ act on A to produce the upper triangular R. This happens one column at a time in the same way that elimination produces the upper triangular U in LU. If A is tndiagonal we can simplify even more to use 2 by 2 rotations. The result is always A = QR and the MATLAB command to orthogonalize A is [Q, R] = qr(A). Gram- Schmidt is still the good process to understand, and the most stable codes look for and use the largest vector a; at each new step. ® REVIEW OF THE KEY IDEAS = 1. If the orthonormal vectors q;,...,q, are the columns of (), then q;rqj = 0 and q] g, = 1 translate into the matrix multiplication QTQ = I. . If Q is square (an orthogonal matrix) then QT = Q~1: transpose = inverse. . The length of Qx equals the length of z: ||Qz| = ||z||. . The projection onto the column space of Q spanned by the ¢’s is P = QQ\". . If Q is square then P = QQT = I andevery b = q,(qTb) + --- + q,,(q} b). A W & W . Gram-Schmidt produces orthonormal vectors q,, g5, g3 from independent a, b, c. In matrix form this is the factorization A = (orthogonal Q)(triangular R). 8 WORKED EXAMPLES = 44 A Add two more columns with all entries 1 or —1, so the columns of this 4 by 4 “Hadamard matrix” are orthogonal. How do you turn Hj into an orthogonal matrix QQ? -y P - I I 1 1 1 -1z z i [1 -1] Hi=ly 1 g o 2 Q= 1 -1 z = i ] ' damard matrix with 1’ —~1's. The block matrix Hg = [H4 H4] is the next Hadamard matrix with 1°s and —1's H, —H,| Whatis the product HJ Hg? The projection of b = (6,0,0,2) onto the first column of Hy is p, = (2,2,2,2). The projection onto the second column is p, = (1, -1, 1, —1). What is the projection pypofb onto the 2-dimensional space spanned by the first two columns? 4.4. Orthonormal Bases and Gram-Schmidt 185 Solution H, can be built from H, just as Hy is built from H,: 1 1 1 1] H H 1 -1 1 -1 Hy = [ Hz _ Hz] =11 1 -1 -1 has orthogonal columns. 1 -1 -1 1 Then Q = H/2 has orthonormal columns. Dividing by 2 gives unit vectors in Q. A 5 by 5 Hadamard matrix is impossible because the dot product of columns would have five 1’s and/or —1’s and could not add to zero. Hg has orthogonal columns of length V8. HT HTHH_2HTH 0 _SIOQ___ HT -HT||H -H[ | 0 2HTH| |0 8I| %\" &5 44 B What is the key point of orthogonal columns? Answer: QTQ is diagonal and easy to invert. We can project onto lines and just add. The axes are orthogonal. Hgng[ H8 Orthogonal Matrices Q and Orthogonal Projections P A has n independent columns in R™ Q has n orthonormal columns in R™ _ - ( - (m x n) A= | a an Q=| q qn . . _ i AT A is symmetric and invertible QT =1 Projection matrix P onto the column space Pb = nearest pointto b P=A(ATA)\"1AT P=QQT\" Least squares solution to Az = b Least squares solution to Qz = b ATAZ = ATh zZ=QTb Gram-Schmidt orthogonalization Independent columns a; A=QR Orthonormal columns q; I | I 1 11 72 Tin ] a Qn q, dn 0 r22 T2n = 0 0 . . I 0 0 0 rpn ] Pseudoinverse A1 of A = QR Pseudoinverse Q+ of Q At =RtQt =R'QT Qt=QT Pseudoinverse AT of A = CRin4.5 A+=R+C+=RT(CTART)‘ICT 186 Chapter 4. Orthogonality Problem Set 4.4 Problems 1-12 are about orthogonal vectors and orthogonal matrices. 1 Are these pairs of vectors orthonormal or only orthogonal or only independent? 1 -1 6 4 cos 6 —sind () [O] and [ 1] (b) [8] and [-.3] (c) [sine] and [ cosH] ' Change the second vector when necessary to produce orthonormal vectors. 2 The vectors (2,2, 1) and (-1, 2, 2) are orthogonal. Divide them by their lengths to find orthonormal vectors q; and g,. Put those into the columns of ) and multiply QTQand QQ™. 3 (a) If A has three orthogonal columns each of length 4, what is AT A? (b) If A has three orthogonal columns of lengths 1,2, 3, what is AT A? 4 Give an example of each of the following: (a) A matrix Q that has orthonormal columns but QQT # I. (b) Two orthogonal vectors that are not linearly independent. (c) An orthonormal basis for R?, including the vector ¢, = (1,1,1)/v/3. 5 Find two orthogonal vectors in the plane £ + y + 2z = 0. Make them orthonormal. 6 If Q) and Q; are orthogonal matrices, show that their product Q,Q is also an or- thogonal matrix. (Use QTQ = I.) 7 If Q has orthonormal columns, what is the least squares solution T to Qx = b? 8 If q, and q, are orthonormal vectors in R®, what combination is closest to a given vector b? q, + q; 9 (a) Compute P = QQT when g, = (.8,.6,0) and g, = (—.6, .8,0). Verify that P =P (b) Prove that always (QQT)? = QQT by using QTQ = I. Then P = QQT is the projection matrix onto the column space of Q). 10 Orthonormal vectors are automatically linearly independent. (a) Vector proof: When c;q, +c2q, +c3q3 = 0, what dot product leads to ¢; = 0? Similarly c; = 0 and ¢z = 0. Thus the g@’s are independent. (b) Matrix proof: Show that Qx = 0 leads to £ = 0. Since () may be rectangular, you can use QT but not Q1. 11 (a) Gram-Schmidt: Find orthonormal vectors g, and g, in the plane spanned by a=(1,3,4,57\"7and b= (-6,6,8,0,8). (b) Which vector in this plane is closest to (1,0,0,0,0)? 4.4. Orthonormal Bases and Gram-Schmidt 187 12 13 14 15 17 18 19 20 If @), az, a3 is an orthogonal basis for R?, show that z; = aJb/a] a;. - . 21 ] b=uza+z202+2303 OF @ a; az| |z2| =0b. ! J LT3, What multiple of @ = [1] should be subtracted from b = [§] to make the result B orthogonal to a? Sketch a figure to show a, b, and B. Complete the Gram-Schmidt process in Problem 13 by computing g, = a/||a|| and g, = B/||B|| and factoring into QR: . bof=lo «f[5 i) (a) Find orthonormal vectors q,, g5, @5 such that g,, g, span the column space of - - y ; i (b) Which of the four fundamental subspaces contains q3 ? - —9 4 . (c) Solve Az = (1,2, 7) by least squares. What multiple of @ = (4,5,2,2) is closest to b = (1,2,0,0)? Find orthonormal vectors q, and g, in the plane of @ and b. Find the projection of b onto the line through a: F'l' (lﬂ a= |1 and b=|[3| and p=? and e=b-p=\" 1, . b Compute the orthonormal vectors g, = a/||al| and g, = e/||e]|. (Recommended) Find orthogonal vectors A, B, C by Gram-Schmidt from a, b.c a=(1,-1,0,0) b=(0,1,-1,0) c=(0,0,1,-1). A, B, C and a, b, c are bases for the vectors perpendiculartod = (1,1, 1,1). If A = QR then ATA = RTR = triangular times triangular. Gram-Schmidt on A corresponds to elimination on AT A. The pivots for AT A must be the squares of diagonal entries of R. Find Q and R by Gram-Schmidt for this A: N A B O QB True or false (give an example in either case): o et (—1 A= 2 2 = - (a) Q! is an orthogonal matrix when Q is an orthogonal matrix. (b) If Q (3 by 2) has orthonormal columns then ||Qz|| always equals ||z||. 188 Chapter 4. Orthogonality 21 Find an orthonormal basis for the column space of A and project b onto C(A). 1 -2 [ —4 ] 1 0 -3 1 3 0] 22 Find orthogonal vectors A, B, C by Gram-Schmidt from L) 3 - [y [— i | a=]1 and b= |-1 and c¢ = L2. LO | - O 23 Find q,,q,, q; (orthonormal) as combinations of a,b. c (independent columns). Then write A as QR: 1 A=10 0 24 (a) Find a basis for the subspace S in R* spanned by all solutions of T1+To+z23—T4 =0. (b) Find a basis for the orthogonal complement S + (c) Find b, in Sand b, in S* sothatb; + by = b = (1,1,1,1). 25 Ifad—bc>0,theentriesin A = QR are a —c| [a®+ ab+cd [a b] _le a] | 0 ad — be c df Va®+c va? + c? Write A = QR when a,b,c,d = 2,1,1,1 and also 1,1,1,1. Which entry of R becomes zero when the columns are dependent and Gram-Schmidt breaks down? Problems 26-29 use the Q R code in equation (11). It executes Gram-Schmidt. 26 Show why C (found via C” in the steps after (11)) is equal to C in equation (8). 27 Equation (8) subtracts from c its components along A and B. Why not subtract the components along a and along b? 28 Where are the mn? multiplications in equation (11)? 29 Apply the MATLAB grcodetoa = (2,2,-1),b = (0,-3,3),c = (1,0,0). What are the q’s? 4.4. Orthonormal Bases and Gram-Schmidt 189 Problems 30-35 involve orthogonal matrices that are special. 30 31 32 33 34 35 36 37 The first four wavelets are in the columns of this wavelet matrix W: 1 1 v2 0] w=l|l 1-v2 0 211 -1 0 V2 1 -1 0 -2 What is special about the columns? Find the inverse wavelet transform W1, (a) Choose c so that @ is an orthogonal matrix: (1 -1 -1 -1 -1 1 -1 -1 Q=c|_; 1 1 -1 -1 -1 -1 1 Project b = (1, 1,1, 1) onto the first column. Then project b onto the plane of the first two columns. If w is a unit vector, then Q = I —2uuT is a reflection matrix (Example 3). Find Q1 from u = (0, 1) and Q, from u = (0, v/2/2, v/2/2). Draw the reflections when Q, and (2 multiply the vectors (1,2) and (1,1, 1). Find all matrices that are both orthogonal and lower triangular. Q = I — 2uuT is a reflection matrix when uTu = 1. Two reflections give Q% = I. (a) Show that Qu = —u. The mirror is perpendicular to u. (b) Find Qv when uTv = 0. The mirror contains v. It reflects to itself. Challenge Problems (MATLAB) Factor [Q, R] = qr(A) for A = eye(4) — diag([1 1 1],-1). You are orthogonalizing the columns (1, -1,0,0) and (0,1, -1,0) and (0,0,1, —1) and (0,0,0,1) of A. Can you scale the orthogonal columns of Q to get nice integer components? If A is m by n with rank n, qr(A) produces a square Q and zeros below R: R The factors from MATLAB are (m by m)(m by n) A=[Q: Q2] [ 0] : The n columns of @, are an orthonormal basis for which fundamental subspace? The m —n columns of Q), are an orthonormal basis for which fundamental subspace? We know that P = QQT is the projection onto the column space of Q(m by n). Now add another column a to produce A = [Q a]. Gram-Schmidt replaces a by what vector ¢? Start with a, subtract ,divideby _____ tofind q. 190 Chapter 4. Orthogonality 4.5 The Pseudoinverse of a Matrix Rank r=m=n=2 r=m<n=3 r=n<m=3 \"1 0° 1 0 1 00 I=[ ] IL=[ ] Ir=| 0 1 0 1 010 0 0 I =inverseof I I = left inverse of I'p (IL)(Ir) =1 (I)(I)=1 Ir = right inverse of I, (IR)(IL) # 1 Only the first matrix [ is truly invertible. I and Ir have one-sided inverses but not true inverses. Every matrix A has a pseudoinverse A7t That word includes inverses and left inverses and right inverses. We will start with a summary of one-sided inverses. Ahasaleftinverse: ATA =T A has a right inverse : AAT=1 A has independent columns A has independent rows A can be tall and thin: rank r = n A can be short and wide : rank r = m Az = b has no solution or 1 solution Az = b has 1 solution or many solutions The nullspace N(A) = zero vector The left nullspace N(AT) = zero vector AT A is n X n and invertible AAT is m X m and invertible The leftinverseis AT = (ATA)~1AT The right inverseis A1 = AT(AAT)-1 The left column with AT A = I describes the matrices in this least squares chapter. The rank is # = n < m and the m equations Az = b may be unsolvable : too many equations. ThenZ = AT bis the vector that solves ATAZ = ATb and makes the error e = b — AZ as small as possible : this is least squares. The right column with AAT = I describes the opposite problem. Now the rank is r = m < n and Az = b has infinitely many solutions. In this case et = Atbis the minimum length solution to Az = b. That solution is in the row space of A and all other solutions z = 1 + z,, have a nullspace component z,—which increases the length. What would you expect when the rank of A has » < m and » < n? There is no one-sided inverse. The pseudoinverse A1 (chosen next) will give the best z+ = 4+ b. That vector z+ minimizes the error ||Az — b||2. Also, among all the vectors z = z+ 4 Zpullspace that give the same Az and same error, the vector 1 has minimum length : 21 is the “minimum norm least squares solution” to Az = b. Among all the vectors that minimize || Az — b|[?, =T = AT b also minimizes ||z||2. Key idea: z 1 has nullspace component = 0. If A has dependent columns, we need 4+ So this section is optional but the topic fits here. At is explained using the foyr fundamental subspaces. It can be computed from A = CR or A = QR or the SVD. 4.5. The Pseudoinverse of a Matrix 191 The Pseudoinverse AT (n X m) of a Matrix A (m X n) 1. Every vector y in m dimensions has two perpendicular parts y = b + 2 2. bis in the column space of A and z is in the nullspace of AT : ATz =0 and Atz=0 3. Ax = b for one vector x in the row space of A. Invert this part to find Atb==z The pseudoinverse of A has Atb=z Atz=0 A+y =Atb+Atz==¢ Here is the big picture for AT . Notice that AT shares the same four subspaces as AT. The difference is that A1 A4 brings x in the row space back to the same x. In fact AtA is exactly the n by n projection matrix Prow onto the row space of A. And symmetrically, AAT is exactly the m by m projection matrix P,jj,mn ©nto the column space of A. AT inverts A when it can: column space to row space. A Row space to column space of A AT Column space to row space of A row column space of A \\Reverse the 4 subspaces for AT space of A row space of A column space of AT Pseudoinve,.s‘e A+y =2 ATZ =A+Z =0 nullspace of AT = nullspace of At nullspace of A I 0 ] row space AAT = [ I 0 ] col space tA= ATA [O 0 | projection 0 0| projection Figure 4.11: b= Az in the column space of A goes back to x = Atbin the row space. Any y = b+ z also goes backto & = A1b because ATz = 0. Then (A’{'A)2 =ATA AT was proposed by Moore and again by Penrose (who won the 2020 Nobel Prize in Physics). It is the matrix that produces symmetric projections Prow and Fyojumn : Prow=ATA=(ATA)? = (ATA)T P, ymn=4AT=(4AT)2=(4A1)T 192 Chapter 4. Orthogonality Those tests for AT will be useful when we compute At from A= CRor A = QR. Example 1 A=[2 0] y=[yl]+[ 0 ]=b+z A+=[1(/)2 g] 00 0 Y2 _ n .. =bi 20 /2 = v b= 0 is in the column spaceof A Az =bis 0 0] o } 10 - b p 1 z= -;l‘isinthenullspaccofAT ATz=0is-(2) gii] = 8 | + Fo Y1/2 +_[1/2 0 T — ATy=A\"0+ATz2=24+0= 0 - Then AT = 0 O and A7 A= 00 This extends to an easy and useful case : a diagonal matrix D, square or rectangular: 2 0 0] \"1/2 0 0 The pseudoinverseof D=| 0 3 0 |isDt=| 0 1/3 0 (00 0 0 0 0 We invert where we can. But 1/0 is taken to be zero ! If the diagonal matrix D has ad- ditional zero rows (or zero columns) then D7 has additional zero columns (or zero rows). That example extends from a diagonal D to any matrix A = BDC, if B and C are invertible. Then AT will be C—1D¥ B~1. Again, we can invert B and C but we can’t invert the zeros in D. So DT replaces D~! and AT replaces A~ 1. is its left inverse AT = % [ 1 1 ] The pseudoinverse of A = [ } The pseudoinverse of A= [ 1 1 ] isits rightinverse A1 = % [ i ] Many matrices don’t have an inverse matrix A~! with A=!4 = I and AA™! = L. That honor is limited to square matrices with independent columns and independent rows. For a true A~1, only the zero vector is in the nullspace of A and the nullspace of AT, This chapter worked with tall thin matrices (m > n) with independent columns. The matrix has rank r = n. The zero vector is alone in the nullspace N(A). Then AT A is invertible (n by n). In this case AT = (ATA)~1 AT is a left-inverse of A. Wehave ATA = (ATA)\"*ATA = I but we do not have AAT =1 (1) The next possibility is a short wide matrix (m < n) with independent rows. The rank of A is m and the zero vector is alone in the left nullspace N(AT). Then AAT is invertible (m by m). In this case At = AT(AAT)~1 s aright-inverse of A. Now we have AAT = AAT(AAT)~1 = I but we do not have ATA=1 @ 4.5. The Pseudoinverse of a Matrix 193 The Important Action of A is Row Space to Column Space Here is an important point about the row space and column space of any matrix A. We know that they have the same dimension r (the rank). That is part of the important point but not all of it. The other part is that two vectors £ and X in the row space cannot go to the same vector Ax = AX in the column space. Every vector b = Az in the col- umn space comes from one and only one vector z in the row space : Then Atb=1=z. We can easily prove that useful fact. Suppose Az = b and also AX = b with x and X both in the row space. Then the difference £ — X is in the nullspace of A, because A(x — X) = b— b= 0. Butx — X is also in the row space (subtraction leaves us in the space). Since the nullspace is always orthogonal to the row space, £ — X in both spaces is orthogonal to itself. Then £ — X must be the zero vector. We can express the same idea in other nice ways. Suppose the vectors x; to T, are a basis for the row space of A. Then the vectors Ax; to Az, are a basis for the column space of A. It helps that both spaces have dimension r—the great fact from Chapter 1. Actually the neat factorization A = CR that we learned in Chapter 1 (and completed in Chapter 3 by elimination) will now pay off. A = CR tells us the pseudoinverse At The matrix C is m by r with r independent columns coming directly from A. R is r by n with r independent rows coming from the reduced echelon form of A. Those led us to A = C R. Their pseudoinverses now lead us to At =RtcCcT. The Pseudoinverse AT = RTCt of A = CR At this point we use the one-sided inverses of C and R. Again, C has r independent columns and R has r independent rows. Then CTC and RRT are invertible r by r matrices. The pseudoinverses of C and R are one-sided inverses: CTC = I and RRT = I. Cct =(CTC)-'CT and Rt = RT(RRT)-!. 3) By good fortune that left inverse of C and right inverse of R are in the order RTCt to tell us the pseudoinverse of A = C'R. This is the key fact! But see Problem 10. Pseudoinverse At =RtCt = RT(RRT)-}(CTC)-1CT = RT(CTCRRT)-!CT = RT(CTART)-1CT| () We feel bound to give outline proofs for these various facts that fit together so well. The key points are that CTC and RRT are invertible when C has independent columns and R has independent rows. Here we give examples that apply formula (4) for AT That discussion will continue on this book’s website math.mit.edu/linearalgebra. 194 Chapter 4. Orthogonality Remark We have explained AT based on the four fundamental subspaces of A. Moore and Penrose took a different approach. Penrose required these four conditions on the matrix At —and those conditions identify the same pseudoinverse : AATA=A4 Ataat=A1T (UATAHT=A1TA (AAT)T=44T|0) Then At A and AAT are projections onto the row space and column space of A. Remark A lot of effort goes into choosing basis vectors with specially good properties. Among the very best properties is orthogonality : the basis vectors are perpendicular. The main point of Chapter 7 will be finding the famous “singular vectors” of A. Those are orthogonal unit vectors v; to v, in the row space and Av, to Avy are orthogonal in the column space. In applied linear algebra, that is perfection! The same A+ comes from A = CRand A = QR and the SVD: A = UXVT, Example of AT from A = Incidence Matrix of a Graph The m by n incidence matrix in Section 3.5 has a row for every edge in the graph and a column for every node. The ith row has two nonzeros, —1 and 1, to identify the leaving node and entering node for the ith edge. Flows can go with or against the arrows, which just set a positive direction on each edge. -1 1 0 0] edgea -1 0 1 © b A= 0 -1 1 0 -1 0 o0 1 d 0 -1 0 1 | e 3 Column4of A= — Columns 1 + 2 + 3, socolumn4of Ris —1,-1,—1 This matrix A has m = 5,n = 4 and rank r = 3. Its first three columns are independent : -1 1 0] 0 -1 01 1 0 A=CR= 0 -1 1 01 0 -1 (6) 0 0 0 0 -1 0 0 -1 R - 4.5. The Pseudoinverse of a Matrix 195 Recall the basic facts about linear equations Ax = b. There is a solution when b is a combination of the columns of A. The minimum norm solution x is the only solution in the row space of A (so &g is a combination of the rows). Other solutions £ = xo + T, with larger norm ||z||? = ||xo||? + ||zn||? include a nullspace part with Az, = 0. Every A is invertible from its row space to its column space: dimension r to r. Azg=by ATb : =z : T in row space of A 9 by in column space of A &, in nullspace of A y ¢ b, in nullspace of AT Az, =0 A+bn =0 How to compute A1 ? The recommended formulais AT = VETUT from the singular value decomposition A = UXV'T. Here U and V are square orthogonal matrices withU~! = UT = Ut and V-! = VT = V1. The m by n diagonal matrix has an n by m diagonal pseudoinverse >+ (replace eacho; > 0in X by 1/0; in E'*'). Each diagonal zero in ¥ remains a diagonal zero in rt. When A is a small matrix of integers (like the incidence matrix), there is another way to compute AT . Instead of the SVD, formula (6) showed A = CR. Then AT = RTCT in (4) involves fractions but not irrational numbers (those come from the SVD). Pseudoinverseof A = CR At = RT(CTART)—ICT (7 A7t and AT have the same row space and the same column space. The difference is : AT Aisa symmetric projection matrix onto that row space. We can check the four Penrose conditions in equation (5), and we can check (A'*'A)2 = At A from (7): (At A)2 = RT(CTART)\"!CTART(CTART)\"1CTA=ATA| @ Similarly (AA+)2 equals AAT. And if A were invertible we would see that At =41 For the graph incidence matrix above, the computation of AT in formula (7) is exact: - . -2 -2 0 -2 0] 4 0 0 cTART=| o 4 o|andat=2%| 2 0 -2 0 -2 ) - - 0 -1 -1 3 3, The four subspaces for AT are exactly the four subspaces for At Check the nullspaces of AT and AT. Then the row spaces and the column spaces must agree by orthogonality. Each row of A adds to zero — Each column of AT adds to zero Row2of A=Rowl+Row3 — Column2of At = Column 1 + Column 3 Rowlof A=Row4 —-Row5 — Columnl of At = Column 4 — Column 5 The equation ATy = 0 is Kirchhoff’s Current Law (flow in = flow out at each node). This is one of the great equations of circuit theory and computational engineering. 196 Chapter 4. Orthogonality Problem Set 4.5 1 Find the pseudoinverses AT of these matrices : 1 1 (2 0 2 0 0 Alz[ll] Ap=10 4 A3=[040] | 0 0 2 If Ahas rank 1, then it has the simple form A = crT = one column times one row. Show that AT =rcT /(rTr) (cTc) satisfies the four Penrose conditions in eqn (5). 3 If you add a row of zeros to a matrix A, show that you add a column of zeros to At Hint: Do the four Penrose tests stay correct with one extra row of zeros in A and column of zerosin AT 7 4 Whatare the row space C(AT) and column space C(A) for this matrix A ? Find AT with the same four subspaces as AT : 1 00 a=lo 1 0] 5 Find the pseudoinverses of A = [ (1) g ?) ] and AT. Explain (AT)T = (A1)T. 6 If P is a projection matrix (PT = P = P?) show that Pt = P satisfies Penrose in equation (3J). 7 For any matrix A = CR, equation (8) verified that (A+A)2 = At A, Verify similarly that (AAT)2 = AAT. 8 For any matrix, multiply AT = RT(CTART)~!CT times A = CR and simplify to show that AT A and AAT are symmetric. Combined with (A'*'A)2 = AtA from Problem 7, this verifies that AT A and AAt are symmetric projections. 9 Suppose you remove node 4 and the two edges into it from the graph in this section (leaving a triangle graph). Write down the incidence matrix A and find At. Can you factor A = C'R and recompute At from equation (7)? 10 For A=[1 0]and B=[ i] show that (AB) 1 isnot BT At but (BA)t+ = AtBT. The first factor needs independent columns, the second factor needs independent rows. 11 Why does the pseudoinverse of At always equal A\" 4.5. The Pseudoinverse of a Matrix 197 Thoughts on the Victory of Orthogonality If I look back at the linear algebra in this book, orthogonal vectors and matrices have won. You could say that they deserved to win. Their success goes all the way back to Chapter 1 on lengths and dot products. Let me recall some of their victories and add new ones. 1 The length of Qx equals the length of z: (Qz)T(Qz) = 2TQTQzx = T = ||z||2 2 The dot product (Qx)T (Qy) equals the dot product zTy: zTQTQy=zTy. 3 The projection matrix onto the column space of Q (m by n) is P = QQT = (QQT)2. 4 The least squares solution to Qx = b (m > n) is = QTb = QT b (pseudoinverse). If we add that Q is square, then QT = Q~!. More is coming in Chapters 5, 6, 7. 5 All powers Q\" and products Q,Q: ... Qx are orthogonal : | determinant | = 1. 6 The eigenvectors g of a symmetric matrix S can be chosen orthonormal. 7 The singular vectors of every matrix are orthonormal. That list shows something important. The success of orthogonal matrices is tied to the sum of squares definition of length: ||z||> = zTz. In least squares, the derivative of ||Ax — b||? leads to a symmetric matrix S = ATA. Then S is diagonalized by an orthogonal matrix () (the eigenvectors will come in Chapter 6). Every A is diagonalized by two orthogonal matrices (), and ()2 (the singular vectors in Chapter 7). And here is more about orthogonal matrices: A = QS and A = QR. 8 Every invertible matrix equals an orthogonal @ times a symmetric S. Example Az[z g]=—\\}—5[f _;]\\/g[i ;]=QS 9 Every invertible matrix equals an orthogonal Q times an upper triangular R. Example () and R come from the Gram-Schmidt algorithm in Section 4.4 : A (2 0]-22 4[5 4] on S5 Determinants 5.1 3 by 3 Determinants and Cofactors 5.2 Computing and Using Determinants 5.3 Areas and Volumes by Determinants The determinant of a square matrix is an astonishing number. If det A = 0, this signals that the column vectors are dependent and A is not invertible. If det A is not zero, the formula for A~! will have a division by the determinant. Section 5.1 will find 3 by 3 determinants and then the “cofactor formulas” for det A and the inverse matrix. There are three useful formulas for det A. After elimination we can multiply the pivots from the triangular matrix U in PA=LU. Alwaysdet P=1 or —1 and det L =1. So the product of pivots in U gives det A or —det A. This is usually the fastest way. A second formula uses determinants of size n — 1: the “cofactors” of A. They give the best formula for A~!. The i,j entry of A~! is the j,i cofactor divided by det A. The big formula for det A adds up n ! terms—one for every path down the matrix. Each path chooses one entry from every row and column, and we multiply the n entries on the path. Reverse the sign if the path has an odd permutation of column numbers. A 3 by 3 matrix has six paths and the big formula has six terms—one for every 3 by 3 permutation. Section 5.2 begins with key properties like det AB = (det A) (det B). To prove that great fact, we begin with three simple properties that lead to every determinant. From those three, everything else follows—including the famous Cramer’s Rule to solve linear equations Az = b. Section 5.3 moves to geometry : The volume of a tilted box in n dimensions. The column vectors in the edge matrix E give n edges going out from the corner at (0, . ..,0). Then the volume of the box is | det E|. This connects geometry to linear algebra. This chapter could easily become full of formulas! When an n by n matrix multiplies all points in an n-dimensional shape, it produces another shape. A cube goes to a tilted box. A circle goes to an ellipse, and a sphere goes to an ellipsoid. The ratio of the output volume to the input volume is always | det A|. Determinants could tell us everything, if only they were not so hard to compute. 198 5.1. 3 by 3 Determinants and Cofactors 199 5.1 3 by 3 Determinants and Cofactors 1 The determinant of A = LZ 3} is ad — bc. The singular matrix [Z 22?;] has det = m = [2 (117] has det PA = bc — ad = — det A. Row exchange 0 1][a b . PA reverses signs 11 0|c d Det is linear in 3 The determinant of [ row 1 by itself. ra+yA zb+yB C d ] is x(ad — bc) + y(Ad — Bc). 4 3 by 3 determinants have 3! = 6 terms in equation (1). They are separated into 3 terms using cofactors in equation (4). Those cofactors divided by det A produce A~! in equation (7y Determinants lead to beautiful formulas. But often they are not practical for computing. This section will focus on 3 by 3 matrices, to see how determinants produce the matrix A~ 1 (This formula for A~! was not seen in Chapters 1 to 4.) First come 2 by 2 matnces: 1 O 0 1 a b det[o 1]—1 det[ ]——1 det[c d]—ad—bc det[ c d ]zbc — ad a b Those examples show an important rule. Determinants reverse sign when two rows are exchanged (or two columns are exchanged). This rule becomes a key to determinants of all sizes. The matrix has no inverse when its determinant is zero. For 2 by 2 matrices, det A = 0 means a/c = b/d. The columns are parallel. For n by n matrices, det A = 0 means that the columns of A are not independent. Then a combination of columns gives the zero vector: Az = 0 with # 0. A is not invertible. These properties and more will follow after we define the determinant. 3 by 3 Determinants 2 by 2 matrices are easy: ad — bc. 4 by 4 matrices are hard. Better to use elimination (or a laptop). For 3 by 3 matrices, the determinant has 3! = 6 terms, and often you can compute it by hand. We will show how. Every permutation has determinant 1 or —1. Start with the identity matrix (det I = 1) and exchange two rows (then det = —1). Exchanging again brings back det = +1. You quickly have all six permutation matrices. Each row exchange will reverse the sign of the determinant (+1 to —1 or else —1 to +1): det = +1 -1 +1 -1 +1 -1 If 1 exchange two rows of A—say rows 1 and 2—then its determinant changes sign. This will carry over to all determinants : Row exchange multiplies det A by —1. 200 Chapter 5. Determinants When you multiply a row by a number, this multiplies the determinant by that number. Suppose the three rows are abc and pq r and = y z. Those nine numbers multiply +1. - p- - - - po - - - \" a 17 b b c be a q p r q p r z z 7 T J) Y . -l - - b - - - e - - - det = +aqz —-bpz +brzx —cqzx +cpy —ary Finally we use the most powerful property we have. The determinant of A is linear in each row separately. As equation (4) will show, we can add those six determinants. To remember the plus and minus signs, I follow the arrows in this picture of the matrix. N\\ Combine those 6 simple determinants into det A = det=|p 7 - - - +++ Notice! Those six terms all have one entry from each row of the matrix. They also have one entry from each column. There are 6 = 3! terms in det A because there are six 3 x 3 permutation matrices. A 4 x 4 determinant will have 4! = 4 x 3 x 2 x 1 = 24 terms. + agz + brz + cpy —ary — bpz —cqzx (1) Let me jump in with examples of sizes 1, 2, 3, 4. This is pretty sudden but you can see their determinants. My favorite matrices have 2’s on the main diagonal and —1’s on the diagonals above and below. Here are sizesn =1,2,3: 2 _1 2 -1 0 A, = (2] A,:[ ] Az3=|-1 2 -1 -1 2 0 -1 2] det A; = 2 det A, =4-1=3 det A3 =8—-2-2=4 What are the six terms for det A3 = 4? The diagonal gives (2) (2) (2) = 8. Five more: Other [2 1T -1 I -1 I o1l 0 determinant ﬁVC —l _1 _1 _1 2 erminants 1 -2,-2,0,0,0 terms - - 4L 2.J LO J o —1 J LO - So the six terms add to 8 — 2 — 2 = 4 and this will be det A3. Please watch the pattern: Each of the n! = 3! = 6 terms involves one number from every row and every column. When any of these numbers is zero, that whole term is zero. Computing det A is much easier when the matrix has many zeros. Now we move to the 4 x 4 matrix A; with 24 terms. Here is the key simplification and it will teach us a lot. The 4 x 4 determinant comes from two 3 x 3 determinants ! 5.1. 3 by 3 Determinants and Cofactors 201 T 2 -1 ~1 -1 0] Ag= 1 92 1 det Ag=2det| -1 2 -1|=(—=1)det| 0 2 -1 L 12 I -1 2] | 0 -1 2] Those 3 x 3’s from rows 2, 3, 4 are the cofactors of 2 and —1 in row 1. (2) Notice : 2 in the top row of A is multiplying a 3 by 3 from the other rows. One number from each row and each column! The 2 came from row 1, column 1. So 2 multiplies the 3 x 3 determinant in rows and columns 2, 3, 4. That 3 x 3 matrix is A3 with det = 4. The final 3 by 3 matrix in equation (2) also uses rows 2, 3, 4 of A. The bold —1 comes from row 1, column 2. So the 3 by 3 matrix must use columns 1,3, 4. The determinant of that 3 by 3 is —3 (please check this). Finally the overall determinant of A; is 8 — 3 = 5. One more thing to learn about that last 3 by 3 from columns 1, 3,4. In equation (2) it is multiplied by the —1 from row 1, column 2 of A4. But equation (2) has another mysterious —1 ! Where does that minus come from ? 1t is there because row 1, column 2 is a “minus position”. 1 4+ 2 is an odd number ! The sign pattern was already set for 2 x 2. det[a b]zdet[a ‘+det[ b]zatimesdminusbtimesc. (3) c d d c Finally A,, A,, A3, A4 have determinants 2,3,4,5. You can guess that det As = 6. Same idea as before det As = 2det Ay — det Az = 2(5) — 4 = 6. We will explain. Cofactors and a Formula for A1 For 3 x 3 determinants with 6 terms like aqz, I can explain the *“cofactor formulas™ for the determinant and the inverse matrix. The idea is to reduce from 3 x 3 to 2 x 2. Two of the six terms in det A start with a and with b and with ¢ from row 1. Cofactor formula 3 X 3 matrix det A=a(qgz —ry)+b(rz—pz)+c(py —gx).| A We separated the factors a, b, ¢ in row 1 from their “cofactors”. Each cofactor is a 2 by 2 determinant. The factor a in the 1, 1 position takes its cofactor from row and column 2 and 3. Every row and column is used once ! Notice that b in row 1, column 2 has cofactor rx — pz. There we see the other rows 2,3 and columns 1,3. But the actual 2 by 2 determinant would be pz — rx. The cofactor C; reversed those + signs: 1 + 2 is odd. Here is the definition of cofactors C;; and the (—1)**7 rule for their plus and minus signs. For the 7, j cofactor Cj;;, remove row ¢ and column 3 from the matrix A. Ci; equals (—1)*+7 times the determinant of the remaining matrix (size n — 1). The cofactor formula alongrow tis det A =a;;Ci1 + -+ + @inCin (5) 202 Chapter 5. Determinants Key point: The cofactor C; just collects all the terms in det A that are multiplied by a;;. Thus the cofactors of a 2 by 2 matrix involve 1 by 1 determinants d, ¢, b, a times (—1)**7, d —c -b a |’ Now notice something wonderful. If A multiplies CT (not C) we get det A times I: ad - be 0 | detA 0 0 ad - bc | 0 det A Dividing by det A, cofactors give our first and best formula for the inverse matrix A™. The cofactors of A = [ ‘: 3 ] are in the cofactor matrix C = [ ACT = [ ] = (det A)I. (6) Inverse matrix formula A 1 =CT/det A (M This formula shows why the determinant of an invertible matrix cannot be zero. We need to divide the matrix CT by the number det A. Every entry in the inverse matrix A~! is a ratio of two determinants (size n — 1 for the cofactor in CT divided by size n for det A). The example below has determinant 1, so A~! is exactly CT = adjugate matrix. Notice how (3, removes row 3 and column 2. That leaves a 2 by 2 matrix with determi- nant 1. Since (—1)**3 = —1, this becomes —1 in CT. It goes into row 2, column 3. Example of A1 111177 [1 -1 o CT Determinant = 1 Al=101 1 =10 1 —-1|-= EE (8) Cofactorsin CT 0 0 1 0 0 1 The diagonal entries of ACT are always det A. That is the cofactor formula (5). Problem 18 will show why the off-diagonal entries of ACT are always zero. Those numbers turn out to be determinants of matrices with two equal rows. Automatically zero. A typical cofactor C3; removes row 3 and column 1. In our 3 by 3 example, that leaves a 2 by 2 matrix of 1's, with determinant = 0. This is the bold zero in A~ : 1, 3 position. If we change to 24, the determinant is multiplied by (2) (2) (2) = 8. All cofactors in C are multiplied by (2) (2) =4. Then (24)~! =4C7/8det A is A~! divided by 2. Right! Example: The —1, 2, —1 Tridiagonal Matrix The cofactor formula reduces n by n determinants to their cofactors of size n — 1. The —1, 2, —1 matrix is a great example because row 1 has only two nonzeros 2 and —1. So we only need two cofactors of row 1—and they will lead us to A,,_; and A,_2. Watch how this happens for the determinant of the —1, 2, —1 matrix A,, : _\"1’ '; _(1) g 2 -1 0] —1 -1 0] det| o 7 5 _3 =2(-1)'\"*1det| -1 2 —-1|-1(-1)\"*2det| 0 2 -1 On the right hand side, the first matrix gives 2 det A,,—1. The last term has —1 from the 1,2 entry of A,. It also has (=1)!*? = ~1 from the sign (—1)**7 that goes with every cofactor. And that last cofactor itself has —1 in the top left entry, with makes three —1’s. 5.1. 3 by 3 Determinants and Cofactors 203 That last cofactor of A,, is showing its own cofactor of size n — 2. That matrix is A,_2! det A,, = 2det A,,_; —det A,,_,. 9) Working directly we found det A, = 2,3,4,5 for n = 1,2,3,4. All those fit with equation (9), as in det Ay = 5 = 2(4) — 3. But now we have found det A, =n +1 for every n. This fits the formula (9) because n + 1 = 2(n) — (n — 1). The cofactor formula is most useful when the matrix entries are mostly zero. Then we have few cofactors to find. And the entries of those cofactors are also mostly zero. For a dense matrix full of nonzeros, we look next for a better way to find its determinant. The better way brings back elimination, to produce easy determinants of L and U. Problem Set 5.1 Questions 1-5 are about the rules for determinants. 1 If a 4 by 4 matrix has det A = %, find det(2A) and det(—A) and det(A*) and det(A™1). 2 Ifa3by 3 matrix has det A = —1, find det(1A) and det(—A) and det(A?) and det(A~!). What are those four answers if det A =0? 3 True or false, with a reason if true or a counterexample if false : (a) The determinantof I + A is 1 + det A. (b) The determinant of 4A is 4 times the determinant of A. 0 0 (c) The determinant of AB — BA is zero. Try an example with A = [ 0 1 ] : 4 Exchanging rows reverses the sign of det A.-Which row exchanges show that these “reverse identity matrices” J3 and J; have |J3| = —1but |J4| = +1? 0o ¥ det |0 1 0] =-1 but det | =+1. 1 0 0 0100 - - 10 0 0 5 Forn = 5,6,7, count the row exchanges to permute the reverse identity J, to the identity matrix I,,. Propose a rule for every det J, and predict whether Jyo1 has determinant +1 or —1. 6 Find the six terms in equation (1) like +aqz (the main diagonal) and —cqz (the anti-diagonal). Combine those six terms into the determinants of A, B, C. 1 -1 0 (2 1 4 \"1 2 3] A=| -1 1 -1 B=|4 2 8 C=|45 6 0 -1 1 6 3 12 7 8 9 204 10 1 12 13 14 15 16 17 18 19 20 Chapter 5. Determinants If youaddrow1 = [a b c] torow 2 = [p q r] to get [p+ a g+b r+c] in row 2, show from formula (1) for det A that the 3 by 3 determinant does not change. Here is another approach to the rule for adding two rows : [ row 1 ] \" row 1 ] row 1 \" row 1 det | rowl4+row2 | =det| rowl |+det| row2 | =0+ det | row 2 i row J ) _row3_ _row3J _row3- Show that det AT = det A because both of those 3 by 3 determinants come from the same six terms like brz. This means det PT = det P for every permutation P. Do these matrices have determinant 0,1, 2, or 3 ? 0 0 1 0 1 1] 1 1 17 1 1 1 A=1|1 0 0| B=]1 01| Cc=1]1 11| D=0 1 o0]. 01 0 1 1 0] 11 1 11 1 If the entries in every row of A add to zero, solve Ax = O to prove det A = 0. If those entries add to one, show that det(A — I) = 0. Does this meandet A = 1? Why does det(P, P,) = (det P,) times (det P;) for permutations ? If P, needs 2 row exchanges and P, needs 3 row exchanges to reach I, why does P, P, reach [ from 2 + 3 exchanges ? Then their determinants will be (—1)° = (=1)?%(-1)3. Explain why half of all 5 by 5 permutations are even (with det P = 1). Find the determinants of a rank one matrix A and a skew-symmetric matrix B. 1] 0 1 3] A=|2([1 -45] ad B=|[-1 0 4 3 -3 -4 0] If the i, j entry of A is i times j, show that det A = 0. (Exception when A = [1].) If the 7, j entry of Ais ¢ + j, show that det A = 0. (Exception whenn = 1 or 2.) Place the smallest number of zeros in a 4 by 4 matrix that will guarantee det A = 0. Place as many zeros as possible while still allowing det A # 0. If all the cofactors are zero, how do you know that A has no inverse? If none of the cofactors are zero, is A sure to be invertible? Cofactor formula when two rows are equal. Write out the 6 terms in det A when a 3 by 3 matrix has row 1 = row 2 = g, b, c. The determinant should be zero. Why is a matrix that has two equal rows always singular? Then det A = 0. If we combine the cofactors from one row with the numbers in another row, we will be computing det A* when A* has equal rows. Then det A* equals 0. This is what produces the off-diagonal zeros in ACT = (det A) I. From the cofactor formula ACT = (det A)I show that det C = (det A)™~1, Suppose det A = 1 and you know all the cofactors of A. How can you find A ? 5.2. Computing and Using Determinants 205 5.2 Computing and Using Determinants K Useful facts : det AT = det A and det AB = (det A) (det B) and |det Q| = 1\\ 2 Elimination matrices have det £ = 1 so det EA = det A is unchanged. 3 Cramer’s Rule finds £ = A~'b from ratios of determinants (a slow way). 4 det A = % product of the pivots in A = LU (a much faster way). {The big formula for det A has n! terms from n! permutations (very slow if n > 3)./ The determinant of a square matrix tells us a lot. First of all, an invertible matnx has det A # 0. A singular matrix has det A = 0. When we come to eigenvalues A and eigenvectors & with Az = Az, we will write that eigenvalue equation as (A — AI)x = 0. This means that A — AI is singular and det(A — AI) = 0. We have an equation for A. Overall, the formulas are useful for small matrices and also for special matrices. And the properties of determinants can make those formulas simpler. If the matrix is triangular or diagonal, we just multiply the diagonal entries to find the determinant : Triangular matrix (a b c] @ - . suia a. r det g 1t | =det q =aqz (1) Diagonal matrix z | . If we transpose A, the determinant formula gives the same result Transpose the matrix det(AT) = det(A) (2) If we multiply AB, we just multiply determinants (this is a wonderful fact) Multiply two matrices det(AB) = (det A) (det B) (3) A proof by algebra can get complicated. We will give a simple proof of (3) by geometry. When we add matrices, we do not just add determinants! (See this from I + 1. The determinant is 2™ not 2.) Here are two good consequences of equations (2) and (3): Orthogonal matrices Q have determinant 1 or —1 We know that QTQ = I. Then (det Q)2 = (det QT) (det Q) = 1. Therefore det Q is £1. Invertible matrices have det A = £ (product of the pivots) 4) If A= LU thendet A = (det L) (detU) = detU. Triangular U : Multiply the pivots. If PA = LU because of row exchanges, then det P = 1 or —1. Permutation matrix ! Multiplying the pivots Uy; Usz...Unn on the diagonal reveals the determinant of A. Elimination is how determinants are computed by virtually all computer systems for linear algebra. The cost to find U in Chapter 2 was n3/3 multiplications. 206 Chapter 5. Determinants Proving those Properties (1) to (4) The proofs of (1) to (4) can begin with three simple properties of determinants. 1. The n by n identity matrix has det I = 1. 2. Exchanging two rows of A reverses det A to —det A. 3. If row 1 of A is a combination cv + dw, then add 2 determinants: [ cv+dw | v L w ] row 2 row 2 row 2 det - = ¢ det + d det - (5) | rown | | rown | | rown In other words “the determinant is linear with respect to row 1.” Then Property 2 leads us further: det A is linear with respect to every row separately. We can exchange rows and exchange back. So nothing is special about row 1. Property 2 also leads to this simple fact. If A has two equal rows, its determinant is zero. Exchanging the rows has no effect on A. Then det A = — det A forces det A = 0. Now use rule 3: Subtracting d times row ¢ from row j leaves det A unchanged. i row 1 ] row1 | rowl | det | ow2—drowl | —det| Tow2 | — 4| rowl | — det A (6) p— e - Tow n | rown rown This was “linearity in row 2 with row 1 fixed”. It means (again) that our elimination steps from the original matrix A to an upper triangular U do not change the determinant. Then elimination (without row exchanges) is the way to simplify A and its determinant. No row exchange det A =detU = Uy Uz ...U,, = product of the pivots| (7) Here is a proof that det AB = (det A)(det B). Assume det B # 0. We will show that D(A) = det AB /det B has properties 1,2, 3 above. Then D(A) must be the same as det A. Here are those three properties confirmed for D( A). 1. If A = Ithen D(I) = det B/ det B correctly gives the answer D(I) = 1. 2. Exchanging rows of A will exchange rows of AB. Then D(A) changes sign ! 3. Multiplying row 1 of A by ¢ will multiply D(A) by c. If row 1 of A is v + w, then row 1 of AB is vB + wB. Equation (5) separates det(AB) into two determinants. So D(A) also follows equation (5). From 1,2, 3, det(AB) /det B must be det A. 5.2. Computing and Using Determinants 207 Cramer’s Rule to Solve Az = b A neat idea gives the first component z; of the solution vector to Ax = b. Replace the first column of I by z. This triangular M, has determinant ;. When you multiply by A, the first column becomes Ax which is b. The other columns of B, are copied from A. Key idea [ 4 IRE (1) g -21 a2 a3 5 @ 32 =102 a2 a3| = Dj. AM, = By I | lxs 0 1] |bs a3 as3) We multiplied a column at a time. Take determinants of the three matrices to find z, : Productrule |[(detA)(z;) = detB; or x; = detB;/det A.| (9) This is the first component of & in Cramer’s Rule. Changing a column of A gave B,. To find x2 and Bs, put the vectors & and b into the second columns of I and A: Same idea - - (1) 1 g? : B (10) a, a; as T2 = |a az| = Dj. AM2 = 32 I 0 I3 1 1 Take determinants to find (det A)(x2) = det Bs. This gives 2 = (det B;)/(det A). Example 1 Solving 3r; + 4r2 = 2 and 5z + 612 = 4 needs three determinants: Put 2 and 4 into each B The determinants of B; and B, are —4 and 2. Those are divided by det A = —2: -4 2 3 4 2 2 A — A1 = —_— = = —_——= - = Findxz =A b Iy = _2—2 I2 —9 1 Ch“kls 6][—1] [4] CRAMER’s RULE If det A is not zero, Az = b is solved by determinants: _ detB, det B, __ detB, ~ det A 2= et A \" detA (11) Iy The matrix B; has the jth column of A replaced by the vector b. To solve an n by n system, Cramer’s Rule evaluates n + 1 determinants (of A and the n different B’s). When each one is the sum of n! terms—applying the “big formula” with all permutations—this makes a total of (n + 1)! terms. It would be crazy to solve equations that way. But we do finally have an explicit formula for the solution to Ax = b. Example 2 Cramer’s Rule is not efficient for numbers but it is well suited to letters. For n = 2, find the two columns x and y of A~! by solving AA™! = A[z y| = I. 208 Chapter 5. Determinants e a)[n]-[3] w=[2a][n]-[3] Those share the same matrix A. We need |A| and four determinants for 1, T2, ¥1, ¥2: Al = a b and 1 b a 1 0 b a O “lc d 0 d c O 1 d c 1 The last four determinants are d, —c, —b, and a. (They are the cofactors!) Here is A™!: d —c -b a 1 d —-b = —, = —, = —, = — dA—lz . SR 7 R VTR VTRR R V T ad — be [ —c a ] A~ involves the cofactors of A. When the right side of Ax = b is a column of I, as in AA~! = I, the determinant of each B; in Cramer’s Rule is a cofactor of A. You can see those cofactors for n = 3. Solve Az = (1,0.0) to find column 1 of A~!: : ; 1 a2 a an 1 a any a2 1 Determinants of B's 0 au 013 a“ o al3 all al2 0 are Cofactors of A 22 @23 21 23 21 Q22 0 asz2 4asz3 asi 0 ass asy as2 0 That determinant of B, is the cofactor Cj; = ag2a33 — as3zasz. Notice that the correct minus sign appears in det B = —(az;a33 — a23a3;). This cofactor C2 goes into col- umn 1 of A~!. When we divide by det A, we have computed the same A~! as before: .. T C; a-1= C (12) det A det A FORMULAFOR A~! (A7), = The Big Formula for the Determinant : n! Terms So far we have found two formulas for det A. One was the cofactor formula, which reduces det A to a combination of smaller determinants. (This worked well when A had only 3 nonzero diagonals and lots of zeros.) The second was the pivot formula, coming from elimination and PA = LU. This worked well because L and U are triangular in Chapter 2. And det P = %1. Our big formula is not so great, because it has all of the n! terms in det A. Every n by n permutation matrix P (n! P’s) produces a term Dp in the big formula. Each matrix P has n 1’s: a single 1 in each row and each column. We multiply the n numbers a;,a25 - - - an,, in those positions, to find their contribution Dp to the overall determinant of A. And we remember to include the factor det P = +1 in Dp. This accounts for an even or odd permutation. Now add up all the terms like Dp = ad or aqz. Big Formula det A = Sum of all Dp = n! terms from n! permutations P| (13) For n = 2 this formula is exactly ad — bc (2 permutations give Dp = ad and —bc). For n = 3 we have 3! = 6 permutations and 6 terms in equation (13). det A = +aqz + brz + cpy — ary — bpz — cqzx. 5.2. Computing and Using Determinants 209 Problem Set 5.2 1 If det A = 2, what are det A~! and det A™ and det AT ? 2 Compute the determinants of A, B, C, D. Are their columns independent? r 1 1 0] 1 23] ~_[(4 0 p_[A 0O A=1|1 0 1| B=1|4 5 6 0 A 0 B 01 1 7 8 9 A and B are 2 by 2 3 Show that det A = 0, regardless of the five numbers marked by z’s: I I I What are the cofactors of row 1? A=10 0 =z]|. What is the rank of A? 0 0 =z What are the 6 terms in det A? 4 (a) If D,, = det(A™), could D,, grow toward infinity even if all |4;;| < 1? (b) Could D,, approach zero even if all |A;;| > 17 Problems 5-9 are about Cramer’s Rule for z = A~1b. 5 Solve these linear equations by Cramer’s Rule z; = det B,/ det A: . 21 + T2 =1 (a) 2? 12:‘2 — ; (b) T1+2r0+ z73=0 . 2= T2 + 213 = 0. 6 Use Cramer’s Rule to solve for y (only). Call the 3 by 3 determinant D: az+by+ cz=1 (b) dr+ey+fz=0 gz + hy+ iz=0. ar + by =1 (a) cx+dy=0 7 Cramer’s Rule breaks down when det A = 0. Example (a) has no solution while (b) has infinitely many. What are the ratios ; = det B;/ det A in these two cases? 2r, + 32 =1 4z, + 69 = 2 21 +3z2 =1 41 + 629 =1 (same line) (a) (parallel lines) (b) 8 Quick proof of Cramer’s rule. The determinant is a linear function of column 1. It is zero if two columns are equal. When b = Az = z,a, + 12a2 + T3a3 goes into the first column of A, we have the matrix B; and Cramer's Rule £, = det B;/det A: |b a2 a3| = |:B1al + T2a2 + T3Q3 Q2 03| = xllal a) 03| = 1, det A. What steps lead to the middle equation? 9 If the right side b equals the first column of A, solve the 3 by 3 system Ax = b. How does each determinant in Cramer’s Rule lead to this solution x ? 210 10 11 12 13 14 15 16 17 18 19 Chapter 5. Determinants Suppose E,, is the determinant of the n by n three-diagonal 1,1,1 matrix. By cofactors of row 1 show that E, = E,,_, — E,,_5. Starting from £}, = 1 and E, =0find Ej3, Ey, ..., Eg. By noticing how the E’s repeat, find E'; 0. » 110 D11 ¢ E1=|1| Ez‘—‘ E3=1 1 1 E4= . 1 1 01 1 0O 1 1 1 0 01 1 If a 3 by 3 matrix has entries 1,2, 3,4,...,9, what is the maximum determinant ? I would use a computer to decide. This problem does not seem easy. True or false : The determinant of ABC is (det A) (det B) (det C) Reduce A to U and find det A = product of the pivots : ﬂ 1 A 2 3- A= 1 1 1 2 1 2 1 2 3 By applying elimination to produce an upper triangular matrix U/, compute 1.2 3 0] 1 -1 0 O 2 6 6 1 -1 2 -1 0 detl 1003 ™ o1 2 020 7 0 0 -1 2 Use row operations to compute these determinants: 101 201 301] 1 a a* (b a)(c— a)(c - b) det | 102 202 302 and det|1 b b*| = . o 103 203 303 1 ¢ 2 Vandermonde determinant In a 4 by 4 determinant, decide det P = 1 or —1 for these big formula terms : taj2a21a34a43 + aj3a22a31a044 + aj1a22a34043 (a) If a;; = az2 = a3z = 0, how many of the six terms in det A will be zero? (b) If aj1 = a22 = a3z = agq = 0, how many of the 24 products a;;a2xa31a4m are sure to be zero? The big formula has n! terms. But if an entry of A is zero, (n — 1)! terms disappear. If A has only three nonzero diagonals (in the center of A), how many terms are left? For n = 1,2,3,4 that tridiagonal determinant of A has 1,2, 3,5 nonzero terms. Those are Fibonacci numbers! Show why a tridiagonal 5 by 5 determinant has 5 + 3 = 8 nonzero terms (Fibonacci again). Use the cofactors of a;; and a;2. The Big Formula has 24 terms if A is 4 by 4. How many terms include a;3 and a22 ? It is a challenge to write down all 24 terms in det(A). 5.3. Areas and Volumes by Determinants 211 5.3 Areas and Volumes by Determinants K A parallelogram in 2D starts from (0, 0) with sides e; = (a,b) and ez = (c, d). \\ 2 Area of the parallelogram = | Determinant of the matrix E' = [el e ] | = |ad—bc|. 3 Attilted box in 3D starts with three edges e;, ez, ez out from (0,0, 0). {Volume of a tilted box = | Determinant of the 3 x 3 edge matrix E'|. / Determinants lead to good formulas for areas and volumes. The regions have straight sides but they are not square. A typical region is a parallelogram—or half a parallelogram which is a triangle. The problem is: Find the area. For a triangle, the area is %bh: half the base times the height. A parallelogram contains two triangles with equal area, so we omit the % Then parallelogram area = base times height. Those formulas are simple to remember. But they don’t fit our problem, because we are not given the base and height. We only know the positions of the corners. For the triangle, suppose the corner points are (0,0) and (a,b) and (c,d). For the parallelogram (twice as large) the fourth corner will be (a+c, b+d). Knowing a, b, c, d, what is the area ? Y, (c,d) (a+c,b+d) Triangle and parallelogram The base could be the lowest side and its length is v/a? + b2. To find the height, we could create a line down from (c, d) that is perpendicular to the baseline. The length h of that perpendicular line would involve more square roots. But the area itself does not involve square roots ! Its beautiful formula ad — bc is simple fora = d = 1,b = ¢ = 0 (a square). Area of parallelogram = | Determinant of matrix | =% = |ad — bc|.| (1) Our goal is to find that formula by linear algebra: no square roots or negative volumes. We took the absolute value |ad — bc| to avoid a negative area. We also have a more difficult goal. We need to move into 3 dimensions and eventually into n dimensions. We start with four corners 0,0,0 and a,b,c and p,q,r and z,y, 2 of a box. (The box is tilted. Every side is a parallelogram. It will look lopsided.) 212 Chapter S. Determinants If we use the area formula (1) as a guide, we could guess the correct volume formula: (2) Volume of box = | Determinant of matrix | = + 0 O N S Q 3 N R Our first effort stays in a plane. For this case we use geometry. Figure 5.1 shows how adding pieces to a parallelogram can produce a rectangle. When we subtract the areas of those six pieces, we arrive at the correct parallelogram area ad — bc (no square roots). The picture is not very elegant, but in two dimensions it succeeds. < . - (a + ¢,b + d) = (base, height) of rectangle b be ! ab/2 \"\"\" (c,d) Area of parallelogram cdf2 d Rectangle minus 6 pieces d cd/2 (a+c) (b+d)-2bc—ab—-cd (av b) _____ =ad — bc ab/2 . bc |b a C Figure 5.1: Adding six simple pieces to a parallelogram produces a rectangle. Would a similar construction be possible in three dimensions? Following Figure 5.1, I believe we could add simple pieces to make a tilted box into a rectangular box— but it doesn’t look easy. And there is a much better way : Use linear algebra. Areas and Volumes by Linear Algebra In working on this problem, I came to an understanding. If we do more algebra, then we need less geometry. Very often, linear algebra comes down to factoring a matrix. We will look there for ideas. A box in n dimensions has n edges e;, e, .. ., e, going out from the origin. The parallelogram in two dimensions had two vectors e; = (a,b) and ez = (c,d). Those vectors e give two corners or n corners of the “box”. In the 2-dimensional picture, the fourth corner was e; + e;. In the n-dimensional picture, the other corners of the box would be sums of the e’s. The box is totally determined by the n edges in the matrix E . N | - Edge matrix E; = [ ‘; ; ] and E,=| e, -+ e, (i;::.:dngse; N Our goal is to prove that the volume of the box is | det E|. We considered three possible factorizations of E, to reach this goal. They are taken from Chapters 2 and 4 and 7. 5.3. Areas and Volumes by Determinants 213 The third factorization is called the Singular Value Decomposition of E: the SVD. Lower times upper triangular E=LU Orthogonal times upper triangular R E=QR Orthogonal - Diagonal - Orthogonal E=UXTVT Those factors of E are square matrices because E is square (n by n). Remember that the determinant of L is 1 (all ones on its diagonal). The determinant of any orthogonal matrix Qis1or —1. Thendet L = |det Q| = |det U| = | det V| = 1. We will certainly depend on the multiplication rule det AB = (det A) (det B). Three options for | det E: Box volume = |det E| = |detU | = |det R| = |det |. (3) The problem is to connect the volume of the box to one of those determinants—to connect geometry to linear algebra. Start with the geometry of an orthogonal matrix Q. Key idea for any region in R™: Multiplying all points by an orthogonal matrix Q does not change the volume. Let me understand this first and then use E = QR. Multiply by any matrix : Straight lines stay straight, areas can change Multiply by an orthogonal Q : T« and Ty are the same as (Qz)T(Qx) and (Qx)! (Qy) Lengths and angles and box shapes and volumes are not changed by rotations Q. This remains true for curved regions. We divide them into many small cubes plus thin curved pieces. The total volume of those curved pieces can approach zero. The volumes of the cubes are not changed by Q. The shapes for R and for E = Q R have the same volume. R is a triangular matrix. Its box has a volume we can compute. For a parallelogram in the zy plane, the base and height are the diagonal entriesu and wof Rin E = QR: (v, w) [ u v ] ! base = u, height = w R - w . 0 w i, |area| = |u times w| = |det R | *-—UY —> (‘U, 0) The key point is: The main diagonal of R shows the height in each new dimension. When we multiply those numbers on the diagonal of R, we get the volume of the box and also the determinant of the triangular matrix R. The volume formula |det E'| is now proved in all dimensions because |det Q| = 1 and |det E| = |det R|. Final comment : The Singular Value Decomposition E = UXVT has two orthogonal matrices U and V. The number | det E| is equal to | det £|. And this matrix ¥ is diagonal. Y. gives a perfectly normal rectangular box in R\". This SVD approach by ULV T looks simpler than Q R, where the triangular matrix R produced the tilted box above. But... 214 Chapter 5. Determinants But that tilted figure shows a clear geometric meaning for the diagonal entries u and w : base and height. The geometry of the SVD will be seen in Chapter 7. It is beau- tifully clear for ellipses in n dimensions. But the singular values are not so clear for boxes. ¥ gives the lengths of the axes of an ellipse but not the sides of a rectangular box. For a box with straight sides, E' = QR leads directly to volume of box = | det R)|. Every shape has the growth factor det E when all points are multiplied by E. Problem Set 5.3 1 For a 3-dimensional cube with four corners at (0, 0,0),(1,0,0), (0,1.0),(0,0,1), what is the edge matrix E'? For a tilted box with edges e;, ez, e3 going out from (0,0, 0), what are the 8 corners of the box ? Suppose the edge vectors e;. e2, e3 in 3 dimensions are perpendicular unit vectors. Describe the box and find its volume. If the edge vectors have lengths ||e;|| = 1,]||ez2|| = 2,||es|| = 3, guess the largest possible volume of the box. Describe the box that reaches this maximum volume. Suppose an n by n matrix E has columns e), e, ..., e,. Problem 3 becomes |determinant of E | < product of column lengths =||e, || ||e2|| - - - ||en]|. Hint: Why does the triangular R in E = QR have diagonals R;; < ||e;||? (a) Find the area of the parallelogram with edges v = (3, 2) and w = (1, 4). (b) Find the area of the triangle with sides v, w, and v + w. Draw it. (c) Find the area of the triangle with sides v, w, and w — v. Draw it. (a) The corners of a triangle are (2, 1) and (3,4) and (0, 5). What is the area? (b) Add a comer at (-1, 0) to make a lopsided region (four sides). Find the area. This Hadamard matrix H has orthogonal rows. The box is a hypercube ! 1 1 1 1 1 1 -1 -1 s Compute |det H| = 1 -1 -1 1l° volume of a hypercube in R 1 -1 1 -1 The sides have length 2 An n-dimensional cube has how many comers? How many edges? How many (n — 1)-dimensional faces? The cube in R™ whose edges are the rows of 2] has volume . A hypercube computer has parallel processors at the corners with connections along the edges. The triangle with corners (0,0), (1,0), (0, 1) has area 3. The pyramid in R3 with four corners (0,0,0), (1,0,0), (0,1,0), (0,0, 1) has volume . What is the vol- ume of a pyramid in R* with five comers at (0, 0, 0, 0) and the four columns of I ? 5.3. Areas and Volumes by Determinants 215 10 1 12 13 14 15 If the edge matrix E'is an orthogonal matrix, the box has volume : If the edge matrix E'is a singular matrix, the box has volume If the volume in R™ is V, the box for 2E has volume : (2 3 14 Draw parallelograms for [g ‘11 ] and ] . Can you see any reason for equal areas ? Transposing the edge matrix gives a matrix with the same determinant and u v 0 w a new parallelogram with the same area. Can you draw it and recompute its area? Draw the parallelogram P with edges from (0, 0) to (a, b) and (c, d). Its area is det [ (; ; ] = det [ a 0 ] + det [ 0 ¢ ] Can you see how to produce the parallelogram P from the rectangle with sides a, d and the rectangle with sides b, c? If the 5 by 5 matrix A contains five 1’s and twenty 2’s, what is the largest possible determinant ? Suppose a 2 by 2 matrix E has column vectors of length ||e; || and ||e2||. Show that |det E'| < || e1]| times || ez||. This is Hadamard’s inequality (2 by 2 case). It was the key idea in Problem 4. Parallelogram area < | side 1 | times | side 2 |. Chapter 6 Eigenvalues and Eigenvectors 6.1 Introduction to Eigenvalues: Ar = Az 6.2 Diagonalizing a Matrix 6.3 Symmetric Positive Definite Matrices 6.4 Complex Numbers and Vectors and Matrices 6.5 Solving Linear Differential Equations Eigenvalues and eigenvectors have new information about a square matrix—deeper than its rank or its column space. We look for eigenvectors x that don’t change direction when they are multiplied by A. Then Az = Ax with eigenvalue A. (You could call X the stretching factor.) Multiplying again gives A%z = A%z. We can go onwards to A%z = \\100% And we can combine two or more eigenvectors : Al + 22) = \\T) + A2 A2(61$1 + 62:'02) = C] /\\f:nl + c2/\\§:c2 When we separate the input into eigenvectors, each eigenvector just goes its own way. The eigenvalues are the growth factors in A\"z = A™z. If all |\\;| < 1 then A™ will eventually approach zero. If any |A;| > 1 then A™ eventually grows. If A = 1 then A\"z never changes (a steady state). For the economy of a country or a company or a family, the size of A is a critical number. Properties of a matrix are reflected in the properties of the A’s and the x’s. A symmetric matrix S has perpendicular eigenvectors—and all its eigenvalues are real numbers. The kings of linear algebra are symmetric matrices with positive eigenvalues. These “positive definite matrices” signal a minimum point for a function like the energy f(x) = -a:TS:c That is the n-dimensional form of the calculus test d f /dz? > 0 for a minimum of f(z). This chapter ends by solving linear differential equations du/dt = Awu. The pieces of the solution are u(t) = e*z instead of u,, = A\\\"z—exponentials instead of powers. The whole solution is u(t) = e”*u(0). For linear differential equations with a constant matrix A, please use its eigenvectors. Section 6.4 gives the rules for complex matnccs—mcludmg the famous Fourier matrix. 216 6.1. Introduction to Eigenvalues: Az = Az 217 6.1 Introduction to Eigenvalues: Az = A\\x [11f [Ic = Az |then # O is an eigenvector of A and the number X is the eigenvala 2 Then r\"m = A\"m] foreverynand (A+cl)x = A+ c)rand A~ 'z = /A if A # 0. 3 (A- M)z = 0 = the determinant of A — AI is zero: this equation produces n A’s. 4 Check N’sby (A1)(A2)---(An) =det Aand A\\;+ - - -+, =diagonal sumay; +- - -+ ann. &Projections have A = 1 and 0. Rotations have A = e*® and e*%: complex numbers) This chapter enters a new part of linear algebra. The first part was about Az = b: linear equations for a steady state. Now the second part is about change. Time enters the picture—continuous time in a differential equation du/dt = Awu or time steps k=1,2,3,...in uxy+1 = Auk. Those equations are NOT solved by elimination. We want “eigenvectors’ x that don’t change direction when you multiply by A. The solution vector u(t) or uy stays in the direction of that fixed vector &. Then we only look for the number (changing with time) that multiplies x : a one-dimensional problem. A good model comes from the powers A, A%, A3,... of a matrix. Suppose you need the hundredth power A'%. Its columns are very close to the eigenvector x = (.6. .4): 8 .3 _[70 451 , [650 .525 100 . [-6000 .6000 A=[, ] A2‘[.30 .55] A3‘[.350 .475] A7~ 4000 .4000 A100 was found by using the eigenvalues 1 and 1/2 of this A, not by multiplying 100 matrices. Eigenvalues and eigenvectors are a new way to see into the heart of a matrix. To explain eigenvalues, we first explain eigenvectors. Almost all vectors will change direction, when they are multiplied by A. Certain exceptional vectors x are in the same direction as Ax. Those are the “eigenvectors”. Multiply an eigenvector by A, and the vector Az is a number A times the original . The basic equation is Az = Az. The number ) is an eigenvalue of A. The eigenvalue A tells whether the special vector z is stretched or shrunk or reversed— when it is multiplied by A. We may find A = 2 or % or —1. The eigenvalue A could be zero ! Then Az = Ox means that this eigenvector is in the nullspace of A. If A is the identity matrix, every vector has Az = x. All vectors are eigenvectors of I. All eigenvalues “lambda” are A = 1. This is unusual to say the least. Most 2 by 2 matrices have rwo eigenvector directions and rwo eigenvalues. We will show thatdet(A — AI)=0. This section explains how to compute the z’s and A’s. It can come early in the course. We only need the determinant ad — bc of a 2 by 2 matrix. Example 1 uses det(A—AI)=0 to find the eigenvalues A = 1 and A = § for the matrix A that appears above. 218 Chapter 6. Eigenvalues and Eigenvectors Example 1 The key numbers in det(4 — AI) are .8 +.7 = 1.5 = % on the diagonal of A and (.8)(.7) — (.3) (-2) = .5 = determinant of A = % : 8 3 8-A 3 | _ o 3 1 1 Az[.z .7] det[ 2 .7-,\\]‘A AT =0GA-D{rA-3 ] I factored the quadratic into A — 1 times A — % to see the two eigenvalues A = 1 and % This tells us that A — I and A — %I (with zero determinant) are not invertible. The eigenvectors ; and Z are in the nullspaces of A — I and A — 11. (A - Iz, = 0is Az, = x,. That first eigenvector is x, = (.6, .4). (A - %1)32 = 0is Az = 2x,. That second eigenvectoris 2 = (1, —1): 2 .6 '8 3][6] [ .6 T, = and Az, = = (Ax; = £, means that A\\; = 1) | 4 2 7)1 4 | 4] 1] '8 3][ 1] 5 .. T2=|_4 and Az, = 2 7| = t—.SW (this is %132 SO A\\ = %). From Az, = z; we get A>z, = Az, = x,. Every power of A will have A\"z, = z,. Multiplying x, by A gave 3, and if we multiply again we get A%z, = (3)? times ;. The eigenvectors of A remain eigenvectors of A2. The eigenvalues \\ are squared. This pattern succeeds because the eigenvectors x;,x- stay in their own directions. The eigenvalues of A'® are 1'% =1 and (3)'® = very small number. An eigenvector of A is also an eigenvector of every A™. Then A\"z = \\\"z. Other vectors do change direction. But all other vectors are combinations of the two eigenvectors x and ;. The first column (.8, .2) of A is the combination (.6, .4)+(.2, —.2). Separate column 1 of A .8 6] [ .2 into eigenvectors [2] - [4] t L_,2] =T1 +.2x,; (1) .6 _ .6 _ .6 4] 4] ] 4 T + 4 A Az A3 +.1 +.05 i [\"’ g] L |-a | [-.05 | | 8] [.7] [.65 _[. Figure 6.1: The first columns of A, A%, A3 are [2] : [3] : [.35] approaching [4J 6.1. Introduction to Eigenvalues: Az = Az 219 Multiply each x; by A 8 . 1 _ 1.6 A (.7 for column 1 of A2 1 2 ® zl+§('2)z2 1.4 t -1 1.3 ) Each eigenvector is multiplied by its eigenvalue, when we multiply by A. At every step x; is unchanged, because \\; = 1. But , is multiplied 99 times by A\\ = 3 : 2 [ very | Column 1 8] . 99 6 et 4 [8] sy 21+ (21 Pm =[]+ | sma || @ vector | This is the first column of A!%. The number we originally wrote as .6000 was not exact. We left out (.2) times () which wouldn’t show up for 30 decimal places. The eigenvector &, is a “steady state” that doesn’t change (because it has A; = 1). The eigenvector T is a “decaying mode” that virtually disappears (because Az = .5). The higher the power of A, the more closely its columns approach the steady state. This particular A is called a Markov matrix. Its largest eigenvalue is A = 1. Its eigenvector ; = (.6,.4) is the steady state—which all columns of A* will approach. See Appendix 8. A giant Markov matrix is the key to Google's superfast web search. For projection matrices P, the column space projects to itself (Px = x). The nullspace of P projects to zero (Px = Ox). The eigenvalues of P are A = 1and A = 0. D .0 ] has eigenvalues A = 1 and A = 0. Example 2 [The projection matrix P = [ Its eigenvectors are £; = (1,1) and 2 = (1, —1). Then Pz, = x, (steady state) and Pzy = 0 (nullspace). This example is a Markov matrix and a singular matrix (with A = 0). Most important, P is a symmetric matrix. That makes its eigenvectors orthogonal. 1. Markov matrix : Each column of P adds to 1. Then A = 1 is an eigenvalue. 2. P is a singular matrix. A = 0 is an eigenvalue. 3. P = PT is a symmetric matrix. Perpendicular eigenvectors [ i ] and [ _i ] The only eigenvalues of a projection matrix are 0 and 1. The eigenvectors for A = 0 (which means Px = Ox) fill up the nullspace. The eigenvectors for A = 1 (which means Pz = ) fill up the column space. The nullspace is projected to zero. The column space projects onto itself. The projection keeps the column space and destroys the nullspace : . 11 2| |3 : 10 2 Projecteachpart v = [—l] + [2] = [1] projectsonto Pv = [0] + [2] Projections have A = 0 and 1. Permutations have all |A\\| = 1. The next matrix E'is a reflection and at the same time a permutation. E also has special eigenvalues. 220 Chapter 6. Eigenvalues and Eigenvectors Example 3 | The exchange matrix E = [ } | has eigenvalues 1 and —1. The eigenvector (1,1) is unchanged by E. The second eigenvector is (1, —1)—its signs are reversed by E. A matrix with no negative entries can still have a negative eigenvalue! The eigenvectors for E are the same as for P, because £ = 2P — I : 0 1] _.[5 5] [1 0 E=2P-1 [1 0]:2[.5 .5]‘[0 1]' *) When a matrix is shifted by I, each )\\ is shifted by 1. No change in the eigenvectors. T2 Pz, =z, T2 Ex,=x, A=1and0 / Pz;=0zx, A=1and -1 Y . . . ™ E o= —T2 Projection P onto a line Reflection E across a line Figure 6.2: Projections P have eigenvalues 1 and 0. Exchanges E have A = 1 and -1. A typical = changes direction, but an eigenvector AT = Az stays on the line through . The Equation for the Eigenvalues: det(A — AI) =0 For projection matrices we found A’s and £'s by geometry: Pr = x and Pz = 0. For other matrices we use determinants and linear algebra. This is the key calculation— almost every application starts by solving det(A — AI) = 0 and Az = Ax. First move Az to the left side. Write the equation Ax = Ax as (A — AI)x = 0. The matrix A — AI times the eigenvector is the zero vector. The eigenvectors make up the nullspace of A — AI. When we know an eigenvalue )\\, we find an eigenvector by solving (A — M)z = 0. Eigenvalues first. If (A — AI)x = 0 has a nonzero solution, A — AI is not invertible. The determinant of A — AI must be zero. This is how to recognize an eigenvalue \\: Eigenvalues The number A is an eigenvalue of A if and only if A — AT is singular. Equation for the n eigenvalues of A det(A — AI) = 0. (5) This “characteristic polynomial” det(A — AI) involves only A, not . Since A appears all along the main diagonal of A—\\I, the determinant in (5) includes (—A)™. Then equation (5) has n solutions A; to A\\, and A has n eigenvalues. 6.1. Introduction to Eigenvalues: Az = Az 221 An n by n matrix has n eigenvalues (repeated \\’s are possible !) Each A leads to x : For each eigenvalue ) solve (A — AI)x = 0 or Az = Az to find an eigenvector x. Example4 A= [ 1 2 ] is already singular (zero determinant). Find its A’s and &’s. 2 4 When A is singular, A = 0 is one of the eigenvalues. The equation Az = Oz has solutions. They are the eigenvectors for A = 0. But det(A — AI) = 0 is the way to find all A’s and x’s. Always subtract AI from A: Subtract \\ along the diagonaltofind A — A\\l = [1 ; A 4 E )‘] : (6) Take the determinant “ad — bc” of this 2 by 2 matrix. From 1 — )\\ times 4 — A, the “ad” part is A2 — 5\\ + 4. The “bc” part, not containing ), is 2 times 2. 1 - A 2 det[ 9 4— ) [-a-nu-n-@@=-¥-n o Set this determinant A®> — 5\\ to zero. One solution is A = 0 (as expected, since A is singular). Factoring into A times A — 5, the other rootis A = 5: det(A — M) = A2 —5X =0 yields the eigenvalues A; =0 and Az = 5. Now find the eigenvectors. Solve (A — AI)x = 0 separately for \\y =0and A\\ =5 (A-0)x = [; ?1 ‘Z = gj yields an eigenvector I = 2} forA; =0 -4 2] [y] [o] . . vyl [1 (A-5Dz [ 2 —1] |z = |o. yields an eigenvector Hit orA2 =95 The matrices A — 0] and A — 51 are singular (because 0 and 5 are eigenvalues). The eigenvectors (2, —1) and (1, 2) are in the nullspaces: (A — AI)x = 0is Az = A\\x. We need to emphasize: There is nothing exceptional about A = 0. Like every other number, zero might be an eigenvalue and it might not. If A is singular, the eigenvectors for A = 0 fill the nullspace: Ax = Oz = 0. If A is invertible, zero is not an eigenvalue. In the example, the shifted matrix A — 51 is singular and 5 is the other eigenvalue. Summary To solve the eigenvalue problem for an n by n matrix, follow these steps : 1. Compute the determinant of A — A\\I. With )\\ subtracted along the diagonal, this determinant starts with A\" or —A™. It is a polynomial in A of degree n. 2. Find the roots of this polynomial, by solving det(A — AI) = 0. The n roots are the n eigenvalues of A. They make A — A singular. 3. Foreach eigenvalue ), solve (A — A\\I)x = 0 to find an eigenvector x. 222 Chapter 6. Eigenvalues and Eigenvectors A note on the eigenvectors of 2 by 2 matrices. When A — Al is singular, both rows are multiples of a vector (a, b). The eigenvector is any multiple of (b, —a). The example had A = 0 : rows of A — 07 in the direction (1, 2); eigenvector in the direction (2, —1) A =5 :rows of A — 51 in the direction (—4, 2); eigenvector in the direction (2, 4). Previously we wrote that last eigenvector as (1,2). Both (1,2) and (2,4) are correct. There is a whole line of eigenvectors—any nonzero multiple of & is as good as z. MATLAB's eig(A) divides by the length, to make each eigenvector x into a unit vector. We must add a warning. Some 2 by 2 matrices have only one line of eigenvectors. This can only happen when two eigenvalues are equal. (On the other hand A = I has equal eigenvalues and plenty of eigenvectors.) Without a full set of eigenvectors, we don’t have a basis. We can’t write every v as a combination of eigenvectors. In the language of the next section, we can’t diagonalize a matrix without n independent eigenvectors. Determinant and Trace Bad news first: If you add a row of A to another row, or exchange rows, the eigenvalues usually change. Elimination does not preserve the A’s. The triangular U has its eigenvalues sitting along the diagonal—they are the pivots. But they are not the eigenvalues of A! Eigenvalues are changed when row 1 is added to row 2: 1 3 1 3 U—[ ] hasA=0and A =1; A—[2 6 0 0 ] has A=0and A\\ = 7. Good news second: The product A\\, times Ay and the sum \\y + A2 can be found quickly from the matrix. For this A, the product is 0 times 7. That agrees with the determinant of A (which is 0). The sum of eigenvalues is 0 + 7. That agrees with the sum down the main diagonal (the trace is 1 + 6). These quick checks always work. The product (A1) - -+ (An) of the n eigenvalues equals the determinant of A A1 + A2 + <+ - + A, equals the sum of the n diagonal entries = (trace of A) Those checks are very useful. They are proved in Problems 16—17 and again in the next section. They don’t remove the pain of computing A’s. But when the computation is wrong, they generally tell us so. To compute the correct A’s, go back to det(A — A\\I) = 0. The trace and determinant do tell everything when the matrix is 2 by 2. We never want to get those wrong ! Here trace = 3 and det = 2, so they all have A = 1 and 2: 1 9 3 1 7 -3 Az[o 2]\"’[-2 o]“[lo —4]' ) That first A is one of the best matrices for finding eigenvalues: because it is triangular. Why do the eigenvalues 1 and 2 of that triangular matrix lie along its diagonal ? 6.1. Introduction to Eigenvalues: Ax = Az 223 Imaginary Eigenvalues One more bit of news (not too terrible). The eigenvalues might not be real numbers. Example 5 The 90° rotation Q = [(1) —(1) are \\7; =t and Ay = —t. Then A\\ + A3 = trace = 0 and \\1\\3 = determinant= 1. ] has no real eigenvectors. Its eigenvalues After a rotation, no real vector Qx stays in the same direction as (x = 0 is useless). There cannot be an eigenvector, unless we go to imaginary numbers. Which we do. To see how i = \\/—1 can help, look at Q2 which is —I. If Q is rotation through 90°, then Q? is rotation through 180°. Its eigenvalues are —1 and —1. (Certainly — Iz = —1x.) Squaring Q will square each ), so we must have A = —1. The eigenvalues of the 90° rotation matrix Q are +i and —i, because i2 = —1. Those A\\’s come as usual from det(Q — AI) = 0. This equation gives A2 + 1 = 0. Its roots are 7 and —i. We meet the imaginary numbers ¢ and —: also in the eigenvectors: Complex 0 -1( |1 — 4 1 and 0 -1 ]z — (] eigenvectors 1 0] |sf i 1 Oof |1 \"1} Somehow these complex vectors ; = (1,7) and €2 = (7, 1) keep their direction as they are rotated in complex space. Don’t ask me how. This example makes the important point that real matrices can easily have complex eigenvalues and eigenvectors. The particular eigenvalues ¢ and —1 also illustrate two properties of the special matrix Q. 1. Q is an orthogonal matrix so the absolute value of each A is |A| = 1. 2. Q) is a skew-symmetric matrix so each A is pure imaginary. A symmetric matrix (ST = §) can be compared to a real number. A skew-symmetric matrix (AT = —A) can be compared to an imaginary number. An orthogonal matrix (QTQ = I) corresponds to a complex number with |A| = 1. For the eigenvalues of S and A and Q, those are more than analogies—they are facts to be proved. The eigenvectors for all these special matrices are perpendicular. Somehow (i, 1) and (1,1) are perpendicular (Section 6.4 will explain the dot product of complex vectors). Eigenvalues of AB and A+ B The first guess about the eigenvalues of AB is not true. An eigenvalue A of A times an eigenvalue 3 of B usually does not give an eigenvalue of AB: False proof ABz = ABx = BAx = B)z. 9) It seems that 3 times A is an eigenvalue. When z is an eigenvector for A and B, this proof is correct. The mistake is to expect that A and B automatically share the same eigenvector x. Usually they don’t. Eigenvectors of A are generally not eigenvectors of B. 224 Chapter 6. Eigenvalues and Eigenvectors A and B could have all zero eigenvalues while 1 is an eigenvalue of AB and A + B. o[ 0ot e[y 8] e nen ] For the same reason, the eigenvalues of A + B are generally not A + 3. Here A + B has eigenvalues 1 and —1 while A and are zero. (At least the trace of A + B is zero.) The false proof suggests what is true. Suppose really is an eigenvector for both A4 and B. Then we do have ABx = A\\3x and BAx = ASx. When all n eigenvectors are shared by A and B, we can multiply eigenvalues and we find AB = BA. That test is important in quantum mechanics—time out to mention this application of linear algebra: A and B share the same n independent eigenvectors if and only if AB = BA. Heisenberg’s uncertainty principle In quantum mechanics, the position matrix P and the momentum matrix Q do not commute. In fact QP — P(Q = I (these are infinite matrices). To have Px = 0 at the same time as Qx = 0 would require * = Iz = 0. If we knew the position exactly, we could not also know the momentum exactly. Problem 36 derives Heisenberg’s uncertainty principle | Pz|| ||Qz| > 3| x| ® REVIEW OF THE KEY IDEAS = 1. Az = Az says that eigenvectors keep the same direction when multiplied by A. 2. Az = )z also says that det(A — A\\I) = 0. This equation determines n eigenvalues. 3. The sum of the A’s equals the sum down the main diagonal of A (the trace). The product of the A’s equals the determinant of A. 4. Projections P, exchanges E, 90° rotations Q have eigenvalues 1,0, —1,1, —t. Singular matrices have A = 0. Triangular matrices have A’s on their diagonal. 3. Special properties of a matrix lead to special eigenvalues and eigenvectors. That is a major theme of Chapter 6. 8 WORKED EXAMPLES = 6.1 A Find the eigenvalues and eigenvectors of A and A2 and A~! and A + 41 [ 2 -1 . [ 5 -4 A-[_l 2] wd A= | S ;- Check the trace \\; + A\\, = 4 and the determinant A; times A2 = 3. 6.1. Introduction to Eigenvalues: Az = Az 225 Solution The eigenvalues of A come from det(A — AI) = 0: _ 2 -1 _12=A -1 2 _ A_[—l 2] det(A AI)—’ _1 2_,\\\\—,\\ —42+3=0. This factors into (A — 1)(A — 3) = 0 so the eigenvalues of A are A\\; = 1 and A\\, = 3. For the trace, the sum 2 + 2 agrees with 1+ 3. The determinant 3 agrees with the product A; A2. The eigenvectors come separately by solving (A — AI)x = 0 which is Az = A\\z: A=1: (A-Dx = [_i —i] [;] = [g] gives the eigenvector £, = [” T A=3 (A-3Nz = [’1 '1] [y _ O gives the ei , 1 -1 -1 = 0 gI1ves eelgenvecora:r_; = _1 A2 and A~! and A + 41 keep the same eigenvectors as A itself. Their eigenvalues are Mand A= 'and A + 4: 1 1 1+4=5 2 . 2 _ 2 _ 1 poe 2 oand T A® has eigenvalues 1 =1and3° =9 A hasland3 A + 41 has 3+d4=T Notes for later sections: A has orthogonal eigenvectors (Section 6.3 on symmetric matrices). A can be diagonalized since A\\; # Az (Section 6.2). A is similar to every 2 by 2 matrix with eigenvalues 1 and 3 (Section 6.2). A is a positive definite matrix (Section 6.3) since A = AT and the \\’s are positive. 6.1 B How can you estimate the eigenvalues of any A? Gershgorin gave this answer. Every eigenvalue of A must be “near” at least one of the entries a;; on the main diagonal. For ) to be “near a;;” means that |a;; — A| is no more than the sum R; of all other |a;;| in that row 7 of the matrix. Then R; is the radius of a circle centered at a;;. Every X is in the circle around one or more diagonal entries a;;: |ai; — A| < R;. Here is the reasoning. When A is an eigenvalue, A — Al is not invertible. Then A — A cannot be diagonally dominant (see Section 2.5). So at least one diagonal entry a,; — A is not larger than the sum R; of all other entries |a;;| (we take absolute values!) in row i. Example Every eigenvalue X of this A falls into one or both of the Gershgorin circles: The centers are a and d. The radii of the circles are R; = |b| and Rz = |c|. c d Second circle: |A —d| < |c| Those are circles in the complex plane, since A could certainly be a complex number. A [ a b ] Firstcircle: |\\ —a| < || 226 Chapter 6. Eigenvalues and Eigenvectors 6.1 C Find the eigenvalues and eigenvectors of this symmetric 3 by 3 matrix S': Symmetric matrix 1 -1 0] Singular matrix S=|-1 2 -1 Tracel+2+1=4 0 -1 Iy Solution Since all rows of S add to zero, the vector x = (1,1,1) gives Sx = 0. This is an eigenvector for A = 0. To find A, and A3, compute the 3 by 3 determinant: 1-x2 -1 0 | =(1-=-XN2-NMN1Q-=-X)-2(1-)) det(S—-A)=| -1 2-X2 =1 | =(@1=-N[2-A)(1-])-2] 0 -1 1-A =1 =A)(=AN)(EB =) Those three factors give A = 0, 1, 3 as the solutions to det(S — AI) = 0. Each eigenvalue corresponds to an eigenvector (or a line of eigenvectors): 1 [ 1] 1 z=|1]| Sz;,=0x; x2=]| 0| Szxo=1x297 x3=|-2| Sx3=323. 1 -1 | LT |t I notice again that eigenvectors are perpendicular when S is symmetric. We were lucky to find A = 0.1, 3. For a larger matrix I would use eig(A), and never touch determinants. The full command [ X ,E] = eig(A)will produce unit eigenvectors in the columns of X. Problem Set 6.1 1 The example at the start of the chapter has powers of this matrix A: _[8 3 . _[70 45 o |6 .6 A\"[.z .7] and 4 “[.30 .55] and A ‘[.4 .4]' Find the eigenvalues of these matrices. All powers have the same eigenvectors. Show from A how a row exchange can produce different eigenvalues. 2 Find the eigenvalues and eigenvectors of these two matrices : 1 4 2 4 A-[2 3] and A+I—[2 4]. A+ IThasthe ____ eigenvectors as A. Its eigenvalues are by 1. 3 Compute the eigenvalues and eigenvectors of A and A~!. Check the trace ! A=[‘l) f] and A“=[—}ﬁ (1)] A~ hasthe eigenvectors as A. When A has eigenvalues \\; and )\\, its inverse has eigenvalues _____ 6.1. Introduction to Eigenvalues: Az = Az 227 4 10 1 Compute the eigenvalues and eigenvectors of A and A2 _ -1 3 2 | 7T =3 A—[ZO] andA-[_2 6]' A? has the same as A. When A has eigenvalues \\; and A2, A2 has eigenvalues . In this example, how do you see that A2 + A2 = 13? Find the eigenvalues of A and B (easy for triangular matrices) and A + B: 3 0 1 1 4 1 A_[l 1] and B—[O 3] and A-&-B—[1 4]. Eigenvalues of A + B (are equal to)(are not equal to) eigenvalues of A plus eigen- values of B. Find the eigenvalues of A and B and AB and BA: 1 0 1 2 1 2 3 2 A—[l l] and B—[ ] and AB—[1 3] and BA--[1 1]. (a) Are the eigenvalues of AB equal to eigenvalues of A times eigenvalues of B? (b) Are the eigenvalues of AB equal to the eigenvalues of BA? Elimination produces A = LU. The eigenvalues of U are on its diagonal; they are the . The eigenvalues of L are on its diagonal; they are all . The eigenvalues of A are not the same as (a) If you know that is an eigenvector, the way to find Aisto (b) If you know that ) is an eigenvalue, the way to find x is to What do you do to the equation Az = Az, in order to prove (a), (b), and (c)? (a) A? is an eigenvalue of A2, as in Problem 4. (b) A~ 1is an eigenvalue of A~!, as in Problem 3. (c) XA+ 1is an eigenvalue of A + I, as in Problem 2. Find the eigenvalues and eigenvectors for both of these Markov matrices A and A, Explain from those answers why A'% is close to A*°: 1.6 .2 o |1/3 1/3| _ .. . n A—[.4 .8] and A —[2/3 2/3]—hmntofA Here is a strange fact about 2 by 2 matrices with eigenvalues Ay # Aa: The columns of A — A, 1 are multiples of the eigenvector 2. Any idea why this should be? 228 12 13 14 15 16 17 18 Chapter 6. Eigenvalues and Eigenvectors Find three eigenvectors for this matrix P (projection matrices have A=1 and 0): 2 Projection matrix P=1.4 0 4 0] 8 0]. 0 1 If two eigenvectors share the same A, so do all their linear combinations. Find an eigenvector of P with no zero components. From the unit vector u = (§,%,3,2) construct the rank one projection matrix P = uu™. This matrix has P? = P because u'u = 1. (a) Pu=1u comes from (uuT )u=1u( ). Then A = 1. (b) If v is perpendicular to u show that Pv = 0. Then A = 0. (c) Find three independent eigenvectors of P all with eigenvalue A\\ = 0. Solve det(Q — AI) = 0 by the quadratic formula to reach A = cos 0 + i sin 6: sinf cosf Q= [COSO — s 0] rotates the Ty plane by the angle 6. No real \\’s. Find the eigenvectors of Q by solving (Q — M)z = 0. Use i2 = —1. Every permutation matrix leaves £ = (1,1,. . .,1) unchanged. Then A = 1. Find two more A’s (possibly complex) for these permutations, from det(P — A\\I) = 0: 0 1 0] 0 0 1] P=|0 0 1] and P=1|0 1 O0f. All Al =1 10 0 1 0 0 The determinant of A equals the product A; Az - - - A,,. Start with the polynomial det(A — AI) separated into its n factors A2 — A (always possible). Then set A = 0: det(A-M)=(A=A)(A2=A)---(Apn—=A) so detA= Check this rule for det A in Example 1 where the Markov matrix has A = 1 and 3. The sum of the diagonal entries (the trace) equals the sum of the eigenvalues: A:[z 3] has det(A—-)\\I)=A2—(a+d)A+ad—bé=O. The quadratic formula gives the eigenvalues A= (a+d++/ )/2and A= , Their sum is If Ahas A\\; = 3and A\\, = 4 thendet(A — \\I) = If Ahas \\; = 4and A\\; = 5thendet(A — AI) = (A —4)(A = 5) = A2 — 9\\ + 20. Find three matrices that have trace a + d = 9 and determinant 20 and A = 4, 5. 6.1. Introduction to Eigenvalues: Az = Ax 229 19 20 21 22 23 24 25 26 27 A 3 by 3 matrix B is known to have eigenvalues 0, 1, 2. This information is enough to find three of these (give the answers where possible): (a) the rank of B (b) the determinant of BT B (c) the eigenvalues of BTB (d) the eigenvalues of (B? + I)™!. Choose the last rows of A and C to give eigenvalues 4,7 and 1, 2, 3: 0 1 0 1 0] Companion matrices A= [* *] C=10 0 1 X E 3 - The eigenvalues of A equal the eigenvalues of AT. This is because det(A4 — A1) equals det(AT — AI). That is true because . Show by an example that the eigenvectors of A and AT are not the same. Construct any 3 by 3 Markov matrix M: positive entries down each column add to 1. Show that AfT(1,1,1) = (1,1,1). By Problem 21, A = 1 is also an eigenvalue of M. Challenge: A 3 by 3 singular Markov matrix with trace % has what \\’s ? Find three 2 by 2 matrices that have A\\, = A2 = 0. The trace is zero and the determinant is zero. A might not be the zero matrix but check that A2 = 0. This matrix is singular with rank one. Find three A’s and three eigenvectors: 1] 2 1 2] A=|2([212]=(4 2 4]. 1 2 1 2] Suppose A and B have the same eigenvalues A, . . ., A, with the same independent eigenvectors x;,. . .,T,. Then A = B. Reason: Any vector x is a combination C1Ty + - + chTn. Whatis Ax? What is Bx? The block B has eigenvalues 1,2 and C has eigenvalues 3,4 and D has eigenval- ues 5, 7. Find the eigenvalues of the 4 by 4 matrix A: 0 1 3 0] A= B C| _|-2 3 0 4 ~ {0 D| | 0 O0 6 1 0 01 6] Find the rank and the four eigenvalues of A and C: 1 1 1 1] 1 0 1 0] 1 111 01 01 A=l 111 ™M C=11 01 0 1111 010 1] 230 28 29 30 31 32 33 Chapter 6. Eigenvalues and Eigenvectors Subtract I from A in Problem 27. Find the A’s and then the determinants of - . 0111 '0—1—1-1} 1 01 1 s 4_ -1 0 -1 =1 B:A—I= 1101 and C—I A= ~1 -1 0 —1 1110 -1 -1 -1 0] (Review) Find the eigenvalues of A, B, and C: ] 0 1] 2 2 27 and B=10 0 and C=1|2 2 3 0 2 . 300 2 2 2] When a + b=c + d show that (1, 1) is an eigenvector and find both eigenvalues: a b A= [c d] ' If we exchange rows 1 and 2 and columns 1 and 2, the eigenvalues don’t change. Find eigenvectors of A and B for A = 11. Rank one gives A = A3 = 0. S O o N O 1 A=10 0 1 1] 6 3 3 A= 13 3| and B=PAPT=[2 1 4 4] 8 4 oo O N Suppose A has eigenvalues 0, 3, 5 with independent eigenvectors u, v, w. (a) Give a basis for the nullspace and a basis for the column space. (b) Find a particular solution to Az = v + w. Find all solutions. (c) Az =wu has no solution. If it did then would be in the column space. Challenge Problems Show that u is an eigenvector of the rank one 2 x 2 matrix A = uv™. Find both eigenvalues of A. Check that \\; + )\\, agrees with the trace ujv; + ugvs. Find the eigenvalues of this permutation matrix P from det (P — AI) = 0. Which vectors are not changed by the permutation? They are eigenvectors for A = 1. Can you find three more eigenvectors? -0 O O OO - 6.1. Introduction to Eigenvalues: Ax = Az 231 35 There are six 3 by 3 permutation matrices P. What numbers can be the determinants of P? What numbers can be pivots? What numbers can be the trace of P? What four numbers can be eigenvalues of P, as in Problem 15? 36 (Heisenberg’s Uncertainty Principle) AB — BA = I can happen for infinite ma- trices with A = AT and B = —B™. Then xTx = T ABz — T BAx < 2||Az| || Bz|. Explain that last step by using the Schwarz inequality |[uTv| < ||u||||v]|. Then Heisenberg’s inequality says that ||Az||/||lz| times ||Bz|/||z| is at least 3. It is impossible to get the position error and momentum error both very small. 37 Find a 2 by 2 rotation matrix (other than I) with A3 = I. Its eigenvalues must satisfy A3 = 1. They can be e2™*/3 and e~2\"*/3, What are the trace and determinant of A ? 38 (a) Find the eigenvalues and eigenvectors of A. They depend on c: 4 1-c A= [.6 c ] ' (b) Show that A has just one line of eigenvectors when ¢ = 1.6. (c) This is a Markov matrix when ¢ = 0.8. Then A™ approaches what matrix 4>? Eigshow in MATLAB There is a MATLABdemo (just type eigshow), displaying the eigenvalue problem for a 2 by 2 matrix. It starts with the unit vector € = (1,0). The mouse makes this vector move around the unit circle. At the same time the screen shows Az, in color and also moving. Possibly Ax is ahead of x. Possibly Az is behind x. Sometimes Az is parallel to x. At that parallel moment, Az = Az (at z; and x3 in the second figure). y =(0.1) 4 [08 03 b~ — 102 0.7 N Ay = (Q3,0.7) \\ Ax = (0.8,0.2) ol s - x =(1,0) S~ ~ =~ circleof x’s These are not eigenvectors Ax lines up with x at eigenvectors The eigenvalue ) is the length of Az, when the unit eigenvector x lines up. The built-in choices for A illustrate three possibilities: 0, 1, or 2 real vectors where Ax crosses . The axes of the ellipse are singular vectors in Section 7.1. 232 Chapter 6. Eigenvalues and Eigenvectors 6.2 Diagonalizing a Matrix ﬁe columns of AX = XA are Azy = Axi. The eigenvalue matrix A is diagonal. 2 n independent eigenvectors in X diagonalize A |A = XA X! and A* = XAkX-1 3 Solve ux41 = Aug by up = Afup = XA X Tug = | ey (A)*zy + -+ + en(An)*a, 4 No equal eigenvalues = eigenvector matrix X is invertible and A can be diagonalized. Repeated eigenvalue = A might have too few independent eigenvectors. Then X ~! fails. kEvery matrix C = B~! AB has the same eigenvalues as A. These C’s are “similar” to Aj When z is an eigenvector, multiplication by A is just multiplication by a number A: Az = A\\z. All the difficulties of matrices are swept away, and we can follow the eigen- vectors separately. The eigenvalues go into a diagonal matrix, and A'%° = XA10X -1, The point of this section is very direct. The matrix A turns into a diagonal matrix A when we use the eigenvectors properly. This is the matrix form of our key idea. We start right off with that one essential computation. The next page explains why AX = XA, Diagonalization Suppose the n by n matrix A has n linearly independent eigenvectors Z,.....x,. Put those x; into the columns of an invertible eigenvector matrix X. Then X ~! AX is the diagonal eigenvalue matrix A : Eigenvector matrix X -1 — A — . Eigenvalue matrix A XTAX =A= \"o ' (1) The matrix A is “diagonalized.” We use capital lambda for the eigenvalue matrix, because the small A’s (the eigenvalues) are on its diagonal. Example 1 This A is triangular so its eigenvalues are on the diagonal: A = 2 and A = 6. o) o o) loa) =[5 4] xX-1 A X = A Eigenvectors (1] |1 go into X 0|11 Inotherwords A = XAX !, Thenwatch A2 = XAX \" 1XAX\"1. S0 A2 is XA3X\"L. A? has the same eigenvectors in X. It has squared eigenvalues 4 and 36 in A3. 6.2. Diagonalizing a Matrix 233 Why is AX = XA? A multiplies its eigenvectors, which are the columns of X. The first column of AX is Az,.Thatis A, ;. Each column of X is multiplied by its eigenvalue : A times X AX =A Ty P A1) L Ann - - This matrix AX splits into X times A (not AX, that would multiply rows of X): X times A A1T) b Ann - | = XA. Keep those matrices in the right order! Then A; multiplies the first column z,, as shown. The diagonalization is complete, and we can write AX = X A in two good ways: AX=XA is X 1AX =A or A=XAX\"L. (2) The matrix X has an inverse, because its columns (the eigenvectors of A) were assumed to be linearly independent. Without n independent eigenvectors, we can't diagonalize A. A and A have the same eigenvalues A;,...,A,. The eigenvectors are different. The job of the original eigenvectors x;, . . ., &, was to diagonalize A. Those eigenvectors in X produce A = XAX~!. You will soon see their simplicity and importance and meaning. The kth power will be A* = X A*¥X ~1 which is easy to compute using X !X = I Ak = (XAX\"1)(XAX\"Y)...(XAX~1) = XAkX 1, Ak = XAkX-1 [2 4\"_1 1][2* 1 -1]_[2¢ 6% -2k = Ak Example 1 0 6/ |0 1 65[{0 1{ (O 6 |7 Withk = 1 we get A. Withk =0we get A° =1 (and \\° =1). Withk = —1 we get A\", When k = 2, you can see how A2 = [4 32; 0 36] fits that formula. Here are four small remarks before we use A again in Example 2. Remark 1 Suppose the eigenvalues are n different numbers (like 2 and 6). Then it is automatic that the n eigenvectors will be independent. The eigenvector matrix X will be invertible. Any matrix that has no repeated eigenvalues can be diagonalized. Remark 2 We can multiply eigenvectors by any nonzero constants. A(cx) = A(czx) is still true. In Example 1, we can divide £ = (1,1) by v/2 to produce a unit vector. MATLAB and virtually all other codes produce eigenvectors of length ||x|| = 1. 234 Chapter 6. Eigenvalues and Eigenvectors Remark 3 The eigenvalues in A come in the same order as the eigenvectors in X, To reverse the order of 2 and 6 in A, put the eigenvector (1, 1) before (1,0) in X: 1 _ — — X AX—Anew [1 1] [D 6] [1 :] = [: 2] —Anew To diagonalize A we must use an eigenvector matrix. From X ~!4AX = A we know that AX = XA. Suppose the first column of X is z. Then the first columns of AX and XA are Az and A\\, z. For those to be equal, £ must be an eigenvector. Remark 4 (repeated wamning for repeated eigenvalues) Some matrices have too few eigenvectors. Those matrices cannot be diagonalized. Here are two examples : : : 1 -1 0 1 Not diagonalizable A= [l —l} and B = [O 0]. Their eigenvalues happen to be 0 and 0. Nothing is special about A = 0, the problem is the repetition of A. All eigenvectors of the first matrix are multiples of (1, 1): Only one line Az =0 ans 1 -1 10 and 11 of eigenvectors T=rE me 1 -1/ [®] \" |o T= There is no second eigenvector, so this unusual matrix A cannot be diagonalized. Those matrices are the best examples to test any statement about eigenvectors. In many true-false questions, non-diagonalizable matrices lead to false. Remember that there is no connection between invertibility and diagonalizability: - Invertibility is concerned with the eigenvalues (we must have A # 0). - Diagonalizability is concerned with the eigenvectors (n independent x’s). Each eigenvalue has at least one eigenvector! A — AI is singular. If (A — Az =0 leads you to = 0, ) is not an eigenvalue. Look for a mistake in solving det(A — \\I) = 0. Fact: Eigenvectors for n different \\’s are independent. Then we can diagonalize A. Independent x from different A\\ Eigenvectors that correspond to distinct (all different) eigenvalues are linearly independent. An n by n matrix that has n different eigenvalues (no repeated A’s) must be diagonalizable. Proof Suppose ¢,z + cox2 = 0 (n = 2). Multiply by A to find c; Ay &y + co 22 = 0. Multiply by A, to find c; Aax; + c2A2x2 = 0. Now subtract one from the other: Subtraction leaves (A; — A2)cixy = 0. Therefore ¢; = 0. Since the A’s are different and ; # 0, we are forced to the conclusion that ¢; = 0. Similarly ¢ = 0. Only the combination with ¢; = ¢, = 0 gives ¢;x; + c2x2 = 0. So the eigenvectors ; and £, must be independent. 6.2. Diagonalizing a Matrix 235 This proof extends directly to n eigenvectors. Suppose that c;xy + -+ + chx, = 0. Multiply by A, multiply by A,,, and subtract. This multiplies z, by A\\, — A, = 0, and z,, is gone. Now multiply by A and by A,_; and subtract. This removes x,_,. Eventually only x, is left: Wereach (A7 — A2)--- (A1 — An)cixy =0 which forces ¢; = 0. (3) Similarly every c; =0. When the A’s are all different, the eigenvectors are independent. A full set of eigenvectors can go into the columns of the invertible eigenvector matrix X. Example 2 Powers of A The Markov matrix A = [:§ 3] in the last section had A1 = 1 and A\\ = .5. Here is A = X AX ! with those eigenvalues in the diagonal A: |8 3| _ (6 1]11 Off1 1] _ 1 Markov example A—[.2 .7]—[.4 _1] [0 .5} [.4 —.GJ_XAX : The eigenvectors (.6, .4) and (1, —1) are in the columns of X. They are also the eigenvec- tors of A2. Watch how A? has the same X, and the eigenvalue matrix of A% is A?: Same X for A2 A2 = XAX \" IXAX 1= |XA2X1, (4) Just keep going, and you see why the high powers A* approach a “steady state™: 6 111 o 1 1 k _ k -1 __ Powersof A Ak = XAkX _[_4 _1] [0 (.5)*] [.4 _.6]. As k gets larger, (.5)* gets smaller. In the limit it disappears completely. That limit is A>: 6 1|1 Oj|1 1 .6 .6 . o _ Limitk = oo A% = [.4 —1] [0 OJ [.4 —.6] - [.4 .4]' The limit has the eigenvector ; in both columns. We saw this A on the very first page of Chapter 6. Now we see it coming from powers like A1%0 = X A0 X -1, Question = When does A* — zero matrix? Answer All || < 1. Similar Matrices : Same Eigenvalues Suppose the eigenvalue matrix A is fixed. As we change the eigenvector matrix X, we get a whole family of different matrices A = X AX ~!—all with the same eigenvalues in A. All those matrices A (with the same A) are called similar. This idea extends to matrices that can’t be diagonalized. Again we choose one constant matrix C (not necessarily A). And we look at the whole family of matrices A = BCB™!, allowing all invertible matrices B. Again those matrices A and C are called similar. We are using C instead of A because C might not be diagonal. We are using B instead of X because the columns of B might not be eigenvectors. We only require that B is invertible—the key fact about similar matrices stays true: Similar matrices A and C have the same n eigenvalues. 236 Chapter 6. Eigenvalues and Eigenvectors All the matrices’ A = BCB™! are “similar”” They all share the eigenvalues of C. Proof If Cx = Az, then BCB™! has the same eigenvalue A with new eigenvector Bz : Similar matrix, same A (BCB~!)(Bz) = BCx = BAx = A\\(Bz). (5) A fixed matrix C produces a family of similar matrices BCB~!, allowing all B. When C is the identity matrix, the “family” is very small. The only memberis BIB~! = I. The identity matrix is the only diagonalizable matrix with all eigenvalues A = 1. The family is larger when A = 1 and 1 with only one eigenvector (not diagonalizable). The simplest C is the Jordan form—developed in Appendix 5. All the similar matrices have two parameters r and s, not both zero: always determinant = 1 and trace = 2. 2 11 1} : _ 1_|1—7s T C—[O l]—Jordanforrngnvo=.-sA—BCB —[ _ g2 1+rs]' (6) For an important example I will take eigenvalues A = 1 and 0 (not repeated!). Now the whole family is diagonalizable with the same eigenvalue matrix A. We get every 2 by 2 matrix with eigenvalues 1 and 0. The trace is 1, the determinant is zero, here they are: All 10 11 5 5 _xy' similar A‘[o o] A‘[o 0] °'A‘[.5 .5]°’a\"yA“_' The family contains all 2 x 2 matrices with A2 = A, including A = A when B = I. If Ais mbynand Bisnbym, AB and B A have the same nonzero eigenvalues. Proof. Start with this identity between square matrices (easily checked). The first and third matrices are inverses. The “size matrix” shows the shapes of all blocks. I -A AB 0 I Al |0 O mXm mXn 0 I B 0||0 I| | B BA nXm mnxn This equation D~! ED = F says F is similar to E—they have the same m+n eigenvalues. AB 0] hasthe m eigenvalues of AB 0 O | hasthe n eigenvalues of BA B 0 plus n zeros B BA plus m zeros So AB and B A have the same eigenvalues except for |n — m| zeros. Wow. Fibonacci Numbers We present a famous example, where eigenvalues tell how fast the Fibonacci numbers grow. Every new Fibonacci number is the sum of the two previous F'’s . The sequence |0,1,1,2,3,5,8,13,...| comesfrom |Fi,, = Fi,q1 + Fk. These numbers turn up in a fantastic variety of applications. Plants and trees grow in a spiral pattern, and a pear tree has 8 growths for every 3 turns. For a willow those numbers can be 13 and 5. The champion is a sunflower of Daniel O’Connell, which had 233 seeds in 144 loops. Those are the Fibonacci numbers F3 and Fy2. Our problem is more basic. 6.2. Diagonalizing a Matrix 237 Problem: Find the Fibonacci number Fyo9 The slow way is to apply the rule Fi+2 = Fir4+1 + Fj one step at a time. By adding Fg = 8 to F7 = 13 we reach F3 = 21. Eventually we come to Figo. Linear algebra gives a better way. The key is to begin with a matrix equation uxy; = Aui. That is a one-step rule for vectors, while Fibonacci gave a two-step rule for scalars. We match those rules by putting two Fibonacci numbers Fj4; and F} into a vector. Then you will see the matrix A. Fryo2 = Fep1 + Fi : Fii1 Let = .| The rule i [ ] Fr+1 = Fe Fi S uk+1=[i (1)] ur = Aug.|| (7) Every step multiplies by A = [] 3]. After 100 steps we reach w100 = A'%®uy: _ |1 _ (1 _ |2 _ |3 _ | Fio Upg = ol’ u, = 1!’ U2 = 1] Uz = 291 U100 = FIOO . Powers of A are just right for eigenvalues! Subtract A from the diagonal of A: 1-x 1 a=+1 A—)\\Iz[ 1 _)‘] leadsto det(A—-A)=A2-X-1 b=-1 c=— The equation A2 — XA — 1 = 0 is solved by the quadratic formula (—b £ v/b2 — 4ac)/2a: 1-v5 ~1618| and A = 2\\/—z—.618 Eigenvalues 1++vV5 of A A = Solving (A — AI)x = 0 leads to eigenvectors ; = (A;,1) and T2 = (A2.1). Step 2 finds the combination of those eigenvectors that gives the starting vector ug = (F1. Fp) = (1,0): f_ 1 M| A2 _ T - [0]—/\\1—)‘2([1] [1]) D ® Step 3 multiplies ug by A'® to find u;00. The eigenvectors x; and x; stay separate! z, is multiplied by ()% and z; is multiplied by (A2)!%: 100... _ ().\\100 100 steps from ug Ui00 = (A1) 3;1 g\"\\z) T2 9) 1 — A2 We want Fgo = second component of u;09. The second components of &, and x, are 1. The difference between \\; = (1 + v/5)/2 and A; = (1 — v/5)/2 is v/5. And A1°° = 0. 100th Fibonacci number = MO0 — )00 1 (1 10 1)‘1 — )‘2 = nearest integer to —( +2\\/§) . (10) V5 Every F; is a whole number. The ratio Fjo;/Fioo must be very close to the limiting ratio (1 + v/5)/2. The Greeks called this number the “golden mean”. For some reason a rectangle with sides 1.618 and 1 looks especially graceful. 238 Chapter 6. Eigenvalues and Eigenvectors Matrix Powers A* Fibonacci’s example is a typical difference equation uxy; = Auk. Each step multiplies by A. The solution is u, = A*uy. We want to make clear how diagonalizing the matrix gives a quick way to compute A* and find uy in three steps. The eigenvector matrix X produces A = XA X ~!. This is a factorization of the matrix, like A = LU or A = QR. The new factorization is perfectly suited to computing powers, because every time X ~' multiplies X we get I: Powers of A Afug = (XAX ) (XAX Dug = XA X 1y I will split X A* X ~1ug into Steps 1,2, 3 that show how eigenvalues lead to u. 1. [ Write ug as a combination X¢ = ¢;x; + - - - + ¢, T, of eigenvectors.| € = X ~1u,. 2. | Multiply each eigenvector ; by ();)*.| Now we have A* X ~1u,. 3. [ Add up the pieces c;(A;)*x; to find the solution u = A*ug. | This is X A% X ~1u. Solution for u,y) = Aur |ux = Afug =c; (M) *fz1 + - + cn(Mn)fzn. || (11) In matrix language A* equals (XAX ~!)* which is X times A* times X 1. In Step I, the eigenvectors in X lead to the c’s in the combinationug = ¢ + - - - + CLT: Ci Step 1 was up=\\|x; - Tn . | . Thissays that {u, = Xc. (12) Cn L -l _ - The coefficients in Step 1 are ¢ = X ~'ug. Then Step 2 multiplies by A*. The final result ux = Y ci(\\)*x; in Step 3 is the product of X and A* and X ~1u: - 1 -(Al)k ] -Clq AkUO=XAkX-luO=XAkc= T ... ZTn . . L ]l (An)*] Len. (13) This result is exactly ux = €1(A1)*T1 + -+ - + cn(An)*Tn. It solves Uk = Auy. - Example 3 Start from uo = (1,0). Compute A*u, for this faster Fibonacci: 1 2 _ 12 _ 1 A= [1 0] has A\\ =2 and z; = [1] y Ag = —1 and Ty = [—1] ) This matrix is like Fibonacci except the rule is changed to Fi,o = F,., + 2Fk. The new numbers start with 0, 1,1, 3. Then A; = 2 makes these F}. grow faster. 6.2. Diagonalizing a Matrix 239 Find ux = A*ug in 3 steps ug = c1Z1 + 22 and ux =¢;(\\ )k z; + 62(1\\2)\"32 1) _1f2] . 11 1 Step 1 uO_[O]_§[1J+§[—1] SO cl—c2—§ Step 2 Multiply the two parts by (A;)* = 2% and (\\2)* = (—1)* Step 3 Combine eigenvectors c; (A1), and cz(A2)* x5 into uy: 1 2 1 1 F; _ Ak _ 1ok 1k _ | Fk+ ur = Ay U = 32 [1] + 3( 1) [—l] [ F, ] . The new F is (2% — (—1)*)/3, starting with 0, 1,1, 3. Now Fjgo = 2199/3. Behind these numerical examples lies a fundamental idea: Follow the eigenvectors. In Section 6.5 this is the crucial link from linear algebra to differential equations: A* will become e*t. Chapter 8 sees the same idea as “transforming to an eigenvector basis.” The best example of all is a Fourier series, built from the eigenvectors e*®. Nondiagonalizable Matrices (Optional) Suppose A is an eigenvalue of A. We discover that fact in two ways: 1. Eigenvectors (geometric) There are nonzero solutions to Az = Az. 2. Eigenvalues (algebraic) The determinant of A — Al is zero. The number A may be a simple eigenvalue or a multiple eigenvalue, and we want to know its multiplicity. Most eigenvalues have multiplicity M = 1 (simple eigenvalues). Then there is a single line of eigenvectors, and det(A — AI) does not have a double factor. For exceptional matrices, an eigenvalue can be repeated. Then there are two different ways to count its multiplicity. Always GM < AM for each A: 1. | (Geometric Multiplicity = GM)| Count the independent eigenvectors for . Then GM is the dimension of the nullspace of A — Al 2. | (Algebraic Multiplicity = AM)| AM counts the repetitions of A among the eigenvalues. Look at the n roots of det(A — AI) = 0. If A has A = 4,4, 4, then that eigenvalue has AM = 3 and GM = 1, 2, or 3. The following matrix A is the standard example of trouble. Its eigenvalue A = 0 is repeated. It is a double eigenvalue (AM = 2) with only one eigenvector (GM = 1). AM = 2 A=[O 1] has det(A—AI)=‘ -A 1_ 2 A'—'0,0b“t 1 GM=1 0 0 - 0 =X 1 eigenvector | O 240 Chapter 6. Eigenvalues and Eigenvectors There “should” be two eigenvectors, because A* = 0 has a double root. The double factor A2 makes AM = 2. But there is only one eigenvector x = (1,0) and GM = 1. This shortage of eigenvectors when GM is below AM means that A is not diagonalizable. The next three matrices all have the same shortage of eigenvectors. Their repeated eigenvalue is A = 5. Traces are 5 plus 5 = 10 and determinants are 5 times 5 = 25: oo 4 aeft ) a2 Those all have det(A — AI) = (A — 5)2. The algebraic multiplicity is AM = 2. But each A — 51 has rank r = 1. The geometric multiplicity is GM = 1. There is only one line of eigenvectors for A = 5, and these matrices are not diagonalizable. ® REVIEW OF THE KEY IDEAS = 1. If A has n independent eigenvectors x;,.. ., &,, they go into the columns of X. A is diagonalized by X X 'AX=A and A=XAX\"1 2. The powers of A are A¥ = XA*X 1. The eigenvectors in X are unchanged. 3. The eigenvalues of A* are ()%, .., (A.)* in the matrix A*. 4. The solution to ux,; = Auy starting from ug is ux = A*ug = XA*¥ X ~luy: ur = c1(A)*x) + - + ca(An)*x, provided ug =ciTy + - + Can. S. A is diagonalizable if every eigenvalue has enough eigenvectors (GM = AM). ® WORKED EXAMPLES = 6.2 A The Lucas numbers are like the Fibonacci numbers except they start with L, = 1and L, = 3. Using the same rule Lx,2 = Lxy1 + Ly, the next Lucas numbers are 4,7,11,18. Show that the Lucas number Lgg is A1%0 + A3%0, : Solution ui4) = |} },]uk is the same as for Fibonacci, because L2 = Li4+1 + Lk is the same rule (with different starting values). The equation becomes a 2 by 2 system: Lk Ley2 =Lgy1 + L 11 Lctuk—[ L ] The rule Liat = Liss 1S Uyl = 1 0 U. The eigenvalues and eigenvectors of A = [1 3] still come from A% = A + 1: 1-— Al = 1+2\\/5 and z; = [All] Ay = 2\\/5 and x; = ['\\2] . 6.2. Diagonalizing a Matrix 241 Now solve ¢ + coxa = u; = (3,1). The solution is ¢; = A; and ¢c; = Ay. Check: . [ A2+X2 ] [traceof A2] [3] A1m1+/\\2z2—[)‘1+)‘2 = | traceof A [~ |1 |T™ u100 = A”u, tells us the Lucas numbers (L1g1, L10o). The second components of the eigenvectors o) and &, are 1, so the second component of u1qg is the answer we want: Lucas number Lioo = a1} + c2A3° = A100 4 2100, Lucas starts faster than Fibonacci, and ends up larger by a factor near v/5. 6.2 B Find the inverse and the eigenvalues and the determinant of this matrix A: C 4 -1 -1 -1° -1 4 -1 -1 -1 -1 4 -1 -1 -1 -1 4 A =5 x eye(4) — ones(4) = - Describe an eigenvector matrix X that gives X ~1AX = A. Solution What are the eigenvalues of the all-ones matrix ones (4) ? Its rank is 1, so three eigenvalues are A = 0,0,0. Its trace is 4, so the other eigenvalue is A = 1. Subtract this all-ones matrix from 51 to get our matrix A above. Subtract the eigenvalues 4,0,0,0 from 5,5, 5, 5. The eigenvalues of A are 1.5.5. 5. The determinant of A is 125, the product of those four eigenvalues. The eigenvector for A=1lisx = (1,1,1,1) or (c,c,c,c). The other eigenvectors are perpendicular to x (since A is symmetric). The nicest eigenvector matrix X is the symmetric orthogonal Hadamard matrix H. The factor -21- produces unit column vectors. p 1 1 1 1° . 11 -1 1 -1 T -1 Orthonormal eigenvectors X = H =301 1 21 217 H =H\". 1 -1 -1 1 The eigenvalues of A~! are 1,3, :,2. The eigenvectors are the same as for A. So A~! = HA~'H ™!, The inverse matrix is surprisingly neat: 2 1 1 1 1 A7l = zx (eye(4) + ones(4)) = z i 1 1 2 1 2 b - A is a rank-one change from 5I. So A~! is a rank-one change from 1/5. In a graph with 5 nodes, the determinant 125 counts the “spanning trees” (trees that touch all nodes). Trees have no loops (graphs and trees are in Section 10.1). With 6 nodes, the matrix 6 * eye(5) — ones(5) has the five eigenvalues 1,6, 6, 6, 6. 242 Chapter 6. Eigenvalues and Eigenvectors Problem Set 6.2 1 (a) Factor these two matricesinto A = XA X ~!: 1 2 1 1 A= [0 3] and A= [3 3] . b) fA=XAX\"'thenA3=( ) ) )andA~'=( ) )( ). 2 If A has \\; = 2 with eigenvector &, = [(1,] and \\» = 5 with x5 = [1] use XA X ~!tofind A. No other matrix has the same \\’s and x’s. 3 Suppose A = XAX ™1, What is the eigenvalue matrix for A + 2I? What is the eigenvector matrix? Check that A+2I = ( )( )( )~ L. 4 True or false : If the columns of X (eigenvectors of A) are linearly independent, then (a) A is invertible (b) Ais diagonalizable (c) Xisinvertible (d) X isdiagonalizable. 5 If the eigenvectors of A are the columns of I, then Ais a matrix. If the eigen- vector matrix X is triangular, then X ~! is triangular. Prove that A is also triangular. 6 Describe all matrices X that diagonalize this matrix A (find all eigenvectors of A): A= [‘; (2)] . Then describe all matrices that diagonalize A™\". 7 Write down the most general matrix that has eigenvectors [ 1 ] and [_} ] : Questions 8-10 are about Fibonacci and Gibonacci numbers. 8 Diagonalize the Fibonacci matrix by completing X ~!: 1 1 _ Al A2 Al 1 0f |1 1 0 Ag ' Do the multiplication XA*X ~![1] to find its second component. This is the kth Fibonacci number Fi = (Af — M%) /(A1 — A2). 9 Suppose G 42 is the average of the two previous numbers G, ; and Gk: Gk+2 = ‘lg'Gk-i-l + %Gk e [Gk+2] [ ] [Gk+1] Gir+1 = G4 Gr+1 (a) Find the eigenvalues and eigenvectors of A. (b) Find the limit as n = oo of the matrices A™ = XA X 1, (c) If Go = 0 and G} = 1 show that the Gibonacci numbers approach 3. 6.2. Diagonalizing a Matnx 243 10 Prove that every third Fibonacci numberin 0,1,1,2,3,... is even. Questions 11-14 are about diagonalizability. 11 True or false: If the eigenvalues of A are 2, 2, 5 then the matrix is certainly (a) invertible (b) diagonalizable (c) notdiagonalizable. 12 True or false: If the only eigenvectors of A are multiples of (1, 4) then A has (a) noinverse (b) arepeatedeigenvalue (c) nodiagonalization XAX !, 13 Complete these matrices so that det A = 25. Then check that A = 5 is repeated— the trace is 10 so the determinant of A — A is (A — 5). Find an eigenvector with Az = 5x. These matrices are not diagonalizable—no second line of eigenvectors. a=[® ] wa 4= ] we a=[! 14 The matrix A = (3 1] is not diagonalizable because the rank of A — 31 is Change one entry to make A diagonalizable. Which entries could you change? Questions 15-19 are about powers of matrices. 15 A*¥ = XA*X ! approaches the zero matrix as k — oc if and only if every A has absolute value less than . Which of these matrices has A* — 0? 6 .9 6 9 Ay = [.4 .1] and Ap = [.1 .6]' 16 (Recommended) Find A and X to diagonalize A; in Problem 15. What is the limit of A*¥ as k — 00? What is the limit of X A*X ~1? In the columns of this limiting matrix you see the 17 Find A and X to diagonalize A; in Problem 15. What is (A2)®ug for these ug? 3 e[ 1] o e[ 18 Diagonalize A and compute X A* X ! to prove this formula for A*: 4= 2 -1 Gk _ 1[1+3\" 1-3* [ k_ L1+ -1 2] \" ‘2[1-3* 1+3‘=]' 19 Diagonalize B and compute X A* X ~1 o prove this formula for B*: _ o 1 sk gk _ g4k 7 [0 4] has Bk=[o 4 ] 244 20 21 22 23 24 25 26 27 28 30 Chapter 6. Eigenvalues and Eigenvectors Suppose A = XAX ~!. Take determinants to prove det A = det A = A A --- A, This quick proof only works when Acanbe _____ Show that trace XY = trace Y X, by adding the diagonal entries of XY and Y X: _la b _lq T X—[c d] and Y—[s t]' Now choose Y to be AX 1. Then XAX ™! has the same trace as AX !X = A. This proves that the trace of A equals the trace of A = the sum of the eigenvalues. AB — BA = I is impossible since the left side has trace = If A= XAX™!, diagonalize the block matrix B = [4 ,9]. Find its eigenvalue and eigenvector (block) matrices. Consider all 4 by 4 matrices A that are diagonalized by the same fixed eigenvector matrix X. Show that the A’s form a subspace (cA and A; + A2 have this same X). What is this subspace when X = I? What is its dimension? Suppose A2 = A. On the left side A multiplies each column of A. Which of our four subspaces contains eigenvectors with A = 1? Which subspace contains eigenvectors with A = 0? From the dimensions of those subspaces, A has a full set of independent eigenvectors. So a matrix with A?> = A can be diagonalized. (Recommended) Suppose Az = Ax. If A = 0 then x is in the nullspace. If A # 0 then is in the column space. Those spaces have dimensions (n — ) + 7 = n. So why doesn’t every square matrix have n linearly independent eigenvectors? The eigenvalues of A are 1 and 9. The eigenvalues of B are —1 and 9. Find a matrix square root A = Xv/A X 1. Why is there no real matrix v'B ? 5 4 4 5 A—[4 5] and B—[5 4]. If A and B have the same \\’s with the same independent eigenvectors, their factor- izations into are the same. So A = B. Suppose the same X diagonalizes both A and B. They have the same eigenvectors inA=XA;X\"'and B= XA,X\"!. Prove that AB = BA. (a) If A = [35] then the determinant of A — AT is (A — a)(\\ — d). Check the “Cayley-Hamilton Theorem” that (A — al)(A — dI) = zero matrix. (b) Test the Cayley-Hamilton Theorem on Fibonacci’s A = [} 3]. The theorem predicts that A2 — A — I = 0, since the polynomial det(A4 — AI) is A2 — A —1. Substitute A = XAX ™! into the product (A — A\\ I)(A — X2I)--- (A — A, 1) and explain why this produces the zero matrix. We are substituting the matrix A for the number X in the polynomial p(A) = det(A — AI). The Cayley-Hamilton Theorem says that this product is always p(A) = zero matrix, even if A is not diagonalizable. 6.2. Diagonalizing a Matrix 245 31 32 33 34 35 36 38 If A= [29] and AB = BA, show that B = 2] is also a diagonal matrix. B has the same eigen as A but different eigen . These diagonal matrices B form a two-dimensional subspace of matrix space. AB — BA = 0 gives four equations for the unknowns a, b, c, d—find the rank of the 4 by 4 matrix. The powers A* blow up if any |A;| > 1 and D* approaches zero if all |\\;| < 1. Peter Lax gives these striking examples in his book Linear Algebra. Show B4 = I and C3 = —1I. 3 2 3 2 5 7 5 6.9 . [1 4] = [\"5 —3] “= [-3 -4] b= [—3 -4] I|A1024” > 10700 BlO24 =T 01024 — _C ”D1024” < 10—78 Challenge Problems The nth power of rotation through 4 is rotation through né: An — l cosf@ —siné Jn _ [ cosnf —sinnf sin 6 cos @ sin nd cosnf | Prove that neat formula by diagonalizing A = XAX~!. The eigenvectors (columns of X) are (1,3) and (i, 1). You need to know Euler’s formula e*® = cos 6 + isin6. The transpose of A = XAX 1is AT = (X \"1)TAXT. The eigenvectorsin ATy = Ay are the columns of that matrix (X ~1)T. They are often called left eigenvectors of A, because yT A = AyT. How do you multiply matrices to find this formula for A? Sum of rank-1 matrices A= XAX\"!= /\\la:lyf + -+ )\\n:cny’,f. The inverse of A = eye(n) + ones(n) is A~! = eye(n) + C * ones(n). Multiply AA~! to find that number C (depending on n). Suppose A, and A, are n by n invertible matrices. What matrix B shows that A; A4, = B(A1A3)B~!? Then Az A, is similar to A; Ay: same eigenvalues. When is a matrix A similar to its eigenvalue matrix A ? A and A always have the same eigenvalues. But similarity requires a matrix B with A = BAB~!. Then B is the matrix and A must have n independent (Pavel. Grinfeld) Without writing down any calculations, can you find the eigenvalues of this matrix ? Can you find the power 42023 ? 110 55 —164 A= 42 21 -62 88 44 -131 246 Chapter 6. Eigenvalues and Eigenvectors 6.3 Symmetric Positive Definite Matrices ﬁ A symmetric matrix S has n real eigenvalues \\; and n orthonormal eigenvectors qi\\ 2 Sis diagonalized by an orthogonal eigenvector matrix Q | S = QAQ ™! = QAQT 3A Positive definite S: all A > 0 and all pivots > 0 and all upper left determinants > 0. 3B The energy testis T Sz >0 for all £#0. Then S= AT A with independent columns in A. { Positive semidefinite allows A = 0, pivot = 0, determinant = 0, energy &' S = 0, any U Symmetric matrices S = ST deserve all the attention they get. Looking at their eigen- values and eigenvectors, you see why they are special : 1 All n eigenvalues )\\ of a symmetric matrix S are real numbers. 2 The n eigenvectors g can be chosen orthogonal (perpendicular to each other). The identity matrix S = [ is an extreme case. All its eigenvalues are A = 1. Every nonzero vector Z is an eigenvector: Ix = 1. This shows why we wrote “can be chosen” in Property 2 above. Repeated eigenvalues like A; = A\\, = 1 give a choice of eigenvectors. We can choose them to be orthogonal. We can rescale them to be unit vectors (length 1). Then those eigenvectors q;,...,q, are not just orthogonal, they are orthonormal. The eigenvector matrix for SQ = QA has QTQ = 1. L —a— [ | ][00 .T:Oz#Jldt , o 10 . q'q’{lizj eads to : qi .- gn | = .0 1 0 —— ] | | ] lo - 01 We write () instead of X for the eigenvector matrix of S, to emphasize that these eigenvectors are orthonormal: QTQ = I'and QT = Q1. This eigenvector matrix is an orthogonal matrix. The usual A = XAX ! becomes S = QAQT : Spectral Theorem Every real symmetric matrix S has the form S = QAQT. Every matrix of that form is symmetric: Transpose QAQT to get QTTATQT = QAQT. Quick Proofs : Orthogonal Eigenvectors and Real Eigenvalues Suppose first that Sz = Az and Sy = Oy. The symmetric matrix S has a nonzero eigenvalue A and a zero eigenvalue. Then y is in the nullspace of S and x is in the column space of S (z = Sz/) is a combination of the columns of S). But S is symmetric: column space = row space! Since the row space and nullspace are always orthogonal, we have proved that the eigenvector z is orthogonal to the eigenvector y. 6.3. Symmetric Positive Definite Matrices 247 When that second eigenvalue is not zero, we have Sy = ay. In this case we look at the matrix S — al. Then (S — al)y =0y and (S — al)z = (A — a)x with A —a # 0. Now y is in the nullspace and x is in the column space (= row space!) of S — al. So yTx = 0: Orthogonal eigenvectors of S whenever the eigenvalues are different. Note on complex numbers That proof of orthogonality assumed real eigenvalues and eigenvectors of S. To prove this, suppose they could involve complex numbers. Multiply St = Az by the complex conjugate vector T' (every i changes to —i). That gives Z Sz = AT ' x. When we show that ZT ¢ and T* Sz are real, we see that ) is real. I would like to leave complex numbers and complex matrices for the end of this section. The rules and the matrices are important. But positive definite matrices are so beautiful, and they connect to so many ideas in linear algebra, that they deserve to come first. Positive Definite Matrices We are working with real symmetric matrices S = ST. All their eigenvalues are real. Some of those symmetric matrices (not all) have an additional powerful property. Here is that important property, which puts S at the center of applied mathematics. Test 1 A positive definite matrix S has all positive eigenvalues We would like to check for positive eigenvalues without computing those numbers A. You will see four more tests for positive definite matrices, after these five examples. 1 S = [ (2) g ] 1s positive definite. Its eigenvalues 2 and 6 are both positive 2 S=0qQ (2) g QT is positive definite if QT = Q1 : same A = 2 and 6 (2 0] . .. e e 3 S=C 0 6 C\" is positive definite if C is invertible (energy test) 4 S = Z . is positive definite when @ > 0 and ac > b? (det test) 5 S = (2) g is only positive semidefinite. Ithas all A > O butnot A > 0 ! J Try Test 1 on these examples: No, No, Yes. The other four tests may give faster answers. { 9 (21 0 S=[2 1] S = vv T (rank 1) S=11 21 01 2| A =3and -1 A=||v||>and 0 A=2,2-12,24+2 248 Chapter 6. Eigenvalues and Eigenvectors The Energy-based Definition : Test 2 May I bring forward the most important idea about positive definite matrices ? This new approach doesn’t directly involve eigenvalues, but it turns out to be a perfect test for A > 0. This is a good definition of positive definite matrices : Test 2 is the energy test. S is positive definite if the energy Sz is positive for all vectors © # 0 | (1) Of course S = I is positive definite: All \\; = 1. The energy Iz = xTx is positive if € # 0. Let me show you the energy in a 2 by 2 matrix. It depends on * = (z,,x2). Energy zTS:t:[zl z2][i 3][i;]=2x3+8$1$2+9$§ Is this positive for every = (z,,z2) except (0,0) ? Yes, it is a sum of two squares : TSz = 213 + 81113 + 922 = 2(7, + 212)° + 73 = positive energy. We must connect positive energy £ Sz > 0 to positive eigenvalues A > 0 If Sz = Az then TSz = AxTz. So A > 0 leads to energy =™ Sz > 0. That line tested TSz for each separate eigenvector . But more is true. If every eigenvector has positive energy, then all nonzero vectors x have positive energy: If 7Sz > 0 for the eigenvectors of S, then TSz > 0 for every nonzero vector z. Here is the reason. Every x is a combination ¢;x; + - + ¢, &, of the eigenvectors. Those eigenvectors can be chosen orthogonal because S is symmetric. We will now show : zT Sz is a positive combination of the energies z\\ka:{:ck > ( in the separate eigenvectors. 'Sz =(ax] +-+cnz))S(azi+ -+ cnZn) = (Clz'lr + .-+ cn:cI) (clAlzl +---+ Cn/\\na’n) = C%Alib’lricl +---+ C%Anm;{zn > 0 ifevery A; > 0. From line 2 to line 3 we used the orthogonality of the eigenvectors of S: z)x; = 0. Here is a typical use for the energy test, without knowing any eigenvalues or eigenvectors. If S; and S are symmetric positive definite, so is S; + S Proof by adding energies : a:T(Sl +S)x = 'S z+xTSz>0+0 The eigenvalues and eigenvectors of S; + Sz are not easy to find. The energies just add. 6.3. Symmetric Positive Definite Matrices 249 Three More Equivalent Tests So far we have tests 1 and 2 : positive eigenvalues and positive energy. That energy test quickly produces three more useful tests (and probably others, but we stop with three) : Test 3 S = AT A for some matrix A with independent columns Test 4 All the leading determinants D,, D,, ..., D,, of S are positive Test 5 All the pivots of S (coming from elimination) are positive Test 3 applies to S = AT A. That matrix is symmetric: (ATA)T = ATATT = ATA. Why must columns of A be independent in this test ? Connect AT A to the energy test : S=ATA Energy = 'Sz = T ATAz = (Az)T (Az) = ||Az|®. (2) The energy is the length squared of the vector Az. This energy is positive provided Az is not the zero vector. To assure Ax # 0 when & # 0, the columns of A must be independent. In this 2 by 3 example, A has dependent columns. Now AT A has rank 2, not 3. 1 1] 111 2 3 4] S is not positive definite. S=ATA=|1 2 1 2 3| =|3 5 7| Itispositive semidefinite. 1 3 4 7 10| z'Sz=|Az||* >0 This A has column 1 + column 3 = 2 (column 2). Then £ = (1, —2, 1) has zero energy. It is an eigenvector of ATA with A = 0. This S = AT A is only positive semidefinite, because the energy and the eigenvalues of S touch zero. Equation (2) says that S = AT A is at least semidefinite : ™ Sz = ||Ax||? is never negative. Semidefinite allows energy / eigenvalues / determinants / pivots of S to be zero. Determinant Test and Pivot Test The determinant test is the fastest for a small matrix. I will mark the four “leading determinants” D;, D4, D3, Dy in this 4 by 4 symmetric second difference matrix. 2|1 1st determinant D; =2 >0 -1 2|-1 2nd determinant D, =3 > 0 Testd 5= 21 2|-1|| ™5 31d determinant Dy = 4 >0 -1 2 J 4th determinant Dy =5 > 0 The determinant test is here passed ! The energy £ Sz must be positive. Eigenvalues too. Leading determinants are closely related to pivots (the numbers on the diagonal after elimination). Here the first pivot is 2. The second pivot % appears when %(row 1) is added to row 2. The third pivot g appears when %(new row 2) is added to row 3. Those fractions 2,3 4 are ratios of determinants! The last pivot is %. Always a ratio. 1’2’3 Dy The kth pivot equals the ratio of the leading determinants (sizes k and k — 1) k-1 So Test 5 is passed (next page) exactly when Test 4 is passed.The pivots are positive. 250 Chapter 6. Eigenvalues and Eigenvectors Test 4 The leading determinants are all positive Test 5 The pivots are positive I can quickly connect these tests 4 and 5 to the third test S = ATA. In fact elimination on S produces an important choice of A. Remember that elimination = triangular factorization (S = LU). Up to now L has had 1's on its diagonal and U contained the pivots. But with symmetric matrices we can balance S as LDLT: 2 -1 o] [ 1 12 -1 o0° -1 2 -1|=|-1 1 2 -1 | ThisisS=LU (3 2 L 0 -1 2f | 0 -F 1] s - Put pivots 1 17 2 111 -3 OW intoD | -1 1 2 1 -2 | S=LDLT @) 2 4 forTest5 | 0 -% 1| 3 ]l 1 - B 1 . Share those pivots V2 1 v2 —\\/; 0 between ATand A | - % \\/g \\/—%- — % = ATA (5 for Test 3 0 —./2 4 3 | s Vi ]| Vi I am sorry about those square roots—but the pattern S = AT Aisasuccess: A = v DLT. Elimination factors every positive definite S into AT A (A is upper triangular) (5) is the Cholesky factorization S = ATA with /pivots on the main diagonal of A. To apply the S = AT A test when S is positive definite, we must find at least one possible A. There are many choices for A, including (1) symmetric and (2) triangular. 1 If S=QAQT, take square roots of those eigenvalues. A=QvAQT = AT has S= AT A. 2 If § = LU = LDLT with positive pivots in D, then S = (LvV'D) (VDLT) = ATA. Summary The 5 tests for positive definiteness of S = ST involve 5 different parts of linear algebra—pivots, determinants, eigenvalues, S = ATA, and energy. Each test gives a complete answer by itself : positive definite or semidefinite or neither. Positive energy zT Sz > 0 is the best definition. It connects them all. When S is a symmetric positive definite 2 by 2 matrix, here are four of the tests : s_|@ b| determinants a > 0,ac—b*>0 pivots a > 0,(ac—b?)/a >0 b ¢ eigenvalues A; >0, A2 >0 energy azr’ + 2bzy +cy? >0 6.3. Symmetric Positive Definite Matrices 251 This page analyzes two examples: S is definite, T is semidefinite. g = 9 3| Positive T = 9 3| Positive 3 3| Definite ~ |3 1| Semidefinite Determinants 9 and 18 Determinants 9 and 0 Pivots 9 and 2 Pivots 9 and 0 Energy 922 + 6y + 3y?=(3z +y)3 + 293 Energy 9z° + 6zy + y*> = (3z + y)? Trace 12, Det18, A2 -120+18=0 Trace 10, Det0, X2 —-10A=0 Eigenvalues A\\; = 6 + 3v/2and )\\, = 6 — 3v/2 Eigenvalues \\) =10and A\\, =0 Eigenvectors [\\/2_1_ 1] and [ \\/5_ _:_ 1] Eigenvectors [:13] and [ -;} _rnrr_|1 O][9 0][1 3 et 1 0][9 O]f1 4 S=LDL =1 1][0 2][0 1 T=EPE=13 1o offo o '3 013 1 T 3 0][3 1 T 1 vallo vz| =44 1 0f{0 0 The graph of energy E(z, y) is a bowl The graph of energy E(x.y) is a valley E(z,y) is a strictly convex function E(z,y) is a convex function Cross-section E'=1 is an ellipse Cross-section E'=1is aband 3r+y = +1 Axes of the ellipse along eigenvectors of S Axes of the band along eigenvectors of T E has its minimum along a line E has its minimum at a point E(x,y) 252 Chapter 6. Eigenvalues and Eigenvectors Positive Definite Matrices and Minimum Problems This page is about functions of a vector . Start with the energy E':. EnergyE=zTS:c [I y][i ‘;”;]=5x2+81y+5y2>0 The graph of that energy function E(z, y) is a bowl opening upwards. The bottom point of the bowl has energy E = 0 when z = y = 0. This connects minimum problems in calculus with positive definite matrices in linear algebra. For the best minimization problems, the function f(x) is strictly convex—the bowl curves upwards. The matrix of second derivatives (partial derivatives) is positive definite at all points. We are in 3 or more dimensions, but linear algebra identifies the crucial properties of the second derivative matrix S. The second derivatives of the . energy 3 ' Sz are in the matrix S F =~ -2-1:TS:L' >0 For an ordinary function f(r) of one variable z, the test for a minimum at z¢ is famous: Minimum First derivative ﬁ _ Second derivative d2_f at xo is zero dz is positive dz? >0 For f(z.y) with two variables, the second derivatives go into a matrix : positive definite ! Minimum 8f _0f _ 0 and S— 0%f/0z? 0O%f/0x0y | is positive definite atzo,yo Or Oy | 8%f/0z0y O8*f/0y? at zo, Yo The graph of z = f(z,y) is flat at that point zg,yo because 0f/0x = 9f/dy = 0. The graph goes upwards because the second derivative matrix S is positive definite. So we have a minimum point of the function f(z,y). Similarly for f(z,y,2). Example We know that E = z2 + y? has its minimum at z = y = 0. What about f= e v’ 9 Bf/a:r = 2.1'61:+y: _ SCCO!'Id fzz fzy — 2 + 4112 4:1:y 2242 af |0y = 2ye* *V derivatives | f,z fyy dry 2+ 4y? That matrix S is positive definite for every z and y ! Such a function f is strictly convex. 6.3. Symmetric Positive Definite Matrices 253 Positive Semidefinite Matrices Often we are at the edge of positive definiteness. The determinant is zero. The smallest eigenvalue is A = 0. The energy in its eigenvectoris T Sz = £T0z = 0. These matrices on the edge are “positive semidefinite”. Here are two examples (not invertible) : r - 2 -1 -1 . : . g |Y 2| .4 T=|—1 2 —jp| @aecpositvesemidefinite 2 4 1 -1 2 J but not positive definite S has eigenvalues 5 and 0. Its trace is 1 + 4 = 5. Its upper left determinants are 1 and 0. The rank of S is only 1. This matrix S factors into AT A with dependent columns in A: Dependent columns in A 1 21 |1 01 2| _ AT A Positive semidefinite S 2 4] |2 0]|0 O] ' If 4 is increased by any small number, the matrix S will become positive definite. The cyclic T also has zero determinant. The eigenvector z = (1,1,1) has Tz = 0 and energy 1 Tz = 0. Vectors x in all other directions do give positive energy. Second differences T ( 2 -1-11 [1-1 0][ 1 0-1] from first differences A -1 2 -1] = 0 1 -1 -1 1 0]. Columns add to (0,0,0) |-1 -1 2| -1 0 1] 0-1 1 Positive semidefinite matrices have all A > 0 and all zT Sz > 0. Those weak inequalities ( > instead of > ) include positive definite S along with the singular matrices at the edge. If S is positive semidefinite, so is every matrix ATSA: If zTSx > 0 for every vector z, then (Az)TS(Azx) > 0 for every x. We can tighten this proof to show when ATSA is actually positive definite. But we have to guarantee that Az is not the zero vector—to be sure that (Az)* S(Az) is not zero. Suppose T Sz > 0 and Az # 0 whenever z is not zero. Then ATS A is positive definite. Again we use the energy test. For every £ # 0 we have Az # 0. The energy in Az is strictly positive: (Ax)TS(Ax) > 0. The matrix ATSA is called “congruent” to S. ATS A is important in applied mathematics. We want to be sure it is positive definite (not just semidefinite). Then the equations ATSAx = f in engineering can be solved. Here is an extension called the Law of Inertia. If ST = S has P positive eigenvalues and N negative eigenvalues and Z zero eigenvalues, then the same is true for ATS A—provided A is invertible. 254 Chapter 6. Eigenvalues and Eigenvectors The Ellipse ax? + 2bzy + cy? =1 Think of a tilted ellipse zT Sz = 1. Its center is (0, 0), as in Figure 6.3a. Turn it to line up with the coordinate axes (X and Y axes). That is Figure 6.3b. These two pictures show the geometry behind the eigenvalues in A and the eigenvectors of S in Q). The eigenvector matrix Q lines up with the ellipse. The tilted ellipse has T Sz = 1. The lined-up ellipse has X TAX = 1 with X = Qz. Example 1 Find the axes of this tilted ellipse 5z% 4+ 8y + 5y? = 1. Solution Start with the positive definite matrix S that matches this equation: The equation is [1: y] [i g] [;] = 1. The matrixis S = [3 ‘51] : The eigenvectors are [}] and [_} ] Divide by v/2 for unit vectors. Then S = QAQT: Eigenvectors in Q 5 4/ _ 1 (1 1f{(9 0 1 1 1 Eigenvalues 9 and 1 4 5 1 -1{{0 1 1 -1} V2 V2 Now multiply by [z y] on the left and [‘;] ontherighttogetzT Sz = (TQ)A(Qx): 2 2 T 2 2 rT+Yy r—y z Sx = sum of squares 5z°+ 8xy+5 =9(—) +1( ) : 6) > Y / \\/5 \\/§ ( The coefficients are the eigenvalues 9 and 1 from A. Inside the squares are the eigenvectors q, = (1, 1)/v2and q, = (1,-1)/V2. The axes of the tilted ellipse point along those eigenvectors. This explains why S = QAQ?T is called the “principal axis theorem”—it displays the axes. Not only the axis directions (from the eigenvectors) but also the axis lengths (from the eigenvalues). To see it all, use capital letters for the new coordinates that line up the ellipse : : Tty _ Ty _ 3 y2— Lined up 7 and 7 Y and 9X°+Y 1. The largest value of X2 is 1/9. The endpoint of the shorter axis has X = 1/3and Y = 0. Notice: The bigger eigenvalue ), gives the shorter axis, of half-length 1//A; = 1/3. The smaller eigenvalue A, = 1 gives the greater length 1/y/A; = 1. In the zy system, the axes are along the eigenvectors of S. In the XY system, the axes are along the eigenvectors of A—the coordinate axes. All comes from S = QAQT. 6.3. Symmetric Positive Definite Matrices 255 y Y A A 1 L 0,1) (7 %) \\3 i G\") % - X = X —1 -1 1 zTSxr=1 XTAX =1 (77 NGING) Figure 6.3: The ellipse TSz = 522 + 8zy + 5y® = 1. Lined upitis 9X2 + Y% = 1. Optimization and Machine Learning This book will explain gradient descent to minimize f(x). Each step to Ty, takes the steepest direction at the current point xx. But that steepest direction changes as we descend. This is where calculus meets linear algebra, at the minimum point =*. Calculus The partial derivatives of f are all zero at =* : I 0 2 0z; 0z Linear algebra The matrix S of second derivatives is positive definite If S is positive definite (or semidefinite) at all points £ = (r;,...,Z,), then the function f(x) is convex. If the eigenvalues of S stay above some positive number 9, then f(x) is strictly convex. These are the best functions to optimize. They have only one minimum, and gradient descent will find it. Please see Chapter 9. Machine learning produces “loss functions” with hundreds of thousands of varnables. They measure the error—which we minimize. But computing all the second denvatives is barely possible. We use first derivatives to tell us a direction to move—the error drops fastest in the steepest direction. Then we take another descent step in a new direction. This is the central computation in least squares and neural nets and deep learning. The key to the great success of deep learning is the special form of the learning function F(x,v). That is described in Chapter 10. The “training data” is in the vector v. The weights that decide the importance of that training data go into x. If the training 1s well done, then F can identify new data V' that it has never seen. 256 Chapter 6. Eigenvalues and Eigenvectors All Symmetric Matrices are Diagonalizable This section ends by returning to the proof of the (real) spectral theorem S = QAQT, Every real symmetric matrix S can be diagonalized by a real orthogonal matrix Q. When no eigenvalues of A are repeated, the eigenvectors are sure to be independent. Then A can be diagonalized. But a repeated eigenvalue can produce a shortage of eigenvectors. This sometimes happens for nonsymmetric matrices. It never happens for symmetric matrices. There are always enough eigenvectors to diagonalize S = ST, The proof comes from Schur’s Theorem: Every real square matrix factors into A = QTQ~! = QTQT for some triangular matrix T. If A is symmetric, then T = QTAQ is also symmetric. But a symmetric triangular matrix T is actually diagonal. Thus Schur has found the diagonal matrix we want. Schur’s theorem is proved on the website math.mit.edu/linearalgebra. Here we just note that if A can be diagonalized (A = XAX 1) then we can see that triangular matrix T'. Use Gram-Schmidt from Section 4.4 to factor X into QR : R is tnangular. Then A=XAX'=QRAR'Q!'=QTQ™! withtriangular T = RAR™ . 8 WORKED EXAMPLES = 6.3 A Test these symmetric matrices S and T for positive definiteness : p= - p= - 2 -1 0 2 -1 C S=1]-1 2 -1 and T=1|-1 2 -1 ! 0 -1 2J o c -1 2. Solution The pivots of S are 2 and % and 53-, all positive. Its upper left determinants are 2 and 3 and 4, all positive. The eigenvalues of S are 2 — v/2 and 2 and 2 + /2, all positive. That completes three tests. Any one test is decisive ! Eigenvalues give the symmetric choice A = QvAQT. This succeeds because ATA = QAQT = S. This also shows that the —1,2, —1 matrix S is positive definite. Three choices A,, A;, A3 give three different ways to separate S into AT A : TSz = 212 — 21,15 + 222 — 22523 + 212 Rewrite with squares ||A1:'B||2 =1:%+($2—271)2+ ($3—$2)2+I§ S = A'erl |A22[? = 2(z1 — 322)° + 3(z2 — 223)\" + §23 S =LDLT = AT A, | Asz|)? = M (gTx)? + A2(g €)% + A3(qT x)? S = QAQT = A As The determinant of T reveals when that matrix is positive definite : TestonT detT =4+2c-2c®=(1+c)(4—2c) must be positive. Atc = —land c = 2 we get det T = 0. Between ¢ = —1 and ¢ = 2 this matrix T is positive definite. The corner entry ¢ = 0 in S was safely between —1 and 2. 6.3. Symmetric Positive Definite Matrices 257 Problem Set 6.3 1 10 Suppose S = ST. When is ASB also symmetric with the same eigenvalues as S? (a) Transpose ASB to see that it stays symmetric when B = (b) ASB is similar to S (same eigenvalues) when B = Put (a) and (b) together. The symmetric matrices similar to S look like (___)S(__). For S and T, find the eigenvalues and eigenvectors and the factors for QAQT : 2 2 2] 1 0 2 S=12 0 0] and T=1[0 -1 -2 2 0 0] 2 -2 0] 9 12 12 16| Find all orthogonal matrices that diagonalize S = [ (a) Find a symmetric matrix [tl, ‘1’] that has a negative eigenvalue. (b) How do you know it must have a negative pivot? (c) How do you know it can’t have two negative eigenvalues? If C is symmetric prove that ATCA is also symmetric. (Transpose it.) When A is 6 by 3, what are the shapes of C and ATCA? Find an orthogonal matrix @ that diagonalizes S = [\": g] Whatis A ? If A = 0 then the eigenvalues of A must be . Give an example that has A # 0. But if A is symmetric, diagonalize it to prove that A must be a zero matnx. Write S and T in the form Az, 2T + A2xox; of the spectral theorem QAQT 3 1 9 12 e s=13 3] T=[3 1| Geplel=tei=y, Every 2 by 2 symmetric matrix is /\\lzlm'lr + /\\2:1:2:1:'{ = M1 P} + A2 P,. Explain P+ P = x1T] + T2z = I from columns times rows of Q. Why is PP, = 07? What are the eigenvalues of A = [_g 3]? Create a 4 by 4 antisymmetric matrix (AT = —A) and verify that all its eigenvalues are imaginary. (Recommended) This matrix M is antisymmetric and also . Then all 1ts eigenvalues are pure imaginary and they also have |\\| = 1. (|| Mz|| = ||x|| for every x so || Ax|| = ||z]|| for eigenvectors.) Find all four eigenvalues from the trace of M : 0 1 1 1 1 (-1 0 -1 1 : : : M = -\\—/_5 1 1 0 -1 can only have eigenvalues i or — 1. -1 -1 1 0 258 1 12 14 15 16 17 18 19 Chapter 6. Eigenvalues and Eigenvectors Show that this A (symmetric but complex) has only one line of eigenvectors: A= [1 _12] is not even diagonalizable: eigenvalues A = 0, 0. AT = Ais not such a special property for complex matrices. The good property is A’ = A. Thenall \\'s are real and the eigenvectors are orthogonal. Find the eigenvector matrices () for S and X for B. Show that X doesn’t collapse atd = 1, even though A = 1 is repeated. Are those eigenvectors perpendicular? 0 d 0] —d S=|d 0 0] B=|o0 0 0 1 0 0 1 1 0 have A =1,d, —d. 0 d Write a 2 by 2 complex matrix with S = S (a “Hermitian matrix™). Find ), and A for your complex matrix. Check that Efmg = 0 (this is complex orthogonality). True (with reason) or false (with example). (a) A matrix with n real eigenvalues and n real eigenvectors is symmetric. (b) A matrix with n real eigenvalues and n orthonormal eigenvectors is symmetric. (c) The inverse of an invertible symmetric matrix is symmetric. (d) The eigenvector matrix () of a symmetric matrix is symmetric. (e) The main diagonal of a positive definite matrix is all positive. (A paradox for instructors) If AAT = AT A then A and AT share the same eigen- vectors (true). A and AT always share the same eigenvalues. Find the flaw in this conclusion: A and AT must have the same X and same A. Therefore A equals AT, Are A and B invertible, orthogonal, projections, permutations, diagonalizable ? Which of these factorizations are possible: LU, QR, XAX ™!, QAQT ? 0 0 1 1 0 0 1 1 1 What numberbin A = [';' ‘5] makes A = QAQT possible? What number will make it impossible to diagonalize A? What number makes A singular? Find all 2 by 2 matrices that are orthogonal and also symmetric. Which two numbers can be eigenvalues of those two matrices? This A is nearly symmetric. But what is the angle between the eigenvectors ? U b S (e R , 1 4 [ = {0 1+10-15 as eigenvectors [ | an [?] 6.3. Symmetric Positive Definite Matrices 259 20 21 22 23 If Amax is the largest eigenvalue of a symmetric matrix S, no diagonal entry can be larger than Ap,... What is the first entry a;; of S = QAQT? Show why @11 < Amax- Suppose AT = — A (real antisymmetric matrix). Explain these facts about A: (a) T Az = 0 for every real vector z. (b) The eigenvalues of A are pure imaginary. (c) The determinant of A is positive or zero (not negative). For (a), multiply out an example of £T Az and watch terms cancel. Or reverse T (Ax)to —(Azx)Tx. For (b), Az = Azleadsto 2T Az = A\\ZTz = )||z||2. Part(a) shows that 2T Az = (x —iy)T A(z + iy) has zero real part. Then (b) helps with (c). If S is symmetric and all its eigenvalues are A = 2, how do you know that S must be 21 7 Key point: Symmetry guarantees that S = QAQT. What is that A? Which symmetric matrices S are also orthogonal ? Show why S? = I. What are the possible eigenvalues A ? Then S must be QAQT for which A ? Problems 24—49 are about tests for positive definiteness. 24 25 26 27 28 Suppose the 2 by 2 tests @ > 0 and ac — b%> > 0 are passed by S = [: bl (i) A1 and A2 have the same sign because their product A A2 equals : (i) That sign is positive because A; + A2 equals .SoA; >0and X\\, > 0. Which of Sy, Sz, S3, S4 has two positive eigenvalues? Use a test, don’t compute \\’s. Also find an z so that £ S;x < 0, s0 S; is not positive definite. 5 6 _1 -2 1 10] . [1 10 51= [6 7} 52 = [—2 -5] 53 = [10 100] 54= [10 101] | For which numbers b and ¢ is positive definite ? Factor S into LDLT. 1 b 2 4 c b ol O I Vi I Write f(z,y) = z2 + 4zy + 3y as a difference of squares and find a point (z, y) where f is negative. No minimum at (0, 0) even though f has positive coefficients. The function f(z,y) = 2zy certainly has a saddle point and not a minimum at (0, 0). What symmetric matrix S produces [ T y ] S [;] =2zy ? What are its eigenvalues ? Test to see if AT A is positive definite in each case: A needs independent columns. 112] 1 2 1 1 2 11 A=[ ] and A= |1 2| and A=[ 0 3 .2 l.J 260 30 31 32 35 36 37 39 Chapter 6. Eigenvalues and Eigenvectors Which 3 by 3 symmetric matrices S and T produce these quadratics ? TSz = 2(z? + 22 + 22 — 1125 — 7223). Why is S positive definite ? e'Tz = 2(z? + 23 + 7% — 2122 — 2123 — 2223). Why is T semidefinite ? Compute the three upper left determinants of S to establish positive definiteness. Verify that their ratios give the second and third pivots in elimination. 2 2 0] Pivots = ratios of determinants S = (2 5 3]|. 0 3 8 For what numbers c and d are S and T positive definite? Test their 3 determinants: ¢ 1 1] 1 2 3 S=11 ¢ 1 and T=1[2 d 4. 1 1 ¢ 3 4 5 Find a matrix witha > 0 and ¢ > 0 and a + ¢ > 2b that has a negative eigenvalue. If S is positive definite then S~ is positive definite. Best proof: The eigenvalues of S~! are positive because . Can you use another test ? A positive definite matrix cannot have a zero (or even worse, a negative number) on its main diagonal. Show that this matrix fails the energy test z TSz > 0: 4 1 1] [z; [1'1 ) 1:3] 1 0 2| |zy| isnotpositive when (z,,z2.23)=( , , ). b1 2 5_' _13‘ A diagonal entry s;; of a symmetric matrix cannot be smaller than all the A’s. If it were, then S — s;;I would have eigenvalues and would be positive definite. ButS — s;;I hasa on the main diagonal. Give a quick reason why each of these statements is true : (a) Every positive definite matrix is invertible. (b) The only positive definite projection matrix is P = 1. (c) A diagonal matrix with positive diagonal entries is positive definite. (d) A symmetric matrix with a positive determinant might not be positive definite ! For which s and t do S and T have all A > 0 (therefore positive definite) ? 8 —4 -41 't 3 0 S=1|-4 s —4 and T=13 t 4]. _—4 -4 8| L0 4 t From S = QAQT compute the positive definite symmetric square root QvAQT of each matrix. Check that this square root gives ATA = S : 5 4 10 6 6.3 40 41 42 43 45 46 47 49 . Symmetric Positive Definite Matrices 261 Draw the tilted ellipse 2 + zy + y* = 1 and find the half-lengths of its axes from the eigenvalues of the corresponding matrix S. With positive pivots in D, the factorization S = LDLT becomes LvVDVDLT. (Square roots of the pivots give D = vDVD.) Then C = vDLT yields the Cholesky factorization A = C™C which is “symmetrized L U™ 3 1 4 8 From Cz[o 2 8 25 ] findS. From S= [ ] find C = chol(S5). In the Cholesky factorization S = CTC, with C = V'DLT, the square roots of the pivots are on the diagonal of C. Find C (upper triangular) for 9 0 0] 1 1 1] S=1{01 2 and S=1|1 2 2 0 2 8 12 7 : oy cos§ —sinf||[2 Of| cos@ sinb Without multiplying S = [sin 0 cos 0] [O 5] [_ inf cos 0] , find (a) the determinant of S (b) the eigenvalues of S (c) the eigenvectors of S (d) a reason why S is symmetric positive definite. The graph of z = z2 + y? is a bowl opening upward. The graph of z = r’—ylisa saddle. The graph of z = —z% — y? is a bowl opening downward. What is a test on a, b, cfor z = ar? + 2bzy + cy? to have a saddle point at (x,y) = (0.0)? Which values of ¢ give a bowl and which c give a saddle point for the graph of z = 422 + 12zy + cy?? Describe this graph at the borderline value of c. When S and T are symmetric positive definite, ST might not even be symmetric. But start from STz = Az and take dot products with Tx. Then prove A > 0. Suppose C is positive definite (so yTCy > 0 whenever y # 0) and A has indepen- dent columns (so Az # 0 whenever & # 0). Apply the energy test to zTATCAx to show that S = ATC A is positive definite: the crucial matrix in engineering. Important! Suppose S is positive definite with eigenvalues A\\; > A2 > ... > A, (a) What are the eigenvalues of the matrix A\\, I — S? Is it positive semidefinite? (b) How does it follow that \\;zTx > T Sz for every x? (c) Draw this conclusion: The maximum value of zTSx/zTxis _ For which a and c is this matrix positive definite ? For which a and c is it positive semidefinite (this includes definite) ? a a a | All 5 tests are possible. S=]|la a+c a-c The energy T Sz equals 'a a-c a+c_ a(z) + 22 +13)* + c(z2 — 23)2. 262 Chapter 6. Eigenvalues and Eigenvectors 6.4 Complex Numbers and Vectors and Matrices Everything begins with = 1. By definition it solves £2 = —1. From there we discover the roots x; and x3 of any quadratic az? + bz + ¢ = 0. Then in one big step we allow any polynomial coz™ + ;™! + .-+ + ¢, = 0. Those coefficients ¢cg to ¢,, can be complex numbers ¢ = a + bt. The loop is closed : All complex polynomials of degree n have n complex roots x; to x,,. This was proved by Gauss, the greatest mathematician. -b+ Vb? — 4ac _ —b =+ i Vdac — b2 . 2a B 2a ' Always two roots, real or complex (or one double root when b?> = 4ac). For the roots of polynomials of degree n = 3 and n = 4, there are complicated formulas—not often used. A formula for the roots is not possible for n > 5. Example1 az?+bzx+c=0 for z = Complex numbers : Addition and multiplication and division (five rules). z1i+2z2=(a+ib)+(z+iy)=(a+x)+i(b+y) 2123 = (a + ib)(z + iy) = (ax — by) + i(bx + ay) : Notice (ib) (i1y) = —by Z=z1+1iy=2x — 1y = “Complex conjugate of z = = 4 1y” 2 |z|2 = z times z=(z+iy)(z-iy) =z +y?=r z1 _ (a+ib) _ (a+ib)(x—1y) _ (ax +by) +i(br — ay) zZ (z+iy) (z+1y)(z—1y) z? + y? Z 1 : You see why division is a little awkward : for 2 the best we can do 1s ﬁ; z Or use the polar form |z = |z|e*® = re*? | with Euler’s formula(e*® = cos @ + isin6 Comparing z+iy with re® =r cos §+irsin 6 tells us that = cos 6 and y = r sin 6. The polar form is ideal for multiplying complex numbers: Add angles and multiply r’s. 2122 = | 21| |22]€*(6r162) Z\" = rheind 21 _ |z ei(01—63) Z2 |22| N 0= imaginary axis t z=z+1y 12| =1 2| real TY —1 Z=zT—1y Figure 6.4: Complex plane coordinates: + iy or re*® = r cos 8 + ir sin 0 (polar) The next two pages give the rules for complex vectors and matrices. Then we explain the Fourier matrix : the most important complex matrix of all. 6.4. Complex Numbers and Vectors and Matrices 263 One final point about the eigenvalues of a real n by n matrix. The determinant of A—A\\J is a polynomial P()) of degree n. Then P(\\) = A? — (trace of A)\\ + (det of A) for a 2 by 2 matrix. The coefficients of that polynomial are real. We know that real polynomials P()) can produce complex roots, as in A2 + 1 = 0 with A = 44, —i. Here is the point: The complex roots of a real polynomial P()\\) come in conjugate pairs A\\ = a * b:. Another example is (A — 1)2 + 1 = 0 which produces A =1 + ¢ and 1 —i. The quadratic formula involves +v/b% — 4ac. If b?> < 4ac, the two square roots are a conjugate pair. In general we know : If P(x 4+ 1y) = an(x + 1y)™ + - - - + ag = 0 with real coefficients a,, to ao, then also P(x — iy) = 0. For real A, the eigenvectors for A and ) are v and T because Av = \\v gives AT = \\v. Complex Vectors : Lengths ||v|| and Inner Products 77w Length ||[v||2 =%Tv = |1|?+ -+ +|va|? >0 Notice 7T v and not vTv T T Dot product =0 w =T w; + -+ + UnWy Notice v w and not vT w The length of v = (1, 4) is ||v|| = v/2. The length of w = (1 + 4, 0) is also ||w|| = V2 Complex Matrices A and Complex Transpose A The key point is to replace AT or AT A or AAT by A orA AorAZA\" . Inthe same way, the test QT Q = I for an orthogonal matrix becomes GTQ = I. When a complex matrix is transposed, replace each entry x + iy in the transpose by £ — 1y. Many computer systems do this automatically. Real symmetric matrices ST = S Complex Hermitian matrices S =s§ Orthogonal eigenvectors Real eigenvalues Real orthogonal matrices QT = Q~! Complex unitary matrices —QT =Q! Orthogonal eigenvectors Eigenvalues |A\\| =1 () =€) If A has an eigenvalue A then A has an eigenvalue . Eigenvalues are real if A = A The Hermitian matrices A A and AA\" have real eigenvalues > 0: positive semidefinite : - . =T - . Eigenvectors of Hermitian matrices S = S have Z'y = 0: orthogonal eigenvectors. S=§T= [ 3 f 3 3 —5 31] is a typical Hermitian matrix with A\\=8 and A= —1 (real \\). Its eigenvectors T = L dy = 1- & 144 9¥T _11] are complex and orthogonal . ETy = 0. 264 Chapter 6. Eigenvalues and Eigenvectors The Four Fundamental Subspaces of a Complex Matrix A Suppose A is a complex matrix. Its columns span the column space C(A). The solutions x to Ax = 0 span the nullspace N(A). But the other two spaces naturally change from C(AT) and N(AT) to C(A' ) and N(A\" ). Then we do again have orthogonality between the complex row space C(KT) and nullspace N(A). And also orthogonality between the column space C(A) and the left nullspace C(ZT). Complex matrices fit perfectly into linear algebra when A replaces A™. We want the two pairs of subspaces to remain orthogonal in complex n-dimensional space C\". But the definition of orthogonality for complex vectors is Z'y = 0. So we must redefine two of the four fundamental subspaces, when complex numbers appear. The row space C(AT) becomes C(AT). The left nullspace N(AT) becomes N(AT). The rank r and the dimensions of these subspaces stay the same. Let me add the complex inner product in function space—an integral of T(t)y(t) dt instead of a sum of T yi. Please notice the most important complex basis—all the functions e*™* for —oo < n < 20 and 0 < z < 27. Those give the Fourier basis in Hilbert space: a complete orthogonal basis with these zero inner products between e*™* and e*** : r=2nw 2r . 2 ei(k—n)z / e'*Te N7 =/ e'k—mzdy = =0if k # n. 0 0 =0 t(k —n) S' = S: Real Eigenvalues and Orthogonal Eigenvectors Now we can complete the important steps at the beginning of Section 6.3 : Real symmetric matrices S = ST have real eigenvalues. And we can extend that statement to include oys . =T . . complex Hermitian matrices S = S . Their eigenvalues are also real. Proof : If Sz = Az then 'Sz = A\\Z'z. The number T x = ||a:||2 is positive. The number Z ' Sz is real because it equals its complex conjugate :cT S Tx. Then the ratio is z'S A= :c_T T = real number (1) Tz Maximizing that “Rayleigh quotient” in (1) produces the largest eigenvalue of S. The best will be the eigenvector for Sz = Mmax . Minimizing the Rayleigh quotient produces Ap:n. The in-between eigenvalues of S are “saddle points” of the ratio in (1). First derivatives are zero and second derivatives of the quotient in (1) have mixed sngns. Finally we establish that S has orthogonal eigenvectors, so that S=QAQ ! = QAQ : This requires two steps, to a triangular T and then a diagonal A. 1. Schur’s theorem proves that every square matrix A = QT@T for a triangular T.. 2. IfA=S=T thenT is the diagonal eigenvalue matrix A and S = QA-QT. 1 is proved on the website math.mit.edu/linearalgebra. 2 is proved in the Problem Set. 265 6.4. Complex Numbers and Vectors and Matrices Complex Eigenvalues and Eigenvectors of a Permutation P P is an important real matrix. It has complex eigenvalues A and eigenvectors . To find A\\ and x, start with x; Ar2 = A2 and z; = Az3 = A3. The final equation z; = Az4 becomes 1 = A%, This equation has four solutions ! They are the eigenvalues of P, all with |A| = 1. The determinant of P — A\\I is A4 — 1. The eigenvaluesare A = 1,1,1%2 = —1,13 = —i, 1 (first component). Then o = Ar; = A and z3 \"0 1 0 0] [z ] [ Io | BN 10 0 10 x2 | _ | T3 | _ T2 Pr=19 00 1|z |= |z || 3 2) _1 0 0 0‘ _14_ L:BlJ L.’B4- Those eigenvalues add to 0 = trace of P. The imaginary roots ¢ and —t of the real polynomial A* — 1 are a conjugate pair, as expected. Here is the Fourier matrix F' with columns of F' = eigenvectors of P. The key equationis F~!PF = A or PF = FA. 01001[1 1 1 1 11111 0010]|l1 & 12 43 1 i 2 3 i PE=1000 1|1 i i |71 2 it 8 12 =FA ) 1 0001 4% 4% ) |1 4 9| When P is N by N, the same reasoning leads from PV = I'to AN = 1. The N eigenvalues of P are again equally spaced around the circle. Now they are powers of the complex number w = e2™*/N at 360/N degrees = 27/N radians. The solutionsto AN =1 are A\\=w, w?,...,w\"V 1,1 with w=¢e3\"/N,| 4) In the complex plane, the first eigenvalue is e = cos 8 + i sin 8 and the angle 8 is 27/N. The angles for the other eigenvalues are 26, 36, ..., N6. Since 8 is 27 /N, that last angle is N6 = 27 and that eigenvalue is A = €™ which is cos 27 + isin 27 = 1. Imaginary axis w Figure 6.5: Eigenvalues of Py : The 8 powers of w = e3™*/8 add to zero (pair them off). 266 Chapter 6. Eigenvalues and Eigenvectors The Fourier matrix Fg is 8 by 8. It contains the eigenvectors of the permutation Pj. If we count rows and columns from Q to 7 (starting at 0 the way engineers do), then the eigenvalues \\x = w* of the 8 by 8 permutation are Ag to A7. The Fourier matrix Fn (the eigenvector matrix for Py) has w’* in row 3, column k. The columns of Fy = eigenvectors of Py are orthogonal : F:, Fn = NI First proof : The permutation Py is an orthogonal matrix = orthogonal eigenvectors. Second proof: The eigenvectors are complex, so orthogonality means EJqu =01f j #k. N a’ —1 a—1 (1.7&.7%,..., V\"), vk, w?,... , wN V5 = 14a+a%+- - +aV 71 = Here @ = ww*. If j = k thena = 1. The dot product of every column of F' with itself (the length squared) is N. If j # k then a” is W~ w* = 1 times 1. So the sum above is 0/(a — 1) = 0. The Fourier matrix F; has orthogonal columns. All Circulant Matrices C Share the Fourier Eigenvectors If Px = Az then P2z = APx = A\\2z. Eigenvectors of P are eigenvectors of P?. Multiply again by P to find P3z = APz = Az. Then P3 also has the same eigenvectors . Every power of P will have P*x = A™x, where x is a column of the Fourier matrix F. More than that, every combination C = coI + ¢; P 4+ c2P? + c3P3 has Cx = (co + c1 A + c2A? + c3A3)x. These matrices C are called circulants, and you have to see how they look. Here is the eigenvector of P with Px = Ax = i1x. The same x is also an eigenvector of the circulant C'! The columns of F' are eigenvectors of P and also of C. - - p- n - - Cp €C1 C» C3 1 1 C3 C €C1 C2 ) . .9 .3 () Cz = o | = C C C . 5 c; c3 g € i2 (co + c1t + €21 + c31°) ;2 (5) c1 ¢ ¢z ¢ || eigenvalue of C 33 The name “circulant matrix” expresses the pattern inside C. The main diagonal is col. The diagonal above is ¢y P. That diagonal circles around into the bottom corner, like P. The c’s on every upper diagonal circle around to end on a lower diagonal. All circulants C have the same eigenvectors: They are the columns of F'. This means that CF = F A. All circulant matrices are diagonalized by the same Fourier matrix. The diagonal eigenvalue matrix A contains the four eigenvalues of C. Those are the four complex numbers ¢y + c; A + c2A% + ¢3A3 with A = 1 and i and 2 and 3. 6.4. Complex Numbers and Vectors and Matrices 267 The Fast Fourier Transform The discrete Fourier transform (DFT) of a vector is the vector Fz. The signal is z. In frequency space it is F'x. That step uses the Fast Fourier Transform (FFT). This is the fast multiplication that makes modern signal processing possible. The key to the FFT is a factorization of Fy. The middle factor has two copies of Fn/2. That cuts the work almost in half—because the first factor only has diagonal matrices, and the third factor is just a permutation. Here is one step in the FFT: Three factors Fyy — [ I B ] [ Fnya FO ] [ evenO,2,... ] 6) N/3 I -B 0 odd 1,3,... This is simplest to see for N = 4. Here are F; and two copies of F5 and B : 1 1 1 1] 10 1 o]]1 1 11 0 0 0° 1 ¢ ¢ #3101 0 |1 -1 0010 . 12 ¢4 # |71 0 -1 0 1 1|{lo1o0o0]| P 1 @ ¢ P2 [01 0 -i]] 1 -1{]0 00 1 Suppose we apply this idea to Fjgo4x. Normal matrix-vector multiplication needs (1024)? steps—more than a million. The first factor in (6) needs only N steps from B. The third factor has no multiplications. The middle factor with F’s is destined to be reduced to four diagonal copies of F5s56 and then eight copies of Fj2g. This is recursion. In the end there are the diagonal B’s—10 matrices because 1024 = 29, The FFT costs about 10(1024) = N log, N complex multiplications instead of N2. A million steps have been reduced by a factor near to 100. You may need to adjust F and x, to start with N = power of 2. Wikipedia provides a more careful count of multiplies and adds. The FFT has many variations—from special and beautiful patterns inside F. Multiplying Two Circulant Matrices CD = DC Multiplying C = 2I + P + 3P3 times D = 3I + P + P? + 4P3 is like third grade multiplication of 3012 times 4113, with the important difference that P* = I : 3 01 2 CD=6I+5P+3P>+18P®+7P%+3P°+12P6 4 1 1 3 9 0 3 6 =(6+7)I+(5+3)P+(3+12)P?+18P° 3 0 1 2 3 0 1 2 =131+ 8P +15P3 + 18P3 since P4 =1 12 0 4 8 12 3 718 3 5 6 Multiply C D = Convolve top rows ¢ & d Two key points: We don’t carry part of 18 into the next column. And we replace P4, P5,P® by I, P, P2. This “multiplication of vectors” is called cyclic convolution : c®d=(21,03) ® (3,1,1,4) = (13,8,15,18) =d ® c (8) We multiply powers of P with P4 = I. The answer would be the same for D times C. 268 Chapter 6. Eigenvalues and Eigenvectors But there is no need to keep writing P’s when they are understood. It is the coefficients 13. 8,15, 18 that go into the circulant matrices CD = DC. p - - - 210 3 311 4 13 8 15 18 | 3 210 4 3 1 1 18 13 8 15 CD=149 39 1|]l1 431/ 15 1813 8|=-PC O 103 2f[1143| | 815 18 13| Ordinary convolutionis ¢ * d = (2,1,0,3) % (3,1,1,4) = (6,5,3.18,7,3,12). Multiply polynomials c and d of degree 3 (4 coefficients) to get a 6th degree polynomial (7 coefficients). Then ¢ % d changestoc & d = (13, 8,15, 18) by using P4 = I. The Convolution Rule in Signal Processing Start with the formula Fc for the eigenvalues Ao to Axy_; of any circulant matrix C. From CF = FA, the top row of C multiplies each eigenvector gi in F' to give A times 1: - p- - [ Xo co+c1 4+ enN-1 co ' 'N_l R f| @ |-re AN-1 | Co + w1 +---+CN-1w(N-1)(N—l)_ | CN-1 | Example for N = 2 with w = 2\"/ = —1 in the Fourier matrix (eigenvector matrix) F': _l 1 e Ao(C)_Co-FCl_l 1 Co| F‘[l —1] C—[cl co] [Al(C) le-—al| |1 -1]|la =Fe (1) The other fact we need is that all the circulant matrices C' and D and CD have the same eigenvectors g, to q _, (the columns of F). Then the eigenvalues just multiply ! CDgq; = CAi(D)g; = Ai(C)Ai(D)g;. This is the second fact and the symbol .* is a way to express it : \" M(CD) ] [ M(@XD)] [2(C) ] [ X(D)] M(CD) [ = | MCOMD) | = | M(C) | x| M(D) (12) i ‘ . | ) i A . That Hadamard product .» multiplies vectors component by component. Now we use (10) to substitute F'c and F'd for the last two vectors in (12). The left side becomes F(c & d), when we remember from (9) that the top row of C'D contains the cyclic convolution ¢ @) d. Here is the beautiful result that simplifies signal processing : Convolving vectors is multiplying transforms Convolution Rule F(c &® d) = (Fc).x(Fd) (13) C D for circulant matrices is ¢ @) d for top rows and (F'c).*(F'd) for eigenvalues. 6.4. Complex Numbers and Vectors and Matrices 269 Problem Set 6.4 1 For the complex number z =1 — 4, find Z and r = |2| and 1/z and the angle 6. 2 Find the eigenvalues and eigenvectors of the Hermitian matrix S = [ 1 i ; . ; ' ] : 3 If @TQ = I (unitary matrix = complex orthogonal) and Qx = Az, show that |\\| = 1. (01 0 4 Find the Fourier matrix F3 with orthogonal columns = eigenvectorsof | 0 0 1 |. |1 0 0 5 (Challenge) Extend equation (7) to the 6 by 6 matrix identity that connects Fg to F3. 6 As in equation (9), multiply the 3 by 3 circulant matrices C' and D with top rows 1,1,1 and 1,2, 1. Separately find the cyclic convolution of those two vectors by multiplying 111 times 121 and then reducing 5 numbers to 3 numbers based on P’ =1 7 Check the Convolution Rule in equation (13) that connects those vectorsc = (1.1. 1) and d = (1,2, 1) through the 3 by 3 Fourier matrix F3 with w® = 1. 8 Verify Euler’s great formula e = cos 8 + i sin 8 using these first terms for e z1+i0+1(i0)2+1(i0)3 cosf~1— =67 sinfx0— 65 2 6 2 6 9 Find cos 26 and sin 26 from cos 0 and sin @ using e*? (Euler) and (e'?) (¢'?) = €%, 10 What test decides if a circulant matrix C is invertible? Is C~! also a circulant ? Why ? 11 Which circulant matrices are the squares of circulant matrices ? 12 When would a circulant be Hermitian (ET = C) or unitary (ET =C1)? 13 Find the discrete Fourier transform Fz of z = (1,0,1,0) and z = (0,1,0.1). 14 If w = 27¥/64 then w? and \\/w are among the and roots of 1. 15 Find the eigenvalues of the circulant second difference matrix C. 2 -1 0 -1] |1 -1 2 -1 0]_ 3 C= 0 -1 2 117 2 -P-P -1 0 -1 2 -l 270 Chapter 6. Eigenvalues and Eigenvectors 6.5 Solving Linear Differential Equations (If Az = Az then u(t) = e*x will solve % = Au. Each ) and x give a solution e*‘A 2 If A= XAX 'then |u(t) = eAtu(0) = XeM X~ 1u(0) = cie*tx) + - - + cne* 'y, 3 Matrix exponential eAt = [+ At +---+ (At)\"/n!+--- = XeMXVif A= XAX™L, 4 A is stable and u(t) — 0 and e* — 0 when all eigenvalues of A have real part < 0. Second order eqn / \" ’ B u _ 0 1j|lu (Firstordersystem u”+Bu'+Cu=0 means [u'] B [—C —B][ub Eigenvalues and eigenvectors and A = XAX™! are perfect for matrix powers AF, They are also perfect for differential equations du/dt = Au. This section is mostly linear algebra, but to read it you need one fact from calculus: The derivative of eAl is Al The whole point of the section is this : Constant coefficient differential equations can be converted into linear algebra. Single equations in (1) and systems du/dt = Au in (2). ; : The ordinary equations d—zt‘ = u and % = Au are solved by exponentials: du d i produces u(t) = Ce® d—tt‘ = Au produces u(t) = Ce*| (1) At time t = 0 those solutions start from u(0) = C because € = 1. This “initial value” tells us C. The solutions u(t) = u(0)e’ and u(t) = u(0)e™t start from u(0). We just solved a 1 by 1 problem. Linear algebra moves to n by n. The unknown is u(t), a vector changing with time ¢. It starts from u(0). The n equations contain a square matrix A. We combine n solutions u(t) = etz from n eigenvalues Az = A\\x: [ ul(O) 1 | un(0) ] These differential equations are linear. If u(t) and v(t) are solutions, so is Cu(t) + Dv(t). We will need n constants like C and D to match the n components of w(0) at time zero. Our first job is to find n “pure exponential solutions” u = e*zx by using Az = \\x. Notice that A is a constant matrix. In other linear equations, A changes as ¢ changes. In nonlinear equations, A changes as u changes. We don’t have those difficulties, du/dt = Au is “linear with constant coefficients”. Those and only those are the dif- ferential equations that we will convert directly to linear algebra. System of du _ n equations | gt — Au| starting from the vector u(0) = att = 0. (2) Solve linear constant coefficient equations by exponentials u = eMx when Az = \\z. 6.5. Solving Linear Differential Equations 271 Solution of du/dt = Au Our pure exponential solution will be e times a fixed vector z. You may guess that A is an eigenvalue of A, and x is the eigenvector. Substitute u(t) = eAt du/dt = Au to prove you are right. The factor et will cancel to leave Az = Az : Z into the equation Aty du Choose u = ¢ NS Y, . Nt when Az = \\x dt AeMx| agreeswith | Ay = AeAlyp 3) All components of this special solution u = ez share the same e*. The solution grows when A > 0. It decays when A < 0. If A is a complex number, its real part decides growth or decay. The imaginary part w gives oscillation e*“* like a sine wave. dt 1 0 2 This is a vector equation for w. It contains two scalar equations for the components y and 2. They are “coupled together” because the matrix A is not diagonal: d d d_u = Au d [yJ = [O 1] [y] means that d—Zzz and i =Y. d Example 1 Solve = - Au= [ 01 ]u starting from u(0) = [ 4 ] dt dt |z 1 0f ]|z dt The idea of eigenvectors is to combine those equations in a way that gets back to 1 by 1 problems. The combinations y + 2 and y — z will do it. Add and subtract equations : d d Wtz =z+y and —(y-z)=-(y-2). The combination y 4+ z grows like et, because it has A = 1. The combination y — 2 decays like et because it has A = —1. Here is the point: We don’t have to juggle the original equations du/dt = Au, looking for these special combinations. The eigenvectors and eigenvalues of A will do it for us. This matrix A has eigenvalues 1 and —1. The eigenvectors x are (1,1) and (1, —1). The pure exponential solutions u; and u; take the form etz with A1 =1land Ay = —1: u,(t) = eMig, = et [i] and us(t) = e)\"-’ta:g =et [_i] : 4) Complete solution u(t) _~t|1 —t| 1]_ Cel + De ! Combine u; and u; u(t) = Ce [1] +De -1 | cet — De—t | ) With these two constants C and D, we can match any starting vector u(0) = ( Sett = 0 and €° = 1. Example 1 asked for the initial value to be u(0) = (4, u(0) decides C 1 Cand D 1 With C = 3 and D = 1 in the solution (5), the initial value problem is completely solved. ]+D[_:]=[;] yields C=3 and D =1. 272 Chapter 6. Eigenvalues and Eigenvectors For n by n matrices we look for n eigenvectors. The numbers C and D become c; to c,,. 1. Write u(0) as a combination ¢, x; + - - - + ¢, &, of the eigenvectors of A. 2. Multiply each eigenvector x, by its growth factor eAit, 3. The solution to du/dt = Au is the same combination of those pure solutions e*z : u(t) = clexltml 4+ 4 cne’\\\"tmn. (6) Not included : If two A’s are equal, with only one eigenvector, another solution is needed. (It will be te*x.) Step 1 needs a basis of n eigenvectors to diagonalize A = XAX ™1, Example 2 Solve du/dt = Au knowing the eigenvalues A = 1, 2, 3 of A: Example of du/dt = Au 1 1 1] El Equation for u(t) pri 0 2 1|u starting from u(0)= |7 Initial condition u(0) 0 0 3] 4 The eigenvector matrix X has ; = (1,0,0) and x2 = (1,1,0) and z3 = (1,1, 1). Step 1 The vector u(0) = (9,7,4) is 2x; + 3z, + 4x3. Then (c;, 2, c3) = (2, 3,4). Step 2 The factors e** give exponential solutions u = et:rl and €2t$2 and 63t$3. Step 3 The combination that starts from u(0) is u(t) = 2eta:1 + 362t:c2 + 463ta:3. The coefficients 2, 3, 4 came from solving the linear equation c; x; + cox2 + czx3 = u(0): ' 1fa]l [1 1 1][2] [9] T, T2 3| ||l =101 1 3|l =17 whichis Xc = u(0). (7) - - Lc3. ..0 O 1& b4d L4. You now have the basic idea—how to solve du/dt = Au. The rest of this section goes further. We solve equations that contain second derivatives, because they arise so often in applications. We also decide whether u(t) approaches zero or blows up or just oscillates. At the end comes the matrix exponential eAt. The short formula eAtu(O) solves the equation du/dt = Au in the same way that A*ug solves the equation uxy; = Aui. Example 3 will show how “difference equations” help to solve differential equations. All these steps use the A’s and the &’s. This section solves the constant coefficient problems that turn into linear algebra. Those are the simplest but most important differential equations—whose solution is completely based on growth factors el Second Order Equations The most important equation in mechanics is my’”’ 4 by’ 4+ ky = 0. The first term is the mass m times the acceleration a = y”. Then by’ is damping and ky is force. 6.5. Solving Linear Differential Equations 273 The unknown y(t) is the position of the mass m at the end of the spring. This is a second- order equation (it is Newton’s Law F = ma). It contains the second derivative y”’ = d?y/dt2. It is still linear with constant coefficients m, b, k. In a differential equations course, the method of solution is to substitute y = % Each derivative of y brings a factor \\. We want y = et to solve the spring equation: d m—+b£+ky=0 becomes (m/\\2+bz\\+k)e>‘t=0. (8) Everything depends on mA? + b\\ + k = 0. This equation (8) for A has two roots A, and Ao. Then the equation for y has two pure solutions y; = et and Yo = erat, Their combinations c,y; + cays give the complete solution. This is not true if A\\; = Aa. In a linear algebra course we expect matrices and eigenvalues. Therefore we turn the scalar equation (8) (with y\") into a vector equation for y and y' : First derivative only ! Suppose the mass is m = 1. Two equations for u = (y, y') give du/dt = dy/dt =y’ d [y 0 1 WYy e S[]-[2 ][] o The first equation dy/dt = y' is trivial (but true). The second i 1s equation (8) connecting y\" to ¥’ and y. Together they connect u’ to u. Now we solve u’ = Au by eigenvalues of A: AN = [‘A L . 5 B _k —b—A] has determinant A° +bA + k = 0. The equation for the \\’s is the same as (8)! Itis still A + b\\ + k = 0, sincem = 1. The roots \\; and )2 are now eigenvalues of A. The eigenvectors and the solution are 1 1 1 1 T, = [,\\1] Ty = [)‘2] u(t) = cleAlt [Al] +c;>eA2t [M] : The first component of u(t) has y = clexlt + cze&t—the same solution as before. It can’t be anything else. In the second component of u(t) you see the velocity dy/dt. The 2 by 2 matrix A in (9) is called a companion matrix—a companion to the equation (8). Example 3 Undamped oscillation with y\"’ + y = 0 and y = cost This is our master equation with mass m = 1 and stiffness k = 1 and d = 0: no damping. Substitute y = e into y” + y = 0 to reach A2 + 1 = 0. The rots are A = i and A = —i. Then half of e* + e** gives the solution y = cost. As a first-order system, the initial values y(0) = 1, y’(0) = 0 go into ©(0) = (1,0): PSS N [ The eigenvalues of A are again the same A = ¢ and A = —i (no surprise). As y = cost oscillates down and up, the vector (y(t),y’(t)) (cost, — sint) goes around a circle. 274 Chapter 6. Eigenvalues and Eigenvectors A is anti-symmetric with eigenvectors £, = (1,1) and &2 = (1, —¢). The combination that matches u(0) = (1,0) is 3(z; + x2). Step 2 multiplies the x's by e** and e™*. Step 3 combines the pure oscillations e and e™* to find y = cost as expected: All good. The circular path of u = (cost, —sint) is drawn in Figure 6.6. The radius is 1 because cos? t + sint = 1. The “period” 27 is the time for a full circle. Figure 6.6: The exact solution u = (cost, — sint) stays on a circle, completed at t = 2. Forward differences Y;, Y3, ... in (11 F) spiral out in 32 steps, overshooting y(27) = 1. Difference Equations To display a circle on a screen, replace y”’ = —y by a difference equation. Here are three choices using Y (t + At) — 2Y (t) + Y (t — At). Divide by (At)? to approximate d?y/dt?. F Forward fromtimen — 1 —Y,-1 (1F) C Centered at time n Ynt1 = 2¥n + Yoy = =Y, (110) ) (At)? B Backward from time n + 1 —Yn41 (11B) Figure 6.6 shows the exact y(t) = cost completing a circle at t = 27. The three dif- ference methods don’t complete a perfect circle in 32 time steps of length At = 27/32. The spirals in those pictures will be explained by eigenvalues A for 11F, 11B,11C. Forward || > 1(spiral out) Centered |\\|=1 (best) Backward |\\| <1 (spiral in) The 2-step equations (11) reduce to 1-step systems U,y = AU ,,. Toreplace u = (y,y ') the unknown is U, = (Y,,Z,). We take n time steps of size At starting from Up: Forward Y,,, =Y, +AtZ, 1 At||Y. (11F) Zpy1=2, - AtY, ecomes Unia=|_ A, 5 ||z, | =AUn (12) Those are like Y’ = Z and Z’ = —Y . They are first order equations, so we have a matrix in Un4+1 = AU,. Eliminating Z would recover the second order equation (11 F) for Y. 6.5. Solving Linear Differential Equations 275 My question is simple. Do the points (Y, Z,,) stay on the circle Y? + Z* = 1\" No, they are growing to infinity in Figure 6.6. We are taking powers A™ and not e so we test the magnitude |\\| = /1 + (At)? of the eigenvalues X of A in (12): Eigenvaluesof A A\\ =1+ tAt Then |A\\| > 1 and (Y,, Z,) spirals out The backward choice in (11 B) will do the opposite in Figure 6.7. Notice the new matrix : Backward Yo —Yn= At Zny 1S 1 —At|| Yop _ Y, =U (13) Difference Znt1 — Zn = —At Yo At 1 ||z Tz |75 That matrix has eigenvalues 1 + iAt. But we invert it to reach Up4y from U,. IA| < 1 explains why the solution spirals inward (left figure) for backward differences. Figure 6.7: Backward differences (11B) spiral in. Leapfrog (11C) stays near the circle. The second figure shows 32 steps with the centered choice (11 C). Now the solution stays close to the circle (Problem 28) if At < 2. This is the leapfrog method, constantly used. The second difference Yn 41 — 2Y, + Y, _1 “leaps over” the value Yy, in (11 C). This is the way a chemist follows the motion of molecules (molecular dynamics leads to giant computations). Computational science is lively because one differential equation can be replaced by many difference equations—some unstable, some stable, some neutral. Problem 26 has a fourth (very good) method that exactly completes the circle. Real engineering and real physics deal with systems of equations (not just a single mass at one point). The unknown y is a vector. The coefficient of y\" is a mass matrix M : n masses on the main diagonal. The coefficient of y is a stiffness matrix K, not a number k. The coefficient of ¢’ is a damping matrix which might be zero. The vector equation My” + Ky = f is a major part of computational mechanics. 276 Chapter 6. Eigenvalues and Eigenvectors Stability of 2 by 2 Matrices For the solution of du/dt = Au, there is a fundamental question. Does the solution approachu = 0 ast — oc? Is the problem stable ? Does energy dissipate ? A solution that includes €' is unstable. Stability depends on the eigenvalues of A. The complete solution u(t) is built from pure solutions eMx. If the eigenvalue \\ 1s real, we know exactly when eAt will approach zero: The number \\ must be negative. If the eigenvalue is a complex number A = r + is, the real part r must be negative. When e splits into e\"e?St, the factor e!St has absolute value fixed at 1: 1st Euler’s formula e’ =cosst +isinst has |ei$t|2 = cos? st +sin’ st = 1. Then |e*!| = e™ and the real part of A controls the growth (r > 0) or the decay (r < 0). The question is : Which matrices have negative eigenvalues? More accurately, when are the real parts of the A\\’s all negative ? 2 by 2 matrices must pass two tests. A is stable and u(t) — 0 when all eigenvalues A of A have negative real parts. For any 2 by 2 matrix A = [2 g] here are the two tests : A1+2A2<0 Thetrace T =a+d = A; + A2 must be negative. (147) AA2>0 The determinant D = ad — bc = A\\ A2 must be positive. (14D) Reason If the \\’s are real and negative, their sum is negative. This is the trace T' = a + d. Their product is positive. This is the determinant D. The argument also goes in reverse. If D = A; A2 is positive, then A; and A, have the same sign. If T = A} + Ay is negative, that sign will be negative. When A; and A are complex numbers, they must have the form r + is and r — is. Otherwise T and D will not be real. The determinant D is automatically positive, since (r+is)(r —is) =% + s%. Thetrace Tisr + is + r — is = 2r. So a negative trace T means that r < 0 and the matrix is stable. The two tests in (14) are correct. The Exponential of a Matrix We want to write the solution u(t) in a new form eAtu(O). Here is et for matrices! Matrix exponential e eAt = I+ At + L(At)* + L(At)3 + - - (15) Its t derivative is AeAt A+ A%+ % A2 +... = AeAt Its eigenvalues are e (I + At + %(At)z +--)e=(1+ A+ %(,\\t)'l 4.z Then u(t) = eA*u(0) solves % = Ae'u(0) = Au(t), 6.5. Solving Linear Differential Equations 277 The number that divides (At)™ is “n factorial”. This is n! = (1)(2)---(n — 1)(n). The factorials after 1,2,6 are 4! = 24 and 5! = 120. They grow quickly. The series always converges and its derivative is always AeAt. Therefore eAtu(O) solves the differential equation with one quick formula—even if there is a shortage of eigenvectors. This chapter emphasizes how to find u(t) = eAtu(O) by diagonalization. Assume A does have n independent eigenvectors, so we can substitute A = XAX ™! into the series for e, Whenever X A X~ X AX ! appears, cancel X \"' X in the middle: Use the series eAt = [+ XAX 1t + L(XAX1t)(XAX1t) + Factor out X and X 1 =X[T+At+3(At)2+--- ] X1 At is diagonalized ! eAt = X eAt X1, et has the same eigenvector matrix X as A. Then A is a diagonal matrix and so is e\\t The numbers e*it are on the diagonal. Multiply X elt x ~1u(0) to recognize u(t): ( | [eMt - ¢ | eAtu(O)zXeAtX'lu(O)= Ty -+ Tp t 1. (16) ernt | [cn __J s -l 9 - At This solution e“**u(0) is the same answer that came in equation (6) from three steps : 1. u(0) = 11 + - -+ + cnTn = Xc. Here we need n independent eigenvectors. 2. Multiply each x; by its growth factor eMit to follow it forward in time. 3. The best form of u(t) = eAtu(O) is u(t)= clexltml +---+ cneA\"ta:n. (17) Example 4 Use the infinite series to find eAAt for A = [_2 (l,] Notice that A = I': oo e el el A3, A% A7, A8 will be a repeat of A, A%, A3, A%. The top right corner has 1,0, —1,0 repeating over and over in powers of A. Then t — -t3 starts the infinite series for e in that top right corner, and 1 — -t2 starts the top left corner: 1240 =134 A=+ At+ LA+ IAR 4= | ° - —t+%;t3—--- l—%t2+--- That matrix eA* shows the infinite series for cos ¢ and sin ¢ : perfect. 1 0 1 At _ | cost sint A_[—l 0] € —[—sint cost]' (18) 278 Chapter 6. Eigenvalues and Eigenvectors At At This A is antisymmetric (AT = —A). Its exponential e** is an orthogonal matrix. The eigenvalues of A are ¢ and —i. Then the eigenvalues of e“** are e’ and e, 1 The inverse of et is always e~ AL, 2 The eigenvalues of eAt gre always eAl, 3 When A is antisymmetric, eAt i orthogonal. Inverse = transpose = e— At Antisymmetric is the same as skew-symmetric: AT = —A. Now A has pure imaginary eigenvalues like i and —i. Then e!* has eigenvalues like ' and e~**. Their absolute value is |A| = 1: neutral stability, pure oscillation, energy is conserved. So ||u(t)|| = ||u(0)]|. If A is triangular, the eigenvector matrix X is also triangular. So are X ! and eAt. The solution u(t) is a combination of eigenvectors. Its short form is e“1!(0). d Example5 Solve d_lt‘ = Au= [(1) ;] u starting from u(0) = [3] att = 0. Solution The eigenvalues 1 and 2 are on the diagonal of A (since A is triangular). The eigenvectors are (1,0) and (1,1). Then et produces u(t) for every u(0) : t _ ot L2t u(t) = XeM X ~14(0) is [(1) }] [e e2t] [(1) i]u(O) = [o € e;;e ]u(o), That last matrix is eAL. It is nice because A is triangular. The situation is the same as for Ax = b and inverses. We don’t need A~! to find z, and we don’t need eAt to solve du/dt = Au. But as quick formulas for the answers, A~'b and eAtu(O) are unbeatable. Example 6 Solve y” + 4y’ + 3y = 0 by substituting et and also by linear algebra. Solution Substituting y = eAt yields (A2 + 4\\ + 3)eAt = (0. That quadratic factors into A2 4+42+3 = (A+1)(A+3) = 0. Therefore \\; = —1 and Az = —3. The pure solutions are y, = et and Y2 = e\"3t. The complete solution ¥y = ¢,y + c2y2 approaches zero. To use linear algebra we set u = (y,y'). Then the vector equation is u’' = Au: dy/dt =y du [ 0 1J converts to —dt- = u. dy'/dt = -3y — 4y -3 -4 This A is a “companion matrix” and its eigenvalues are again A\\; = —1 and A\\; = -3. Same quadratic det(4A - \\) = :g _41_ N M 4+42+3=0. The eigenvectors of A are (1, A1) and (1, A;). Either way, the decay in y(t) comes from et and e~ 3¢. With constant coefficients, calculus leads to linear algebra Az = \\x. Example 7 Substituting y = e into y” — 2y’ + y = 0 gives an equation with repeated roots: A> —2\\ +1 =0is (A - 1) = 0 with A = 1, 1. A differential equations course would propose et and te' as two independent solutions. Here we discover why. 6.5. Solving Linear Differential Equations 279 Linear algebra reduces 3\" = 2y’ — y to a vector equation for u = (y,9'): dly]| [ ¢ . du 01 E[y']_[zy’-y] is dt—Au_[_l 2]14. (20) A has a repeated eigenvalue A = 1, 1 with trace = 2 and det A = 1. The only eigen- vectors are multiples of * = (1,1). Diagonalization is not possible. This matrix A has only one line of eigenvectors. So we compute e from its definition as a series Short series et =eltelA-Dt = et [I + (A - D). (21) That “infinite” series for e(4~D* ended quickly because (A — I)? is the zero matrix. You can see tet in equation (21). The first component of e u(0) is our answer y(t) e e B B s Our last examples are really pushing the limits of a linear algebra course ! The ordinary differential equation du/dt = Au changes to the heat equation and the wave equation. These are partial differential equations 8u /8t = 8%u/8x? and 8%u/8t? = 8%u/dx32. Heat equation In Figure 6.8a, the temperature at the center starts at 2v/2. Heat diffuses into the neighboring boxes: du/dt = Au (outside boxes frozen at 0°). The rate of heat flow between boxes is the temperature difference. From box 0, heat flows right and left at the rate u; — ug and u_; — up. So the flow out is u; — 2ug + u_, in the second row of Au. Wave equation d?u/dt?> = Au has the same eigenvectors z. But now eigenvalues A < 0 lead to oscillations etz and e~z The frequencies come from w? = —\\: 2 : : : i1—(6“‘)t:c) — A(e™!z) becomes (iw)2e™iz =A™z and w? = - dt? There are two square roots of —), so we have e*tz and e ~*!z. With three eigenvectors this makes six solutions to u”” = Au. A combination will match the six components of u(0) and u'(0). Since u' = 0 in this problem, eWiz and e~ Wiz produce 2 cos wt . t=0 . -1 1 0] ¢=9 -m £ Heat source =] 1 -2 1 dt t > O\\ 0 1 -1) Strike a violin string l/l\\ —2 -1 0 1 2 -2 =2)=0 P _4u w@)=0 u\\— = —_—= = dt t>0 Figure 6.8: (Heat equation) Spike at ¢t =0 diffuses away. (Wave equation) Waves on a string. 280 Chapter 6. Eigenvalues and Eigenvectors Problem Set 6.5 d 1 Find two X’s and =’s so that u = e)‘tz solves d_lt‘ = [g :1;] u. What combination u = cle)‘lta:l + cze)‘2ta:2 starts from u(0) = (5, —2)? 2 Solve Problem 1 for u = (y, z) by back substitution, z before y: dz dy Solve = =2 from z(0) = —2. Then solve i 4y + 3z from y(0) = 5. The solution for y will be a combination of et and ef. The \\’s are 4 and 1. 3 (a) If every column of A adds to zero, why is A = 0 an eigenvalue ? (b) With negative diagonal and positive off-diagonal adding to zero, u’ = Au will be a “continuous” Markov equation. Find the eigenvalues and eigenvec- tors, and the steady state as t — 00 -2 3 du Solve -(E=[ 2 _3 ]u with u(0) = [11] . What is u(oc)? 4 A dooris opened between rooms that hold v(0) = 30 people and w(0) = 10 people. The movement between rooms is proportional to the difference v — w: v _ w—-v and dw dt dt Show that the total v + w is constant (40 people). Find the matrix in du/dt = Au and its eigenvalues and eigenvectors. Whatare vand watt =1andt = 00 ? =v—-w. 5 Reverse the diffusion of people in Problem 4 to du/dt = — Au : dv dw Vv and oW The total v+w still remains constant. How are the A’s changed now that A is changed to —A? But show that v(t) grows to infinity from v(0) = 30. b -1 : has complex eigenvalues: 6 A=Y . has real eigenvalues but B = a 1 b 1 Find the conditions on a and b (real) so that all solutions of du/dt = Awu and dv/dt = Bv approach zero as t = 00: Re A < 0 for all eigenvalues. 7 Suppose P is the projection matrix onto the 45° line y = z in R2. What are its eigenvalues? If du/dt = — Pu (notice minus sign) can you find the limit of u(t) at t = oo starting from u(0) = (3,1)? 6.5. Solving Linear Differential Equations 281 10 1 12 The rabbit population shows fast growth (from 67) but loss to wolves (from —2w). The wolf population always grows in this model (—w? would control wolves): dr dw Z—Gr—2w and E—2r+w. Find the eigenvalues and eigenvectors. If 7(0) = w(0) = 30 what are the populations at time t? After a long time, what is the ratio of rabbits to wolves ? (a) Write (4,0) as a combination ¢;&; + cax of these two eigenvectors of A: 0 1}]1 _; 1 0 1f|1f_ .|1 1 of|i| =i -1 of |-i| = 7\" |-il\" (b) The solution to du/dt = Au starting from (4, 0) is cleit:cl + cze-itmg. Sub- stitute e'f = cost + isint and e \"* = cost — isint to find u(t). Find A to change y” = 5y’ + 4y into a vector equation for u = (y,y’) a=lvl=| )= y What are the eigenvalues of A? Find them also from y” = 5y’ + 4y withy = e’\". The solution to y”’ = 0 is a straight line y = C + Dt. Convert to a matrix equation : d 0 1] [y . At | ¥(0) o [;’,] = [O O] [y’] has the solution [g,] =e t[y’(O)]' This matrix A has A = 0,0 and it cannot be diagonalized. Find A% and compute eAt — 1 + At + 3 A%t? + .- -. Multiply your eAt times (y(0),4'(0)) to check the straight line y(t) = y(0) + y'(0)t. Substitute y = e into y” = 6y’ — 9y to show that A = 3 is a repeated root. This is trouble; we need a second solution after e3¢, The matrix equation is il = oly] Show that this matrix has A = 3, 3 and only one line of eigenvectors. Trouble here too. Check that y = te3! is a second solution to y” = 6y’ — 9y. Note In linear algebra the serious danger is a shortage of eigenvectors. Our eigenvectors (1, A1) and (1, A\\2) are the same if A} = A2. Then we can’t diagonalize A. In this case we don’t yet have two independent solutions to du/dt = Au. In differential equations the danger is also a repeated A\\. Aftery = e solution has to be found. It turns out to be y = )‘t, a second teAl. This “impure” solution (with an extra t) appears in the matrix exponential eAt, Example 7 showed how. 282 Chapter 6. Eigenvalues and Eigenvectors 13 (a) Write down two familiar functions that solve the equation d?y/ dt? = —9y. Which one starts with y(0) = 3 and y'(0) = 0? (b) This second-order equation y” = —9y produces a vector equation u’ = Au: ) g1 ) Find u(t) by using the eigenvalues and eigenvectors of A: u(0) = (3,0). 14 The matrix in this question is skew-symmetric (AT = — A): du 0 ¢ -b] u] = cug — bus - =1 0 a|lu or uy, = aug — Cu t | b —a 0 ug = bu; — aus. (a) Thederivative of ||u(t)||? = u+u2+u? is 2u;u)+2usul+2uzuf. Substitute u}. uh, uj to get zero. Then ||u(t)||? stays equal to ||u(0)]|°. (b) AT =—A makes Q= eAt orthogonal. Prove QT = e—At from the series for Q. 15 A particular solution to du/dt = Au—bisu, = A~'b, if Ais invertible. The usual solutions to du/dt = Au give u,. Find the complete solution u = u, + uy,: du du 1 0 4 O | Questions 16-22 are about the matrix exponential eAt, 16 Write five terms of the infinite series for eAt. Take the t denivative of each term. Show that you have four terms of AeAt, Conclusion: eAtuo solves u’' = Au. 17 The matrix B = |9 \"‘] has B2 = 0. Find Bt from a (short) infinite series. Check that the derivative of eBt is BeBt, 18 Starting from u(0) the solution at time T is eATu(O). Go an additional time ¢t to reach et eATu(O). This solution at time ¢ + T can also be written as AT equals Conclusion: el times e 19 If A2 = A show that the infinite series produces eAt =1+ (el — 1)A. 20 Generally e A.B # eBeA. They are both different from e+ B Check this for SR A RPN ] 6.5. Solving Linear Differential Equations 283 21 22 23 24 25 26 Put A = [} 3] into the infinite series to find eAAL, First compute A2 and A™: el gl ][5 ) (Recommended) Give two reasons why the matrix exponential eAt (a) Write down its inverse. (b) Why are its eigenvalues eAt is never singular : nonzero? Yos+1 — 2Y, + Y, 1 = —(At)?Y, can be written as a one-step difference equation : Yor1 =Y, + At Z, 1 0][Yarn] [1 At][Y, Zn+1 - Zn - At Yn+1 At 1 Zn+1 - 0 1 Zn Invert the matrix on the left side to write thisas U, 41 = AU ,,. Show that clet A=1. Choose the large time step At = 1 and find the eigenvalues A; and A, = A, of A: 1 1 Az[—l 0 ] has |A1| = |A2| = 1. Show that A® = I so ug = uo exactly. That leapfrog method in Problem 23 is very successful for small time steps At. But find the eigenvalues of A for At = /2 and 2. Any time step At > 2 will lead to |A| > 1, and the powers in U, = A\"U, will explode. 1 V2 1 2 borderline A—[—\\/i -1] and A_[—2 -3] unstable A very good idea for y”’ = —y is the trapezoidal method (half forward/half back). This may be the best way to keep (Yn, Z,,) exactly on a circle. . 1 A2 [Yau ] [ 1 Ay2][Ya Trapezoidal [At/2 { ][Zn:ll]—[-At/2 { ][Zn]. (a) Invert the left matrix to write this equation as U, 41 = AU,. Show that A is an orthogonal matrix: ATA = I. These points U,, never leave the circle. A = (I — B)~!(I + B) is always an orthogonal matrix if BT = —B. (b) (Optional MATLAB) Take 32 steps from U = (1, 0) to U3, with At = 27 /32. Is U3 = Ug? I think there is a small error. Explain one of these three proofs that the square of e4 is e24. 1. Solving with e fromt = 0to 1 and then 1 to 2 agrees with e24 from 0 to 2. 2. The squared series (I + A+ ATQ +-++)? matches I + 24 + g%): +.-=e24 3. If A can be diagonalized then (XeAX‘l)(XeAX‘l) = Xe2Ax-1, 284 Chapter 6. Eigenvalues and Eigenvectors Computing the Eigenvalues : The Q R Algorithm This page is about a very remarkable algorithm that finds the eigenvalues of A. The steps are easy to understand. Starting from A, we take a big step to a similar matrix Ay = B! AB (remember that similar matrices have the same eigenvalues). If Ay were a trian- gular matrix, then its eigenvalues would be along its main diagonal and the problem would be solved. This first step to Ag can get close to triangular, but we have to allow one extra nonzero diagonal. This is Step 1 and we omit the details : i a a2 - ‘ Qin | az a2 . a2n Ao = B-IAB = 0 aszz ° . a3n 0 0 . ! 0 0 * Qpn-1 Qnpn A is a Hessenberg matrix : one nonzero subdiagonal. All the steps to A;, Ao, ... will pro- duce Hessenberg matrices. The magic is to make that subdiagonal smaller and smaller. Then the diagonal entries of the matrices A,, A,, ... approach the eigenvalues of A. Ev- ery step has the form Ay, = B, 1A By, so all the A’s are similar matrices—which guarantees no change in the eigenvalues. The most beautiful and important matrices A are symmetric. In that case every step preserves the symmetry, provided B is an orthogonal matrix with Q! = QT : If AT = Athen A} = (Q7'4Q)T = (QTAQ)T = QTAQ =Q 'AQ = Ap. (22) Symmetry tells us : If there is only one nonzero subdiagonal, then there is only one nonzero superdiagonal. In other words, every step to Axy1 = B, YA By = Q;lAka will have only three nonzero diagonals. The computations are fast and so is the convergence. Now we are ready to explain the step from Ax to Ax41 = B ' Ay Bx. It begins with a Gram-Schmidt orthogonalization Ax = QxRx. Remember from Section 4.4 that Qy is orthogonal and Ry is upper triangular. To find A4, we reverse those factors: QRalgorithm [ A, = RQx = Q' ArQx (23) A +1 has the same eigenvalues as Ay (similar matrices). By equation (22), A4, is sym- metric if Ai is symmetric—because Qi is orthogonal. The magic (that we will justify only by an example) is that the subdiagonals of A;, A2, ... become smaller and smaller. They all have the same eigenvalues, which begin to appear quickly on the main diagonal. cos@ siné cosf® —siné 1 cos@sin@ Example Ao = [ sin 6 ] QR [ sinf cos@ ] [ 0 —sind ] Reversing the factors to RQ) produces A,, with the same eigenvalues as Ag. But look how the off-diagonal entries have droppcd from sin @ to — sin® 6 ; 1 cosHsmO —smH cos (1 + sin? 0) —sin® @ sm0 A =RQ = [ —sin% 6 —gin®@® —sin260cosf 6.5. Solving Linear Differential Equations 285 Check : The trace is still cos@ as in Ag. The determinant is still — sin? 8, But the off- diagonal term has dropped from sin @ in Ag to — sin® @ in A;. We are seeing cubic conver- gence (very rare)! The new matrix A, is nearly diagonal, and its last entry — sin® 8 cos 6 is very close to a true eigenvalue of A and A;. It helped that the (2,2) entry of Ap was zero. If not, we can always shift Ay by a matrix cl to achieve (Ag)22 = 0, and shift back at the end of the step. The “Q R algo- rithm with shifts” is a numerical success—very far from the original bad idea of computing the characteristic polynomial det(A — AI) and finding its roots. That is a failure. Computational linear algebra is a well-developed subject with excellent textbooks of its own. We mention two books here, and many more on the website. Numerical Linear Algebra, SIAM, 1997, L.N. Trefethen and David Bau And a comprehensive textbook of 756 valuable pages is Matrix Computations, 4th Edition, Golub and Charles van Loan, Johns Hopkins. Thoughts about Differential Equations Constant coefficient linear equations are the simplest to solve. This Section 6.5 shows you part of a differential equations course, but there is more. Here are two highlights: 1. The second order equation mu’ + bu’ + ku = 0 has major importance in ap- plications. The exponents X in the solutions u = e** solve mA? + bA + k = 0. Underdamping b2 <4mk Critical damping b2 =4mk Overdamping b° > 4mk This decides whether A; and A, are real roots or repeated roots or complex roots. With complex A =a + iw the solution u(t) oscillates from e** as it decays from e®‘. 2. Our equations had no forcing term f(t). We were finding the “nullspace solution”. To u,(t) we need to add a particular solution up(t) that balances the force f(t). This solution can also be discovered and studied by Laplace transform : Input f(s) at time s t Growth factor eA(t—*) Uparticular = / eAlt—2) f(s)ds. Add up outputs at time ¢ 0 In real applications, nonlinear differential equations are solved numerically. A method with good accuracy is “Runge-Kutta”. The constant solutions to du/dt = f(u) are u(t) = Y with f(Y) = 0. Then du/dt = 0: no movement. Far from Y, the computer takes over. This basic course is the subject of my textbook (a companion to this one) on Differential Equations and Linear Algebra. The website is math.mit.edu/dela. The individual sections of that book are described in a series of short videos, and a parallel series about numerical solutions was prepared by Cleve Moler : ocw.mit.edu/resources/res-18-009-learn-differential-equations-up-close-with-gilbert- strang-and-cleve-moler-fall-2015/ www.mathworks.com/academia/courseware/learn-differential-equations.html 7 The Singular Value Decomposition (SVD) 7.1 Singular Values and Singular Vectors 7.2 Image Processing by Linear Algebra 7.3 Principal Component Analysis (PCA by the SVD) This chapter develops one idea. That idea applies to every matrix, square or rectangular. It is an extension of eigenvectors. But now we need two sets of orthonormal vectors: input vectors v; to v,, and output vectors u; to u,,. This is completely natural for an m by n matnx. The vectors v, to v, are a basis for the row space; u; to u, are a basis for the column space. Then we recover A from r pieces of rank one, with r = rank(A) and positive singular values o7 > 02 2> --- > o, > 0 in the diagonal matrix X. SVD A=UZVT =g,u1v] + 0ouov] +--- + o,u, v} r L] The right singular vectors v; are eigenvectors of AT A. They give bases for the row space and nullspace of A. The left singular vectors u; are eigenvectors of AAT. They give bases for the column space and left nullspace of A. Then Av; equals o;u; for 1 < r. The matrix A is diagonalized by these two orthogonal bases: AV = UX. Note that the nonzero columns of UX are o u;,...,0,u,. Then UL times VT in the box above is a column times row multiplication—parallel to S = QAQ™ for symmetric S. Each u; = v; when A is a symmetric positive definite matrix S. Those singular vectors will be the eigenvectors q,. And the singular values o; become the eigenvalues of S. If A is not square or not symmetric, then we need A = ULV instead of S = QAQT. Figure 7.1 will show how each step from z to VTz to VT to ULV Te = Az acts on a circle of unit vectors . They stretch the circle into an ellipse of vectors Ax. The SVD is a valuable way to understand a matrix of data. In that case AAT is the sample covariance matrix, after centering the data and dividing by n — 1 (Section 7.3). Its eigenvalues are o? to o2. Its eigenvectors are the u's in the SVD. Principal Component Analysis (PCA) is totally based on the singular vectors of the data matrix A. The SVD allows wonderful projects, by separating a photograph = matrix of pixels into its rank-one components. Each time you include one more piece a,-u,-v;r, the picture becomes clearer. Section 7.2 shows examples and a link to an excellent website. Section 7.3 describes PCA and its connection to the covariance matrix in statistics. 7.1. Singular Values and Singular Vectors 287 7.1 Singular Values and Singular Vectors mhe Singular Value Decomposition of any matrixis A = UV T or AV = Ua 2 Singular vectors v;, u; in Av; = o;u; are orthonormal : VIVv=IlandUTU=1. 3 The diagonal matrix ¥ contains the singular valuesgy > g2 > --- > o, > 0. {The squares o2 of those singular values are eigenvalues A; of AT A and AAT j The eigenvectors of symmetric matrices are orthogonal. We want to go beyond symmetric matrices to all matrices—without giving up orthogonality. To make this possible for every m by n matrix we need one set of n orthonormal vectors vy, ..., v, In R\" and a second orthonormal set u1, ..., u,, in R™. Instead of Sz = Ax we want Av = ou. This unsymmetric matrix A has orthogonal inputs v,,v2 = (1,1) and (—-1,1): 5 4 (| 1 9 5 4 || -1 -1 £ F RS L) I e T A The outputs (9, 3) and (—1, 3) are also orthogonal ! Those v’s and u’s are not unit vectors but that is easily fixed. (1,1) and (—1, 1) need to be divided by v2.(3,1)and (-1, 3) need to be divided by v10to give u; and u;. The singular values o4, o2 are 3v5and V5: 5 4 S5 4 IAv=a'u [0 3]‘01—3\\/3141 and [0 3]v2—\\/5u2. (2) We can move from these two vector formulas to one matrix formula AV = UX. The inputs v, and v- go into V. The outputs o34 and oou areinUE,withﬁzi. P 1 28 P 1U1 2U2 \\/-1_6 \\/5 w=[33][3 2] [3][ ]es A V2 V10 ) V and U are orthogonal matrices ! If we multiply AV = UX by VT, A will be alone: AV =UX becomes IA = U2VT|. This splits into A = alulv'lr + 0’2‘!421)2T 4) Those SVD equations say everything except how to find the orthogonal v’s and u’s. Every matrix A is diagonalized by two sets of singular vectors, not one set of eigenvectors. In this 2 by 2 example, the first piece is more important than the second piece because oy = 3V5 is greater than oy = V5. To recover A, add the pieces alulvf + 0’202‘0:{2 o T CH ] Rt HH S I T 288 Chapter 7. The Singular Value Decomposition (SVD) The Geometry of the SVD The SVD separates amatrixinto A =UX VT : (orthogonal) x (diagonal) x (orthogonal). In two dimensions we can draw those steps. The orthogonal matrices U and V rotate the plane. The diagonal matrix X stretches it along the axes. Figure 7.1 shows rotation times stretching times rotation. Vectors v on the circle go to Av = ou on an ellipse. P . L -_---- ------ - - - - - - - - - - - - — haso; = 3andog =1 1. z V5 2 vT ) ~~ ™\\ ~~ 1)) U1 E “E O = 1 Rotateby VT Stretch by ¥ Rotate by U Figure 7.1: U and V are rotations and possible reflections. X stretches circle to ellipse. This picture applies to a 2 by 2 invertible matrix with o; > 0 and g2 > 0. First is a rotation of any z to VTz. Then I stretches that vector to SV Tx. Then U rotates to ULVTz. We kept all determinants positive to avoid reflections. The four numbers a.b. c.d in the matrix connect to two rotation angles 0, ¢ and two numbers o1.03 in L. 4-1]@ b | | cosf@ -—sind o1 cos¢ sin ¢ (5) | e d| | sin@ cosf 09 —sing cos¢ | Question 1 If the matrix is symmetric then b = ¢. Now A has 3 (not 4) parameters. How do the 4 numbers 0, ¢, 01, 02 reduce to 3 numbers for a symmetric matrix? Question2 If0 = 30°and o, = 2and 03 = 1 and ¢ = 60°, what is A ? Check det = 2. Ao v3/2 -1/2 || 2 1/2 v3/2| 1| 33 5 1/2 V3/2 1 || -v3/2 172 | -1 3V3 4 Question 3 If A is 3 by 3 (9 parameters) then ¥ has 3 singular values. U and V have 3 rotations each to make 3 + 3 + 3 = 9 numbers. What are those 3 rotations for a pilot ? (6) Answer: An airplane can “pitch” and “roll” and “yaw”. (See Wolfram EulerAngles). Question4 If S = QAQT is symmetric positive definite, what is its SVD ? Answer: The SVD is exactly UEVT = QAQT. The matrix U = V = Q is orthogonal. And the diagonal eigenvalue matrix A becomes the singular value matrix X. Question 5 If S = QAQT has a negative eigenvalue (Sx = —a ), what is the singular value o and what are the vectors v and u ? Answer : The singular value will be & = 4a (positive). One singular vector (either u or v) must be —x (reverse the sign). Then Sz = —ax is the same as Sv = ou. The two sign changes cancel. 7.1. Singular Values and Singular Vectors 289 The Full Size Form of the SVD The full picture includes basis vectors for the nullspaces of A and AT. Those vectors v, 41 to v, and u,4; to u,, complete the orthogonal matrices V (n by n) and U (m by m). Then the matrix X is (m by n) like A. But X is all zero except for the r singular values g1 > 02 2 -+ 2 0, > 0 on its main diagonal. Those ¢’s multiply the vectors u; to u,. Full size SVD _ 1 . r AV =UX o1 (ﬂ (mxn)(nxn)= Alvger v, VrpreeVn|=|ugety vppye-um '-a o (m x m) (m x n) \" VT=V—1 UT=U—1 L 4 - ;.0 0 O.J When we multiply on the right by VT = V-1, this equation AV = UX tumns into the most famous form A = UXVT of the Singular Value Decomposition. The matrices V and U are square—with bases for row space + nullspace, column space + N(AT). The Reduced Form of the SVD That full form AV = UX can have a lot of zeros in ¥ when the rank of A is small and the nullspace of A is large. Those zeros contribute nothing to matrix multiplication. The heart of the SVD is in the first r v’s and u’s and o’s. We can change AV = UX to AV, = U, X, by removing the parts that are sure to produce zeros. This leaves the reduced SVD where X,. is now square: (m xn)(n xr)=(m x r) (r X r). Reduced SVD i | ( 17 o ] AV, =U,%X, Al vy .. v |=] u1 .. u, (7 Av; = o;u4 | row space | _ column space | | Or | We still have V,T Vr = Iy and Ug Uy = Iy from those orthogonal unit vectors v’s and u’s. When V3 and Uy are not square, we can’t have full inverses: Vyp V,'.r # I and Up U,T # 1. But A = U,.X, VT = u;00v] + - + uropv} is true. I prefer this reduced form, because the other multiplications in the full size form A = UZVT give only zeros. Example 1 Our 2 by 2 matrix had r = m = n. The reduced form was the full form. Example2 A=[1 2 2]=[1][3][1 2 2]/3= U.L,V. hasr = 1and 0, = 3. The rest of UL VT contributes nothing to A, because of all the zeros in £. The key separation of A into oyuv] + +-- + o,u,v} stops at o;u,v] because the rank is 1. 290 Chapter 7. The Singular Value Decomposition (SVD) Proof of the SVD The goal is A = ULV T. We want to identify the two sets of singular vectors, the u’s and the v’s. One way to find those vectors is to form the symmetric matrices AT A and AAT : ATA = (vZTUT) (UEVT) = VETEVT (because UTU = I) (8) AAT = (UzvT) (vETUT) = UEEZTUT (because VTV = 1) 9) Both (8) and (9) produced symmetric matrices. Usually AT A and AAT are different. Both right hand sides have the special form QAQT. Eigenvectors of AT A and AAT are the singular vectors in V and U. So we know from (8) and (9) how the three pieces of A= UZVT connect to those symmetric matrices AT A and AAT. V contains orthonormal eigenvectors of AT A U contains orthonormal eigenvectors of AAT o3 to o2 are the nonzero eigenvalues of both AT Aand AAT We are not quite finished, for this reason. The SVD requires that Av, = o ux. It connects each right singular vector v, to a left singular vector ux, for k = 1,...,7. When I choose the v’s, that choice will decide the signs of the u’s. If Su = Au then also S(—u) = A(—u) and I have to know the sign to choose. More than that, there is a whole plane of eigenvectors when A is a double eigenvalue. When I choose two v’s in that plane, then Av = owu will tell me both u’s. The plan is to start with the v’s. Choose orthonormal eigenvectors v,,...,v, of ATA. Then choose o, = v k. To determine the u’s we require that Av = ou: v’s then u’s ATAv, =o02v, andthen uy = % for k=1,...,7|(10) k This produces the SVD ! Let me check that those vectors u; to u, are eigenvectors of AAT. T 2 AATy, =AAT(ﬁ) =A(A A\"\") —AZ% o2y (D) Ok Ok The v’s were chosen to be orthonormal. I must check that the «’s are also orthonormal: Av:\\T [ Av vT(ATAvk) o 1 if 9=k U; U ( 0; ) ( Ok ) 0;j Ok o; vj Yk { 0 if j#k (12) Notice that (AAT)A = A(ATA) was the key to equation (11). The law (AB)C = A(BC) is the key to a great many proofs in linear algebra. Moving those parentheses is a powerful idea. It is permitted by the associative law. It completes the proof. 7.1. Singular Values and Singular Vectors 291 0 3 With rank 2, this A has two positive singular values o; and o,. We will see that o is larger than Amax = 5, and o3 is smaller than Apin = 3. Begin with AT A and AAT : 25 20 41 12 20 25 12 9 Example1 (completingnow) Find U and X and V for our original A = [ > 4 ] : ATA = [ AAT=[ Those have the same trace A\\; + A2 = 50 and the same eigenvalues A\\; = af = 45 and Ay = or% = 5. The square roots are oy = V45 = 3v/5 and 03 = v/5. Then o, times 0> equals 15, and this is the determinant of A. The next step is to find V. The key to V is to find the eigenvectors of AT A (with eigenvalues 45 and 5) : 25 20 1 1 20 20 -1 -1 BRI FY N 1 I el Y Then v; and v, are those orthogonal eigenvectors rescaled to length 1. Divide by V2. . . 111 1 ]-1 . Right singular vectors v, = % [1] and vy = E[ 1] (as predicted) The left singular vectors are u; = Av;/0; and u; = Av,y/0,. Multiply v;, v2 by A: 3 [1 1 [1 A'U] = 75' 3] = \\/‘-13—[3] = o1 u; 1 [ -3 1 [ -3 o = 5[] = Al H] - e The division by v/10 makes u; and u; unit vectors. Then 0; = V45 and 0, = V5 as expected. The Singular Value Decomposition of A is U times X times VT. NotV.) s Vel @ v-7sls 1) =70 Finally we have to choose the last n — r vectors v, to v, and the last m — r vec- tors %,4+1 to U,,. This is easy. These v’s and u’s are in the nullspaces of A and AT, We can choose any orthonormal bases for those nullspaces. They will automatically be orthogonal to the first v’s in the row space of A and the first u’s in the column space. This is because the whole spaces are orthogonal: N(4) L C(AT) and N(AT) L C(A). The proof of the SVD is complete by that Fundamental Theorem of Linear Algebra. To say again: Good codes do not start with ATA and AAT. Instead we first produce zeros in A by rotations that leave only two diagonals (and don’t affect the o°’s). The last page of this section describes a successful way to compute the SVD. 292 Chapter 7. The Singular Value Decomposition (SVD) Now we have U and V' and X in the full size SVD of equation (1): m u’s and n v’s. You may have noticed that the eigenvalues of ATA are in £TE, and the same numbers of 10 o? are also eigenvalues of AAT in £XT. An amazing fact: BA always has the same nonzero eigenvalues as AB. BAis nby n and AB is m by m. AB and BA: Equal Nonzero Eigenvalues If Ais m by n and B is n by m, AB and B A have the same nonzero eigenvalues. Start with ABx = Az and A # 0. Multiply both sides by B, to get BABx = ABz. This says that Bz is an eigenvector of BA with the same eigenvalue A—exactly what we wanted. We needed \\ # 0 to be sure that Bz is truly a nonzero eigenvector of BA. Notice that if B is square and invertible, then B~!(BA)B = AB. This says that BA is similar to AB : same eigenvalues. But our first proof allows A and B to be m by n and n by m. This covers the important example of the SVD when B = AT, In that case AT A and AAT both lead to the r nonzero singular values of A. If m is larger than n, then AB has m — n extra zero eigenvalues compared to BA. Singular Vectors for 2 by 2 Matrices Here is a picture proof of the 2 by 2 SVD. We look for perpendicular vectors v and w so that Av and Aw are also perpendicular. In our first guess V = (1,0) and W = (0,1) the angle 6 from AV to AW is too small (below 90° in the first figure). Then the angle 180 — 0 between AW and — AV is too large (above 90° in the second figure). Therefore there must be a v between V and W that is just right. The angle from Av to Aw is exactly 90° and the SVD is proved : Orthogonal inputs v_L w, orthogonal outputs Av 1 Aw. 144 A(-V) Figure 7.2: Angle 8 from AV to AW is below 90°. Angle 180 — 6 from AW to —AV is above 90°. Somewhere in between, as v moves from V toward W, the angle from Av to Aw is exactly 90°. The pictures don’t show vector lengths. The next page will establish a new way to look at v;. The previous pages chose the v’s as eigenvectors of ATA. Certainly that remains true. But there is a valuable way to understand these singular vectors one at a time instead of all at once. 7.1. Singular Values and Singular Vectors 293 The First Singular Vector v, Maximize the ratio . The maximum is o, at the vector x = v;. (14) The ellipse in Figure 7.1 showed why the maximizing & is v,. When you follow v, across the page, it ends at Av; = o;u;. The longer axis of the ellipse has length ||Av, || = 0. But we aim for an independent approach to the SVD ! We are not assuming that we already know U or X or V. How do we recognize that the ratio ||Az||/||x|| is a maximum when £ = v;? One easy way is to square our function and work with § = ATA: |Az|? «TATAz «7Scx lz|? Tz Tz’ Problem: Find the maximum value A of (15) To maximize this “Rayleigh quotient”, write any T as ¢; ++ + - + ¢, &, With Sx; = \\;x; : TS M+ +EM eTe A+ +c2 < A; = largest eigenvalue of S. (16) Then the best = to maximize the ratio in (15) is an eigenvector of S ! c'Sx ||Ax|? =Tz ||z|? Sz = Ax and the maximum of is \\1(S) = o2(A). For the full SVD, we need all the singular vectors and singular values. To find v, and o9, we adjust the maximum problem so it looks only at vectors & orthogonal to v;. .. ||AT \" : : Maximize ”” ”” under the condition v'lrz = 0. The maximumis o3 at r = v,. T “Lagrange multipliers” were invented to deal with constraints on x like viz = 0. And Problem 9 gives a simple direct way to work with this condition v] = 0. Every singular vector vy4; gives the maximum ratio ||Az||/||x|| over all vectors = that are perpendicular to the first vy,...,vx. We are finding the axes of an ellipsoid and the eigenvectors of symmetric matrices AT A or AAT : all at once or separately. Question: Why are all eigenvalues of a square matrix A less than or equal to o, ? Answer : Multiplying by orthogonal matrices U and VT does not change vector lengths : |Az|| = |UZV 2| = [[EVTz|| < oul[VTl| = oullz|| forallz. (17) An eigenvector has ||Az|| = |A|||z||. Then (17) gives |A| ||z|| < o1 ||z|| and |A| < &4. 294 Chapter 7. The Singular Value Decomposition (SVD) Question: If A = zy\" has rank 1, what are u; and v; and o, ? Check that [\\;| < o;. Answer : The singular vectors u; = x/||x|| and v; = y/||y|| have length 1. Then o} = l|z|| ||y|| is the only nonzero number in the singular value matrix 2 Here is the SVD: ” “ (” ””yll)m ulo'lv'lr- (18) Observation The only nonzero eigenvalue of A = zy! is A\\; = yTx. The eigenvector is T because Az = (zy’)z = = (y'x) = A\\ . Then the inequality |\\;| < o1 becomes exactly the Schwarz inequality [y Tz| < ||z|| ||y]|. Rank 1 matrix A=zy! = Computing Eigenvalues and Singular Values What is the main difference between the symmetric eigenvalue problem Sx = Az and Av = ou? How much can we simplify S and A before computing \\’s and o’s ? Eigenvalues are the same for S and Q~!SQ = QTSQ when Q is orthogonal. So we have limited freedom to create zeros in Q~'SQ (which stays symmetric). If we try for too many zeros in Q~'S, the final Q will destroy them. The good @~ 'SQ will be tridiagonal : we can reduce S to three nonzero diagonals. Singular values are the same for A and QI—IAQ2 even if Q, is different from Q). We have more freedom to create zeros in Q7 ' AQ,. With the right Q’s, this will be bidiagonal (two nonzero diagonals). We can quickly find Q and Q; and Q2 so that ‘a; b 1+ for X’s ( c1 d, - b b - 0 d Q ISQ — 1 Z': .2 . Ql 1AQ2 — C02 .2 . (19) i an J foro’'s - | 0 cn | The reader will know that the singular values of A are the square roots of the eigenvalues of S = AT A. And the singular values of Q7 ' AQ, are the same as the singular values of A. Multiply (bidiagonal) (bidiagonal) to see tridiagonal. This offers an option that we should not take. Don’t multiply AT A and find its eigenvalues. This is unnecessary work and the condition of the problem will be unneces- sanly squared. The Golub-Kahan algorithm for the SVD works directly on A, in two steps : 1. Find @Q; and Q5 so that A* = Ql-lAQz is bidiagonal as in (19) : same o’s 2. Adjust the shifted QR algorithm (Section 6.5, page 284) to preserve singular values of A* Step 1 requires O(mn?) multiplications to put an m by n matrix A into bidiagonal form. Then later steps will work only with bidiagonal matrices. Normally it then takes O(n?) multiplications to find singular values (correct to nearly machine precision). The full algorithm is described on pages 489492 in the 4th edition of Golub-Van Loan : the bible. When A is truly large, we turn to random sampling. With very high probability, randomized linear algebra gives accurate results. Most gamblers would say that a good outcome from careful random sampling is certain. 7.1. Singular Values and Singular Vectors 295 Problem Set 7.1 1 Find AT A and AAT and the singular vectors v, v, u;, u; for A: 0 1 0 0 0 A= has rank r = 2. The eigenvalues are 0, 0, 0. 0 0 8 0 Check the equations Av; = o;u; and Avy; = guz and A = alulv'f + agugv;r : If you remove row 3 of A (all zeros), show that g; and o2 don’t change. 2 Find the singular values and also the eigenvalues of B : 0 1 0] g B = 0 0 8 has rank r = 3 and determinant ——. . 1000 d Compared to A above, eigenvalues have changed much more than singular values. 3 The SVD connects v’s in the row space to u’s in the column space. Transpose A = UZVT to see that AT = VETUT goes the opposite way, from u’s to v’s: ATy = opv fork=1,....,r ATy, =0fork=r+1,...,m What are the left singular vectors u and right singular vectors v for the transpose [5 0 ; 4 3] of our example matrix ? 4 When Av, = &kuk and ATuy =0V, show that S has eigenvalues ox and —oy : S = [ /fT g ] has eigenvectors [ :: } and [ —:: J and trace = 0. The eigenvectors of this symmetric matrix S tell us the singular vectors of A. 5 Find the eigenvalues and the singular values of this 2 by 2 matrix A. 21 . T, |20 10 r [ 5 10 A—[4 2] with AA-[IO 5] and AA ‘[10 20 |- The eigenvectors (1, 2) and (1, —2) of A are not orthogonal. How do you know the eigenvectors vy, v2 of ATA will be orthogonal ? Notice that ATA and AAT have the same eigenvalues A\\; = 25 and A2 = 0. 6 FindtheSVDfactorsUandEandVTforAl=[i ?}andA«z:[i (1) : (I)J 7 The MATLAB commands A = rand (20, 40) and B = randn (20, 40) produce 20 by 40 random matrices. The entries of A are between 0 and 1 with uniform probability. The entries of B have a normal “bell-shaped™ probability distribution. Using an svd command, find and graph their singular values o, to 02¢9. Why do they have 20 o’s ? 296 10 1 12 13 14 15 16 17 18 Chapter 7. The Singular Value Decomposition (SVD) Why doesn’t the SVD for A+ Tuse Z + I? If A = Y o,u;v] why is At = S vul/o;? If A = Q is an orthogonal matrix, why does every singular value of () equal 1? A symmetric matrix S = AT A has eigenvalues A; > 0to A, > 0. Its eigenvectors v; to v,, are orthonormal. Then any vector has the form x = c,v; + --- + c,v,,. TS M+ + M Tz S+ +c2 R(z) = Which vector x gives the maximum of R? What are the numbers ¢, to c,, for that maximizing vector £ ? Which & gives the minimum of R ? To find 02 and v, from maximizing that ratio R(x), we must rule out the first singular vector v, by requiring £Tv; = 0. What does this mean for ¢; ? Which ¢’s give the new maximum o2 of R(x) at the second eigenvector x = v, ? Find AT A and the singular vectors in Av; = o1u; and Avy = ous: 2 2 3 3 A—[_l 1] and A—[4 4]. For this rectangular matrix find v,,v2,v3 and u;,u2 and 0,,05. Then write the SVDfor Aas ULVT = (2x2) (2 x 3)(3 x 3). 110 A=lo1 1] If (AT A)v = 0%v, multiply by A. Move the parentheses to get (AAT) Av = o2 (Av). If v is an eigenvector of AT A, then _____is an eigenvector of AAT, Find the eigenvalues and unit eigenvectors v,, v of AT A. Then find u; = Avy/o;: |1 2 t,_ |10 20 T _| 9 15 A—[3 6] and A A_[2O 401 and AA —[15 45]. Verify that u, is a unit eigenvector of AAT. Complete the matrices U , 5, V. 1 2 o T SVD [36]=[u1 uz][lo][vl ‘vg]. (a) Why is the trace of AT A equal to the sum of all aZ; ? (b) For every rank-one matrix, why is 02 = sum of all afj? If A = UZVT is a square invertible matrix then A~! = The largest singular value of A™! is therefore 1/0min(A). The largest eigenvalue has size 1/|A\\(A)|min. Then equation (17) says that o min(A) < |A(A)|min. Suppose A = UTVT is 2 by 2 with g; > g2 > 0. Change A by as small a matrix as possible to produce a singular matrix Ag. Hint: U and V' do not change. 7.2. Image Processing by Linear Algebra 297 7.2 Image Processing by Linear Algebra /l An image is a large matrix of grayscale values, one for each pixel and color. \\ 2 When nearby pixels are correlated (not random) the image can be compressed. Q Flags often give simple images. Photographs can be compressed by the SVD. / Image processing and compression are major consumers of linear algebra. Recognizing images often uses convolutional neural nets in deep learning. These topics in Chapters 7-8 represent part of the enormous intersection of computational engineering and mathematics. This section will begin with stylized images like flags: often with low complexity. Then we move to photographs with many more pixels. In all cases we want efficient ways to process and transmit signals. The image is effectively a large matrix (!) that can represent light/dark and red/green/blue for every small pixel. The SVD offers one approach to matrix approximation: Replace A by Ax. The sum A of r rank one matrices o; ujv;r can be reduced to the sum A of the first k terms. This section (plus online help) will consider the effect on an image such as a flag or a photograph. Section 7.3 will explore more examples in which we need to approximate and understand a matrix of data. Start with flags. More than 30 countries chose flags with three stripes. Those flags have a particularly simple form: easy to compress. I found a book called “Flags of the World™ and the pictures range from one solid color (Libya’s flag was entirely green duning the Gaddafi years) to very complicated images. How would those pictures be compressed with minimum loss ? The linear algebra answer is: Use the SVD. Notice that 3 stripes still produce rank 1. France has blue-white-red vertical stripes and b w r in its columns. By coincidence the German flag is nearly its transpose with the colors Black-White-Red : bbwwrr] [1][bbwwrr] ‘\"BBBBBB]| [B][111111] bbwwrr 1 BBBBBB B bbwwrr 1 WWWWWW W bbwwrr | |1| France wwwwww|~|w| CGermany bbwwrr 1 RRRRRR R bbwwrr| |1 RRRRRR| |R Each matrix reduces to two vectors. To transmit those images we can replace N pixels by 2N pixels. Similarly, Italy is green-white-red and Iceland is green-white-orange. But many many countries make the problem infinitely more difficult by adding a small badge on top of those stripes. Japan has a red sun on a white background and the Maldives have an elegant white moon on a green rectangle on a white rectangle. Those curved images have infinite rank—compression is still possible and necessary, but not to rank one. 298 Chapter 7. The Singular Value Decomposition (SVD) A few flags stay with finite rank but they add a cross to increase the rank. Here are two flags (Greece and Tonga) with rank 3 and rank 4. T Greece Tonga [ see four different rows in the Greek flag, but only three columns. Mistakenly, I thought the rank was 4. But now I think that row 2 + row 3 = row 1 and the rank of Greece is 3. On the other hand, Tonga’s flag does seem to have rank 4. The left half has four rows: all white-short red-longer red-all red. We can’t produce any row from a linear combination of the other rows. The island kingdom of Tonga has the champion flag of finite rank! Singular Values with Diagonals Three countries have flags with only two diagonal lines : Bahamas, Czech Republic, and Kuwait. Many countries have added in stars and multiple diagonals. From my book I can’t be sure whether Canada also has small curves. It is interesting to find the SVD of this matrix with lower triangular 1’s—including the main diagonal—and upper triangular 0’s. 1 0 0 0] 1 0 0 0] Flag with 1100 4 | -1 1 0 0 atriangle -1 110 P4 =] o 1 1 o 1111 0 0 -1 1 A has full rank 7 = N. All eigenvalues are 1, on the main diagonal. Then A has N singular values (all positive, but not equal to 1). The SVD will produce n pieces o; u; v} of rank one. Perfect reproduction needs all n pieces. In compression the small o’s can be discarded with no serious loss in image quality. We want to understand the singular values for n = 4 and also to plot all ¢’s for large n. The graph on the next page will decide if A is greatly compressed by the SVD (no). Working by hand, we begin with AAT (a computer would proceed differently): 1 1 1 1] 2 -1 0 0] 1 2 2 2 -1 2 -1 0O T _ T\\-1 _ -1I\\T 4-1 _ : AA_1233and(AA)—(A)A— 0 1 2_1.(1) 123 4 0 0 -1 1 That —1,2,—1 inverse matrix is included because all its eigenvalues have the form 2 — 2 cosf. We know those eigenvalues! So we know the singular values of A. 7.2. Image Processing by Linear Algebra 299 - ! gives o(A)=vVA= ! NA4T) = 2—2cosf 4sin%(6/2) 2sin(0/2) 2) The n different angles 0 are equally spaced, which makes this example so exceptional: o 3 T oam+1'2n4+1\"\" (2n-1)«w 4 2n+1 (n = 4 includes 8 = 3—91 with 28ing = l) . The important point is to graph the n singular values of A. Those numbers drop off (unlike the eigenvalues of A, which are all 1). But the dropoff is not steep. So the SVD gives only moderate compression of this triangular flag. Great compression for Hilbert. 102 : — . 10° v , - | Singular values of tril(ones (40)) Singular values of hilb (40) . All larger than 1/2 100} Very steep dropoff 4 10\"} b 10.5 [ ) ) .. .. .‘. 10| .0 %0 10 o 100 . ......... T ) ....°‘°°0000.ooooooo ° 105} * 0000.00000............ g L ‘ . 20 . . ’ %% 10 20 30 0w % 10 20 30 40 Figure 7.3: Singular values of the 40 by 40 tniangle of 1’s (it is not compressible). The evil Hilbert matrix H(i,5) = (i +j —1)~! has low effective rank : we must compress it. The striking point about the graph is that the singular values for the triangular matrix never go below % Working with Alex Townsend, we have seen this phenomenon for 0-1 matrices with the 1’s in other shapes (such as circular). This has not yet been explained. Image Compression by the SVD Compressing photographs and images is an exceptional way to see the SVD in action. The action comes by varying the number of rank one pieces cuv? in the display. By keeping more terms the image improves. We hoped to find a website that would show this improvement. By good fortune, Tim Baumann has achieved exactly what we hoped for. He has given all of us permission to use his work : https://timbaumann.info/svd-image-compression-demo/ The next page shows an earlier version of his website. 300 Chapter 7. The Singular Value Decomposition (SVD) Uncompressed image. Slider at 300. IMAGE SIZE 600 x 600 #PIXELS = 360000 UNCOMPRESSED SIZE proportional to number of pixels COMPRESSED SIZE approximately proportional to 600 x 300 + 300 + 300 x 600 = 360300 COMPRESSION RATIO 360000,/360300 = 1.00 Show singular values O |'I'I'I'I'|'|'I'I'I'I'I'I'I'I'|'I'I'I'|'I|I'I'I'I'I|I'I'I'I'I|I'|'|'I'I'|'I'I'I'l'| 10 20 100 200 300 400 500 600 Compressed image. Slhder at 20. IMAGE SIZE 600 x 600 #PIXELS = 360000 UNCOMPRESSED SIZE proportional to number of pixels COMPRESSED SIZE approximately proportional to 600 x 20 + 20 + 20 x 600 = 24020 COMPRESSION RATIO 360000,/24020 = 14.99 Show singular values O I I B A 'll(;\"'ll\"\"'2|0.|'lll.l'l'“'”;&;'“l'“2'&')'“““3'&';“““4.&';“““l;&)““““ Change the number of singular values using the slider. Click on one of these images to compress it: { | - r # 3 . V. /8 . \\ R/ { 4 o ' y 1Y {48 f 11\" ' l,‘ ) { ’ | PN SIS \"y ‘ g . . >3 Y . . - s « b ’ \" ] ' v ’ » . - . g 4 3 You can compress your own images by using the [ﬁle pickeﬂ or by dropping them on this page. 7.2. Image Processing by Linear Algebra 301 This is one of the five images directly available. The position of the slider determines the compression ratio. The best ratio depends on the complexity of the image—the girl and the Mondrian painting are less complex and allow higher compression than the city or the tree or the cats. When the computation of compressed size adds 600 x 80 + 80 + 80 x 600, we have 80 terms cuv ! with vectors u and v of dimension 600. The slider is set at 80. You can compress your own images by using the “file picker” button below the six sample images provided on the site, or by dropping them directly onto the page. One beautiful feature of Tim Baumann’s site is that it operates in the browser, with instant results. This book’s website math.mit.edu/linearalgebra can include ideas from readers. Please see that edited site for questions and comments and suggestions. Problem Set 7.2 1 We usually think that the identity matrix I is as simple as possible. But why is [ difficult to compress ? Create the matrix for a rank 2 flag with a horizontal-vertical cross. This is highly compressible. 2 These flags have rank 2. Write A and B in any way as u,v; + uov.. 1 2 1 1] 2 ASweden - AFinland = 2 2 2 2 BBenin = [ 1 3 } 1211 3 Now find the trace and determinant of BBT and also BTB in Problem 2. The singular values of B are close to 0 = 28 — 7 and 03 = 4. Is B compressible or not? W N 4 Use [U, S, V] = svd (A) to find two orthogonal pieces cuv’ of Asweden- 5 A matrix for the Japanese flag has a circle of ones surrounded by all zeros. Suppose the center line of the circle (the diameter) has 2N ones. Then the circle will contain about 7N ones. We think of the flag as a 1-0 matrix. Its rank will be approximately C N, proportional to N. What is that number C ? Hint: Remove a big square submatrix of ones, with corners at £45° and £135°. The rows above the square and the columns to the right of the square are independent. Draw a picture and estimate the number cN of those rows. Then C = 2c. 6 Here is one way to start with a function F(z,y) and construct a matrix A. Set Ai; = F(¢/N,3j/N). (The indices i and j go from 0 to N or from —N to N.) The rank of A as NV increases should reflect the simplicity or complexity of F. Find the ranks of the matrices for the functions F} = zy and F» = ¢ + y and F3 = x2 4 y3. Then find three singular values and singular vectors for F3. 7 In Problem 6, what conditions on F(z,y) will produce a symmetric matrix S ? An antisymmetric matrix A ? A singular matrix M ? A matrix of rank 2 ? 302 Chapter 7. The Singular Value Decomposition (SVD) 7.3 Principal Component Analysis (PCA by the SVD) The “principal components” of A are its singular vectors, the orthogonal columns u; and v; of the matrices U and V. This section aims to apply the Singular Value Decomposi- tion A = UZVT. Principal Component Analysis (PCA) uses the largest o’s connected to the first u’s and v’s to understand the information in a matrix of data. We are given a matrix A, and we extract its most important part Ax (largest o’s): Ar = o1uyv] + .-+ opurv, withrank (4;) = k. A solves a matrix optimization problem—and we start there. The closest rank k matrix to A is A,. In statistics we are identifying the rank one pieces of A with largest variance. This puts the SVD at the center of data science. In that world, PCA is “unsupervised” learning. Our only instructor is linear algebra— the SVD tells us to choose A;x. When the learning is supervised, we have a big set of training data. Deep Learning constructs a (nonlinear!) function F' to correctly classify most of that data. Then we apply this F' to new data, as you will see in Chapter 10. Principal Component Analysis is based on matrix approximation by Ax. The proof that A; is the best choice was begun by Schmidt (1907). He wrote about operators in function space; his ideas extend directly to matrices. Eckart and Young gave a new proof (using the Frobenius norm to measure A — A). Then Mirsky allowed any norm || A|| that depends only on the singular values—as in the definitions (2), (3), and (4) below. Here is that key property of the special rank k matrix Ay = oyu v +-- -+ OrUKV} : Ai isclosestto A If B hasrank k then ||[A — Ax|| < |[|A— BJ|. | (1) Three choices for the matrix norm || A|| have special importance and their own names: ||Az|| Spectral norm ||A||2 = max T o1 (often called the £2 norm) (2) Frobeniusnorm ||A||r = \\/af + -+ 02 (7) also defines || A||F (3) Nuclear norm ||A||N =01+ 02+ -+ 0, (the trace norm) (4) These norms have different values already for the n by n identity matrix : =1 |Illr=vn |||v=n. (5) Replace I by any orthogonal matrix Q and the norms stay the same (because all o; = 1): IRl2=1 |IRlr=vn ||IQlln =n. (6) More than this, the spectral and Frobenius and nuclear norms of any matrix stay the same when A is multiplied (on either side) by an orthogonal matrix. So the norm of A=UZVTequals the nomof £:||A|| =||X]|| because U and V are orthogonal matrices. 7.3. Principal Component Analysis (PCA by the SVD) 303 Norm of a Matrix We need a way to measure the size of a vector or a matrix. For a vector, the most important norm is the usual length ||v||. For a matrix, Frobenius extended the sum of squares idea to include all the entries in A. This norm || A||r is also named Hilbert-Schmidt. lolP=v?+---+v2 |AE=al)+ - -+al,+ - -+ai+---+ad, D Clearly [[v]| > 0 and ||cv|| = |c] ||v]l. Similarly [|Alr > 0 and |lcAllF = |c| [|AllF- Equally essential is the triangle inequality for v + w and A + B: Triangle inequalities ||v + w|| < ||v|| + ||w|| and ||A+ Bl|r < ||A||F + ||Bl|F (8) We use one more fact when we meet dot products v w or matrix products AB : Schwarz inequalities |[vTw| <|jv||||w|]| and ||AB||r <||AllrlIBllF (9) That Frobenius matrix inequality comes directly from the Schwarz vector inequality : I(AB);;]* < ||row i of A||? ||column j of B||?>. Add for all i and j to see || AB||%. This suggests that there could be a dot product of matrices. Itis A+ B = trace(AT B). Note. The largest size || of the eigenvalues of A is not an acceptable norm! We know that a nonzero matrix could have all zero eigenvalues—but its norm || A|| cannot be zero. In this respect singular values are superior to eigenvalues: o1 = || Al|2. The Eckart-Young Theorem The theorem was in equation (1): If B has rank k then ||A — Ax|| < ||A — B||. In all three norms ||A|| and ||A||r and ||A||n, We come closest to A by cutting off the SVD after k terms. The closest matrix is Ax = alulvl R akukvk ThlS is the fact to use in approximating A by a low rank matrix ! We need an example and it can look extremely simple : a diagonal matrix A. - - 4 0 0 0] 4 0 00 | 03 00)]. 030 0 The rank 2 matrix closest to A = 00 2 0|8 Ay = 000 0 100 0 1, 0000 The difference A — Aj is all zero except for the 2 and 1. Then ||A — A;||r = V22 + 12. How could any other rank 2 matrix be closer to A than this A, ? Please realize that this deceptively simple example includes all matrices of the form Q1AQ2. The norms and the rank are not changed by the orthogonal matrices @Q;, Q2. So this example includes all matrices with singular values 4,3,2,1. The best approxi- mation A2 keeps 4 and 3. Several proofs are collected in Linear Algebra and Learning from Data (Wellesley-Cambridge Press). Chi-Kwong Li has simplified Mirsky’s proof that Ay is closest to A for all norms like || A||9 and ||A|| g that depend only on the o’s. 304 Chapter 7. The Singular Value Decomposition (SVD) Principal Component Analysis Now we start using the SVD. The matrix A is full of data. We have n samples. For each sample we measure m variables (like height and weight). The data matrix Ag has n columns and m rows. In many applications it is a very large matrix. The first step is to find the average (the sample mean) along each row of Ag. Subtract that mean from all m entries in the row. Now each row of the centered matrix A has mean Zzero. The columns of A are n points in R™. Because of centering, the sum of the n column vectors is zero. So the average column is the zero vector. Often those n points are clustered near a line or a plane or another low-dimensional subspace of R™. Figure 7.3 shows a typical set of data points clustered along a line in R® (after centering Ag to shift the points left-right and up-down to have mean (0, 0) in A). How will linear algebra find that closest line through (0,0)? It is in the direction of the first singular vector u; of A. This is the key point of PCA ! A is 2 x n (large nullspace) X/ x AAT is 2 x 2 (small matrix) AT Ais n x n (large matrix) Two singular values oy > 02 > 0 Figure 7.4: Data points (columns of A) are often close to a line in R? or a subspace in R™. The Geometry Behind PCA The best line in Figure 7.4 solves a problem in perpendicular least squares. This is also called orthogonal regression. 1t is different from the standard least squares fit to n data points, or the least squares solution to a linear system Ax = b. That classical problem in Section 4.3 minimizes || Az — b||. It measures distances up and down to the best line. Our problem minimizes perpendicular distances. The older problem leads to a linear equation ATAZ = ATb for the best Z. Our problem leads to singular vectors u; (eigenvectors of AAT). Those are the two sides of linear algebra: not the same side. The sum of squared distances from the data points to the u; line is a minimum. To see this, separate each column a; of A into its components along u; and u; : n n n Y llasl? =\" laTwi P+ laTusf?. (10) 1 1 1 The sum on the left is fixed by the data. The first sum on the right has terms u{ a;ja] u,. It adds to u] (AAT)u,. So when we maximize that sum in PCA by choosing the top eigenvector u; of AAT, we minimize the second sum. That second sum of squared distances from data points to the best line (or best subspace) is the smallest possible. 7.3. Principal Component Analysis (PCA by the SVD) 305 The Geometric Meaning of Eckart-Young Figure 7.4 was in two dimensions and it led to the closest line. Now suppose our data matrix A is 3 by n: Three measurements like age, height, weight for each of n samples. Again we center each row of the matrix, so all the rows of A add to zero. And the points move into three dimensions. We can still look for the nearest line. It will be revealed by the first singular vector u, of A. The best line will go through (0,0,0). But if the data points fan out compared to Figure 7.4, we really need to look for the best plane. The meaning of “best” is still this: The sum of perpendicular distances squared to the best plane is a minimum. That plane will be spanned by the singular vectors u; and u;. This is the meaning of Eckart-Young. It leads to a neat conclusion : The best plane contains the best line. The Statistics Behind PCA The key numbers in probability and statistics are the mean and variance. The “mean” is an average of the data (in each row of Ag). Subtracting those means from each row of A produced the centered A. The crucial quantities are the “variances™ and “covariances”. The variances are sums of squares of distances from the mean—along each row of A. The variances are the diagonal entries of the matrix AAT. Suppose the columns of A correspond to a child’s age on the r-axis and its height on the y-axis. (Those ages and heights are measured from the average age and height.) We are looking for the straight line that stays closest to the data points in the figure. And we have to account for the joint age-height distribution of the data. The covariances are the off-diagonal entries of the matrix AAT. Those are dot products (row ¢ of A) - (row j of A). High covariance means that increased height goes with increased age. (Negative covariance means that one vanable increases when the other decreases.) Our first example has only two rows from age and height: the symmetric matrix AAT is 2 by 2. As the number n of sample children increases, we divide by n — 1 to give AAT its statistically correct scale. T The sample covariance matrix is defined by S = n—1 The factor is n — 1 because one degree of freedom has already been used for mean = 0. This example with six ages and heights is already centered to make each row add to zero: Example A= 3 -4 7 1 -4 -3 7 -6 8 -1 -1 -7 For this data, the sample covariance matrix S is easily computed. It is positive definite. Variances and covariances S = —l—AAT = [ 25 40 20 25 6-1 ' 306 Chapter 7. The Singular Value Decomposition (SVD) The two orthogonal eigenvectors of S are u; and u;. Those are the left singular vectors (often called the principal components) of A. The Eckart-Young theorem says that the vector u, points along the closest line in Figure 7.4. The second singular vector u, will be perpendicular to that closest line. Important note PCA can be described using the symmetric S = AAT/(n — 1) or the rectangular A. No doubt S is the nicer matrix. But given the data in A, computing S can be a computational mistake. For large matrices, a direct SVD of A is faster and more accurate. By going to AAT we square 0; and o, and the condition number o, /o,. In the example, S has eigenvalues near 57 and 3. Their sum is 20 + 40 = 60, the trace of S. The first rank one piece \\/5_7u1v1r is much larger than the second piece \\/§u2v:{. The leading eigenvector u; = (0.6, 0.8) tells us that the closest line in the scatter plot has slope near 8 /6. The direction in the graph nearly produces a 6 — 8 — 10 right triangle. The Linear Algebra Behind PCA Principal Component Analysis is a way to understand n sample points a;,...,a, in m-dimensional space—the data. That data plot is centered: all rows of A add to zero. The crucial connection to linear algebra is in the singular values and the left singular vectors u; of A. Those come from the eigenvalues )\\; = 0,-2 and the eigenvectors of the sample covariance matrix S = AAT/(n - 1). The total variance in the data comes from the squared Frobenius norm of A : Total variance T = ||A||2/(n — 1) = (|la1||* +--- + ||@a|*)/(n = 1). (D) This is the trace of S—the sum down the diagonal. Linear algebra tells us that the trace equals the sum of the eigenvalues of the sample covariance matrix S. The SVD is producing orthogonal singular vectors u; that separate the data into uncorrelated pieces (with zero covariance). They come in order of decreasing variance, and the first pieces tell us what we need to know. The trace of S connects the total vaniance to the sum of variances of the principal components up,..., U, : Total variance @T=02+:.-403. (12) The first principal component u; accounts for (or “explains”) a fraction o2 /T of the total variance. The next singular vector u; of A explains the next largest fraction o2 /T. Each singular vector is doing its best to capture the meaning in a matrix—and all together they succeed. The point of the Eckart-Young Theorem is that k singular vectors (acting together) explain more of the data than any other set of k vectors. So we are justified in choosing u) to uy as a basis for the k-dimensional subspace closest to the n data points. The “effective rank” of A and S is the number of singular values above the point where noise drowns the true signal in the data. Often this point is visible on a “scree plot” showing the dropoff in the singular values (or their squares 0?). Look for the “elbow” in the scree plot (Figure 7.2) where the signal ends and noise takes over. 7.3. Principal Component Analysis (PCA by the SVD) 307 Problem Set 7.3 1 Suppose Ay holds these 2 measurements of 5 samples : 5 4 3 21 “‘[4 101f1] Find the average of each row and subtract it to produce the centered matrix A. Com- pute the sample covariance matrix S = AAT/(n — 1) and find its eigenvalues A, and A2. What line through the origin is closest to the 5 samples in columns of A ? 2 Take the steps of Problem 1 for this 2 by 6 matrix Ag : 4 _[1 01010 =11 2 3 3 21 3 The sample variances s?, s2 and the sample covariance s, are the entries of S. 123 ] What is 0, ? Find S after subtracting row averages from Ag = [ 5 9 9 4 From the eigenvectors of S = AAT, find the line (the u; direction through the center point) and then the plane (u; and u; directions) closest to these four points in three-dimensional space:: B ' -1 0 0 A=|0 0 2-2 1 1-1-1 5 Compare ordinary least squares (Section 4.3) with PCA (perpendicular least squares). They both give a closest line C + Dt to the symmetric data b = —1,0,1 at times t=-3,1,2. 1 =3 ] \" —1 ] Leastsquares : ATAZ = ATb A=1|1 1 b=| 0 PCA : eigenvector of AAT 1 2 1] (singular vector u; of A) 6 Theidea of eigenfaces begins with IV images : same size and alignment. Subtract the average image from each of the N images. Create the sample covariance matrix S = T A;AT/N — 1 and find the eigenvectors (= eigenfaces) with largest eigenvalues. They don’t look like faces but their combinations come close to faces. Wikipedia gives a code for this dimension reduction pioneered by Turk and Pentland. 7 What are the singular values of A — A3 if A has singular values 5,4,3,2,1 and Aj is the closest matrix of rank 3to A ? 8 If A has 0y = 9 and B has 0, = 4, what are upper and lower bounds to o, for A + B ? Why is this true ? Chapter 8 Linear Transformations 8.1 The Idea of a Linear Transformation 8.2 The Matrix of a Linear Transformation 8.3 The Search for a Good Basis The study of linear algebra can begin with matrices, or it can begin with linear transforma- tions. We chose matrices. Another author might say : “Matrices are only a special case of linear transformations.” And going back even farther : “The real numbers are only a special case of a field.” All these topics and more are proper parts of the branch of mathematics called Algebra. I am not quite sure how to respond. Perhaps I would say : “Before you study languages in general (the subject of linguistics), you need to know at least one language.” And working with column vectors and matrices and linear equations and inverses and vector spaces and orthogonality and linear independence and rank is so fascinating and rewarding. To me, the variety of matrices is wonderful. It is like a community of people with different relations and different jobs. Matrices are unique, but factorization connects them to other matrices. You are allowed to have favorites. I hope you do. P.S. Section 8.3 in this chapter discusses good bases for vector spaces of functions. The most natural extension from R™ to infinite dimensions is called “Hilbert space”. The lengths and the inner products of functions f(z) and g(z) have a natural form: 1112 = / f@Pdz and (f,g) / f(@)g(z)d The best bases q1(z),q2(z),... are orthogonal as always, and we will suggest three favorites. But there are many good ways to measure the length of f(z) and its derivatives. That paragraph left Algebra behind, and opened the way to Functional Analysis. 8.1. The Idea of a Linear Transformation 309 8.1 The Idea of a Linear Transformation ﬁ A linear transformation T takes vectors v to vectors T'(v). Linearity requires \\ T(cv+dw)=cT(v)+dT(w)| Note T(0) =0soT(v) =v+ ug is not linear. 2 The input vectors v and outputs T(v) can be in R™ or matrix space or function space. 3 If Ais m by n, T(v) = Av is linear from the input space R to the output space R™™. 4 The derivative T'(f) = % is linear. The integral T+ (f)= / f(t) dt is its pseudoinverse. 0 \\ 3 The product ST of two linear transformations is still linear: | (ST)(v) = S (T(v))./ When a matrix A multiplies a vector v, it “transforms” v into another vector Av. In goes v, out comes T'(v) = Av. A transformation T follows the same idea as a function. In goes a number r, out comes f(x). For one vector v or one number z, we multiply by the matrix or we evaluate the function. The deeper goal is to see all vectors v at once. We are transforming the whole space V when we multiply every v by A. Start again with a matrix A. It transforms v to Av. It transforms w to Aw. Then we know what happens to u = v + w. There is no doubt about Au, it has to equal Av + Aw. Matrix multiplication T'(v) = Awv gives an important linear transformation : A transformation T assigns an output T'(v) to each input vector v in V. The transformation is linear if it meets these requirements for all v and w : (@) T(v+w)=T(v)+T(w) (b) T(cv) =cT(v) forallec. If the input is v = 0, the output must be T'(v) = 0. We combine rules (a) and (b) into one : Linear transformation T(cv + dw) mustequal cT(v)+ dT(w). Again I can test matrix multiplication for linearity: A(cv + dw) = cAv + dAw is true. A linear transformation is highly restricted. Suppose T adds ug to every vector. Then T'(v) = v + ug and T(w) = w + up. This isn’t good, or at least it isn't linear. Applying T to v + w produces v + w + up. That is not the same as T'(v) + T'(w): Shift is not linear v+w+uy isnot T(v)+T(w)=(v+up)+ (w+ up). The exception is when ug = 0. The transformation reduces to T'(v) = v. This is the identity transformation (nothing moves, as in multiplication by the identity matrix). That is certainly linear. In this case the input space V is the same as the output space W. 310 Chapter 8. Linear Transformations The linear-plus-shift transformation T'(v) = Av + uy is called “affine”. Straight lines stay straight although T is not linear. Computer graphics works with affine transformations because we must be able to shift images. This is developed on the website. Example 1 Choose a fixed vectora = (1,3, 4), and let T'(v) be the dot producta - v: The inputis v = (v, v2.v3). The outputis T'(v) =a-v = vy + v + 4vs. Dot products are linear. The inputs v come from three-dimensional space, so V = R3, The outputs are just numbers, so the output space is W = R!. We are multiplying by the row matrix A = {1 3 4]. Then T'(v) = Av. You will get good at recognizing which transformations are linear. If the output involves squares or products or lengths, v# or v,v; or ||v||, then T is not linear. Example2 The length T(v) = ||v|| is not linear. Requirement (a) for linearity would be |v + w|| = ||v]] + ||w||. Requirement (b) would be ||cv|| = c||v||. Both are false! Not (a): The sides of a triangle satisfy an inequality ||v + w|| < ||v|| + ||w]|. Not (b): The length || — v|| is ||v|| and not —||v||. For negative c, linearity fails. Example 3 (Rotation) T is the transformation that rotates every vector by 30°. The “domain” of T is the zy plane (all input vectors v). The “range” of T is also the zy plane (all rotated vectors T'(v)). We described T without a matrix: rotate the plane by 30°. Is rotation linear? Yes it is. We can rotate two vectors and add the results. The sum of rotations T (v) + T (w) is the same as the rotation T'(v + w) of the sum. The whole plane is turning together, in this linear transformation. The rule of linearity extends to combinations of three vectors or n vectors : Linearity u=cv;+cv2+:---+c,v, musttransform to (1) T(u) = clT(vl) + CgT(vz) +---+ ch(v,,) The 2-vector rule starts the 3-vector proof: T'(cu + dv + ew) = T(cu) + T(dv + ew). Then linearity applies to both of those parts, to give three parts : cT'(u) + dT'(v) + T (w). The n-vector rule (1) leads to a very important fact about linear transformations : BASIS TELLS ALL Then you know T'(u) for every vector u in the space. Suppose you know T'(v) for all vectors v,,..., v, in a basis You see the reason: Every u in the space is a combination of the basis vectors v;. Then linearity tells us that T'(u) is the same combination of the outputs T'(v;). 8.1. The Idea of a Linear Transformation 311 Lines to Lines, Triangles to Triangles Figure 8.1 shows the line from v to w in the input space. It also shows the line from T'(v) to T'(w) in the output space. Linearity tells us: Every point on the input line goes onto the output line. And more than that: Equally spaced points go to equally spaced points. The middle point u = v + Jw goes to the middle point T(u) = 3T (v) + 3T (w). The second figure moves up a dimension. Now we have three corners v, v2, v3. Those inputs have three outputs T'(v;), T'(v2), T'(v3). The input triangle goes onto the output triangle. Equally spaced points stay equally spaced (along the edges, and then between the edges). The middle point u = %(vl + v2 + v3) goes to the middle point T(u) = 3(T(v1) + T(v2) + T(v3)). T(va) 1@ —> @ T(v) V) Figure 8.1: Lines to lines, equal spacing to equal spacing, u = 0 to T'(u) = 0. This page in the book is visual, not theoretical. We will show four houses and the matrices that produce them. The columns of H are the eleven corners of the first house. (H is 2 by 12, so plot2d in Problem 25 will connect the 11th corner to the first.) A multiplies the columns of the house matrix to produce the corners AH of the other houses. House _|-6 -6 -7 0 7 6 6 -3 -3 0 0 -6 matrix -7 2 1 8 1 2 -7 -7 -2 =2 -7 -7 | o 4= [cos35° —sm35 = 0 1 sin 35° cosSS 07 03 03 07 » L Figure 8.2: Linear transformations of a house drawn by plot2d(A = H). The 3BluelBrown YouTube channel of Grant Sanderson has excellent graphics. 312 Chapter 8. Linear Transformations Linear Transformations in Calculus Example4 The transformation T takes the derivative of the input: T'(u) = du/dz. How do you find the derivative of u = 6 — 4z + 3z2? You start with the derivatives of 1, z, and x2. Those are the basis vectors. Their derivatives are 0, 1, and 2z. Then you use linearity for the denivative of any combination: du T = 6 (derivative of 1) — 4 (derivative of z) + 3 (derivative of 2°) = —4 + 6x. All of calculus depends on linearity! Precalculus finds a few key derivatives, for \" and sin r and cos z and e*. Then lineanty applies to all their combinations. The chain rule is special to calculus. That produces (df /dg) (dg/dz) as the derivative of f(g(z)). Nullspace of T(u) = du/dz. For the nullspace we solve T'(u) = 0. The derivative is zero when u is a constant function. So the one-dimensional nullspace is a line in function space—all multiples of the special solution u = 1. Column space of T(u) = du/dz. In our example the input space contains all quadratics a + bz + cz?. The outputs (the column space) are all linear functions b + 2cx. Notice that the Counting Theorem is still true: r + (n — r) = n. dimension (column space)+dimension (nullspace) = 2+1 = 3 = dimension (input space) What is the matrix for d/dz? 1 can’t leave derivatives without asking for a matrix. We have a linear transformation T = d/dz. We know what T does to the basis functions: dv, dv, dvs 2 dz dz N dp The 3-dimensional input space V (= quadratics) transforms to the 2-dimensional output space W (= linear functions). If v;,v2,v3 were vectors, this would be the matnx: v, 02,93 =1z, =2r = 2v2. (2) 0 0 2 dr A= [ 010 ] = matrix form of the derivative T = i (3) The linear transformation du/dx is perfectly copied by the matrix multiplication Au. 010 a4 b du 002] b =[2c] Output—;=b+2c:r. d c o - Input u Multiplication Au= [ a + bz + cz? We will connect every transformation to a matrix ! The connection from T to A depended on choosing an input basis 1, z, 2 and an output basis 1, z. Next we look at integrals. They give the pseudoinverse Tt of the derivative! I can’t write T—! and I can’t say “inverse” when f = 1 has derivative df /dx = 0. 8.1. The Idea of a Linear Transformation 313 Example 5 Integration T is also linear: [ (D + Ez)dz = Dz + 1 Ex?. The input basis is now 1, z. The output basis is 1, z, z°. The matrix A* for T+ is 3 by 2: 00 0 D Input Multiplication ATv= |1 0 [ E] =| D Output = Integral of v v=D+ Ez 0 % _-;-E_ T*(v)=D:):+%E.1:2 The Fundamental Theorem of Calculus says that integration is the (pseudo)inverse of differentiation. For linear algebra, the matrix A* is the (pseudo)inverse of the matrix A: 00] 000 A+A=(10 [gég]= 010| and AA+=[:)2]. (@) 03] 001 The derivative of a constant function is zero. That zero is on the diagonal of At A. Calculus wouldn't be calculus without that 1-dimensional nullspace of T = d/dx. Examples of Transformations (mostly linear) Example 6 Project every 3-dimensional vector onto the horizontal plane 2 = 1. The vector v = (zx,y, 2) is transformed to T'(v) = (z,y, 1). This transformation is not linear. Why not? It doesn’t even transform v = 0 into T'(v) = 0. Example 7 Suppose A is an invertible matrix. Certainly T'(v + w) = Av + Aw = T(v) + T(w). Another linear transformation is multiplication by A~!. This produces the inverse transformation T, which brings every vector T'(v) back to v : T~ !(T(v)) =v matches the matrix multiplication A~!(Av) = v. If T(v) = Av and S(u) = Bu, then T(S(u)) matches ABu. We are reaching an unavoidable question. Are all linear transformations from V = R\" to W = R™ produced by matrices? When a linear T is described as a “rotation” or “projection” or “. . .”, is there always a matrix A hiding behind T'? Is T'(v) always Av? The answer is yes! This is an approach to linear algebra that doesn’t start with matrices. We still end up with matrices—after we choose an input basis and output basis. Note Transformations have a language of their own. For a matrix, the column space contains all outputs Av. The nullspace contains all inputs for which Av = 0. Translate those words into “range” and “kernel’” : Range of T = set of all outputs T'(v). Range corresponds to column space. Kernel of T = set of all inputs for which T (v) = 0. Kernel corresponds to nullspace. The range is in the output space W. The kernel is in the input space V. When T is multiplication by a matrix, T'(v) = Awv, range is column space and kernel is nullspace. 314 Chapter 8. Linear Transformations 8 REVIEW OF THE KEY IDEAS = 1. A transformation T takes each v in the input space to 7(v) in the output space. 2. Tislinearif T(v+ w) = T(v) + T(w) and T'(cv) = ¢T'(v): lines to lines. 3. Combinations to combinations: T (c1v1+:--+¢cqvpn) =1 T(v1)+- - - +c¢cn T(vy). 4. T = derivative and T* = integral are linear. So is T(v) = Av from R\" to R™. It is more interesting to see a transformation than to define it. When a 2 by 2 matrix A multiplies all vectors in R2, we can watch how it acts. The eleven house vectors v are transformed into eleven vectors Av. Straight lines between v’s become straight lines between the transformed vectors Av. (The transformation from house to house is linear!) Applying A to a standard house produces a new house—possibly stretched or rotated or otherwise unlivable. 8 WORKED EXAMPLES = 8.1 A The elimination matrix [} 9] gives a shearing transformation from (z,y) to T(z,y) = (z,z + y). If the inputs fill a square, draw the transformed square. Solution The points (1,0) and (2, 0) on the z axis transform by Tto(1,1)and(2,2)on the 45° line. Points on the y axis are not moved: T(0,y) = =eigenvectors with A=1. (1,2) Vertical lines slideup ~ , _ 1 0 (1,1) This is the shearing 1 1 (1,1 Squares go to parallelograms (1,0) 8.1 B A nonlinear transformation T is invertible if every b in the output space comes from exactly one x in the input space: T(x) = b always has exactly one solution. Which of these transformations (on real numbers z) is invertible and what is 7~!? None are linear, not even T3. When you solve T'(x) = b, you are invertling T to find x. Ti(z)=z? Ti(x)=z® Ti(z)=z+9 Ty(z)=e*® Ts(x)=— for nonzeroZz’s T Solution T is not invertible: 2 = 1 has two solutions and 2 = —1 has no solution. T} is not invertible because e* = —1 has no solution. (If the output space changes to positive b’s then the inverse of e* = bis £ = In b.) Notice T = identity. But T2 (z) = z + 18. What are T:2(z) and T2(z)? T,, T3, Ts are invertible: z3 = band £ + 9 = band 1/z = b have one solution z. “lb)=b3 z=T;'(b)=b-9 x=T;'(b)=1/b 8.1. The Idea of a Linear Transformation 315 Problem Set 8.1 1 10 A linear transformation must leave the zero vector fixed: T(0) = 0. Prove this from T(v+ w) = T(v) + T(w) by choosingw = ____ (and finish the proof). Prove it also from T'(cv) = cT'(v) by choosing ¢ = Requirement (b) gives T'(cv) = cT'(v) and also T'(dw) = dT'(w). Then by addition, requirement (a) gives T( ) = ( ). Whatis T(cv + dw + eu)? Which of these transformations are not linear? The input is v = (vy, v2): (a) T(v) = (vg, 1) (b) T(v)=(vn,n) ) T(v)=(0,vy) (d) T(v) = (07 1) (e) T(v) =V —V (f) T(v) = V1 V3. If S and T are linear transformations, is T'(S(v)) linear or quadratic? (a) (Special case) If S(v) = v and T(v) = v, then T(S(v)) = v or v?? (b) (General case) S(v;+v3) = S(v;)+S(v2)and T (v +v2) = T(v1)+ T (v2) combine into T(S(w1+v2) =T(___ )= ___ + Suppose T'(v) = v except that T(0,v2) = (0,0). Show that this transformation satisfies T'(cv) = cT'(v) but does not satisfy T'(v + w) = T(v) + T (w). Which of these transformations satisfy T'(v + w) = T(v) + T (w) and which satisfy T(cv) = cT'(v)? @ T(v)=v/|v] b)) T(v)=vi+vetvs (c) T(v)=(v1,2v2.303) (d) T'(v) = largest component of v. For these transformations of V = R? to W = R?, find T (T (v)). Show that when T(v) is linear, then also T'(T'(v)) is linear. @Tv)=-v (b)) TW)=v+(1,1) (c) T(v) = 90° rotation = (—vs,v;) (d) T(v) = projection = % (v; + va, vy + v2). Find the range and kernel (like the column space and nullspace) of T (@) T(v1,v2) = (v1 — v2,0) (b) T'(v1,v2,v3) = (v1,v2) (c) T(vy,v2) =(0,0) (d) T(vy,v2) = (v1,0). The transformation T'(vq, v2, v3) = (v2, v3,v1) is “cyclic”. What is T(T'(v))? What is T3(v)? What is T1%(v)? Apply T a hundred times to (vy, v, v3). Suppose a linear T transforms (1,1) to (2,2) and (2,0) to (0, 0). Find T'(v): @v=(22 ) v=Q1) () v=(-11) (@@ v=/(a,b). 316 1 12 Chapter 8. Linear Transformations A linear transformation from V to W has a linear inverse from W to V when the range is all of W and the kernel (nullspace) contains only v = 0. Then T'(v) = w has one solution v for each w in W. Why are these T\"’s not invertible? (@) T(vy.v9) = (v2.v2) W = R? (b) T(vy.v9) = (vq,v2,v; + v2) W =R3 (c) T(vy.v2) =0 W =R! If T(v) = Av and A is m by n, then T is “multiplication by A.” (a) What are the input and output spaces V and W? (b) Why is range of T = column space of A? (c) Why is kernel of T = nullspace of A? Problems 13-19 may be harder. The input space V contains all 2 by 2 matrices M. 13 14 15 16 17 18 19 M is any 2 by 2 matrix and A = [}13]. The transformation T is defined by T(M) = AM. What rules of matrix multiplication show that T' is linear ? Suppose A = [ 3 g] Show that the range of T is the whole matrix space V and the kernel 1s the zero matnix: (1) If AM = 0 prove that M must be the zero matrix. (2) Find a solutionto AM = B for any 2 by 2 matrix B. Suppose A = [}, g] Show that the identity matrix [ is not in the range of T. Find a nonzero matrix M such that T(M) = AM is zero. Suppose T transposes every 2 by 2 matrix M. Try to find a matrix A which gives AM = MT. Show that no matrix A will do it. To professors: Is this a linear transformation that doesn’t come from a matrix? The matrix should be 4 by 4! The transformation T that transposes every 2 by 2 matrix is definitely linear. Which of these extra properties are true? (a) T? = identity transformation. (b) The kernel of T is the zero matrix. (c) Every 2 by 2 matrix is in the range of T. (d) T(M) = —M is impossible. Suppose T(M) = [39][M][39]. Find a matrix with T(M) # 0. Describe all matrices with T'(M) = 0 (the kernel) and all output matrices T'(M ) (the range). Why does every linear transformation T from R? to R? take squares to parallelo- grams ? Rectangles also go to parallelograms (squashed if T is not invertible). 8.1. The Idea of a Linear Transformation 317 Questions 20-26 are about house transformations. The output is T(H) = AH. 20 How can you tell from the picture of T (house) that A is (a) a diagonal matrix ? (b) a rank-one matrix ? (c) a lower triangular matrix ? 21 Draw a picture of T (house) for these matrices: _[2 0 7 11 D—[ ] and A_[.3 .3J and U-[O 1]. 22 What are the conditionson A = [: 3] to ensure that T (house) will (a) sit straight up ? (b) rotate the house with no change in its shape ? 23 Describe T (house) when T'(v) = —v + (1,0). This T is “affine”. 24 Change the house matrix H to add a chimney. 25 The standard house is drawn by plot2d(H). Circles from o0 and lines from —: z=H(Q1,),y=H2,)\" axis([-1010-1010]), axis('square’) pIOt(Iv Y, ! O,a I.Y, ,-’); Test plot2d(A '+ H) and plot2d(A’* A * H) with the matrices in Figure 8.2. 26 Without a computer sketch the houses A * H for these matrices A: 1 0 % T D 9 1 1 [0 .1] and [.5 .5] and [-.5 .5] and [1 0] ' 27 This code creates a vector theta of 50 angles. It draws the unit circle and then it draws T (circle) = ellipse. T'(v) = Av takes circles to ellipses. A=[21;12] % You canchange A theta = [0:2 * pi/50:2 * pi]; circle = [cos(theta); sin(theta)]; ellipse = A * circle, axis([-4 4 —4 4)]); axis('square’) plot(circle(1,:), circle(2,:), ellipse(1,:), ellipse(2,:)) 28 What conditions on det A = ad — bc ensure that the output house AH will (a) be squashed onto a line? (b) keep its endpoints in clockwise order (not reflected)? (c) have the same area as the original house? 318 Chapter 8. Linear Transformations 8.2 The Matrix of a Linear Transformation KLinearity tells us all T'(v) if we know T'(v;),...,T(v,) for an input basis v,,...,v,. \\ 2 Column j in the “matrix for 7\"’ comes from applying T to the input basis vector v ;. 3 Write T(v;j)=a;jw; + - - - + am;wm in the output basis of w’s. Those a;; go into column j. 4 The matnx for T'(x) = Az is A, if the input and output bases = columns of I,, xn and I, xm. 5 When the bases change to v's and w’s, the matrix for the same T changes from A to W 1AV {Best bases: V = W = eigenvectors and V, W = singular vectors change A to A argy The next pages assign a matrix A to every linear transformation T. For ordinary column vectors, the input v is in V = R\" and the output T'(v) is in W = R™. The matrix A for this transformation will be m by n. Our choice of bases in V and W will decide A. The standard basis vectors for R™ and R™ are the columns of I. That choice leads to a standard matrix. Then T(v) = Av in the normal way. But these spaces also have other bases, so the same transformation T is represented by other matrices. A main theme of linear algebra is to choose the bases that give the best matrix (a diagonal matrix) for T'. All vector spaces V and W have bases. Each choice of those bases leads to a matrix for T. When the input basis is different from the output basis, the matrix for T'(v) = v will not be the identity I. It will be the “change of basis matrix”. Here is the key idea: Suppose we know outputs T'(v) for the input basis vectors v; to vy,. Columns 1 to n of the matrix will contain those outputs T'(v;) to T (v,,). A times ¢ = matrix times vector = combination of those n columns. Ac gives the correct combination ¢;T(v;) 4 - - + cn T (vn) = T'(v). Reason Every v is a unique combination c,v; + --- + c,v, of the basis vectors v;. Since T is a linear transformation (here is the moment for linearity), T'(v) must be the same combination c;,T(v;) + - - - + c,T(v,,) of the outputs T'(v;) in the columns. Our first example gives the matrix A for the standard basis vectors in R? and R3. The two columns of A are the outputs from v; = (1,0) and v, = (0, 1). Example 1 Suppose T transforms v; = (1,0) to T'(v,) = (2, 3, 4). Suppose the second basis vector vz = (0, 1) goes to T(v2) = (5,5,5). If T is linear from R? to R3 then its “standard matrix” is 3 by 2. Those outputs T'(v;) and T'(v2) go into the columns of A: 2 5 (2 5 . 7 A=1]3 5 cp=1landcy =1giveT(vy +v2)= |3 5 []= 8 45 4 5 9 8.2. The Matrix of a Linear Transformation 319 Change of Basis : Matrix B Example 2 Suppose the input space V = R? is also the output space W = R2. Suppose that T(v) = v is the identity transformation. You might expect its matrix to be I, but that only happens when the input basis is the same as the output basis. I will choose different bases to see how the matrix is constructed. For this special case T'(v) = v, I will call the matrix B instead of A. We are just changing basis from the v’s to the w’s. Each v 1s a combination of w; and wy. Inp.ut v, vo| = 3 6 Out.put wy wyl= 3 0 Changev1=1w1+1w2 basis 3 8 basis of basis v, = 2w; + 3w> Please notice! 1 wrote the input basis v;,v; in terms of the output basis w;,w,. That is because of our key rule. We apply the identity transformation T to each input basis vector: T'(v;) = v; and T'(v2) = vo. Then we write those outputs v, and v in the output basis w; and w,. Those bold numbers 1,1 and 2, 3 tell us column 1 and column 2 of the matrix B (the change of basis matrix): WB =V so B = w-lv. ezt o el el SEE 8 change of basis 1 2 When the input basis is in the columns of a matrix V, and the output basis is in the columns of W, the change of basis matrix for T = I is B = W~1V. Thekey I see aclear way to understand that rule B = W1V Suppose the same vec- tor u is written in the input basis of v’s and the output basis of w’s. I will do that three ways: o Tr c1 - - ar dl is |v; +-- U, C = wr - wn . | and Ve=Wd. Lcn dna - 3 -l 3 U=cCcv; +- - +ChVn u=d1w1+---+dn’wn The coefficients d in the new basis of w’sared = W~Ve. Then Bis W=V, (2) This formula B = W1V produces one of the world's greatest mysteries: When the standard basis V = I is changed to a different basis W, the change of basis matrix is not W but B = W 1. Larger basis vectors have smaller coefficients! -1 [ T ] in the standard basis has coefficients [wl wz] [ : ] in the wy, w3 basis. Y 320 Chapter 8. Linear Transformations Construction of the Matrix for T Now we construct a matrix for any linear transformation. Suppose 7' transforms the space V (n-dimensional) to the space W (m-dimensional). We choose a basis vy, ..., v, for V and we choose a basis w, ..., w,, for W. The matrix A will be m by n. To find the first column of A, apply T to the first basis vector vy. The output T'(v; ) is in W, T(v,) is acombination a); w;+---+amWwm Of the output basis for W. These numbers a1y, . . .,am1 go into the first column of A. Transforming v; to T'(v;) matches multiplying (1,0,...,0) by A. It yields that first column of the matnx. When T is the derivative and the first basis vector is 1, its derivative is T'(v;) = 0. So for the derivative matrix below, the first column of A is all zero. Example 3 The input basis of v’s is 1, z, 2%, z3. The output basis of w’s is 1, z, z°. o ) dv . . . 1) Then T takes the derivative: T(v) = T and A = “derivative matrix”. T - C ) - Ifv=oc; +coz + c3z% + cy13 ( 0100 ( c; ( 2 th dv 2 Ac = 0 0 2 O c = 263 ena = 1co + 2¢c37 + 3c4z 0 0 0 3 J cj _J | 3c4 J Key rule: The jth column of A is found by applying T' to the jth basis vector v; T (v;) = combination of output basis vectors = ajjw; + - - + @ Wn. (3) These numbers a;; go into A. The matrix is constructed to get the basis vectors right. Then linearity gets all other vectors right. Every v is a combination c1v; + - -+ + CcpVp, and T(v) is a combination of the w’s. When A multiplies the vector ¢ = (cy,. . .,¢n) in the v combination, Ac produces the coefficients in the T'(v) combination. This is because matrix multiplication (combining columns) is linear like T'. Every linear transformation from V to W converts to a matrix using the bases. Example4 For the integral T (v), the first basis function is again 1. Its integral is the second basis function z. So the first column of the “integral matrix” A% is (0, 1,0, 0). \"0 0 0] [0 ] The integral of d; + dzz + d3z? g |1 00 [ Zl ] d 1 1 = - is dz + —dgzz + —d3$3 0 % 0 d2 J -;-dg 2 3 o o0 1 - 9 1 i 3 | | 393 | If you integrate a function and then differentiate, you get back to the start. So AA* = I. But if you differentiate before integrating, the constant term is lost. So At A is not I. 8.2. The Matrix of a Linear Transformation 321 The integral of the derivative of 1 is zero : T*T(1) = integral of zero function = 0. This matches At A, whose first column is all zero. The derivative T has a kernel (the constant functions). Its matrix A has a nullspace. Main idea again: Av copies T'(v). The examples of the derivative and integral made three points. First, linear trans- formations T' are everywhere—in calculus and differential equations and linear algebra. Second, spaces other than R™ are important—we had functions in V and W. Third, if we differentiate and then integrate, we can multiply their matrices A+ A. Matrix Products A B Match Transformations TS We have come to something important—the real reason for the rule to multiply matrices. At last we discover why! Two linear transformations T and S are represented by two matrices A and B. Now compare T'S with the multiplication AB: When we apply the transformation T to the output from S, we get T'S by this rule: (T'S)(u) is defined to be T (S(u)). The output S(u) becomes the inputto T. When we apply the matrix A to the output from B, we multiply AB by this rule: (AB)(z) is defined to be A(Bx). The output Bx becomes the input to A. Matrix multiplication gives the correct matrix AB to represent T' S. The transformation S is from a space U to V. Its matrix B uses a basis u;..... U, for U and a basis vy, ..., v, for V. That matrix is n by p. The transformation T is from V to W as before. Its matrix A must use the same basis v, ...,v, for V—this is the output space for S and the input space for T'. Then the matrix AB matches T'S. Multiplication The linear transformation T'S starts with any vector u in U, goes to S(u) in V and then to T(S(u)) in W. The matrix AB starts with any x in RP?, goes to Bx in R™ and then to ABx in R™. The matrix AB correctly represents T'S : TS : U->V oW AB : (mbyn)(nbyp)=(mbyp). Product of transformations T'S matches product of matrices AB. An important case 1s when the spaces U, V, W are the same and their bases are the same => square matrices. Example 5 S rotates the plane by 6 and T also rotates by §. Then T'S rotates by 2. This transformation T2 corresponds to the rotation matrix A? through 26 : cos20 - sin 20} @ 3 _ 2 _ : 2 _ T =S A=B T“ = rotation by 26 A= [ sin20 cos 20 A2 = [cosO —sinOJ [cose -sin0] cos?@ —sin®d —2sinfcosd 5) sin@ cos@| |sinf cos@ - 2sinfcosf cos?8 — sin? 6 Comparing (4) with (5) produces cos20 = cos? — sin’6 and sin20 = 2sin6 cos . 322 Chapter 8. Linear Transformations Example 6 S rotates by the angle 8 and T rotates by —6. ThenT'S = I leadsto AB = I. In this case T(S(u)) is u. We rotate forward and back. For the matrices to match, ABzx must be x. The matrices with § and —6 are inverses. cosf sinf| |cosf —sinf cos? 6 + sin® @ 0 _ sinf cosf 0 cos?@ +sin?0| — AB=[ I —sinf cosf Choosing the Best Bases Now comes the final step in this section of the book. Choose bases that diagonalize the matrix. With the standard basis (the columns of I) our transformation T\" produces some matrix A—probably not diagonal. That same T is represented by different matrices when we choose different bases. The two great choices are eigenvectors and singular vectors: Eigenvectors If T transforms R\" to R\", its matrix A is square. If there are n independent eigenvectors, choose those as the input and output basis. In this good basis, the matrix for T is the diagonal eigenvalue matrix A. Example 7 The projection matrix T projects every v = (z,y) in R? onto the line y = —z. Using the standard basis, v, = (1,0) projects to T'(v;) = (3,-3). For v = (0, 1) the projection is T(v2) = (—3, ). Those are the columns of A: NI NI o5 Standard bases Projection matrix A [ Not diagonal - ] has AT = A and A% = A. Now comes the main point of eigenvectors. Make them the basis vectors ! Diagonalize ! When the basis vectors are eigenvectors, the matrix becomes diagonal. v; = w; = (1,-1) projects to itself : T'(v;) = vy and \\; =1 vy = w3 = (1, 1) projectstozero : T'(v2) = 0 and A\\, =0 .. |1 0 A1 O} _ The new matrix 1s [O 0]—[0 A2]—1\\. (6) Eigenvector bases Diagonal matrix Eigenvectors are the perfect basis vectors. They produce the eigenvalue matrix A. What about other choices of input basis = output basis? Put those basis vectors into the columns of B. We saw above that the change of basis matrices (between standard basis and new basis) are B;, = B and B,y = B~1. The new matrix for T is similar to A : A, ew = B~1AB in the new basis of b’s is similar to A in the standard basis : Apstob's =B 'suandardtob's Astandard Bb'stostandard () I used the multiplication rule for the transformation ITI and the matrices B~!AB. 8.2. The Matnix of a Linear Transformation 323 Finally we allow different spaces V and W, and different bases v’s and w’s. When we know T and we choose bases, we get a matrix A. Probably A is not symmetric or even square. But we can always choose v’s and w’s that produce a diagonal matrix. This will be the singular value matrix £ = diag (o, ...,0;) in the decomposition A = ULV'T, Singular vectors The SVD says that U~! AV = X. The right singular vectors v1,..., U, Will be the input basis. The left singular vectors u,,...,u,,, will be the output basis. By the rule for matrix multiplication, the matrix for the same transformation in these new bases is B,.s ABi, = U 'AV = X. Diagonal! I can’t say that ¥ is “similar” to A. We are working now with two bases, input and output. But those are orthonormal bases and they preserve the lengths of vectors. Following a good suggestion by David Vogan, I propose that we say: X is “isometric” to A. Definition C = Q7' AQ; is isometric to A if Q, and Q, are orthogonal. Example 8 To construct the matrix A for the transformation T = %, we chose the input basis 1, z, z2, 3 and the output basis 1, z, z°. The matrix A was simple but unfortu- nately it wasn’t diagonal. But we can take each basis in the opposite order. Now the input basis is 23, 2, z, 1 and the output basis is 2, r, 1. The change of basis matrices B;, and B,y are permutations. The matrix for T'(u) = du/dr with the new bases is the diagonal singular value matrix B, AB;, = Z witho’s = 3.2.1: 0 0 B lAB;, = 1 0 0. (8) 0 0 out B J_ J 11 |1 J Well, this was a tough section. We found that 23, 2, z have derivatives 372, 2z, 1. ® REVIEW OF THE KEY IDEAS = 1. If we know T'(v;),...,T(v,) for a basis, linearity will determine all other T'(v). Linear transformation T° Matrix A(mbyn) Column jof Aisa;jtoam; 2. { Inputbasisv,,...,v, — represents T exactly when T'(v;) = Output basis w,. . ., W, in these bases ajwy + -+ ApjWn 3. The change of basis matrix B = W~V = B B, represents the identity T'(v) = v. 4. If A and B represent T and S, and the output basis for S is the input basis for T', then the matrix AB represents the transformation T'(S(u)). 5. The best input-output bases are eigenvectors and/or singular vectors of A. Then B~'AB = A = eigenvalues B;} AB;, = T = singular values. 324 Chapter 8. Linear Transformations Problem Set 8.2 Questions 1-4 extend the first derivative example to higher derivatives. 1 The transformation S takes the second derivative. Keep 1,z, 22, 23 as the input basis v,.v,.v3, v4 and also as output basis w, wo, w3, ws. Write S(v;), S(v2), S(v3), S(v,) in terms of the w’s. Find the 4 by 4 matrix A, for S. 2 What functions have S(v) = 0? They are in the kernel of the second derivative S. What vectors are in the nullspace of its matrix A in Problem 1? 3 The second derivative A; is not the square of a rectangular first derivative matrix A;: 0100 Ai=10 0 2 0] doesnotallow Af = A,. 0 0 0 3] Add a zero row 4 to A; so that output space = input space. Compare A% with A,. Conclusion: We want output basis = basis. Then m = n. 4 (a) The product T'S of first and second derivatives produces the third derivative. Add zeros to make 4 by 4 matrices, then compute A; A, = As. (b) The matrix A2 corresponds to S? = fourth derivative. Why is this zero? Questions 5-9 are about a particular transformation T and its matrix A. 5 With bases v, , v2, v3 and w;, w2, w3, suppose T'(v;) = wo and T'(v;) = T'(v3) = w; + w3. T is a linear transformation. Find the matrix A and multiply by the vector (1,1, 1). What is the output from T when the input is v; + v, + v3? 6 Since T(v2) = T(v3), the solutions to T'(v) = 0 are v = . What vectors are in the nullspace of A? Find all solutions to T'(v) = w». 7 Find a vector that is not in the column space of A. Find a combination of w’s that is not in the range of the transformation T'. 8 You don’t have enough information to determine T2. Why is its matrix not necessar- ily A2? What more information do you need? 9 Find the rank of A. The rank is not the dimension of the whole output space W. It is the dimension of the of T. Questions 10-13 are about invertible linear transformations. 10 Suppose T'(v;) = w; + w2 + w3 and T'(v2) = w2 + w3 and T'(v3) = ws. Find the matrix A for T using these basis vectors. What input vector v gives T'(v) = w;? 11 Invert the matrix A in Problem 10. Also invert the transformation T—what are T-!(w;) and T~} (w;) and T~} (w3)? 12 Which of these are true and why is the other one ridiculous? @ T 'T=I - ® T 'T(v))=mv ) T YT (w,)) =w,. 8.2. The Matrix of a Linear Transformation 325 13 Suppose the spaces V and W have the same basis v, v,. (a) Describe a transformation T (not I') that is its own inverse. (b) Describe a transformation T (not I) that equals T2 (c) Why can’t the same T be used for both (a) and (b)? Questions 14-19 are about changing the basis. 14 15 16 17 18 19 (a) What matrix B transforms (1,0) into (2, 5) and transforms (0, 1) to (1, 3)? (b) What matrix C transforms (2, 5) to (1,0) and (1, 3) to (0,1)? (c) Why does no matrix transform (2, 6) to (1,0) and (1, 3) to (0,1)? (a) What matrix M transforms (1,0) and (0, 1) to (r,t) and (s, u)? (b) What matrix N transforms (a, c) and (b, d) to (1,0) and (0,1)? (c) What condition on a, b, ¢, d will make part (b) impossible? (a) How do M and N in Problem 15 yield the matrix that transforms (a, c) to (7, t) and (b,d) to (s, u)? (b) What matrix transforms (2, 5) to (1,1) and (1,3) to (0,2)? If you keep the same basis vectors but put them in a different order, the change of basis matrix B is a matrix. If you keep the basis vectors in order but change their lengths, B is a matrix. The matrix that rotates the axis vectors (1,0) and (0, 1) through an angle 6 is Q. What are the coordinates (a, b) of the original (1,0) using the new (rotated) axes? This inverse can be tricky. Draw a figure or solve for a and b: cosf —sinf 1 cosf —sinf Q= [sinO cosO] [0] _a[sin0]+b[ cosﬂ]' The matrix that transforms (1,0) and (0,1) to (1,4) and (1,5) is B = : The combination a(1,4) + b(1,5) that equals (1,0) has (a,b) = ( , ). How are those new coordinates of (1, 0) related to B or B~!? Questions 20-23 are about the space of quadratic polynomials y = A + Bz + Cz?. 20 21 The parabola w; = %(z2 + ) equalsoneatz = 1,and zeroatz = 0 and z = —1. Find the parabolas w2, w3, and then find y(z) by linearity. (a) wo equalsoneatzr =0andzeroatz =1andz = —1. (b) w3 equalsoneatz = —-1andzeroatz =0andz = 1. (c) y(z)equalsdatr =1andSatz =0and 6 at z = —1. Use w, w2, w3. One basis for second-degree polynomials is v; = 1 and v = £ and v3 = z2. Another basis is w;, w2, w3 from Problem 20. Find two change of basis matrices, from the w’s to the v’s and from the v’s to the w’s. 326 22 23 24 25 26 27 28 30 31 32 33 Chapter 8. Linear Transformations What are the three equations for A, B, C if the parabola y = A + Bz + Cz? equals 4atz =aandSatz =band6atx = c? Find the determinant of the 3 by 3 matrix. That matrix transforms values like 4, 5, 6 to parabolas y—or is it the other way? Under what condition on the numbers m;, ms, . .., mg do these three parabolas give a basis for the space of all parabolas a + bz + cz?? v =m) +moxT + m33:2, Vo =myg + msT + m6x2, V3 = M7 + MmgT + mgl‘2. The Gram-Schmidt process changes a basis a;,a2,a3 to an orthonormal basis d,,q,.q3. These are columns in A = QR. Show that R is the change of basis matrix from the a’s to the g’s (a, is what combination of ¢’s when A = QR ?). Elimination changes the rows of A to the rows of U with A = LU. Row 2 of A is what combination of the rows of U? Writing AT = UTLT to work with columns, the change of basis matrix is B = LT. We have bases if the matrices are Suppose v, , v2, v3 are eigenvectors for T'. This means T'(v;) = A;v; fori =1,2,3. What is the matrix for T when the input and output bases are the v’s? Every invertible linear transformation can have I as its matrix! Choose any input basis vy, . .., v,. For output basis choose w; = T'(v;). Why must T be invertible? Using v; = w; and v = w;, find the standard matrix for these 1°’s: (@) T(v;) =0and T(v3) = 3vy () T(vy) =v;and T(v; + v2) = v;. Suppose T reflects the zy plane across the z axis and S is reflection across the y axis. If v = (z,y) what is S(T'(v))? Find a simpler description of the product ST. Suppose T is reflection across the 45° line, and S is reflection across the y axis. If v =(2,1) thenT(v) = (1,2). Find S(T'(v)) and T(S(v)). Usually ST # T'S. The product of two reflections is a rotation. Multiply these reflection matrices to find the rotation angle: cos 26 sin 26 cos 2a sin 2a sin 20 - cos26 sin2a —cos2al|’ Suppose A is a 3 by 4 matrix of rank r = 2, and T'(v) = Av. Choose input basis vectors v, v2 from the row space of A and v3,v4 from the nullspace. Choose output basis vectors w; = Av;, w2 = Av; in the column space and w3 from the nullspace of AT. What specially simple matrix represents T in these special bases? The space M of 2 by 2 matrices has the basis v;,v2,v3,v4 in Worked Example 8.2 A. Suppose T multiplies each matrix by [22]. With w’s equal to v’s, what 4 by 4 matrix A represents this transformation T on matrix space? True or False: If we know T'(v) for n different nonzero vectors in R™, then we know T'(v) for every vector v in R™. 8.3. The Search for a Good Basis 327 8.3 The Search for a Good Basis ( With a new input basis B, and output basis By, €every matrix A becomes B, ABi.\\ 2 Bin = Bou =“generalized eigenvectors of A” produces the Jordan form J=B~!AB. 3 The Fourier matrix F = B;, = B, diagonalizes every circulant matrix (use the FFT). @nes, cosines, etk*, Legendre and Chebyshev : those are great bases for function spacy This is an important section of the book. The first chapters prepared the way by ex- plaining the idea of a basis. Chapter 6 introduced the eigenvectors and Chapter 7 found singular vectors v and w. Those basis vectors are two winners but many other choices are very valuable. So many computations begin with a choice of basis. The input basis vectors will be the columns of Bj,. The output basis vectors will be the columns of Byy. Always Bin and By are invertible—basis vectors are independent ! Pure algebra If A is the matrix for a transformation T in the standard basis, then B;,l A B;, is the matrix in the new bases. (1 The standard basis vectors are the columns of the identity: Bin = Inxn and Bost = Imxm- Now we are choosing special bases to make the matrix clearer and simpler than A. When Bip, = B,y = B, the square matrix B~!AB is similar to A: same eigenvalues. Applied algebra Applications are all about choosing good bases. Here are four important choices for vectors and three choices for functions. Eigenvectors and singular vectors led to A and ¥ in Section 8.2. The Jordan form is new. 1 B;, = B, = eigenvector matrix X. Then X —14AX = eigenvalues in A. This choice requires A to be a square matrix with n independent eigenvectors. “A must be diagonalizable.” We get A when Bj, = By is the eigenvector matrix X. 2 B, = V and B,y = U : singular vectors of A. Then U\"'AV = diagonal X. Y is the singular value matrix (with oy,...,0, on its diagonal) when B;, and B,y are the singular vector matrices V and U. Recall that those columns of B;, and Boy: are orthonormal eigenvectors of ATA and AAT. Then A = ULV T givesE = U~'AV. 3 B,, = B, = generalized eigenvectors of A. Then B~ AB = Jordan form J. A is a square matrix but it may only have 8 independent eigenvectors. (If s = n then B is X and J is A.) In all cases Jordan constructed n — s additional “generalized” eigenvectors, aiming to make the Jordan form J as diagonal as possible i) There are s square blocks along the diagonal of J. ii) Each block has one eigenvalue A, one eigenvector, and 1’s above the diagonal. The best J has n 1 x 1 blocks, each containing an eigenvalue. Then J = A (diagonal). 328 Chapter 8. Linear Transformations Example 1 This Jordan matrix J has eigenvalues A = 2,2, 3, 3 (two double eigen- values). Those eigenvalues lie along the diagonal because J is triangular. There are two independent eigenvectors for A = 2, but there is only one line of eigenvectors for A = 3. This will be true for every matrix C = BJB™! that is similar to J. ( 2 1 Two 1 by 1 blocks . _ 2 One 2 by 2 block Jordan matrix J = 3 1 J has 3 eigenvectors I 0 3 j Eigenvalues 2,2, 3,3 Two eigenvectors for A = 2 are ; = (1,0,0,0) and 2 = (0, 1, 0, 0). One eigenvector for A = 3is x3 = (0,0,1,0). The “generalized eigenvector” for this Jordan matrix is the fourth standard basis vector 4 = (0,0, 0, 1). The eigenvectors for J (normal and generalized) are just the columns x;, T2, T3, T4 of the identity matrix [. Notice (J — 3I)x4 = x3. The generalized eigenvector x4 connects to the true eigenvector x3. A true x4 would have (J — 3I)x4 = 0, but that doesn’t happen here ! Every matrix C = BJB™! that is similar to this J will have true eigenvectors b;. b,. bs in the first three columns of B. The fourth column of B will be a generalized eigenvector by of C, tied to the true bs. Here is a quick proof that uses Bxz = b3 and Bx4 = b4 to show: The fourth column b, is tied to b3 by (C — 3I)bs = bs. (BJB™' —3I)by=BJz4—3Bxy=B(J —3[)xy=Bzz=bs. (2 The point of Jordan’s theorem is that every square matrix A has a complete set of eigenvectors and generalized eigenvectors. When those go into the columns of B, the matrix B~!AB = J is in Jordan form. Based on Example 1, here is a description of J. The Jordan Form For every A, we want to choose B so that B~ AB is as nearly diagonal as possi- ble. When A has a full set of n eigenvectors, they go into the columns of B. Then B = X. The matrix X ~'AX is diagonal, period. This is the Jordan form of A—when A can be diagonalized. In the general case, eigenvectors are missing and A can’t be reached. Suppose A has s independent eigenvectors. Then it is similar to a Jordan matrix with s blocks. Each block has an eigenvalue on the diagonal with 1’s just above it. This block accounts for exactly one eigenvector of A. Then B contains generalized eigenvectors as well as ordinary eigenvectors. When there are n eigenvectors, all n blocks will be 1 by 1. In that case J = A. The Jordan form solves the differential equation du/dt = Au for any square matrix A = BJB™!. The solution e#*u(0) becomes u(t) = Be’*B~u(0). J is triangu- lar and its matrix exponential €”’t involves e*t times powers 1,¢,...,t*~1. Overall the Jordan form deals optimally with repeated eigenvalues. 8.3. The Search for a Good Basis 329 (Jordan form) If A has s independent eigenvectors, it is similar to a matrix J that has s Jordan blocks J; ..., Js onits diagonal. B contains *“generalized eigenvectors™ : Jordanformof A B~ 'AB-= =J. (3) Jordan block in J J; = . . 4) Matrices are similar if they share the same Jordan form J—not otherwise. The Jordan form J has an off-diagonal 1 for each missing eigenvector (and the 1's are next to the eigenvalues). In every family of similar matrices, we are picking one outstanding member called J. It is nearly diagonal (or if possible completely diagonal). We can quickly solve du/dt = Ju and take powers J*. Every other matrix in the family has the form BJB~!. Jordan’s Theorem is proved in my textbook Linear Algebra and Its Applications. Please refer to that book (or more advanced books) for the proof. The reasoning is rather intricate and in actual computations the Jordan form is not at all popular—its cal- culation is not stable. A slight change in A will separate the repeated eigenvalues and remove the off-diagonal 1’s—switching Jordan to a diagonal A. Proved or not, you have caught the central idea of similarity—to make A as simple as possible while preserving its essential properties. The best basis B gives B-'AB =J. Question Find the eigenvalues and all possible Jordan forms if A? = zero matrix. Answer The eigenvalues must all be zero, because Az = Az leads to A’z = Az = Oz. The Jordan form of A has J2 = 0 because J> = (B~'AB)(B~'AB) = B~'A’B = 0. Every block in J has A = 0 on the diagonal. Look at JZ for block sizes 1,2, 3: 0107 T ‘ 2 2 01 0 0 [O]’—'[O] [O 0]=[0 0] 88 5 - - - Conclusion: If J2 = 0 then all block sizes must be 1 or 2. J? is not zero for 3 by 3. The maximum rank of J is n/2, when there are n/2 blocks, each of size 2 and rank 1. 330 Chapter 8. Linear Transformations Now come three great bases of applied mathematics: Fourier, Legendre, and Chebyshev. Their discrete forms are vectors in R\". Their continuous forms are func- tions. Since they are chosen once and for all, without knowing the matrix A, these bases B;i, = B,y probably don’t diagonalize A. But for many important matrices A in applied mathematics, the matrices B! AB are close to diagonal. 4 Bj, = B,y = Fourier matrix F Then Fz is a Discrete Fourier Transform of x. Those words are telling us : The Fourier matrix with columns = eigenvectors of P in equation (6) is important. Those are good basis vectors to work with. We ask: Which matrices are diagonalized by F'? This time we are starting with the eigenvectors (1, A, A%, A3) and finding the matrices that have those eigenvectors: 01 0 O] [1 ] T 1 IfA‘zlthen Pz = 8 g :) ? i2 = A i2 = \\T. (5) 1.0 0 0] A 2 P is a permutation matrix. The equation Pz = Ax says that T is an eigenvector and A is an eigenvalue of P. Notice how the fourth row of this vector equation is 1 = A%, That rule for A makes everything work. Does this give four different eigenvalues A? Yes. The four numbers A = 1,2, —1, —% all satisfy A* = 1. (You know i® = —1. Squaring both sides gives i = 1.) So those four numbers are the eigenvalues of P, each with its eigenvector £ = (1, A, A2, A\\3). The eigenvector matrix F' diagonalizes the permutation matrix P above: : 1 ] Eigenvector [1 1 1 1 ] Eigenvalue : . o : matrix A 1 matrix is 1 ¢+ -1 — ©6) for P -1 Fourier 1 2 1 (-i)? i —i matrix F |1 & -1 (-i)? Those columns of F' are orthogonal. They are eigenvectors of P and every circulant matrix C = ¢yl + ¢, P + ca P? 4 c2 P3. Unfortunately this Fourier matrix F is complex (it is the most important complex matrix in the world). Multiplications F'x are done millions of times very quickly, by the Fast Fourier Transform. rCo C1 C2 63- Circulant _le g1 e matrix C2 C3 Cop C1 &1 €2 €3 Co has four eigenvectors in the Fourier matrix F has four eigenvalues in the vector F'c A = ¢y + ¢ + ¢ + c3 has eigenvector (1,1,1,1) For more about circulant matrices please see Section 6.4. 8.3. The Search for a Good Basis 331 Circulant matrices have constant diagonals. The same number ¢y goes down the main diagonal. The number c; is on the diagonal above, and that diagonal “wraps around” or “circles around” to the southwest corner of C. This explains the name circulant and it indicates that these matrices are periodic or cyclic. Even the powers of A cycle around because A* = 1 leads to A%, A8, A7, A8 = )\\, \\2 A3, )\\4, Constancy down the diagonals is a crucial property. It corresponds to constant coeffi- cients in a differential equation. This is exactly when Fourier works perfectly! . d%u The equation proie —u issolvedby u = cpcost + c; sint. . d%u . The equation i tu cannot be solved by elementary functions. These equations are linear. The first is the oscillation equation for a simple spring. It is Newton’s Law f = ma with mass m = 1, a = d®u/dt?, and force f = —u. Constant coefficients produce the differential equations that you can really solve. The equation u /\" = tu has a variable coefficient ¢. This is Airy’s equation in physics and optics (it was derived to explain a rainbow). The solutions change completely when ¢ passes through zero, and those solutions require infinite series. We won't go there. The point is that equations with constant coefficients have simple solutions like e, You discover \\ by substituting e** into the differential equation. That number X is like an eigenvalue. For u = cost and u = sint the number is A = ¢. Euler’s great formula e't = cost + isint introduces complex numbers as we saw in the eigenvalues of P and C. Bases for Function Space For functions of z, the first basis I would think of contains the powers 1,z,z2, 3. ... Unfortunately this is a terrible basis. Those functions z\" are just barely independent. 219 is almost a combination of other basis vectors 1,z,...,z°. It is virtually impossible to compute with this “ill-conditioned” basis. If we had vectors instead of functions, the test for a good basis would look at BT B. This matrix contains all inner products between the basis vectors (columns of B). The basis is orthonormal when BT B = I. That is best possible. But the basis 1, z, r? ... produces the evil Hilbert matrix: BT B has an enormous ratio between its largest and smallest eigenvalues. A large condition number signals an unhappy choice of basis. When the integrals go from z = 0 to z = 1, the inner product of * with 7 is 1 piti+l 1==1 1 / ) dr = —— ] = — = entries of Hilbert matrix BT B 0 t+3+1],0 t+3+1 Note Now the columns of B are functions instead of vectors. We still use BTB to test for independence. So we need to know the dot product (inner product is a better name) of two functions—those are the numbers in BT B. 332 Chapter 8. Linear Transformations The dot product of vectors is just Ty = T,y; + - -+ + TnyYn. The inner product of functions will integrate instead of adding, but the idea is completely parallel : Inner product (f,g) = [ f(z)g(z)dz Complex inner product (f,g) = [ f(z) g(z)dz, f = complex conjugate Weighted inner product (f,g)w = [w(z) f(z)g(z)dz, w = weight function Orthogonal Bases for Function Space Here are the three leading even-odd bases for theoretical and numerical computations: 5. The Fourier basis 1,sinx, cos x, sin 2z, cos 2z, ... . 2 1 4 3 6. The Legendre basis 1, z, % — 5, x° — gz, 7. The Chebyshev basis 1, z, 2z? — 1, 423 — 3z, ... The Fourier basis functions (sines and cosines) are all periodic. They repeat over every 27 interval because cos(z + 27) = cosz and sin(z + 27) = sinz. So this basis is especially good for functions f(z) that are themselves periodic: f(z + 27) = f(x). This basis is also orthogonal. Every sine and cosine is orthogonal to every other one. Most important, the sine-cosine basis is also excellent for approximation. If we have a smooth periodic function f(z), then a few sines and cosines (low frequencies) are all we need. Jumps in f(z) and noise in the signal are seen in higher frequencies (larger n). We hope and expect that the signal is not drowned by the noise. The Fourier transform connects f(z) to the coefficients ax and by in its Fourier series : Fourier series f(z) = ao + by sinz + a; cosz + by sin2x + azcos 2z + - - - We see that function space is infinite-dimensional. It takes infinitely many basis func- tions to capture perfectly a typical f(z). But the formula for each coefficient (for example a3) is just like the formula bTa/ a”a for projecting a vector b onto the line through a. Here we are projecting the function f(z) onto the line in function space through cos 3z : _ (f(z),cos 3z) [ f(z) cos 3z dzx ~ (cos 3z,co08 3z) = [ cos 3z cos 3z dzx’ Fourier coefficient ag3 (7) Note The sine-cosine basis is not so excellent when the function f(z) has a jump (like a 0-1 function). The Fourier approximations will overshoot the jump. This Gibbs phenomenon doesn’t get better by including more sines and cosines in the approximation. 8.3. The Search for a Good Basis 333 Legendre Polynomials and Chebyshev Polynomials The Legendre polynomials are the result of applying the Gram-Schmidt idea (Section 4.4). The plan is to orthogonalize the powers 1, z,z?,... To start, the odd function z is already orthogonal to the even function 1 over the interval from 1 to 1. Their product (z)(1) =z integrates to zero. But the inner product between 2% and 1 is [ 2% dz = 2/3: 2’ 1 24 2 : 1 (z,1) _ [z’ dz — ﬁ = % Gram-Schmidt chooses z3 — 3 = Legendre (1,1) [ldr 2 Similarly the odd power z3 has a component 3z /5 in the direction of the odd function z : 3, ‘dr 2 3 ((:1;’ :;) f; di 2;3 g Gram-Schmidt chooses 3 — gz = Legendre Continuing Gram-Schmidt for z4,z°,... produces the orthogonal Legendre functions. Finally we turn to the Chebyshev polynomials 1, z, 22 — 1,4x3 — 3z. They don't come from Gram-Schmidt. Instead they are connected to 1,cos@,cos 26, cos36. This gives a giant computational advantage—we can use the Fast Fourier Transform. The connection of Chebyshev to Fourier appears when we set £ = cos 0 Chebyshev 212 —1 =2(cosf)? —1 = cos 26 to Fourier 43 — 3z = 4(cosh)? — 3(cosf) = cos 30 The n*? degree Chebyshev polynomial T}, () converts to cos nd = Ty, (cos 8). Note These polynomials are the basis for a big software project called “chebfun”. Every function f(z) is replaced by a super-accurate Chebyshev approximation. Then you can integrate f(z), and solve f(x) = 0, and find its maximum or minimum. More than that, you can solve differential equations involving f(z)—fast and to high accuracy. When chebfun replaces f(z) by a polynomial, you are ready to solve problems. ® REVIEW OF THE KEY IDEAS = 1. A basis is good if its matrix B is well-conditioned. Orthogonal bases are best. 2. Also good if A = B~ AB is diagonal. But the Jordan form J is very unstable. 3. The Fourier matrix diagonalizes constant-coefficient periodic equations : perfection. 4. The basis 1, z,z2, ... leads to BT B = Hillbert matrix: Terrible for computations. S. Legendre and Chebyshev polynomials are excellent bases for function space. 334 Chapter 8. Linear Transformations Problem Set 8.3 1 In Example 1, what is the rank of J — 31 ? What is the dimension of its nullspace ? This dimension gives the number of independent eigenvectors for A = 3. The algebraic multiplicity is 2, because det (J — AJ) has the repeated factor (A —3)2. The geometric multiplicity is 1, because there is only 1 independent eigenvector. 2 These matrices A; and A, are similar to J. Solve A, B; = B1J and A; By = ByJ to find the basis matrices B; and B, with J = Bl‘lAlBl and J = B{lAng. 01 0 4 4 -8 trace 0 + 0 J—[OOJ Al—[o 0] A2—[2 _4] determinant 0 3 This transpose block JT has the same triple eigenvalue 2 (with only one eigenvector) as J. Find the basis change B so that J = B~'JT B (which means BJ = JTB): (2 1 0] 2 0 0] J=10 2 1 JT=11 2 0 _OO2J 012J 4 J and K are Jordan forms with the same zero eigenvalues and the same rank 2. But show that no invertible B solves BK = JB, so K is not similar to J : JT is similar to J=B\"1JTB 010 0] [0 0 ] Different 1 0 010 0 0 block sizes ! 0 - - L 5 If A3 = 0 show that all A = 0, and all Jordan blocks with J3 = 0 have size 1,2, or 3. It follows that rank (A) < 2n/3. If A\" =0whyisrank (A) <n? te A A1 0 6 Show that u(t solves — =JuwithJ = 0 A\\ and u(0) = 1 J is not diagonalizable so te* enters the solution to du/dt = Ju. 7 Show that the difference equation viy2 — 2A\\Uk4+1 + A2vr = O is solved by vr = AF and also by vx = kA*. Those correspond to e* and te*t in Problem 6. 8 What are the 3 solutions to A3 = 1 ? They are complex numbers A = cosf+isinf = e'%. Then A3 = €3 = 1 when the angle 30 is O or 27 or 47. Write the 3 by 3 Fourier matrix F with columns (1, A, A?). 9 Check that any 3 by 3 circulant C has eigenvectors (1, A\\, A?) from Problem 8. If the rows of your matrix C' contain cp,c;,c2 then its eigenvalues are in F c. lfor—L< z <L 10 Using formula(7) find a3 cos 3z in the Fourier series of f(z) = { Ofor L<|z|<2r Chapter 9 Linear Algebra in Optimization 9.1 Minimizing a Multivariable Function 9.2 Backpropagation and Stochastic Gradient Descent 9.3 Constraints, Lagrange Multipliers, Minimum Norms 94 Linear Programming, Game Theory, and Duality This chapter combines calculus with linear algebra. The overall problem is to minimize a function F'(x) that depends on many variables * = (z,,z2,...,Z,). Suppose we know the first denivatives of F'—those are partial derivatives and they go into the gradient vector VF = (0F/0x,,...,0F/0z,). Then at a “smooth” minimum point * of F(x), calculus tells us that V F(x*) = 0. That is a fundamental problem: To solve n equations VF(z) = 0 with n unknowns z; to z,,. An important model problem has a quadratic function Q(z) = %:cTS z — b'z. In this case the gradient vectoris VQ(x) = Sz — b. So VQ = 0 is a set of n linear equations Sz = b. When S is a symmetric positive definite (constant) matrix, the solution £* will be the minimum point for Q(x). When F is not quadratic, its gradient V F is not linear and its second derivative matrix S(x) is not constant. But if S(x) is everywhere positive definite, then F is a convex function. Its graph is still (roughly) a bowl. Section 9.1 follows —V F down towards z*. And Section 9.2 shows how to compute that gradient V F' by “backpropagation”. There is another serious possibility to deal with in 9.3 and 9.4. There can be restrictions (constraints) on the allowed solutions . A model problem could minimize a quadratic Q(x) or a linear cost ¢z with the constraints Az = b and > 0 (meaning that every r; > 0). Now the graph of Q(x) (the bowl) is chopped off by m vertical planes z; = 0. The key problem is to know if z; > 0 or z; = 0 at the minimum. The way to deal with a constraint like £, > 0 is to introduce a Lagrange multiplier A;. Before that, Sections 9.1-2 explain gradient descent and stochastic gradient descent. Those are the workhorses of deep learning in Chapter 10, with many variables in . 335 336 Chapter 9. Linear Algebra in Optimization 9.1 Minimizing a Multivariable Function Suppose F(zxy,...,x,) is a function of n variables. We need basic information about the first and second denivatives of F'. Those are “partial derivatives” when n > 1. The important facts are in equations (1) and (2). I don’t believe you need a whole course (too much about integrals) to use these facts in minimizing the function F(z). 2 F(z + Az) = F(z) + Az aF (x) + L (Azx)? a°F ()| (1) dx 2 dx? One function F’ One variable x This is the beginning of a Taylor series—and we don’t often go beyond that second order term. The first terms F'(r)+ (Ax)(dF /dr) give a first order approximation to F'(r+ Axr), using information at z. Then 1/2 (Ax)2F // makes it a second order approximation. The Ar term gives a point on the tangent line—tangent to the graph of F'(x). The (Ar)? term moves from the tangent line to the “tangent parabola”. The function F will be convex—its slope increases and its graph bends upward, as in y = x2—when the second derivative is positive : d?F /dz? > 0. Equation (2) lifts (1) into n dimensions. One function F' 1 Variables 2, toz,, & +AT) R F (x)+(Az)T VF+2 (Az)T S (Az) | ) This is the important formula! The vector V F is the gradient of F'—the column vector of n partial derivatives 0F /0x, to OF /Ox,. S is the symmetric matrix (Hessian matrix) of second derivatives S;; = 0°F/0z; 0z; = 0°F |0z Ox;. The graph of y = F(x,,...,z,) is now a surface in (n + 1)-dimensional space. The tangent line becomes a tangent plane at x. When the second derivative matrix S is positive definite, F' is a strictly convex function: it stays above its tangents. A convex function F has a minimum at z* if VF(x*) = 0: n equations for x*. This third formula from calculus goes directly to our vector equation VF(x) = 0. Starting from a point , we want to take a step Ax that brings us close to the solution. VF(z4+Az)=VF(z)4+S(z) Ax =0 leadsto Az =—- S~ 1(z)VF(z)| (3) That formula for Az tells us one step in Newton’s Method. We are at a point x, and we move to Tr41 = Tx + Az. The points g, 1, T2, . . . often converge quickly to the minimum point £* where VF(x*) = 0. But the cost of fast convergence can be very high, to compute at each step all the second derivatives 9° F /dz;0z; in the matrix S(z). Example 1 Our model function F(z) = 32TSx — 2Tb has VF = Sz —b. Newton’s method using equation (3) solves VF(z*) = Sz* — b = 0 in one step! z) — o =Ax=—-S\"1VF(xz¢) = -S~!(Szo — b) = —x0 + S~ 1b Cancel —x to see the point £; where VF = 0: itisx; = S~1b = z\". 9.1. Minimizing a Multivariable Function 337 Example 2 Apply Newton’s method to solve VF = 12 — 4 = 0. The derivative is S(z) = 2z. Step kis Az = —(1/S(zx))VF(zx): 1 4 Tkt1 — Tk = (—=1/2zk) (2 —4) of Tpqr = 5 (tk + —) : (4) Tk The square root is £* = v/4 = 2 = 1 (2 + 3). The key point is fast convergence to z*. Start from o = 4: :c—l(:c+4 l4-+-4 = 2.9 T\\ 4, 2 4) — “ 1 4 1 4 1 Ty = - —)==(25+—)==(25+16)=2. 2 2(x1+xl) 2(5+2.5) 5 (25+16) = 2.05 If you take 2 more steps, you see how quickly the error (the distance from V4 = 2) goes to zero. The count of zeros after the decimal point in zx — 2 doubles at every step. o =4 ) = 2.5 z2 = 2.05 z3 = 2.0006 z4 = 2.000000009 The wrong decimal is twice as far out at each step. The error x; — 2 is squared : 1 4 1 1 wei =2 =3 (ot =) =22 2 @on =22 [llewa - 2l % fllon - 27| Squaring the error explains the speed of Newton's method—provided x;. is close to x*. Summary Newton’s method is fast near the true solution x*, because it uses the second derivatives of F(x). But those can be too expensive to compute—especially in high dimensions. Often neural networks are simply too large to use all the second denivatives of F'(x). Gradient descent is the algorithm of choice for deep learning. Gradient Descent = Steepest Descent This section is about a fundamental problem: Minimize a function F(x1,...,Zn). Calculus teaches us that all the first derivatives 0F /0z; are zero at the minimum (when F is smooth). If we have n = 20 unknowns (a small number in deep learning), then minimizing one function F produces 20 equations 0F /0x; = 0. “Gradient descent” uses the derivatives OF [Ozx; to find a direction that reduces F(x). The steepest direction, in which F'(x) decreases fastest at xj, is given by the negative gradient —V F(xy): Steepest Direction Gradientdescent xj41 = xx — s VF(xi) ) (5) is a vector equation for each step k = 1,2,3,... and s is the stepsize or the learning rate. We hope to move toward the point * where the graph of F(x) hits bottom. 338 Chapter 9. Linear Algebra in Optimization We are willing to assume for now that 20 first derivatives exist and can be computed. We are not willing to assume that those 20 functions also have 20 convenient derivatives 0/0z;(OF /Oz;). Those are the 210 second derivatives of F—which go into a 20 by 20 symmetric matrix S. (Symmetry reduces n? = 400 to %(nz + n) = 210 denivatives.) The second derivatives would be very useful extra information, but in many problems we have to go without. You should know that 20 first derivatives and 210 second derivatives don’t multiply the computing cost by 20 and 210. The neat idea of automatic differentiation—rediscovered and extended as backpropagation in machine learning—makes the cost much smaller. Return for a moment to equation (5). The step —s,V f(xx) includes a minus sign (to descend) and a factor si (to control the the stepsize) and the downhill vector VF (containing the first derivatives of F' computed at the current point k). A lot of thought and computational experience have gone into the choice of stepsize and search direction. The Derivative of f(x) : n =1 The derivative of f(x) involves a limit—this is the key difference between calculus and algebra. We are comparing the values of f at two nearby points r and x + Az, as Azx approaches zero. More accurately, we are watching the slope A f /Ax between two points on the graph of f(r): (6) d A Azx) — Derivative of f at x —f = limit of —f = limit of f(z + Az) — f(z) : dx Azx Az This is a forward difference when Az > 0. It is a backward difference when Az < 0. When we have the same limit from both sides, that number is the slope of the graph at z. The ramp function ReLU(z) = f(z) =max(0, x) is heavily involved in deep learning. ReLU has unequal slopes 1 to the right and O to the left of x = 0. So the derivative of ReLU does not exist at that corner point in the graph. For the smooth function f(x) = x2, the ratio Af/Ax will safely approach the derivative df /dz from both sides. But the approach could be slow (just first order). Look again at the point z = 0, where the true derivative df /dr = 2z is now zero: Af f(az) - £(0) _ (Az)? -0 ¥ _, 111 t. — at - . = imit = — = . e ratio Az atr =0 1s e e Ax Then limit Iz As explained in Section 2.5, we get a better ratio (closer to the limiting slope df /dz) by averaging the forward difference (where Az > 0) with the backward difference (where Az < 0). Their average is a more accurate centered difference. Centered 1 [ f(z+Az)-f(z) . f(z—Az)- f(a:)] _f(z+Azx) - f(z—Ax) o atz 2 Az ~Ar B 2Acx For the example f(z) = z2 this centering will produce the exact derivative df /dz = 2z. In the picture we are averaging plus and minus slopes to get the correct slope 0 at z = 0. 9.1. Minimizing a Multivaniable Function 339 For all smooth functions, the centered differences reduce the error to size (Ax)2. This is a big improvement over the error of size Az for uncentered differences f(z + Az) — f(z). centered flz)=0 A f(z)=z slope 0 slope 1 backward forward Figure 9.1: ReLU function = Ramp from deep learning. Centered slope of f =z? is exact. Most finite difference approximations are centered for extra accuracy. But we are still dividing by a small number 2 Az. And for a multivariable function F(z,, z3,...,T,) we will need ratios AF /Azx; in n different directions—possibly for large n. Those ratios approximate the n partial derivatives that go into the gradient vector grad F = V F. , , 8F OF 0x, o, Its components are the n partial derivatives of F. V F points in the steepest direction. Examples 3-5 will show the value of vector notation (V F is always a column vector). Example 3 For a constant column vector @, F(x) = aTz has gradient VF =a. The partial derivatives of F = a1, + - - - + a, I, are the numbers OF /Oxi = ax. Example 4 For a symmetric matrix S, the gradient of F' = %zTSz is VF = Sz. To see this for n = 2, write out the function F'(z;, z2). The matrix S is 2 by 2: o[ xg][a b] [zl]z%azf%—%cx% [Bf/axl]_[aa:l+b;rg]_s[:c1]. 2 b c| |z + bz 29 Of /0zy| by +cxa| | X3 Example 5 For a positive definite symmetric matrix S, the minimum of a quadratic F(z) = 1 7Sz — a'z is the negative number Fip = —3 TS 'a. This is an important example! The gradientis VF = Sz — a. The minimum occurs at x* = S—1a where first derivatives of F are zero: I BF/Ba:l ] VF = =Sr—a=0at z*=S5\"'a =argminF. (8) OF [0z, | As always, that notation arg min F stands for the point £* where the minimum of F(x) is reached. Often we are more interested in this minimizing point £* than in the actual minimum value Fpy, = F(z*) at that point. 340 Chapter 9. Linear Algebra in Optimization 1 Fpin is -;—(S\"a)TS(S‘la) ~aT(S7'a) = %aTS‘la —a'S7'a= —EaTS‘la. The graph of F is a bowl passing through zero at £ = 0 and dipping to Fmin at o*. Example 6 The determinant F(z) = det X is a function of all n? variables Tij. In the formula for det X, each z,; along a row is multiplied by its “cofactor” C;;. This cofactor in Chapter 5 is a determinant of size n — 1, using all the entries in X except row ¢ and column j—and C;; also includes the plus or minus sign (—1)**+7 : J(det X) o = C;; inthe matrix C of cofactors of X . ¥ Partial derivatives VF = Example 7 The logarithm of the determinant is a most remarkable function : L ii L(X) = log (det X) has partial derivatives 9 ¢ tentry of X1, i B:z:ij det X —J The chain rule for L=log F is (OL/0F)(0F [0xi;)=(1/F)(0F /0r;;) = (1/det X) C;;. Then this ratio of cofactor C;; to det X gives the j, 1 entry of the inverse matrix X1, It is neat that X~ contains the n? first derivatives of L = logdet X. The second derivatives of L are remarkable too. We have n? variables z;; and n? first derivatives in VL = (X~ ')T. This means n* second derivatives! What is amazing is that the matrix of second derivatives is negative definite when X = S is symmetric positive definite. So we reverse the sign of L: positive definite second derivatives = convex function. — log (det S) is a convex function of the entries of the positive definite matrix S The Geometry of the Gradient Vector V F Start with a function F'(z, y) of n = 2 variables. Its gradientis VF = (8 F/8x,8F/ dy). This vector changes length as we move the point z, y where the derivatives are computed : F OF OF\\* [(O6F\\? VF = (E’ B—y) Length =||VF|| = \\/(Ez_) +(—6;) = steepest slope of F That length ||V F|| tells us the steepness of the graph of z = F(z,y). The graph is normally a curved surface—like a valley in zyz space. At each point there is a slope OF [0z in the z-direction and a slope 0F /3y in the y-direction. The steepest slope is in the direction of VF = grad F. The magnitude of that steepest slope is ||V F||. Example 8 The graph of a linear function F(z,y) = az + by is the plane z = qz + by. The gradient is the vector VF = of partial derivatives. The length of that vector 5 a b IVF|| = va% + b? = slope of the roof. The slope is steepest in the direction of Y F. 9.1. Minimizing a Multivariable Function 341 That steepest direction is perpendicular to the level direction. The level direction 2 = constant has az + by = constant. It is the safe direction to walk, perpendicularto V F. The component of V F' in that flat direction is zero. Figure 9.2 shows the two perpendicular directions (level and steepest) on the plane z = F(z,y) = ¢ + 2y. 1 ) -vr slope is ||V F|| = v/5 in this direction steepest direction [ negative gradient -V F level direction [ _f ] =(VF)* F = z + 2y is constant on this line slope is —v/5 in this direction Figure 9.2: The negative gradient —V F' gives the direction of steepest descent. Our basic Example 9 will be F(z,y) = az? + by?. Its gradientis VF = [ L;z;c ] VF tells us the steepest direction, changing from point to point. We are on a curved surface (a bowl opening upward). The bottom of the bowl is at z = y = 0 where the gradient vector is zero. The slope in the steepest direction is ||V F||. At the minimum, VF = (2azx,2by) = (0,0) and slope = zero. The bowl is circular if a = b. The level direction has z = az? + by? = constant height. That plane z = constant cuts through the bowl in a level curve. In this example the level curve ar? + by = cisan ellipse. But there is a serious difficulty for steepest descent:: The steepest direction changes as you go down ! The gradient doesn’t point to the bottom ! z steepest direction V F up and down the bowl az? + by* = 2 \\g flat direction (V F)+ along the ellipse az? + by? = constant - Yy / o The steepest direction is perpendicular to the flat direction but z the steepest direction is not aimed at the minimum point Figure 9.3: Steepest descent moves down the bowl in the gradient direction [ :ga:c] : Let me repeat. At the point Zg, yo the gradient direction for F = ax? + by? is along YF = (2azo, 2byg). The steepest line is (z,y) = (zo, yo) — s(2axo, 2byo) for all s. But if a # b, the lowest point (z,y) = (0, 0) does not lie on that line for any s. 342 Chapter 9. Linear Algebra in Optimization We will not find that minimum point (0, 0) in one step of “gradient descent”, The steepest direction does not lead to the bottom of the bowl—except when b = a and the bowl is circular. Water changes direction as it goes down a mountain. Sooner or later, we must change direction too. In practice we keep going in the gradient direction and stop when our cost function F' is not decreasing quickly. At that point Step 1 ends and we recompute the gradient V F. This gives a new descent direction for Step 2. An Important Example with Zig-Zag The example F(z.y) = %(:1:2 + by?) is extremely useful for 0 < b < 1. Its gradient V F has two components 0F /0r = x and 0F /0y = by. The minimum value of F is zero. That minimum is reached at the point (z*,y*) = (0,0). Best of all, steepest descent with exact line search (the best s) produces a simple formula for each (rk. yx) in the'slow progress down the bowl toward (0, 0). Starting at (xg.yo) = (b, 1) we find these points: b—1\\\" 1-b\\\" 1 —b\\32%k —b (- —~ F(zr,ye)=| —— | F(xo, 9 Lk = (b | 1) Yk (1 I b) (Tky Yi) (1 I b) (o, Yo) If b = 1, you see immediate success in one step. The point (z,, y;) is (0, 0). The bowl 1s perfectly circular with F' = %(12 + y?). The negative gradient direction goes exactly through (0,0). Then the first step of gradient descent finds that correct minimizing point. The real purpose of this example is seen when b is small. The crucial ratio in equation (9) is r = (1 — b)/(1 +b). For b = {5 this ratio is r = 9/11. Forb = 15 the ratio is 99/101. The ratio is approaching 1 and the progress toward (0,0) has virtually stopped when b is very small. Figure 9.4 shows the frustrating zig-zag pattern of the steps toward (0, 0). Every step is short and progress is very slow. This is a case where the stepsize si in Ty = Tk — sk V F(x;) was exactly chosen to minimize F' (an exact line search). But the direction of —V F, even if steepest at (zx, yk ), is pointing far from the final answer (z*, y*) = (0,0). The bowl has become a narrow valley when b is small. We are uselessly crossing the valley instead of moving down the valley to the bottom at (0, 0). Momentum and the Path of a Heavy Ball The slow zig-zag path of steepest descent is a real problem. We have to improve it. Our model example F = %(:ﬁ:2 + by?) has only two variables z, y and its second derivative matrix is diagonal—constant entries Fy, = 1 and Fy,,, = b. But it shows the zig-zag problem very clearly when b = Apin / Amax is small. Key idea: Zig-zag would not happen for a heavy ball rolling downhill. Its momentum carries it through the narrow valley—bumping the sides but moving mostly forward. So we add momentum with coefficient 3 to the gradient (Polyak’s important idea). This gives one of the most convenient and potentially useful ideas in deep learning. 9.1. Minimizing a Multivariable Function 343 Gradient Descent The first descent step starts out perpendicular to the level set. As it crosses through lower level sets, the function F(z,y) is decreasing. Eventually its path is tangent to a level set L. Descent has stopped. Going further will increase F. The first step ends. The next step is perpendicular to L. So the zig-zag path took a 90 ° turn. Note to the reader Deep learning in Chapter 10 in- volves a sum of many functions F(x,v;)—one for every vector v; in the training set. The the gradient is a sum of many gradients—too many to compute at each step. The solution in Section 9.2 is to randomly (“stochastically”) choose only one or a few functions at every descent step. Figure 9.4: Slow convergence on a zig-zag path to the minimum of F = x2 + by?. Key idea: The direction 2z, of the new step remembers the previous direction 2. Descent with momentum |Tr41 = Tk — 82, with 2, = VF(xx) + Bzk-1| (10) Now we have two coefficients to choose—the stepsize s and also 3. Most important, the step to x,+1 in equation (10) involves 2;_;. Momentum has turned a one-step method (gradient descent) into a two-step method. To get back to one step, we have to rewrite equation (10) as two equations (one vector equation) for (z,z) at time k + 1: Descent with Tk+1 = T — 82k momentum Zk+1 — VF(zi4) = Bz (11) With those two equations, we have recovered a one-step method. This is exactly like re- ducing a single second order differential equation to a system of two first order equations. The Quadratic Model When F(x) = %:cTSa: is quadratic, its gradient VF = Sz is linear. This is the model problem to understand: S is symmetric positive definite and V F (x4, ) becomes STy in equation (11). Our 2 by 2 supermodel is included, when the matrix S is diagonal with entries 1 and b. For every matrix S, you will see that its largest and smallest eigenvalues determine the best choices for 8 and the stepsize s—so our 2 by 2 case actually contains the essence of the whole problem. To follow the steps of accelerated descent, track each eigenvector q of S. Suppose Sq = Agand xy = crqand z; = drqand VF; = Sxx = Ack q. Then our equation (11) connects the numbers cx and di at step k to the next cx4+, and di4; at step k+ 1. 344 Chapter 9. Linear Algebra in Optimization eigenvectorq —Acik41 + di41 = Bde |—=A 1]||drsr 0 pB Finally we invert the first matrix (— A becomes + ) to see each descent step clearly : Descent step ck+1| |1 Of|1 —sffex| |1 —8 Ck | _ R Ck (13) multipliesby R [{dx+1| |[A 1]|0 Blldc| |A B — As || dk dy. After k steps the starting vector is multiplied by R*. For fast convergence to zero (which is the minimum of F' = %zTSz) we want both eigenvalues e; and e; of R to be as small as possible. Clearly those eigenvalues of R depend on the eigenvalue A of S. That eigenvalue A could be anywhere between A . (S) and Amax(S). Our problem is: Following the Ck+1 = cx — 8dj 1 O)jckyr1| |1 —s8||ck = di (12) It seems a miracle that this problem has a beautiful solution. The optimal s and /3 are 2 2 s = 2 and §=|Y2max = VAmin | (14) ’\\mm vV Amax + )‘min Think of the 2 by 2 supermodel, when S has eigenvalues Amax = 1 and A\\;, = b: 2 2 ) () These choices of stepsize and momentum give a convergence rate that looks like the rate in equation (9) for ordinary steepest descent (no momentum). But there is a crucial difference : The number b in (9) is replaced by v/b in (15). (16) 2 2 Ordinary 1-b Accelerated 1—-vb descent factor 140 descent factor 1+ Vb So similar but so different. The real test comes when b is very small. Then the ordinary descent factor is essentially 1 — 4b, very close to 1. The accelerated descent factor is essentially 1 — 4v/b, much further below 1. To emphasize the improvement that momentum brings, suppose b = 1/100. Then vb = 1/10 (ten times larger than b). The convergence factors in equation (16) are 1.01 1.1 Ten steps of ordinary descent multiply the starting error by 0.67. This is matched by a single momentum step. Ten steps with the momentum term multiply the error by 0.018. Notice that the condition number Amax/Ayin, = 1/b of S controls everything. 99 \\? 9?2 Steepest descent ~ .96 Accelerated descent — | = .67 9.1. Minimizing a Multivariable Function 345 Note 1 Nesterov also created an improvement on gradient descent—different from momentum. Note 2 “‘Adaptive” methods use all earlier choices of the step direction to modify the current direction V L for the step to 4. The formula for the current direction becomes D) = 0Dk_y + (1 — 8)V Ly. Class projects confirmed the speedup that comes from using the earlier directions which went into Dj._;. The batch size B in stochastic gradient descent can increase at later steps. Problem Set 9.1 1 For which functions F'(z) are equations (1) and (2) exactly correct for all x ? 2 Write down Newton’s method for the equation 2 + 1 = 0. It can’t converge (there is no real solution). If z,, = cos 8/ sin @ show that z,,,; = cos 26/sin26. This is an example of “‘chaos” in College Math J. 22 (1991) 3-12. 3 The determinant of a 1 by 1 matrix is just det X = z;;. Find the first and second derivatives of F'(X) = — log(det X) = —logz,; for £1; > 0. Sketch the graph of F = — log z to see that this function F is convex. 4 A symmetric 2 by 2 matrix has det(A) = ac — b%. Show from the matrix of second derivatives that F = — log (ac — b?) is a convex function of a, b, c. 5 What is the gradient descent equation Tx4+) = &) — 8V F(xx) for the least squares problem of minimizing F(x) = 3||Az — b|?? 6 Find the gradient of F and the gradient descent equations for F'(z,y) = % (z? + %y:'). Starting from (o, o) = (3, 1) confirm or improve equation (9) for (1. y1). 7 Add momentum to Problem 6 and take one “accelerated” step : equations (10)—(11). 8 Show that this non-quadratic example has its minimumatz =0 and y = +20: 1 T 1 0 Take one gradient descent step from (zo, yo) = (1,1). 346 Chapter 9. Linear Algebra in Optimization 9.2 Backpropagation and Stochastic Gradient Descent This short section explains the key computational step in deep learning. The goal of that step is to optimize the set of weights x in the learning function F'(x.v). We are given the “training data”—a large set of input vectors vy, . .., vn. For each input vector v; we know the correct output vector w;. Our goal is to choose the weights x so that the error vectors e; = F(x.v;) — w; are as small as possible. We are fitting the learning function F to the N points of training data, by choosing weights that minimize the total loss L(x): N N Minimize L(z)= %Zl(ei) = %ZZ(F(m,vi) —w;). (1) 1=1 1i=1 The best weights x will depend on the form we choose for the learning function F(x, v). The weights also depend on the loss function £ that we choose to measure the errors e;. If we choose the “square loss™ £(e) = ||e||?, our problem is to find a least squares fit of the training data (v, w;),..., (VN,wWyN). Here is a big picture of the problem. We are creating a function F' that approximates or even “interpolates” the training data: F(v;) = w; fort = 1,..., N. Then when we have new test data V', we will estimate the unknown output W by our function F(V). Suppose we are trying to read a handwritten number W. We have N pictures v,,...,vN of known numbers w, ..., wy. Based on that training data, how do we weight the pixels in the new picture V to learn the number W in that picture ? Interpolation is an old problem. Deep learning 1s a new approach—much more suc- cessful than past approaches. One of the worst approaches is to fit the data by a very high degree polynomial F. The result is extremely unstable. A small change in the input V produces a large change in the output W = F(V'). We have chosen the wrong form (a polynomial) for the learning function F'. Chapter 10 will describe a much better choice for the form of F'. This section is about “fitting the function F' to the training data”. We are choosing the weights x to achieve F'(x,v;) = w; for each data point. Three big steps are involved in constructing and using the learning function F : 1 Choose the form of F(z, v). It includes a nonlinear function, often “ReLLU”. 2 Compute the weights z in F' that best fit the training data F'(x, v;) = w;. 3 Using those weights, evaluate W = F(z, V') for new test data V. Step 1 is the goal of Chapter 10 in this book. We will create a “neural net” to describe the form of F. The key point is that F' is produced by a chain of simpler functions F} to FL : F(a:,v) = FL(FL—I(- . (Fg(Fl('v))))) (2) Each function F'i in that chain has its own set of weights ;. You will see in Chapter 10 that the individual functions F'x have a simple piecewise linear form. It is the chain of functions F'; to F' 1 that produces a powerful learning function F'. Section 10.1 will count the (large) number of linear pieces of F' created by ReLU. 9.2. Backpropagation and Stochastic Gradient Descent 347 This section describes the backpropagation algorithm that efficiently finds the denva- tives of F' with respect to the weight matrices x; to z, in the functions F'; to F';. By knowing those derivatives 0F ;. /0x, we can use gradient descent (or stochastic gradient descent) to optimize the weights. So once the form of F' is chosen, deep learning is all about computing the weights x that minimize a sum of losses £(x, v;): Loss function £(x,v;) = F(z,v;) — w; = approximate — true A personal note. 1 am greatly in debt to two friends who explainéd the computation of the gradients VF and V£ (the derivatives of the loss with respect to the weights). One is Alexander Craig, who was a student in my Math 18.065 class at MIT. His project for the class was backpropagation, and his report was wonderful. Alex shared in the writing of this section 9.2. My other source of support was Andreas Griewank, one of the principal creators of “automatic differentiation”. This algorithm finds the derivatives of F' and £ with respect to each of the weights with amazing speed. Applied recursively to the chain of L functions F = F(Fr-1(...F,)) this is backpropagation. The reader will know that based on this computational engine, deep learning has been an overwhelming success. The Multivariable Chain Rule We begin with the chain rule for a composite function h(xz) = f(g(x)) of several input variables z,,...,x,. The outputs from g(x) have n components g,,....g,. Then the outputs from h(x) = f(g(x)) have p components hj,...,hy. The Jacobian matrices O0f/0g and 8g/Ozx contain the partial derivatives of f and g separately : Oh | Oh T Oq1 . O¢ a 6 n a 6 m af | V' ’ ag | 7 . = ; ; —=| : (3) Og Ox Ofp ... O Ogn .. O | 91 Ogn | Oy OIm Each h; depends on the g’s and each g; depends on the z’s. Therefore h; depends on Ty,...,Zm. The chain rule aims to find all the derivatives Oh;/0zi. The scalar rule is a dot product (row i of 8f/8g) - (column k of 8g/8x). The multivariable chain rule for Oh/Ox is exactly the matrix product (8f/8g) (8g/0x): Oh; Oh; Og Oh; 99, _ . 8f 8g ox) 55 Oz Tt Bgn Oz (I‘O\\Hof B—g) . (coluan of 5;) 4) The learning function F' is a chain of functions F'; to F';. Then the matrix chain rule for the z-derivatives of Fi (Fr—1(. .. F1(v))) becomes a product of L matrices: F - Matrix Chain Rule _8__ _ OF OF., OF, = coo —m 5 Oz 8FL_1 GFL_Q Oz () 348 Chapter 9. Linear Algebra in Optimization Stochastic Gradient Descent Before we compute derivatives by backpropagation, there is an important decision to make. The loss function L(x) that we minimize is a sum of individual errors £( F'(x,v;) — w;) for every sample v, in the training set. The derivative of F'(x, v,) for one sample is hard enough ! Computing denivatives for every sample with respect to every weight at every step of steepest descent is an enormous task. We have to reduce this computation. Stochastic gradient descent randomly selects one sample input or a small batch of B samples, to improve the weights from x4 to xx1. This works surprisingly well. Often the first descent steps show big improvements in the weights x. Close to the optimal weights, the iterations can become erratic and we may not see fast convergence—but high precision is not needed or expected. Often we can stop the descent iterations early. SGD reduces the cost of each gradient step (from weights T to Tx41) to an acceptable level. SGD for Ordinary Least Squares Least squares provides an excellent (and optional) test of this “stochastic”’ idea. We are minimizing || Az — b||?. At each step we randomly choose one sample row a; of A. We adjust the current approximation &y so the column x4, ; solves equation i of Ax = b: Tiel = Tk + bt||a:|1|1::k a;r solves a;xryy = b;. (6) Kaczmarz introduced this method, thinking to cycle over and over from the first row of A to the last row. Strohmer and Vershynin changed to randomly chosen rows a; (stochastic descent). They proved a form of exponential convergence to the solution of the least squares equation AT AZ = ATb. A key point in the convergence shows up in Figure 9.5 : Fast start toward the best ** and oscillating finish. So stop early. few steps BN 1 102 | ™ : \\ ~ Loss Function L ) % 100 3“_“‘\\ many e steps A= 102 /M NPRAAANIGEN A DG 5 10 15 20 25 30 35 40 45 Iterations Figure 9.5: The left figure shows a trajectory of stochastic gradient descent with two unknowns. The early iterations succeed but later iterations oscillate (as shown in the inset). On the right, the quadratic cost function decreases quickly at first and then fluctuates instead of converging. The four paths in that graph start from the same x¢ with random choices of i as in equation (6). 9.2. Backpropagation and Stochastic Gradient Descent 349 The Key Ideas of Backpropagation For the present, we keep the idea simple and the notation simple. Our learning function F' depends on the weights x (the matrices A; to Ay and the vectors b; to b, at layers 1 to L). The special simplifying feature is that each weight enters only one of the functions F'; to F', in the chain. We will wnte &; for the matrix A; and vector b; that enter layer j : F; = ReLU(A,;F;-1 + b;). ReLU is a famously simple function—see Figure 10.1. The chain rule for the derivative of the loss £ with respect to the weights x; in F'; is 2@__ 4 . OF. OBFL_l WBFJ-H fa_Fl 7 6;z:j B aFL BFL_l aFL_z 6Fj 6$j The key point is that this involves only the weights z; to z.—not earlier weights. So the efficient computation starts with 9¢/0F - 0FL/0F_, (involving Ap and bp). Then the next step introduces only the weights Ay, and by _; in £ _;. We have every reason to choose the backward mode in this differentiation : Computing 9 F /O involves only the last weights from z; to . Remember that the weights &, include the matrix A; and the vector b; . The derivative of F; with respect to A; will need three components (this makes it a tensor). We will compute that derivative a few pages onward. Here we take the final step from the gradient of F' to the gradient of the loss £. This step displays the second big reward for computing successive dernivatives backward from the output. Second key point: The loss €(x, v) is a scalar function of the output vector F'(x.v). In the simplest case of a “square loss”, that function is the squared length of the error: 04 Loss: ¢(z) = (w - F(x,v))\" (w - F(x,v)) and 5F 2(w - F(x.v)) (8) Here w is the true (desired) output from the original training sample v, and F'(x.v) is the computed output at layer L using the weights. The key point for this and other choices of £ is that €/OF is a vector. Then the backward chain of matrices My = 0Fk/0F_, multiplies that vector. At every step this multiplication becomes a vector times a matrix ! (vector) (matrix) ot 4]4 | not (matrix) (matrix) dx; (((ﬁML) ML-I) '“MJH) (9) Suppose for example that each matrix M is n by n. Then each vector-matrix step in the chain needs n? multiplications. If we choose the opposite order, with matrix-matrix steps, those would have n3 multiplications. The backward order (L first) is very much faster. Summary Reverse-mode differentiation is more efficient than forward-mode for two key reasons : 1. Most operations are vector-matrix multiplications (or vector-tensor). 2. Work will be reused for the remaining derivative computations. 350 Chapter 9. Linear Algebra in Optimization Reverse mode differentiation achieves these efficiencies by rearranging the parentheses in the chain rule from left to nght—from the loss function at the output back to layer j: o6 ((( 06 OF, \\ OFL1\\ 0F.\\ OF Back-Prop ab,-\"(((aFL aF,,_l) aFL_z) aFJ-) a6, | (10 Since 0€/0F] is a vector, our intermediate results are mostly vectors. So we are performing vector-matrix multiplications. When we compute 8¢/0A; we cannot avoid a single vector-tensor contraction at the end—because A is a matrix. Section 10.2 computes that matnix BFJ/BA, Summary In addition to this speed-up for every operation, reverse-mode also re- quires fewer total operations because it allows us to reuse work in a remarkably elegant way. The key insight here is that the chain rule expressions for all derivatives at layers J < k share the same prefix 0€/0F}. Since reverse-mode groups the terms from the left, it can reuse 9£/0F in all computations for layers 7 < k. Backpropagation uses this optimization at every layer, resulting in the following algorithm: To 1nitialize, perform a forward pass through the chain using input v, and store all intermediate results F}; . Also, compute 0¢/0F . Then, forj=L—>1do Compute and store o¢ 0 OF; o€ 0 OF; and ob; OF; 0b; 8A; OF, 0A, if 7 # 1 then Compute 0€/0F;_, to be used at the next iteration : ot 0t OF; OFj_1 OF; OF;_; end if end for Thus backpropagation computes all the required derivatives in a single backward pass through the network. It requires only 2L vector-matrix multiplies and L vector-tensor contractions. To compute the same values, forward-mode differentiation would require O(L?) matrix-tensor contractions and matrix-matrix multiplies. Backpropagation is more efficient by multiple orders of magnitude. Backpropagation in Keras and TensorFlow Keras is one of the most popular machine learning libraries available today. It provides an simple interface to neural networks in Python, and it executes backpropagation internally using the TensorFlow library. 9.2. Backpropagation and Stochastic Gradient Descent 351 TensorFlow’s implementation of backpropagation (see arXiv 1610.01178) is a signif- icantly more complex and powerful generalization of the algorithm we described above. TensorFlow is designed to differentiate arbitrarily complex chains of functions. To do this, it constructs a computational graph representing the function to be differentiated. Then it inserts into that graph the operations representing the derivatives, usually working back- wards from output to input. At that point, TensorFlow can perform the derivative compu- tation for arbitrary input to the network. In the case where the network has the structure we describe above, TensorFlow’s computational graph with derivatives would produce the same operations as our pseudocode. The essay https://www.simplilearn.com/keras-vs-tensorflow-vs-pytorch-article may be useful in coding backbpropagation. Backpropagation on the Website Coding for deep learning is discussed on the website https://math.mit.edu/linearalgebra. It has its own section on that sitte—we hope it allows you to create a learning function F(x.v) and test it on examples. Python and Julia and MATLAB are the website languages. Derivatives of Av + b We now compute the derivatives of each layer F; with respect to the weights A; and b; introduced at that layer. The layer equation begins with the vector w; = A,v;_; + b, and then computes v; = ReLU(w;). We will take the easiest step first, and we drop the index j for now. Our function is w(A,b) = Av + b. First of all, what is the derivative of the ith output w; = (Av + b); with respect to the jth input b; ? An unusual question with an easy answer: Bw,- 6w.- If : =3, then E}Tzl If 1+ #7 then 53;=O. (11) This 1 — 0 alternative appears so often in mathematics that it has its own notation d;; : 6ij = { (1) ::: ;; } = the entries in the identity matrix [ Next, what is the derivative of the ith output w; with respect to the (j, k) input 4, ? Suddenly we have three indices i, j, k. This indicates that our denvative is a tensor (more exactly, a 3-way tensor). Tensors are like matrices but with more indices—instead of a rectangle of numbers in a matrix, we now have a 3-dimensional “box of numbers”. Note We remark here that tensors are important, theoretically and also computation- ally. (They enter multilinear algebra.) But they are not trivial extensions of the idea of matrices. Even the rank of a tensor is not easy to define, and the SVD factonization (for example) fails. At least rank one seems straightforward: The tensor entries T;;x have the product form a;bjcx for three vectors a,b,c—like A;b; for a rank one matrix. 352 Chapter 9. Linear Algebra in Optimization Here is the tensor derivative formula when w = Av + b: 8w,~ OA This says : Row j of A affects component i of w = Av + b only if ; = . And when we do have j = i, the number A in that row is multiplying vi. So the derivative of the output w; with respect to the matrix entry Ay is just vy when i = j. w ] _ [ b, ] n [ a3iv) + ai2v2 ijk = = vkd,-j as in T111 =N and T122 = (. (12) Example There are six b’s and a’s in [ wo b2 a21v; + a2 . . Bwl 6w1 Bwl awl 6w1 8w1 Derivativesof wy — =1,— =0, — = vy, — = vy, —— = = 0. 1 abl abg 30.11 1 8012 v2 8021 8022 Derivatives for Hidden Layers Now suppose there is one hidden layer, so L = 2. The output is w = v = v2, the hidden layer contains v, and the input is vg = v. v; = ReLU (bl + Al'vO) and w = by + Ayv; = by + A2RelLU (b, + Al'vo). Equations (11)—(12) give the derivatives of w with respect to the last weights b, and As. The function ReLU is absent at the output and v is v;. But the derivatives of w with respect to b; and A, do involve the nonlinear function ReLU acting on b; + A;vo. So the derivatives in Jw/JA; need the chain rule 8f/0x = (8f/08g)(dg/0x): Q’li _ 8[A2ReLU (bl + Al‘vo)] 6(b1 + Al'vo) 0A; 0A, . = A2 RelLU '(b1+A1v0) 594 1 Chain rule (13) That chain rule has three factors. Starting from vg at layer L —2 = 0, the weights b; and A, bring us toward the layer L — 1 = 1. The derivatives of that step are exactly like (11) and (12). But the output of that partial step is not v _;. To find that hidden layer we first have to apply ReLU. So the chain rule includes its derivative ReLU’. Then the final step (to w) multiplies by the last weight matrix A,. The Problem Set extends these formulas to L layers. They could be useful. But with pooling and batch normalization, automatic differentiation seems to defeat hard coding. Very important Notice how formulas like (13) go backwards from w to v. Auto- matic backpropagation will do this too. “Reverse mode” starts with the output. Details of the Derivatives w /0 A, We feel some responsibility to look more closely at equation (13). Its nonlinear part ReLU ! comes from the derivative of the nonlinear activation function. The usual choice is the ramp function ReLU(z) = (z);+ = limiting case of an S-shaped sigmoid function. d(ReLU) [0 <0 0 when (b; + Ajvp); <0 de 11 z>0 1 when (b + Ajv0); >0 ReLU’(b; + A, v) ={ 9.2. Backpropagation and Stochastic Gradient Descent 353 100 100 ReLU Leaky ReLU slope 0 slope 1 slope 1/100 slope 1 0 — | e | -30 0 50 100 0 50 100 Figure 9.6: The graphs of ReLU and Leaky ReLU (two options for nonlinear activation). Returning to formula (13), write A and b for the matrix A, _, and the vector b _ that produce the last hidden layer. Then ReLU and A; and b, produce the final output w = v;. Our interest is in Jw/0A, the dependence of w on the next to last matrix of weights. w = AL (ReLU(Av + b)) + by and g—i—’ = A ReLU'(Av + b) 0(Av + b) 14 9 (14) We think of ReLU as a diagonal matrix of ReLU functions acting component by component on Av+b. Then J = ReLU '(Av+b) is a diagonal matrix with 1’s for positive components and 0’s for negative components. (Don’t ask about the derivative of ReLU at 0): _ ow J0(Av + b) w = A, ReLU(Av +b) and a—ALJ ey We know every component (vi or zero) of the third factor from the derivatives in (11)—(12). (15) When the sigmoid function ReLU, replaces the ReLU function, the diagonal matrix J = ReLU(f (Av + b) no longer contains 1's and 0's. Now we evaluate the derivative dR,/dz at each component of Av + b. In practice, backpropagation finds the derivatives with respect to all A’s and b's. It creates those derivatives automatically and very effectively. Problem Set 9.2 1 For the equations £+ 2y = 3 and 2z + 3y = 5, test the Kaczmarz iteration in equation (6) starting with (xg, yo) = (0, 0). Compare the cyclic order (equation 1,2, 1,2,...) with random order. 2 Write down the first backpropagation step to minimize ¢(z) = (1 — F(z))? and F(z) = F,(Fi(z)) with F; = (Fy(z)) = cos(sinz) and £ = 0. This 1-variable question can’t show the virtues of backpropagation. 354 Chapter 9. Linear Algebra in Optimization Suppose ReLU is replaced by the smooth nonlinear activation function tanh: eT — e 2 eI + e-—I (a) What 3 numbers give the limit of tanh(z) ast—»0and r oo and > —00? Hyperbolic tangent tanh(z) = (b) Sketch the graph of y = tanh(z) from z = —oo to £ = +o00. (c) Find the derivative dy/dz to show that tanh is steadily increasing. Before ReLU, tanh was a popular nonlinear “activation function” in deep learning. It is an option in playground.tensorflow.org. Because it is a smooth function, its derivatives can be computed for all inputs (unlike ReLU). The input b could be a scalar or a vector: Find the gradient with respect to b of the vector with components tanh(Ax + b). F = F;(z, F\\(y)) is a function of z and y. Find its partial derivatives 0F/0z and O0F'/0y. Which of these derivatives would you compute first ? 9.3. Constraints, Lagrange Multipliers, Minimum Norms 355 9.3 Constraints, Lagrange Multipliers, Minimum Norms We start with the line 3 4+ 4y = 1 in the zy plane. That is the constraint—our solution must be a point that lies on that line. The problem is to find the point (z,y) with minimum norm. The solution (z*, y*) will depend on the choice of norm. Here are the three most popular ways to measure the size ||v|| of a vector v = (z,y). ¢ norm ||v||; = |z| + |y All norms must obey ||cv|| = || ||v|| and the €2 norm ||v||2 = /22 + 2 triangle inequality ||v + w|| < ||v|| + ||w]]- ¢°° norm ||v||cc = max of |z| and |y|| €° norms ||v||, = (|z|P + |y|P)!/P forp > 1 For the vector v = (1, 1), those three norms are ||v|| = 2,v/2, and 1. A good way to visualize each norm is to draw the regions where ||v||; < 1and ||v||2 £ 1and ||v]|c < 1. The picture below shows a diamond and a circle and a square. For the £P norms, the shapes of the regions ||v||, < 1 would gradually approach the square as p — oo. To see why p < 1 fails to give a norm, we include the picture for p = -,_1; In that case v = (1,0) and w = (0, 1) have norm 1, but v + w has norm (1 + 1) = 4. This violates the triangle inequality ||v + w|| < ||v|| + ||w|| and shows that p = 3 does not give a legal norm. v2Z2 V2 11 v v1) £ norm / ( 2) €2 norm / ( lz| + |yl < 1 2 +9y2 <1 diamond \\ / circle \\ / G RY . 11 % norm p=1/2 (3 2) z| <1,y <1 Viegl+ iyl <1 (1,0) s |’Jare — , not convex 9 not a norm y=-1 Figure 9.7: The important vector norms ||v||1, ||v]|2, ||v]|cc and a failure for p < 1. If we move up to v = (z,y, 2) in three dimensions, the circle will become a ball 72 + y? + 2% < 1. The square ||v||o < 1 will become a cube —~1 < z,y,2 < 1. The diamond |z| + |y| + |z| < 1 from the £; norm will now have six corners. A sharp comer of that diamond will still be first to touch a line or a plane in 3D, as the diamond grows. The next figure shows the winning vectors v* in the three important norms. 356 Chapter 9. Linear Algebra in Optimization Now we are ready to minimize ||v||; and ||v||2 and ||v||e With the linear constraint 3z + 4y = 1. A picture shows the line and the three minimum points, depending on the choice of norm. We are scaling down the pictures of ||v|| = 1 until they barely touch the constraint line. The touching points v* are the solutions for the £! and €2 and £°° norms. (0.3) has lv*|ls = § (z535) hasliv®lla=5 (3.3) hasv*llec =7 v* v* Figure 9.8: The solutions v™* to the £! and €2 and £°° minimizations. The first is sparse. The first figure displays a highly important property of the minimizing solution to the £! problem: That solution v* = (0, ;) has only one nonzero component. The vector v* is sparse. This is because a diamond touches a line at a sharp point. The line (or a plane like 3z + 4y + 5z = 1 in three dimensions) contains the vectors that satisfy the constraints Av = b. The diamond expands to meet that plane at the corner (z.y,2) = (0,0, ) of the diamond ! And v* = (0,0, ¢) has two zeros. The essential point is that the solutions to those £; problems are sparse. They have few nonzero components, and those components have meaning. By contrast the least squares solution (using £2) has many small and non-interesting components in n dimensions. By squaring, those components become very small and hardly affect the £2 distance. One final observation: The “£° norm” of a vector v counts the number of nonzero components. But this is not a true norm. The points with ||v]|o = 1 lie on the z axis or y axis—one nonzero component only. The figure for p = % on the previous page becomes even more extreme—;just a cross or a skeleton along the two axes. Of course this skeleton is not at all convex. The “zero norm” violates the fundamental requirement that ||2v|| = 2||v||. In fact ||2v||o = ||v||o = number of nonzeros in v. The wonderful observation is that we can find a sparse solution to Av = b by using the £! norm. We have “convexified” that £° skeleton (which lies only along the axes). We filled out the skeleton to make it convex, and the result was the £! diamond. Summary These vector problems show the importance of the choice of norm. For the matrix problems of Chapter 10, we will again have an important decision—this time for a matrix norm. Convexity will again be a crucial property—tied to the triangle inequality ||A + B|| < ||A|| + ||B|| for matrix norms. 9.3. Constraints, Lagrange Multipliers, Minimum Norms 357 Lagrange Multipliers = Derivatives of the Cost Now we will redo the £2 problem using Lagrange multipliers, because that idea extends to all “quadratic programs” and even to “convex programs”. You will see how Lagrange multipliers deal with constraints. We want to bring out the meaning of those multipliers A1, - .., Am. After introducing them and using them, it is a big mistake to discard them. Our first example is in two dimensions. The function F is quadratic. The set K is linear. Minimize F(z) = 2 + «3 ontheline K: a1z, + azz2 = b On the line K, we are looking for the point that is nearest to (0,0). The cost F(z) is distance squared. In Figure 9.7, the constraint line was tangent to the circle at the winning point x* = (z7,z3). We discover this from simple calculus, after we bring the constraint equation a1x, + a2x2 = b into the function F = :r\"l’ + :t%. This was Lagrange’s beautiful idea. Subtract from F'(x) an unknown multiplier A times a,z; + a2z3 — b Lagrangian L(x, ) = F(z) — A(a1z1 + axz2 — b) =z2 + 22 — Aa1z1 + az2z3 — b) (1) Set the derivatives 9L /0x, and 8L /0x3 and OL /3 to zero. Solve those three equations for x,;,x2, A. OL/8xy =2x) — Aa; =0 (2a) OL/0x3 =2x3 — Aaz =0 (2b) OL/8A\\ = —(a1x; + az2x3 — b) =0 (the constraint') (2¢) The first equations give £; = %)‘al and o = %Aag. Substitute into a;r; + a2xy = b: 1., 1., 2b EAal +§A0«2 =band A= a?+ag. (3) Substituting A into (2a) and (2b) reveals the closest point (23, z3) and the minimum cost (z1)? + (x3)*: a,b . 1 azb “3 wa b (1)* + (x3) —a§+a§ 1 - _ _\\agy = T =-)a> = e M DS S A A s The derivative of the minimum cost with respect to the constraint level b is the Lagrange multiplier \\! 2 i( b )— 25 = . (4) 358 Chapter 9. Linear Algebra in Optimization Quadratic Programming with Positive Definite S We now move that example from the plane R? to the space R™. Instead of one constraint on we have m constraints ATz = b. The matrix AT will be m by n. There will be m Lagrange multipliers Ay, ..., A, : one for each constraint. The cost function F(x) = %zTSa: allows any symmetric positive definite matrix S. Up to now we had S = [I. z* - slope x5 /x] = az2/a; ¢l minimum cost T1 \\ constraint line =\\2 *\\2 —_— (x3)* + (x3) a1T, + azx; = b slope —a, /as Figure 9.9: The constraint line is tangent to the minimum cost circle at the solution z*. In three dimensions the circle and line become a sphere and plane. Problem: Minimize F = %zTS:c subject to ATz = b. &) With m constraints there will be m Lagrange multipliers A = (A, ... 1) They bunld the constraints ATz = b into the Lagrangian L(z,\\) = 3 2T Sz — The n + m derivatives of L give n + m equations for a vector z in R™ and A in R'\" z-derivativesof L: Sx—AA =0 (6) M-derivativesof L: ATz =b The first equations give £ = S~1AX. Then the second equations give ATS~1AX = b. This determines the optimal multipliers A* and then the optimal z* = S~ !A\\*: Solution A\\*,z° A* =(ATS™!A)\" b z* =S 1A(ATS14) . () 1 1 Minimum cost F* = 2(z*)7S2* = 5b\"(ATS™14)71 ATS™1SS-1A(ATS ™1 A)7'b. This simplifies a lot ! Minimum cost F* = %bT(ATS\"lA)\"lb &) Gradient of cost Tl (ATS-1A)\"1b= A\" 9.3. Constraints, Lagrange Multipliers, Minimum Norms 359 This is truly a model problem. Please note that F(z) = 3zTSz — ¢’z can include a linear term, and that some authors reverse A to —A. When the constraint changes to m inequalities AT < b, the multipliers become \\; > 0 and the problem becomes distinctly harder! A zero component A\\! = 0 in the solution signals that the ith constraint a] z < b; is not active. The minimum of F would be the same without that ith constraint. Here is the “saddle point matrix” or “Karush-Kuhn-Tucker matrix” in (6): acrm k[ %] 5 41[5]-[2] o That symmetric matrix K is not positive definite or negative definite. Suppose you multiply the first block row [S A] by ATS™! to get [AT ATS-!A|. Subtract from the second block row to see a zero block: o -t [ 5]=[5) This is just elimination on the 2 by 2 block matrix K. That new block in the 2, 2 position is called the Schur complement (named after the greatest linear algebraist of all time). We reached the same equation ATS~'AX = b as before. Elimination is simply an organized way to solve linear equations. The first n pivots were positive because S is positive definite. Now there will be m negative pivots because —~ATS~1A is negative definite. This is the unmistakable sign of a saddle point in the Lagrangian L(x, \\). That function L = ' S — AT(ATz — b) is convex in = and concave in A Minimax = Maximin There is more to learn from this problem. The z-derivative and the A-derivative of L were set to zero in equation (6). We solved those two equations for * and A*. That pair (z*,\\\") is a saddle point of L = ;x\"Sz - AT(ATz — b). By solving (6) we found the minimum cost and its derivative in (8). Suppose we separate this into two problems: a minimum and a maximum problem. First minimize L(x, A) for each fixed A\\. The minimizing * depends on A. Then find the A\\* that maximizes L(x*(\\), A). Minimize Latz®* = S™'A A Atthatpointz*, minL = —% ATATS-1AX+ATD Maximize that minimum \\* = (ATS1A4)\"'b gives L= %bT(ATS‘lA)“b max min y 1T, ,To-14\\-1 A 3L—-2-b(ASA)b This maximin was x first and A second. The reverse order is minimax: A first, £ second. +ooif ATz £ b The maximum over A of L(x, A) = %zTSa: ~AT(ATz - b) is { 1,76 if ATz = b §$ Il = 360 Chapter 9. Linear Algebra in Optimization The minimum over of that maximum over X is our same answer 1b\" (ATS~14)~1b. min max _lT T co-1 -1 .y L=3bT(4TsT1A)Th 6L 0L i i At the saddle point (z*, A\\*) we have — = g M mn y - mnmax g ox -a—A-=Oan A T — o A Quadratic Programming : General Case Now we have reached a dominant problem in applied and computational optimization. The quantity to minimize is still a quadratic starting with 3T Sz. The constraints to satisfy are still linear. The difference is that S may be only semidefinite and some or all of the constraints may be inequalities. In this case the Lagrange multipliers also obey linear inequalities ! Normally, either the constraint on & or the constraint on A turns out to be tight (meaning that equality holds). The difficulty is : We don’t know which of those inequalities is the tight one. And even just 10 constraint pairs would lead to 2'° configurations of (A = 0) or (A > 0). We cannot try all of those 1024 possibilities. We won’t have to, if we depend properly on linear algebra. Our reference for this topic is the excellent textbook “Numerical Optimization” by Wright and Nocedal. Here is the basic problem of quadratic programming : 1 Minimize gq(z) = EzTSm +zTc subjectto alx=b; i=1tom (equations) ajz>c; j=1top (inequalities) The problem is convex if S = ST is positive semidefinite, as we assume. Nonconvex problems could have several local minima. Start with only equality constraints Az = b and no inequalities. Introduce Lagrange multipliers A; to A, as before, for those m constraints on & : 1 The “Lagrangian” is L = EmTSa: +zTc-AT(Az - D). Its = derivatives give Sz* + c — ATA* = 0. Its A derivatives give Ax* = b (reproducing the constraints) As before, those equations yield the KKT matrix. This is the central matrix of constrained optimization: size n 4+ m: B3R P A N I Notice that many authors define the problem so that — AT replaces AT in the matrix equa- tion. Wright and Nocedal show how a simple restatement of the problem leads to the symmetric (but not positive definite or even semidefinite !) KKT matrix in (11). 9.3. Constraints, Lagrange Multipliers, Minimum Norms 361 Crucial question : When is the KKT matrix invertible ? We can certainly assume that A has full rank m—the constraints are independent. Then we see an easy case that assumes all independent columns in K : If S is positive definite, then K is invertible. But now S might be only semidefinite ! In that case S needs help from A, to make those first n columns of K independent. Here is the key condition that allows A to give full support to S—making K invertible even if S is not: If the columns of N are a basis for the nullspace of A, then the reduced matrix N TSN must be positive definite for K to be invertible. Note that N has n — m columns, so NTSN has size n — m. The assistance from A is producing independence of the first n columns of K, when S by itself has dependent columns. Here is the proof. 0 . : : S AT [ w Suppose the KKT matrix is not invertible : [ A0 ” v ] = [ 0 ] Then Aw = 0. So w = Ny for some y: T . w S AT w | _ T __ _TarT O—[v] [A 0 ][v]—wa—yNSNy. This proves: If K is not invertible then NTSN is not positive definite. Some vector w # 0 is in the nullspace of both S and A. In the good case, when the KKT matrix is invertible, how should we solve equation (11) for the optimal and A ? This is a numerical question. The easy choice is elimination, with row exchanges to avoid small pivots. But row exchanges will ruin the symmetry of K. (Symmetry normally makes elimination go twice as fast, because we only need half of the matrix.) Wright and Nocedal develop stable algorithms that find and A quickly. Inequality Constraints Finally we come to problems with p inequalities a;rz > c; in addition to the m equality constraints @] £ = b;. Here at the start is the crucial question: Which of those p inequali- ties are “active” at the optimal solution x* of the problem ? If we knew that “active set” in advance, we could turn those inequalities into equations—and just ignore the “inactive” inequalities. But we don’t know active from inactive until we solve the problem. Here is a rough description of a successful active set method. As we solve the problem, we have a “working set” of equality constraints at each step. If our current solution &, does not minimize the quadratic for this working set problem, we temporarily ignore all constraints outside the working set and solve that subproblem. Now move as far as possible toward that solution, until running into a “blocking constraint”. Add that constraint to the new working set and solve again. Wright and Nocedal turn that rough idea into a very useful active set algorithm. The final working set is the correct active set of constraints that become equalities at the optimal x*. When we describe the simplex method for linear programming, you will see a similar idea. Each step moves in a cost-reducing direction until it hits a new constraint. 362 Chapter 9. Linear Algebra in Optimization Dual Problems in Science and Engineering Minimizing a quadratic 3T Sz with a linear constraint ATz = b is not just an abstract exercise. It is the central problem in physical applied mathematics—when a linear differential equation is made discrete. Here are two major examples in engineering. 1 Network equations for electrical circuits Unknowns : Voltages at nodes, currents along edges Equations : Kirchhoff’s Laws (balance of currents at each node) Constraint: Ohm’s Law (current proportional to voltage drop) Matrices: ATS~!A is the conductance matrix. 2 Finite element method for structures Unknowns : Displacements at nodes, stresses in the structure Equations : Balance of forces at each node Constraint : Stress-strain relations (Hooke’s Law for a spring) Matrices: ATS~!A is the stiffness matrix. The full list would extend to every field of engineering. For stable problems, the stiffness matrix and the conductance matrix are symmetric positive definite. Normally the con- straints are equations and not inequalities. Then mathematics offers three approaches to the modeling of the physical problem: (i) Linear equations with the stiffness matrix or conductance matrix ATS—1 A (11) Minimization with currents or stresses as the unknowns x (ii1) Maximization with voltages or displacements as the unknowns A In the end, the linear equations (i) are the popular choice. We reduce equation (9) to equation (10). Those network equations account for Kirchhoff and Ohm together. The structure equations account for force balance and elastic properties of the material. All electrical and mechanical laws are built into the final system with ATS~1A4, For problems of fluid flow, that system of equations is often in its saddle point form. The unknowns and A are velocities and pressures. The numerical analysis is well described in Finite Elements and Fast Iterative Solvers by Elman, Silvester, and Wathen. For network equations and finite element equations leading to conductance matrices and stiffness matrices ATC A, one reference is my textbook on Computational Science and Engineering. The video lectures for Math 18.085 are on OpenCourseWare ocw.mit.edu. In statistics and least squares (linear regression), the matrix ATX~!A includes ¥ = covariance matrix. We divide by variances o2 to whiten the noise: ¥ = 1. For nonlinear problems, the energy is no longer a quadratic 3™ Sx. Geometric non- linearities appear in the matrix A. Material nonlinearities (usually simpler) appear in the matrix S. Large displacements and large stresses are a typical source of nonlinearity. 9.3. Constraints, Lagrange Multipliers, Minimum Norms 363 Problem Set 9.3 1 For v = (z,y) on the constraint line 3z + 4y = 1, find the point v; = (z,,y,) that minimizes ||v||P = |z|P + |y|P. As p increases from p = 1 to p = oo, that point v}, should move down the line from v} = (0, §) tov3, = (3, 3). “The triangle inequality ||v + w|| < ||v|| + ||w]|| is satisfied when the unit ball ||lv|| < 1 has a convex shape”—and not otherwise. This means that between any two points v, v in the ball where ||v|| < 1, the straight line must stay in the ball. Connect this convexity requirement to the triangle inequality. Minimize F(z) = ;2T Sz = 122 + 222 subjectto ATz = z; + 3z, = b. (a) What is the Lagrangian L(x, A) for this problem ? (b) Find and solve for £* and A\" the 3 equations “derivative of L = zero”. (c) Draw Figure 9.7 for this problem with constraint line tangent to cost circle. (d) Verify that the derivative of the minimum cost is F* /8b = A*. Minimize F(z) = 1 (z? + 4z2) subject to 21 + z; = 5. Find and solve the three equations 9L /0z, = 0 and OL/0z2 = 0 and OL/OA = 0. Draw the constraint line 2x) + T2 = 5 tangent to the ellipse 1 (z? + 423) = Fpp;, at the minimum (z7, 3). The saddle point matrix K in equation (9) reduces to U by elimination. How many positive pivots for K ? How many positive eigenvalues for K ? 1 0 1° S A S A Kz[AT O]: ? g g - U_[O —ATS—IA] For any invertible symmetric matrix S, the number of positive pivots equals the number of positive eigenvalues. The pivots appear in § = LDLT (triangular L) The eigenvalues appearin § = QAQT (orthogonal Q). A nice proof sends L and D to Q and A. The eigenvalues don’t cross zero: The same signs in D and A. Prove this “Law of Inertia” for any 2 by 2 invertible symmetric matrix S': S has 0 or 1 or 2 positive eigenvalues when it has 0 or 1 or 2 positive pivots. 1. Take the determinant of LDLT = QAQT to show that det D and det A have the same sign. If the determinant is negative then S has __ positive eigenvalue in A and __ positive pivot in D. 2. If the determinant is positive, S could be positive definite or negative definite. Show that both pivots of S are positive when both eigenvalues are positive. Find the minimum value of F(z) = 1 (22423 + ) with one constraint T1 + T2 + 3 = 3 and then with an additional constraint £, + 2z9 + 3z3 = 12. The first minimum value should be less than the second minimum value: Why ? The two problems have a __ and __ tangent to a sphere in RS Describe the points = [ I Iy ]T in the square |z;| < 1,|z2| < 1 by a system Az < b of linear inequalities. The problem is to find A and b. 364 Chapter 9. Linear Algebra in Optimization 9.4 Linear Programming, Game Theory, and Duality This section is about piecewise linear optimization problems—not quadratics. The standard LP problem has linear cost Tz (to be minimized). It has linear constraints Az = b. And it requires £ > 0 (every component has rx > 0). It is those inequalities that make life interesting. My favonite is the max flow / min cut problem coming below. An inequality constraint £, > 0 has two states—active and inactive. If the minimizing solution ends up with ;. > 0, then that requirement was inactive—it didn’t change any- thing. Its Lagrange multiplier will have A7 = 0. The minimum cost is not affected by that constraint on ri. But if the constraint £, > 0 actively forces the best * to have z; = 0, then the multiplier will have A\\{ > 0. So the optimality condition is £ A,, = O for each k. Our approach here will be to see the “duality” between a minimum problem and a max- imum problem— two linear programs that are solved at the same time. Dantzig invented the simplex algorithm to find the optimal solution. One more point about linear programming. It solves all 2-person zero sum games. Profit to one player is loss to the other player. The game begins with a payoff matrix A, and the rules are simple. At every step, player R chooses a row of A and player C' chooses a column. Usually not the same choices every turn—a mixed strategy tends to be better. Then R pays C the number in that row and column of A. The best strategies = and y (min for R, max for C) give a saddle point of =T Ay. Three person games (like Jeopardy!) are another world entirely, much more difficult. Linear Programming Linear programming starts with a cost vector ¢ = (cy, - . ., ¢n ). The problem is to minimize the cost F(z) = ¢1z1+- - -+ cp Ty = ¢! . The constraints are m linear equations Az = b and n inequalities ; > 0,...,z, > 0. We just write £ > 0 to include all n components: Linear Program Minimize c* z subject to Az = band £ > 0 (1) If Ais 1by 3, Az = b gives a plane like z; + z2 + 2x3 = 4 in 3-dimensional space. That plane will be chopped off by the constraints z; > 0,z2 > 0,z3 > 0. This leaves a triangle on a plane, with corners at (z;,z2,z3) = (4,0,0) and (0, 4,0) and (0,0, 2). Our problem is to find the point z* in this triangle K that minimizes the cost cT . Because the cost is linear, its minimum is reached at a corner of K. Linear program- ming has to find that minimum cost corner £*. Computing all corners is exponentially impractical when m and n are large. So Dantzig’s simplex method finds one starting cor- ner that satisfies Az = b and £ > 0. Then it moves along an edge of the triangle K to another (lower cost) corner. The cost ¢T drops at every step. It is a linear algebra problem to find the steepest edge and the next corner (where that edge ends). The simplex method repeats this step many times, from corner to corner. New starting corner, new steepest edge, new lower cost corner in the simplex method. Each step reduces the cost ¢Tz. The method stops when the corner has minimum cost. 9.4. Linear Programming, Game Theory, and Duality 365 Our first interest is to identify the dual problem—a maximum problem for ¢ in R™. It is standard to use y instead of A for the dual unknowns—the Lagrange multipliers. Dual Problem Maximize yTb subjectto ATy <c. (2) This is another linear program for the simplex method to solve. It has the same inputs A, b, c as before. When the matrix A is m by n, the matrix AT is n by m. So ATy < ¢ has n constraints. A beautiful fact: yT b in the maximum problem is never larger than cT = in the minimum problem. Maximum of yTb in (2) < minimum of ¢Tz in (1) Weak duality yTb=yT(Az) = (ATy)Tz < c'z 3) Maximizing pushes yTb upward. Minimizing pushes ¢T& downward. The great duality theorem (the minimax theorem) says that they meet at the best £* and the best y*. Duality The maximum of yTb in (2) equals the minimum of cTz in (1). The simplex method will solve both problems at once. For many years that method had no competition. Now this has changed. Newer algorithms go directly through the set of allowed x’s instead of traveling around its edges (from corner to corner). Interior point methods are competitive because they can use calculus to achieve steepest descent. The situation right now is that either method could win—along the edges or inside. Key points about linear programming and the simplex method 1. The set of vectors with Az = b is chopped off by the planes r; > 0. 2. The minimum cost ¢ occurs at a corner of that feasible set K. 3. The simplex method travels from corner to corner on the boundary of K. 4. It stops at the best corner, where all outgoing edges increase the cost c' z. An Example is Online This book’s website math.mit.edu/linearalgebra develops an example: A Ph.D and a friend and a computer charge only $5, $3, $8 for each hour of homework help, to solve 4 hard problems. They work x;, T2, z3 hours and they solve 1, 1, 2 problems per hour. Minimize cost ¢* & = 5z;+3z2+8x3 withx > 0and Az = 7, +z2+273 = 4 problems. The plane Ax = 4 cuts the x,, x2, T3 axes at the corners (4, 0, 0) and (0, 4,0) and (0,0, 2): all by Ph.D or all by friend or all by computer. One of those corners is the optimal solution ! In this small example you can find the cost of all three corners. In a realistic problem, the simplex method moves to a lower cost corner as one x; drops to zero and another z, increases from zero. Duality holds at the eventual winning corner. 366 Chapter 9. Linear Algebra in Optimization Max Flow-Min Cut Here is a special linear program. The matrix A will be the incidence matrix of a graph. The equation ATy = 0 means that flow in equals flow out at every node. Each edge of the graph has a capacity constraint M;—which the flow y; along that edge cannot exceed. The maximum problem sends the greatest possible flow from the source node s to the sink node t. This flow is returned from ¢ to s on a special edge with unlimited capacity— drawn on the next page. The constraints on ¥ are Kirchhoff’s Current Law ATy = 0 and the capacity bounds |y;| < M, on each edge of the graph. The beauty of this example is that you can solve it by common sense (for a small graph). In the process, you discover and solve the dual minimum problem, which finds the “mincut’ of smallest capacity. Maximize the flow M from source s to sink ¢t (and back). The capacity M; is shown on every edge 3 cuts: capacity 17 15 14 = capacity of the min cut Figure 9.10: The max flow M is bounded by the capacity of any cut (dotted line). By duality, the capacity of the minimum cut equals the maximum flow : Here M = 14. Begin by sending flow out of the source. The three edges going out from s have capacity 7 + 2 + 8 = 17. Is there a tighter bound than M < 17? Yes, a cut through the three middle edges only has capacity 6 + 4 + 5 = 15. Therefore 17 cannot reach the sink. Is there a tighter bound than M < 15? Yes, a cut through five later edges only has capacity 3 + 2 + 4 + 3 + 2 = 14. The total flow M cannot exceed 14. Is that flow of 14 achievable and is this the tightest cut? Yes, 14 is the min cut (it is an £! problem'). By duality 14 is also the max flow. Wikipedia shows a list of faster and faster algorithms to solve this important problem. It has many applications. If the capacities M; are integers, the optimal flows y; are integers. Normally integer programs are extra difficult, but not here. A special max flow problem has all capacities M; = 1 or 0. The graph is bipartite (all edges go from a node in part 1 down to a node in part 2). We are matching people in part 1 to jobs in part 2 (at most one person per job and one job per person). Then the maximum matching is M = max flow in the graph = max number of matchings. This bipartite graph allows a perfect matching: MI M = 5. Remove the edge from 2 down to 1. Now only M = 4 assignments are possible, because 1 2 3 4 5 2 and 5 will only be qualified for job 5. For bipartite graphs, max flow = min cut is Konig’s theorem and Hall’s marriage theorem. 9.4. Linear Programming, Game Theory, and Duality 367 Two Person Games Games with three or more players are very difficult to solve. Groups of players can combine against the others, and those alliances are unstable. New teams will often form. It took John Nash to make good progress, leading to his Nobel Prize (in economics!). But two-person zero-sum games were completely solved by von Neumann. We will see their close connection to linear programming and duality. Here are the rules. The players are R and C. There is a payoff matrix A. At every turn, player R chooses a row of A and player C chooses a column. The number in that row and column of A is the payoff from R to C. Then R and C take another turn. R wants to minimize the payoff and C' wants to maximize. Here is a very small payoff matrix. It has two rows for R to choose and three columns for C': Y1 Y2 U3 I1 1 0 2 $23—l 4 Payoff matrix C likes those large numbers in column 3. R sees that the smallest number in that column is 2 (in row 1). Both players have no reason to move from this simple strategy of column 3 for C and row 1 for R. The payoff from R to C is 2 on every turn. 2 is smallest in its column and largest in its row This is a saddle point. C cannot expect to win more than 2. R cannot expect to lose less than 2. The optimal strategies «* and y* are clear: row 1 for R and column 3 for C. But a change in column 3 will require new thinking by both players. i Y2 ¥3 . I 1 0 4 New payoff matrix |3 -1 2 R likes those small and favorable numbers in column 2. But C will never choose that column. Column 3 looks best (biggest) for C, and R should counter by choosing row 2 (to avoid paying 4). But then column 1 becomes better than column 3 for C, because winning 3 in column 1 is better than winning 2. You are seeing that C still wants column 3 but must go sometimes to column 1. Also R must have a mixed strategy: choose rows 1 and 2 with probabilities r, and z3. The choice at each turn must be unpredictable, or the other player will take advantage. So the decision for R is two probabilities x; > 0 and z3 > Othataddtozr, + 2 = 1. row 1 1 0 4 row 2 3 -1 2 xi(rowl) + xa(row2) | £1 +3x3 —x3 4T + 273 R will choose fractions z; and x; to make the worst (largest) payoff as small as possible. Remembering z2 = 1 — z,;, this will happen when the two largest payoffs are equal: 368 Chapter 9. Linear Algebra in Optimization x, + 3xq = 4z + 2T2 means T, + 3(1 — x;) = 4z, + 2(1 — x1). That equation gives ] = % and z; = %. The new mixed row is 2.5, —.75, 2.5. Similarly C will choose columns 1,2, 3 with probabilities y, y2, y3. Again they add to 1. That mixed strategy combines the three original columns into a new column for C. columnl column2 column3d mix 1,2,3 1 0 4 Y1 + 4ys 3 -1 2 3'y1 — Y2 + 2’y3 C will choose the fractions y; + y2 + y3 = 1 to make the worst (smallest) payoff as large as possible. That happens when y, = 0 and y3 = 1 —y;. The two mixed payoffs are equal : n+41-y) =3y +2(1-y;) gives -3y +4=y1+2 and y;=y;=§. The new mixed column has 2.5 in both components. These optimal strategies identify 2.5 as the value of the game. With the mixed strategy =] = % and 15 = %, Player R can guarantee to pay no more than 2.5. Player C can guarantee to receive no less than 2.5. We have found the saddle point (best mixed strategies), with payoff = 2.5 both ways. Von Neumann'’s minimax theorem for games gives a solution for every payoff matnx. It 1s equivalent to the duality theorem min clz = maxy'b for linear programming. Semidefinite Programming (SDP) The cost to minimize is still cTx : linear cost. But now the constraints on x involve sym- metric matrices S. We are given So to S,, and S(x) = So+ 15, + - - + £, S, is required to be positive semidefinite (or definite). Fortunately this is a convex set of &’s—the average of two semidefinite matrices is semidefinite. (Just average the two energies v Sv > 0.) Now the set of allowed x’s could have curved sides instead of flat sides : So+ 1151 + 225, = [311 :1312] is positive semidefinite when x; > 0and z;x2 > 1. Minimizing the maximum eigenvalue of S(x) is also included with an extra variable ¢ : Minimize t so that tI — S(x) is positive semidefinite. For those and most semidefinite problems, interior-point methods are the best. Essentially we are solving a least squares problem at each iteration—usually 5 to 50 iterations. As in linear programming, there is a dual problem (a maximization). The value of this dual is never above the value cTx of the original. When we maximize in the dual and minimize c*z in the primal, we hope to make those answers equal. But this might not happen for semidefinite programs with matrix inequalities. SDP gives a solution method for matrix problems that previously looked too difficult. 9.4. Linear Programming, Game Theory, and Duality 369 Problem Set 9.4 1 10 Is the constraint £ > 0 needed in equation (3) for weak duality ? Is the inequality ATy < c already enough to prove that (ATy)Tz < ¢Tz? I don’t think so. Suppose the constraints are z; + 22 + 23 = 4and z; > 0,z > 0,z3 > 0. Find the three corners of this triangle in R®. Which corner minimizes the cost c'z =5z, + 3z3 + 8237 What maximum problem for y is the dual to Problem 2 ? One constraint in the primal problem means one unknown y in the dual problem. Solve this dual problem. Suppose the constraintsare > 0and z; + 2r3+ 4 =4and 2 + 3 — 24 = 2. Two equality constraints on four unknowns, so a corer like £ = (0,6,0,4) has 4 — 2 = 2 zeros. Find another corner with € = (z,, z2,0,0) and show that it costs more than the first corner. Find the optimal (minimizing) strategy for R to choose rows. Find the optimal (maximizing) strategy for C to choose columns. What is the payoff from R to C in each game, at this optimal minimax point £*, y* ? Payoff 1 2 1 4 matrices 4 8 8 2 If AT = — A (antisymmetric payoff matrix), why is this a fair game for R and C with minimax payoff equal to zero ? Suppose the payoff matrix is a diagonal matrix X with entries g; > g2 > ... > o,,. What strategies are optimal for player R and player C'? Convert ||(z1,Z2,73)|l1 < 2 in the £! norm to eight linear inequalities Az < b. The constraint ||x|| < 2 in the £°° norm also produces eight linear inequalities. In the £2 norm, ||z|| < 2 is a quadratic inequality 72 + 22 + 22 < 4. But in semidefinite programming (SDP) this becomes one matrix inequality X X T <41 Why is this constraint X X T < 41 equivalent to zTx <47 Suppose the constraints on z,,...,z, are Az < band z > 0 (every z; > 0). If two vectors & and X satisfy these constraints, show that their midpoint M = %3 + % X satisfies the constraints. The constraint set K is convex. Note Duality offers an important option: Solve the primal or the dugl. That applies to optimization in machine learning. Chapter 10 Learning from Data 10.1 Piecewise Linear Learning Functions 10.2 Creating and Experimenting 10.3 Mean, Variance, and Covariance The training set for deep learning is a collection of inputs v and the corresponding outputs w. Each input is a vector with p components v, to v, (the “features” of that input). Each output is a vector with components w; to w, (the “classifiers” of that output). The form of the learning function F'(x,v) is chosen in advance to include “weights” x so that F'(x. v) is very close to (or equal to) w. With good training and good weights, the function F’ can be applied to unseen test data V from a statistically similar population. Success comes if F'(x, V') is close to the correct output W from that test data—which the learning function F' has not seen. Section 10.1 describes F(x,v) as a chain of L functions F(Fp_,(...(F2(F})))). Each function F; has its own weights ;. Those weights consist of a matrix A, and a vector bi. The input to Fj is the output vx_; from Fj_;. The linear part of Fi uses the weights to form Axvi—; + bx. To each component of this vector we apply a nonlinear “activation function”. A simple and successful choice is — _ly i y=20 ReLU (y) = max(y,0) = { 0 if y<0 By applying ReLU to each component of Axvx—1 + bk, we have vx = Fi(vg—;). This vector v lies on layer k, and it is the input to the next function Fi4; in the chain (with its own weights Ax4+1 and biy1). Notice that ReLU is a piecewise linear function (two pieces). Then each component of vy is also a piecewise linear function. Section 10.1 will attempt to count the pieces in the first function v; = Fj(vo) and in the chain of functions v, = F'(vg). Section 10.2 describes (among other things) a wonderful website for experiments. Page 385 includes a link to Python and Julia codes on math.mit.edu/linearalgebra. 370 371 Generalization : Why is Deep Learning So Effective on New Data ? A central question for deep learning is generalization. This refers to the behavior of a neural network on test data that it has not seen. If we construct a function F'(z, v) that successfully classifies the known training data v, will F continue to give correct results when v is outside the training set ? The answer must lie in the stochastic gradient descent algorithm (Chapter 9) that chooses weights. Those weights & minimize a loss function L(z,v) over the training data. The question is : Why do the computed weights do so well on the test data? Often we have more free parameters in than data in v. In that case we can expect many sets of weights (many vectors &) to be equally accurate on the training set. Those weights could be good or bad. They could generalize well or poorly. Our algorithm chooses a particular & and applies those weights to new test data V. An unusual experiment produced unexpectedly positive results. The components of each input vector v were randomly shuffled. So the individual features represented by v suddenly had no meaning. Nevertheless the deep neural net learned those random- ized samples. This random labeling of the training samples (the expennment has become famous) is described in arXiv: 1611.03530. Patterns in Data This part of the book is a great adventure—hopefully for the reader, certainly for the author, and it involves the whole science of thought and intelligence. You could call it Machine Learning (ML) or Artificial Intelligence (AI). Human intelligence created it (but we don’t fully understand what we have done). Out of some combination of ideas and failures, attempting at first to imitate the neurons in the brain, a successful approach has emerged to finding patterns in data. What is important to understand about deep learning is that those data-fitting computa- tions, of almost unprecedented size, are often heavily underdetermined. There are a great many points in the training data, but there can be far more weights to be computed in a deep network. The art of deep learning is to find, among many possible solutions, one that will generalize to new data. It is a remarkable observation that learning on deep neural nets with many weights leads to a successful tradeoff: F is accurate on the training set and the unseen test set. We depend on Chapter 9 for the key algonthm of stochastic gradient descent to compute good weights & in the learning function F(z, v). 372 Chapter 10. Learning from Data 10.1 Piecewise Linear Learning Functions Suppose one of the digits 0,1,...,9 is drawn in a square. How does a person recognize which digit it is ? That neuroscience question is not answered here. How can a computer recognize which digit it is ? This 1s a machine learning question. Probably both answers begin with the same idea: Learn from examples. So we start with M different images (the training set). An image will be a set of p small pixels—or a vector v = (vy,...,vp). The component v; tells us the “grayscale” of the ith pixel in the image : how dark or light it is (or Red-Green-Blue for a color image). So we have Al images each with p features: Af vectors v in p-dimensional space. For every v in that training set we know the digit it represents. In a way, we know a function. We have M inputs in R” each with an output from0to 9. But we don’t have a “rule”. We are helpless with a new input. Machine learning proposes to create a rule that succeeds on (most of) the training images. But *“succeed” means much more than that: The rule should give the correct digit for a much wider set of test images, taken from the same population. This essential requirement is called generalization. What form shall the rule take? Here we meet the fundamental question. Our first answer might be: F(v) could be a linear function from R? to R'° (a 10 by p matrix). The Af inputs v would be vectors with p features each, from the samples in the training set. The 10 outputs would be probabilities of the numbers 0 to 9. The difficulty is : Linearity is far too limited. Artistically, two zeros could make an 8. Images don’t add. In recognizing faces instead of numbers, we will need a lot of pixels— and the input-output rule from F' is nowhere near linear. Artificial intelligence languished for a generation, waiting for new ideas. There is no claim that the absolutely best class of functions has now been found. That class needs to allow a great many parameters (called weights). And it must remain feasible to compute all those weights (in a reasonable time) from knowledge of the training set. The choice that has succeeded beyond expectation—and has turned shallow learning into deep learning—is Continuous Piecewise Linear (CPL) functions. Linear for sim- plicity, continuous to model an unknown but reasonable rule, and piecewise to achieve the nonlinearity that is an absolute requirement for real images and data. This leaves the crucial question of computability. What parameters will quickly de- scribe a large family of CPL functions? In approximating differential equations, finite elements start with a triangular mesh. But specifying many individual nodes in RP is expensive. Much better if those nodes are the intersections of a smaller number of lines (or hyperplanes). Please know that a regular grid is too simple. Here is a first construction of a piecewise linear function of the data vector v. Choose a matrix A; and vector b;. Then set to zero (this is the nonlinear ReLU step) all negative components of A;v + b;. Then multiply by a matrix A2 to produce 10 outputs in w = F(v) = A2(ReLU (A;v + b)). That vector ReLU (A; v + b) forms a “hidden layer” between the input v and the output w in Figure 10.1. 10.1. Piecewise Linear Learning Functions 373 Vi—1 Axvi_ Axvi_) + by Vi —— ° Input (be); ReLU Output layer >e ® layer k1 (bk )2 ReLU Vi = 3 > — @ = Pk-1 (bi)3 ReLU P =4 numbers numbers —>@ Figure 10.1: Step k to layer v has three parts: vy = Fi(vk-1) = ReLU(Axvi—) + bi) 1 Multiply Axvi—1 (px rows) 2 Add vector by 3 Apply ReLU to each component The ReLU function produces two linear pieces : either output = input (if input > 0) or output = zero (if input < 0). Originally, a smooth curve like 1/(1 + e~*) was expected to help in optimizing the weights A;,b;, A;. That assumption proved to be wrong. The graph of each component of ReLU (A;v + b,) has two halfplanes (one is flat, from the zeros where A;v + b is negative). If A, is p; by po, the input space is sliced by p1 hyperplanes into r pieces. We can count those pieces ! This measures the “expressivity” of the function F). The count in this section is a sum of binomial coefficients p! /n!(p — n)! r(p1,Po) = (pl ) + (pl ) 4+ 4 (pl) linear pieces of Fj(vo) 0 1 Po This number gives an impression of the graph of F;—a mosaic of r flat pieces. But our function is not yet sufficiently expressive, and one more idea is needed : add more layers. Here is the indispensable ingredient in the learning function F. The best way to create complex functions from simple functions is by composition. Each Fi uses Ay and b. followed by the nonlinear ReLU : Fi(v) = ReLU (Axv + bi). The composition of L functions is the chain F(v) = Fp(Fp-1(... F2(F1(v)))). We have L — 1 hidden layers before the final output layer. The network becomes “deeper” as L increases. The great optimization problem of deep learning is to compute weights Ay and by that will make the outputs F(v) nearly correct—close to the digit w(v) that the image v represents. This problem of minimizing some measure of F(v) — w is solved by following a gradient downhill. The gradient of this complicated function is computed by backpropagation—the workhorse of deep learning that executes the chain rule. A historic competition in 2012 was to identify the 1.2 million images collected in ImageNet. The breakthrough neural network in AlexNet had 60 million weights. Its accuracy (after 5 days of stochastic gradient descent) cut in half the next best error rate. Deep learning had arrived. Our goal here was to identify continuous piecewise linear functions as powerful approximators. That family is also convenient—closed under addition F' 4+ G and max- imization max(F', G) and composition F'(G). The magic is that the learning function gives accurate results on images v that F' has never seen. 374 Chapter 10. Learning from Data The Construction of Deep Neural Networks Deep neural networks have evolved into a major force in machine learning. Step by step, the structure of the network has become more resilient and powerful—and more easily adapted to new applications. One way to begin is to describe essential pieces in the structure. Those pieces come together into a learning function F(x, v) with weights x that capture information from the training data v—to prepare for use with new test data. Here are important steps in creating that function F': 1 Key operation Composition F(x,v) = F3(F3(F;(v))) 2 Keyrule Chain rule for x-derivatives of F’ 3 Key algorithm Stochastic gradient descent to find the best weights x 4 Key subroutine Backpropagation to execute the chain rule 5 Key nonlinearity ReLU(y) = max(y, 0) = ramp function Our first step is to describe the pieces Fi, F5, F3, ... for one layer of neurons at a time. The weights x that connect the layers v are optimized in creating F'. The vector v = vy comes from the training set, and the function Fj produces the vector v at layer k. The whole success is to build the power of F' from those pieces Fj in equation (1). F} is a Piecewise Linear Function of vy _; The input to Fj is a vector vi_; of length px_;. The output is a vector v, of length py, ready for input to Fi.,. This function F; has two parts, first linear and then nonlinear: 1. The linear part of Fj yields Arvi—1 + bx (that bias vector b, makes this “affine’) 2. A fixed nonlinear function like ReLU is applied to each component of Axvyi_, + bi Vi = Fi(vk—1) = ReLU (Agvi—1 + bi) (1) The training data for each sample is in a feature vector vg. The matrix A, has shape Pk by pk—1. The column vector by has p, components. These A, and b, are weights constructed by the optimization algorithm. Frequently stochastic gradient descent computes optimal weights z = (A;,b,...,AL,br) in the central computation of deep learning. It relies on backpropagation to find the x-derivatives of F', to solve VF = 0. The activation function ReLU(y) = max(y,0) gives flexibility and adaptability. Linear steps alone were of limited power and ultimately they were unsuccessful. ReLU is applied to every “neuron” in every internal layer. There are p; neurons in layer k, containing the p, outputs from A vi_; + br. Notice that ReLU itself is continuous and piecewise linear, as its graph shows. (The graph is just a ramp with slopes 0 and 1. Its derivative is the usual step function.) When we choose ReLU, the composite function F = Fp --(F2(Fi(v))) has an important and attractive property: The learning function F' is continuous and piecewise linear in v. 10.1. Piecewise Linear Learning Functions 375 One Internal Layer (L = 2) Suppose we have measured po = 3 features of one sample in the training set. Those features are the 3 components of the input vector v = vg. Then the first function F} in the chain multiplies vo by a matrix A; and adds an offset vector b; (bias vector). If A, is 4 by 3 and the vector b; is 4 by 1, we have p; = 4 components of A,vg + b;. That step found 4 combinations of the 3 original features in v = vg. The 12 weights in Figure 10.1 were optimized over many feature vectors vy in the training set, to choose a 4 by 3 matrix (and a 4 by 1 bias vector) that would find 4 insightful combinations. The final step to reach v, is to apply the nonlinear “activation function” to each of the 4 components of A;vg + b;. Historically, the graph of that nonlinear function was often given by a smooth “S-curve”. Particular choices then and now are in Figure 10.2. ReLU(x) tanh(z/2) 4{ o ' ) ' ) ! 0— . . . < d/dx ReLU(x) d/dx tanh(z/2) 1 ) Y ' 0.5 ' V ' 0 , w of -4 -2 0 2 4 -4 -2 0 2 4 Figure 10.2: The Rectified Linear Unit and a sigmoid option for nonlineanty. Previously it was thought that a sudden change of slope would be dangerous and pos- sibly unstable. But large scale numerical experiments indicated otherwise ! A better result was achieved by the ramp function ReLU(y) = max(y,0). We will work with ReLU': Substitute A;vo + b; into ReLU to find v, (v1)x = max((A;vo+b)x,0).| (2) Now we have the components of v; at the four “neurons” in layer 1. The input layer held the three components of this particular sample of training data. We may have thousands or millions of samples. The optimization algorithm found A, and b;, usually by stochastic gradient descent using backpropagation to compute gradients of the overall loss. Suppose our neural net is shallow instead of deep. It only has this first layer of 4 neurons. Then the final step will multiply the 4-component vector v; by a 1 by 4 matnx A, (a row vector). It can add a single number b, to reach the value v = Ayv; + ba. The nonlinear function ReLU is not applied to the output. Overall we compute v3 = F(x, vg) for each feature vector vy in the training set. (3) The steps are v2 = A2v; + by = A2 (ReLU (4,99 + b))) + b2 = F(z, vo). 376 Chapter 10. Learning from Data The goal in optimizing £ = (A,, b;, A2, b2) is that the output values v, = v, at the last layer £ = 2 should correctly capture the important features of the training data vy. For a classification problem each sample vy of the training data is assigned 1 or —1. We want the output v, to have that correct sign (most of the time). For a regression problem we use the numerical value (not just the sign) of v,. We do not choose enough weights Ax and by to get every sample correct. And we do not necessarily want to! Overfitting the training data could give erratic results when F is applied to new and unknown test data. Depending on our choice of loss function L(x,v2) to minimize, this can be least squares or entropy minimization: “square loss” or ‘“cross-entropy loss”. Our hope is that the function F' has learned the data. This is machine learning. We want a balance where the function F' has learned what is important in recognizing dog versus cat—or identifying an oncoming car versus a turning car. Machine learning doesn’t aim to capture every detail of the numbers 0,1,2...,9. It just aims to capture enough information to decide correctly which number it is. The Graph of the Learning Function F'(v) The graph of F(v) is a surface made up of many, many flat pieces—they are planes or hyperplanes that fit together along all the folds where ReLU gives a change of slope. This 1s like origami except that this graph has flat pieces going to infinity. And the graph might not be in R*—the feature vector v = v has py components. Part of the mathematics of deep learning is to estimate the number of flat pieces and to visualize how they fit into one piecewise linear surface. That estimate comes after an example of a neural net with one internal layer. Each feature vector vg contains po measurements like height, weight, age of a sample in the training set. In the example, F had three inputs in v and one output v,. Its graph will be a piecewise flat surface in 4-dimensional space. The height of the graph is v = F'(vg), over the point vy in 3-dimensional space. Limitations of space in the book (and severe limitations of imagination in the author) prevent us from drawing that graph in R*. Nevertheless we can try to count the flat pieces, based on 3 inputs and 4 neurons and 1 output. Note 1 With only m = 2 inputs (2 features for each training sample) the graph of F' 1s a surface in 3D. We can attempt to describe it. Note 2 You actually see points on the graph of F' when you run examples on playground.tensorflow.org. It is pictured in Section 10.2. That website offers four options for the training set of points vg. You choose the number of layers and neurons. Please choose the ReLU activation function! Then the program counts epochs as gradient descent optimizes the weights. (An epoch sees all samples on average once.) If you have allowed enough layers and neurons to correctly classify the blue and orange training samples, you will see a polygon separating them. That polygon shows where F' = 0. It is the cross-section of the graph of F'(v) at height zero. 10.1. Piecewise Linear Learning Functions 377 We will discuss experiments on this playground.tensorflow.org site in Section 10.2. Important Note : Fully Connected versus Convolutional We don’t want to mislead the reader. Those “fully connected™ nets are often not the most effective. If the weights around one pixel in an image can be repeated around all pixels (why not ?), then one row of A is all we need. The row can assign zero weights to faraway pixels. Local convolutional neural nets (CNN’s) are also described in Section 10.2. You will see that the count grows exponentially with the number of neurons and layers. That is a useful insight into the power of deep learning. We badly need insight because the size and depth of the neural network make it difficult to visualize in full detail. Counting Flat Pieces in the Graph : One Internal Layer It is easy to count entries in the weight matrices A, and the bias vectors by. Those numbers determine the function F'. But it is far more interesting to count the number of flat pieces in the graph of F. This number measures the expressivity of the neural network. F(x,v) is a more complicated function than we fully understand (at least so far). The system is deciding and acting on its own, without explicit approval of its “thinking”. For driverless cars we will see the consequences fairly soon. Suppose vg has pg components and A;vg + b; has p; components. We have p; func- tions of vg. Each of those linear functions is zero along a hyperplane (dimension py — 1). When we apply ReLU to that linear function it becomes piecewise linear, with a fold along that hyperplane. On one side of the fold its graph is sloping, on the other side the function changes from negative to zero. Then the next matrix A, combines those p; piecewise linear functions of vp, so we now have folds along p; different hyperplanes. This describes each piecewise linear component of the next layer A2(ReLU(A;vp + by)) in the typical case. You could think of p; straight folds in the plane (the folds are actually along p, hyper- planes in pg-dimensional space). The first fold separates the plane in two pieces. The next fold from ReLU will leave us with four pieces. The third fold is more difficult to visuahze, but Figure 10.3 will show that there are seven (not eight) pieces. In combinatorial theory, we have a hyperplane arrangement—and a theorem of Tom Zaslavsky counts the pieces. The proof is presented in Richard Stanley’s great textbook on Enumerative Combinatorics (2001). But that theorem is more complicated than we need, because it allows the fold lines to meet in all possible ways. Our task 1s simpler because we assume that the fold lines are in “general position”. For this case we now apply the neat counting argument given by Raghu, Poole, Kleinberg, Gangul, and Dickstein: On the Expressive Power of Deep Neural Networks, arXiv : 1606.05336. 378 Chapter 10. Learning from Data Theorem Suppose the graph of F'(v) has folds along p; hyperplanes. Those come from p; linear equations af v + b; = 0, in other words from ReLU at p; neurons. If v has py components, then the p, folds produce 7 linear regions in F'(v): \"(PloPo)=(%l)+(pll)+---+(z;) (4) These binomial coefficients are P11\\ _ pr! : _ P1)\\ _ P11\\ _ : (i)_i!(pl-i)! wnhO!—land(O)—land(i)—O fori > p;. Example The function F(z,y.z2) = ReLU (x)+ReLU (y) + ReLU (z) has 3 folds along the 3 planes r = 0,y = 0, 2 = 0. Those planes divide R® into r(3,3) = 8 pieces where F=z+y+zandz+ zandz and 0 (and 4 more). Adding ReLU (z + y+ 2z — 1) givesa fourth fold and (4. 3) = 15 pieces of R®. Not 16 because the new fold plane z +y+z = 1 does not meet the 8th original piece where z < 0,y < 0,z < 0. George Polya’s famous YouTube video Let Us Teach Guessing cut a cake by 5 planes. He helps the class to find r(5,3) = 26 pieces. Our cakes can be high-dimensional. One hyperplane in R™ produces ( (1)) + ( i) = 2 regions. And 2 hyperplanes will produce 7(2.pp) = 1+ 2 + 1 = 4 regions provided pg > 1. When py = 1 we have two folds in a line, which only separates the line into (2, 1) = 3 pieces. The count r of linear pieces will follow from the recursive formula Proof of formula (@) |T(P1:P0) =7(P1 = L,po) +7(p1 — 1,po — 1). (5) To understand that recursion, start with p; — 1 hyperplanes and r(p, — 1, pg) regions. Add one more hyperplane H (dimension pop — 1). The established p; — 1 hyperplanes cut H into r(p; — 1, po — 1) regions. Each of those pieces of H divides one existing region into two, adding r(p1 — 1, po — 1) regions to the original (p; — 1, pg); see Figure 10.3. So the recursion is correct, and we now apply equation (5) to compute 7(p,, po). The count starts at 7(1,0) = (0, 1) = 1. Then (4) is proved by induction on p; + pg: Po po—1 -1 -1 r(p1—17p0)+r(pl-1’p0-1)=;(pli )+; (plz ) po—1 [P -1 p1—1 p1 —1 -(\"o \")+ 2|+ p po—1 p Po p _ 1 1 _ 1 -(5)+ 2 (2)-2(%) o The two terms in brackets (second line) became one term because of a useful identity : -1 n-1\\_{( m IR ( ; )+ ( P41 ) = (i +1) and the mc?uctnon 1s complete. (7) 10.1. Piecewise Linear Learning Functions 379 Mike Giles suggested Figure 10.3 to show the effect of the last hyperplane H. 4 Start with 2 folds Fold lines r(2,2) = 4 pieces 1,2, 3,4 ina plane la 3a po = 2 New fold H creates 2a r(2,1) = 3 new pieces 1b, 2b, 3b Total r(3,2) = 7 pieces b /2 \\3bH Figure 10.3: The (2,1) = 3 pieces of H create 3 new regions. Then the count becomes r(3,2) = 4 + 3 = 7 flat regions in the continuous piecewise linear surface v, = F(vo). A fourth fold will cross all 3 existing folds and create 4 new regions,so r(4,2) = 11. Flat Pieces of F'(v) with More Hidden Layers Counting the linear pieces of F'(v) is much harder with 2 internal layers in the network. Again vg and v, have pg and p; components. Now A;v; + by will have po components before ReLU. Each one is like the function F for one layer, described above. Then appli- cation of ReLU will create new folds in its graph. Those folds are along the lines where a component of A;v; + b2 1s zero. Remember that each component of Asv; + be is piecewise linear, not linear. So it crosses zero (if it does) along a piecewise linear surface, not a hyperplane. The straight lines in Figure 10.3 for the folds in v; will only be piecewise straight for the folds in v,. So the count becomes variable, depending on the details of vo. A4;.b;, A2, and b. Still we can estimate the number of linear pieces. We have p, piecewise straight lines (or piecewise hyperplanes) from p2 ReLU’s at the second hidden layer. If those lines were actually straight, we would have a total of p; + p2 folds in each component of v3 = F(vg). Then the formula (4) to count the pieces would have p, + ps in place of p:. This estimate is confirmed by Hanin and Rolnick (arXiv: 1906.00904). So the count of neurons, not layers and depth, decides the number of linear pieces in F(z.v). Boris Hanin has shown that the depth-width ratio L /W is critical for neural nets. The “chain” or “composition” of F}.’s would simply represent matrix multiplication if all our functions were linear: Fi(v) = Axv. Then F3(vo) = A3A24,v0: just one matrix. For nonlinear F) the meaning is the same: Compute v; = Fj(vg), then v2 = F3(v,), and finally v3 = F3(v2). This operation of composition F3(F2(Fy(vo))) is far more powerful in creating functions than addition or multiplication. Dropout Dropout is the removal of randomly selected neurons in the network. Those are compo- nents of the input layer vg or of hidden layers v,, before the output layer v, . All weights in the A’s and b’s connected to those dropped neurons disappear from the net. Typically hid- den layer neurons might be given probability p = 0.5 of surviving, and input components might have p > 0.8. The main objective of random dropout is to avoid overfitting. It is an inexpensive averaging method compared to combining predictions from many networks. 380 Chapter 10. Learning from Data Problem Set 10.1 1 In the example F = ReLU (z) + ReLU (y) + ReLU (z) that follows formula (4) for r, suppose the 4th fold comes from ReLU (r+y+ z). Its fold plane x+y+2 = 0 now meets the 3 original fold planes ¢ = 0,y = 0, z = 0 at a single point (0,0, 0)— an exceptional case. Describe the 12 (not 15) linear pieces of G = sum of these four ReLU’s. 2 Suppose we have m = 2 inputs and N neurons on a hidden layer, so F(z,y) is a linear combination of N ReLU’s. Write out the formula for (N, 2) to show that the count of linear pieces of F' has leading term %N 2, 3 Suppose we have N = 18 lines in a plane. If 9 are vertical and 9 are horizontal, how many pieces of the plane ? Compare with (18, 2) when the lines are in general position and no three lines meet. 4 What weight matrix A; and bias vector b, will produce RelL.U (r + 2y — 4) and ReLU (3z — y + 1) and ReLU (2 + 5y — 6) as the N = 3 components of the first hidden layer ? (The input layer has 2 components = and y.) If the output w is the sum of those three ReLU’s, how many pieces in w(z,y) ? 5 Folding a line four times gives r (4, 1) = 5 pieces. Folding a plane four times gives r (4,2) = 11 pieces. According to formula (4), how many flat subsets come from folding R® four times ? The flat subsets of R® meet at 2D planes (like a door frame). N 6 The binomial theorem finds the coefficients ( ]Z) in (a + b)N = Z ( IZ ) akpV K, 0 For a =b=1 what does this reveal about those coefficients and (N, m) form > N? 7 InFigure 10.3, one more fold will produce 11 flat pieces in the graph of z = F(z,y). Check that formula (4) gives r (4,2) = 11. How many pieces after five folds ? 8 Explain with words or show with graphs why each of these statements about Continuous Piecewise Linear functions (CPL functions) is true : M The maximum M (z,y) of two CPL functions Fj(z,y) and F;(z,y) is CPL. S The sum S(z,y) of two CPL functions F;j (z, y) and Fy(z,y) is CPL. C If the one-variable functions y = F;(z) and z = F,(y) are CPL, so is the composition C(z) = z = F3(F(x)). 9 How many weights and biases are in a network with py = 4 inputs in each feature vector vg and N = 6 neurons on each of the 3 hidden layers ? How many activation functions (ReLU) are in this network, before the final output ? 10 What is the smallest number of pieces that 20 fold lines can produce in a plane ? 11 How many pieces are produced from 10 vertical and 10 horizontal folds ? 12 What is the maximum number of pieces from 20 fold lines in a plane ? 10.2. Creating and Experimenting 381 10.2 Creating and Experimenting playground.tensorflow.org This section begins with a website that beautifully illustrates deep learning. Layers can be added or removed; so can nodes (neurons) on each layer. Four examples are open for experiment—three are relatively easy and the fourth is not. This educational website came from Daniel Smilkov at Google, and it is highly recommended for simplicity and clarity. Here is the picture that appears when you go to playground.tensorflow.org. We have added names in bold letters for the decisions to be made by the user, including 1 The number of layers and nodes (neurons) on each layer—not really hidden 2 The “learning rate” to control the stepsize in gradient descent (try .003) 3 The nonlinear activation (ReLU is recommended) applied at each node 4 The batch size (1 or more) and regularization in stochastic gradient descent 5 A choice of four sets of training data (three easy and one with difficult spirals) N » R, - 000 000 START CYCLES STEPSIZE GO TO RelLU ADD MORE HIDDEN LAYERS UTPUT Error at each step + ~— ADD MORE NODES TO LAYER + -~ E] Easy Easy Hard WEIGHTS WEIGHTS 4 EXAMPLES BALL IN RING EXAMPLE 1 l af=luls} BATCH When you choose an example and press ‘Start’, the system begins to cycle through the blue and orange data points—aiming to create weights in a learning function F' that classifies those points correctly. Heavier curves between nodes indicate greater weights. Each cycle is an epoch. You could start with the first two features z; and z3 (the position of every data point). The output on the right shows the loss function approaching a minimum (hopefully near zero). And you see how F' is separating the blue and orange points more or less correctly. The separator is a white line where F' = (. The Problem Set suggests additional experiments. Dataset 4 needs more neurons ! 382 Chapter 10. Learning from Data Generalization and Double Descent Here is a big question about deep learning : Why does it succeed so well ? And why does it sometimes fail (as in the playground website) ? Success is measured by computing the matrix weights & so that the function F'(x, v) is accurate for the training data v—and then applying F' to new test data V. Often but not always the results are good. The output F(z.V) is accurate, with weights that were chosen for the v’s. Without help, the system has generalized to the new data. This success needs explanation, which has begin to come. For linear problems, the classical test 1s “stability” of the approximation. Small changes of input bring only small changes of output. But our F is emphatically nonlinear as we add new data. Statistics has issued a strong warning against overfitting the data. In a typical exam- ple of least squares, the results become less stable as the number n of fitting parameters approaches the number m of fitted data. But our F is a nonlinear function of the data. Expenments are needed to point the way. Test error ~Jraining error- ~ : :Exact interpolation “: n Number of Weights Figure 10.4: This is Belkin’s double descent curve (arXiv 2003.00307). It shows the test error going down as n increases, and then up for overfitting. The surprise is that the error goes back down for n > m. Among many solutions, a good one is chosen. The least squares error curve has “double descent”. As n comes close to m, the error increases as expected. At n = m we are fitting the m data points too tightly. Small errors are amplified. But for n increasing beyond m, the error decreases again. We would seem to be overfitting but when we choose the minimum norm solution, with no component in the nullspace of A, the accuracy improves and the error curve begins another descent. Belkin has led the analysis (still evolving) of this phenomenon. 10.2. Creating and Experimenting 383 Convolutional Neural Nets : Sharing the Weights Up to now, the weight matrices A; between layers of the neural net have been “dense”. All entries in Ax were independent parameters. Our object was to fit the data as closely as possible, using every entry in A, as an available parameter. But for some input data (like images), this process is too expensive. An image has too many pixels and their interdependence is far from random. Nearby pixels are highly correlated. Distant pixels are weakly correlated. By restricting the weight matrices Ay, the training 1s faster and much more efficient. Convolutional nets were an important idea. The restriction to local convolution matrices is successful in many problems. A con- volution applies the same weights around each point in the image. It is shift invariant. And the convolution is local if the weight applied to all faraway pixels 1s zero. In signal processing, these are band matrices with constant diagonals : known as “filters” or “Toeplitz matrices”. In image processing (2-dimensional, with column vectors A, B, C), the local convolution might have 3% =9 free parameters in A, B, C to be repeated around every point. e s a b c | A ABZC ] Flltel:n 1D 0 b c Fllter— in 2D l- ABC o : a bec . : ABZC Toeplitz matrix - abec Block Toeplitz matrix . ABC ‘] Figure 10.5: 2D convolutions replace a row of 3 weights by a square ABC of 9 weights. Max-pooling Multiply the previous layer v by A, as before. Then from each even-odd pair of outputs like components 2 and 3 of Av + b, keep the maximum. Please notice right away : Max-pooling is simple and fast, but taking the maximum is not a linear operation. It is a fast route to dimension reduction, and it may also reduce the danger of overfitting. For an image (a 2-dimensional signal) we might use max-pooling over 2 by 2 squares of pixels. Each dimension is reduced by 2, The image dimension is reduced by 4. This speeds up the training, when the number of neurons on a hidden layer is divided by 4. Normally a max-pooling step is given its own separate place in the overall architecture of the neural net. A part of that architecture might look like this : § } e—s—— > weights in A max-pooling _1 i i v, in layern Uni+1 = ReLU (Av, + b,) vn42 = max2(v,4) 384 Chapter 10. Learning from Data Four Big Successes of Deep Learning One day there will be a proper history of this subject—the ups and downs, successes and failures, and the i1deas that made successes out of the failures. Here I report on two great achievements of the DeepMind team in London, plus a success with language translation and a new code that can specially affect students : by solving all your Problem Sets! 1 AlphaGo Zero’s neural network was trained using TensorFlow. At the start it knew nothing about Go beyond the rules. It only saw the stones on the board. The key was reinforcement learning, playing 4.9 million games against itself. By evaluating the strength of each position, it developed the skills required to beat top humans (The earlier AlphaGo took months of training to achieve the same level.) 2 AlphaFold was named the Scientific Breakthrough of the Year in 2021. It predicts the 3D structure of millions of proteins. Knowledge of that structure had been a serious bottleneck in drug discovery, and AlphaFold predicts the fine details of a protein with phenomenal accuracy. This work (by many authors at DeepMind) ought to be a candidate for a Nobel Prize. Demis Hassabis founded DeepMind in 2010 and remains the CEO. He spoke in 2022 about the big problem of understanding intelligence, and the specific problem of pro- tein structure. I think you would enjoy his lecture on cbmm.mit.edu/video/using- ai-accelerate-scientific-discovery. 3 GoogleTranslate Language translation is a perfect example of the Golden Rule of Machine Learning : Don’t code every grammatical rule and exception. Let the com- puter discover the rules. If you are teaching your friend a game, just start to play. In a way, that is the whole principle of machine learning. Start with a training set, not a learning function. It is examples that produce the function. For language translation just as for chess and Go, the system will learn what it needs to know. 4 A Neural Network Solves, Explains, and Generates University Math Problems This network can understand and solve typical problems in math homeworks—overa wide range of courses. What will become of Problem Sets ? The code is automatically synthesized using OpenAl’s Codex transformer in a highly original way. Iddo Drori led a large team to a code that far surpasses its competitors—the problem was recognized and he found a solution. The paper is found at arXiv: 2112.15594. It appeared in PNAS : Proceedings of the National Academy of Sciences (2022). By chance, the first test examples came from the Fifth Edition of the textbook you are reading. Linear algebra was first! Now the examples come from a whole range of college math courses. At this moment, the authors don’t know the eventual effect on teaching mathematics. I have no idea—but Problem Sets won’t be the same! 10.2. Creating and Experimenting 385 Problem Set 10.2 Problems 1-2 use the blue ball, orange ring example on playground.tensorflow.org. When learning succeeds, a white polygon separates blue from orange in the figure. 1 Does learning succeed for N = 4 neurons ? What is the count (N, 2) of flat pieces in F'(v)? The white polygon shows where flat pieces in the graph of F(v) change sign as they go through the base plane z = (0. How many sides in the polygon ? Reduce to N = 3 neurons in one layer. Does F still separate blue from orange ? How many flat pieces 7(3, 2) in F(v) ? Reduce furtherto N = 2. Example 2 has blue and orange in two quadrants each. With one layer, do N = 3 neurons and even N = 2 neurons classify that training data correctly ? How many flat pieces are needed for success ? Describe the unusual graph of F(v) when N = 2. Example 4 with blue and orange spirals is much more difficult! With one hidden layer, can the network learn this training data? Describe the results as N increases. How many neurons bring complete separation of the spirals with two hidden layers ? I found that 4 + 4 + 2 and 4 + 4 + 4 neurons give very unstable iterations for that spiral graph. There were spikes in the training loss until the algorithm stopped trying. playground.tensorflow.org was a gift to the world from Daniel Smilkov. Python and Julia Codes: Recognizing Handwritten Digits The codes on this book's website are a gift from Ruochen Li, who took the Learning from Data course 18.065 in 2022. I am very grateful! Image recognition is an excellent example based on measuring the grayscale of each pixel in the training set taken from MNIST (a typical size is 60,000 images). The problem is made more difficult by irregular handwnting and uncentered placement. You can experiment with different sizes of the training set. Try activation functions like tanh in addition to ReLU. The batch size and the number of cycles (epochs) and the learning rate (stepsize) determine the training accuracy and speed. An ap- propriate loss function to minimize is categorical_crossentropy, when there are 10 categories 0 to 9 for the output number. And “softmax” is the final step when con- verting 10 numbers wy to wg into 10 probabilities py to pg that add to 1: 1 9 Softmax Probabilities p;=—e\"s where §= ) ewx k=0 I hope your experiments are successful. Please see math.mit.edu/linearalgebra and https://www.simplilearn.com/keras-vs-tensorflow-vs-pytorch-article. 386 Chapter 10. Learning from Data 10.3 Mean, Variance, and Covariance ﬁ The probabilities p; to p,, of outcomes x; to x,, are positive numbers adding to N 2 The mean m is the average value or expected value of the outcome: m = ) p;x;. 3 The variance o2 is the average squared distance from the mean ¥ p;(x; — m)2. g€ 5q &The covariance of random z,, y; with means m;,m,is ) >_ p, ; (xi—mz) (y; —my The mean is simple and we will start there. Right away we have two different situations. We may have the results (sample values) from a completed trial, or we may have the expected results (expected values) from future trials. Examples will show the difference : Sample values Five random freshmen have ages 18,17,18,19,17 Sample mean %(18 +17+ 184194 17) = 17.8 based on the data Probabilities The ages in a freshmen class are 17 (20%), 18 (50%), or 19 (30%) Expected age E [z] of a random freshman = (0.2) 17 + (0.5) 18 + (0.3) 19 = 18.1 Both numbers 17.8 and 18.1 are correct averages. The sample mean starts with N samples x1 to x5 from a completed trial. Their mean is the average of the N observed samples : 1 Samplemean m=pu= —ﬁ(m1+z2+-o-+mN) (1) The expected value of starts with the probabilities py, . .., p, of the ages x,,...,z,: Expected value m = E[z] = p1z1 + p222 + - + PnZTn (2) This is p - . The number E[z] tells us what to expect, m = u tells us what we got. A fair coin has probability po = 3 of tails and p; = J of heads. Then E[z] = (0)+3(1). The fraction of heads in N coin flips is the sample mean. The “Law of Large Numbers” says that with probability 1, the sample mean will converge to its expected value E[z] = 3 as the sample size N increases. This does not mean that if we have seen more tails than heads, the next sample is likely to be heads. The odds remain 50-50. The first 1000 flips do affect the sample mean. But 1000 flips will not affect its limit— because you are dividing by N — oo. Note Probability and statistics are essential for modern applied mathematics. With multiple experiments, the mean m is a vector. The variances/covariances go into a matrix. Then linear algebra diagonalizes that covariance matrix and we can understand it. 10.3. Mean, Variance, and Covariance 387 Variance (around the mean) The variance o2 measures expected distance (squared) from the expected mean E[z]. The sample variance S? measures actual distance (squared) from the actual sample mean. The square root is the standard deviation o or S. After an exam, I email the results p and S to the class. I don’t know the expected m and o2 because I don’t know the probabilities po to pioo for each score. (After 60 years, I still have no idea what to expect.) The distance is always from the mean—sample or expected. We are looking for the size of the “spread” around the mean value £ = m. Start with N samples. 1 Sample variance S? = N—1 [(:cl —m)? 4.+ (zn - m)z] (3) The sample ages ¢ =18, 17, 18, 19, 17 have mean m =17.8. That sample has vaniance 0.7 1 1 S% = \" [(:2)% + (—-8)2 + (2)* + (1.2)* + (- .8)%] = ;(28)=0.7 The minus signs disappear when we compute squares. Please notice ! Statisticians divide by N — 1 = 4 (and not N = 5) so that S? is an unbiased estimate of 0. One degree of freedom is already accounted for in the sample mean. An important identity comes from splitting each (x — m)? into z2 — 2mz + m?: sum of (z; — m)* = (sum of z?) — 2m(sum of z;) + (sum of m?) = (sum of z°) — 2m(Nm) + Nm? sum of (z; — m)? = (sum of ?) — Nm?2. (4) This is an equivalent way to find (x; — m)? +--- + (zy — m?) by adding 22 + - - - + 7. Now start with probabilities p; (never negative !) instead of samples. We find expected values instead of sample values. The variance o2 is the crucial number in statistics. Variance o2 =E [(z — m)?| =p1(z1 —m)? + -+ pu(zn —m)3.[ () We are squaring the distance from the expected value m = E[z]. We don’t have samples, only expectations. We know probabilities but we don’t know the experimental outcomes. Equation (3) for the sample variance S extends directly to equation (6) for the variance 2 : Sum of p; (z;—m)? = (Sum of p;z?)—(Sum of p;z;)? or |a? = E[z?] — (E[z])?| (6) Example 1 Coin flipping has outputs z = 0 and 1 with probabilities pg = p; = % Meanm = 3(0) + 3(1) = 3 = average outcome = E[z] Variance 0% = 1(0- 1)’ + 1(1-1)* = 3 + § = 1 = average of (distance from m)? For the average distance (not squared) from m, what do we expect? E[x — m)] is zero! 388 Chapter 10. Learning from Data Example 2 Find the variance o of the ages of college freshmen. Solution The probabilities of ages z; = 17,18,19 were p; = 0.2 and 0.5 and 0.3. The expected value was m =) _ p;x; = 18.1. The variance uses those same probabilities : o2 = (0.2)(17 - 18.1)2 + (0.5)(18 — 18.1) + (0.3)(19 — 18.1)2 = (0.2)(1.21) + (0.5)(0.01) + (0.3)(0.81) = 0.49. Then o = 0.7. This measures the spread of 17,18, 19 around E[z], weighted by probabilities 0.2, 0.5, 0.3. Continuous Probability Distributions Up to now we have allowed for ages 17, 18, 19: 3 outcomes. If we measure age in days in- stead of years, there will be too many possible ages. Better to allow every number between 17 and 20—a continuum of possible ages. Then probabilities p, . p2. p3 for ages r,, x2, 3 change to a probability distribution p(x) for a range of ages 17 < r < 20. The best way to explain probability distributions is to give you two examples. They will be the uniform distribution and the normal distribution. The first (uniform) is easy. The normal distribution is all-important. Uniform distribution Suppose ages are uniformly distributed between 17.0 and 20.0. All those ages are “equally likely”. Of course any one exact age has no chance at all. There is zero probability that you will hit the exact number z = 17.1 or z = 17 + V2. But you can provide the chance F'(x) that a random freshman has age less than z: The chance of age less than z = 17 is F'(17 The chance of age less than £ = 20 is F'(20) The chance of age less than z is F(z) = 3(x — 17) 17 to 20 : F goesfrom O to 1 )=10 r < 17 won’t happen =1 r < 20 will surely happen From 17 to 20, the cumulative distribution F'(x) increases linearly because p is constant. You could say that p(z)dz is the probability of a sample falling in between = and r + dz. This is “infinitesimally true”: p(z)dz is F(z + dz) — F(x). Here is calculus: p(z) is the derivative of F(x) and F(z) is the integral of p(x). b Probabilityofa < z < b = [ p(z) dec = F(b) — F(a) @) F(b) is the probability of z < b. Subtract F'(a) to keep 2> a. That leaves Prob {a < z < b}. 10.3. Mean, Variance, and Covariance 389 Mean and Variance of p(x) l F cumulative F(z) = A «pdf” p(z) = = 1 p= o integral of p probabl!nty that 2 derivative of F P robabll.nty that a sample is below x sample is near z — 1, _ dF 1 -F(.’B) = 3(x —17) 1 p(z) = — _1 44/TTF=1 o, |PTs P » ’ 17 20 T 17 920 T Figure 10.6: F(x) is the cumulative distribution and its derivative p(z) = dF/dz is the probability density function (pdf). The area up to z under the graph of p(z) is F(z). What are the mean m and variance a2 for a probability distribution ? Previously we added p;i&; to get the mean (expected value). With a continuous distribution we integrate zp(z) : Mean m = E[z] = /mp(z) dr Variance o2 =E [(z — m)?| = /p(a:) (x — m)?dz . : 3 When ages are uniform between 17 and 20, the mean is m = 18.5 with vanance 0? = 1 i ; 1 1 1 2 3 2 = - — 2 = - —_ 2 = - e 3 = —(1]. 3 = -, o / (r — 18.5)“dr /3(:1: 1.5)“dr 9(.1: 1.5) 9( 5) y 17 0 4 =0 That is a typical example, and here is the complete picture for a uniform p(z), 0 to a. 1 T Uniformfor0 < x < a Density p(z) = — Cumulative F(z) = - a a 2 Meanm = [z p(m)d:n:g- Variancea’=f—(1:——) dr = — 0 For one random number between 0 and 1 (mean %) the variance is 02 = % Normal Distribution : Bell-shaped Curve The normal distribution is also called the “Gaussian” distribution. It is the most important of all probability density functions p(z). The reason for its overwhelming importance comes from repeating an experiment and averaging the outcomes. Each experiment has its own distribution (like heads and tails). The average approaches a normal distribution. 390 Chapter 10. Learning from Data Central Limit Theorem (informal) The average of N samples of “any” probability distribution approaches a normal distribution as N — oo. '981 2 /0.3 .84 3 / p(z) e Vamo | F(e) = [ p(m)y 1 — 00 - - F(0) = .16 .02- , —20 -0 0 o 20 —20 —0 0 g 20 Figure 10.7: The standard normal distribution p (x) has mean m = 0 and ¢ = 1. The “standard normal distribution” p(z) is symmetric around z = 0, so its mean value is m = 0. It is chosen to have a standard variance 0% = 1. It is called N(0,1). The graph of p(x) = e=% /2 is the bell-shaped curve with variance 02 = 1. 1 var The integral to compute o uses the idea in Problem 11 to reach 1. Figure 10.7 shows a graph of p(z) for N (0, 0%) and also its cumulative distribution F'(z) = integral of p(z). From F(x) = area under p(x) you see a very important approximation for opinion polling: 2 The probability that a random sample falls between —o and g is F (o) — F(—0) = 3 Similarly, the probability that a random z lies between —20 and 20 (“less than two standard deviations from the mean”) is F(20) — F(—20) =~ 0.95. If you have an experimental result further than 20 from the mean, it is fairly sure to be not accidental. The normal distribution with any mean m and standard deviation o comes by shifting and stretching =% /2/\\/2r. Shift x to £ — m. Stretchz — m to (x — m)/o. Gaussian density p(z) p(z) = 1 o—(x — m)?/202 Normal distribution N(m, o2) ov2n (8) The integral of p(z) is.F(:z:)—thc probability that a random sample will fall below z. There is no simple formula to integrate e~%/2 50 F(z) is computed very carefully. 10.3. Mean, Varnance, and Covariance 391 Mean and Variance : IN Coin Flips and N — oo Example 3 Suppose z is 1 or —1 with equal probabilities p; = p_; = 3. The mean value is . = 3(1) + 3(—1) = 0. The varianceis 02 = 3(1)> + 3(-1)? =1. The key question is the average Ay = (z; + --- + zn)/N. The independent z; are 1 and we are dividing their sum by N. The expected mean of Ay is still zero. The law of large numbers says that this sample average Ay = (#heads — #tails)/ N approaches zero with probability 1. How fast does Ax approach zero? What is its vari- ance o2, ? 0.2 0’2 0.2 0.2 1 By linearity a,zv—N2+N2+ --+—=N—=1—v- since 0 = 1. Here are the results from three numerical tests: random 0 or 1 averaged over N tnals. [48 1's from N = 100] [5035 1’s from N = 10000] [19967 1’s from N = 40000). The standardized X = (z — m)/o = (Ay — 1) x 2V/N was [-.40] [.70] [-.33]. The Central Limit Theorem says that the average of many coin flips will approach a normal distribution. Let us begin to see how that happens: binomial approaches normal. The “binomial” probabilities pg, . . . , pn count the number of heads in /N coin flips. For each (fair) flip, the probability of heads is % For N = 3 flips, the probability of heads all three times is (%)3 = &. The probability of heads twice and tails once is %, from three sequences HHT and HTH and THH. These numbers % and % are pieces of (3 + l)3 = % + % + % + % = 1. The average number of heads in 3 flips is 1.5. 1 3 3 Mean m = (3 heads)g + (2 heads)-s- + (1 hea d)8 +0= g + g + g = 1.5 heads With N flips, Example 3 (or common sense) gives a mean of m = X z;p; = % heads. The variance o? is based on the squared dzstance from this mean N/2. With N = 3 the variance is 02 = 2 7 (which is N/4). To find 0? we add (z; — m)? p; withm = 1.5: 3 3 1 9+3+3+9 S+ (1-15)2=+(0-15)?%-= S +(1-18) 2+ (0-15) 2 5 1 3 S— — 2 -— — 2 = —. For any N, the variance for a binomial distribution is 03, = N/4. Thenon = vVN/2. Figure 10.8 will show how the probabilities of 0, 1, 2, 3, 4 heads in N = 4 flips come close to a bell-shaped Gaussian. We are seeing the Central Limit Theorem in action. That Gaussian is centered at the mean valuem = N/2 = 2. 392 Chapter 10. Leamning from Data To reach the standard Gaussian (mean 0 and vanance 1) we shift and rescale. If z is the number of heads in N flips—the average of N zero-one outcomes—then z is shifted by its mean m = N/2 and rescaled by o = v/N /2 to produce the standard X : r—m :c—lN o \\/_/2 Subtracting m is “centering” or “detrending”. The mean of X is zero. Shifted and scaled X = (N=4has X =x—2) Dividing by o is “normalizing” or ‘“standardizing”. The variance of X is 1. It is fun to see the Central Limit Theorem giving the right answer at the center point X = 0. At that point, the factor e~ */2 equals 1. We know that the variance for N coin flips is 02 = N/4. The center of the bell-shaped curve has height 1/v/27 o = \\/2/%N. What is the height at the center of the coin-flip distribution (binomial distribution)? For N = 4, the probabilities py to p4 for 0, 1,2, 3, 4 heads come from (% + %)4. 6 1 1\\ 1 4 6 4 1 Center probability = T (- + _) 2 2 16 16 16 16 16 PN/a = \\/2/TN, 78> p(z) =1 / \\ uniform o binomial , approaches \\ A[f heads / Gaussian \\ N flips area=1 1 _ 1 / \\ } 6 2* . -3 0 3 M=0 N/2 N Figure 10.8: The probabilities p = (1,4,6,4,1)/16 for the number of heads in 4 flips. These p; approach a Gaussian distribution with variance 02 = N/4 centered at m = N/2. For X, the Central Limit Theorem gives convergence to the normal distribution N(0, 1). The binomial (3 + %)N tells us the probabilities for 0,1, ..., N heads 1 N! 2N (N/2)! (N/2)! For N = 4, those factorials produce 4!/2!2! = 24/4 = 6. For large N, Stirling’s formula vV2rN(N/e)VN is a close approximationto N !. Use this formula for N and twice for N/2: Limit of binomial ~ vV2rN(N/e)N _ V2 1 Center probability PN/2 = OF aN(N/2¢)N m——ma- The last step used the variance 0> = N/4 for N coin-flips. The result 1/v/27o matches the center value (1/2/N above) for the Gaussian. The Central Limit Theorem is true: The center term is the probability of 3 heads, & tails (10) The centered binomial distribution approaches the normal distribution N(0, o2). 10.3. Mean, Vanance, and Covariance 393 Covariance Matrices and Joint Probabilities Linear algebra enters when we run M different experiments at once. We might mea- sure age and height (M = 2 measurements of N children). Each experiment has its own mean value. So we have a vector m = (mg4,my) containing two mean values. Those could be sample means of age and height. Or m, and m, could be expected values of age and height based on known probabilities. A matrix becomes involved when we look at variances in age and in height. Each experiment will have a sample variance S? and an expected 02 = E [(z; — m;)?] based on the squared distance from its mean. Those variances 0% and o2 will go on the main diagonal of the “variance-covariance matrix”. So far we have made no connection between the two parallel experiments. They measure different random variables, but age and height are not independent ! If we measure age and height for children, the results will be strongly correlated. Older children are generally taller. Suppose the means m, and my, are known. Then 02 and o2 are the separate variances in age and height. The new number is the covariance o4, which measures the connection of each possible age to each possible height. Covariance o4, = E [(age — mean age) (height — mean height)]. | (11) This definition needs a close look. To compute o,p, it is not enough to know the probability of each age and the probability of each height. We have to know the joint probability p,n of each pair (age and height). This is because age is related to height. Pan = probability that a random child has age = a and height = h: both at once pi; = probability that experiment 1 produces z; and experiment 2 produces y; Suppose experiment 1 (age) has mean m,;. Experiment 2 (height) has its own mean m;. The covariance between experiments 1 and 2 looks at all pairs of ages ; and heights y;. To match equation (11), we multiply by the joint probability p;; of that age-height pair. Expected value of . Covariance 0,3 = ii(zg —m . —ma)| (12 (x — m,)(y — m2) 12 azl:li,sz i@ 1)(¥; 2)| (12) To capture this idea of “joint probability p;;” we begin with two small examples. Example 4 Flip two coins separately. With 1 for heads and 0 for tails, the results can be (1,1) or (1,0) or (0, 1) or (0,0). Those four outcomes all have probability (%)2 = %. For independent experiments we multiply probabilities : The covariance will be zero. pi; = Probability of (3, j) = (Probability of ¢) times (Probability of j). 394 Chapter 10. Learning from Data Example 5 Glue the coins together, facing the same way. The only possibilities are (1.1) and (0. 0). Those have probabilities % and % The probabilities p;o and pg; are zero. (1.0) and (0. 1) won’t happen because the coins stick together: both heads or both tails. and Ps = ] i [0% Let me stay longer with P, to show it in good matrix notation. The matrix shows the probability p;; of each pair (z;, y;)—starting with (z,, y;) = (heads, heads) and (z,, y2) = (heads, tails). Notice the row sums p;, p2 and column sums P;, P, and the total sum = 1. Probability matrix P = [ P11 P12 ] pu1 +p12 = P1 (ﬁrSt ) Joint probability matrices P, = pij for Examples 4 and 5 T N |- Q (I NI T T P21 P22 P21 + P22 = D2 coin (second coin) columnsums P; P; 4 entries add to 1 Those sums p;,p2 and P,, P, are the marginals of the joint probability matrix P: p1 = p11 + p12 = chance of heads from coin 1 (coin 2 can be heads or tails) P, = p1;1 + p21 = chance of heads from coin 2 (coin 1 can be heads or tails) Example 4 showed mdependent random variables. Every probability p;; equals p; times p; ( times 5 gave p;; = 7 in that example) In this case the covariance o ;, will be zero. Heads or taxls from the ﬁrst coin gave no information about the second coin. Zero covariance o2 for independent trials 0 o, o? 0 . . . V= 2 | = diagonal covariance matrix V. Independent experiments have 0,2 = 0 because every p;; equals (p;)(p;) in equation (12). m—ZZ@, )(p,)(zi—m1)(y;—m3) = [Z(p, -—mn][Z<pj)<yj—m2)]=10n01. J J Example 6 The glued coins show perfect correlation. Heads on one means heads on the other. The covariance ;2 moves from 0 to oy times o2. This is the largest possible value of 012. Here itis (3)(3) = 012 = (}), as a separate computation confirms: Means = - - -Hvoro+ 2(o0-1)(0-1)=1 cams=35 q12=5 2 2 2 2 2]~ 14 Heads or tails from coin 1 gives complete information about heads or tails from coin 2: Glued coins give largest possible covariances Vo = o? 0103 Singular covariance matrix: determinant = 0 glue 0102 ag 10.3. Mean, Variance, and Covariance 395 Always 0203 > (012)2. Thus 0, is between —0,02 and 0,0,. The matrix V IS positive deﬁmte (or 1n this singular case of glued coins, V is positive semidefinite). Those are important facts about all M by M covariance matrices V' for M experiments. Note that the sample covariance matrix S from N trials is certainly semidefinite. Every sample X = (age, height) contributes to the sample mean X = (m,, m;,). Each rank-one term (X; — X)(X; — X)T is positive semidefinite and we just add to reach the sample matrix S. No probabilities in S, use the actual outcomes : Xi+-+Xn o (X “ X)X =-X) T+ +(Xn-X)(Xn-X)T (13 X= N N-1 The Covariance Matrix V is Positive Semidefinite Come back to the expected covariance 0,2 between two experiments 1 and 2 (two coins) : o12 = expected value of [(outputl — mean 1) times (output2 — mean 2)] (14) o12 = ) pij (xi — mq) (y; — m2). The sum includes all pairs ¢, j. pi; > 0 is the probability of seeing output z; in experiment 1 and y; in experiment 2. Some pair of outputs must appear. Therefore the n? joint probabilities p;; add to 1. Total probability (all pairs) is 1 z Z pi; = L. (15) all 1,7 Here is another fact we need. Fix on one particular output x; in experiment 1. Allow all outputs y; in experiment 2. Add the probabilities of (z;, 1), (z;, Y2)s -0 (T4, Un): Row sum p; of P Z Pij = probability p; of z; in experiment 1. (16) 1=1 Some y; must happen in experiment 2! Whether the two coins are completely separate or glued, we get the same answer 3 > for the probability py = pyy + pu that coin 1 is heads: 1 1 1 1 1 (separate) Pyy + Pyt = 1 + 1= > (glued) Pyy + Pyt = 2 +0= 2 That basic reasoning allows us to write one matrix formula that includes the covarnance 012 along with the separate variances o and o3 for experiment 1 and experiment 2. We get the whole covariance matrix V by adding the matrices V;; for each pair (i, j): Covariance matrix V=X ¥ pi (zi—my)? (zi—mi)(y;—m3) V = sum of all V;; (-’Ba—m1)(y_, —m3) (y;—m3)? (17 all 2,7 396 Chapter 10. Learning from Data Off the diagonal, this is equation (12) for the covariance ;2. On the diagonal, we are getting the ordinary variances o and o3. I will show in detail how we get V;; = o2 by using equation (16). Allowing all j just leaves the probability p; of z; in experiment 1: Vll = ZZP.‘j(Ii - m1)2 = z (probabnllty Of.‘L'l') (.’L‘z — m1)2 = 0’?. (18) all 1.7 all ¢ Please look at that twice. It is the key to producing the whole covariance matrix by one formula (17). The beauty of that formula is that it combines 2 by 2 matrices V;;. And the matrix V;; in (17) for each pair of outcomes i, j is positive semidefinite : Vi; has diagonal entries p,-,-(:l:i—ml)2 >0 and p,-J-(yj—mg)2 >0 and det(V;;)=0. That matrix V;; has rank 1. Equation (17) multiplies p;; times column U times row U (-Ti‘ml)2 (:L'i\"ml)(yj—m2)J=|:Ii—m1] [23,' —my Y; —mg] (19) (zi — my)(y; — ma) (y; — m2)? y; — ma Every matrix p;;UUT is positive semidefinite. So the whole matrix V (the sum of those rank 1 matnces) is at least semidefinite—and usually V' is positive definite : The covariance matrix V is positive definite unless the experiments are dependent. Now we move from two variables £ and y to M variables like age-height-weight. Each child has an age-height-weight vector X with M = 3 components. The covari- ance matrix V is now M by M. The matrix V is created from the output vectors X and their average X = E[X] : Covariance matrix V=E[(X—Y) (X—-X.)T] Vij = Dij (X,-—m,-) (XJ —mj) (20) Remember that X X T and YYT = (column) (row) are M by M matrices. 2 For M = 1 (one variable) you see that X is the mean m and V is the variance o2. For M = 2 (two coins) you see that X is (my,m2) and V matches equation (17). The expectation always adds up outputs times their probabilities. For age-height-weight the output could be X = (5 years, 31 inches, 48 pounds) and its probability is ps 31,48 - Now comes a new idea. Take any linear combination cTX = c; X1 + -+ + ey Xm. With ¢ = (6,2, 5) this would be c* X = 6 x age + 2 x height + 5 x weight. By linearity we know that mean of cT X = expected value E [cTX] = cTE[X] =cT X : E [cT X] = ¢\"E [X] = 6 (expected age) + 2 (expected height) + 5 (expected weight). 10.3. Mean, Variance, and Covariance 397 More than the mean of ¢T X, we also know its variance 62 = ¢*Ve: Variance of ¢T X = ¢TE [ X-X)(X-X) ]c c'Ve (21) Now the key point: The variance of c* X can never be negative. So ¢TVe > 0. New proof : The covariance matrix V is positive semidefinite by the energytestc*V ¢ > 0. Covaniance matrices V' open up the link between probability and linear algebra: V equals QA QT with eigenvalues )\\; > 0 and orthonormal eigenvectors g, to q,,. Diagonalizing the covariance matrix V' means finding M independent experiments as combinations of the original M experiments. The Covariance Matrixfor Z = AX Here is a good way to see 02 when z = z + y. Think of (z,y) as a column vector X. Think of the 1 by 2 matrix A = [ 1 1 ] mulnplymg that column vector X = (z,y). Then AX is the sum z = z + y. The variance o2 goes into matrix notation as o2=[1 1] [ 0; Z;” ] [ } J whichis 03 = AVAT. (22) Now for the main point. The vector X could have M components coming from M experiments (instead of only 2). Those experiments will have an M by M covariance matrix Vx. The matrix A could be K by M. Then AX is a vector with K combinations of the M outputs (instead of one combination z + y of two outputs). That vector Z = AX of length K has a K by K covariance matrix V7. Then the great rule for covariance matrices—of which equation (22) was only a 1 by 2 example—is this beautiful formula: The covariance matrix of AX is A (covariance matrix of X) AT : The covariance matrixof Z = AX Vz = AVx AT (23) To me, this neat formula shows the beauty of matrix multiplication. I won’t prove this formula, just admire it. It is constantly used in applications. The Correlation p Correlation pg, is closely related to covariance o,,. They both measure dependence or independence. Start by rescaling or “standardizing” the random variables z and y The new X = x/0, and Y = y/oy have variance 053 = o3 = 1. This is just like dividing a vector v by its length to produce a unit vector v/||v|| of length 1. The correlation of and y is the covariance of X and Y. If the original covariance of z and y was o, then rescaling to X and Y gives cormrelation pzy = 0=y /00y, T Correlation p,, = e covariance of — and al Always -1 < p,, <1 398 Chapter 10. Learning from Data Problem Set 10.3 1 If all 24 samples from a population produce the same age r = 20, what are the sample mean y and the sample variance S2 ? What if £ = 20 or 21, 12 times each ? 2 Add 7 to every output z. What happens to the mean and the variance ? What are the new sample mean, the new expected mean, and the new variance ? 3 Weknow: 3 of all integers are divisible by 3 and 1 of integers are divisible by 7. What fraction of integers will be divisible by 3 or 7 or both ? 4 Suppose you sample from the numbers 1 to 1000 with equal probabilities 1/1000. What are the probabilities pg to pg that the last digit of your sample is 0,...,9? What is the expected mean m of that last digit? What is its variance ° ? 5 Sample again from 1 to 1000 but look at the last digit of the sample squared. That square couldend withz = 0, 1, 4, 5, 6, or 9. What are the probabilities pg, p;, ps, ps, Pe. Po ? What are the (expected) mean m and variance o of that number z ? 6 (a little tricky) Sample again from 1 to 1000 with equal probabilities and let x be the first digit (z = 1 if the number is 15). What are the probabilities p; to pg (adding tol)ofz =1,...,9? What are the mean and variance of ? 7 Suppose you have N = 4 samples 157,312,696, 602 in Problem 5. What are the first digits z; to x4 of the squares? What is the sample mean x ? What is the sample variance S? ? Remember to divide by N — 1 = 3 and not N = 4. 8 Equation (4) gave a second equivalent form for S? (the variance using samples): 1 2 _ 2 _ 2 2 S4 = N 1 sum of (z; — m)*° = N1 [(sum of z7) — Nm?]. Verify the matching identity for the expected variance o2 (using m = L p; ;) o2 = sumof p; (z; — m)? = (sum of p; x2) — m?2. 9 Computer experiment : Find the average A,000000 of @ million random 0-1 samples ! What is your value of the standardized variable X = (A N — l) x 2/ N ? 10 For any function f(r) the expected value is E[f] = \" p; f(z:) or [ p(z) f (z) d:c (discrete or continuous probability). The function can be = or (z — m)? or z? If the mean is E[zr] = m and the variance is E[(x — m)?] = o2, what is E[:c’] ? 11 Show that the standard normal distribution p(z) has total probability [ p(x) dx = 1 as required. A famous trick multiplies [ p(z)dz by [ p(y)dy and computes the integral over all z and all y (—o00 to 00). The trick is to replace dx dy in that double integral by r dr d (polar coordinates with 22 + y2 = r2) Explain each step: 00 21r/ (z)dz /p(y) dy = // =+ 24p dy = / / -7*/3 y dr d6 = 2. —00 —00 6=0r=0 10.3. Mean, Variance, and Covariance 399 Thoughts about the success of Deep Learning A mathematician naturally asks, “What question has deep learning answered ? Is it a unique and complete answer, or will there be new ideas that replace or extend it ? What is the key to its success 7’ I don’t have fully satisfying answers, but the questions are important. The framework seems clear. We are given training data v, and we know what object w it describes. We want to create a function F so that F'(v) is close to or equal to w. (Close means an approximation F(v) =~ w. Equal means an interpolation F(v) = w.) The essential requirement is that F' generalizes successfully to new test data V. The assumption is that v and V' come from populations that are statistically similar or even identical. Success is achieved if F(V') is near the actual result W from the test data V. In brief, we look for a function that (nearly) interpolates the training data and success- fully extrapolates to the test data. The hard question is: What shall be the form of F'? Successful generalization is asking for “stability”—if V' is near to v, then F (V') is near to F'(v). We can also hope for simplicity—constructing F'(v) and evaluating F'(V') must not take forever. I think it is correct to say that classical answers failed. If F' is a polynomial—or any linear combination of preselected functions—then good results in a high-dimensional space have been very elusive. At the other extreme is the “finite element method”\"—which uses low degree piecewise polynomials and more or less preselected interpolation points. That idea works well in structural engineering. It seems to be less successful for the tremendous variety of applications of deep learning—which often has no visible structure at all. The key to success (unexpected success) could lie in composition Fr (. . . (F2(F1(v)))) of simple functions. That process quickly brings an amazing complexity. The functions in this book were continuous and piecewise linear (provided ReLU is chosen). Other authors have explored the use of rational functions (known to provide good ap- proximation to a wide class of functions but dangerous near poles). And the crucial under- standing of generalization continues to move forward. It is truly remarkable that the composition of such simple functions has revolutionized our power of approximation and our boldness in new applications. A1 TheRanksof ABand A+ B From Chapters 1 to 3, we know that rank of A = rank of AT. This page establishes more key facts about ranks: When we multiply matrices, the rank cannot increase. You will see this by looking at column spaces and row spaces. 3 shows one special situation B = AT when the rank stays the same. Then you know the rank of AT A. Statement 4 becomes important when data science factors A into UXV'T or CR. Here are five key facts in one place. The most important fact is rank of A = rank of AT. 1 Rank of AB <rank of A Rank of AB < rank of B 2 Rank of A + B < (rank of A) 4+ (rank of B) 3 Rank of ATA =rank of AAT =rankof A =rank of AT 4 If Ais m by r and B is r by n—both with rank r—then A B also has rank r Statement 1 involves the column space of AB and the row space of AB : C(AB) is contained in C(A) so the dimension of C(AB) cannot be larger Every column of AB is a combination of the columns of A (matrix multiplication) Every row of AB is a combination of the rows of B (matrix multiplication) Remember from Section 1.4 that row rank = column rank. We can use rows or columns. The rank cannot grow when we multiply AB. Statement 1 in the box is frequently used. Statement 2 Each column of A + B is the sum of (column of A) + (column of B). rank (A + B) < rank (A) + rank (B) is true. Bases for C(A) and C(B) span C(A + B). rank (A + B) = rank (A) + rank (B) is not always true. It is certainly false if A= B =1. Statement 3 A and AT A both have n columns. They also have the same nullspace. (See Problem 4.19.) So n — r is the same for both, and the rank r is the same for both. Then rank(AT) > rank(ATA) = rank(A). Exchange A and AT to show equal ranks. Statement 4 We are told that A and B have rank r. By Statement 3, AT A and BBT have rank . Those are r by r matrices so they are invertible. So is their product AT ABBT. Then r = rank of (ATABBT) < rank of (AB) < rank of A = r. So AB has rank r. Note This does not mean that every product of rank r matrices will have rank r. Statement 4 assumes that A has exactly r columns and B has r rows. B A can easily fail. 1 A=1|1 B=[1 2 -3] AB has rank 1 But BA is zero! 1 he - A2 Matrix Factorizations 1. A = CR = (basis for column space of A) (basis for row space of A) Requirements: C is m by r and R is r by n. Columns of A go into C if they are not combinations of earlier columns of A. R contains the nonzero rows of the reduced row echelon form Ry = rref(A). Those rows begin with an 7 by r identity matrix so A=CR=[C CF |P =] indepcolumns dep columns | permute columns. C =firstr W =firstrbyr B = firstr _ -1 2. A=CW™B (independent columns) (invertible submatrix) (independent rows) Requirements : C and B contain r columns and r rows of A. Those columns meet those rows in the square invertible matrix W (Section 3.2). Then W~!B = R. 3. A=LU = ( lower triangular L )( upper triangular U ) 1’s on the diagonal pivots on the diagonal Requirements: No row exchanges as Gaussian elimination reduces square A to U. 4 A- LDU = lower triangular L pivot matrix upper tnangular U ) - ~ \\I’s on the diagonal D is diagonal 1’s on the diagonal Requirements : No row exchanges. The pivots in D are divided out from rows of U to leave 1°s on the diagonal of U. If A is symmetric then U is LT and A = LDLT. 5. PA = LU (permutation matrix P to avoid zeros in the pivot positions). Requirements: A is invertible. Then P,L,U are invertible. P does all of the row exchanges on A in advance, to allow normal LU. Alternative: A = L, P\\U,. 6. S = CTC = (lower triangular) (upper triangular) with v/D on both diagonals Requirements : S is symmetric and positive definite (all n pivots in D are positive). This Cholesky factorization C = chol(S) hasCT = Lv/D,s0oS = CTC = LDLT. 7. A = QR = (orthonormal columns in Q) (upper triangular matrix R). Requirements : A has independent columns. Those are orthogonalized in Q by the Gram-Schmidt or Householder process. If A is square then Q! = QT. 8. A=XAX—1=(eigenvectorsin X)(eigenvaluesin A)(eigenvectorsof AT in X —1), Requirements : A must have n linearly independenteigenvectorsin X : AX = X A. 9. S =QAQT = (orthogonal matrix Q) (real eigenvalue matrix A) (QT is Q). Requirements : S is real and symmetric: ST = S. This is the Spectral Theorem. 401 402 10. 11. 12, 13. 14. 15. 16. A2 Matrix Factorizationg A = BJB~! = (generalized eigenvectors of A in B) (Jordan blocks in J) (B~1), Requirements: A is square. This Jordan form J has a block for each linearly independent eigenvector of A. Every block has only one eigenvalue: Appendix A5, orthogonal m x n singular value matrix orthogonal Uismxm o1,...,0, 0N its diagonal Visnxn Every A is included. This Singular Value Decomposition (SVD) has eigenvectors of AAT in U and eigenvectors of ATA in V; o; = /Ai(ATA) = \\/X;(AAT). Those singular values are o; > 02 > --- 2 o, > 0. By column-row multiplication A=UEVT=( T A=UXIVT =g uyv] +---+ o,u, v} = sum of r matrices of rank 1. If S is symmetric positive definitethenU =V =Q and ¥ = A and S = QAQT. A+ —vs+pT- ( orthogonal) (n x m pseudoinverse of X ) ( orthogonal ) Visnxn/)\\1/0,,...,1/0, ondiagonal /] \\ U ism x m Requirements: None. The pseudoinverse AT has A*A = projection onto row space of A and AA* = projection onto column space. A* = A~! if A is invertible. The shortest least-squares solution to Az = bisx* = A*b. This solves AT Azt = ATb. A = QS = (orthogonal matrix Q) (symmetric positive definite matrix .S). Requirements: A is invertible. This polar decomposition has S2 = ATA. The factor S is semidefinite if A is singular. The reverse polar decomposition A = KQ has K2 = AAT. Both have Q = UV'T from the SVD. A =UAU~?! = (unitary U) (eigenvalue matrix A) (U~ which is UT). Requirements: Aisnormal: A' A = AA\" . Its orthonormal (possibly complex) eigenvectors are columns of U. Complex A unless A = A\" = Hermitian matrix. A = QT Q™! = (unitary Q) (triangular T with \\’s on diagonal) (Q—! = aT). Requirements: Schur triangularization of any square A. There is a matrix Q with orthonormal columns that makes Q! AQ triangular: Section 6.4. F o= I D||Fy2 even-odd | one FFT step in its \"~|I -D F, /2 | | permutation| matrix form: nton/2 = m Requirements: F,, = Fourier matrix with entries w’* wherew™ =1: F, F,, = nl. Dhas 1,w,...,w\"2-! on its diagonal. Forn = 2t the Fast Fourier Transform will compute F,, & with only $nf = 1nlog, n multiplications from ¢ stages of D's. The FFT combines the 3-size transforms y ! =Fpnc’ andy” =Fc” inoy= F,c. \" i M - : yjzy]',+(wn)1yj and yj+m=yj’—(wn)’y1 for j=0,...,m-1 A3 Counting Parameters in the Basic Factorizations A=LU A=QR S=QAQT A=XAX\"! A=QS A=UZVT This 1s a review of key ideas in linear algebra. The ideas are expressed by those factor- izations and our plan is simple: Count the parameters in each matrix. We hope to see that in each equation like A = LU, the two sides have the same number of parameters. For A = LU, both sides have n? free parameters from 3n(n — 1) 4+ in(n + 1) = n. L : Triangular n x n matrix with 1’s on the diagonal % n(n—1) inlL U : Triangular n x n matrix with free diagonal % nn+1) inU Q : Orthogonal n x n matrix % nn—1) inQ S : Symmetric n x n matrix % n(n+1) inS A : Diagonal n x n matrnix n in A X : n x n matrix of independent eigenvectors n®-n in X Comments are needed for Q. Its first column q, is a unit vector. The requirement ||q,|| = 1 has used up one of the n parameters in q,. Then g, has n — 2 parameters—it is a unit vector and it is orthogonal to q;. The sum (n— 1)+ (n —2) +-- -+ 1 equals % n(n-—1) free parameters in (0 : n columns averaging (n — 1)/2 free parameters. The eigenvector matrix X has only n? — n parameters, not n%. If z is an eigenvector then so is cx for any ¢ # 0. We could require the largest component of every & to be 1. This leaves n — 1 parameters for each eigenvector (and no free parameters for X -, The count for the two sides now agrees for the first five factorizations in line 1 above. For the SVD, use the reduced form Amxn = UmxrZrxrV,,, (known zeros are not free parameters !) Suppose that m < n and A is a full rank matnx with r = m. The parameter count for A is mn. The total count for U, X,V is also mn. The reasoning for orthonormal columns in U and V is the same as it was above for the columns of Q. 1 1 U has -2-m(m —1) ¥ hasm V has (n-1)+:--+(n-m)=mn — Em(m-l-l) _ Finally, suppose that A is an m by n matrix of rank r. How many free parameters in a rank » matrix ? We can count again for UmxrBrer,.En : 1 1 U has (m—1)+:--+(m-r) =mr — -2-r(r+ 1) V has nr — Er(r-l- 1) ¥ has r The total parameter count for a matrix A of rankrismr +nr—r? = (m+n—-r)r. We reach the same total for A = CR in Section 1.4. The r columns of C were taken directly from A. The row matrix R includes an r by r identity matrix (fixed not free !). Then the count for A = CR agrees with the previous count for A = UXVT, when the rank is r: C has mr parameters R has nr — r? parameters Total mr + nr — r3. 403 A4 Codes and Algorithms for Numerical Linear Algebra LAPACK is the first choice for dense linear algebra codes. ScaLAPACK achieves high performance for very large problems. COIN/OR Here are sources for specific algorithms. Direct solution of linear systems Basic matrix-vector operations Elimination with row exchanges Sparse direct solvers (UMFPACK) QR by Gram-Schmidt and Householder Eigenvalues and singular values Shifted QR method for eigenvalues Golub-Kahan method for the SVD Iterative solutions Preconditioned conjugate gradients for S = b Preconditioned GMRES for Ax = b Krylov-Amoldi for Az = Az Extreme eigenvalues of S Optimization Linear programming Semidefinite programming Interior point methods Convex Optimization Randomized linear algebra Randomized factorizations via pivoted QR A = C M R columns/mixing/rows Fast Founier Transform Repositories of high quality codes ACM Transactions on Mathematical Software Deep learning software Deep learning in Julia Deep learning in MATLAB Deep learning in Python and JavaScript Deep learning in R provides high quality codes for the optimization problems of operations research. BLAS LAPACK SuiteSparse, SuperLU LAPACK LAPACK LAPACK Trilinos Tnlinos ARPACK, Tnlinos, SLEPc see also BLOPEX CLP in COIN/OR CSDP in COIN/OR [POPT in COIN/OR CVvX, CVXR users.ices.utexas.edu/ ~pgm/main_codes.html| FFTW.org GAMS and Netlib.org TOMS Fluxml.ai/Flux.jl/stable Mathworks.com/learn/tutorials/deep—learning—onramp.html Tensorflow.org, Tensorflow.js Keras, KerasR A5 The Jordan Form of a Square Matrix Some square matrices A with repeated eigenvalues don’t have n independent eigenvectors. Therefore they can’t be diagonalized by X~'AX = A: X is not invertible. Jordan established a nearly diagonal form J = BAB™! for every matrix A. The “Jordan form” J has k Jordan blocks Ji, ... Ji when A has k independent eigenvectors. J has the same eigenvalues as A. Jh A 10 J=BAB = . . has Jordan blocks J; = : 1 Jk | Ai o _J -l If A can be diagonalized, then k = n and B = X. So J is the eigenvalue matrix A (all n blocks J, to J,, are 1 by 1). If A can’t be diagonalized, then one or more blocks will be larger. Each block J; of size n; has only one true eigenvector =; = (1,0,...,0) and one eigenvalue \\;. The matrix B contains eigenvectors of A along with “generalized eigenvec- tors” of A. Here is an example rather than a proof. '3 110 1 0 Example J=|0 3|o| hasA=33dwihonly o1 10tol nda=B-1B. o3| two genuine eigenvectors 0 , o R e o This Jordan form makes AN = B~!JVB and e!? = B~!e'/ B as simple as possible. For powers of J, we just compute the powers of each Jordan block : 3 11\" _[3¥ N3N-! 3 1],\\_[e* t o3| “ o 3V “P\\lo 3|7 0 & That exponential formula is telling us the missing solution to the differential equation dU/dt = JU (and also du/dt = Au). The usual solution has . We can’t just use that twice, when A = 3 is repeated. The missing solution is te3t. And a triple eigenvalue A = 3 with only one eigenvector (and one Jordan block) would also involve t2e3¢. The Cayley-Hamilton Theorem p(A) = Zero Matrix “Every matrix A satisfies its own characteristic equation p(A) = 0.” The determinant of A — Al is a polynomial p(A). The n solutions to p(A) = 0 are the eigenvalues of A. Our example above has p()\\) = (A — 3)3 with a triple eigenvalue A = 3, 3, 3. Then Cayley- Hamilton says that p(A) = (A — 3I)3 has to be the zero matrix. Jordan makes this easy, because (A — 3I)3 = B(J — 3I)3B~! and (J — 3I)3 is certainly the zero matrix : 0101 [o o0 o0 0 Example (J-3I*=|0 0 0| =|0 0 0| Then(4-30)3=0. ' 000] |0O0O A6 Tensors In linear algebra, a tensor is a multidimensional array. (To Einstein, a tensor was a function that followed certain transformation rules.) An ordinary matrix is a 2-way tensor. A 3-way tensor T is a stack of matrices. Its elements T;;x have three indices : row number t and column number j and “tube number” k. An example is a color image. It has 3 slices corresponding to red-green-blue. The slices T,.T,. T; show the density of one of those primary colors RGB (k = 1 to 3), at each pixel (7, ) 1n the image. ] p—— P — — vector matrix (2-way array) tensor (3-way array) Another example is a joint probability tensor. Now p; ;x 1s the probability that a random individual has (for example) age ¢ and height j and weight k. The sum of all those numbers Pijk Will be 1. For i = 9, the sum of all pg;r would be the fraction of individuals that have age 9—the sum over one slice of the tensor. A fundamental problem—with tensors as with matrices—is to decompose the tensor T into simpler pieces. For a matrix A that was accomplished by the SVD. The pieces that add to A are matrices (we should now say 2-tensors), with the special property that each piece is a rank-one matrix uov®. Linear algebra allowed us to require that the u’s from different pieces were orthogonal, and the v’s were also orthogonal and that there were only r pieces (r < mand r < n). Sad to say, this SVD format is not possible for a 3-way tensor. We can still ask for R rank-one pieces that approximately add to T\": C P Decomposition T'~ayobioci+---+agrobrocp. (1) Orthogonality of the a’s and of the b’s and of the ¢’s is generally impossible. The number of pieces is not set by T (its “rank” is not well defined). But an approximate decomposition of this kind is still useful in computations with tensors. One option is to solve alternately for the a; (with fixed b; and c;) and then for the b; (fixed a; and ¢;) and then for the ¢; (fixed a; and b;). Those subproblems can be reduced to least squares. Other approximate decompositions of T' are possible. The theory of tensor decompositions (multilinear algebra) is driven by applications. We must be able to compute with T'. So the algorithms for multiway tensors are steadily improving, even without the orthogonality properties of an SVD. A7 The Condition Number of a Matrix Problem The condition number measures the ratio of (change in solution) to (change in data). The most common problem is to solve n linear equations Az = b in n unknowns z. In this case the data is b and the solution is £ = A~'b. The matrix A is fixed. The change in the data is Ab and the change in the solutionis Az = A~! Ab. We have to decide the meaning of “change”. Do we compute the absolute change || Ab|| or the relative change ||Ab||/||b||? That decision for the data b will bring a similar decision for the condition number of A. Absolute max [|Az|| Lo |IA\"'Ab|| Relative _ max ||Az||/||z]] condition ~ b.Ab ||Ab|| ||Ab|| condition b, Ab ||Ab||/||b]| (D The absolute choice looks good but it has a problem. If we divide the matrix A by 10, we are multiplying A~! by 10. The absolute condition goes up by 10. But solving Az = b did not become 10 times harder. The relative condition number is the right choice. _ max ||[AT'Ab|| ||Az|] cond(d) = b ab \" [A6] el = ||A? = Jmax 2 A=Al = 2= (2) If A is the simple diagonal matrix X with entries 0y > - -+ 2 0y = Omin, then its norm is 0max = 1. The norm of A~! is 1/0min. The orthogonal matrices U and V in the SVD leave the norms unchanged. So the ratio o'max / Omin is cond(A). We are using the usual measure of length ||z||? = 22 + --- + z2. Notice that o (nof Amin) measures the distance from A to the nearest singular matrix. At first we might expect to see A — Apinl, bringing the smallest eigenvalue to zero. Wrong. The nearest singular matrix to A = ULV is U(X — omin])VT because the orthogonal matrices U and VT don’t affect the norm. Bring the smallest singular value to zero. Each eigenvalue of A also has a condition number. Suppose A is a simple root (not a repeated root) of the equation det(A — AI) = 0. Then Az = Az and ATy = Ay for unit eigenvectors ||z|| = ||y|| = 1. The condition number of \\ is 1/|yTz|. In other words it is 1/| cos 8|, where 0 is the angle between the right eigenvector & and the left eigenvector y. (The name left comes from the equation y* A = AyT, with yT on the left side of A.) Notice that a symmetric matrix A will have y = x with cos@ = 1. The eigenvalue problem is perfectly conditioned for symmetric matrices just as Qx = b is perfectly condi- tioned for orthogonal matrices with ||Q||||@ \"] = 1 = condition number of Q. The formula 1/|yTx| comes from the change AA ~ (yTAAz)/yTz in the eigenvalue created by a small change A A in the matrix. 407 A8 Markov Matrices and Perron-Frobenius This appendix is about positive matrices (all a;; > 0) and nonnegative matrices (all a;; >0). Markov matrices M are important examples, when every column of M adds to 1. Positive numbers adding to 1 makes you think of probabilities. A useful fact about any Markov matrix M : The largest eigenvalue is always A = 1. We know that every column of M — I adds to zero. So the rows of A/ — [ add to a row of zeros, and M — I is not invertible: A = 1 is an eigenvalue of M. Here are two examples: 5 0.8 0.3 0.2 0.7 1 ] has eigenvalues 1 and 5 B = [ 0 1 0 ] has eigenvalues 1 and —1 That matrix A is typical of Markov. The eigenvectors are £, =(0.6,0.4) and x, = (1,-1): (08 03][06]_, [06]. . i o A>0 Amx >0 = is a steady state 02 07|04 04 y Eigenvector > 0 r - - - - - 08 0.3 1 1 1. . e . { 02 07 t -1 | =3 -1 is a “transient” that disappears Our favorite example is based on rental cars in Chicago and Denver. We start with 100 cars in Chicago and no cars in Denver: y, = (100,0). Every month we multiply the current vector y,, by A to find y,,,; : the number in Chicago and Denver after n + 1 months: _ | 100 | 80 | 70 _ | 6 _ | 60 Yo = 0 Y = 20 Yy, = 30 Yz = 35 Yo = 40 | That steady state (60.40) is an eigenvector of A for A = 1. If we had started with Yo = (60,40) then we would have stayed there forever. Starting at (100, 0) we needed to get 40 cars to Denver. You see that number 40 at time zero reduced to 20 at time 1, 10 at time 2, and 5 at time 3. That is the effect of the other eigenvalue A = % dividing its eigenvector by 2 at every step: e Yy = 5 : _[60],[ 40 _[60 20 60 40 Yo = 40 40| Y17 40 —20 40 —40 Thisis y, = A\"y, coming from the single step equationy,,, ; = Ay,,. In matrix notation, A\" approaches rank one ! . i yany-1_[06 1][1 1 1] . ,e _[06 04 AT=(XAXT)T=XATX ‘[0.4 -1][ (%)\"][0.4 —0.6]—”1 ‘[0.4 0.4] You have now seen a typical Markov matrix with Amax = 1. Its eigenvector (0.6, 0.4) is the survivor as time goes forward. All small eigenvalues have A\\ — 0. But our second Markov example has a second eigenvalue A = —1. Now we don’t approach a steady state . 0 l] has eigenvalue \\; = 1 withx; = [ 1 : 1 1 0 ] andAg—-lwnhzg—[_l]. 5-| | A8 Markov Matrices and Perron-Frobenius 409 The zeros in B allow that second eigenvalue A3 = —1 to have the same size as A\\; = 1. All cars switch cities every month (Chicago to Denver and Denver to Chicago). If we start at y, = (60, 40) then the next month reverses to y, = By, = (40, 60): _[60] [50 10 _[40] _[50 10 Yo=l40|=[50]F|-10] Y1~ [60][50]7| -10] ¥Y2=¥ ¥s=¥i-- No steady state because A, = —1 also has size |\\o| = 1. This will not happen when the Markov matrix A has all a;; > 0. |A\\1| = |A2| might happen when B has some B;; = 0. Perron found the proof in the first case A;; > 0. Then Frobenius allowed B;; = 0. In this short appendix we stay with Perron. Every positive matrix A is allowed. Theorem (Perron) All numbers in Az = Amax Z are strictly positive. Proof Start with A > 0. The key idea is to look at all numbers ¢ such that Az > tz for some nonnegative vector & (other than £ = 0). We are allowing inequality in Az > tz in order to have many small positive candidates ¢. For the largest value tmax (which is attained), we will show that equality holds: Ax = tmax . Then tmax is our eigenvalue Amax and z is the positive eigenvector—which we now prove. If Ax > tmaxx is not an equality, multiply both sides by A. Because A > 0, that produces a strict inequality A’z > tmax Az. Therefore the positive vector y = Az satisfies Ay > tmax y. This means that tmax could be increased. This contradiction forces the equality Ax = tmaxx, and we have an eigenvalue. Its eigenvector Z is positive because on the left side of that equality, Az is sure to be positive. To see that no eigenvalue can be larger than tmax, suppose Az = Az. Since A and 2 may involve negative or complex numbers, we take absolute values: |A||z| = |Az| < Az by the “triangle inequality.” This |z| is a nonnegative vector, so this |A| is one of the possible candidates t. Therefore |\\| cannot exceed tmax—which must be Amax. Many Markov examples start with a zero in the matrix (Frobenius) but then A? or some higher power A™ is strictly positive (Perron). So we can apply Perron. These “pnmutive matrices” also have one steady state eigenvector from the eigenvalue A = 1. The big example is the Google matrix G that is the basis for the PageRank algonthm. (A major company survives entirely on linear algebra.) The matrix starts with A;; = 1 when page j has a link to page i. Then divide each column j of A by the number of outgoing links: now column sums = 1. Finally Gi; = adi; + (1 —a)/N is Google’s choice so that every Gi; > 0 (Perron). See Wikipedia and the book by Amy Langville and Carl Meyer (which is quickly found using Google). ) Reference Amy Langville and Carl Meyer, Google’s Page Rank and Beyond : The Science of Search Engine Rankings, Princeton University Press (2011). A9 Elimination and Factorization If a matnx A has rank r, then its row echelon form (from Gauss-Jordan elimination) contains the identity matrix in its first 7 independent columns. How do we interpret the matrix F' that appears in the remaining columns of that echelon form? F multiplies those first r independent columns of A to give its n — r de- pendent columns. Then F reveals bases for the row space and the nullspace of the orniginal matrix A. And F is the key to the column-row factorization A = CR. 1. A natural goal for any matrix A of rank r is to factor A into C times R, where C has r columns and R has r rows. Those columns and rows will be bases for the column space and the row space of A. (If we go carefully, our construction could prove that column rank = row rank for any matrix. And if we are truly careful, we could produce orthogonal columns in C and orthogonal rows in R—using singular vectors from the SVD.) Here we only accept actual columns of A in C, so the construction is more elementary and less stable. C contains the first r independent columns of A. Then all other columns of A must be combinations C'F of those independent columns. That matrix F is part of the row factor R = [ I F ] P,and A = CR is achieved: A=CR=[C CF|P=|Independent cols Dependent cols ] Permute cols (1) If the r independent columns come first in A, that permutation matrix will be P = I. Otherwise we need P to permute the n columns of C and CF into correct position in A. The factorization A = CR is confirmed but not computed. How do we determine the first 7 independent columns in A and the dependencies C F of the remaining n—r columns ? This 1s the moment for row operations on A. Three operations are allowed, to put A into “reduced row echelon form” . (a) Subtract a multiple of one row from another row (below or above) (b) Exchange two rows (c) Divide a row by its first nonzero entry All linear algebra teachers and a positive fraction of students know the reduced row echelon form rref(A). It contains an r by r identity matrix I (only zeros can precede those 1’s) in the rows of R. The position of I reveals the first r independent columns of A. The remaining m — r dependent rows of A must become zero rows to maintain rank r : 0 0 All our row operations (a)(b)(c) are invertible. (This is Gauss-Jordan elimination : operat- ing on rows above the pivot row as well as below.) As usual, the matrix that reduces A to this canonical form (echelon form) is less important than the factorization A = C R that it uncovers in equation (1). Elimination reduces A to rref(A) = [ I F ]P (2) 410 A9 Elmination and Factorization 411 2. Before we apply A = CR to solving Az = 0, we want to give a column by column (left to right) construction of rref(A) from A. After k columns, that part A of the matrix is in its own rref form. We are ready for column k + 1. This new column has an upper part u and a lower part £. First kK 4+ 1 columns [ I F u 0 0 ]Pk followed by [ P ] 3) The big question is : Does this new column k 4+ 1 join with I, or Fj, ? If £ is all zeros, the new column is dependent on the first k columns. Then u joins with F to produce Fj; in the next step to column k + 2. If £ is not all zero, the new column is independent of the first k£ columns. Pick any nonzero in £ (preferably the largest) as the pivot. Move that row of A up into row k + 1. Then use that pivot row to zero out (by standard elimination) all the rest of column k + 1. (That step is expected to change the columns after k + 1.) Column k + 1 joins with I to produce Ii ;. We are ready for column k + 2. At the end of elimination, we have a most desirable list of column numbers. They tell us the first independent columns of A. Those are the columns of C. They led to the identity matrix I, py - in the row factor R of A = CR. 3. Itis I and F and P that we put directly to use in solving Az = 0. They reveal a natural basis in X of n — r column vectors in the nullspace of A : A=C[ I F ]P muliplies X = P\" [ I\"F ] togive AX =0. (4 With PPT = I for the permutation, each column of X solves Az = 0. Those special solutions in X tell us what we know : Every dependent column of A is a combination of the independent columns in C. Gauss-Jordan elimination leading to A = CR is less efficient than the Gauss process that directly solves Az = b. The latter stops at a triangular system Uz = ¢, and solves for x by back substitution. Gauss-Jordan has the extra cost of eliminating upwards. If we only want to solve equations, stopping at a triangular factorization LU is faster. 4. Block elimination The row operations from A to its echelon form produce one zero at a time. A key part of that echelon form is an r by r identity matrix. If we think on a larger scale—instead of one row at a time—that output [ tells us that some r by r matrix has been inverted. Following that lead brings a “matrix understanding” of elimination. Suppose that the matrix W in the first rows and columns of A is invertible. Then elimination will take all its instructions from W ! One entry at a time—or all at once by “block elimination”—W will change to I. In other words, the first r rows of A will yield I and F. This identifies F' as W —! H. And the last m — r rows will become zero rows. 412 A9 Elimination and Factorization J K Block elimination A = [ W H ] 5 [ g ; F ] = rref(A). (5) That just expresses the facts of linear algebra : If A begins with r independent rows, then the remaining m—r rows are combinations of those first 7 rows : [ J K ] =JW-! [ W H ] : Those m — r rows get eliminated. In general that first r by r block might not be invertible. But elimination will find W. We can move IV to the upper left comer by row and column permutations Pp and Fk. Then the full expression of block elimination to row reduced echelon form is W H ] [ I W-H ] (6) P\"AP‘::[J K 0 0 5. This raises an interesting point. Since A has rank r, we know that it has r inde- pendent rows and r independent columns. Suppose those rows are in a submatrix B and those columns are in a submatrix C. Is it always true that the r by r “intersection” W of those rows with those columns will be invertible ? (Everyone agrees that somewhere in A there is an 7 by r invertible submatrix. The question is whether B N C' can be counted on to provide such a submatrix.) The answer is yes. The intersection of r independent rows of A with r independent columns does produce a matrix W of rank r. W is invertible. Proof : Every column of A is a combination of the r columns of C. Every column of the r by n matrix B is the same combination of the r columns of W. Since B has rank r, its column space is all of R\". Then the column space of W is also R\" and the square submatrix W has rank r. This note has been submitted for publication in the American Mathematical Monthly. If it is accepted, then a full acknowledgment of the priority of the Monthly will appear in future printings of this book. A10 Computer Graphics Computer graphics deals with images. The images are moved around. Their scale is changed. Three dimensions are projected onto two dimensions. All the main operations are done by matrices—but the shape of these matrices is surprising. The transformations of three-dimensional space are done with 4 by 4 matrices. You would expect 3 by 3. The reason for the change is that one of the four key operations cannot be done with a 3 by 3 matrix multiplication. Here are the four operations:. Translation (shift the origin to another point Py = (xo, yo, 20)) Rescaling (by c in all directions or by different factors c;, ¢z, c3) Rotation (around an axis through the origin or an axis through F;) Projection (onto a plane through the origin or a plane through F). Translation is the easiest—just add (xo, Yo, 20) to every point. But this is not linear! No 3 by 3 matrix can move the origin. So we change the coordinates of the origin to (0,0,0,1). This is why the matrices are 4 by 4. The “homogeneous coordinates” of the point (z, y, 2) are (r,y. 2z, 1) and we now show how they work. 1. Translation Shift the whole three-dimensional space along the vector vg. The ongin moves to (g, Yo, 20). This vector vy is added to every point v in R*. Using homogeneous coordinates, the 4 by 4 matrix T shifts the whole space by vy ‘1 0 0 O] . : 0O 1 0 O Translation matrix T = 0o 0 1 ol To Yo 2z 1] Important: Computer graphics works with row vectors. We have row times matrix instead of matrix times column. You can quickly check that [0 0 0 1]T = [zp yo 20 1. To move the points (0,0, 0) and (z, y, z) by vo, change to homogeneous coordinates (0,0,0,1) and (z,y, 2,1). Then multiply by T. A row vector times T gives a row vector. Every vmovestov+vo: [t y 2 1)T = [t +z0 y+yo 2+ 20 1} The output tells where any v will move. (It goes to v +vg.) Translation is now achieved by a matrix, which was impossible in R>. 2. Scaling To make a picture fit a page, we change its width and height. A copier will rescale a figure by 90%. In linear algebra, we multiply by .9 times the identity matrix. That matrix is normally 2 by 2 for a plane and 3 by 3 for a solid. In computer graphics, with homogeneous coordinates, the matrix is one size larger: Rescale the plane: S = 9 Rescaleasolid: S = O OO oo O O o0 OO _—O O O 413 414 A10 Computer Graphics Important: S is not cI. We keep the “1” in the lower corner. Then [z, y, 1] times S is the correct answer in homogeneous coordinates. The origin stays in its normal position because 001]S =[001]. If we change that 1 to c, the result is strange. The point (cx,cy,cz,c) is the same as (r.y.z.1). The special property of homogeneous coordinates is that multiplying by cl does not move the point. The ongin in R3 has homogeneous coordinates (0, 0,0, 1) and (0.0.0. c) for every nonzero c. This is the idea behind the word “homogeneous.” Scaling can be different in different directions. To fit a full-page picture onto a half- page, scale the y direction by % To create a margin, scale the x direction by %. The graphics matrix is diagonal but not 2 by 2. It is 3 by 3 to rescale a plane and 4 by 4 to rescale a space: LY (9% | ) Scaling matrices S = and S = 1 3 : : 1 - - That last matrix S rescales the z, y, z directions by positive numbers c;, ¢z, c3. The extra column 1n all these matrices leaves the extra 1 at the end of every vector. Summary The scaling matrix S is the same size as the translation matrix 7. They can be multiplied. To translate and then rescale, multiply vT'S. To rescale and then translate, multiply vST. Are those different? Yes. The point (z.y. z) in R® has homogeneous coordinates (z,y, 2, 1) in P3. This “pro- jective space” is not the same as R*. It is still three-dimensional. To achieve such a thing, (cx. cy. cz, c) is the same point as (z, y, 2, 1). Those points of projective space P* are really lines through the origin in R*. Computer graphics uses affine transformations, linear plus shift. An affine transforma- tion T is executed on P° by a 4 by 4 matrix with a special fourth column: 'au a2 as 0- rT(l,0,0) 0- A= a1 az2 a3 0 _ T(O,I,O) 0 asy Qa3z2 AQass 0 - T(O, 0, 1) 0 au ae agn 1] [T(0,0,0) 1 The usual 3 by 3 matrix tells us three outputs, this tells four. The usual outputs come from the inputs (1,0,0) and (0, 1,0) and (0, 0, 1). When the transformation is linear, three outputs reveal everything. When the transformation is affine, the matrix also contains the output from (0, 0,0). Then we know the shift. 3. Rotation A rotation in R? or R is achieved by an orthogonal matrix Q. The determi- nant is +1. (With determinant —1 we get an extra reflection through a mirror.) Include the extra column when you use homogeneous coordinates! ‘cosf —sinf O] ] becomes |R = | siné cosf O 0 0 1 ] - cosf@ —sinfé Plane rotation Q= [sin 0 cosf A10 Computer Graphics 415 This matrix rotates the plane around the origin. How would we rotate around a different point (4,5)? The answer brings out the beauty of homogeneous coordinates. Translate (4, 5) to (0, 0), then rotate by 0, then translate (0,0) back to (4,5): 1 0 0] [cosf§ —sind 0] [1 0 O] vI_RT, =z y 1]| 0 1 Of|sinf coséd 0]|0 1 0]. -4 -5 1|[ 0 0 1) (4 5 1] I won’t multiply. The point is to apply the matrices one at a time: v translates to vT_, then rotates to vT_ R, and translates back to vT_ RT., . Because each point [z Y 1] IS a row vector, T_ acts first. The center of rotation (4. 5)—otherwise known as (4,5, 1)—moves first to (0.0.1). Rotation doesn’t change it. Then T, moves it back to (4,5,1). All as it should be. The point (4, 6. 1) moves to (0, 1, 1), then turns by 6 and moves back. In three dimensions, every rotation @ turns around an axis. The axis doesn’t move—it is a line of eigenvectors with A = 1. Suppose the axis is in the 2 direction. The 1 in Q is to leave the z axis alone, the extra 1 in R is to leave the origin alone: cosf —sinf 0 Q g Q= |sinf cosf O and R= 0 0 1 0 - - LO 0 0 IJ Now suppose the rotation is around the unit vector @ = (a;. a2, az). With this axis a, the rotation matrix () which fits into R has three parts: - 2 p- - /] a;ar a1a37 0 asy —Qa; Q= (cosf) + (1 —cosf) |ajaa a3 azaz| —sinf|-a3 0 a (1) aas aoas a§ a —Qa OJ The axis doesn’t move because a@Q = a. When a = (0,0,1) is in the 2 direction, this Q becomes the previous ()—for rotation around the z axis. The linear transformation () always goes in the upper left block of R. Below it we see zeros, because rotation leaves the origin in place. When those are not zeros, the transfor- mation is affine and the origin moves. 4. Projection In a linear algebra course, most planes go through the origin. In real life, most don’t. A plane through the origin is a vector space. The other planes are affine spaces, sometimes called “flats.” An affine space is what comes from translating a vector space. We want to project three-dimensional vectors onto planes. Start with a plane through the origin, whose unit normal vector is n. (We will keep n as a column vector.) The vectors in the plane satisfy nTv = 0. The usual projection onto the plane is the matrix I — nn™. To project a vector, multiply by this matrix. The vector n is projected to zero, and the in-plane vectors v are projected onto themselves: I-nn\"n=n-nnTn)=0 and (I-nnTv=v-nnTv)=v. 416 A10 Computer Graphics In homogeneous coordinates the projection matrix becomes 4 by 4 (but the origin doesn’t move): T I —-nn Projection onto the plane n' v =0 P = - -0 O O - - Now project onto a plane nT (v — vy) = 0 that does not go through the origin. One point on the plane is vg. This is an affine space (or a flat ). It is like the solutions to Av = b when the night side is not zero. One particular solution vg is added to the nullspace—to produce a flat. The projection onto the flat has three steps. Translate v to the origin by T_. Project along the n direction, and translate back along the row vector vyg: — T Projection onto a flat T_PT, = [ vI (1)] [I gm (1)] [vI (1)] : — Vo 0 I can’thelp noticing that T_ and T, are inverse matrices: translate and translate back. They are like the elementary matrices of Chapter 2. The exercises will include reflection matrices, also known as mirror matrices. These are the fifth type needed in computer graphics. A reflection moves each point twice as far as a projection—the reflection goes through the plane and out the other side. So change the projection I — nnT to I — 2nnT for a mirror matrix. The matrix P gave a “parallel” projection. All points move parallel to n, until they reach the plane. The other choice in computer graphics is a “perspective” projection. This 1s more popular because it includes foreshortening. With perspective, an object looks larger as it moves closer. Instead of staying parallel to n (and parallel to each other), the lines of projection come toward the eye—the center of projection. This is how we perceive depth in a two-dimensional photograph. The basic problem of computer graphics starts with a scene and a viewing position. Ideally, the image on the screen is what the viewer would see. The simplest image assigns just one bit to every small picture element—called a pixel. It is light or dark. This gives a black and white picture with no shading. You would not approve. In practice, we assign shading levels between 0 and 2° for three colors like red, green, and blue. That means 8 x 3 = 24 bits for each pixel. Multiply by the number of pixels, and a lot of memory is needed! Physically, a raster frame buffer directs the electron beam. It scans like a television set. The quality is controlled by the number of pixels and the number of bits per pixel. In this area, the standard text is Computer Graphics: Principles and Practice by Hughes, Van Dam, McGuire, Skylar, Foley, Feiner, and Akeley (3rd edition, Addison-Wesley, 2014). Notes by Ronald Goldman and by Tony DeRose were excellent references. A10 Computer Graphics 417 m REVIEW OF THE KEY IDEAS = 1. Computer graphics needs shift operations T'(v) = v+wvg as well as linear operations T(v) = Av. 2. A shift in R™ can be executed by a matrix of order n + 1, using homogeneous coor- dinates. 3. The extra component 1 in [z y 2 1] is preserved when all matrices have the numbers 0.0.0,1 as last column. Problem Set 10.3 1 A typical point in R? is zi 4 yj + zk. The coordinate vectors 4, j, and k are (1,0, 0), (0,1,0), (0.0.1). The coordinates of the point are (z, y, z). This point in computer graphics is zt + yj + zk + origin. Its homogeneous coord:- nates are ( , , . ). Other coordinates for the same pointare ( , , , ). A linear transformation T is determined when we know T (1), T(j), T (k). For an affine transformation we also need T'( ). The input point (z,y, z,1) is trans- formedto T (2) + yT'(3) + zT(k) + Multiply the 4 by 4 matrix T for translation along (1,4,3) and the matrix T; for translation along (0, 2, 5). The product T'T; is translation along . Write down the 4 by 4 matrix S that scales by a constant c. Multiply ST and also TS, where T is translation by (1,4,3). To blow up the picture around the center point (1, 4, 3), would you use vST or vTS? What scaling matrix S (in homogeneous coordinates, so 3 by 3) would produce a 1 by 1 square page from a standard 8.5 by 11 page? What 4 by 4 matrix would move a corner of a cube to the origin and then multply all lengths by 2? The comer of the cube is originally at (1, 1, 2). When the three matrices in equation 1 multiply the unit vector a, show that they give (cos@)a and (1 — cos #)a and 0. Addition gives aQ = a and the rotation axis is not moved. If b is perpendicular to a, multiply by the three matrices in 1 to get (cos )b and 0 and a vector perpendicular to b. So Qb makes an angle 6 with b. This is rotation. What is the 3 by 3 projection matrix I — nnT onto the plane %:c + %y + %z =0?In homogeneous coordinates add 0,0, 0, 1 as an extra row and column in P. 418 A10 Computer Graphics 10 With the same 4 by 4 matrix P, multiply T_ PT, to find the projection matrix onto the plane %:c + % y+ %z = 1. The translation T_ moves a point on that plane (choose one) to (0,0, 0, 1). The inverse matrix Ty moves it back. 11 Project (3, 3, 3) onto those planes. Use P in Problem 9 and T_ PT in Problem 10. 12 If you project a square onto a plane, what shape do you get? 13 If you project a cube onto a plane, what is the outline of the projection? Make the projection plane perpendicular to a diagonal of the cube. 14 The 3 by 3 mirror matrix that reflects through the plane nTv = 0is M = I - 2nnT. Find the reflection of the point (3,3, 3) in the plane 2z + 2y + 12 = 0. 15 Find the reflection of (3,3, 3) in the plane %.’L‘ + %y + %z = 1. Take three steps T_MT, using 4 by 4 matrices: translate by T_ so the plane goes through the origin, reflect the translated point (3, 3,3, 1)T- in that plane, then translate back by T,. 16 The vector between the origin (0, 0,0, 1) and the point (z, y, z, 1) is the difference v = . In homogeneous coordinates, vectors end in . So we add a to a point, not a point to a point. 17 If you multiply only the last coordinate of each point to get (z, y, z, c), you rescale the whole space by the number . This is because the point (z,y, z,c) is the sameas( , , ,1). Index of Equations A=CR,22,93,97,98, 142,401 AF 50,235 A=CR=[C CF |P,142,410 A~ =CT/det A, 208 A=CW™'B, 108, 401 Anew = B~1AB, 322 A=CTC, 261 A,;; = F(i/N,j/N), 301 A= LDU, 63, 401 |A = Ak|| < ||A - B[, 302, 303 A= LU,>53,59, 83, 401 |AB||F <||A||r||B||F, 303 A= QR, 143,176, 182, 185, 197, 401 |Az - b||* = ||Az - p||* + ||e]|?, 164 A=QS, 197, 402 ||AZ — b||? = error, 164 A=QTQ!: Schur, 256 1Al = ||Z|, 302 A=UZVT, 286,287 |AllF = Vo2 +--- + 02, 302 A= XAX\"1,232, 233,277,401 B=W-1V,319 A= Mziy] + -+ aZny,, 245 BA =1,103, 147 [w H I F BA # AB, 28 A‘[ J K ] - [ 0 0 ]\"“2 B-LAB. 327 A=uvT, 230 C = vDLT, 261 (AB)C = A(BC), 27, 29, 290 E =zTSz,252 (AB)T = BT AT, 67 EA = Ry = rref(A), 142 (AT = (A7), 67 EA=U,49,53 (det AB) = (det A)(det B), 205 E-'=1L,53 A(B+C)=AB+ AC,29 F(c ® d) = (Fc).*(Fd), 268 AB-BA=1,244 Fpin = 10T (ATS714)~'b, 358 AB = BA: Same A, 244 Fiy2 = Fiyy + Fi, 236 AB =1Iand BA =1, 147 H=WEF,95 ACT = (det A)1, 204 J = B~14B, 327 AV = UYL, 286, 287 S A AV, = U,X,, 289 KKT=] 41 ¢ [3% AX = XA, 233 K = toeplitz[2,-1,0,0], 78 A~CMR, 108 KU/h*=F, 1 A\\b= A~1b, 105 L=1zTSz - AT (ATz - b), 358 Av = ou, 287 My\" + Ky=f,275 At = (ATA)\"1AT, 190 P = A(ATA)1AT, 151, 155, 185 AT = AT(AAT)~1, 190 P =QQT, 185 At = ptct,193 P =aa\"/a\"a, 154 At = RT(CTART)~'CT, 185, 195 PA = LU, 45,65,401 A+ = vs+UT, 195, 402 P2 = P = PT, 151,154,157, 159 AT(b - AZ) = 0, 156 P! = P, 64 ATA =8, 256 Feolumn = A4, 191 ATAZ = ATb, 155, 163 Prow = AT A, 191 AT = — A, 282 Q|| = |||, 178 A% =0, 329 Q=1I-2uu’, 178 AF = XAkX 1,233 Qt =QT, 185 419 420 QTQ = 1,176,177 (Qz)T(Qy) ==y, 178 R(z) = z'Sz/x Tz, 296 R=[1 F].9% R=[1 F |P,939 RT=Q7'b, 182 Ry = rref(A), 93 Ry = rref(A) = 6 FO ] P, 94 Rox =d, 104 S = AT A, 249, 250, 256 S =ATCA, 261 S =CTC, 401 S = QAQT, 246, 254, 256, 401 S =QAQ7 1,246 S=8ST,69 S =M\\ziz] + Mxax], 257 (ST)(v) = S(T'(v)), 309 T (M) = AM, 316 T(u)=c1T(v1)+--- T(u) = du/dz, 312 T(cv) = cT(v), 315 T(cv ( + cpT(v,), 310 cv + dw) = ¢ T(v) + d T (w), 309 f df/d:z 309 = [, f(t)dt, 309 ( ) =0, 321 T,-J'k = a:jk vké,J, 352 U=K\\F,78 Uz =c, 41 X-1AX = A, 232 Yo = 2Ya +Ya_1/(At)?, 274 [X. E] = eig(A), 226 Az = -S~1(z)VF(z), 336 Ay =y(z + h) - y(z), 74 A~ = CT/det A, 202 F(z,v) = F1(...(F2(Fy(v)))), 346 F(z,v;) =~ w;, 346 F(z,y) = 3(x? + by?), 342 Fin=-a'S1a/2, 339 (X) = log (det X) 340 =N Et l 346 l(m v,) = F(z, v,) -w,, 347 F(xz*) =0, 336 Index of Equations VF=(8F/6$1, VF =Sz —b,336 vQ(z) = Sx — b, 335 Z=z+iy=a — iy, 262 |2|2 = z times Z = r?, 262 e*® = cos @ + isin@, 262 ,OF [01,), 335 e=b-p, 151 aTb = —a, 153 p aTa T4+l = Tk + Az, 336 = (-b+ Vb? — dac)/2a, 262 x = A\\b, 45 T =, + Ty, 105,107 T = Tpart + any Ty, 104 T = Trow + Tpyl] » 145 r = % vazl Z(F(a:.v,-) — w,-), 346 TSz = \\xTx , 248 TSz > 0,248, 253, 260 “’complete = Tp + 18] + C282, 110 yTb=y\"(Az) = (ATy)Tz < ', 365 yTA = \\yT, 245 z = |z|e*® = reif, 262 z\" = r\"e'n9 262 2122 = | 21| |22|€(@11+93) 262 u(t) = Cet, 270 t) = Ce\"‘m, 271 ( ( u(t) =e4 u(O) 277 (t) = eAtu(0) = Xert X 1u(0),270 u = ez when Az = Az, 270, 271 ur = Arug, 238 ur = XA X 1y, 232, 238 Uk = Aug, 238 cos n = T, (cos @), 333 cosf ~1—6%/2,269 det A = sum of all Dp, 208 det A = A1A2 cee An, 244 det A = + (product of the pivots), 205 det AB = (det A)(det B), 198, 206 det AT = det A, 204, 205 det E < |ley]|...||en||, 214 det = )\\, A2 A, 228 Index of Equations oF oF OF| _ F L 2L 9h g, —6-;=8FL_1 OFL_g . 6::: Rel < 0, 270 I F rref(A) = [ 0 0 }P,410 cond(A) = [|A7 || || A||] = Omax/min, 407 N(ATA) = N(A), 151, 161 MAB) = A(BA), 292 A= (1++/5)/2,237 A\\=Z'Sz/T x, 264 A=w,w?,...,wN\"1 1,265 AN =1, 265 [ A I]—+[I A1 ], 57 (A b]>[ Ry d], 104 VF(z + Ax) = VF(zx) + S(z) Az =0, 336 Q Q=11263,269 5\" = 5,258, 264 TSz = ATz, 247 OF*/0b = (ATS 1A4)~1b = \\*, 358 sinf = 6 — 63 /6, 269 IMQ)| =1, 263 lev]] = : IF1I* = [ 1f(z)I? dz, 308 [€ks1 — || = 1/4 |2k — 2*||%, 337 [v+ w|| < ||v]] + [|w]], 363 |v||? = 7\" v, 263 a/z =b; andaTx > cj, 360 b= Anin/Amax, 342 cv + dw, 2 cleMtxy + ... + cpetntz,, 272,277 ‘dQ'U./dxz = f(.’B), 77 du/dt = e’z = Au, 271 du/dt = \\u, 270 d*F [dz? > 0, 336 d*u/dt? = Au, 279 eAl = 1 4 At + L(At)2 +---,276 e’ and tet, 278 et = XeA‘ X~1,277 etz = eMg, 276 eAeB # eBeA, 282 421 (e#)? = 24,283 (eAt)-l — e-At 278 (f, ) [ f(z)g(z)dz, 308, 332 d%y/2dz?, 74 2 mﬂ+bd—+ky 0,273 mA? + bA + k= 0,273 mu’l +bu’ + ku = 0,285 n > m,98 p(A) = zero matrix, 244 p = Az, 155 p=AZ = A(ATA)1ATb, 155 g(z) =1/2zTSz + z7¢, 360 w = e*™/N 265 z1 = det B,/ det A, 207 Tk+1 = Tk — Sk VF(zx), 337 y\"' = -y,273 y(t) = cyy1 + caya, 273 1/z =%/|z|? 262 522 + 8zy + 5y% = 9X2+Y%2=1,254 GM < AM, 239 F(z + Az) =~ F(z) + (Az)TVF +3 (Az)T S (Az), 336 dim(V) + dim(W) = dim(VNW) +dim(V+W), 128 Circulants CD = DC, 268 Dot product 5w, 263 Hermitian 3 = S, 263 Markov Amax = 1, 408 Max = min, 365, 367 Max flow = Min cut, 366 Pivot = Dy /Dy, 249 Rank (AB) < rank(A), 137, 400 Rank (AB) < rank(B), 103, 137, 400 Rank of AT A = rank of A, 400 Row rank = Column rank, 32, 400 Schur A = QTQ™!, 402 Stability of ODE ReA < 0, 275 Steepest slope = ||V F|], 340 Trace XY =trace Y X, 244 Transpose of A = d/dt, 69 Unitary matrix QT = Q1,263 1, 254 Index of Notations Matrices AAT, 70, 287, 290, 296 AB and BA, 236 A\\b, 105 At 196 ATA, 70, 149, 157, 167, 287, 290, 296 A’ A, 263 AT, 68 ATSA, 253 ATS-14, 362 CR, 31 CW-1B,99 E, 49,53 eAt 270,272, 276, 282 F,93 F(z + Ax), 336 H, 311 K,T,B,176 L,53 LDLT, 70 (L+)L, 149 LU, 48 M, 108 P, 93 PAQ, 66 R, 33,93 Ry = mref(A), 33, 93, 108, 113, 410 0 A S=| gt 0| T(S(u)), 313, 315 U, 42 U.xV,268 295 W, 95,99, 411 X, 232 A, 232 3, 286, 289 (0f/0g) (0g/0x), 347 —1, 2, —1 matnix, 298 log(det X), 340, 345 Vectors F(z,v), 346 arg min F', 339 grad FF=VF, 336 b— Az, 155 c ® d, 267, 268 c % d, 268 f(g(x)), 347 x*, 335 ¢! norm, 355 €2 norm, 302, 355 £°° norm, 355 ¢P norm, 355 rand, 48 Vector Spaces C(A), 20-25, 88 C(AB), 37 C(AT), 88 C(ZT) and N(ZT), 264 N(A), 88 S+T,92 SUT, 92 S, 149 U-> VoW, 321 VUW, 128 Z, 86,121 422 Numbers, Functions, Codes Amax., 409 [ A b ], 104 [ Ro d ], 104 x, 155, 163 02,290 O max / O min» 407 |A| S g1, 293 |Az||/||]|, 293 n? steps, 58 n3/3 steps, 58 ryT/zTy, 236 AM, 239 GM, 239 Julia, 184, 351, 370, 385 LAPACK, 184, 404 Python, 184, 350, 351, 370, 385 MATLAB, 169, 184, 233, 351,404 BLAS, 28, 404 COIN/OR, 404 chebfun, 333 DFT, 330 FFT, 402 Keras, 350 Multiplier A\\, 357 PCA, 286, 302, 304, 306 RelU, 346, 353 plot2d(H), 317 SGD, 348 SVD, 286, 302 Index For computer codes please see the Index of Notations and the website A Acceleration, 74, 272, 344 Active set, 361 Adaptive method, 345 Add vectors, 2 Adjugate matrix CT, 202 Affine, 310 Age-height distribution, 305 Alexander Craig, 347 All-ones matrix, 241 Alternating matrix, 56 Angle between vectors, 11, 12, 15 Antisymmetric matrix, 257, 259 Area of parallelogram, 211, 212 Area of triangle, 211 Arg min, 339 Associative Law, 29, 38, 290 Augmented matrix, 45, 104 Automatic differentiation, 338, 347 Axes of an ellipse, 214, 231, 251, 254 B Back substitution, 41, 58, 83, 109 Backpropagation, 335, 338, 347, 349, 351 Backslash, 105 Backward difference, 76, 79, 275 Banded matrix, 78 Basis, 22, 33, 84, 115, 118, 122, 125, 130, 147, 289, 310 Basis for nullspace, 230 Batch normalization, 352 Baumann compression, 300 Best straight line, 167 Bidiagonal matrix, 294 Big Figure, 132, 146, 166, 191 Big formula for det A, 198, 208, 210 Bipartite graph, 366 Block elimination, 70, 412 Block matrix, 56, 70, 71, 99, 103 Boundary condition, 76, 79 Bowl, 212, 251, 252 Box volume, 213 Breakdown of elimination, 43 C Calculus, 74, 165, 336, 338 Cayley-Hamilton, 244, 405 Center point (2. 3), 173 Center the data, 286 Centered difference, 75, 82, 338 Chain of functions, 346, 347 Chain rule, 347, 349 Change of basis, 123, 318, 319, 323, 325 Chebyshev polynomial basis, 332, 333 Chess matrix, 140 Cholesky factorization S = CTC, 250, 261 Circulant matrix, 266, 268, 269, 327, 330 Closest line, 163, 167, 171, 304 Closest rank k matrix, 302 Coefficient matrix, 39 Cofactors of det A, 198, 201, 340 Column matrix C, 21, 22, 30,97 Column operations, 29 Column picture, 19, 44 Column rank, 23, 37 Column space, 18, 20, 25, 87, 109, 289 Column space of AB, 91 Column way, 4, 27, 28 Columns times rows, 35, 60 Combination of columns, 19 Commutative, 28 Companion matrix, 229, 278 Complement V4, 145 Complete solution, 104, 105, 107, 110, 271, 282 Complex conjugate, 262 Complex eigenvalues, 265 Complex Hermitian, 264 Complex numbers, 247, 262 Component, 1 424 Compressing images, 297, 299 Computational graph, 351 Computer graphics, 310, 413 Computing eigenvalues, 294 Condition number, 344, 407 Congruent matrices, 253 Conjugate pairs A = a £ b1, 263 Constant coefficients, 270, 331 Constant diagonals (Toeplitz), 78, 331 Constraints, 335, 356, 358, 360, 369 Convex function, 251, 335, 336, 340 Convex in r, concave in A, 359 Convexity, 356, 404 Convolution matrix, 78 Convolution rule, 268 Corner, 364, 369 Correct language, 116 Cosine formula, 11, 12 Cost, 335, 357, 364 Cost of elimination, 57 Counting parameters, 403 Counting Theorem, 101, 133, 312 Covariance matrix, 174, 286, 305 Cramer’s Rule, 207, 209 Cnitical damping, 285 Cube, 7, 8 Cube in n-dimensions, 214 Current Law, 102, 135, 195 Cyclic convolution, 267, 269, 331 D Damping matrix, 272, 275 Dantzig, 364 Deep learning, 255, 335, 404 Dense matrix, 78 Dependent columns, 20, 95, 124 Derivative matrix, 320 Derivative of the cost, 357, 363 Derivatives of det X, 340 Derivatives of || Az ||2, 174 Derivatives of E = || Az - b||2, 165 Descent factor, 344 Det A has n! terms, 208 Determinant, 50, 54, 78, 200, 276 Determinant 1 or —1, 199 Index Determinant of A,,, 200, 201 Determinant test, 249 Diagonal matrix A or X, 168 Diagonalization, 232, 242, 256, 287 Diamond, 355 Difference, 74, 76 Difference equation, 77, 274 Diffusion, 280 Dimension, 33,98, 115, 120, 122, 145 Dimension reduction, 307 Distributive Law, 29 Domain, 310 Don’t multiply AT A, 294 Dot product, 9, 68 Dual problem, 362, 365 E Echelon form, 94, 95, 410 Eckart-Young, 302, 303, 306 Edge matrix, 212, 214 Effective rank, 306 Eigenfaces, 307 Eigenvalue matrix, 232 Eigenvalues of A*, 240 Eigenvalues of AB, 227 Eigenvector basis, 322 Eigenvector matrix, 232, 233, 238 Eigenvectors of circulants, 265, 266 Eight rules, 84, 85, 89 Electrical circuits, 362 Elimination, 32, 42, 58, 93, 96, 141,410 Elimination matrix, 39, 42, 49 Ellipse, 251, 254, 317 Empty set, 121 Energy T Sz, 246, 248 Equivalent tests, 249 Error, 151, 155, 337 Euler’s formula, 135, 262, 269, 276 Even permutation, 71, 204 Every row and column, 200 Exchange matrix, 27, 43 Existence of solution, 147 Exponential convergence, 348 Exponentials, 270 Extend to a basis, 119 Index F Factorization, 34, 258, 403,410 Factorization of Fy, 267 Fast convergence, 337 Fast Fourier Transform, 267, 402 Fast start, 348 Feasible set, 365 Fibonacci numbers, 210, 236, 237, 242 Field, 308 Filter, 78 Find a basis, 119 Finite element, 362 First order, 336 First order accurate, 76 First order system, 270 Fitting a line, 167 Five tests for positive definite, 250 Fixed supports, 77, 80 Flag, 297 Floor, 145, 149 Fluid flow, 362 Forcing term, 285 Formula for A~!, 201, 208 Forward difference, 76, 338 Four possibilities for Az = b, 108 Four subspaces, 84, 129, 132, 150, 264, 289 Four ways to multiply AB, 35 Fourier basis, 264, 332 Fourier matnx, 265, 269, 327, 330, 402 Fourier series, x, 170, 179, 332, 334 Fourier transform, 269 Fredholm Alternative, 148 Free columns, 98 Free parameters, 403 Free variables, 94 Free-free matrix, 79 Frobenius norm, 302 Full circle, 274 Full column rank, 104, 106, 117 Full row rank, 104, 107, 108 Full SVD, 289 Function space, 84, 85, 121, 127, 327 Fundamental subspaces, 129, 132, 143 425 Fundamental Theorem, 84, 133, 146 G . Galileo, 170 Gauss, 262 Gauss-Jordan, 56, 57 Generalized eigenvector, 327, 328, 405 Geometry of the SVD, 288 Golden mean, 237 Golub-Kahan method, 294, 404 Golub-Van Loan, 294 Google matrix, 409 Gradient, 335, 336, 339, 340 Gradient descent, 2595, 339, 337, 347 Gram-Schmidt, 175, 180, 182, 256, 333 Graph, 74, 134, 135, 194 Group of matrices, 73 Growth factor, 285 H Hadamard matrix, 184, 214, 241 Hadamard product, 268 Hadamard’s inequality, 215 Heart rate, 174 Heat equation, 279 Heavy ball, 342 Hermitian matrix, 258, 263, 269 High degree polynomial, 346 High probability, 108 Hilbert matrix, 299, 331 Hilbert space, 264, 308 Horizontal line, 173 House matrix, 311, 317 Householder reflections, 184, 404 Hyperplane, 38 I Identity matrix I, 5, 309, 351 Identity transformation, 319 Image processing, 297 Imaginary eigenvalue, 257 Incidence matrix, 102, 134, 135, 140, 194 Independent, 115, 124, 147, 157 Independent columns, 20, 30, 190, 410 Independent eigenvectors, 234, 235 Independent rows, 190 426 Inequalities, 360 Inequality constraints, 361, 364 Infinite dimensions, 332 Infinitely many solutions, 40 Initial value, 270 Inner product, 68, 69 Input basis, 318 Input space, 309 Input vector, 286, 346 Integration Tt,313 Integration by parts, 69 Interpolate, 346 Intersection of spaces, 92 Inverse matrix, 39, 50, 103, 202 Inverse of AB, 51 Inverse of E, 49, 52 Inverse transformation, 313 Invertible, 50, 95, 314, 412 Invertible submatrix, 102, 146 Isometric, 323 Iterations, 348 Iterative solutions, 404 J Japanese flag, 301 Jeopardy, 364 Jordan block, 329, 334, 405 Jordan form, 236, 327-329, 402, 405 K Konig's theorem, 366 Kaczmarz, 348 Kalman filter, 158 Kernel is nullspace, 313, 315 Kirchhoff’s Laws, 102, 135, 195, 362 KKT matrix, 359-361 L Lagrange multipliers, 293, 357, 364 Lagrangian, 357, 358, 360, 363 Laplace transform, 285 Large angle, 12 Largest o’s, 302 Law of Cosines, 14 Law of Inertia, 253, 363 Lax (Peter), 245 Index Leading determinants, 249 Leaky ReLU, 353 Leapfrog method, 275, 283 Learning function F'(x, v), 255, 346 Learning rate (stepsize), 337 Least squares, 143, 164, 304, 346, 348 Left eigenvectors, 245 Left inverse, 50, 103, 190 Left nullspace N(AT), 131, 141 Left singular vector, 286 Legendre basis, 332 Length of vector, 9 Level direction, 341 Line of matrices, 87 Line of solutions, 107 Linear combination, 2, 85 Linear in each row (det A), 200 Linear independence, 115, 116 Linear programming, 364 Linear transformation, 308, 309, 318, 320 Linearly dependent, 117 Lines to lines, 311 Loop in the graph, 135 Loss function, 346, 347 Lucas numbers, 240 M Machine learning, 255 Main diagonal, 43 Many bases, 119 Markov equation, 280 Markov matrix, 227, 235, 408 Mass matrix, 275 Matrix, 1, 18 Matrix approximation, 297 Matrix chain rule, 347 Matrix exponential, 270, 272, 276, 282 Matrix multiplication, 27, 29, 34, 35 Matrix norm, 356 Matrix space, 84, 90, 121, 127, 140 Maximin, 359 Maximum flow, 366 Mean square error, 175 Mechanics, 272 Meshwidth, 76 Index 427 Minimax = maximin, 359, 365, 368 Nullspace, 88,93, 97, 142, 144, 291, 411 Minimize a function, 335, 339 Minimum cost, 358 Minimum cut, 366 Minimum norm solution, 112, 146, 190 Minimum point £*, 255, 335 Minimum problems, 252 Mixed strategy, 364, 367 Mixing matrix, 108 Modified Gram-Schmidt, 183 Molecular dynamics, 275 Momentum, 342, 343 Multilinear algebra, 351 Multiple eigenvalue, 239 Multiplier, 1, 49, 53 N Narrow valley, 342 Nearest singular matrix, 296, 407 Negative eigenvalue, 257 Nesterov, 345 nethb.org, 28, 58 Network equations, 362 Neural net, 346 Newton's Law F' = ma, 272 Newton’s method, 336 No loops, 241 No repeated eigenvalues, 233 No row exchanges, 61 No solution, 40, 148 Nocedal-Wright, 360, 361 Node, 134 Nondiagonalizable, 239 Nonlinearity, 362 Nonzero eigenvalues, 292 Nonzero pivots, 41 Nonzero rows of Rp, 130 Nonzero solutions of Az = 0, 98 Norm, 355 Normal equation, 156, 167 Normal matrix, 402 Normal vector, 13 Not a norm, 355 Not diagonalizable, 234, 334 Nuclear norm, 302 0 Ohm’s Law, 362 One and only one solution, 40 One sample input, 348 One step, 336 Optimal strategy, 369 Optimization, 255 Orthogonal columns, 168 Orthogonal complement, 144, 146, 154 Orthogonal component, 145 Orthogonal eigenvectors, 246, 263 Orthogonal inputs, 287 Orthogonal matnix, 73, 176, 177, 246 Orthogonal projection, 159 Orthogonal subspaces, 149 Orthogonal vectors, 144 Orthogonality (Victory of), 197 Orthogonalize, 175, 180 Orthonormal, 143, 176, 197, 246, 290 Outer product, 68 Output basis, 318, 320 Output space, 309 Output vector, 286, 346 Overdamping, 285 P Paradox for instructors, 258 Parallelogram, 7, 215 Parameter count, 403 Partial derivatives, 336 Partial differential equation, 279 Partial pivoting, 66 Particular solution, 105, 282 Payoff matrix, 367, 369 Penrose, 194, 196 Perfect matching, 366 Penod 27, 274 Permutation, 45, 56, 64, 65, 71, 142, 178, 199, 265, 330,410 Perpendicular, 10, 304 Perpendicular distances, 304 Perron’s Theorem, 409 Picture proof, 292 Index 428 - T Reduced SVD, 289 46 _ l;;f:;:ﬁi;:i%é Reflection matrix, 178, 326 Pivot, 41, 42, 63, 66, 96, 105 Regression, 39‘? 207 Pivot columns, 105, 109, 130 gelatnlve con4;)t;;on, : iti ite, 249 ental cars, l;;vote te5St for posiive definite Repeated eigenvalue, 243, 279, 281, 328 poa:l;r ’form of r + iy, 262 Repeated root, 278, 281 Polynomial has n roots, 262 Residual, 168 . Pooling, 352 Reuse comquatlons, .350 Positive definite, 79, 246, 247, 250 Reverse ldentltysmgzngx.;s? - Positive eigenvalues, 247, 363 Reverse order, 45, ’ T ’ Positive pivots, 79, 249, 363 Reverse subspaces for A™, 191 Positive semidefinite, 246, 247, 253 Right angles, 176 Powers of A. 235. 238 Right inverse, 50, 103, 190 Principal axis theorem, 254 Right si}ngular vector, 286 Principal components, 286, 302, 304, 306 Right tnang;;,59 Probability, 36, 386395 Roots of 1, Product of pivots, 198, 205 Rotation, 177, 228, 288, 322, 346 iecti PRy Roundoff errors, 66 Projecting b onto a, 153 ’ Prg;:zt;mgl 143. 151. 153. 194 Row exchange, 43, 45, 47, 65, 199 Projection matrix, 151, 154, 155, 179 EOW Opetratw';;, ﬁ, 139, 142 Projects, 286 Ow piclure, 15, Prgjof of A= LU. 5860 Row space, 22, 23, 88, 144, 289 Proof of the SVD, 290 Row way, 4, 27, 28 Pseudoinverse, 169, 191, 193, 196 rref, 34, 95, 96 Pythagoras, 9, 14 S Q Saddle point, 259, 261, 359, 363, 367, 368 Quadratic formula, 228 Saddle-point matrix, 70, 359-361 Quadratic model :;43 Same eigenvalues, 235 Quadratic programming, 358, 360 Same four subspaces, 141 Sample columns, 108 R Schur complement, 359 Ramp function ReLU(z), 338, 352 _ Schur’s Theorem, 256, 264 Random matrix, 36, 38 Schwarz inequality, 9, 13, 231, 303 Random sampling, 108, 294 Scree plot, 306 Range is column space, 310, 313 Second derivative matrix, 252 Rank, 22, 198 Second difference, 76, 274 Rank r matrix, 403 Second difference matrix, 249, 269 Rar!k one, 23, 2?, 60, 139, 286, 287, 294 Second order approximation, 74, 76 Ratio of determinants, 207, 260 Second order equations, 270, 272, 285 Rayleigh quotient, 264, 293 Second order term, 74, 76, 336 Real eigenvalues, 246 Semidefinite, 249-251 Semidefinite programming, 368 Sharp corner, 355, 356 Reduce to a basis, 119 Reduced row echelon form (see rref), 95 Index Shearing, 314 Shift is not linear, 309 Shift-invariant, 78 Shifted Q R algorithm, 294 Shifted times, 168 Short videos, 285 Shortest solution, 169 Sigmoid function, 352 Signal processing, 268 Signs of eigenvalues, 253 Similar matrix, 235, 236, 329 Simplex method, 364 Singular matrix, 46, 79 Singular value, 287, 307 Singular vectors, 231, 287, 291, 323 Skew-symmetric, 282 Slope of the roof, 340 Small angle, 12 Small multiplications, 35, 37 Solvable equations, 87 Span, 88, 115, 118, 122, 125, 147 Spanning trees, 241 Sparse, 78, 356 Special solutions, 93, 94, 97, 99, 100, 105, 109, 141, 142 Spectral norm, 302 Spectral Theorem, 246 Spiral, 274 Springs, 80 Square loss, 346, 349 Square root QvVAQT, 260 Squared length, 9 Squares to parallelograms, 316 Stability u(t) — 0, 276 Stable matrix, 270, 276 Standard basis, 118, 319 Statistics, 305 Steady state, 280, 408 Steepest descent, 255, 337, 341 Steepest edge, 364 Stepsize, 337, 338 Stiffness matrix, 275, 362 Stochastic gradient descent, 335, 348 Strassen, 28 Stress-strain, 362 Stretching, 288 Strictly convex, 251, 252, 255, 336 Strohmer and Vershynin, 348 Subspace, 835, 86, 90 Subspaces for AT, 195 Sum of errors, 348 Sum of rank-1, 34, 245 Sum of spaces, 92 Sum of squares, 197, 254, 256 SVD, 288, 290, 406 SVD fails for tensors, 351 Symmetric and orthogonal, 259 Symmetric inverse, 70 Symmetric matrix, 62, 64, 69, 246 T Tangent line, 74, 336 Tangent parabola, 74, 75, 336 Taylor series, 75, 336 Tensor, 349, 351, 352, 406 TensorFlow, 350, 351 Test data, 346 Third grade multiplication, 267 Three formulas for det A, 198 Three properties of det A, 206 Tilted ellipse, 261 Tim Baumann, 299 Toeplitz matnx, 78 Tonga’s flag, 298 Too many equations, 163 Total loss L(z), 346 Total variance, 306 Trace of a matrix, 276 Training data, 255, 346 Transforms, 179 Transient, 408 Transpose, 6769 Transpose of AB and A~1, 67 Trapezoidal method, 283 Trees, 135 Trefethen-Bau, 285 429 Triangle inequality, 9, 13, 303, 355, 363 Triangle of 1’s, 298, 299 Triangular matrix, 41, 43, 51, 264 430 Index Tndiagonal matrix, 63, 78, 294 Vandermonde matrix, 171, 210 Two equal rows, 204, 206 Variable coefficient, 331 Two person game, 364, 367 Variance, 302 Two-sided inverse, 50, 52 Vector, 1 U Vector space, 31, 32, 84,89, 115 Unbiased, 173 Vector-matrix step, 349 Uncertainty Pninciple, 231 Vertical distances, 164, 165 Undamped oscillation, 273 Voltages, 135, 362 Underdamping, 285 Volume of a box, 198, 212 Underdetermined, 108 Volume of a pyramid, 214 Union of spaces, 92 W Uniqueness, 115, 147 Unit ball, 355, 363 \\xau, 145, 149 Unit circle, 11 ave equa.tnon, 279 Unit vector, 10, 12, 14, 287 Weak duality, 365, 369 Unitary matrix, 269 We!)sne, 351 Update average, 158 Weights, 346 Upper triangular, 39, 41, 42, 182 7 \\% Zero in pivot, 43 Valley, 251 Zero vector, 86, 89, 119 Value of the game, 368 Zig-zag, 342, 343 Six Great Theorems of Linear Algebra / Dimension Theorem All bases for a vector space have the same number of vectors. \\ Counting Theorem Dimension of column space + dimension of nullspace = number of columns. Rank Theorem Dimension of column space = dimension of row space. This is the rank. Fundamental Theorem The row space and nullspace of A are orthogonal complements in R™. SVD There are orthonormal bases (v's and u’s for the row and column spaces) so that Av; = o Ui. Qpectral Theorem If ST = S there are orthonormal q's sothat Sqg; = Aiq; and S = QAETJ Linear Algebra Websites math.mit.eduflinearalgebra This book’s website has problem solutions and extra help math.mit.edu/learningfromdata Linear Algebra and Learning from Data (2019) ocw.mit.edu MIT’s OpenCourseWare includes video lectures in Math 18.06, 18.065, 18.085 math.mit.edu/weborder.php To order all books published by Wellesley-Cambridge Press","libVersion":"0.3.2","langs":""}