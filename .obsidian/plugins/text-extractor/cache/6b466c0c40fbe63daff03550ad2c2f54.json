{"path":"sem5/VLSI1/VRL/Top-Down-Digital-VLSI-Design/Appendix-A---Elementary-Digital-Electronics_2015_Top-Down-Digital-VLSI-Desig.pdf","text":"APPENDIX A ELEMENTARY DIGITAL ELECTRONICS A.1 INTRODUCTION Working with electronic design automation (EDA) tools requires a good understanding of a multitude of terms and concepts from elementary digital electronics. The material in this chapter aims at explaining them, but makes no attempt to truly cover switching algebra or logic optimization as gate-level synthesis is fully automated today. Readers in search of a more formal or more comprehensive treatise are referred to specialized textbooks and tutorials such as [219] [220] [36] and the seminal but now somewhat dated [221].1 Textbooks that address digital design more from a practical perspective include [222] [223] [224] [225] [226]. Combinational functions are discussed in sections A.2 and A.3 with a focus on fundamental properties and on circuit organization respectively before section A.4 gives an overview on common and not so common bistable memory devices. Section A.5 is concerned with transient behavior which then gets distilled into a few timing quantities in section A.6. At a much higher level of abstraction, section A.7 finally sums up the basic microprocessor data transfer protocols. A.1.1 COMMON NUMBER REPRESENTATION SCHEMES Our familiar decimal number system is called a positional number system because each digit in a number contributes to the overall value with a weight that depends on its position (which was not so with the ancient Roman numbers, for instance). In a positional number system, there is a natural number B ≥ 2that servesas a base,e.g. B = 10 for decimal and B = 2 for binary numbers. Each digit position i is assigned a weight Bi so that when a non-negative number gets expressed with a total of w digits, the value follows as a weighted sum (aiMSD , aiMSD−1, aiMSD−2, ..., aiLSD+1, aiLSD )B = iMSD∑ i=iLSD aiBi (A.1) where iMSD ≥ iLSD and w = iMSD − iLSD + 1. A radix point is used to separate the integer part made up of all digits with indices iMSD ≥ i ≥ 0 from the fractional part that consists of those where −1 ≥ i ≥ iLSD. When writing down an integer, we normally assume iLSD = 0. As an example, 17310 stands for 1 · 102 + 7 · 101 + 3 · 100 (iMSD = 2, iLSD = 0, w = 3). The leftmost digit position has the 1 Those with a special interest in mathematics may want to refer to appendix 3.11 where switching algebra is being put into perspective with fields and other algebraic structures. 473 474 APPENDIX A ELEMENTARY DIGITAL ELECTRONICS largest weight BiMSD while the rightmost digit has the smallest weight BiLSD. In the context of binary numbers, these two positions are referred to as the most (MSB) and as the least significant bit (LSB) respectively. 101.012, for instance, stands for 1 · 22 + 0 · 21 + 1 · 20 + 0 · 2−1 + 1 · 2−2 = 5.2510 (iMSB = 2, iLSB =−2, w = 5). Table A.1 Representations of signed and unsigned integers with four bits. bit pattern interpreted int e rpre ted as s i gned MSB LSB as unsigned offset-binary 2’s complem. 1’s complem. sign & magn. a3 a2 a1 a0. O-B 2’C 1’C S&M 15 7 −1 [− ] 0 −7 14 6 −2 −1 −6 13 5 −3 −2 −5 12 4 −4 −3 −4 11 3 −5 −4 −3 10 2 −6 −5 −2 9 1 −7 −6 −1 1111. 1110. 1101. 1100. 1011. 1010. 1001. 1000. 8 0 −8a −7 [− ] 0 0111. 7 −1 7 7 7 0110. 6 −2 6 6 6 0101. 5 −3 5 5 5 0100. 4 −4 4 4 4 0011. 3 −5 3 3 3 0010. 2 −6 2 2 2 0001. 1 − 7 1 1 1 0000. 0 −8a 0 0 0 bit weights 23 22 21 20. idem − 23 −23 22 21 20. −(23−1) 22 21 20. ±(22 21 20). sign inversion n.a. (a3 a2 a1 a0)+1 a3 a2 a1 a0 a3 a2 a1 a0 VHDL type unsigned n.a. signed n.a. n.a. SysVer modif. unsigned n.a. signed n.a. n.a. a Has no positive counterpart, sign inversion rule does not apply. As for signed numbers, several schemes have been developed to handle them in digital circuits and computers. Table A.1 illustrates how the more common ones map between bit patterns and numbers. For conciseness, integers of merely four bits are tabulated there. The leftmost bit always indicates whether a number is positive or negative. Except for that one bit, offset- binary and 2’s complement encodings are the same. What they further have in common is that the most negative number has no positive counterpart (with the same number of bits). Conversely, two patterns for zero exist in 1’s complement and in sign-and-magnitude representation which complicates the design of adders, subtractors, comparators, and other arithmetic units. What makes the 2’s complement format so popular is the fact that any adder circuit can be used for subtraction if arguments and result are coded in this way. A.1 INTRODUCTION 475 Observation A.1. Digital hardware just deals with 0s and 1s and attaches no semantic meaning to the symbols it manipulates. What gives a bit pattern a meaning as a character, as a signed or unsigned, as an integer or fractional number, as a fixed-point or floating point number, etc., essentially is the interpretation by humans, or by human-made software. A bit pattern remains absolutely devoid of meaning unless the pertaining number representation scheme is known.2 Hardware description languages (HDL) such as VHDL and SystemVerilog provide designers with various data types and with index ranges to help them keep track of number formats. A.1.2 FLOATING POINT NUMBER FORMATS By definition, a floating point number a encoded using binary digits obeys a = s · f · 2 e (A.2) where s is the sign, f the mantissa, and e the exponent. Word width follows as w = 1 + # e + # f . The industry standard IEEE 754 defines coding formats for w = 16, 32, 64 and 128 respectively. A single precision floating point number, for instance, occupies w = 32 bit with # e = 8and # f = 23, and is formatted s eeee eeee \u0002 ffff ffff ffff ffff ffff fff . The standard further stipulates that numbers are to be normalized to the interval [1...2) prior to encoding which implies that the binary digit immediately preceding the radix point is always 1 and, hence, devoid of information. Rather than storing the full mantissa 1 \u0002 ff...f , only the fractional bits to the right of the radix point ff...f are made part of the data word. The uncoded bit to the left is referred to as hidden bit. The exponent is coded in offset-binary format with a bias of 2# e−1 −1. In the occurrence of # e = 8, this means that e must be augmented by 127 before being stored in the ee...e field, and vice versa for decoding. In theory, the exponent can so cover a range of −(2# e−1 −1) ≤ e ≤ 2# e−1. As for the sign bit, s = 0 for positive and s = 1 for negative numbers by convention. Now for the exceptions. First observe that it is not possible to express zero as permanently tying the hidden bit to 1 does not allow for a zero mantissa, and so renders the above coding scheme utterly useless. A more viable format must accommodate at least one non-normalized number. Then there must be a way to tell that a result lies beyond the numerical range representable with the bits available, just think of 1 0 and other calculations. We further wish to express that 0 0 , taking the square root of a negative number, and similar operations yield undefined results. Not a number (NaN) is the name for such arithmetic exceptions. A practical floating point system must thus set aside a few bit patterns to accomodate particular situations. In the IEEE 754 std, those special codes are easily identified by their exponent field made of either 0sor 1s throughout. Interpretation thus departs from a strictly mathematical one for two out of the 2# e possible exponent patterns. Table A.2 illustrates all this with the aid of a toy format seee \u0002 ffff 2 As an analogy, a pocket calculator handles only numbers and does not know about any physical unit involved, e.g. [m], [kg], [s], [μA], [k\u0002] and [USD]. It is up to the user to enter arguments in correct units and to know how to read the results. Table A.2 Toy example for a floating point format with eight bits w = 8. bit pattern binary mathematical interpretation special meaning assigned msb lsb code as a normalized number to bit pattern se 2 e1 e0.f − 1... exp. mant. num. value interpreted as mant. end value 1111[1].1111 255 4 1+ 15 16 − 31 NaN n.a. none ... ... ... ... ... NaN n.a. none 1111[1].0000 240 4 1+ 0 16 − 16 neg. inﬁnity n.a. −∞ 1110[1].1111 239 3 1+ 15 16 − 15.5 ... ... ... ... ... 1110[1].0000 224 3 1+ 0 16 − 8.0 ... ... ... ... ... four more binades of negative numbers ... ... ... ... ... 1001[1].1111 159 − 2 1+ 15 16 − 0.484 375 ... ... ... ... ... 1001[1].0000 144 − 2 1+ 0 16 − 0.250 000 1000[1].1111 143 − 3 1+ 15 16 − 0.242 187 5 1000[0].1111 0+ 15 16 − 0.117 187 5 ... ... ... ... ... ... ... ... 1000[1].0010 130 − 3 1+ 2 16 − 0.140 625 0 1000[0].0010 0+ 2 16 − 0.015 625 0 1000[1].0001 129 − 3 1+ 1 16 − 0.132 812 5 1000[0].0001 0+ 1 16 − 0.007 812 5 1000[1].0000 128 − 3 1+ 0 16 − 0.125 000 0 1000[0].0000 0+ 0 16 [− ]0 0111[1].1111 127 4 1+ 15 16 31 NaN n.a. none ... ... ... ... ... NaN n.a. none 0111[1].0000 112 4 1+ 0 16 16 pos. inﬁnity n.a. + ∞ 0110[1].1111 111 3 1+ 15 16 15.5 ... ... ... 1+ ... ... 0110[1].0000 096 3 1+ 0 16 8.0 0101[1].1111 095 2 1+ 15 16 7.75 ... ... ... ... ... 0101[1].0000 080 2 1+ 0 16 4.00 0100[1].1111 079 1 1+ 15 16 3.875 ... ... ... ... ... 0100[1].0000 064 1 1+ 0 16 2.000 0011[1].1111 063 0 1+ 15 16 1.937 5 ... ... ... ... ... 0011[1].0000 048 0 1+ 0 16 1.000 0 0010[1].1111 047 − 1 1+ 15 16 0.968 75 ... ... ... ... ... 0010[1].0000 032 − 1 1+ 0 16 0.500 00 0001[1].1111 031 − 2 1+ 15 16 0.484 375 ... ... ... ... ... 0001[1].0000 016 − 2 1+ 0 16 0.250 000 0000[1].1111 015 − 3 1+ 15 16 0.242 187 5 0000[0].1111 0+ 15 16 0.117 187 5 ... ... ... ... ... ... ... ... 0000[1].0010 002 − 3 1+ 2 16 0.140 625 0 0000[0].0010 0+ 2 16 0.015 625 0 0000[1].0001 001 − 3 1+ 1 16 0.132 812 5 0000[0].0001 0+ 1 16 0.007 812 5 0000[1].0000 000 − 3 1+ 0 16 0.125 000 0 0000[0].0000 0+ 0 16 0 A.1 INTRODUCTION 477 where # e = 3, # f = 4, and where the offset for the exponent is +3.3 For clarity, the hidden bit is shown between square brackets [.] there. Last but not least, note that handy IEEE 754 converters are available on the Internet. A.1.3 NOTATIONAL CONVENTIONS FOR TWO-VALUED LOGIC The restriction to two-valued or bivalent logic4 seems to suggest that the two symbols 0 and 1 from switching algebra should suffice as a basis for mathematical analysis. This is not so, however, and two more logic values are needed so that we end up with a total of four symbols.5 0 stands for a logic zero. 1 stands for a logic one. X denotes a situation where a signal’s logic state as 0 or 1 remains unknown after analysis. - implies that the logic state is left open in the specifications because it does not matter for the correct functioning of a circuit. One is thus free to substitute either a 0 or a 1 during circuit design which explains why this condition is known as don’t care. The mathematical convention for identifying the logic inverse of a term, aka “Boolean complement”, is by overlining it. That is, if a is a variable, then its complement shall be denoted a.6 Most obviously, one has 0 = 1, 1 = 0, and a = a. 3 While the data type is float(3 downto -4) in VHDL notation, such a format is too restrictive to be useful. 4 Note that binary is almost always used instead of bivalent. This is sometimes misleading as the same term also serves to indicate that a function takes two arguments. The German language, in contrast, makes a distinction between “zweiwertig” (bivalent) and “zweistellig” (taking two arguments). 5 This is still insufficient for actual circuit design. A more adequate set of nine logic values has been defined in the IEEE 1164 standard and is discussed in full detail in section 4.2.3; what we present here is just a subset. 6 Unfortunately, this practice is not viable in the context of EDA software because there is no way to overline identifiers with ASCII characters. A more practical and more comprehensive naming scheme is being proposed in section 6.7. Taking the complement is expressed by appending the suffix _B to the original name so that the Boolean complement of Qux is denoted as Qux_B (pronounced “qux bar”). For the present appendix, however, we mostly stick to one-letter names, such as q and q, as multi-character strings tend to obfuscate the math. 478 APPENDIX A ELEMENTARY DIGITAL ELECTRONICS A.2 THEORETICAL BACKGROUND OF COMBINATIONAL LOGIC A digital circuit is said to be combinational if its present output gets determined by its present input exclusively when in steady-state condition. Such a circuit has no state and can, therefore, just as well be qualified as memoryless. This contrasts with sequential logic, the output of which depends not only on present but also on past input values. Sequential circuits must necessarily keep their state in some kind of storage elements, which is why they are also referred to as state-holding or as memorizing. In this section, we confine our discussion to combinational functions and begin by asking “How can we state a combinational function and how do the various formalisms differ?” A.2.1 TRUTH TABLE Probably the most popular way to capture a combinational function, is to come up with a truth table, that is with a list that indicates the desired output for each input. Table A.3 A truth table of three variables that includes don’t care entries. xy z g 00 0 1 00 1 1 01 0 - 01 1 - 10 0 1 10 1 0 11 0 - 11 1 0 Let us calculate the number of possible logic functions of n variables. Observe that a truth table comprises 2n fields each of which must be filled either with a 0 or a 1 (don’t care conditions do not contribute any extra functions). So there are 22n different ways to complete a truth table and, hence, 22n distinct logic functions. n functions 1 4 2 16 as listed in table A.5 3 256 4 65 536 A.2 THEORETICAL BACKGROUND OF COMBINATIONAL LOGIC 479 A.2.2 THE n-CUBE A geometric representation is obtained by mapping a logic function of n variables onto the n- dimensional unit cube. This requires a total of 2n nodes, one for each input value. Edges connect all node pairs that differ in a single variable. A drawing of the n-cube for truth table A.3 appears in fig.A.1.Note that the concept of n-cubes can be extended to arbitrary numbers of dimensions although representing them graphically becomes increasingly difficult. x y z g(x,y,z) 1(000) 1(001) - (011) - (010) 1(100) 0(101) 0(111) - (110) FIGURE A.1 3-cube equivalent to table A.3. A.2.3 KARNAUGH MAP The Karnaugh map, an example of which is shown in table A.4, is another tabular format where each field stands for one of the 2n input values. The fields are arranged such as to preserve adjacency relations from the n-cube when the map is thought to be inscribed on a torus. Although, extensions for five and six variables have been proposed, the merit of easy visualization which that Karnaugh maps so popular tends to get lost beyond four variables. Table A.4 Karnaugh map equivalent to table A.3. yz g 00 01 11 10 x 0 1 1 - - 1 1 0 0 - A.2.4 PROGRAM CODE Logic operations can further be captured using an HDL or some other formal language, an example is given in listing A.1. Note that, while the function described continues to be combinational, its description is procedural in the sense that the processing of the associated program code must occur step by step. 480 APPENDIX A ELEMENTARY DIGITAL ELECTRONICS LISTING A.1 A piece of behavioral VHDL code that is equivalent to table A.3 entity gfunction is port ( X: in Std_Logic; Y: in Std_Logic; Z: in Std_Logic; G: out Std_Logic ); end gfunction; architecture procedural of gfunction is begin process (X,Y,Z) variable temp: Std_Logic; begin temp := ’-’; if Y=’0’ then temp := ’1’; end if; if X=’1’ and Z=’1’ then temp := ’0’; end if; G <= temp; end process; end procedural; A.2.5 LOGIC EQUATIONS What truth tables, Karnaugh maps, n-cubes, and procedural HDL code examples have in common, is that they specify — essentially by enumeration — input-to-output mappings. Put differently, they all define a logic function in purely behavioral terms. Logic equations, in contrast, also imply the operations to use and in what order to apply them. Each such equation suggests a distinct gate-level circuit and, therefore, also conveys information of structural nature. Even in the absence of don’t care conditions, a great variety of logically equivalent equations exist that implement a given truth table. Since, in addition, it is always possible to expand a logic equation into a more complex one, we note Observation A.2. For any given logic function, there exist infinitely many logic equations and gate-level circuits that implement it. Fig.A.2 illustrates the symbols used in schematic diagrams to denote the subcircuits that carry out simple combinational operations. Albeit fully exchangeable from a purely functional point of view, two equations and their associated gate-level networks may significantly differ in terms of circuit size, operating speed, energy dissipation, and manufacturing expenses. Such differences often matter from the perspectives of engineering and economy. A.2 THEORETICAL BACKGROUND OF COMBINATIONAL LOGIC 481 buffer inverter 2-input AND 2-input OR 2-input NAND 2-input NOR power connection logic 1 ground connection logic 0 2-input XOR 2-input EQV 2 3-input MAJ 3-input MIN 2 1 0 2-way MUX AB CI S CO full adder AB S CO half adder++ FIGURE A.2 Schematic icons of common combinational functions. Example The Karnaugh map below defines a combinational function of three variables. Equations (A.3) through (A.12) all implement that very function. Each such equation stands for one specific gate- level circuit and three of them are depicted next. They belong to equations (A.6), (A.11) and (A.12) respectively. More circuit alternatives are shown in fig.A.4. yz f 00 01 11 10 x 0 1 1 0 0 1 0 1 0 1 x y z (b) (a) (c)f x y z f y z x f FIGURE A.3 A selection of three circuit alternatives for the same logic function. \u0002 482 APPENDIX A ELEMENTARY DIGITAL ELECTRONICS A.2.6 TWO-LEVEL LOGIC Sum-of-products Any switching function can be described as a sum-of-products (SoP) where sum and product refer to logic or and and operations respectively, see equations (A.3) and (A.4), for instance.7 Disjunctive form is synonymous to sum-of-products. A product term that includes the full set of variables is called a minterm or a fundamental product. The name canonical sum stands for a sum-of-products expression that consists of minterms exclusively. The right-hand side of (A.3) is a canonical sum whereas that of (A.4) is not. f = x y z ∨ x yz ∨ x yz ∨ xy z (A.3) f = x y ∨ yz ∨ xy z (A.4) Product-of-sums As the name suggests, product-of-sums (PoS) are dual to SoP formulations. Not surprisingly, the concepts of conjunctive form, maxterm, fundamental sum, and canonical product are defined analogously to their SoP counterparts. Two PoS examples are given in (A.5)and (A.6), you may want to add the canonical product form yourself. f = (x ∨ y ∨ z)(x ∨ y ∨ z)(x ∨ y)(x ∨ y)(x ∨ y ∨ z) (A.5) f = (x ∨ y ∨ z)(y ∨ z)(x ∨ y) (A.6) Other two-level logic forms SoP and PoS forms are subsumed as two-level logic, aka two-stage logic, because they both make use of two consecutive levels of or and and operations. Any inverters required to provide signals in their complemented form are ignored as dual-rail logic is assumed.8 As a consequence, not only (A.3) through (A.6), but also (A.7)and (A.8) qualify as two-level logic. f = (x y) (yz) (xy z) (A.7) f = x y z ∨ yz ∨ xy (A.8) Incidentally, observe that (A.7) describes a circuit that consists of nand gates and inverters exclusively. As illustrated in fig.A.4, this formulation is easily obtained from (A.4) by applying De Morgan’s 7 We will denote the sum and product operators from switching algebra as ∨ and ∧ respectively to minimize the risk of confusion with the conventional arithmetic operators + and · . However, for the sake of brevity, we will frequently drop the ∧ symbol from product terms and write xyz when we mean x ∧ y ∧ z. In doing so, we imply that ∧ takes precedence over ∨. 8 The term dual-rail logic refers to logic families where each variable is being represented by a pair of signals a and a that are of opposite value at any time (e.g. in CVSL). Every logic gate has two complementary outputs and pairwise differential inputs. Taking the complement of a variable is tantamount to swapping the two signal wires and requires no extra hardware. This situation contrasts with single-rail logic where every variable is being transmitted over a single wire (e.g. in standard CMOS and TTL). A complement must be obtained explicitly by means of an extra inverter. A.2 THEORETICAL BACKGROUND OF COMBINATIONAL LOGIC 483 theorem9 followed by bubble pushing, that is by relocating the negation operators from all inputs of the second-level gates to the outputs of the first-level gates. f x y z (a) (b) (c) de Morgan bubble pushing f x y z f x y z FIGURE A.4 Translating an SoP logic (a) into a NAND-NAND circuit (c) or back. Observation A.3. It is always possible to implement an arbitrary logic function with no more than two consecutive levels of logic operations. This is why two-level logic is said to be universal. The availability of manual minimization methods, such as the Karnaugh or the Quine-McCluskey method [221], the multitude of circuit alternatives to be presented in sections A.3.1 through A.3.3, together with the — now largely obsolete — belief that propagation delay directly relates to the number of stages have further contributed to the popularity of two-level logic since the early days of digital electronics. A.2.7 MULTI-LEVEL LOGIC Multi-level logic, aka multi-stage logic, differs from two-level logic in that logic equations extend beyond two consecutive levels of or and and operations. Examples include (A.9)and (A.10)where three stages of ors and ands alternate, (A.11) with the same operations nested four levels deep also belongs to this class. f = (x ∨ z) y ∨ x (y z) (A.9) f = (x z ∨ y)(x ∨ y ∨ z) (A.10) f = x y ∨ x (yz ∨ yz) (A.11) Equation (A.12) below appears to have logic operations nested no more than two levels deep as well, yet the inclusion of an exclusive or function10 makes it multi-level logic. This is because the xor function 9 The De Morgan theorem of switching algebra states x ∨ y = xy (= x ∧ y ) and (x ∧ y =) x y = x ∨ y. 10 The exclusive or xor is also known as antivalence operation, and its negated counterpart as equivalence operation eqv or xnor. Please further note that or and and operations take precedence over xor and eqv. 484 APPENDIX A ELEMENTARY DIGITAL ELECTRONICS is more onerous to implement than an or or an and and because substituting those for the xor results in a total of three consecutive levels of logic operations. f = x z ⊕ y (A.12) The circuits that correspond to (A.11)and (A.12) are depicted in fig.A.3b and c respectively. Drawing the remaining two schematics is left to the reader as an exercise. Originally somewhat left aside due to the lack of systematic and affordable procedures for their minimization, multi-level logic has become popular with the advent of adequate computer tools. VLSI also destroyed the traditional preconception that fewer logic levels would automatically bring about shorter propagation delays. A.2.8 SYMMETRIC AND MONOTONE FUNCTIONS A logic function is said to be totally symmetric iff it remains unchanged for any permutation of its variables; partial symmetry exists when just a subset of the variables can be permuted without altering the function. A logic function is characterized as being monotone or unate iff it is possible to rewrite it as a sum-of-products expression where each variable appears either in true or in complemented form exclusively. If all variables are present in true form in the SoP, then the function is called monotone increasing, and conversely monotone decreasing if all variables appear in their complemented form. Examples c = xy ∨ xz ∨ yz (A.13) s = xyz ∨ x yz ∨ xy z ∨ x y z (A.14) h = x ∨ y z = xy ∨ xz (A.15) m = xz ∨ yz (A.16) n = xy ∨ xz (A.17) o = wx ∨ yz = w y ∨ w z ∨ x y ∨ x z (A.18) function name symmetric monotone c (A.13) 3-input majority (MAJ) totally increasing s (A.14) 3-input exclusive or (XOR) totally no h (A.15) anonymous partially increasing m (A.16) 2-way multiplexer (MUX) no no n (A.17) anonymous no yes o (A.18) anonymous partially decreasing \u0002 A.2 THEORETICAL BACKGROUND OF COMBINATIONAL LOGIC 485 A.2.9 THRESHOLD FUNCTIONS Many combinational functions can be thought to work by counting the number of variables that are at logic 1 and by producing either a 0 or a 1 at the output depending on whether that figure exceeds some fixed number or not.11 Per force, all such threshold functions are totally symmetric and monotone. Examples include or and and functions along with their inverses. Probably more interesting are the majority function (maj) and its inverse the minority function (min) that find applications in adders and as part of the Muller-C element. maj and min gates always have an odd number of inputs of three or more. This is because majority and minority are mathematically undefined for even numbers of arguments and are of no practical interest for a single variable. In the occurrence of a 3-input maj gate (A.13), the condition for a logic 1 at the output is # 1′s ≥ 2 as reflected by its icon in fig.A.6c. A.2.10 COMPLETE GATE SETS A set of logic operators is termed a (functionally) complete gate set if it is possible to implement arbitrary combinational logic functions from an unlimited supply of its elements. Examples and counterexamples Complete gate sets include but are not limited to the subsequent sets of operations: {and, or, not}, {and, not}, {or, not}, {nand}, {nor}, {xor, and}, {maj, not}, {min}, {mux}, and {inh}. As opposed to these, none of the sets {and, or}, {xor, eqv}, or {maj} is functionally complete.10 \u0002 Though any complete gate set would suffice from a theoretical point of view, actual component and cell libraries include a great variety of logic gates that implement up to one hundred or more distinct logic operations to better support the quest for density, speed and energy efficiency. Several complete gate sets have cardinality one which means that a single operator suffices to construct arbitrary combinational functions. One such gate that deserves special attention is the 4-way mux. It is in fact possible to build any combinational operation with two arguments from a single such mux without rewiring. Consider the circuit of fig.A.5 where the two operands are connected to the multiplexer’s select inputs. For each 4-bit value that gets applied to data lines p3 through p0, the multiplexer then implements one out of the 16 possible switching functions listed in table A.5. The 4-way mux so effectively acts as a 2-input gate the functionality of which is freely programmable from externally.12 11 Incidentally, observe the relation to artificial neural networks that make use of similar threshold functions. 12 As an extension, note that a 4-way mux can be made to implement a 3-input function by allowing its inputs p3 through p0 not only to connect to 0 or 1 but to a third argument in direct z or negated z form as well. 486 APPENDIX A ELEMENTARY DIGITAL ELECTRONICS Table A.5 The 16 truth tables and switching functions implemented by the circuit of fig.A.5. assignment function implemented assignment function implemented p3 p2 p1 p0 p3 p2 p1 p0 x x y y p setting q = name p setting q = name 0 0 null, never 15 1 unity, always 1 x ∨ y NOR,Pierce 14 x ∨ y OR,sum 2 xy INH x, inhibit 13 x ∨ y y implies x 4 x y INH y,inhibit 11 x ∨ y x implies y 3 x NOT x 12 x pass x 5 y NOT y 10 y pass y 6 x ⊕ y XOR, antival. 9 x ⊕ y EQV , equival. 7 xy NAND, Sheffer 8 1100 11 00 1010 10 10 0000 11 11 0001 11 10 0010 11 01 0100 10 11 0011 11 00 0101 10 10 0110 10 01 0111 10 00 xy AND, product 0 1 2 3 q xy p 0 p 1 p 2 p 3 (a) (b) (c) (d) FIGURE A.5 A programmable logic gate. 4-way multiplexer (d) with the necessary settings for making it work as an inverter (a), a 2-input NAND gate (b), and as an XOR gate (c). A.2.11 MULTI-OUTPUT FUNCTIONS All examples presented so far were single-output functions. We speak of a multi-output function when a vector of several bits is produced rather than just a scalar signal of cardinality one. Example The full adder is a multi-output function of fundamental importance. It adds two binary digits and a carry input to obtain a sum bit along with a carry output. With x, y,and z denoting the three input bits, (A.13)and (A.14) together describe the logic functions for the carry out bit c and for the sum bit s respectively. A.2 THEORETICAL BACKGROUND OF COMBINATIONAL LOGIC 487 00 01 11 10 0 1 x yz c 1110 1000 00 01 11 10 0 1 x yz s 0 ≥ 101 001 1 AB CI S CO xy 2 z s cxy s (a) (b) (c) (d) zc s xy z c+ FIGURE A.6 Full adder. Icon (a), Karnaugh maps (b), and two circuit examples (c,d). \u0002 A.2.12 LOGIC MINIMIZATION Given the infinitely many solutions, we must ask ourselves “How to select an appropriate set of logic equations for some given combinational function?” Metrics for logic complexity and implementation costs The goal of logic minimization is to find the most economic circuit for a given logic function under some speed and energy constraints. The criterion for economy depends on the technology targeted. Minimum package count used to be a prime objective at a time when electronics engineers were assembling digital systems from SSI/MSI components. Today, it is the silicon area occupied by gates and wiring together that counts for full-custom ICs. The number of gate equivalents (GE) is more relevant in the context of field-programmable logic (FPL) and semi-custom ICs. From a mathematical point of view, the number of literals is typically considered as criterion for logic minimization. By literal we refer to an appearance of a logic variable or of its complement. As an example, the right-hand side of (A.4) consists of seven literals that make up three composite terms although just three variables are involved. An expression is said to contain a redundant literal if the literal can be eliminated from the expression without altering the truth table. Equation (A.19), the Karnaugh map of which is shown in fig.A.7a, contains several redundant literals. In contrast, none of the eleven literals can be eliminated from the right-hand side of (A.20) as illustrated by the Karnaugh map of fig.A.7b. The concept of redundancy not only applies to literals but also to composite terms. 488 APPENDIX A ELEMENTARY DIGITAL ELECTRONICS Redundant terms and literals result in redundant gates and gate inputs in the logic network. They are undesirable due to their impact on circuit size, load capacitances, performance, and energy dissipation. What’s more, redundant logic causes severe problems with testability, essentially because there is no way to tell whether a redundant gate or gate input is working or not by observing a circuit’s behavior from its connectors to the outside world. Minimal versus unredundant expressions Unredundant and minimal are not the same. This is illustrated by (A.21), an equivalent but more economical replacement for (A.20) which gets along with just eight literals. Its Karnaugh map is shown in fig.A.7c. Observation A.4. While a minimal expression is unredundant by definition, the converse is not necessarily true. e = x y z t ∨ x zt ∨ xy z t ∨ xz ∨ x yz (A.19) e = x y z ∨ x zt ∨ yzt ∨ xz (A.20) e = x y z ∨ xy t ∨ xz (A.21) 00 01 11 10 zt 00 01 11 10 (a) (b) (c) xy 00 01 11 10 00 01 11 10 xy zt 00 01 11 10 00 01 11 10 xy zt e 1 0 1 01 0 10 00 11 11 0 0 e 1 0 1 01 0 10 00 11 11 0 0 e 1 0 1 01 0 10 00 11 11 0 0 FIGURE A.7 Three Karnaugh maps for the same logic function. Redundant form as stated in logic equation (A.19)(a), unredundant form as in (A.20) (b), and minimal form as in (A.21)(c). Note that for obtaining the minimal expression (A.21) from the unredundant one (A.20), a detour via the canonical expression is required during which product terms are first expanded and then regrouped and simplified in a different manner. Thus, there is more to logic minimization than eliminating redundancy. Next consider function d tabulated in fig.A.8. There are two possible sum-of-products expressions shown in equations (A.22) and (A.23) both of which are minimal and use six literals. The minimal product-of-sums form of (A.24) also includes six literals. We conclude Observation A.5. A minimal expression is not always unique. d = x y ∨ xz ∨ y z (A.22) d = xy ∨ x z ∨ yz (A.23) d = (x ∨ y ∨ z)(x ∨ y ∨ z) (A.24) A.2 THEORETICAL BACKGROUND OF COMBINATIONAL LOGIC 489 00 01 11 10 yz 0 1 (a) (b) (c) x 00 01 11 10 yz 0 1 x 00 01 11 10 yz 0 1 x d 1101 11 d 1101 11 d 1101 11010101 FIGURE A.8 Karnaugh maps for equations (A.22) through (A.24), all of which implement the same function with the same number of literals. Multi-level versus two-level logic Observation A.6. While it is possible to rewrite any logic equation as a sum of products and as a product of sums, the number of literals required to do so grows exponentially with the number of input variables for certain logic functions. The 3-input xor function (A.14), for instance, includes 12 literals. Adding one more argument t asks for 8 minterms each of which takes 4 literals to specify thereby resulting in a total of 32 literals. In general, a n-input parity function takes 2(n−1) · n literals when written in two-level logic form. Asymptotic complexity is not the only concern, however. Multi-level circuits are often faster and more energy- efficient than their two-level counterparts. The process of converting a two-level into an equivalent multi-level logic equation is referred to as factoring, aka structuring, and the converse as flattening. x y z ∨ x yz ∨ x yz ∨ xy z factoring ⇌ flattening x y (z ∨ z) ∨ x (yz ∨ yz) (A.25) Multi-output versus single-output minimization Probably the most important finding on multi-output functions is Observation A.7. Minimizing a vectored function for each of its output variables separately does not, in general, lead to the most economical solution for the overall network. This is nicely illustrated by the example of fig.A.9. Solution (a) which is obtained from applying the Karnaugh method one output bit at a time requires a total of 15 literals (and 7 composite terms). By reusing conjunctive terms for two or more bits, solution (b) manages with only 9 literals (and 7 composite terms). In terms of gate equivalents, overall circuit complexity amounts to 12.5 and 9.5 GEs if all ors and ands get remapped to nand gates by way of bubble pushing. 490 APPENDIX A ELEMENTARY DIGITAL ELECTRONICS x y z t wvu x y z t u wv 00 01 11 10 00 01 11 10 xy zt v - 0 11 - 0 - 1 0- 0 -1 0 -- 00 01 11 10 00 01 11 10 xy zt v - 0 11 - 0 - 1 0- 0 -1 0 -- 00 01 11 10 00 01 11 10 xy zt w 0 1 0 -1 0 - 1 0 -10 -10 0 00 01 11 10 00 01 11 10 xy zt w 0 1 0 -1 0 - 1 0 -10 -10 0 00 01 11 10 zt 00 01 11 10 xy u 110 - - 0 - 1 0 -1 0 -0- 0 00 01 11 10 zt 00 01 11 10 xy u 110 - - 0 - 1 0 -1 0 -0- 0 joint minimization sharing product terms between output bits individual minimization per output bit translation translation (a) (b) (c) (d) FIGURE A.9 A multi-output function minimized in two different ways. Manual versus automated logic optimization Observation A.8. Manual logic optimization is not practical in VLSI design. For real-world multi-output multi-level networks, the solution space of this multi-objective optimization problem (area, delay, energy) is way too large to be explored by hand. Also, solutions are highly dependent on nasty details (external loads, wiring parasitics, cell characteristics, etc.) that are difficult to anticipate during logic design. Logic minimization on the basis of and and or gates with unit delays is a totally unacceptable oversimplification. A.3 CIRCUIT ALTERNATIVES FOR IMPLEMENTING COMBINATIONAL LOGIC 491 A.3 CIRCUIT ALTERNATIVES FOR IMPLEMENTING COMBINATIONAL LOGIC A.3.1 RANDOM LOGIC The term is misleading in that there is nothing undeterministic to it. Rather, random logic refers to networks built from logic gates the arrangement and wiring of which may appear arbitrary at first sight. Examples have been given earlier, see fig.A.9 for instance. Standard cells and gate arrays are typical vehicles for implementing random logic in VLSI. As opposed to random logic, tiled logic exhibits a regularity immediately visible from the layout because subcircuits get assembled from a small number of abutting layout tiles. As tiling combines logic, circuit, and layout design in an elegant and efficient way, we will present the most popular tiled buildings blocks. A.3.2 PROGRAMMABLE LOGIC ARRAY (PLA) A PLA starts from two-level logic and packs all operations into two adjacent rectangular areas referred to as and- and or-plane respectively. Each input variable traverses the entire and-plane both in its true and in its complemented form, see fig.A.10. A product term is formed by placing or by omitting transistors that act on a common perpendicular line. Each input variable participates in a product in one of three ways true 1 complemented 0 not at all - Parallel product lines bring the intermediate terms to the or-plane where they cross the output lines. The sums are then obtained very much like the products, the only difference being that products are not available in complemented form, so that any product enters a sum in either of two ways, namely true 1 not at all 0 The general arrangement as two pairs of interacting meshs yields a very compact layout. What’s more, a PLA is readily assembled from a small set of predefined layout tiles for any logic function. The criteria for logic minimization are not the same as for random logic. The PLA’s width being fixed by the number of input and output variables, PLA minimization software13 must act on the number of conjunctive terms to minimize layout height. 13 Such as the seminal espresso [227], which also introduced the above notation for capturing PLA codings. 492 APPENDIX A ELEMENTARY DIGITAL ELECTRONICS Example Fig.A.10 shows a PLA-style circuit that implements the logic function of fig.A.9c. For the sake of simplicity, switches and resistors have been substituted for the MOSFETs of an actual circuit.14 When comparing to the random logic of fig.A.9d, keep in mind that a PLA gets penalized on such a small function by the overhead associated with complementing all inputs, with distributing signals over both planes, and with restoring voltages to proper levels. 0 OR-plane 1 tile tiletile tile tile tile uv w programmable interaction product line PLA AND-plane tile tile txzy tiletile tile tile input lines 0 - 1 FIGURE A.10 The circuit organization of a NAND-NAND-type PLA drawn with switches and resistors instead of transistors. The function implemented is the same as in ﬁg.A.9. Using the notation introduced before, the PLA’s configuration and programming are expressed in a concise manner as follows. and-plane or-plane xy z t u v w 11 1- 10 1 01 -1 01 1 00 0- 11 1 \u0002 Owing to their superior layout densities, PLAs used to be popular building blocks for combinational functions with many inputs and outputs. They have lost momentum when automatic synthesis of random logic and multiple metal layers became available, but the underlying concepts continue to play an important role in field-programmable logic (FPL). 14 In fig.A.10, the switches in both planes work against static pull-up loads which circuit style is a departure from truly complementary CMOS style. Transistor networks trimmed down in this or a similar fashion are typical for PLAs. A.3 CIRCUIT ALTERNATIVES FOR IMPLEMENTING COMBINATIONAL LOGIC 493 A.3.3 READ-ONLY MEMORY (ROM) While both and- and or-planes are configurable in a PLA, programming of a ROM is confined to the or-plane.15 The role of the fixed and-plane is a assumed by a built-in address decoder that computes all possible minterms from the input variables, see fig.A.11. A ROM is an attractive option when full decoding of inputs is indeed required, but otherwise remains of limited appeal because its size doubles for each extra input or address bit. programmable OR-plane data D2 D1 D0 ROMaddress decoder1 out of 8addressA 0 A 2 1A word line bit line BL WL fixed AND-plane output buffers programmable interaction FIGURE A.11 General ROM arrangement (8 words by 3 bits, grossly simpliﬁed). A.3.4 ARRAY MULTIPLIER Two-level logic is extremely uneconomic for addition, multiplication, and other functions where the SoP includes many minterms that cannot be merged. Early IC designers have thus extended the tiling approach to multi-level logic and specifically to various forms of multipliers. Circuit organization is patterned after the classic procedure thought at elementary school whereby multiplier and multiplicand are placed on two orthogonal sets of parallel lines, with a 1-digit by 1-digit multiplication being carried out at every intersection. Addition is distributed over the grid by 15 Make sure you understand that, in spite of its name, a ROM is a purely combinational function or — which is the same — a memoryless subcircuit unable to hold a state. Further note that the term programmable array logic (PAL) denotes a third breed of tiled two-level logic where the and-plane is programmable and the or-plane is predefined. 494 APPENDIX A ELEMENTARY DIGITAL ELECTRONICS including an adder at every intersection and by having each multiply-add operation propagate its sum and carry to two out of the four adjacent tiles for further processing: the sum towards the bottom and the carry towards the left. The entire multiplier thus consists of largely identical tiles arranged as a two- dimensional array. Communication within the array remains strictly local which minimizes parasitic capacitances and interconnect delays. This is where the commonalities between the various types of array multiplier end. Popular examples include the Braun multiplier for unsigned and the Bough-Wooley multiplier for signed numbers. Booth-recoded multipliers have also been constructed along this line. While tiled multipliers have fallen behind much as PLAs have, their basic circuit organization lives on in random logic implemen- tations. A.3.5 DIGEST Fig.A.12 summarizes the circuit options for implementing a combinational function. Tiled logic does not waste nearly as much resources for wiring as cell-based random logic does because signals are brought from one layout tile to the next directly. Another benefit of ROMs and PLAs is that any reprogramming is limited to minor modifications to one metal mask or two (as long as the array’s overall capacity is not exceeded) whereas any modification to random logic necessitates redoing multiple mask levels and is bound to affect the subcircuit’s footprint. Conversely, the area overhead associated with auxiliary circuits (such as input and output circuitry, decoders, and pull-ups) makes ROMs and PLAs uneconomical for small subfunctions. Developing a layout generator also represents an important investment, part of which must be renewed with each process generation. Only ROMs continues to be routinely supported as combinational macrocells today (together with RAMs for memory functions). One reason is that the availability of multiple metal layers has narrowed down the difference between cell-based and tiled logic in terms of layout density. What really leveraged random logic, however, is that automatic HDL synthesis and multi-level logic optimization have during the 1990s matured into powerful software tools that not only cover combinational but also sequential circuits. Assembling lookup tables, arithmetic logic units, and the like from abutting layout tiles is no longer an option unless the word widths involved are extremely large and unless maximum density, performance, and/or energy efficiency must be sought with limited wiring resources. A.3 CIRCUIT ALTERNATIVES FOR IMPLEMENTING COMBINATIONAL LOGIC 495 single-output function multi-output function two-level logic multilevel logic AND-plane OR-plane both planes programmable programmable programmable PAL PLA ROM array-style computational unit e.g. parallel multiplier random logic of no prac- tical impor- tance cell-based tiled layout design design FIGURE A.12 Design space for implementing combinational functions. 496 APPENDIX A ELEMENTARY DIGITAL ELECTRONICS A.4 BISTABLES AND OTHER MEMORY CIRCUITS Bistable subcircuits are essential building blocks of sequential circuits. What they all have in common is the ability to store one bit of information by assuming either of two stable states. An almost infinite variety of circuit implementations has been developed over the years using various design styles and fabrication technologies. As a consequence, designations for bistables proliferate which continues to generate confusion today. Yet, this is totally unnecessary. Observation A.9. From the perspectives of architecture and logic design, it suffices to consider a bistable’s behavior and to ignore everything about its internal structure and operation. We are going to present a simple taxonomy that rests on behavioral criteria exclusively before deriving a coherent and unambiguous naming convention from that. Clocked storage elements clearly distinguish between input terminals that determine what state transitions shall take place, and others which determine when such transitions must occur. Any terminal that belongs to the latter category is referred to as a clock input.16 Unclocked storage elements, in contrast, do not evidence such a separation. Table A.6 Taxonomy of bistables as a function of their behavior. Bi stabl e clocked unclocked Behavior edge-triggered level-sensitive Data inputs at every active while clock is at any time get evaluated clock edge at active level Name ﬂip-ﬂop latch no single name Examples D-ﬂip-ﬂop D-latch SR-seesaw E-ﬂip-ﬂop Muller-C T-ﬂip-ﬂop MUTEX JK-ﬂip-ﬂop snapper Clock terminal identiﬁed by ∧ identiﬁed by none Clocked bistables must be subdivided further into edge-triggered and level-sensitive ones. In any edge- triggered bistable, it is a transition of the clock signal that causes the data present at the input terminal to be admitted into the circuit and to be stored there. The behavior of level-sensitive memory circuits is slightly more complex in that such circuits may be in pass or in hold mode depending on the logic value of the clock. In pass mode,data are simply propagated from the input to the output which is why this mode is also termed transparent mode. 16 Most clocked bistables are driven from a single clock. Although not really popular with circuit designers, some bistables require a dual-rail clock of two signals CLK and CLK driven by complementary waveforms. We are not concerned with this subordinate detail here. A.4 BISTABLES AND OTHER MEMORY CIRCUITS 497 In hold mode, output data are kept frozen and input data are being ignored which explains why the device is sometimes said to be opaque. Throughout this text, we will consistently refer to a bistable that is both clocked and edge-triggered as a flip-flop. Conversely, we reserve the word latch for any bistable that is clocked and level-sensitive. The phrase clocked bistable is used as a generic term for both.17 Table A.6 puts the names and behaviors of all popular bistables into perspective. A.4.1 FLIP-FLOPS OR EDGE-TRIGGERED BISTABLES The data or D-type ﬂip-ﬂop The D-type flip-flop exhibits the simplest behavior an edge-triggered bistable can have. Most engineers find it easiest to think in terms of D-flip-flops and to convert their designs into other forms, if necessary. The basic D-flip-flop has a clock terminal CLK and a data input D. The output datum is either available in true form Q, or in complemented form Q, or both. Similarly, the clock can induce a state transition either on its rising or on its falling edge, referred to as active edge.18 Please see fig.A.13 for the truth table of a basic rising-edge-triggered D-type flip-flop. Icon and signal waveforms are also shown along with the causality relation. positive edge triggered CLK DQ CLK D Q (a) (b) CLK D 0 1 00 1 101 ↓ ↑ ↑ - - - Q Q Q Q Q Q Q Q keep state unchanged idem idem adopt D as new state idem FIGURE A.13 Rising-edge-triggered D-ﬂip-ﬂop. Truth table (left), icon (a), and waveforms (b). A vast collection of more elaborate flip-flop variations are obtained from the basic D-type by extending its functionality in numerous directions and by combining these new features. Initialization facilities An extra input found on most flip-flops makes it possible to put the circuit into some predefined start state. Such initialization mechanisms come in two flavors. 17 Be warned that many sources use either word indiscriminately for any kind of bistable. The term latch, in particular, is often meant to include some forms of unclocked bistables, see footnote 24. Also, the fact that latches have a pass mode is sometimes emphasized by calling them transparent latches although this property is shared by all latches. For the sake of clarity and simplicity, we refrain from such practices. 18 Flip-flops that trigger on either edge exist, but are not as widely used. 498 APPENDIX A ELEMENTARY DIGITAL ELECTRONICS Both synchronous clear CLR and synchronous load LOD inputs affect flip-flop operation solely on the active clock edge. As the names say, the former imposes logic state 0 while the latter brings the flip-flop into state 1. Either one can be considered as just another data input that masks the regular data input D, see table A.7. Any basic D-type flip-flop is easily upgraded to include a synchronous clear or load by adding one or two gates in front of it. The asynchronous reset RST and the asynchronous set SET, in contrast, have an immediate effect on the flip-flop’s operation because they directly act on the state-preserving memory loop with no intervention from the clock, see table A.7.19 This also explains why the asynchronous (re)set mechanism must be incorporated into the elementary flip-flop circuit itself, there is no way to add it later. We therefore conclude: Observation A.10. A D-type flip-flop with an asynchronous reset input forms a fundamental building block from which any more sophisticated flip-flop, counter slice, or other edge-triggered bistable can be assembled with the aid of a few extra logic gates. Table A.7 Truth tables of rising-edge-triggered D-type flip-flops with synchronous clear (left) and with active-low asynchronous reset (right). CLK CLR D Q Q RST CL K D Q Q Q Q keep state unchanged 01 enter state 0 Q Q idem Q Q keep state unchanged ↓ Q Q idem Q idem ↑ enter state 0 1 ↓ - Q Q Q idem ↑ adopt D as new state 1 ↑ 0 adopt D as new state ↑ idem 1 ↑ 1 0 -- 0 -- 1 -- 10 - -- 11 - 1 - 01 00 0 1 0 1 01 1 0 1 0 idem In case a bistable is equipped with two conflicting initialization inputs, i.e. with CLR and LOD or with RST and SET, it must be specified which of the two takes precedence over the other.20 Although flip- flops with both asynchronous reset and set inputs exist, there is no meaningful application for them in synchronous designs. Scan facility An effective and popular way to ensure the testability of sequential logic is to replace all ordinary D-type flip-flops with special scan flip-flops and to connect them such as to make them cooperate like a shift-register while in scan test mode. A scan flip-flop essentially includes a select function at the input. Depending on the logic value present at the scan mode control terminal SCM, the data admitted into the flip-flop is either taken from the data input D (during normal operation) or from the scan input SCI (during scan test), see table A.8.21 19 Incidentally, note that asynchronous (re)set signals often are of active-low polarity as this used to offer better protection against noise and other fugitive events when combined with unsymmetric TTL levels. 20 Simultaneous activation of asynchronous set and reset inputs of equal precedence levels is disallowed as this could lead to irregular behavior — similarly to that observed when an seesaw is forced into the forbidden state — and/or to anomalous static power dissipation. 21 Please refer to fig.7.6 in the main text for a schematic and the very basics of scan path testing. A.4 BISTABLES AND OTHER MEMORY CIRCUITS 499 Enable/disable facility Not all flip-flops in a circuit need to be updated at every active clock edge, many of them must conserve their state for many consecutive clock cycles. This requires that flip-flops be equipped with a mechanism to enable or disable state transitions via a special control input ENA,see table A.9. A data flip-flop is readily extended to become a so-called enable or E-type flip-flop; it suffices to add a multiplexer in front of its data input that feeds the uncomplemented output back as long as the enable signal remains inactive, see fig.7.26 for an illustration. Table A.8 Truth table of a rising-edge-triggered scan flip-flop. CLK SCM SCI D Q Q Q Q keep state unchanged Q Q idem ↓ Q Q idem ↑ adopt D as new state, “normal operation mode” ↑ idem ↑ adopt SCI as new state, “scan mode” ↑ 0 -- - 1 -- - -- - 0 - 001 0- 1 1 0 10 - 01 11 - 10 idem Table A.9 Truth table of a rising-edge-triggered E-type flip-flop. Q Q Q Q keep state unchanged Q Q idem ↓ Q Q idem ↑ Q Q idem ↑ adopt D as new state ↑ CLK ENA D 0 -- 1 -- -- 0 - 10 0 1 11 1 0 idem The toggle or T-type ﬂip-ﬂop A toggle flip-flop is a bistable that changes state at every active clock edge, see table A.10. It is obtained from a D-flip-flop by providing an inverting feedback from the true output back to the data input. Similarly to the D-type, the T-flip-flop is easily upgraded to include an enable input, in which case toggling takes place only if the enable is active at the active clock edge. Table A.10 Truth table of a rising- edge-triggered T-type flip-flop. CLK Q Q 0 Q Q keep state unchanged 1 Q Q idem ↓ Q Q idem ↑ QQ change state, “toggle” 500 APPENDIX A ELEMENTARY DIGITAL ELECTRONICS The nostalgia or JK-type ﬂip-ﬂop In lieu of a single data input D, the JK-flip-flop has two inputs labeled J and K that, together, determine the state the bistable is going to enter at the next active clock edge, see table A.11. The JK-flip-flop is essentially a leftover from the days of SSI that is kept in today’s cell libraries mainly for reasons of compatibility. What made it popular is its versatility. Permanently tying J and K to logic 1 results in a T-type flip-flop. A toggle flip-flop with enable is obtained when J and K are connected to form a common input. D-type behavior asks for K to be the inverse of J which then serves as data input D. Table A.11 Truth table of a rising-edge- triggered JK-type flip-flop. CLK J K Q Q Q Q keep state unchanged Q Q idem ↓ -- Q Q idem ↑ Q Q idem ↑ adopt J = K as new state ↑ idem ↑ 0- - 1- - 00 01 01 10 10 11 QQ change state, “toggle” A.4.2 LATCHES OR LEVEL-SENSITIVE BISTABLES The data or D-type latch Very much like a basic flip-flop, a basic latch features just a data input D and a clock input CLK. Please note the latter terminal is often referred to as “enable” E or G in datasheets and icons. However, as this input carries the only signal that defines when the latch is to leave its present state and when it is to enter a new one, it clearly must be understood and handled as a clock. passes while high holds while low CLK DQ CLK D Q pass hold CLK Qto DQto DQto(a) (b) CLK D 0 11 11 1 0 - Q Q 00 Q Q pass hold output D idem FIGURE A.14 D-latch transparent on logic 1. Truth table (left), Icon (a), and waveforms (b). D-type latches not only find applications as subcircuits of flip-flops but also as bistable memory devices of their own when used in conjunction with a level-sensitive clocking scheme.22 22 See sections 7.2.4 and 7.2.5 which corroborate the notion of terminal CLK as a clock rather than as an enable input. A.4 BISTABLES AND OTHER MEMORY CIRCUITS 501 A.4.3 UNCLOCKED BISTABLES Unclocked bistables differ from their clocked counterparts in that there is no clock terminal that might trigger a state transition without, at the same time, also contributing towards defining the next state. Put in simple words, there is no distinction between when and what inputs. Observation A.11. Any information stored in an unclocked bistable is vulnerable to spurious events such as glitches, runt pulses, and other noise on the inputs. This is because the absence of a dedicated clock input implies that a transition at some data or control terminal may spark off a state change at any time. The situation sharply contrasts with clocked bistables where there exists no input other than the clock and an optional asynchronous (re)set that can possibly cause the state to flip. The SR-seesaw Any two inverting gates interconnected such as to form a loop exhibit bistable behavior because there is positive feedback and zero latency. Let us begin by analyzing two cross-coupled nor gates. The truth table of the circuit shown in fig.A.15 exposes two alarming peculiarities. Firstly, the input vector S = R = 1 causes the two output terminals to assume identical values thereby violating the rule that outputs Q and Q must always assume complementary values. The situation is often referred to as the forbidden state. Secondly, the outcome is unpredictable when the circuit is switched from the forbidden state to the data storage condition. In the occurrence, when inputs S and R simultaneously change from 1 back to 0, nodes Q and Q will eventually reassume complementary values, but there is no way to tell whether they will settle to 01 or to 10 after abandoning the forbidden 00 state.23 R S Q Q 1 0 0 Q Q 0 1 0 Q Q 1 0 1 0 R 0 1 1 0 S maintain output, data storage condition enter state 0, “reset” enter state 1, “set” noncomplementary output, “forbidden state” FIGURE A.15 Seesaw built from NOR gates. Truth table (left), circuit example (right). 23 This is because the two stable states are separated by a thin line of metastable equilibrium. Any bistable that is brought close to that line must revert to one stable state or the other before normal operation resumes. However, as explained in section 8.3.4, the course is undeterministic and the time it takes is unbounded. 502 APPENDIX A ELEMENTARY DIGITAL ELECTRONICS An alternative circuit is obtained from nand gates, fig.A.16, yet analysis yields analogous results. As both circuits exhibit bistable behavior but operate with no intervention from a clock, we refer to them as level-sensitive SR-type seesaws or simply as seesaws.24 Q S R Q 1 0 Q Q 0 1 0 11 Q 1 0 1 0 S 0 1 1 0 R enter state 0, “reset” enter state 1, “set” maintain output, data storage condition noncomplementary output, “forbidden state” FIGURE A.16 Seesaw built from NAND gates. Truth table (left), circuit example (right). Although seesaws are at the heart of many latch and flip-flop circuits, naked zero-latency feedback loops are unsuitable for data storage in synchronous designs because of various shortcomings they suffer from.25 Safe and useful applications of seesaws are very limited and include the generation of non-overlapping clock signals and the debouncing of mechanical contacts. The edge-triggered SR-seesaw As opposed to the seesaws of figs.A.15 and A.16, the set and reset inputs are edge-triggered rather than level-sensitive. As a consequence, there is no such thing as a forbidden state with noncomplementary output values. Still, the outcome remains unpredictable when both inputs transit from 0 to 1 in overly rapid succession. Also, with two flip-flops, the circuit of fig.A.17b is more costly in terms of transistor count than any other bistable discussed in this section. 24 SR is just an acronym for set-reset. We have coined the name seesaw to avoid the confusion that arises when the more popular terms (asynchronous) SR-flip-flop, SR-latch, S R-latch, and nor|nand-latch are being used. As shown in table A.6, we make it a habit to distinguish between latches, flip-flops, and unclocked bistables. 25 As explained in section 6.4, synchronous circuits boast a consistent separation into signals that trigger state changes and others that determine the sequence of states. Unclocked bistables make no such distinction which makes them vulnerable to hazards and may render their behavior unpredictable. A.4 BISTABLES AND OTHER MEMORY CIRCUITS 503 SQ RQ S− S+ Q+ R+Q− R− DQ Q CLK RST S (a) (c) (b) DQ Q RST CLK R P QQ Q QQ QQ QQ QQ Q 1 0 1 0 R 0 1 1 01 10 0 S ↑ ↑ − − keep state unchanged idem idem idem enter state 0, “reset” enter state 1, “set” FIGURE A.17 Edge-triggered seesaw. Truth table (left), icon (a), circuit example (b), and signal transition graph (STG) (c).26 The Muller-C element The Muller-C element is a bistable with one output and two interchangeable inputs. The output immediately assumes whatever value the two inputs agree on, but preserves its past value when the two input values differ. The behavior can be likened to hysteresis or to a majority seesaw. The idea is easily generalized to more than two inputs. One may also observe that the Muller-C behaves like an and-gate while the output is low, and like an or-gate while the output is high. Some circuit implementations combine logic gates into a zero-latency feedback loop, the most elegant solution being with a 3-input majority gate as shown in fig.A.18c. Other circuits use a memory element of four transistors reminiscent of a snapper, see fig.A.18d.27 26 Signal transition graphs (STG) are extremely helpful for describing the behavior of asynchronous circuits and controllers. Each node stands for an event such as a signal transition. A rising edge is identified by an appended + and a falling edge by a -. The updating of data and the withdrawal of data — returning to a high impedance condition, for instance — are also considered to be events. Each solid edge captures a cause/effect relationship implemented within the subcircuit being modeled while a dashed edge indicates the waiting for a condition to be satisfied by the surrounding circuitry. The position of all marks reflects the present state of the circuit. STGs belong to a subclass of Petri nets known as “marked graphs” and obey the same rules: For an event to take place, each incoming edge must carry a mark. When the transition actually fires, those marks get absorbed and a new mark is placed on every outgoing edge. The number of marks does, therefore, not necessarily remain the same. 27 [228] compares four alternative circuits and concludes that the majority gate implementation is superior to the weak feedback approach in terms of delay and, above all, energy efficiency. 504 APPENDIX A ELEMENTARY DIGITAL ELECTRONICS Table A.12 Truth table of Muller-C element. standard form with B inverted C C 0 enter state 0, “reset” C maintain output C maintain output 0 enter state 0, “reset” C maintain output 1 enter state 1, “set” 1 enter state 1, “set” AB A B 00 0 0 01 0 1 10 1 0 11 1 1 C maintain output C A B CC C B (a) (c) (d) (e) (b) A A+ A− B+ C+ C− B− 2 A B C weak transistor weak transistor A B C ≥ FIGURE A.18 Muller-C element. Icon (a), alternative circuit examples (c,d), and signal transition graph (STG) (e). Icon for a Muller-C with one inverting input (b). Similarly to seesaws, the Muller-C must not be used as part of synchronous designs because of the absence of a clock input. It finds useful applications for the processing of handshake signals in self- timed systems, however [229] [158]. The mutual exclusion element Also known as interlock element, the mutual exclusion element — or MUTEX for short — features two symmetric inputs R1 and R2 that are associated with outputs G1 and G2 respectively. The two outputs are never active at the same time, see table A.13. At rest, both inputs are inactive R1 = R2 = 0 and so are the outputs. When a positive impulse arrives on either input, it gets immediately propagated to the pertaining output. Should a second impulse arrive on the other input later, it will not get passed on until the first impulse has come to an end. Impulses are thus propagated on a “first come, first serve” basis unless two of them arrive simultaneously, in which case the circuit arbitrarily selects one to pass through and withholds the other.28 28 Note the analogy with two persons simultaneously arriving at the door of a closet or phone booth. A.4 BISTABLES AND OTHER MEMORY CIRCUITS 505 A look back tells us that the seesaw already had the capability of discriminating between two events on the basis of their order of arrival. A comparison of the MUTEX truth table with those of figs.A.15 and A.16 reveals that they are in fact the same (except for swapped and inverted output or input terminals respectively). Not surprisingly, one finds a level-sensitive seesaw in the mutual exclusion circuit, see fig.A.19b for an example. What, then, are the four extra transistors good for? Note, to begin with, that the seesaw waits in the forbidden state while R1 = R2 = 0. It then enters either the set or the reset state depending on which input switches from 0 to 1 first. If both inputs go high at the same time, or nearly so, the seesaw is subject to marginal triggering. The circuit then lingers in a state of metastable equilibrium before eventually returning to a stable state and letting one input through.29 From a practical point of view, it is important to make sure that the output signals G1 and G2 remain logically unambiguous, free of glitches, and consistent with the truth table in spite of the seesaw hovering in an irregular condition. The four transistors must, therefore, be understood to form some kind of filter that defers the circuit’s response until the seesaw has recovered. An observation made in [166], namely that the closer in time a pair of rising transitions arrives on the inputs, the longer the MUTEX takes to decide which of them to propagate, comes as no surprise from this perspective. The mutual exclusion element plays a key role in a subcircuit known as arbiter and is instrumental in self-timed circuits. Table A.13 Truth table of the mutual exclusion element. R1 R2 G1 G2 wait let R2 pass let R1 pass 0000 0101 1010 11 G1 G2 let the earlier impulse pass, resolve conﬂict in case of simultaneous arrival R1 G1 R2 G2 (a) (b) MUTEX G1 G2 R1 R2 FIGURE A.19 Mutual exclusion element. Icon (a) and circuit example (b). 29 See footnote 23. 506 APPENDIX A ELEMENTARY DIGITAL ELECTRONICS A.4.4 RANDOM ACCESS MEMORIES (RAM) As everyone knows, random access memories serve as short-term repositories for large quantities of data. A RAM essentially consists of a large array of elementary binary storage cells that share a common input/output or data port D,see fig.A.20. Any access has to occur one data word at a time and the address A serves to identify the data word currently being accessed. As an example, a 1 Mi x 4 bit RAM accepts and returns data as 4-bit quantities and requires a 20 bit address to select one out of the 220 = 1 048 576 memory locations available.30 The overall organization bears many common traits with ROMs in that both the bit cell array and the address decoder are assembled from a few layout tiles. What sets a RAM apart from a ROM are bistable storage cells the state of which can be changed from the data port in very little time. A write enable input read/write ckts and I/O buffers storage array data D2 D10D RAM WR/RD word line bit line BL WLaddress decoder1 out of 8 fixed AND-planeaddressA 0 A 2 1A one-bit storage cell FIGURE A.20 General RAM arrangement (8 words by 3 bits, grossly simpliﬁed). 30 Kibi- (ki), mebi- (Mi), gibi- (Gi) and tebi- (Ti) are binary prefixes recommended by various standard bodies for 210,220, 230 and 240 respectively because the more common decimal SI prefixes kilo- (k), mega- (M), giga- (G) and tera- (T) give rise to ambiguity as 210 ⊕= 103. As an example, 1 Mibyte = 8 Mibit = 8 · 220 bit. A.4 BISTABLES AND OTHER MEMORY CIRCUITS 507 WR/RD that controls the operation of the bidirectional input/output buffers for write and read operation is another important departure. Two techniques for implementing two-valued memory cells prevail today. In a static RAM (SRAM), each bit of data is being stored with the aid of two cross-coupled inverters that form a positive feedback loop. The two stable points of equilibrium so obtained are identified with logic 0 and 1 respectively. As opposed to this, it is the presence or absence of an electrical charge on a small capacitor that reflects the binary information in a dynamic RAM (DRAM). The elementary bit cell is utterly simple and small thereby maximizing the memory capacity available from some given piece of silicon. The ensuing low costs per storage bit are the main reason why DRAMs dominate the mass market for computer main memory in spite of their longer access times. In either case, data storage is volatile exactly as for latches and flip-flops which is to say that the information is lost upon disruption of the supply voltage. To save on the overall pin count, the address is typically being time-multiplexed over a single address port in commodity DRAM components, e.g. 20 bit as a pair of two 10 bit chunks. Some memories handle write and read transfers over separate input and output ports. Also available are dual-port RAMs that feature two independent I/O ports and that allow for two concurrent read/write transfers. 508 APPENDIX A ELEMENTARY DIGITAL ELECTRONICS A.5 TRANSIENT BEHAVIOR OF LOGIC CIRCUITS Our discussion of logic circuits has, so far, been concerned with steady-state conditions and with the end points of switching processes exclusively. As a consequence from delays and inertial effects, circuits assembled from transistors, wires and other real-world components exhibit various transient phenomena, though. A key question is “How do the outputs of digital (sub)circuits evolve from one value to the next?” Let us first be concerned with transient waveforms as witnessed on an oscilloscope hooked to the output of a combinational circuit before turning our attention to the underlying mechanisms that cause them. A.5.1 GLITCHES, A PHENOMENOLOGICAL PERSPECTIVE One might naively expect that binary signals progress from one value to the next in monotonic ramps. Experiments with real circuits reveal that this is not always the case, though. Any kind of fugitive or non-monotonic event on a binary signal is termed a glitch or a hazard.31 Transients are sometimes catalogued from a purely phenomenological point of view, i.e. as a function of the signal waveform observed. steady state before transition steady state after transition transition interval runt pulses fast edge slow ramp any of these waveforms common notation unknown validdata valid gets abstracted into in 0 in 1 on rise on fall switching threshold switching threshold monotonic transitions, no glitching glitching due to static hazards glitching due to dynamic hazards X0 1or01or FIGURE A.21 Transients classiﬁed according to their waveforms. A.5 TRANSIENT BEHAVIOR OF LOGIC CIRCUITS 509 A static hazard manifests itself as a temporary deviation from its steady-state logic value that occurs in response to some other signal change. If a signal moves back and forth before eventually settling to a logic value opposite to the initial one, we speak of a dynamic hazard. As illustrated in fig.A.21, both static and dynamic hazards may be classified in further detail. The voltage excursion during a glitch does not necessarily make a full swing, stunted pulses are being observed as well. They occur when a signal has begun to change (order) just before an antagonistic effect (counterorder) sets in, thereby preventing the first transition from completing. Such spurious events, termed runt pulses, may render circuit operation unpredictable and irreproducible if they reverse direction in the vicinity of the logic family’s switching threshold or if they cut across it for a very short lapse of time before turning back. A.5.2 FUNCTION HAZARDS, A CIRCUIT-INDEPENDENT MECHANISM This first mechanism can be understood from the logic function alone and does not depend on any specific circuit implementation. Consider the Karnaugh map in table A.14, for instance. When the input vector xyz changes from 001 to 011, then the output h switches from 1 to 0. Conversely, the output stays 1 when the input goes from 001 to 000. Table A.14 A Karnaugh map that may give rise to function hazards. yz h 00 01 11 10 x 0 1 1 0 0 1 0 1 1 0 What happens when two or more inputs change at a time? As before, the final output value is found in the appropriate field. The intermediate steps leading to that result will, however, depend on the actual sequence of events at the input. Assume input vector xyz is to change from 001 to 111. Depending on whether variable x or y is switching first, the intermediate input is 101 or 011. While in the first case output h remains at 1 throughout, it temporarily assumes value 0 in the latter case and so gives rise to a static hazard on 1. Similarly, three inputs that change shortly one after the other may give rise to a dynamic hazard, e.g. when input xyz goes from 001 to 011 followed by 111 before settling on 110. This type of transient output in response to two or more input transitions is termed function hazard since it depends on the logic function exclusively. Whether the resulting hazards are of static or of dynamic nature is immaterial in this context. Even the most humble binary functions, such as and and or, exhibit function hazards for an appropriate sequence of input transitions. n-cubes are most convenient for tracing function hazards, see fig.A.22. 510 APPENDIX A ELEMENTARY DIGITAL ELECTRONICS 1(000) 1(001) 0(011) 0(010) 0(100) 1(101) 1(111) 0(110) x y z g(x,y,z) FIGURE A.22 3-cube equivalent to table A.14. One might argue that no intermediate steps and, therefore, also no function hazards would arise if the switching of the inputs were to occur simultaneously and instantaneously. However, due to dissimilar propagation delays along interconnect lines and within the logic circuitry itself, the result is very much the same as if inputs had switched with a timewise offset. In practice, the waveforms that result from function hazards range from barely noticeable runt pulses to multiple full blown glitches, depending on how much the input signals are skewed and depending on the logic and interconnect delays involved. Please note that the existence of a hazard can also pass totally unnoticed from the output waveform, a situation which is sometimes referred to as near hazard.31 A.5.3 LOGIC HAZARDS, A CIRCUIT-DEPENDENT MECHANISM The second mechanism differs from the first in that the switching of a single input variable suffices to generate unwanted transients. Also, the emergence of transients cannot be explained from the function alone but is related to a particular gate-level circuit structure. As an example, consider fig.A.23, a circuit implementing the Karnaugh map of table A.14. 31 Strictly speaking, the term glitch refers to visible waveforms while hazard designates the underlying mechanism that may or may not cause some combinational circuit to develop non-monotonic transients. This distinction is not always maintained, though, as the term hazard is often meant to include the observable effect too. Incidentally, note that a glitch can have causes other than hazards such as ground bounce, crosstalk, or electrostatic discharge (ESD). There is an analogy with medicine in that the clinical picture with its observable symptoms (phenotype) is what matters from a therapeutic and personal point of view. A more profound analysis, in contrast, is concerned with why some living creatures have a higher predisposition for being struck by certain diseases than others do (genotype). Whether the illness actually manifests itself in a given individual or not is of little interest from this epidemiologic perspective. A.5 TRANSIENT BEHAVIOR OF LOGIC CIRCUITS 511 s h t x y z reconvergencefanout FIGURE A.23 A simple combinational network exposed to logic hazards. Let’s assume that input vector xyz changes from 101 to 001. The fact that the combinational function maps either input to logic 1 may lead us to believe that the output would steadily remain at that value. Yet, unequal propagation delays will cause the switching of inner nodes to be slightly skewed. In the occurrence, the additional inverter in the upper path might be responsible for delaying the switching of s with respect to t. Vector st then changes from 10 via 11 to 01 and output h switches from 1 via 0 back to 1 which amounts to a static hazard. This behavior, which cannot be explained as a function hazard because initial and final node are adjacent in the n-cube, is referred to as a logic hazard. Note, by the way, that the inverse input change, i.e. xyz from 001 to 101, produces no glitch since vector st goes from 01 via 00 to 10, three values all of which result in a 1 at the output. This is not a general characteristic of logic hazards, however. From a more detailed point of view, logic hazards can be traced back to a combination of reconvergent fanout and a function hazard. Let us view the logic circuit of fig.A.23 as being composed of two subcircuits. For certain inputs yz, the subcircuit to the left will broadcast a change of variable x to both of its output nodes s and t. The subcircuit to the right where these two signals recombine is then confronted with multiple changing inputs, and output h may or may not develop a glitch depending on the circuit’s detailed timing characteristics. j s t r reconvergencefanout x y z FIGURE A.24 Same circuit as in ﬁg.A.23 with logic hazards suppressed. 512 APPENDIX A ELEMENTARY DIGITAL ELECTRONICS Reconvergent fanout is a necessary but not a sufficient condition for logic hazards. This is documented by the circuit of fig.A.24, a modification of fig.A.23 obtained from adding two gates. When xyz changes from 101 to 001 the new node r stays 0, thereby preventing output j from moving away from its steady- state value. Both the original and the modified circuits contain reconvergent fanout, but only the original one is glitching in response to a single input change. Exact conditions for the existence of logic hazards are being given in [221]. As a rule, multi-level networks tend to glitch more intensively than two-level networks because multiple paths of different lengths are more likely to coexist. Similarly to what was found for function hazards, the waveforms caused by logic hazards depend on timing and other circuit details. The suppression of hazards in fig.A.24 has been bought at the expense of introducing redundant hardware. In fact, the added gates — represented by two extra literals in the logic equation — do not affect the logic function of the network and are, therefore, redundant. As redundant logic is almost impossible to test and entails superfluous switching activity, this approach is not recommended in the context of VLSI design. A.5.4 DIGEST Our findings on transient phenomena in combinational logic are best summed up as follows. Observation A.12. Hazards may or may not bring forth glitches, extra signal events unwanted and unaccounted for on the logic and higher levels of abstraction. Whether a hazard actually materializes as a rail-to-rail pulse, as a runt pulse, or not at all depends on circuit structure, gate and interconnect delays, load conditions, wiring parasitics, layout arrangement, operating conditions (PTV), on-chip variations (OCV), and other implementation details of relatively minor importance. Observation A.13. A combinational circuit is susceptible to develop hazards ◦ if two or more input variables change at a time,32 or ◦ if the circuit includes reconvergent fanout and if one or more inputs change at a time. In conclusion, all logic networks able to carry out some form of computation may give rise to hazards and glitches. Needless to say this includes almost all digital circuits of technical interest.33 32 “At a time” here means before all transients in response to the earlier input change have died out. 33 How to design safe circuits in spite of hazards is a major topic of section 6.3. A.6 TIMING QUANTITIES 513 A.6 TIMING QUANTITIES We have now learned about transient events that are associated with the working of combinational logic. Next we want to abstract all relevant inertial effects of digital components and subcircuits to a small set of quantities. Essentially, we want to unambiguously state when a signal is valid, when it is subject to evolve, and when it is safe to update it without worrying about inner details of the circuits concerned. A.6.1 DELAY PARAMETERS SERVE FOR COMBINATIONAL AND SEQUENTIAL CIRCUITS steady state before transition steady state after transition transition interval input tpd c tcd c contamination delay propagation delay timing parameters common notation unknown validdata valid monotonic transitions, no glitching fast edge slow ramp temporal uncertainty output runt pulses glitching due to static hazards glitching due to dynamic hazards transition X0 1or01or FIGURE A.25 Transients revisited. Note from fig.A.25 that it takes a pair of delay parameters to adequately describe how long transient phenomena persist at the output of a digital circuit in response to a change at one of the circuit’s inputs. One of these two timing quantities is very popular, while the other is not. 514 APPENDIX A ELEMENTARY DIGITAL ELECTRONICS tpd Propagation delay.34 The time required to process new input from applying a stable logic value at a (data or clock) input terminal until the output has settled on its final value, i.e. until all transients in response to that input change have died out. tcd Contamination delay, aka (output) retain delay and retain time.35 The inertial time from altering the logic value at a (data or clock) input until the output value no longer remains the same, i.e. until transients begin in response to that input change. By definition, 0 ≤ tcd < tpd must hold for any physical component or (sub)circuit. Contamination delay figures are rarely published in datasheets, though.36 As a stopgap, safety-minded engineers often substitute the lower bound for the missing parameters by admitting tcd ≡ 0. In doing so, causal behavior is taken for granted but any inertial effects are ignored. While this is a viable workaround in many cases, contamination delay is an essential quality of any flip-flop.37 Observation A.14. Signal propagation along some path through a digital network is captured by a pair of timing parameters termed contamination and propagation delay respectively. Their numeric figures are chosen so as to enclose all transient phenomena (such as ramps, hazards, glitches, and runt pulses) that might occur at the output in response to some input change. What does this mean for a typical digital circuit that features multiple input and/or output terminals? If the propagation delay of such a circuit is to be characterized by a single quantity, it is obviously necessary to consider all of the terminals. Propagation delay of vectored inputs and outputs is, therefore, determined by whichever signal takes the uppermost amount of time to traverse a circuit (longest path). Conversely, it is the quickest travel time through the logic that determines the contamination delay (shortest path). In the occurrence of a (level-sensitive) latch, the point of time when output data become valid depends on the data input in some situations and on the clock input in others. Two propagation delays are thus required to characterize the timing of a latch. tpd ld indicates the data-to-output delay (from input D to output Q) while tpd lc states the clock-to-output delay (from CLK to Q). The same argument obviously also applies to the contamination delay. 34 Also referred to as “settling time” or “maximum delay” by some authors. 35 The terms “internal delay”, “output hold time”, and “minimum (propagation) delay” are synonyms found in the literature. We do not support their usage as they lend themselves to confusions, however. 36 There are various reasons for this. For one thing, the concept of contamination delay is virtually unknown to most practitioners and textbooks in digital electronics. For another thing, manufacturers are reluctant to commit themselves to minimum values for any delay parameter because they reserve the right to upgrade their fabrication processes at any time in search of better performance or lower manufacturing costs. Also, there is a common industry practice known as down- binning which implies shipping a faster device against an order for a slower part. Because of its superior speed, a faster device is bound to exhibit a shorter contamination delay than the original part. 37 Equating all contamination delays with zero is an inadmissible oversimplification that makes it impossible to gain a more profound understanding of how clocked circuits operate. The functioning of a simple shift register, for instance, cannot be explained under that assumption. Refer to section 7.2.2 for an in-depth analysis. A.6 TIMING QUANTITIES 515 tsu ff tpd ff tcd ff tho ff CLK (a) (b) (c) D Q CLK D Q pass hold tho latsu la tpd lc tcd lc tcd ld tpd ld CLK Qto DQto tclk ritclk fa tclk hi tclk lo tclk ritclk fa tclk hi tclk lo tpd c tcd c tpd c tcd c X F positive edge triggered Flip-Flop passes while high holds while low Latch Combinational Logic XF CLK DQ CLK DQ FIGURE A.26 Timing characteristics of combinational subcircuits and of clocked bistables. A.6.2 TIMING CONDITIONS GET IMPOSED BY SEQUENTIAL CIRCUITS ONLY The orderly functioning of any memoryzing (sub)circuit — flip-flop, latch, RAM, or larger sequential circuit — requires that it be driven by a clock of clear-cut waveform. Ambiguous voltages, glitches, sluggish ramps, and stunted waveforms are unacceptable. Clocked circuits further impose dead times during which the clock must remain stable before it may be allowed to toggle again. Two pairs of timing conditions have been defined to capture the characteristics of an acceptable clock. Note that all four parameters relate to no other signal than to the clock itself. 516 APPENDIX A ELEMENTARY DIGITAL ELECTRONICS tpu clk min Clock minimum pulse width. The time span during which the clock signal must firmly be kept either low or high before it is permitted to swing back to the opposite state. Shorter clock pulses must be avoided as a bistable is otherwise likely to behave in an unpredictable way. In practice, it is often necessary to distinguish between tlo clk min clock minimum low time (MPWL) which refers to logic 0 (pause), and thi clk min clock minimum high time (MPWH) which refers to logic 1 (mark). Both combine into tpu clk min = max(tlo clk min, thi clk min). tra clk max Clock maximum ramp time. The timewise allowance for the clock to ramp from one logic state to the opposite one. Driving a bistable with overly slow waveforms may cause inner nodes to float or to be placed under control of conflicting drivers which may lead to an irrecoverable loss of data. To prevent this from happening, a pair of maximum transition or ramp times is imposed on the clock.38 tri clk max clock maximum rise time refers to the transition from 0 to 1 while tfa clk max clock maximum fall time is concerned with the inverse transition. Both combine into tra clk max = min(tri clk max, tfa clk max). Any clocked (sub)circuit further imposes requirements on the timewise relationship between any of its data inputs and the clock that is driving that (sub)circuit. tsu Setup time. The lapse of time immediately before the active clock edge during which an input is required to assume a fixed logic value of either 0 or 1 at the input of a clocked (sub)circuit. The setup condition is here to make sure all inner nodes have settled to values determined by new input data before the (sub)circuit locks into the corresponding state in response to the subsequent active clock edge. Violating the setup requirement must be avoided under any circumstance because bistables are otherwise likely to behave in an unpredictable way. tho Hold time. The lapse of time immediately after the active clock edge during which data are required to remain logically unchanged at the input of a clocked (sub)circuit. The hold condition assures that all inner nodes have properly settled so that the new state is maintained even when the stimuli that caused the transitions in the first place are being removed. Violating the hold requirement must be avoided for the reasons explained before. Although either the setup or the hold time may assume a negative value for certain components or (sub)circuits,39 tsu + tho > 0 always holds. 38 There is another reason for doing so. Timing data of clocked subcircuits vary with the waveform of the driving clock. Yet, for reasons of economy, it is common industrial practice to characterize flip-flops, latches, RAMs, and the like for one typical clock ramp time, e.g. for tclk ra = tclk ri = tclk fa = 50 ps. Driving them with a clock waveform that exhibits much slower ramps will cause the actual parameter values of tpd, tcd, tsu,and tho to significantly deviate from the figures published in datasheets and simulation models. 39 A negative hold time means that a data input is not required to preserve its value until after the active clock edge for being properly stored in the (sub)circuit, but is free to resume new transient activities before that time. Similarly, a negative setup time means that the input is allowed to switch until after the active clock edge. A.6 TIMING QUANTITIES 517 Observation A.15. Setup time and hold time together demarcate a brief lapse of time in the immediate vicinity of the active clock edge. Their numerical values are chosen so as to guarantee that data get stored and/or processed as intended under all circumstances. Any ambiguous or changing input during the aperture so defined is likely to expose any bistable or other clocked (sub)circuit to marginal triggering and to cause it to fail in an unexpected and unpredictable way.40 To prevent this from happening, the logic value must be kept constant and logically well-defined throughout. In the occurrence of a circuit with multiple input bits, such as a register, every bit must remain stable either at 0 or 1 throughout. Bistables with extra control inputs and/or with multiple clocks impose additional timing conditions. More specifically, asynchronous set and reset signals must not be released in the immediate vicinity of an active clock edge otherwise exposing the bistable to marginal triggering. tsu rst Recovery time, aka release time. Indicates by how much time the deactivation of an asynchronous (re)set input must precede the active clock edge such as to allow a bistable to unlock properly before taking up normal operation. tho rst Home time is more suggestive than the popular synonym removal time. Indicates for how much time an asynchronous (re)set input must remain activated following the active clock edge in order to safely bring a bistable home into its reset state. If the asynchronous (re)set is released too early, then chances are that the bistable falls into some undetermined state or a metastable condition. Some datasheets explicitly specify recovery and home times while others refer to these quantities as particular cases of setup and hold times.41 A.6.3 SECONDARY TIMING QUANTITIES ARE DERIVED FROM PRIMARY ONES The timing quantities introduced so far suffice for characterizing digital circuits. Yet, it is sometimes convenient to define quantities that are derived from those primary parameters. tcw Data-call window, aka aperture time, setup-and-hold window, and sampling time. The overall time span during which data must maintain a constant and well-defined value at the input of a memorizing (sub)circuit tcw = tsu + tho > 0. tid Insertion delay. As the name suggests, this term denotes the extra delay that is inflicted on a signal when a given (sub)circuit is being inserted into a signal’s propagation path. In the occurrence of a combinational circuit, insertion delay is the same as the propagation delay on the longest path, that is tid c = max(tpd c) = tc. 40 You may want to consult section 8.4.1 for more information on what exactly happens with a bistable circuit in this case. How to determine the setup and hold time figures of a given bistable is also explained there. 41 The difference is that an asynchronous control signal must subsequently retain its passive value indefinitely unless the circuit is to be reinitialized to its start state whereas any ordinary (synchronous) control signal is essentially free to switch after the hold time has expired. Using separate terms thus seems justified, but we will not insist on this. 518 APPENDIX A ELEMENTARY DIGITAL ELECTRONICS For flip-flops, one has tid ff = tsu ff + tpd ff = tff (where tpd ff refers to the non-inverting output unless indicated otherwise) because this is the minimum lapse of time an edge-triggered bistable takes to store and propagate a data item provided the active clock edge is optimally timed. fto ff max Maximum toggling rate. No flip-flop can be made to operate faster than its insertion delay and its minimum clock pulse widths permit. The utmost clock frequency thus is fto ff max = 1 Tto ff min where Tto ff min = max(tid ff , thi clk min ff + tlo clk min ff ). Although this quantity is given much publicity in advertisements, it remains of little practical interest as it leaves no room for any data processing activity. rsl Slew rate. The average velocity of voltage change during a logic transition, that is rsl = \u0003u \u0003t . The quantity is positive for rising edges rsl ri = Uh−Ul tri and negative for falling ones rsl fa = Ul−Uh tfa . δ Duty cycle. The average proportion of time during which a signal is active or a load is energized. For an active-high signal of period T, one has δ = thi T = thi thi+tlo if the switching is so fast that ramp times can be ignored. 0 ≤ δ ≤ 1 holds by definition. A.6.4 TIMING CONSTRAINTS ADDRESS SYNTHESIS NEEDS Timing constraints differ from delay parameters and timing conditions in that they do not describe the timewise behavior of an existing component or design for the purposes of analysis and simulation, but serve to express target characteristics of a circuit-to-be for the purposes of design and synthesis. As illustrated in fig.4.21, most timing constraints specify an upper bound for some propagation delay. Asking for a minimum contamination delay also makes sense in certain situations. A.7 BASIC MICROPROCESSOR INPUT/OUTPUT TRANSFER PROTOCOLS 519 A.7 BASIC MICROPROCESSOR INPUT/OUTPUT TRANSFER PROTOCOLS While microcomputer architectures are beyond the scope of this text, we briefly review the three fun- damentally different I/O transfer protocols because many ASICs are to interface with a microprocessor bus system. A peripheral device that wishes to deliver data to a microcomputer has to ask for an input transfer operation. Conversely, an output transfer is solicited when the peripheral needs to obtain data. In either case, the peripheral sets a service request flag. Three conceptually different ways exist for notifying the microcomputer about such a condition and for handling the subsequent data transfer, see fig.A.27. service requests idlingI/O handling backgroundCPU activity: Polling I/O active wait data transfer by service routine Interrupt-driven I/O data transfer by interrupt service routine Direct memory access preparation final inspectiondata transfer by cycle stealing delayed response FIGURE A.27 The three basic data transfer protocols for microcomputer input/output. 1. Polling. In this scheme, the CPU actively waits for service requests to arrive. The peripheral is wired to some port on a peripheral interface adapter (PIA) so that the CPU can examine its status by way of read operations on the pertaining PIA address. Whenever the operating system expects a peripheral device to ask for I/O operations in the near future, it enters a program loop which makes it periodically read that port and check the appropriate bit position there to find out whether a service request is pending or not. If so, the program branches to a service routine which tells the CPU how many data words to transfer, where to get them from, how to process them, and where to deliver them to. The routine is also in charge of setting the service request flag back to its inactive state once the peripheral has been serviced. If not so, the CPU proceeds with executing the loop’s code which makes it repeat read-and- check operations until the expected service request finally arrives. Designing the polling loop always is a compromise, see fig.A.27. A tight loop ensures a fast response but leaves little room for doing anything useful while waiting, whereas an ampler loop 520 APPENDIX A ELEMENTARY DIGITAL ELECTRONICS provides room to carry out computations in the background, but results in prolonged response times as the request bit per force gets examined less frequently. 2. Interrupt-driven. As opposed to polling, the CPU is not locked in a loop but is free to execute code in a background process. In order to make this possible, the request flag is brought out to a line that connects to a special input of the CPU termed interrupt request. Activation of this line diverts program execution to an interrupt service routine once execution of the current instruction has completed. The routine first causes the CPU to properly suspend the current process by saving the contents of critical registers to memory or on a special stack. The remainder of the I/O operation occurs in much the same way as for polling. After having completed the interrupt service routine, the CPU resumes the suspended process. Processor instruction sets typically include a pair of special instructions that allow programmers to temporarily suspend the interrupt mechanism in order to bring critical code sequences to an end without having their execution delayed or broken up. Withholding interrupt requests in this way is referred to as interrupt masking. 3. Direct Memory Access (DMA). The CPU is freed from most of the burden associated with I/O transfers by delegating them to a special hardware unit termed DMA controller that is hooked to the service request line in lieu of the CPU. Before a series of I/O operations can begin, the CPU in a preparatory step instructs the DMA controller how many data words to transfer and indicates their destination and/or source addresses. The CPU itself is not involved in the subsequent transfer operations, which contrasts sharply with polling and interrupt-driven I/O. Instead, the DMA controller handles the actual data moves by stealing memory cycles from the CPU for its own memory accesses whenever notified by the peripheral that a data item is waiting to be accepted or delivered. At the end of the commissioned series of transfers, the DMA controller typically informs the CPU by way of an interrupt. As part of the pertaining service routine, the CPU does then inspect some status register in the DMA controller to find out whether the transfer has been successfully completed or prematurely aborted. Table A.15 The basic I/O transfer protocols compared. Input/Output Data Transfer Scheme Polling Interrupt-driven DMA Hardware overhead close to none, small, moderate, PIA port bit interrupt mechanism DMA controller CPU burden high as CPU has moderate, minimal, to idle in a loop once per data item once per data series Response time unpredictable, a few instruction almost immediate, depends on loop cycles unless masked a few clock cycles Transfer rate moderate fair high A.8 SUMMARY 521 As an improvement to polling, it is possible to combine most of its simplicity with the efficiency of interrupt-driven I/O by having a timer periodically trigger the interrupt line. In the interrupt service routine, the CPU then polls one peripheral device after the other and branches to a specific service routine for each that has a request pending. A.8 SUMMARY • Digital hardware deals with bit patterns rather than with numbers. These patterns assume a meaning as numbers only when interpreted according to the specific number representation scheme that the designers had in mind. • As zero-latency loops may give rise to unpredictable behavior, it is best to avoid them. • Be prepared to observe unexpected transient pulses at the output of any combinational logic unless you have proof to the contrary. • While all datasheets and textbooks mention setup time, hold time, and propagation delay, it is not possible to understand how a simple shift register works without introducing the concept of contamination delay. • Any bistable is either an - Edge-triggered flip-flop, a - Level-sensitive latch, or an - Unclocked bistable. It is most important to keep those apart technically and linguistically. • Before starting up any kind of electronic design automation tool, it is important to - Know what its optimization criteria and limitations are, - Find out whether it does indeed apply to the problem at hand, - Develop an understanding of the available options and controls such as to - Set all options and control knobs to suitable values. • Three basic protocols are available for organizing the transfer of data between a peripheral device and a microcomputer: - Periodically polling the peripheral for service requests in a program loop. - Having a special signal from the peripheral interrupt regular program execution. - By bypassing the CPU with the aid of an extra direct memory access controller.","libVersion":"0.5.0","langs":""}