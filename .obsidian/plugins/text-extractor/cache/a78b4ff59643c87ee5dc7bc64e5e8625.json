{"path":"sem3/LinAlg/UE/s/LinAlg-s-u10.pdf","text":"D-INFK Linear Algebra HS 2024 Bernd G¨artner Robert Weismantel Solution for Assignment 10 1. a) Consider the matrix M := AB. We claim that it has rank(M ) = n. To see this, observe that rank(B) = n implies C(B) = Rn because n is also the number of rows of B. Hence, we get C(M ) = C(A) (and therefore rank(M ) = rank(A) = n). Finally, we can use Proposition 5.5.9 to get (AB)† = M † = B†A†. b) Let A = CR be the CR decomposition of A with C ∈ Rm×r and R ∈ Rr×n where r = rank(A). Observe that C has full column rank and that R has full row rank. Using the definition of the pseudoinverse, we compute A†AA† = (CR) †CR(CR) † = R†(C†C)(RR†)C† = R†C† = A† where we used that R† is a right inverse of R and C† a left inverse of C. c) Assume first that A has full column rank n = rank(A). In this case, we have A† = (A⊤A)−1A⊤ by definition of the pseudoinverse for matrices with full column rank. More- over, notice that A⊤ has full row rank and hence we also get (A⊤)† = A(A⊤A)−1 by definition of the pseudoinverse for matrices with full row rank. Hence, we get (A†) ⊤ = ((A⊤A) −1A⊤)⊤ = A((A ⊤A) −1)⊤ = A((A⊤A) ⊤) −1 = A(A⊤A) −1 = (A⊤)†. We conclude that the statements holds for all matrices with full column rank. Analogously, we can prove that the statement holds if A has full row rank m = rank(A). In that case, we have A† = A⊤(AA⊤)−1 and (A⊤)† = (AA⊤)−1A. Hence, we indeed get (A†)⊤ = (A⊤(AA⊤)−1)⊤ = ((AA ⊤)−1)⊤A = ((AA⊤) ⊤) −1A = (AA ⊤) −1A = (A⊤)†. We conclude that the statement holds for all matrices with full row rank. It remains to prove the general case, i.e. we do not assume anymore that A has full row rank or full column rank. Then by definition, we have A† = R†C† where A = CR is a CR decomposition of A. In particular, we have C ∈ Rm×r and R ∈ Rr×n where r = rank(A). Now observe that we also have A⊤ = R⊤C⊤ with R⊤ ∈ Rn×r and C⊤ ∈ Rr×m and of course, r = rank(A) = rank(A⊤). Hence, we can use Proposition 5.5.9 to get (A⊤)† = (C⊤)†(R⊤)†. We conclude that (A⊤) † = (C⊤) †(R⊤)† = (C†) ⊤(R†)⊤ = (R†C†) ⊤ = (A †)⊤ by using that C has full column rank and R has full row rank and hence (C⊤)† = (C†)⊤ and (R⊤)† = (R†)⊤. d) Let A = CR be a CR decomposition of A with C ∈ Rm×r and R ∈ Rr×n where r = rank(A). We can rewrite A†A = (CR) †CR = R†C†CR Prop. 5.5.2 = R†IR = R⊤(RR⊤) −1R and hence we conclude symmetry of AA† since (A†A) ⊤ = (R⊤(RR⊤) −1R)⊤ = R⊤((RR⊤)−1) ⊤R = R⊤((RR⊤) ⊤) −1R = R⊤(RR⊤) −1R = A†A. By Theorem 5.2.6, the matrix R⊤(RR⊤)−1R = A†A is exactly the projection matrix onto the subspace C(R⊤) = R(R) = R(A) = C(A⊤) (the equality R(R) = R(A) is due to the observation that R can be obtained from A through row operations and deleting 0-rows, and by recalling that row operations preserve the row space). 1 2. We provide two solutions. • In this first solution, we solve this by using our knowledge on pseudoinverses. Consider the function f −1 : C(A) → C(A⊤) given by f −1(x) = A†x for all x ∈ C(A). Observe that the composition f −1 ◦ f is the identity: we know from Exercise 1 that A†A is the projection matrix that projects vectors onto the subspace C(A⊤), and hence we have f −1(f (x)) = A†Ax = x for all x ∈ C(A⊤). This already implies that f is injective. Observe that with an analogous argument we get f (f −1(x)) = AA†x = x for all x ∈ C(A). Hence, f −1 is injective as well which implies that both f and f −1 are bijective. Note that the matrix A†A is in general not the identity matrix. It is crucial that the function f is only defined on C(A⊤) and not on all of Rn. • In this second solution, we start by proving injectivity. For this, let x1, x2 ∈ C(A⊤) be arbitrary and assume that f (x1) = f (x2). We want to argue that this implies x1 = x2. Observe that we have 0 = f (x1) − f (x2) = A(x1 − x2) and therefore x1 −x2 ∈ N(A). Together with C(A⊤)∩N(A) = {0} and x1 −x2 ∈ C(A⊤), we conclude x1 − x2 = 0 and hence x1 = x2. It remains to prove surjectivity. Let y ∈ C(A) be arbitrary. By Theorem 5.1.10, we know that there exists x ∈ C(A⊤) such that {z ∈ Rd : Az = y} = x+N(A) (the theorem applies because the set is non-empty since y ∈ C(A)). In particular, we have f (x) = Ax = y, as desired. 3. For every k ∈ {1, . . . , n − 1}, we define Sn−k = {1, . . . , n − k}. Our strategy is as follows: We first prove inductively that projSn−j (P ) is a polyhedron for all 1 ≤ j < n. This will then allow us to generalize the proof of Lemma 5.6.4 accordingly. • For the base case j = 1, observe that projSn−j (P ) by Theorem 5.6.3. • Thus, fix now an arbitrary n > j > 1 and assume that projS′ n−j (P ) is a polyhedron for j′ = j − 1 (induction hypothesis). • Under the above assumption, we want to prove that projSn−j (P ) is a polyhedron. Indeed, we know that from the induction hypothesis that Q := projS′ n−j (P ) is a polyhedron. Using Theorem 5.6.3 on Q, we hence conclude that projSn−j (P ) is a polyhedron as well. It remains to prove projSn−j (P ) = projSn−j (projSn−k (P )). for all indices 1 ≤ k < j < n. Let j, k be arbitrary such indices. Observe first that the expression projSn−j (projSn−k (P )) is now valid because we proved that projSn−k (P ) is a polyhedron (and projections are defined on polyhedra). Consider first an arbitrary z ∈ projSn−j (P ). By definition of projSn−j (P ), there exist xn−j+1, . . . , xn ∈ R such that the vector [ z1 . . . zn−j xn−j+1 . . . xn]⊤ ∈ R n is in P . By definition of projSn−k (P ), this directly implies that the vector [z1 . . . zn−j xn−j+1 . . . xn−k]⊤ ∈ R n−k 2 is in projSn−k (P ). Using the definition of projSn−j (projSn−k (P )), we conclude that z ∈ projSn−j (projSn−k (P )). For the other direction, consider now an arbitrary vector z ∈ projSn−j (projSn−k (P )). By definition of projSn−j (projSn−k (P )), there exist xn−j+1, . . . , xn−k ∈ R such that the vector [z1 . . . zn−j xn−j+1 . . . xn−k]⊤ ∈ R n−k is in projSn−k (P ). Now using the definition of projSn−k (P ), there must exist xn−k+1, . . . , xn ∈ R such that the vector [ z1 . . . zn−j xn−j+1 . . . xn]⊤ ∈ R n is in P . We conclude that z ∈ projSn−j (P ) by the definition of projSn−j (P ). 4. In the lecture, it was already proven that, given an arbitrary polyhedron P ⊆ Rn for some n, we have projSn−i(P ) ⊆ P (i) for all i ∈ [n] and that P (1) ⊆ projSn−1(P ). We can use this as base case and proceed by induction over i. Fix an arbitrary i > 1 and assume as induction hypothesis that we have P (i−1) = projSn−i+1(P ) for all polyhedra P ⊆ Rn where n ≥ n − i + 1. In the induction step, we want to prove that we also have P (i) = projSn−i(P ) for all polyhedra P ⊆ Rn with some n ≥ i. Thus, let P be an arbitrary such polyhedron and consider the polyhedron P ′ = projSn−i+1(P ) ⊆ Rn−i+1. Applying the base case for P ′ yields projSn−i(P ′) = P ′(1). Fur- ther, we know from Lemma 5.6.4 that projSn−i(P ′) = projSn−i(projSn−i+1(P )) = projSn−i(P ). It remains to prove P (i) ⊆ P ′(1). Using the induction hypothesis, we indeed observe that P ′(1) = (P (i−1))(1) = P (i), where the equality (P (i−1))(1) = P (i) follows from Definition 5.6.5. 5. Assume that P1 = {x ∈ R2 : A1x ≤ b1} and P2 = {x ∈ R2 : A2x ≤ b2} for some A1, A2 ∈ Qm×2 and b1, b2 ∈ Qm and natural number m (without loss of generality we can achieve that the two polyhedra have the same number of constraints by just repeating some constraints). Observe that the system [A1 A2 ] x ≤ [b1 b2 ] has no solution by our assumption P1 ∩P2 = ∅. Hence, Farkas lemma implies existence of a vector y ∈ R2m with y ≥ 0, y⊤ [ A1 A2 ] = 0, and y⊤ [ b1 b2 ] < 0. Let y1, y2 ∈ Rm be such that y = [y1 y2 ]. Observe that y⊤ [ A1 A2 ] = 0 can be rewritten as y⊤ 1 A1 + y⊤ 2 A2 = 0 and hence y⊤ 1 A1 = −y⊤ 2 A2. Similarly, we get y⊤ 1 b1 < −y⊤ 2 b2. Now define v := y⊤ 1 A1 ∈ R2 and w := y⊤ 1 b1 ∈ R. We claim that P1 ⊆ {x ∈ R2 : x · v ≤ w} and P2 ⊆ {x ∈ R2 : x · v > w}. To prove this, let first x ∈ P1 be arbitrary. Then A1x ≤ b1. Using y1 ≥ 0, we hence get y⊤ 1 A1x ≤ y⊤ 1 b1 and thus x ∈ {x ∈ R2 : x · v ≤ w}, as desired. Thus, let now x ∈ P2 be arbitrary. Then A2x ≤ b2. By y2 ≥ 0 we again get y⊤ 2 A2x ≤ y⊤ 2 b2. Using our previous observations, we can rewrite this as −y⊤ 1 A1x ≤ y⊤ 2 b2 < −y⊤ 1 b1. Multiplying both sides with −1 yields x ∈ {x ∈ R2 : x · v > w}, as desired. This concludes the proof. 3","libVersion":"0.5.0","langs":""}