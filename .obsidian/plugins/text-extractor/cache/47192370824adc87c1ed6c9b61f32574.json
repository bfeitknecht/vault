{"path":"sem3/A&D/VRL/script/A&D-script-w05-ADT.pdf","text":"Skript zur Vorlesung Algorithmen und Datenstrukturen Datenstrukturen Herbstsemester 2024 Stand: 16. Oktober 2024 Johannes Lengler David Steurer Inhaltsverzeichnis 1 Abstrakte Datentypen und Datenstrukturen 1 1.1 ADT Liste . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 1.1.1 Arrays . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2 1.1.2 Einfach verkettete Listen . . . . . . . . . . . . . . . . . . . . 3 1.1.3 Doppelt verkettete Listen . . . . . . . . . . . . . . . . . . . . 3 1.2 Stapel / Stack . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 1.3 Schlange / Queue . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 1.4 Priorit¨atswarteschlange / Priority Queue . . . . . . . . . . . . . . . . 7 2 W¨orterb¨ucher 9 2.1 Der abstrakte Datentyp W¨orterbuch . . . . . . . . . . . . . . . . . . 9 2.2 Bin¨are Suchb¨aume . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10 2.2.1 Einf¨ugen in bin¨are Suchb¨aume . . . . . . . . . . . . . . . . . 11 2.2.2 L¨oschen aus bin¨aren Suchb¨aumen . . . . . . . . . . . . . . . . 12 2.3 AVL-B¨aume . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 2.3.1 Problem: unbalancierte B¨aume . . . . . . . . . . . . . . . . . 15 2.3.2 Balancierte Suchb¨aume . . . . . . . . . . . . . . . . . . . . . 15 2.3.3 Einf¨ugen in AVL-B¨aume . . . . . . . . . . . . . . . . . . . . . 18 i ii Kapitel 1 Abstrakte Datentypen und Datenstrukturen Im vorigen Teil des Skripts haben wir uns mit dem Problem des Suchens in Arrays besch¨aftigt und verschiedene Algorithmen zur Sortierung von Arrays kennengelernt. Dabei haben wir festgestellt, dass die Effizienz der Suche stark davon abh¨angt, ob die Daten bereits sortiert sind. Sortieren l¨asst sich als ein Spezialfall eines wichtigeren Themas auffassen, n¨amlich der Organisation von Daten im Speicher, um effizient darauf zugreifen und sie mani- pulieren zu k¨onnen. In diesem Zusammenhang spielen Datenstrukturen eine zentrale Rolle. Eine Datenstruktur ist eine bestimmte Art und Weise, Daten im Speicher eines Computers zu repr¨asentieren. Mit Datenstrukturen eng verkn¨upft ist der Begriff des abstrakten Datentyps (ADT). Ein ADT beschreibt, was wir mit einer Menge von Daten tun wollen, ohne im Detail festzulegen, wie dies genau geschehen soll. Mit anderen Worten: Ein ADT definiert die Operationen, die auf einer Menge von Daten ausgef¨uhrt werden k¨onnen, ohne die konkrete Implementierung dieser Operationen vorzuschreiben. Man kann sich einen ADT als eine Art Wunschliste vorstellen, die festlegt, welche Funktionalit¨aten eine Datenstruktur bereitstellen soll. Die Datenstruktur selbst ist dann die Implementierung dieser Wunschliste. Es ist durchaus m¨oglich, dass es f¨ur einen gegebenen ADT mehrere unterschiedliche Implementierungen gibt, die jeweils verschiedene Vor- und Nachteile haben. 1.1 ADT Liste Als einf¨uhrendes Beispiel betrachten wir den abstrakten Datentyp Liste.1 Eine Liste ist eine geordnete Sammlung von Objekten, bei der die Reihenfolge der Objekte relevant ist. Hier nehmen wir an, dass es sich bei den Objekten um ganze Zahlen handelt, die auch Schl¨ussel genannt werden.2 Formaler: Eine Liste L = (k1, . . . , kn) 1Dieser ADT ist zu Illustrationszwecken gew¨ahlt und entspricht nicht genau den ADT, die etwa in Java verwendet werden. Tats¨achlich unterst¨utzt der ADT “List” in Java schon Dutzende von Operationen. 2Je nach Anwendung trennt man zwischen dem Objekt und dem Schl¨ussel. Ein Beispiel f¨ur Ob- jekte w¨are etwa der Datensatz mit allen Informationen, die eine Universit¨at ¨uber einen Studenten hat, wie Studienfach, Semester, bestandene Kurse und so weiter. Zur Organisation der Daten be- nutzt die Universit¨at als Schl¨ussel die Immatrikulationsnummer. In der Datenstruktur werden dann nur die Schl¨ussel geeignet angeordnet, und an jedem Schl¨ussel wird zus¨atzlich gespeichert, wo der komplette Datensatz zu finden ist. Auf diese Weise verhindert man, komplette Datens¨atze st¨andig im Speicher bewegen zu m¨ussen. 1 2 Abstrakte Datentypen und Datenstrukturen enth¨alt n Schl¨ussel in der Reihenfolge k1, . . . , kn. Die Schl¨ussel k¨onnen dabei auch mehrfach auftreten. Typische Operationen, die auf Listen definiert werden, sind: • insert(k, L): F¨ugt den Schl¨ussel k am Ende der Liste L ein. • get(i, L): Gibt den Speicherort des i-ten Objekts in L zur¨uck. • delete(k, L): Entfernt den Schl¨ussel k aus der Liste L. • insertAfter(k, k′, L): F¨ugt den Schl¨ussel k′ hinter dem Schl¨ussel k in die Liste L ein. Bei den Operationen delete und insertAfter gibt es zwei verschiedene Varian- ten, wie der Schl¨ussel k gegeben sein kann. Entweder ist nur der Wert des Schl¨ussels gegeben, sodass die Routine das Objekt erst finden muss. Oder die Routine bekommt als Input zus¨atzlich die Information, wo in der Datenstruktur der Schl¨ussel k zu fin- den ist. Letzteres ist insbesondere n¨otig, falls Schl¨ussel mehrfach auftreten d¨urfen. Hier gehen wir von der zweiten Variante aus. Es gibt verschiedene M¨oglichkeiten, den ADT Liste zu implementieren. Zwei h¨aufig verwendete Ans¨atze sind Arrays und verkettete Listen. 1.1.1 Arrays Sofern eine obere Schranke f¨ur die maximale L¨ange der Liste bekannt ist, l¨asst sich der ADT Liste mittels eines Arrays implementieren. Dazu wird ein Array A fester Gr¨osse angelegt, dessen Elemente die Schl¨ussel der Liste speichern. Die ℓ Elemente der Liste belegen die ersten ℓ Zellen des Arrays; der restliche Teil des Arrays enth¨alt keine relevanten Daten. Zus¨atzlich speichern wir die L¨ange ℓ der Liste. 1 2 3 4 5 6 7 8 ℓ = 8 Laufzeiten Die Operation insert(k, L) ist in Zeit O(1) m¨oglich. Die erste freie Stelle im Array ist A[ℓ + 1], also schreiben wir den Schl¨ussel an diese Stelle. Ausser- dem erh¨ohen wir ℓ um 1. Die Operation get(i, L) ist ebenfalls in Zeit O(1) m¨oglich, da wir in konstanter Zeit auf den richtigen Speicherort A[i] zugreifen k¨onnen. F¨ur die Operation insertAfter(k, k′, L) m¨ussen wir den Inhalt von allen Zel- len hinter der Position von k um eins nach rechts verschieben. Jede Verschiebung ben¨otigt Zeit Θ(1), deshalb braucht diese Operation im schlimmsten Fall Zeit Θ(ℓ). Die Operation delete(k, L) ben¨otigt im schlimmsten Fall ebenfalls Zeit Θ(ℓ), denn zum Entfernen des Schl¨ussels k m¨ussen wir alle Zelleninhalte dahinter um eins nach links verschieben. Andernfalls w¨urden L¨ucken entstehen.3 3Es erscheint verlockend, eine andere Implementierung der ADT Liste mit einem Array zu ent- wickeln, die L¨ucken zul¨asst. Das kann man nat¨urlich machen, aber dann verschlechtert sich die Laufzeit von get(i, L), da nicht mehr garantiert ist, dass das i-te Element an Position i steht. 1.1 ADT Liste 3 1.1.2 Einfach verkettete Listen Eine einfach verkettete Liste ist eine dynamische Datenstruktur, bei der die Elemente nicht in aufeinanderfolgenden Speicherzellen abgelegt werden, sondern ¨uber Zeiger (Pointer) miteinander verbunden sind. Ein solcher Zeiger ist einfach eine Adresse im Speicher. Jedes Element einer verketteten Liste besteht aus dem Schl¨ussel und einem Zeiger, der auf das n¨achste Element der Liste verweist, d.h. der Zeiger enth¨alt die Speicheradresse des n¨achsten Listenelements. Der letzte Zeiger der Liste hat den Wert null, um anzuzeigen, dass keine weiteren Elemente folgen. Zus¨atzlich haben wir einen Zeiger, der auf das erste Element der Liste zeigt. In der Praxis wird ausserdem manchmal ein zus¨atzlicher Zeiger verwendet, der auf das letzte Element der Liste zeigt. 1 2 3 1.1.3 Doppelt verkettete Listen Eine doppelt verkettete Liste ist ¨ahnlich zu einer einfach verketteten Liste, allerdings speichern wir f¨ur jedes Element zus¨atzlich einen Zeiger auf den Vorg¨anger. Doppelt verkettete Listen ben¨otigen mehr Speicher, sind aber oft komfortabler und daher in der Praxis weiter verbreitet. Zum Beispiel ist die Java-Datenstruktur LinkedList mit einer doppelt verketteten Liste implementiert. 1 2 3 Laufzeiten Viele Laufzeiten sind f¨ur einfach und doppelt verlinkte Listen identisch. Die Operation insert(k, L) ben¨otigt Zeit Θ(1), wenn wir einen Zeiger zum letzten Listenelement haben, sonst Zeit Θ(ℓ). Die Operation get(i, L) ist in beiden F¨allen sehr langsam und ben¨otigt Zeit Θ(i), da wir der Liste Schritt f¨ur Schritt folgen m¨ussen, um den Speicherort des i-ten Elements zu finden. Die Operation insertAfter(k, k′, L) ist dagegen in Zeit O(1) m¨oglich, vorausge- setzt wir bekommen als Input den Speicherort des Schl¨ussels k. Denn dann schreiben wir den Schl¨ussel k′ in eine beliebige Stelle im Speicher, lassen den Zeiger von k auf k′ zeigen und lassen den Zeiger von k′ auf den fr¨uheren Nachfolger von k zeigen. Die Operation delete(k, L) ist sogar noch einfacher: Wir lassen einfach den Zeiger des Vorg¨angers von k auf den Nachfolger von k zeigen, sodass k beim Ver- folgen der Zeiger ¨ubersprungen wird.4 Es gibt aber einen Haken: Bei einer einfach verketteten Liste kennen wir den Speicherort des Vorg¨angers nicht und m¨ussen die Liste von Anfang an durchgehen, um diesen Speicherort zu finden. Daher braucht delete(k, L) f¨ur doppelt verlinkte Listen nur Zeit O(1), bei einfach verlinkten Listen im schlimmsten Fall aber Zeit Θ(ℓ), siehe Abbildungen 1.1 und 1.2. 4Verschiedene Programmiersprache gehen sehr unterschiedlich mit der Frage um, ob man den Speicherort von k explizit freigeben sollte, falls k nicht mehr ben¨otigt wird. 4 Abstrakte Datentypen und Datenstrukturen Array einf. verlinkte Liste dopp. verlinkte Liste insert(k, L) O(1) O(1) O(1) get(i, L) O(1) O(ℓ) O(ℓ) insertAfter(k, k′, L) O(ℓ) O(1) O(1) delete(k, L) O(ℓ) O(ℓ) O(1) Tabelle 1.1 Laufzeiten von verschiedenen Implementierungen des abstrakten Datentyps Liste. Wir gehen davon aus, dass wir bei einfach verlinkten Listen einen Zeiger auf das letzte Element haben, und dass wir bei insertAfter und delete als Input den Speicherplatz des Schl¨ussels k in der Datenstruktur bekommen. 1 2 3 1 2 Abb. 1.1 Der Schl¨ussel 3 kann in einer doppelt verketteten Liste effizient gel¨oscht werden, wenn man den Speicherort des Objekts kennt. Tabelle 1.1 fasst die verschiedenen Laufzeiten zusammen. Wir sehen, dass alle Implementierungen ihre Schw¨achen haben. Wir werden sp¨ater Implementierungen kennenlernen (mit Hilfe von sogenannten W¨orterb¨uchern), die alle Operationen in Zeit O(log n) ausf¨uhren k¨onnen. 1.2 Stapel / Stack Ein verbreiteter abstrakter Datentyp ist der Stapel oder Keller, im Englischen Stack genannt. ¨Ahnlich wie bei der Liste ist es eine geordnete Sammlung von Objekten bzw. Schl¨usseln, aber die Operationen sind eingeschr¨ankter, was effizientere Imple- 1 2 3 1 2 3 ? ? Abb. 1.2 Der Schl¨ussel 3 kann in einer einfach verketteten Liste nicht effizient gel¨oscht werden. Selbst wenn man den Speicherort kennt, kann man den Zeiger des Vorg¨angerelements nicht effizient aktualisieren, weil man seinen Speicherort nicht kennt. 1.3 Stapel / Stack 5 mentierungen erlaubt. Man hat immer nur direkten Zugriff auf den Schl¨ussel, der als letztes eingef¨ugt wird. Der abstrakte Datentyp wird deshalb auch als LIFO f¨ur “Last in, First Out” bezeichnet. In der physischen Welt entspricht dies beispielsweise einem Stapel von Tellern, bei denen man immer nur den obersten Teller wegnehmen kann bzw. einen neuen Teller nur oben auf den Stapel legen kann. Die wichtigsten Operation sind: • push(k, S): Legt ein neues Objekt mit Schl¨ussel k oben auf dem Stapel S ab. • pop(S): Entfernt (und liefert) das oberste Objekt von S. • top(S): Liefert das oberste Objekt von S, ohne es zu l¨oschen. Andere Operationen sind etwa isempty(S), das pr¨uft, ob S leer ist, und emptystack, welches einen leeren Stack erzeugt. s1 s2 ... sn x S push(x) pop Die ADT Stack kann effizient mit einer einfach verketteten Liste implementiert werden, wobei vorne die neusten Schl¨ussel stehen. Bei der Operation push(k, S) m¨ussen wir also ein neues Element am Anfang der Liste einf¨ugen, bei pop(S) das erste Element aus der Liste entfernen. Beides ist in Zeit Θ(1) m¨oglich. push(1, S) 2 3 1 2 3 pop(S) 1 2 3 2 3 Eine Implementierung mit Arrays ist auch m¨oglich, wenn man schon vorher eine obere Schranke f¨ur die Maximall¨ange kennt.5 5Tats¨achlich ist es m¨oglich, diese Einschr¨ankung mit Hilfe von sogenannten dynamischen Arrays zu umgehen. Bei diesen schw¨acht man die Laufzeitgarantien ab. Es wird nicht mehr garantiert, dass 6 Abstrakte Datentypen und Datenstrukturen 1.3 Schlange / Queue Der ADT Schlange oder Warteschlange, Englisch Queue, ist ¨ahnlich wie der Stapel. Allerdings haben wir hier nur Zugriff auf den ¨altesten Schl¨ussel, also den Schl¨ussel, der schon am l¨angsten in der Datenstruktur vorhanden ist. Deshalb nennt man die ADT auch FIFO f¨ur “First in, first out”. In der physischen Welt kommt so eine Reihenfolge etwa bei Warteschlangen im Supermarkt vor, wo Kunden in derselben Reihenfolge bedient werden, in der sie die Schlange betreten haben. s1 s2 · · · snx S Eine Schlange beinhaltet die folgenden Operation: • enqueue(k, S): F¨ugt ein neues Objekt mit Schl¨ussel k hinten an der Schlange S an. • dequeue(S): Entfernt (und liefert) das vorderste Objekt von S. ¨Ahnlich wie bei Stapeln gibt es weitere Operationen wie isempty(S) und emptyqueue. Schlangen k¨onnen effizient mit einer einfach verlinkten Liste realisiert werden, die zus¨atzlich ¨uber einen Zeiger auf das letzte Element der Liste verf¨ugt. Bei enqueue(k, S) wird also ein Element ans Ende der Liste hinzugef¨ugt, bei dequeue(S) wird das erste Element gel¨oscht.6 Beide Operationen sind damit in Zeit O(1) m¨oglich. 1 2 3 dequeue(S) enqueue(3, S) Ist eine Maximalgr¨osse f¨ur die Zahl der Schl¨ussel bekannt7, so ist eine Implemen- tierung mit Arrays ebenfalls m¨oglich. Dabei benutzt man das Array wie einen Ring (“Ringpuffer”, Englisch “ring buffer”), implementiert Anfragen also so, als w¨are der erste Array-Eintrag direkt hinter dem letzten Array-Eintrag gespeichert. F¨ur die Schlange speichert man dann zus¨atzlich noch einen Zeiger auf das erste und das letzte Element der Schlange im Array. jede einzelne Operation Laufzeit O(1) hat. Aber es wird garantiert, dass jede Folge von m Opera- tionen, die mit einer leeren Liste startet, zusammengenommen (amortisiert) Laufzeit O(m) haben. Somit hat jeder Benutzer im Durchschnitt Laufzeit O(1) pro Operation, egal welche Operationen er mit dem dynamischen Array durchf¨uhrt. Sie werden solche fortgeschrittenen Ideen im weiteren Verlauf des Studiums kennenlernen. 6Es ist wichtig, diese Reihenfolge zu benutzen. Es ist nicht m¨oglich, das letzte Element der Liste effizient zu l¨oschen, weil man dann den Zeiger auf das letzte Element nicht aktualisieren kann. Das w¨are nur bei einer doppelt verketteten Liste effizient m¨oglich. 7Wiederum kann man diese Einschr¨ankung umgehen, wenn man sich mit amortisierten Laufzeit- garantien zufrieden gibt. 1.4 Priorit¨atswarteschlange / Priority Queue 7 S First Last Last First 1.4 Priorit¨atswarteschlange / Priority Queue Auch die Priorit¨atswarteschlange, Englisch Priority queue, ist eine ¨ahnliche ADT wie Stapel und Schlange. Allerdings bekommt hier jeder Schl¨ussel beim Einf¨ugen eine nat¨urliche Zahl (oder reelle Zahl) als Priorit¨at, und die Elemente werden dann in Reihenfolge ihrer Priorit¨at aus der Datenstruktur entfernt.8 Die Kern-Operationen sind: • insert(k, p, P ): F¨ugt ein neues Objekt mit Schl¨ussel k und Priorit¨at p in P ein. • extractMax(P ): Entfernt und liefert das Objekt bzw. den Schl¨ussel mit h¨ochs- ter Priorit¨at in P . Trotz des verwandten Namens besteht man meistens nicht darauf, dass Objekte glei- cher Priorit¨at in FIFO-Reihenfolge ausgegeben werden wie bei der Warteschlange. Die Namens¨ahnlichkeit zwischen Warteschlangen und Priorit¨atswarteschlangen ist deshalb etwas irref¨uhrend. Stattdessen wird es dem Algorithmus freigestellt, welches Objekt er in so einem Fall von Unentschieden zur¨uckgibt. Diese Entscheidung wird auf Englisch Tie-Breaking genannt.9 Wir haben bereits eine effiziente Implementierung von Priorit¨atswarteschlan- gen kennengelernt, n¨amlich die Datenstruktur (Max-)Heap. Diese garantiert Zeit O(log n) f¨ur beide Operationen, wobei n die Zahl der Objekte ist, die zum jeweiligen Zeitpunkt in der Priorit¨atswarteschlange sind. Bei Priorit¨atswartschlangen fordern manche Varianten der ADT zus¨atzliche nicht- triviale Operationen. Dazu geh¨ort etwa remove(k, P ), welches das Objekt mit Schl¨ussel k aus P l¨oscht. Eine andere typische Operation ist increaseKey(k, p, P ), welches die Priorit¨at des Objektes mit Schl¨ussel k auf p erh¨oht, falls dieser Wert h¨oher ist als die alte Priorit¨at. Sonst wird die Priorit¨at von k gleich gelassen. Man kann also 8Tats¨achlich ist die Konvention meistens, dass niedrige Zahlen eine h¨ohere Priorit¨at signalisieren. Also h¨atte Wert 1 eine h¨ohere Priorit¨at als ein Wert 2. Da dies zu sprachlicher Verwirrung f¨uhrt, folgen wir dieser Konvention nicht. In der Praxis haben die Operationen meist selbsterkl¨arende Namen wie extractMax oder extractMin. 9Es gibt durchaus Implementierungen von Vorrangwarteschlangen, die bei Unentschieden die FIFO-Reihenfolge garantieren. Das wird aber in der Regel nicht als integraler Bestandteil der ADT Priorit¨atswarteschlange gesehen. 8 Abstrakte Datentypen und Datenstrukturen die Ausgabe eines Objektes vorziehen, aber nicht zur¨uckstellen. Der Grund f¨ur diese Asymmetrie ist, dass viele Algorithmen nur diese Richtung ben¨otigen. Wir werden im Laufe der Vorlesung mehrere solche Algorithmen kennenlernen. Wie bei Listen sind bei beiden Operationen remove und increaseKey in zwei Varianten m¨oglich: entweder die Routine bekommt den Speicherort des jeweiligen Objektes in der Da- tenstruktur als Input, oder sie muss selbst nach dem Schl¨ussel suchen.10 10Nach einem Schl¨ussel zu suchen ist in einem Heap nicht ohne Weiteres m¨oglich. Wir werden im n¨achsten Teil mit Suchb¨aumen eine Datenstruktur kennenlernen, die daf¨ur gut geeignet ist. Man kann solche Datenstrukturen auch kombinieren. F¨ur eine Priorit¨atswarteschlange, die Suchen unterst¨utzen soll, k¨onnte man etwa alle Schl¨ussel sowohl in einem Heap als auch in einem Suchbaum abspeichern. Zus¨atzlich speichert man f¨ur jedes Objekt im Heap mit einem Zeiger, wo diese Objekt im Suchbaum steht, und umgekehrt. Auf diese Weise kann man im Suchbaum nach einem Schl¨ussel suchen, dort nachschauen, wo es im Heap gespeichert ist, und dann Operationen wie remove damit ausf¨uhren. Nat¨urlich ist diese L¨osung recht aufwendig. Kapitel 2 W¨orterb¨ucher 2.1 Der abstrakte Datentyp W¨orterbuch Ein W¨orterbuch W ist eine Datenstruktur, die eine Menge von Schl¨usseln verwaltet, Motivation wobei die Schl¨ussel aus den ganzen Zahlen sind. Dabei darf jeder Schl¨ussel h¨ochstens einmal in W vorkommen. H¨aufig speichert man f¨ur jeden Schl¨ussel k in W zudem einen zugeh¨origen Datensatz d. Man will dann z.B. in einem Telefonbuch nach einem Namen suchen und gegebenfalls die dazugeh¨orige Telefonnummer ausgeben. Wie schon bei Listen beschr¨anken wir uns der Einfachheit halber darauf, lediglich die Schl¨ussel zu speichern. Wir wollen nun die folgenden Operationen effizient ausf¨uhren k¨onnen. 1) search(x, W ): Entscheide, ob der Schl¨ussel x in W vorkommt, und gib in diesem Fall die Position im Speicher zur¨uck. 2) insert(x, W ): F¨uge den Schl¨ussel x in W ein, falls er noch nicht darin vor- kommt. 3) delete(x, W ): Finde und l¨osche den Schl¨ussel x aus W , falls er darin vor- kommt. Die einfachste Realisierung eines W¨orterbuchs verwendet ein unsortiertes Array. Ist die maximale Gr¨osse des W¨orterbuchs bekannt, dann l¨asst sich die Suche auch mit einem sortierten Array realisieren. Verkettete Listen sind eine weitere M¨oglichkeit, ein W¨orterbuch zu implementieren. Leider sind alle diese Varianten nur f¨ur be- stimmte Operationen effizient, und f¨ur andere ist die Laufzeit linear in der Gr¨osse des W¨orterbuchs (vgl. Tabelle 2.1). Wir werden in diesem Kapitel eine neue Daten- struktur kennenlernen, den AVL-Baum, die alle drei Operationen in Zeit O(log n) garantiert. Dazu f¨uhren wir zun¨achst das Konzept eines bin¨aren Suchbaums ein. search insert delete unsortiertes Array O(n) O(1) O(n) sortiertes Array O(log n) O(n) O(n) Doppelt verk. Liste O(n) O(1) O(n) Tabelle 2.1 Laufzeit der Operationen search, insert und delete f¨ur verschiedene Daten- strukturen, die zur Realisierung eines W¨orterbuchs geeignet sind. F¨ur die verketten Listen ist es unerheblich, ob die Elemente sortiert sind oder nicht, da man dort nicht effizient auf das mittlere Element zugreifen kann und deshalb bin¨are Suche nicht funktioniert. 9 10 W¨orterb¨ucher 6 3 1 4 5 8 7 Abb. 2.1 Ein bin¨arer Suchbaum, der die Suchbaum-Bedingung erf¨ullt. F¨ur jeden Knoten x gilt: Alle Schl¨ussel im linken Teilbaum T1 von x sind kleiner als der Schl¨ussel in x, und alle Schl¨ussel im rechten Teilbaum T2 von x sind gr¨osser als der Schl¨ussel in x. Ein Zeiger auf null ist mit einem leeren Kreis dargestellt. 2.2 Bin¨are Suchb¨aume Ein bin¨arer Suchbaum ist ein bin¨arer Baum, in dem jeder Knoten einen Schl¨ussel enth¨alt1, und der die sogenannte Suchbaum-Bedingung erf¨ullt: F¨ur jeden Knoten x im Baum sind alle Schl¨ussel im linken Teilbaum von x kleiner als der Schl¨ussel in x, und alle Schl¨ussel im rechten Teilbaum von x sind gr¨osser als der Schl¨ussel in x. Abbildung 2.1 gibt ein Beispiel f¨ur einen Suchbaum. Anders als bei der Imple- mentierung eines Heaps nehmen wir hier an, dass in jedem Knoten p neben dem Schl¨ussel p.key auch zwei Zeiger mit den Speicherorten der beiden Kindknoten ent- halten sind.2 Hat ein Zeiger den Wert null, so dr¨uckt das aus, dass der entsprechen- de Kindknoten nicht existiert. Im Pseudocode bezeichnen wir mit mit p.left und p.right die beiden Kindknoten, ohne zwischen dem Zeiger und dem entsprechenden Knoten zu unterscheiden. Die Suchbaum-Bedingung hat zwei wesentliche Vorteile: Erstens ist sie sehr ein-Vorteile der Suchbaum- Bedingung fach zu ¨uberpr¨ufen. Und zweitens erm¨oglicht sie es uns, einen Schl¨ussel b in einem Suchbaum mit n Knoten in Zeit O(h) zu suchen, wobei h die H¨ohe des Suchbaums bezeichnet. Der folgende Pseudocode zeigt das Verfahren, welches sehr einfach ist. Search(x, p)Suche nach dem Schl¨ussel x im Suchbaum unterhalb des Knotens p 1 if p ist null then return “nicht gefunden” 2 else if x = p.key then return p 3 else if x < p.key then return Search(x, p.left) 4 else return Search(x, p.right) Befindet sich der Schl¨ussel b im Suchbaum, dann findet der Algorithmus diesen nach maximal h + 1 vielen Schritten, denn der Algorithmus steigt in jedem Schritt um eine Ebene im Baum nach unten ab. Gilt also h ≤ O(log n), dann l¨auft Search in logarithmischer Zeit (wie die bin¨are Suche auf einem sortierten Array). Nat¨urlich 1Es gibt auch Varianten, in denen die Bl¨atter Schl¨ussel enthalten. Es ist weitgehend eine Ge- schmacksfrage, welche Variante man bevorzugt. 2Je nach Anwendung w¨urde man zus¨atzlich auch einen Zeiger zum Elternknoten speichern. 2.2 Bin¨are Suchb¨aume 11 22 7 3 8 12 70 27 31 88 89 Abb. 2.2 Ein Suchbaum mit 10 Knoten und H¨ohe h = 3. Die Suche nach dem Schl¨ussel 27 ben¨otigt drei Vergleiche (22, 70 und 27). Allgemein kommt jede Suche kommt mit h¨ochstens h + 1 Vergleichen aus. hat diese Art zu suchen nur dann Vorteile, wenn die H¨ohe h des Baums tats¨achlich klein ist. Beispiel. Abbildung 2.2 zeigt einen Suchbaum mit 10 Knoten und H¨ohe h = 3, der die Suchbaum-Bedingung erf¨ullt. Um z.B. nach dem Schl¨ussel 27 zu suchen, starten wir an der Wurzel und vergleichen 27 mit dem dort gespeicherten Schl¨ussel 22. Da 27 > 22 gilt, suchen wir im rechten Teilbaum weiter. Der dort gespeicherte Schl¨ussel ist 70, und wegen 27 < 70 suchen wir im linken Teilbaum weiter. Dort finden wir den Schl¨ussel 27. Dieser ist in Tiefe 2, die Suche ist also nach drei Vergleichen beendet. 2.2.1 Einf¨ugen in bin¨are Suchb¨aume Um einen Schl¨ussel x in einen Suchbaum einzuf¨ugen, gehen wir zun¨achst wie bei der Insert Suche in Algorithmus Search vor: Beginnend an der Wurzel steigen wir so lange im Baum nach unten ab, bis wir entweder auf x stossen (dann ist x bereits im Baum enthalten und es ist nichts weiter zu tun), oder aber ein Knoten v erreicht wird, der kein Kind in der ben¨otigten Richtung hat (links, falls x < v.key gilt, oder rechts, falls x > v.key gilt). In diesem Fall f¨ugen wir einen neuen Knoten, der den Schl¨ussel x speichert, als Kind von v ein. Insert(x) F¨uge den Schl¨ussel x in den Suchbaum ein, falls er noch nicht darin vorkommt 1 p ← root ▷ Starte an der Wurzel 2 if p ist null then 3 root ← neuer Knoten mit Schl¨ussel x ▷ Baum war leer 4 return “Erfolgreich eingef¨ugt” 5 repeat 6 if x = p.key then return “Schl¨ussel bereits vorhanden” 7 else if x < p.key then 8 if p.left ist null then ▷ F¨uge x links ein 9 p.left ← neuer Knoten mit Schl¨ussel x 10 return “Erfolgreich eingef¨ugt” 11 else p ← p.left ▷ Suche links weiter 12 else ▷ falls x > p.key 13 if p.right ist null then ▷ F¨uge x rechts ein 14 p.right ← neuer Knoten mit Schl¨ussel x 15 return “Erfolgreich eingef¨ugt” 12 W¨orterb¨ucher 22 7 3 5 8 12 70 27 31 88 89 Abb. 2.3 Der Suchbaum aus Abbildung 2.2 nach dem Einf¨ugen des Schl¨ussels 5. 16 else p ← p.right ▷ Suche rechts weiter Auch Insert ben¨otigt maximal h + 1 viele Schritte, denn es steigt h¨ochstens bis zu einem Blatt im Baum ab, und ein Blatt befindet sich in Tiefe h. Die +1 kommt daher, dass wir mit der Wurzel in Tiefe 0 beginnen. Folglich ist die Laufzeit im schlimmsten Fall wieder linear, und logarithmisch, wenn die H¨ohe des Suchbaums logarithmisch in der Anzahl der gespeicherten Schl¨ussel beschr¨ankt ist. Beispiel. Um den Schl¨ussel 5 in den Suchbaum aus Abbildung 2.2 einzuf¨ugen, starten wir an der Wurzel und vergleichen 5 mit 22. Wegen 5 < 22 gehen wir in den linken Teilbaum. Dort ist der Schl¨ussel 7 gespeichert, und da 5 < 7 gilt, suchen wir im linken Teilbaum weiter. Dort ist der Schl¨ussel 3 gespeichert, also gehen wir in den rechten Teilbaum. Dieser ist leer, und daher f¨ugen wir den Schl¨ussel 5 als rechtes Kind des Knotens mit Schl¨ussel 3 ein (vgl. Abbildung 2.3). 2.2.2 L¨oschen aus bin¨aren Suchb¨aumen Das L¨oschen eines Schl¨ussels x aus einem Suchbaum gestaltet sich etwas schwieri-L¨oschen ger als das Einf¨ugen. Auch hier suchen wir zun¨achst nach dem Knoten p, der den Schl¨ussel x speichert. Wurde der Schl¨ussel nicht gefunden, dann ist nichts weiter zu tun. Ansonsten unterscheiden wir drei F¨alle: 1) p ist ein Blatt, d.h., p hat keine Kinder. 2) p hat genau ein Kind (entweder links, oder rechts). 3) p hat zwei Kinder. Die F¨alle 1 und 2 sind einfach zu behandeln. Im ersten Fall entfernen wir p einfach aus dem Suchbaum. Im zweiten Fall ersetzen wir p durch sein einziges Kind, siehe Abbildung 2.4. Der dritte Fall ist aufw¨andiger. Hier ersetzen wir p durch den Knoten mit dem kleinsten Schl¨ussel im rechten Teilbaum von p, den wir s nennen wollen. Dieser Schl¨ussel ist gr¨osser als alle Schl¨ussel im linken Teilbaum von p, und kleiner als alle anderen Schl¨ussel im rechten Teilbaum von p. Also bleibt die Suchbaum-Bedingung auch nach dem L¨oschen von p erf¨ullt. Schliesslich m¨ussen wir den Knoten s (also den Knoten mit dem kleinsten Schl¨ussel im rechten Teilbaum von p) l¨oschen. Das scheint zu einem rekursiven Algorithmus zu f¨uhren, aber tats¨achlich f¨allt das L¨oschen von s immer in einen der einfachen F¨alle 1) oder 2). Das liegt daran, dass s kein linkes Kind hat, weil dieses sonst ein Knoten mit noch kleinerem Schl¨ussel als s w¨are, das ebenfalls im rechten Teilbaum von p w¨are. Abbildung 2.5 zeigt ein Beispiel. 2.2 Bin¨are Suchb¨aume 13 10 5 T1 9 6 T2 11 T3 −→ 10 5 T1 9 6 T2 T3 Abb. 2.4 DELETE(11) 10 5 T1 9 6 T2 11 T3 −→ 10 6 T1 9 T2 11 T3 Abb. 2.5 DELETE(5) 14 W¨orterb¨ucher Es w¨urde ebenso funktionieren, den Knoten p durch den Knoten mit dem gr¨oss- ten Schl¨ussel im linken Teilbaum von p zu ersetzen statt mit s. Der folgende Pseu- docode beschreibt das Verfahren im Detail. Delete(x)L¨oschen des Schl¨ussels x aus dem Suchbaum 1 p ← Search(x, root) ▷ Suche nach x 2 if p =“nicht gefunden” then return “nicht gefunden” ▷ Suche war erfolglos 3 if p.left = null and p.right = null then ▷ Fall 1: p ist ein Blatt 4 Ersetze p durch null 5 else if p.left = null then ▷ Fall 2: p hat nur rech- tes Kind 6 Ersetze p durch p.right 7 else if p.right = null then ▷ Fall 2: p hat nur linkes Kind 8 Ersetze p durch p.left 9 else ▷ Fall 3: p hat zwei Kin- der 10 s ← p.right ▷ Suche im rechten Teil- baum 11 while s.left ̸= null do 12 s ← s.left ▷ nach dem kleinsten Schl¨ussel 13 p.key ← s.key ▷ ersetze p durch s 14 Ersetze s durch s.right ▷ L¨osche s 15 return “Erfolgreich gel¨oscht” Im Pseudocode sehen wir zwei verschiedene M¨oglichkeiten, einen Knoten durch einen anderen zu ersetzen. In Zeile 13 ¨uberschreiben wir nur den Schl¨ussel von p mit dem Schl¨ussel von s, aber die Position im Baum, also Eltern- und Kindknoten bleiben unver¨andert. In den Zeilen 4, 6, 8 und 14 ersetzen wir dagegen einen Knoten durch einen anderen. Damit ist etwa in Zeile 6 gemeint, dass sowohl der Schl¨ussel von p als auch die Zeiger auf die Kindknoten von p durch Schl¨ussel und Zeiger auf die Kindknoten von p.right ersetzt werden.3 Alternativ kann man auch alle Zeiger auf p durch Zeiger auf p.right ersetzen, sodass der alte Knoten p im Speicher zwar noch existiert, aber nicht mehr auffindbar ist. Dazu muss man sich w¨ahrend der Suche nach p auch den Elternknoten von p merken. Ein ausf¨uhrlicherer Pseudocode f¨ur Zeile 6 w¨are etwa der folgende, wobei hier parent den Elternknoten von p bezeichnen soll. Ausf¨uhrung von Zeile 6 im Pseudocode von Delete(x) 1 if parent = null then ▷ p ist Wurzel 2 root ← p.right 3 else if p = parent.left then ▷ p ist linkes Kind 4 parent.left ← p.right 5 else parent.right ← p.right ▷ p ist rechtes Kind 3Falls man in der Implementierung allerdings auch einen Zeiger p.parent benutzt, so soll dieser nicht ersetzt werden und nach wie vor auf den Elternknoten von p zeigen. 2.3 AVL-B¨aume 15 1 2 3 4 5 6 7 Abb. 2.6 Ein Suchbaum mit sieben Knoten und H¨ohe h = 6. In diesem Fall ist die H¨ohe des Baums linear in der Anzahl der Knoten, folglich ist die Laufzeit der Algorithmen Search, Insert und Delete im schlechtesten Fall auch linear. Wie Search und Insert ben¨otigt auch Delete im schlimmsten Fall Zeit O(h), was linear in der Anzahl der Knoten sein kann. Alle drei Operationen sind also effizient, falls die H¨ohe des Baumes klein ist. Beispiel. Um den Schl¨ussel 88 aus dem Suchbaum in Abbildung 2.2 zu l¨oschen, star- ten wir an der Wurzel und suchen nach dem Schl¨ussel 88. Da er zwei Kinder hat, m¨ussen wir ihn durch den Knoten mit dem kleinsten Schl¨ussel im rechten Teilbaum ersetzen (in diesem Fall: 89). Wir ¨uberschreiben also den Schl¨ussel 88 mit dem Schl¨ussel 89 und l¨oschen anschliessend rekursiv den Schl¨ussel 89 (welcher ein Blatt ist, also einfach gel¨oscht werden kann). 2.3 AVL-B¨aume 2.3.1 Problem: unbalancierte B¨aume Die im vorigen Abschnitt vorgestellten Algorithmen Search, Insert und Delete ben¨otigen alle im schlimmsten Fall Zeit O(h), wobei h die H¨ohe des Suchbaums ist. Ist die H¨ohe logarithmisch in der Anzahl der Schl¨ussel beschr¨ankt (also h ≤ O(log n)), dann laufen alle drei Algorithmen in logarithmischer Zeit. Leider ist die H¨ohe eines Suchbaums im Allgemeinen aber nicht logarithmisch beschr¨ankt. Beispiel. F¨ugen wir die Schl¨ussel 1, 2, 3, 4, 5, 6, 7 in dieser Reihenfolge in einen leeren Suchbaum ein, dann erhalten wir einen Baum der H¨ohe h = 6 (vgl. Abbildung 2.6). Dieser Baum ist in dem Sinne unbalanciert, als dass der linke Teilbaum der Wurzel leer ist, w¨ahrend der rechte Teilbaum die maximale H¨ohe hat. Diese Situation tritt immer dann ein, wenn die Schl¨ussel in sortierter Reihenfolge eingef¨ugt werden. 2.3.2 Balancierte Suchb¨aume Um das Problem unbalancierter B¨aume zu beheben, verwenden wir balancierte Suchb¨aume. Die Idee dabei ist, die Suchbaum-Bedingung um zus¨atzliche Bedin- gungen zu erweitern, die garantieren, dass die H¨ohe des Baumes klein ist, also lo- garithmisch in der Anzahl der Knoten beschr¨ankt bleibt. Das Erreichen einer loga- 16 W¨orterb¨ucher 10 5 2 1 4 8 7 9 15 12 11 14 17 insert(3) −−−−−−→ 9 4 2 1 3 7 5 8 14 11 10 12 17 15 Abb. 2.7 Links ist der einzige perfekt balancierte bin¨are Baum (im Sinne von Heaps) mit den Schl¨usseln 1, 2, 4, 5, 7, 8, 9, 10, 11, 12, 14, 15, 17. Rechts der einzige perfekt balancierte Baum mit denselben Schl¨usseln und zus¨atzlich dem Schl¨ussel 3. Die Positionen der Knoten ¨andern sich drastisch, inklusive der Eltern-Kind-Verh¨altnisse zwischen Knoten. F¨ur ein sol- ches Umbauen der Datenstruktur sind im schlimmsten Fall Θ(n) Operationen n¨otig. rithmischen H¨ohe ist aber nicht trivial, denn wir m¨ussen die Balance nach jedem Insert oder Delete wiederherstellen. Man k¨onnte versuchen, mit perfekt balancierten Suchb¨aumen zu arbeiten, wie wir sie f¨ur Heaps verwendet haben, also vollst¨andig gef¨ullte Layer aus dem letzten, welcher von links nach rechts gef¨ullt ist. Das erweist sich aber als keine gute Idee, weil diese Strukturen zu starr sind. Man kann sich einfach ¨uberlegen, dass es f¨ur jede Menge von Schl¨usseln nur einen solch perfekt balancierten Suchbaum gibt. In 2.7 sehen wir ein Beispiel daf¨ur, wie stark sich der Baum durch Einf¨ugen eines einzelnen Knotens ver¨andern muss. Stattdessen werden wir werden uns auf eine schw¨achere Form der Balancierung beschr¨anken. Ein bin¨arer Suchbaum T ist ein AVL-Baum 4, falls er f¨ur jeden Knoten v in T die AVL-Bedingung erf¨ullt: Der H¨ohenunterschied zwischen dem linken und dem rechten Teilbaum von v betr¨agt h¨ochstens 1. AVL-Bedingung Beispiel. Der Baum in Abbildung 2.2 ist ein AVL-Baum, denn f¨ur jeden Knoten ist der H¨ohenunterschied zwischen dem linken und dem rechten Teilbaum h¨ochstens 1. Der Baum in Abbildung 2.6 ist kein AVL-Baum, denn der H¨ohenunterschied im Wurzelknoten ist 6. Der folgende Baum ist kein AVL-Baum aufgrund von Knoten 7. 22 7 8 12 70 27 31 88 89 Wie wir sehen werden, ist diese Bedingung einerseits stark genug, um logarith- mische H¨ohe zu garantieren. Aber andererseits ist sie flexibel genug, um eine Ver- letzung der AVL-Bedindung durch Insert oder Delete mit wenigen Operationen 4Bennant nach den beiden sowjetischen Mathematikern Georgi Adelson-Velsky and Jewgeni Landis. 2.3 AVL-B¨aume 17 v hℓ(v) hr(v) Abb. 2.8 Definition von hℓ(v) und hr(v). zu beheben. Beim Design von effizienten Datenstrukturen ist es oft n¨utzlich, solche Kompromisse zwischen rigiden Strukturen und v¨olliger Beliebigkeit zu finden. Um die AVL-Bedingung effizient zu ¨uberpr¨ufen, erweitern wir unsere Daten- struktur und speichern f¨ur jeden Knoten v zwei Attribute hℓ(v) und hr(v), die im Wesentlichen die H¨ohe des linken und rechten Teilbaums angeben. Da der linke und rechte Teilbaum leer sein k¨onnte, verschieben wir die H¨ohe hier aber um 1, wir definieren also hℓ(v) = { 0, falls v kein linkes Kind hat, h + 1, falls v ein linkes Kind hat, dessen Teilbaum H¨ohe h hat, und analog f¨ur hr(v). Um zu zeigen, dass AVL-B¨aume eine effiziente Variante von Suchb¨aumen sind, m¨ussen wir zwei Dinge zeigen: • Ein AVL-Baum mit n Knoten hat H¨ohe h ≤ O(log n). • Wenn eine der Operationen Insert oder Delete die AVL-Bedingung verletzt, so k¨onnen wir den Baum in Zeit O(log n) so umbauen, dass die AVL-Bedingung wieder erf¨ullt ist. Wir widmen uns zun¨achst der ersten Frage. Dazu f¨uhren wir die Fibonacci-Folge ein, die den Lesern schon aus anderen Vorlesungen vertraut ist. Diese sind rekursiv definiert durch Fib(1) = Fib(2) = 1 Fib(n) = Fib(n − 1) + Fib(n − 2) f¨ur n ≥ 2. Die Fibonacci-Folge w¨achst exponentiell. Man kann zeigen, dass Fib(n) = Θ(ϕn) ist, wobei ϕ = (1 + √5)/2 der goldene Schnitt ist.5 Damit k¨onnen wir beweisen, dass ein AVL-Baum mit grosser H¨ohe sehr viele Knoten haben muss. Theorem 2.1. Ein AVL-Baum der H¨ohe h hat mindestens Fib(h+1)−1 viele Knoten. Beweis. Sei T (h) die minimale Anzahl von Knoten in einem AVL-Baum der H¨ohe h. Wir zeigen mit vollst¨andiger Induktion T (h) ≥ Fib(h+1)−1. 5Tats¨achlich gilt Fib(n) = ϕn−ψn √5 , wobei ψ = (1 − √5)/2. Da |ψ| ≈ 0.618 < 1, ist der zweite Summand asymptotisch vernachl¨assigbar. Diese Formel l¨asst sich direkt per vollst¨andiger Induk- tion beweisen, ergibt sich aber nat¨urlicherweise aus der Matrizenrechnung und der Theorie der Eigenwerte in der Linearen Algebra. 18 W¨orterb¨ucher Offenbar ist T (0) = 1 und T (1) = 2. F¨ur den Induktionsschritt betrachten wir einen beliebigen AVL-Baum T der H¨ohe h ≥ 2. Per Definition der H¨ohe hat die Wurzel mindestens ein Kind der H¨ohe h − 1. Wegen der AVL-Bedingung existiert auch das andere Kind der Wurzel und hat mindestens H¨ohe h − 2. Diese beiden Teilb¨aume m¨ussen daher mindestens T (h − 1) bzw. T (h − 2) Knoten haben. Zusammen mit der Wurzel hat T deshalb mindestens 1 + T (h − 1) + T (h − 2) Knoten. Folglich gilt T (h) ≥ T (h − 1) + T (h − 2) + 1 ≥ Fib(h) − 1 + Fib(h − 1) − 1 + 1 ▷ Ind.-Hypothese = Fib(h + 1) − 1. Damit ist der Induktionsschritt gezeigt. Umgedreht besagt Satz 2.1, dass die H¨ohe eines AVL-Baums logarithmisch ist. Korollar 2.2. Ein AVL-Baum mit n Knoten hat H¨ohe h ≤ O(log n). Beweis. Wir haben gezeigt, dass n ≥ Fib(h + 1) − 1 ist. Wegen Fib(k) = Θ(ϕk) gibt es eine Konstante c, sodass Fib(k) ≥ c · ϕk f¨ur alle n ≥ 1 ist. Zusammengenommen folgt n ≥ Fib(h + 1) − 1 ≥ c · ϕh+1 − 1, und aufgel¨ost nach h: h ≤ 1 log2 ϕ · logϕ((n + 1)/c) − 1 ≤ O(log n). 2.3.3 Einf¨ugen in AVL-B¨aume Wir haben bereits gesehen, dass das Einf¨ugen eines neuen Knotens in einen Such- baum sehr einfach ist. Das Problem ist, dass die AVL-Bedingung verletzt werden kann. Deshalb m¨ussen wir diese Bedingung nach jedem Einf¨ugen ¨uberpr¨ufen, und wenn sie verletzt ist, den Baum so umstrukturieren, dass die AVL-Bedingung wieder erf¨ullt ist. Zun¨achst ¨uberlegen wir uns, an welchen Stellen die AVL-Bedingung ¨uberhaupt verletzt sein kann. Sei p der Knoten, der durch das Einf¨ugen eines neuen Schl¨ussels x in den AVL-Baum T hinzugef¨ugt wurde. Offenbar haben Teilb¨aume, die x nicht enthalten, ihre H¨ohe nicht ver¨andert. Daher kann die AVL-Bedingung nur an den direkten Vorfahren von p verletzt sein. Unsere Strategie ist nun, die Vorfahren von p von unten nach oben zu durchlaufen. Falls an irgendeiner Stelle die AVL-Bedingung verletzt ist, reparieren wir sie. Bei dem Durchlauf k¨onnen wir gleichzeitig die Werte hℓ und hr anpassen, falls das n¨otig ist. Wir nehmen daher nun an, dass wir einen Vorfahren u betrachten, dass p in den linken Teilbaum Tℓ(u) von u eingef¨ugt wurde (der Fall des rechten Teilbaums ist analog) und gegebenenfalls in Tℓ(u) Reparaturmechanismen durchgef¨uhrt wurden. Sei v das linke Kind von u, also die Wurzel von Tℓ(u). Wie wir sehen werden, k¨onnen Einf¨ugen und Reparaturen Tℓ(u) nur auf zwei Arten ver¨andern: entweder bleibt die H¨ohe von Tℓ(u) dadurch unver¨andert, oder sie erh¨oht sich um genau 1. Wir unterscheiden nun mehrere F¨alle. 2.3 AVL-B¨aume 19 Fall 1: Die H¨ohe von Tℓ(u) hat sich nicht ver¨andert. In diesem Fall m¨ussen wir nichts tun. Wir m¨ussen nicht einmal die H¨ohe hℓ updaten, und k¨onnen auch darauf verzichten, weitere Vorfahren von p anzuschauen. Fall 2: Die H¨ohe von Tℓ(u) ist gewachsen, der Wert hℓ(u) hat sich also auf hℓ(u)+1 erh¨oht. F¨ur die Diskussion bezeichnen wir mit hℓ(u) weiterhin den alten Wert vor der Insert-Operation. Wir unterscheiden drei verschiedene Unterf¨alle. Fall 2A: hr(u) = hℓ(u) + 1. In diesem Fall m¨ussen wir zwar den Wert hℓ(u) an- passen, aber die AVL-Bedingung an u ist erf¨ullt. Die H¨ohe von u ¨andert sich nicht, deshalb ist es auch nicht n¨otig, weitere Vorfahren zu betrachten. (Das w¨urde auf Fall 1 f¨uhren.) Fall 2B: hr(u) = hℓ(u). Wir passen den Wert hℓ(u) an. Die AVL-Bedingung an u ist auch mit dem neuen Wert erf¨ullt, daher m¨ussen wir keine Reparatur durchf¨uhren. Allerdings erh¨oht sich die H¨ohe von u um 1, deshalb m¨ussen wir auch die restlichen Vorfahren betrachten. Fall 2C: hr(u) = hℓ(u) − 1. Die AVL-Bedingung an u ist mit dem neuen Wert verletzt, wir m¨ussen sie also reaprieren. Wir unterscheiden wiederum zwei F¨alle, je nachdem, ob der neue Knoten in den linken oder der rechten Teilbaum von v eingef¨ugt wurde. Fall 2Ci: Der linke Teilbaum Tℓ(v) von v ist gewachsen, nach der Aktualisierung gilt also hℓ(v) = hr(v) + 1. In diesem Fall f¨uhren wir eine Rotation durch: Wir Rotation setzen v an die Position, die zuvor u innehatte und machen u zum rechten Kind von v. Den Baum Tℓ(v) h¨angen wir auf der linken Seite an v, die B¨aume Tr(v) und Tr(u) h¨angen wir auf die linke bzw. rechte Seite von u. Zun¨achst ¨uberzeugen wir uns, dass die Suchbaum-Bedingung weiterhin erf¨ullt ist: • An v: Links von v h¨angt Tℓ(v), welcher nur Schl¨ussel enth¨alt, die kleiner sind als der von v. Rechts von v h¨angt u, das einen gr¨osseren Schl¨ussel als v hat. Die Suchbaum-Bedingung an v ist also erf¨ullt. • An u: Links von u h¨angt Tr(v). Da dieser Baum vorher Teil von Tℓ(u) war, sind alle Schl¨ussel dort kleiner als u. Rechts von u h¨angt Tr(v). Auf beiden Seiten ist die Suchbaum-Bedingung also weiter erf¨ullt. • Am Elternknoten z von v: Dort hing zuvor u. Aber da u und v im selben Teilbaum von z waren, sind ihre Schl¨ussel entweder beide kleiner als der von z (falls es der linke Teilbaum von z war), oder beide gr¨osser (falls es der rechte Telibaum war). In beiden F¨allen ist die Suchbaum-Bedingung weiterhint erf¨ullt. • Alle anderen Knoten behalten dieselben Kinder wie zuvor und erf¨ullen die Suchbaum-Bedingung deshalb weiterhin. Die Reparatur ergibt also einen g¨ultigen Suchbaum. Eine Inspektion der H¨ohen er- gibt, dass die AVL-Bedingung sowohl an u als auch an v erf¨ullt ist, und die neuen Werte hℓ und hr lassen f¨ur die Knoten u, v, w leicht aus den alten Werten ablesen. 20 W¨orterb¨ucher u v A B C −→ v A u B C Abb. 2.9 Rotation u v A w B C D −→ u w v A B C D −→ w v A B u C D Abb. 2.10 Doppelrotation Insbesondere entspricht die neue H¨ohe von v der alten H¨ohe von u. F¨ur den Elternk- noten z hat sich die H¨ohe seines Teilbaums also insgesamt nicht ge¨andert, und wir brauchen die weiteren Vorfahren nicht mehr zu pr¨ufen (Fall 1 f¨ur z). Fall 2Cii: Der rechte Teilbaum Tr(v) von v ist gewachsen, nach der Aktualisierung gilt also hℓ(v) = hr(v) − 1. In diesem Fall sei w das rechte Kind von v. Dann f¨uhren wir eine Doppelrotation 6 durch: Wir setzen w an die Position von u, machen v zumDoppelrotation linken Kind von w und u zum rechten Kind von v. Die beiden B¨aume Tℓ(v) und Tℓ(w) h¨angen wir auf der linken bzw. rechten Seite an v, die beiden B¨aume Tr(w) und Tr(u) h¨angen wir auf der linken bzw. rechten Seite an u. Analog zur einfachen Rotation ¨uberzeugt man sich, dass die Suchbaum-Bedingung in u, v und w weiterhin erf¨ullt ist. In allen anderen Knoten ist sie sowieso weiterhin erf¨ullt, als auch die AVL- Bedingung weiterhin erf¨ullt ist. Ausserdem entspricht die neue H¨ohe von w der alten H¨ohe von u, weshalb wir die Vorfahren wiederum nicht mehr anschauen m¨ussen. Delete Das L¨oschen eines Schl¨ussels aus einem AVL-Baum verl¨auft analog zum Einf¨ugen. Auch hier m¨ussen wir nach dem L¨oschen ¨uberpr¨ufen, ob die AVL-Bedingung verletzt ist, und sie gegebenenfalls reparieren. Zum Reparieren gen¨ugen dieselben Operation wie zuvor: Rotationen oder Doppelrotationen. Einen Unterschied gibt es: Bei Insert haben sowohl die Rotation als auch die Doppelrotation die Ursprungsh¨ohe des Teilbaumes wiederhergestellt. Deshalb konnte man beim Einf¨ugen nach einer Rotation oder Doppelrotation den Prozess beenden, ohne weitere Vorfahren zu betrachten. Beim L¨oschen stellt die jeweilige Rotation oder Doppelrotation die AVL-Bedingung im Teilbaum zwar ebenfalls wieder her, aber es kann passieren, dass sich die H¨ohe des Teilbaums insgesamt um 1 verkleinert. In diesem Fall muss man nach der Reparatur also auch noch die restlichen Vorfahren 6Sie heisst Doppelrotation, weil man sie auch beschreiben kann, indem man erst eine Rotation von v und w, und anschliessend eine Rotation von u und w durchf¨uhrt. 2.3 AVL-B¨aume 21 durchgehen und pr¨ufen, ob die AVL-Bedingung verletzt ist, und sie gegebenenfalls reparieren. Da die Delete-Operation insgesamt ¨ahnlich ist wie Insert, verzichten wir hier auf die Details. Laufzeit Sowohl Insert als auch Delete machen es erforderlich, pro Vorfahr maximal eine Rotation oder Doppelrotation auszuf¨uhren sowie einige der Werte hℓ und hr anzu- passen. Pro Vorfahr ist das in Zeit O(1) m¨oglich. Daher ergibt sich insgesamt eine Laufzeit von O(h) ≤ O(log n). Erweiterungen Mit AVL-B¨aumen kann man neben Search, Insert und Delete noch weitere Ope- rationen effizient in Zeit O(log n) ausf¨uhren. Wir k¨onnen zum Beispiel leicht den kleinsten Schl¨ussel im Suchbaum bestimmen, indem wir immer nach links gehen. Dies haben wir implizit bei der Delete-Operation schon ausgenutzt. Ebenso einfach k¨onnen wir den gr¨ossten Schl¨ussel bestimmen, immer wir immer nach rechts gehen. Damit sind AVL-B¨aume gleichzeitig effizient bei der Bestimmung von Maximum und Minimum, w¨ahrend ein Heap jeweils nur eine der beiden Aufgaben effizient unterst¨utzt. Man kann AVL-B¨aume leicht so erweitern, dass man sogar die Operation get(i) in Zeit O(log n) implementieren kann, die den i-gr¨ossten Schl¨ussel finden soll. Dazu speichern wir in jedem Knoten die Zahl der Knoten im linken bzw. rechten Teil- baum. Die Implementierung von get(i) ist dann tats¨achlich recht simpel und den Lesern als ¨Ubung ¨uberlassen. Man beachte aber, dass man diese Daten dann auch bei den Operationen Insert und Delete aktualisieren muss, insbesondere auch bei (Doppel-)Rotationen. Das ist nicht schwer, aber je mehr solcher Zusatzdaten man einbaut, desto komplexer wird die Datenstruktur. Zusammenfassung AVL-B¨aume sind balancierte Suchb¨aume, die eine logarithmische H¨ohe und damit ei- ne logarithmische Laufzeit f¨ur search, insert und delete garantieren. AVL-B¨aume sind flexibler als Heaps, allerdings ist die Implementierung von AVL-B¨aumen deut- lich aufwendiger.","libVersion":"0.3.2","langs":""}