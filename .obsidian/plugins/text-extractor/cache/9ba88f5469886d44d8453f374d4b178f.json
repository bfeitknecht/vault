{"path":"sem3/LinAlg/UE/s/LinAlg-u09-s.pdf","text":"D-INFK Linear Algebra HS 2023 Afonso Bandeira Bernd G¨artner Solution for Assignment 9 1. a) Consider the matrix M := AB. We claim that it has rank(M ) = n. To see this, observe that rank(B) = n implies C(B) = Rn because n is also the number of rows of B. Hence, we get C(M ) = C(A) (and therefore rank(M ) = rank(A) = n). Finally, we can use Proposition 4.5.9 to get (AB)† = M † = B†A†. b) Assume first that A has full column rank n = rank(A). In this case, we have A† = (A⊤A)−1A⊤ by definition of the pseudoinverse for matrices with full column rank. Moreover, notice that A⊤ has full row rank and hence we also get (A⊤)† = A(A⊤A)−1 by definition of the pseudoinverse for matrices with full row rank. Hence, we get (A †)⊤ = ((A ⊤A) −1A⊤) ⊤ = A((A⊤A) −1)⊤ = A((A⊤A) ⊤) −1 = A(A ⊤A) −1 = (A⊤)†. We conclude that the statements holds for all matrices with full column rank. Analogously, we can prove that the statement holds if A has full row rank m = rank(A). In that case, we have A† = A⊤(AA⊤)−1 and (A⊤)† = (AA⊤)−1A. Hence, we indeed get (A†)⊤ = (A⊤(AA⊤)−1)⊤ = ((AA ⊤) −1)⊤A = ((AA⊤) ⊤) −1A = (AA ⊤) −1A = (A⊤)†. We conclude that the statement holds for all matrices with full row rank. It remains to prove the general case, i.e. we do not assume anymore that A has full row rank or full column rank. Then by definition, we have A† = R†C† where A = CR is a CR decomposition of A. In particular, we have C ∈ Rm×r and R ∈ Rr×n where r = rank(A). Now observe that we also have A⊤ = R⊤C⊤ with R⊤ ∈ Rn×r and C⊤ ∈ Rr×m and of course, r = rank(A) = rank(A⊤). Hence, we can use Proposition 4.5.9 to get (A⊤)† = (C⊤)†(R⊤)†. We conclude that (A⊤) † = (C⊤) †(R⊤)† = (C†) ⊤(R†)⊤ = (R†C†) ⊤ = (A †)⊤ by using that C has full column rank and R has full row rank and hence (C⊤)† = (C†)⊤ and (R⊤)† = (R†)⊤. c) Let A = CR be a CR decomposition of A with C ∈ Rm×r and R ∈ Rr×n where r = rank(A). We can rewrite AA† = CR(CR) † = CRR†C† Prop. 4.5.4 = CIC† = C(C⊤C) −1C⊤ and hence we conclude symmetry of AA† since (AA†) ⊤ = (C(C⊤C)−1C⊤)⊤ = C((C⊤C)−1)⊤C⊤ = C((C⊤C) ⊤) −1C⊤ = C(C⊤C)−1C⊤ = AA†. It remains to prove that AA† is the projection matrix that projects vectors onto C(A). For this, observe that we have C(A) = C(C) (by the properties of CR-decomposition) and that we have already shown AA† = C(C⊤C)−1C⊤. By Theorem 4.2.6, this is exactly the projection matrix that projects vectors onto C(C) = C(A). d) We proceed as in the preceding subtask. Let A = CR be a CR decomposition of A with C ∈ Rm×r and R ∈ Rr×n where r = rank(A). We can rewrite A†A = (CR) †CR = R†C†CR Prop. 4.5.2 = R†IR = R⊤(RR⊤) −1R and hence we conclude symmetry of AA† since (A†A) ⊤ = (R⊤(RR⊤) −1R)⊤ = R⊤((RR⊤)−1)⊤R = R⊤((RR⊤) ⊤) −1R = R⊤(RR⊤) −1R = A†A. By Theorem 4.2.6, the matrix R⊤(RR⊤)−1R = A†A is exactly the projection matrix onto the sub- space C(R⊤) = R(R) = R(A) = C(A⊤) (the equality R(R) = R(A) is due to the observation that R can be obtained from A through row operations and deleting 0-rows, and by recalling that row operations preserve the row space). 2. We solve this by using our knowledge on pseudoinverses. Consider the function f −1 : C(A) → C(A⊤) given by f −1(x) = A†x for all x ∈ C(A). Observe that the composition f −1 ◦ f is the identity: we know from Exercise 1 that A†A is the projection matrix that projects vectors onto the subspace C(A⊤), and hence we have f −1(f (x)) = A †Ax = x for all x ∈ C(A⊤). This already implies that f is injective. Observe that with an analogous argument we get f (f −1(x)) = AA†x = x for all x ∈ C(A). Hence, f −1 is injective as well which implies that both f and f −1 are bijective. Note that the matrix A†A is in general not the identity matrix. It is crucial that the function f is only defined on C(A⊤) and not on all of Rn. 3. a) The linear transformation given by A = I − 2vv⊤ corresponds to reflection along the hyperplane H = {x ∈ R2 : x · v = 0}. To see this, recall that the projection matrix for projection onto Span(v) is given by B = vv⊤ ||v||2 . Since v is a unit vector, this simplifies to B = vv⊤. In particular, this means that A = I − 2B applied to some vector x ∈ R2 will subtract the projection of x onto Span(v) twice from x itself. In other words, assume we split x into two parts x = x| + x⊥ with x| = Bx and x⊥ · v = 0. Then the transformation given by matrix A maps x to the vector Ax = (I−2B)(x|+x⊥) = x|+x⊥−2Bx|−2Bx⊥ = x|+x⊥−2x|−2vv⊤x⊥ = x|−x⊥−0 = x|−x⊥. A picture of this is provided in Figure 1. vxAxx⊥x|Hx⊥−x| Figure 1: A sketch of the situation in subtask a). b) Recall from the lecture (notes) that the 2 × 2 matrix A′ = [ 1√2 − 1√2 1√2 1√2 ] is a rotation matrix. In particular, it rotates vectors in R2 by π 4 in counter-clockwise direction. Observe that the matrix A contains A′ as a submatrix, i.e. we can obtain A′ by removing the second column and second row from A. Now consider an arbitrary vector v ∈ R3 that is spanned by e1 and e3, i.e. v = v1e1 + v3e3. Then applying A to v has no effect on the second coordinate, but it will rotate that v in the plane spanned by e1 and e3. A better way to say this, is that applying A to v corresponds to a rotation around the axis e2 (the vecor orthogonal to the plane spanned by e1 and e3). Finally, observe that this is still true even if we don’t have v2 = 0. Since the second column of A is simply e2, the second coordinate of v remains unchanged when applying A to it. In conclusion, the linear transformation given by matrix A is a rotation by π 4 around the axis spanned by e2. c) Note that we did not specify a direction for the rotation in the exercise. The exercise is still well defined because we want to rotate by π and hence the direction does not matter. We start by thinking about what such a transformation would do to the standard unit vectors e1, e2, e3. The vector e1 should be mapped to e2, the vector e2 should be mapped to e1, and the vector e3 should be mapped to −e3. In particular, we want A   | | | e1 e2 e3 | | |   ! =   | | | e2 e1 −e3 | | |   . From this, we conclude that A has to be the matrix A =   | | | e2 e1 −e3 | | |   . 4. a) Let x ∈ T be arbitrary. By definition, there exist c1, c2, c3 ∈ R + 0 with c1 + c2 + c3 = 1 such that x = c1v1 + c2v2 + c3v3. We compute Ax = A(c1v1 + c2v2 + c3v3) = c1Av1 + c2Av2 + c3Av3 = c1v′ 1 + c2v′ 2 + c3v′ 3 where v′ 1, v′ 2, v′ 3 are the vertices of T ′ given in the exercise. We conclude that indeed we have Ax ∈ T ′. b) Since T ′ is not a single point, two of its vertices must be distinct. Without loss of generality, assume it is v′ 1 and v′ 2, i.e. v′ 1 ̸= v′ 2. We also know that T ′ is not a triangle. Hence, by definition of a triangle, T is either a line segment or the vertices v′ 1, v′ 2, v′ 3 are not distinct. If the former is the case, we are done. Thus, assume now the latter and without loss of generality assume v′ 3 = v′ 2. But then T ′ can be described by just using v′ 1 and v′ 2 as T ′ = {c1v′ 1 + c2v′ 2 + c3v′ 3 : c1, c2, c3 ∈ R + 0 , c1 + c2 + c3 = 1} = {c1v′ 1 + (c2 + c3)v′ 2 : c1, c2, c3 ∈ R+ 0 , c1 + (c2 + c3) = 1} = {c1v′ 1 + c23v′ 2 : c1, c23 ∈ R+ 0 , c1 + c23 = 1}. Notice that we still have v′ 1 ̸= v′ 2 and hence T ′ is a line segment. c) We prove both directions individually. “ =⇒ ” Assume that A has rank 0 and hence A = 0. Then v′ 1 = v′ 2 = v′ 3 = 0 and hence T ′ = {0} is a single point. “ ⇐= ” Assume now T ′ is a single point. Then we must have v′ 1 = v′ 2 = v′ 3. Consider the vectors w1 = v1 − v3 and w2 = v2 − v3. Since T is a triangle, these two vectors must be linearly independent (otherwise they would be collinear and we get that T is actually a line segment, and not a triangle). We have Aw1 = Av1 − Av3 = 0 and also Aw2 = Av2 − Av3 = 0. Hence, the nullspace of A has dimension two and consequently the rank of A (dimension of its column space) must be zero. d) We prove both directions individually. “ =⇒ ” Assume that A has rank 2. Consider again the two linearly independent vectors w1 = v1 − v3 and w2 = v2 − v3. Since A has full rank, the vectors Aw1 = v′ 1 − v′ 3 and Aw2 = v′ 2 − v′ 3 must be linearly independent as well. This directly implies v′ 1 ̸= v′ 2 ̸= v′ 3 ̸= v′ 1 and it remains to argue that T ′ is not a line segment. “ ⇐= ” Assume now T ′ is a triangle. Consider the vectors w′ 1 = v′ 1 − v′ 3 and w′ 2 = v′ 2 − v′ 3. Since T ′ is a triangle, these two vectors must be linearly independent (otherwise they would be collinear and we would get that T ′ is actually a line segment, and not a triangle). We have A(v1 −v3) = Av1 −Av3 = w′ 1 and also A(v2 −v3) = Av2 −Av3 = w′ 2. Hence, the column space of A has dimension two and consequently the rank of A (dimension of its column space) is two. e) We prove both directions individually. “ =⇒ ” Assume that A has rank 1. By subtasks c) and d) we get that T ′ cannot be a single point and it also cannot be a triangle. Hence, T ′ must be a line segment by subtask b). “ ⇐= ” Assume that T ′ is a line segment. By subtasks c) and d) we know that A cannot have rank 0 and it also cannot have rank 2. We conclude that is must have rank 1. 5. Let us denote the four given points by p1, p2, p3, p4, respectively. We want to find r ∈ R+ such that the sum 4∑ i=1(r − ||pi||) 2 is minimized. The key observation of this exercise is that this is the least squares objective of the linear system     1 1 1 1     [ r] =     ||p1|| ||p2|| ||p3|| ||p4||     =       2√2√ 20 9√ 10 4 .       Using the normal equations to solve this we get 4r = [ 1 1 1 1 ]     1 1 1 1     [r] = [1 1 1 1 ]     ||p1|| ||p2|| ||p3|| ||p4||     = 4∑ i=1 ||pi|| and hence r = 1 4 4∑ i=1 ||pi|| = 1 4 (2 + √2 + √ 20 9 + √ 10 4 ).","libVersion":"0.3.2","langs":""}