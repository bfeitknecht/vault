{"path":"sem3/LinAlg/VRL/extra/plans/LinAlg-plan-w03.pdf","text":"Lecture plan Linear Algebra (401-0131-00L, HS24), ETH Z ¨urich Numbering of Sections, Definitions, Figures, etc. as in the Lecture Notes Week 3 Matrices and linear transformations (Section 2.3) Matrix as a “transformation:” input matrix output x ∈ Rn A ∈ R m×n Ax ∈ Rm Definition 2.25: Let A be an m × n matrix. TA : R n → R m is the function defined by TA( x︸︷︷︸ ∈Rn ) = Ax︸︷︷︸ ∈Rm . Example: A = [0 1 1 0 ] TA ([ x1 x2 ]) = [ 0 1 1 0 ] [ x1 x2 ] = [x2 x1 ] (”swap coordinates”) Observation 2.26: Let A be an m × n matrix, x, y ∈ R n and λ ∈ R. Then (i) TA(x + y) = TA(x) + TA(y) and (ii) TA(λx) = λTA(x). Combining (i) and (ii): TA(λx + µy) = λTA(x) + µTA(y). Proof. This just says (i) A(x + y) = Ax + Ay and (ii) A(λx) = λAx; both are true by the rules of vector addition, scalar multiplication, matrix multiplication. Transforming a set X of inputs: TA(X) := {TA(x) : x ∈ X} Examples R 2 → R 2: TA(e1) = Ae1, TA(e2) = Ae2, the two columns of A TA(X)TA(e1)TA(e2) A = [1 0 0 3 4 ] ←− Xe1e2 A = [0 1 1 0 ] −→ TA(X)TA(e1)TA(e2) stretching inputs mirroring TA(X)TA(e1)TA(e2) A = [ 1√2 − 1√2 1√2 1√2 ] ←− Xe1e2 A = [1 − 1 2 0 1 ] −→ TA(X)TA(e1)TA(e2) rotation inputs shear 1 Example R 3 → R 2: orthogonal projection   0 0 0     1 0 0     0 0 1     1 1 1     1 1 0     0 1 1     0 1 0     1 0 1   A = [2 −1 −1 0 2 −1 ] −→ [ 0 0 ][ 2 0 ][ −1 2 ][ 1 2 ][ −1 −1 ][ −2 1 ][ 0 1 ][ 1 −1 ] inputs projection Linear transformations: Definition 2.27: A function T : R n → R m is called a linear transformation if for all x, y ∈ R n and all λ ∈ R, (i) T (x + y) = T (x) + T (y) and (ii) T (λx) = λT (x). Combining (i) and (ii): T (λx + µy) = λT (x) + µT (y). All TA’s are linear transformations (Observation 2.26). (i) and (ii): axioms of linear transformations T x, y −→ T (x), T (y) +  ↓  ↓ + x + y −→ T (x + y) = T (x) + T (y) T T x −→ T (x) ·λ  ↓  ↓ ·λ λx −→ T (λx) = λT (x) T commutative diagrams: T “commutes” with + and · Examples: T = TA for . . . • T (x) = ∑n i=1 xi A = [ 1 1 · · · 1 ] • T (x) = 0 A = 0 • T (x) = x A = I Counterexamples: violation of . . . • T (x) = ∑n i=1 |xi| (ii): if x ̸= 0 and λ < 0, then T (λx) > 0 but λT (x) < 0 • T (x) = u with fixed u ̸= 0 (i): T (x + y) = u but T (x) + T (y) = 2u 2 Lemma 2.28: Let T : R n → Rm be a linear transformation, let x1, x2, . . . , xℓ ∈ R n and λ1, λ2, . . . , λℓ ∈ R. Then T ( ℓ∑ j=1 λjxj ) = ℓ∑ j=1 λjT (xj). ℓ = 0: T (0) = 0, true by (ii) ℓ = 1: (ii) ℓ = 2: combination of (i) and (ii) ℓ > 2? Proof. T ( ℓ∑ j=1 λjxj ) = T ( ℓ−1∑ j=1 λjxj + λℓxℓ ) (i) = T ( ℓ−1∑ j=1 λjxj ) +T (λℓxℓ) (ii) = T ( ℓ−1∑ j=1 λjxj ) +λℓT (xℓ) . Same for ℓ − 1: T ( ℓ∑ j=1 λjxj ) = T ( ℓ−2∑ j=1 λjxj ) + λℓ−1T (xℓ−1) ︸ ︷︷ ︸ T ( ∑ℓ−1 j=1 λj xj ) +λℓT (xℓ) . Repeating for ℓ − 2, . . . , 1: T ( ℓ∑ j=1 λjxj ) = T ( 0∑ j=1 λjxj ) ︸ ︷︷ ︸ T (0)=0 +λ1T (x1) + . . . + λℓ−1T (xℓ−1) + λℓT (xℓ) = ℓ∑ j=1 λjT (xj). Proof by induction (without “repeating”): For ℓ = 0, prove it directly (base case): statement reads as T (0) = 0 which is true. For ℓ > 0, prove the induction step: if the statement is true for ℓ − 1 (induction hypothesis), then it is also true for ℓ. Having done this, we know that the statement is true for all ℓ. Induction step: T ( ℓ∑ j=1 λjxj ) = T ( ℓ−1∑ j=1 λjxj + λℓxℓ ) (i) = T ( ℓ−1∑ j=1 λjxj ) + T (λℓxℓ) (ii) = T ( ℓ−1∑ j=1 λjxj ) + λℓT (xℓ) (as before) = ℓ−1∑ j=1 λjT (xj) + λℓT (xℓ) (induction hypothesis) = ℓ∑ j=1 λjT (xj). 3 The matrix of a linear transformation: Theorem 2.29: Let T : R n → R m be a linear transformation. There exists a unique m × n matrix A such that T = TA. Proof. For T = TA, we need T (ej) = TA(ej) = Aej (j-th column of A), for all j ∈ [n]. Only candidate is A =   | | | T (e1) T (e2) · · · T (en) | | |   . This works: TA(x) = Ax (Definition 2.25, TA) = n∑ j=1 xjT (ej) (Definition 2.4, matrix-vector multiplication) = T ( n∑ j=1 xjej ) (Lemma 2.28) = T (x). Consequence: If we know T (e1), T (e2), . . . , T (en), we know T (x) for all x ∈ R n. Linear transformations and matrix multiplication: Lemma 2.30: Let TA : Rn → R a and TB : Rb → R n be two linear transformations. Then TA(TB(x)) = TAB(x). Proof. TA(TB(x)) = TA(Bx) = A(Bx) = (AB)x = TAB(x). Can be used to prove generalized associativity, for example (AB)(CD) = A((BC)D): T(AB)(CD)(x) = TAB(TCD(x)) = TAB(TC(TD(x))) = TA(TB(TC(TD(x)))) TA((BC)D)(x) = TA(T(BC)D(x)) = TA(TBC(TD(x))) = TA(TB(TC(TD(x))) Same functions ⇒ same matrices (Theorem 2.29). Systems of linear equations (Section 3.1) D = 2S D = C + 3 D + S + C = 17 children’s age puzzle (Section 0.3) x1 − 2x2 = 0 x1 − x3 = 3 x1 + x2 + x3 = 17 standard form (x1 = D, x2 = S, x3 = C) 4 Definition 3.1: A system of linear equations in m equations and n variables x1, x2, . . . , xn is of the form a11x1 + a12x2 + · · · + a1nxn = b1 a21x1 + a22x2 + · · · + a2nxn = b2 ... am1x1 + am2x2 + · · · + amnxn = bm, The aij and bi: known real numbers The xi: unknown real numbers (to be computed) Matrix-vector form: Ax = b :      a11 a12 · · · a1n a21 a22 · · · a2n ... ... . . . ... am1 am2 · · · amn      ︸ ︷︷ ︸ A, m×n      x1 x2 ... xn      ︸ ︷︷ ︸ x∈Rn =      b1 b2 ... bm      ︸ ︷︷ ︸ b∈Rm . A: coefficient matrix x: vector of variables b: right-hand side Solving the system: compute x ∈ R n such that Ax = b. Children’s age puzzle:   1 −2 0 1 0 −1 1 1 1   ︸ ︷︷ ︸ A   x1 x2 x3   ︸ ︷︷ ︸ x =   0 3 17   ︸ ︷︷ ︸ b . Observation 3.2: Let A be an m × n matrix. The columns of A are linearly independent if and only if the system Ax = 0 has a unique solution, x = 0. Proof. Unique solution ⇔ 0 can only be written as a trivial linear combination of the colums ⇔ columns are linearly independent (Lemma 1.19). The PageRank algorithm: works on link graph (circles: web pages, arrows: links) 213456 Which page is most relevant? Old school measure: number of citations (links to the page): page 2 (4 citations) wins. 5 PageRank principles: • Citation from a relevant page counts more. • Citation from a page that cites many pages counts less. → relevance: sum of relevances of citing pages, divided by number of pages cited: x2 = x1 2 + x4 + x5 + x6 4 (same for the other 5 pages) System of 6 linear equations in 6 variables! But with useless solution 0. Fix: use damping factor d close to 1 (for example, d = 7/8): x2 = (1 − d) + d ( x1 2 + x4 + x5 + x6 4 ) Unique solution (rounded): page 3 (rank 1.7307) wins. x1 = 0.31797, x2 = 1.6761, x3 = 1.7307, x4 = 0.31797, x5 = 1.0751, x6 = 0.88217. Computer vectors and matrices: how to store a system of linear equations? b ∈ R m: array b with entries b[0], b[1], . . . , b[m − 1]. b =      b[0] b[1] ... b[m − 1]      (computer vector) Array indices start from 0, not from 1! A ∈ R m×n: array A with m entries A[0], A[1], . . . , A[m − 1]. Each A[i]: array with n entries. A =     |A[0]||A[1]|...|A[m − 1]|      (computer matrix in row notation) A[i] = [ A[i][0] A[i][1] · · · A[i][n − 1]] (computer row vector) A[i][j] is the entry of A in row i and column j (both counting from 0). 6","libVersion":"0.3.2","langs":""}