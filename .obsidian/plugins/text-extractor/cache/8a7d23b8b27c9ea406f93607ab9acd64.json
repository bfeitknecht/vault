{"path":"sem3/A&D/VRL/extra/script/A&D-script-search-sort.pdf","text":"Skript zur Vorlesung Algorithmen und Datenstrukturen Suchen und Sortieren Herbstsemester 2024 Stand: 9. Oktober 2024 Johannes Lengler David Steurer Inhaltsverzeichnis 1 Suchen 1 1.1 Unsortierte Arrays . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 1.2 Sortierte Arrays: Bin¨are Suche . . . . . . . . . . . . . . . . . . . . . 2 1.2.1 Untere Schranke . . . . . . . . . . . . . . . . . . . . . . . . . 3 1.2.2 Einsortierung . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 2 Sortieren 7 2.1 Bubble-Sort . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7 2.2 Selection-Sort . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9 2.3 Insertion-Sort . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10 2.4 Mergesort . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11 2.5 Quicksort . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14 2.5.1 Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14 2.5.2 Grundidee . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 2.5.3 Der Algorithmus . . . . . . . . . . . . . . . . . . . . . . . . . 15 2.5.4 Laufzeit von Quicksort . . . . . . . . . . . . . . . . . . . . . . 17 2.6 Heapsort . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17 2.6.1 Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17 2.6.2 Max-Heap . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 2.6.3 Heapsort . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19 2.6.4 Maximum aus Heap l¨oschen (ExtractMax procedure) . . . . . 20 2.6.5 Daten in Heap umwandeln . . . . . . . . . . . . . . . . . . . 20 2.6.6 Laufzeitanalyse von Heapsort . . . . . . . . . . . . . . . . . . 22 2.6.7 Darstellung eines Bin¨arbaums im Speicher . . . . . . . . . . . 23 2.6.8 Vorteile und Nachteile von Heapsort . . . . . . . . . . . . . . 23 2.7 Untere Schranke f¨ur vergleichsbasiertes Sortieren . . . . . . . . . . . 24 2.7.1 Beweis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 2.7.2 Nicht vergleichsbasiertes Sortieren: Bucketsort . . . . . . . . 25 i ii Kapitel 1 Suchen Dieses Kapitel besch¨aftigt sich mit zwei fundamentalen Aufgaben der Informatik: Suchen und Sortieren. Viele der Berechnungen, die in Rechenzentren durchgef¨uhrt werden, besch¨aftigen sich mit diesen Problemen – dementsprechend wichtig ist es, effiziente Algorithmen zu finden. Beispiel: Finde die Pr¨ufung von Maria M¨uller in einem Stapel aus 600 Pr¨ufungen. Formal betrachten wir das folgende Problem: Definition 1.1 (Suche). Gegeben seien ein Array A mit n Eintr¨agen (bei uns: Zahlen) und ein Element b. Gesucht ist ein Index k mit A[k] = b, oder die R¨uckgabe “nicht gefunden”, falls b nicht in A enthalten ist. Die Annahme, dass das Array mit Zahlen gef¨ullt ist, ist keine Einschr¨ankung. Im Computer ist ja jedes Objekt durch eine Folge von Nullen und Einsen gegeben. Diese kann man immer auch als Zahl interpretieren. Wir betrachten zwei F¨alle: • Fall 1: A ist unsortiert • Fall 2: A ist sortiert, d.h., A[1] ≤ A[2] ≤ · · · ≤ A[n]. 1.1 Unsortierte Arrays In diesem Fall machen wir keinerlei Annahmen ¨uber das Array A. Der einfachste Algorithmus zur L¨osung des Suchproblems in diesem Fall ist die lineare Suche: Linear-Search(A[1..n], b) Lineare Suche 1 for i ← 1, 2, . . . , n do 2 if A[i] = b then return i 3 return “nicht gefunden” Die Laufzeit der linearen Suche betr¨agt im schlechtesten Fall (worst case) offen- Laufzeit sichtlich Θ(n), denn die Schleife wird maximal n Mal durchlaufen. Tats¨achlich ist die lineare Suche optimal, d.h., im schlechtesten Fall muss jeder Suchalgorithmus n viele Vergleiche durchf¨uhren. Um dies zu begr¨unden, stellen wir uns vor, dass b nicht in A enthalten ist. Nachdem der Algorithmus n−1 viele Elemente von A gelesen hat, kann er noch nicht entscheiden, ob b in A enthalten ist, oder nicht (denn b k¨onnte ja im letzten, noch nicht gelesenen Element von A gespeichert sein). 1 2 Suchen 1.2 Sortierte Arrays: Bin¨are Suche Wir nehmen nun an, dass A sortiert ist, d.h., A[1] ≤ A[2] ≤ · · · ≤ A[n]. Die lineare Suche funktioniert nat¨urlich auch in diesem Fall, doch wir k¨onnen die zus¨atzliche Information, dass A sortiert ist, ausnutzen, um deutlich schneller zu sein. Ist A sortiert, dann k¨onnen wir das Suchproblem mittels Divide-and-Conquer l¨osen. Dazu betrachten wir die mittlere Position m = ⌊(n + 1)/2⌋ im Array und unterscheiden drei F¨alle: 1 2 3 · · · m − 1 m m + 1 · · · n − 2 n − 1 n ≤ n 2 -Elemente ≤ n 2 -Elemente 1) Ist b = A[m], dann haben wir den Schl¨ussel b an Position m gefunden und geben daher m zur¨uck. 2) Ist b < A[m], dann suchen wir rekursiv in der linken H¨alfte (also auf den Positionen 1, . . . , m − 1) weiter nach b. 3) Ist b > A[m], dann suchen wir rekursiv in der rechten H¨alfte (also auf den Positionen m + 1, . . . , n) weiter nach b. Dieses Verfahren wird bin¨are Suche genannt. Die Korrektheit der bin¨aren SucheKorrektheit folgt direkt aus der Tatsache, dass A aufsteigend sortiert ist. Ist also b < A[m], dann wissen wir bereits, dass die Positionen m, . . . , n nur Schl¨ussel enthalten, die gr¨osser als b sind. Also reicht es, die Suche auf die ersten m − 1 Positionen einzuschr¨anken. Die Begr¨undung f¨ur den Fall b > A[m] verl¨auft analog. Da der Suchbereich (die Anzahl der noch m¨oglichen Positionen) in jedem Schritt um mindestens ein Element kleiner wird, endet das Verfahren auch nach einer endlichen Zahl von Schritten. Bin¨are Suche kann wie alle rekursiven Algorithmen entweder rekursiv oder ite- rativ implementiert werden. Wir geben hier einen Pseudocode mit der Invariante1, dass A[ℓ..r] der verbleibende Suchbereich ist. Binary-Search(A[1..n], b)Bin¨are Suche 1 ℓ ← 1; r ← n ▷ Initialer Suchbereich 2 while ℓ ≤ r do 3 m ← ⌊(ℓ + r)/2⌋ 4 if A[m] = b then return m ▷ Element gefunden 5 else if A[m] > b then r ← m − 1 ▷ Suche links weiter 6 else ℓ ← m + 1 ▷ Suche rechts weiter 7 return “Nicht vorhanden” Wie viele Schritte T (n) sind dies im schlimmsten Fall? Dazu beobachten wir, dassLaufzeit die Zahl der verbleibenden Positionen sowohl in Fall 2) als auch in Fall 3) h¨ochstens n/2 ist. Da die Laufzeit f¨ur l¨angere Arrays h¨ochstens l¨anger sein kann2, m¨ussen wir 1Das Konzept der Invariante behandeln wir sp¨ater noch ausf¨uhrlicher. 2Diese Eigenschaft kann man formal mit Induktion beweisen. Das werden wir hier aber nicht tun. 1.2 Sortierte Arrays: Bin¨are Suche 3 nach Absteigen in die Rekursion noch h¨ochstens Zeit T (n/2) aufbringen. F¨ur ein Array der L¨ange n = 2k ergibt sich damit die folgende Rekurrenz zur Absch¨atzung der maximalen Zahl T (n) an durchgef¨uhrten Operationen: T (n) ≤ { c falls n = 1 ist, T (n/2) + d falls n ≥ 2 ist, (1) wobei c und d jeweils konstant sind. Wir zeigen nun per vollst¨andiger Induktion, dass T (n) ≤ d log2(n) + c f¨ur alle n = 2k mit k ≥ 0 ist. Anfang: F¨ur k = 0 folgt die Behauptung direkt aus (1). k → k + 1: F¨ur n = 2k+1 sch¨atzen wir ab: T (n) ≤ T (n/2) + d (1) = T (2 k) + d ≤ d log2(2 k) + c + d Induktionshypothese f¨ur n = 2 k = d · (k + 1) + c = d log2(n) + c. Damit ist der Induktionsschritt gezeigt. Damit folgt, dass T (n) ≤ O(log n), was sehr schnell ist. Wie im unsortierten Fall stellt sich auch hier die Frage, ob es besser geht. Die Antwort ist wieder nein – auch wenn der Beweis etwas aufw¨andiger ist. 1.2.1 Untere Schranke Um zu beweisen, dass die Suche in sortierten Arrays im schlimmsten Fall immer Untere SchrankeΩ(log n) viele Vergleiche ausf¨uhrt, betrachten wir wieder einen beliebigen vergleichs- basierten Suchalgorithmus. Wir nehmen an, dass der Algorithmus die Suche durch Vergleiche ausf¨uhrt, d.h., Werte miteinander vergleicht und aufgrund des Resultats (Ja/Nein) Entscheidungen trifft. Einen solchen Algorithmus k¨onnen wir als Entscheidungsbaum darstellen. Jeder innere Knoten in diesem Baum entspricht einem Vergleich, und die beiden Kindkno- ten entsprechen der Fortsetzung, die der Algorithmus je nach Ausgang des Vergleichs f¨allt (Ja/Nein). Da jeder Knoten h¨ochstens zwei Kinder hat, sprechen wir von einem bin¨aren Baum. Die Knoten in der untersten Ebene, auch Bl¨atter genannt, entspre- chen den R¨uckgabewerten des Algorithmus. Ist eine Suche nach einem Element b erfolgreich, dann kann b an jeder der n Positionen des Arrays stehen. F¨ur jede die- ser n m¨oglichen Ergebnisse der Suche muss der Entscheidungsbaum mindestens ein Blatt enthalten. Zus¨atzlich muss es einen Knoten f¨ur ” nicht gefunden“ geben, also muss die Gesamtanzahl der Bl¨atter mindestens n + 1 betragen. Die Anzahl Knoten ist somit auch mindestens n + 1, denn jedes Blatt ist ja insbesondere ein Knoten.3 Die Anzahl von Vergleichen, die ein Algorithmus im schlechtesten Fall ausf¨uhrt, entspricht exakt der H¨ohe des Baums.4 Diese ist definiert als die maximale Anzahl 3Das ist nat¨urlich sehr grob abgesch¨atzt. Als ¨Ubung k¨onnen Sie auch versuchen, sich direkt zu ¨uberlegen, wie viele Bl¨atter ein Baum der H¨ohe h haben kann. Das f¨uhrt zu der minimal besseren unteren Schranke h ≥ log2(n + 1). 4Statt der H¨ohe spricht man manchmal auch von der Tiefe des Baumes oder eines Knotens. Die Wurzel ist also in Tiefe 0, das tiefste Blatt in Tiefe h. 4 Suchen b = A[3]? 3 b < A[3]? b = A[1]? 1 2 b = A[5]? 5 b < A[5]? 4 6 ✓ ✗ ✓ ✓ ✗ ✗ ✓ ✗ ✓ ✗ Abb. 1.1 Bin¨are Suche auf einem Array der L¨ange 6 als Entscheidungsbaum visualisiert. Da sich das zu suchende Element im Erfolgsfall auf sechs m¨oglichen Positionen befinden kann, hat der Entscheidungsbaum die gleiche Anzahl von Knoten. Im ersten Schritt wird A[3] mit b verglichen. Bei Gleichheit ist der Algorithmus fertig, ansonsten werden A[1] bzw. A[5] verglichen, usw. Da der Baum H¨ohe 4 hat (also aus f¨unf Leveln inklusive Bl¨attern besteht), ist die bin¨are Suche auf einem Array mit sechs Elementen nach sp¨atestens vier Vergleichen fertig. von Kanten auf einem Weg von der Wurzel (dem “obersten” Knoten) zu einem Blatt (einem Knoten ohne Nachfolger). Wir beobachten, dass ein bin¨arer Baum der H¨ohe h h¨ochstens 2 0 + 21 + 22 + · · · + 2h = 2 h+1 − 1 < 2h+1 (2) viele Knoten hat. Daraus folgt n + 1 ≤ Anzahl Knoten im Entscheidungsbaum < 2h+1, (3) was aufgel¨ost h > log2(n+1)−1 = Ω(log n) ergibt. Da die Anzahl der im schlimmsten Fall ausgef¨uhrten Vergleiche h betr¨agt und unsere Argumentation f¨ur beliebige Ent- scheidungsb¨aume gilt, haben wir also bewiesen, dass jedes vergleichsbasierte Such- verfahren auf einem sortierten Array im schlechtesten Fall Ω(log n) viele Vergleiche durchf¨uhrt. Ein Argument von dieser Form nennt man auch informationstheoretisches Argu- ment. Es formalisiert die Intuition, dass ein vergleichsbasierter Algorithmus durch h Vergleiche nur “h Bits an Information” sammeln kann und dadurch nur h¨ochstens 2h viele Zust¨ande voneinander unterscheiden kann. Eine Verallgemeinerung dieses Prinzips f¨uhrt auf das Konzept der Entropie, das in der Informatik an vielen ver- schiedenen Stellen auftritt. 1.2.2 Einsortierung Oft will man mit bin¨arer Suche nicht nur herausfinden, ob b in dem Array vorkommt.Einsortierung Manchmal will man (auch) wissen, an welche Position wir b im Array einf¨ugen m¨uss- ten, damit das Array dabei sortiert bleibt.5 Das kann man durch leichte Modifikation von bin¨arer Suche ebenfalls erreichen. Falls b im Array vorkommt, geben wir ja so- wieso die Position von b zur¨uck. Falls b nicht im Array vorkommt, so kann man sich 5Wenn das Element b mehrfach im Array auftritt, gibt es mehrere m¨ogliche Antworten. 1.2 Sortierte Arrays: Bin¨are Suche 5 davon ¨uberzeugen, dass bei Terminierung ℓ = r + 1 gilt6, und dass A[r] < b < A[ℓ] ist. Die gesuchte Position ist in diesem Fall also ℓ. 6Da man die while-Schleife verlassen hat, muss ℓ > r sein. Man muss sich also ¨uberlegen, dass wir ℓ niemals auf einen gr¨osseren Wert als r + 1 setzen, und umgekehrt r niemals auf einen kleineren Wert als ℓ − 1 setzen. 6 Suchen Kapitel 2 Sortieren Wir haben also gesehen, dass Suchen auf sortierten Daten wesentlich schneller ist als auf unsortierten Daten. Wie sortieren wir Daten effizient? Formal betrachten wir das folgende Problem: Definition 2.1 (Sortieren). Gegeben sei ein Array A mit n Zahlen. Gesucht ist eine Permutation (Umordnung) von A, die aufsteigend sortiert ist: A[i] ≤ A[j] f¨ur alle 1 ≤ i < j ≤ n. Wir betrachten die folgenden elementaren Operationen: • Vergleiche, • Vertauschungen zweier Array-Elemente. Bevor wir uns dem Sortierproblem zuwenden, betrachten wir zun¨achst das Pro- blem, zu ¨uberpr¨ufen, ob ein gegebenes Array bereits sortiert ist. Dies k¨onnen wir wie folgt erreichen: Sorted(A) Pr¨ufe Sortiertheit 1 for i ← 1, 2, . . . , n − 1 do 2 if A[i] > A[i + 1] then return false 3 return true Die Laufzeit dieses Algorithmus ist Θ(n). Das ist wieder optimal, denn der Pr¨ufal- gorithmus muss jedes Element mindestens einmal anschauen. 2.1 Bubble-Sort Eine einfache Idee zum Sortieren eines Arrays A ist die folgende: Solange es einen Index i mit A[i] > A[i + 1] gibt, vertausche die Elemente A[i] und A[i + 1]. Diese Idee alleine f¨uhrt jedoch noch nicht zu einem korrekten Algorithmus – wir m¨ussen diese Prozedur mehrmals hintereinander ausf¨uhren: Bubble-Sort(A[1..n]) Bubble-Sort 1 for j ← 1, 2, . . . , n do 2 for i ← 1, 2, . . . , n − 1 do 7 8 Sortieren 3 if A[i] > A[i + 1] then 4 tausche A[i] und A[i + 1] Dieser Algorithmus wird Bubble-Sort genannt. Beispiel. Betrachten wir das Array A = (5, 3, 7, 1, 4). Bubble-Sort f¨uhrt nun die folgenden Schritte durch: j = 1 : 5 3 7 1 4 3 5 7 1 4 3 5 7 1 4 3 5 1 7 4 3 5 1 4 7 j = 2 : 3 5 1 4 7 3 1 5 4 7 3 1 4 5 7 3 1 4 5 7 3 1 4 5 7 j = 3 : 3 1 4 5 7 1 3 4 5 7 1 3 4 5 7 1 3 4 5 7 1 3 4 5 7 Wir beobachten, dass Bubble-Sort in diesem Fall bereits nach drei Durchl¨aufen das Array sortiert hat, obwohl n = 5 ist. Laufzeit Die Laufzeit von Bubble-Sort ist: • Θ(n2) Vergleiche, • O(n2) Vertauschungen. Wiederum kann man sich ¨uberlegen, dass die Schranke f¨ur die Zahl der Vertauschun- gen im worst case scharf ist. Das heisst, dass es Inputs gibt, f¨ur die der Algorithmus Θ(n2) Vertauschungen durchf¨uhrt. Ein solcher Input ist das umgekehrt sortierte Array [n, n − 1, . . . , 3, 2, 1]. Wir verzichten hier auf eine Analyse. Korrektheit Um zu begr¨unden, dass Bubble-Sort tats¨achlich korrekt ist, betrachten wir die folgende Invariante: Nach j Durchl¨aufen der ¨ausseren Schleife sind die j gr¨ossten Elemente am richtigen Ort. Formaler ausgedr¨uckt gilt also A[n − j + 1] ≤ A[n − j + 2] ≤ · · · ≤ A[n], und f¨ur jedes 1 ≤ i < n − j + 1 gilt A[i] < A[n − j + 1]. F¨ur j = 1 ist dies leicht einzusehen: Das gr¨osste Element wird in jedem Schritt der inneren Schleife nach rechts getauscht, und daher im n¨achsten Schritt direkt wieder mit dem n¨achsten Element verglichen. Es “blubbert” daher in jedem Schritt der inneren Schleife eine Position weiter nach rechts, bis es die letzte Position im Array erreicht hat. Der Name von Bubble-Sort beruht auf dieser Analogie. Induktiv folgt die Invariante f¨ur j +1 aus der Invariante f¨ur j: Die j gr¨ossten Ele- mente befinden sich bereits am richtigen Ort. Da sich links von ihnen keine gr¨osseren Elemente mehr befinden, wird der Test in Zeile 3 des Algorithmus nie ergeben, dass eines der Elemente nach links getauscht werden muss. Damit wird der Algorithmus f¨ur i = n − j, .., n − 1 keine Vertauschungen vornehmen. Dar¨uber hinaus “blubbert” das (j+1)-gr¨osste Element, also das gr¨osste Element des restlichen Arrays A[1..n−j], w¨ahrend des (j + 1)-ten Durchlaufs der ¨ausseren Schleife an die letzte Position n − j dieses Teilarrays. 2.2 Selection-Sort 9 Am Ende gilt die Invariante f¨ur j = n. Also sind die n gr¨ossten Elemente am richtigen Ort, d.h., das Array ist sortiert.1 2.2 Selection-Sort Wir werden uns jetzt ¨uberlegen, wie wir dieselbe Invariante wie in Bubble-Sort mit weniger Aufwand erreichen k¨onnen. Wir wollen also wieder erreichen: Nach j Durchl¨aufen der ¨ausseren Schleife sind die j gr¨ossten Elemente am richtigen Ort. Wie erreichen wir die Invariante f¨ur j + 1, falls die Invariante f¨ur j bereits gilt? Dazu m¨ussen wir lediglich das gr¨osste Element im Teilarray A[1, . . . , n − j] finden, und dieses mit A[n − j] vertauschen. Dies f¨uhrt zum folgenden Algorithmus, der Selection-Sort genannt wird: Selection-Sort(A[1..n]) Selection-Sort 1 for j ← n, n − 1, . . . , 1 do 2 k ← Index des Maximums in A[1, . . . , j] 3 tausche A[k] und A[j] Beispiel. Betrachten wir das Array A = (5, 3, 7, 1, 4). Selection-Sort f¨uhrt nun die folgenden Schritte durch: j = 4 : (5, 3, 7, 1, 4) → (5, 3, 4, 1, 7) j = 3 : (5, 3, 4, 1, 7) → (1, 3, 4, 5, 7) j = 2 : (1, 3, 4, 5, 7) → (1, 3, 4, 5, 7) j = 1 : (1, 3, 4, 5, 7) → (1, 3, 4, 5, 7) j = 0 : (1, 3, 4, 5, 7) → (1, 3, 4, 5, 7) Wir beobachten, dass Selection-Sort in diesem Fall bereits nach zwei Durchl¨aufen das Array sortiert hat. Korrektheit Die Korrektheit zeigt man analog zu Bubble-Sort, indem man induktiv zeigt, dass die Invariante erf¨ullt ist. Wir verzichten auf die Details. Laufzeit Beachten Sie, dass der Pseudocode gar nicht genauer spezifiziert, wie wir das Maximum des Arrays A[1..j] finden. Das liegt daran, dass wir annehmen, dass die Leser dieses Problem selbst¨andig l¨osen k¨onnen.2 Erinnern Sie sich daran, dass Pseudocode die Verst¨andlichkeit f¨ur die Leser optimieren soll. Die Ausf¨uhrlichkeit von Pseudocode kann sich also durchaus nach dem addressierten Publikum richten. 1Tats¨achlich kann man sich ¨uberlegen, dass schon die Invariante f¨ur j = n − 1 impliziert, dass das Array sortiert ist. Dann sind n − 1 Zahlen am richtigen Platz, und die letzte muss dann auch am richtigen Platz sein. Daher kann man die ¨aussere Schleife von Bubble-Sort auch schon nach j = n − 1 abbrechen. 2H¨orer der Vorlesung haben das Problem ausserdem schon in der Parallelvorlesung Einf¨uhrung in die Programmierung gesehen. 10 Sortieren Jedenfalls ist es m¨oglich, das Maximum von A[1..j] mit j − 1 Vergleichen zu bestimmen. Wir brauchen also insgesamt ∑n j=1(j − 1) = ∑n−1 j=0 j = n(n−1) 2 = Θ(n2) Vergleiche. Die Laufzeit von Selection-Sort ist also: • Θ(n2) Vergleiche, • O(n) Vertauschungen. Im Vergleich zu Bubble-Sort haben wir also die Zahl an Vertauschungen, aber nicht die Zahl der Vergleiche, verbessert. Damit bleibt die Gesamtlaufzeit Θ(n2). 2.3 Insertion-Sort Wir haben gesehen, dass es zum Verst¨andnis und Korrektheitsbeweis von Bubble- Sort zentral war, eine geeignete Invariante zu finden. Wir k¨onnen das ganze aber auch umdrehen. Wenn wir mit einer Invariante starten, k¨onnen wir aus dieser Invariante einen Algorithmus konstruieren. Betrachten wir etwa die folgende Invariante: Nach j Durchl¨aufen der ¨ausseren Schleife ist das Teilarray A[1, . . . , j] sortiert (es enth¨alt aber nicht zwangsl¨aufig die j kleinsten Elemente des Arrays). Formaler ausgedr¨uckt gilt also A[1] ≤ A[2] ≤ · · · ≤ A[j], Das ist fast dieselbe Invariante wie f¨ur Bubble-Sort3, doch wir verzichten auf die zus¨atzliche Bedingung, dass es sich auch um die j kleinsten Elemente handelt. Anders als bei Bubble-Sort k¨onnen sich die Elemente an den j ausgew¨ahlten Positionen A[1..j] in Zukunft also noch ver¨andern. Wie erreichen wir die Invariante f¨ur j, falls die Invariante f¨ur j − 1 bereits gilt? Dazu m¨ussen wir das Element A[j] an der richtigen Stelle im Teilarray A[1, . . . , j −1] einf¨ugen. Den Platz daf¨ur erreichen wir, indem wir alle Elemente in A[1, . . . , j − 1], die gr¨osser sind als A[j], um eins nach rechts verschieben. Dies f¨uhrt zum folgenden Algorithmus, der Insertion-Sort genannt wird: Insertion-Sort(A[1..n])Insertion-Sort 1 for j ← 2, 3, . . . , n do 2 k ← kleinster Index in {1, . . . , j − 1} mit A[j] ≤ A[k] ▷ A[j] geh¨ort an diese Stelle k 3 x ← A[j] ▷ merke A[j] 4 verschiebe A[k, . . . , j − 1] nach A[k + 1, . . . , j] 5 A[k] ← x 3F¨ur die ersten statt die letzten j Elemente. Bei Bubble-Sort kam die Invariante aus dem Algo- rithmus, und es ergab sich eben so, dass die hinteren Positionen als erstes richtig besetzt wurden. Nun, da wir die Invariante selbst festlegen, k¨onnen wir ebenso gut vorne anfangen. 2.4 Mergesort 11 Beispiel. Betrachten wir das Array A = (5, 3, 7, 1, 4). Insertion-Sort f¨uhrt nun die folgenden Schritte durch: j = 2 : (5, 3, 7, 1, 4) → (3, 5, 7, 1, 4) j = 3 : (3, 5, 7, 1, 4) → (3, 5, 7, 1, 4) j = 4 : (3, 5, 7, 1, 4) → (1, 3, 5, 7, 4) j = 5 : (1, 3, 5, 7, 4) → (1, 3, 4, 5, 7) Wir beobachten, dass Insertion-Sort in diesem Fall vier Durchl¨aufe ben¨otigt. Korrektheit Die Korrektheit kann wieder einfach mit Induktion bewiesen werden, indem man induktiv zeigt, dass die Invariante erf¨ullt ist. Nach dem Durchlauf j = n impliziert die Invariante, dass das Array sortiert ist. Laufzeit Zur Laufzeit haben wir zwei Operationen, die wir verstehen m¨ussen. Zun¨achst m¨ussen wir in Zeile 2 einige Vergleiche ausf¨uhren, um den Index k zu fin- den. Dabei k¨onnen wir ausnutzen, dass gem¨ass Invariante der Bereich A[1, . . . , j − 1] schon sortiert ist. Wir k¨onnen den Index k daher mit bin¨arer Suche finden. Das ben¨otigt h¨ochstens O(log j) ≤ O(log n) viele Vergleiche. Damit ist die Zahl Ver- gleiche insgesamt O(n log n). Man kann versuchen, die Zahl der Vergleiche genauer mit O( ∑n j=2 log j) abzusch¨atzen, das gibt aber ebenfalls nur O(n log n). Tats¨achlich werden wir sp¨ater sehen, dass jeder vergleichsbasierte Sortieralgorithmus mindestens Ω(n log n) Vergleiche ben¨otigt. F¨ur die Zahl an Vertauschungen m¨ussen wir Zeile 4 betrachten. Dort ben¨otigen wir j − k + 1 Vertauschungen, im schlimmsten Fall k = 1 (bei einem umgekehrt sortierten Array) also j Vertauschungen. Insgesamt sind das im schlimmsten Fall Θ(∑n j=2 j) = Θ(n2) Vertauschungen. Die Laufzeit von Insertion-Sort ist also: • O(n log n) Vergleiche, • O(n2) Vertauschungen. Beide Schranken sind im worst case scharf. 2.4 Mergesort Die drei bisher vorgestellten Sortieralgorithmen ben¨otigen im schlechtesten Fall Θ(n2) viele Operationen. Bei Selection-Sort konnten wir die Zahl der Vertauschun- gen reduzieren, bei Insertion-Sort die Zahl der Vergleiche. K¨onnen wir eine Methode finden, bei der wir beide Gr¨ossen gleichzeitig reduzieren? Schauen wir uns an, was Divide-and-Conquer erm¨oglicht. Wir k¨onnen das Array in zwei H¨alften teilen, die beiden H¨alften rekursiv sortieren, und die beiden sortierten H¨alften dann zu einem sortierten Array zusammenf¨ugen. Diese Idee f¨uhrt zum Algorithmus Mergesort. Der folgende Pseudocode soll f¨ur 1 ≤ l ≤ r ≤ n den Bereich von A[l] bis A[r] aufsteigend sortieren. Mergesort(A[1..n], l, r) ▷ sortiert A[l, . . . , r] Mergesort 12 Sortieren 1 if l < r then 2 m ← ⌊(l + r)/2⌋ 3 Mergesort(A, l, m) ▷ sortiere linke H¨alfte 4 Mergesort(A, m + 1, r) ▷ sortiere rechte H¨alfte 5 Merge(A, l, m, r) ▷ verschmelze beide H¨alften Beispiel. Betrachten wir das Array A = (9, 7, 3, 2, 1, 8, 4, 6) und die Indizes l = 1, m = 4 und r = 8. Die Teilarrays A[1, . . . , 4] = (9, 7, 3, 2) und A[5, . . . , 8] = (1, 8, 4, 6) werden zun¨achst rekurisv sortiert. Mergesort f¨uhrt also die folgenden Schritte durch: (9, 7, 3, 2, 1, 8, 4, 6) → (2, 3, 7, 9, 1, 4, 6, 8) sortiere beide H¨alften → (1, 2, 3, 4, 6, 7, 8, 9) merge Korrektheit Wir benutzen vollst¨andige Induktion nach der L¨ange r − l + 1 des zu sortierenden Bereichs. Der Basisfall dieser Rekursion ist l = r, oder anders ausge- dr¨uckt l − r + 1 = 1. In diesem Fall besteht das zu sortierende Teilarray A[l, . . . , r] nur aus einem einzigen Element, ist also bereits sortiert. F¨ur den Induktionsschritt nehmen wir an, dass l < r ist. Die Induktionshypothese ist, dass Mergesort immer korrekt sortiert, wenn es f¨ur einen Bereich aufgerufen wird, der k¨urzer als r − l + 1 ist. Wegen l < r ist auch m < r, und der zu sortierende Bereich des ersten rekursiven Aufrufs Mergesort(A, l, m) ist daher m−l+1 < r−l+1. Nach Induktionshypothese sortiert Mergesort diesen Bereich also korrekt. Ebenso ist m + 1 > l, weshalb der Bereich des zweiten rekursiven Aufrufs ebenfalls k¨urzer als r − l + 1 ist. Also wird auch dieser korrekt sortiert. Sofern die Routine Merge zwei sortierte Teilbereiche korrekt verschmilzt, ist der Output des Algorithmus also korrekt. Merge Es bleibt noch zu beschreiben, wie die Funktion Merge funktioniert. Sie erh¨alt als Eingabe ein Array A und drei Indizes l, m und r, sodass die Teilarrays A[l, . . . , m] und A[m + 1, . . . , r] bereits sortiert sind. Merge soll diese beiden Teilar- rays zu einem sortierten Teilarray A[l, . . . , r] verschmelzen. Dazu beobachten wir, dass wir das kleinste Element sehr einfach finden k¨onnen, denn es ist entweder das kleinste Element von A[l, . . . , m] oder das kleinste Element von A[m + 1, . . . , r]. Da diese Teilarrays sortiert sind, m¨ussen wir nur die beiden ersten Elemente A[l] bzw. A[m + 1] betrachten. Das kleinere der beiden merken wir uns in einem separaten Ar- ray B und l¨oschen es (gedanklich) aus A[l, . . . , m] bzw. A[m + 1, . . . , r], verkleinern eines dieser beiden Arrays also um den ersten Eintrag. Von den beiden verbleibenden Arrays bestimmen wir wiederum das kleinste Element mit derselben Methode, und iterieren den Prozess. Iterativ geschrieben f¨uhrt dies auf den folgenden Pseudocode: Merge(A[1..n], l, m, r)Merge 1 B ← new Array with r − l + 1 cells ▷ so gross wie A[l, . . . , r] 2 i ← l ▷ erstes unbenutztes Ele- ment in linker H¨alfte 2.4 Mergesort 13 3 j ← m + 1 ▷ erstes unbenutztes Ele- ment in rechter H¨alfte 4 k ← 1 ▷ n¨achste Position in B 5 while i ≤ m and j ≤ r do ▷ beide H¨alften noch nicht ausgesch¨opft 6 if A[i] < A[j] then 7 B[k] ← A[i] 8 i ← i + 1 9 k ← k + 1 10 else 11 B[k] ← A[j] 12 j ← j + 1 13 k ← k + 1 14 ¨ubernimm Rest links bzw. rechts ▷ wenn die andere H¨alfte ausgesch¨opft ist 15 kopiere B nach A[l, . . . , r] Beispiel. Die folgende Graphik veranschaulicht die Arbeitsweise von Merge auf dem Beispiel von oben. → A : 2 3 7 9 1 4 6 8 l m r i j B : 1 0 0 0 0 0 0 0 k → A : 2 3 7 9 1 4 6 8 l m r i j B : 1 2 0 0 0 0 0 0 k → A : 2 3 7 9 1 4 6 8 l m r i j B : 1 2 3 0 0 0 0 0 k → A : 2 3 7 9 1 4 6 8 l m r i j B : 1 2 3 4 0 0 0 0 k → A : 2 3 7 9 1 4 6 8 l m r i j B : 1 2 3 4 6 0 0 0 k → A : 2 3 7 9 1 4 6 8 l m r i j B : 1 2 3 4 6 7 0 0 k → A : 2 3 7 9 1 4 6 8 l m r i j B : 1 2 3 4 6 7 8 0 k → A : 2 3 7 9 1 4 6 8 l m r i j B : 1 2 3 4 6 7 8 9 k Korrektheit und Laufzeit von Merge Die Korrektheit von Merge kann mit vollst¨andi- ger Induktion nach k bewiesen werden. Die Induktionsbehauptung ist dabei, dass in B[1..k] die k kleinsten Elemente aus A[l..r] in richtiger Reihenfolge stehen, und dass die restlichen Elemente in A[i..m] und B[j..r] stehen. Wir ¨uberlassen diesen Induktionsbeweis den Lesern. Die Laufzeit von Merge ist O(n), da jedes Element von A nur einmal gelesen und einmal geschrieben wird. Laufzeit Die Laufzeit von Mergesort l¨asst sich durch die folgende Rekursionsglei- chung beschreiben: T (1) = c, T (n) ≤ 2 · T (n/2) + dn f¨ur n = 2 k. 14 Sortieren Dabei ist c die Laufzeit f¨ur den Basisfall und dn die Laufzeit von Merge. Mittels Induktion l¨asst sich zeigen, dass T (n) = dn log2 n + cn ≤ O(n log n) gilt. Mergesort ist also asymptotisch schneller als die drei zuvor vorgestellten Algo- rithmen. Zusatzspeicher Ein Nachteil von Mergesort ist, dass der Algorithmus ein zus¨atzli- ches Array B ben¨otigt. Ein Sortieralgorithmus, der kein zus¨atzliches Array ben¨otigt, wird in-place-Algorithmus genannt. Bubble-Sort, Selection-Sort und Insertion-Sort sind in-place-Algorithmen, Mergesort hingegen nicht.4 Zus¨atzlicher Speicher ist in manchen Situationen kein Problem, in anderen aber schon. Bei einem Array von vielen Gigabyte oder Terrabyte kann der zus¨atzliche Speicherplatz den Algorithmus verlangsamen, weil der Computer auf langsamere Ressourcen zur¨uckgreifen muss, zum Beispiel auf die Festplatte statt den Arbeitsspeicher. Alternative Bestimmung der Laufzeit 1 2 3 4 6 7 8 9 2 3 7 9 7 9 9 7 2 3 3 2 1 4 6 8 1 8 1 8 4 6 4 6 ≤ n Vergleiche + 2n Kopieroperationen ≤ n Vergleiche + 2n Kopieroperationen ≤ n Vergleiche + 2n Kopieroperationen Alternativ l¨asst sich die Laufzeit von Mergesort auch wie folgt bestimmen: Betrach- ten wir alle Aufrufe von Merge im i-ten Level zusammen, d.h. alle Operationen, mit denen man Arrays der L¨ange n/2i zu Arrays der L¨ange n/2i−1 zusammenf¨ugt. Jede der n Zahlen kommt pro Level nur einmal vor. Pro Vergleich wird mindestens eine dieser n Zahlen an der richtigen Stelle eingef¨ugt, und sie braucht anschliessend nicht mehr betrachtet zu werden. Daher gibt es maximal n Vergleiche. F¨ur die Kopierope- rationen ist es ¨ahnlich, ausser dass wir am Ende das Hilfsarray B wieder zur¨uck nach A kopieren und deshalb zwei Kopieroperationen pro Element haben. Die Zahl der Vergleiche pro Level ist daher ≤ n, und die Zahl der Kopieroperationen pro Level ist exakt 2n. Da die Rekursion von Mergesort log2 n Level hat (f¨ur n = 2k), ist die Gesamtlaufzeit log2 n ︸ ︷︷ ︸ Level · n︸︷︷︸ Vergleiche pro Level ≤ O(n log n). 2.5 Quicksort 2.5.1 Motivation In den vorherigen Vorlesungen haben wir verschiedene Sortieralgorithmen kennen- gelernt, darunter Bubble-Sort, Selection-Sort, Insertion-Sort und Mergesort. Die Ta- belle 2.1 fasst die Laufzeiten und Eigenschaften dieser Algorithmen zusammen. 4Es gibt M¨oglichkeiten, bei Mergesort den zus¨atzlichen Speicherbedarf zu reduzieren. Diese sind aber deutlich komplizierter. 2.5 Quicksort 15 Algorithmus Vergleiche Bewegungen Extr. Platz Lokalit¨at Bubble-Sort O(n2) O(n2) O(1) gut Selection-Sort O(n2) O(n) O(1) gut Insertion-Sort O(n log n) O(n2) O(1) gut Mergesort O(n log n) O(n log n) O(n) gut Tabelle 2.1 ¨Ubersicht ¨uber die Laufzeiten und Eigenschaften verschiedener Sortieralgo- rithmen. Bewegungen bezeichnet die Anzahl der Schreiboperationen im Speicher. Extr. Platz gibt an, wie viel zus¨atzlichen Speicherplatz der jeweilige Algorithmus ben¨otigt. Lokalit¨at be- schreibt, ob der Algorithmus im Speicher sequentiell arbeitet, oder ob er im Speicher hin und her springt. Je nach Rechnerhardware kann Lokalit¨at verschieden wichtig sein. Die Grundidee von Mergesort besteht darin, das zu sortierende Array in zwei H¨alften aufzuteilen, die beiden H¨alften rekursiv zu sortieren, und anschliessend die beiden sortierten Teillisten in einem Schritt zu verschmelzen. Die eigentliche Ar- beit f¨allt beim Zusammenf¨uhren an, da dabei die Elemente der beiden Teillisten miteinander verglichen und in die richtige Reihenfolge gebracht werden m¨ussen. 2.5.2 Grundidee Quicksort verfolgt einen ¨ahnlichen Ansatz wie Mergesort, allerdings wird die Reihen- folge der Schritte umgekehrt: Anstatt die Teillisten erst zu sortieren und anschlies- send zusammenzuf¨uhren, werden die Elemente des Arrays zun¨achst in zwei Gruppen aufgeteilt, so dass die Elemente der einen Gruppe kleiner sind als die Elemente der anderen Gruppe. Anschliessend werden die beiden Gruppen rekursiv sortiert. Der Schl¨ussel zu einer solchen Aufteilung ist die Wahl eines sogenannten Pi- votelements. Alle Elemente, die kleiner als das Pivotelement sind, kommen in die linke Gruppe; alle Elemente, die gr¨osser sind als das Pivotelement, kommen in die rechte Gruppe. Nachdem die Elemente in zwei Gruppen aufgeteilt wurden, muss das Pivotelement noch an die richtige Stelle zwischen den beiden Gruppen verschoben werden. 2.5.3 Der Algorithmus Quicksort l¨asst sich gut rekursiv beschreiben. Der Algorithmus erh¨alt als Eingabe ein Array A und zwei Indizes l und r, die den zu sortierenden Bereich festlegen. Die Aufgabe besteht darin, den Bereich von A[l] bis A[r] aufsteigend zu sortieren. Quicksort(A[1..n], l, r) Quicksort 1 if l < r then 2 k ← Aufteilen(A, l, r) ▷ Teile A[l..r] in zwei Gruppen auf 3 Quicksort(A, l, k − 1) ▷ Sortiere linke Gruppe 4 Quicksort(A, k + 1, r) ▷ Sortiere rechte Gruppe Die Routine Aufteilen(A, l, r) f¨uhrt die folgenden Schritte aus: 1. W¨ahle ein Element p als Pivotelement. In unserem Fall w¨ahlen wir das letzte Element des zu sortierenden Bereichs: p ← A[r]. 16 Sortieren 2. Bestimme die korrekte Position k des Pivotelements p im Array A. 3. Verschiebe alle Elemente, die kleiner oder gleich p sind, nach links (in den Bereich A[l..k − 1]) und alle Elemente, die gr¨osser als p sind, nach rechts (in den Bereich A[k + 1..r]). Sofern Aufteilen(A, l, r) diese drei Ziele erreicht, folgt die Korrektheit von Quicksort ganz ¨ahnlich zu der von Mergesort mit vollst¨andiger Induktion. Die Routine Aufteilen(A, l, r) ist leicht zu implementieren, indem wir erst Schritt 2. und anschliessend Schritt 3. ausf¨uhren. Wir stellen hier aber eine besonders elegan- te Alternative vor, die beide Ziele gleichzeitig erreicht. Dazu lassen wir die Position des Pivotelements vorl¨aufig offen. Im verbleibenden Bereich A[l..r − 1] suchen wir von links ausgehend ein Element A[i], das gr¨osser als p ist, und gleichzeitig von rechts ausgehend ein Element A[j], das kleiner als p ist. Wenn wir ein solches Paar finden, dann tauschen wir die Positionen. Auf die Weise stellen wir sicher, dass alle Elemente im Bereich A[l..i] kleiner als das Pivotelement sind, und alle Elemente im Bereich A[j..r] gr¨osser als das Pivotelement sind. Sobald wir das komplette Array auf diese Weise bearbeitet haben, m¨ussen wir nur noch das Pivot-Element zwischen diesen beiden Bereichen einsortieren, was wir mit einer zus¨atzlichen Vertauschung erreichen k¨onnen. 3 6 8 5 1 7 2 9 4 ✓ ✓ p i j → 3 2 8 5 1 7 6 9 4 ✓ ✓ ✓ ✓ ✓ i j → 3 2 1 5 8 7 6 9 4 ✓ ✓ ✓ ✓ ✓ ✓ ✓ ij → 3 2 1 4 8 7 6 9 5 ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ≤ p ≥ p Aufteilen(A[1..n], l, r)Aufteilen 1 i ← l ▷ Index f¨ur die linke Gruppe 2 j ← r − 1 ▷ Index f¨ur die rechte Gruppe 3 p ← A[r] ▷ Pivotelement 4 repeat 5 while i < r and A[i] ≤ p do 6 i ← i + 1 ▷ Suche n¨achstes Element f¨ur rechte Gruppe 7 while j > l and A[j] > p do 8 j ← j − 1 ▷ Suche n¨achstes Element f¨ur lin- ke Gruppe 9 if i < j then 10 Vertausche A[i] und A[j] ▷ Vertausche Elemente 11 until i > j ▷ Schleife endet, wenn sich i und j ”treffen “ 12 Vertausche A[i] und A[r] ▷ Pivotelement an die richtige Stelle tauschen 13 return i ▷ Gib Position des Pivotelements zur¨uck 2.6 Heapsort 17 Die While-Schleifen suchen nach Elementen, die sich auf der falschen Seite des Pi- votelements befinden. Werden zwei solche Elemente gefunden, werden sie vertauscht. Die Tests “i < r” und “j > l” fangen die F¨alle ab, dass das Pivotelement gr¨osser bzw. kleiner ist als alle Elemente aus A[l..r − 1]. Die Repeat-Schleife wird so lange ausgef¨uhrt, bis die Indizes i und j aneinander vorbeigelaufen sind. Am Ende der Routine ist i = j + 1, alle Elemente in A[l..i − 1] sind kleiner als p und alle Elemente in A[j + 1..r − 1] sind gr¨osser als p. Die korrekte Position f¨ur das Pivotelement ist daher k = j + 1 = i. Korrektheit und Laufzeit Um die Korrektheit von Aufteilen(A, l, r) formal zu zei- gen, ist der zentrale Schritt, die folgende Invariante mit vollst¨andiger Induktion zu zeigen: Nach Zeile 8 in jedem Durchlauf der Repeat-Schleife sind alle Elemente in A[l..i − 1] kleiner oder gleich p, und alle Elemente in A[j + 1..r − 1] gr¨osser als p. Die Laufzeit von Aufteilen(A, l, r) ist linear in der L¨ange des zu sortierenden Bereichs, Θ(r − l + 1), da sich der Abstand j − i mit jeder Ausf¨uhrung einer der while-Schleifen um 1 verkleinert. Den Term +1 in Θ(r − l + 1) kann man in den meisten F¨allen weglassen, er ist aber f¨ur den Spezialfall l = r n¨otig. 2.5.4 Laufzeit von Quicksort Die Laufzeit von Quicksort h¨angt davon ab, an welcher Position das Pivotelement landet. Im besten Fall landet das Pivotelement genau in der Mitte des zu sortierenden Bereichs. In diesem Fall teilt sich das Problem in zwei gleich grosse Teilprobleme auf, und wir erhalten die folgende Rekurrenz f¨ur die Laufzeit: T (n) ≤ 2 · T ( n 2 ) + c · n. Dabei ist c · n die Laufzeit der Routine Aufteilen. Aus dieser Rekurrenz ergibt sich eine Laufzeit von O(n log n). Im schlechtesten Fall landet das Pivotelement am Rand des zu sortierenden Be- reichs. Dies ist beispielsweise der Fall, wenn das Array bereits sortiert ist und wir das letzte Element als Pivotelement w¨ahlen. In diesem Fall erhalten wir die folgende Rekurrenz: T (n) = T (n − 1) + c · n. Diese Rekurrenz f¨uhrt zu einer Laufzeit von O(n2). Trotz der schlechten worst-case Laufzeit ist Quicksort neben Mergesort der am weitesten benutzte Sortieralgorithmus. Warum ist das so? Ein Grund ist, dass man die worst-case Laufzeit vermeiden kann, indem man das Pivotelement zuf¨allig w¨ahlt. Diese Art von Algorithmen werden Sie im n¨achsten Semester in der Vorlesung Al- gorithmen und Wahrscheinlichkeit kennenlernen. 2.6 Heapsort 2.6.1 Motivation Selection-Sort basiert auf der Idee, in jedem Schritt das gr¨osste Element des Arrays zu finden und an die richtige Stelle zu verschieben. Die Invariante des Algorithmus ist, dass der rechte Teil des Arrays bereits korrekt sortiert ist. 18 Sortieren 93 40 39 11 1 8 4 6 Abb. 2.1 Beispiel f¨ur einen Max-Heap. Der Flaschenhals von Selection-Sort ist die Suche nach dem gr¨ossten Element, die im i-ten Schritt Zeit Θ(i) ben¨otigt. Dies f¨uhrt zu einer Gesamtlaufzeit von Θ(n2). Die Frage ist nun, ob sich diese Suche nach dem gr¨ossten Element beschleunigen l¨asst. Sofern die Daten in einem unsortierten Array gespeichert sind, ist die Antwort nein: Wir m¨ussen jedes Element des Arrays betrachten, um das gr¨osste Element zu finden. Die Idee von Heapsort besteht nun darin, die Daten nicht in einem Array, sondern in einer speziellen Baumstruktur, einem sogenannten Heap, zu organisieren. Diese Struktur erlaubt es uns, das gr¨osste Element effizient zu finden und zu entfernen. 2.6.2 Max-Heap Ein Max-Heap ist ein bin¨arer Baum, in dem jeder Knoten einen Schl¨ussel 5 hat und der die folgenden beiden Bedingungen erf¨ullt: 1. Vollst¨andiger Bin¨arbaum: Jeder Knoten hat genau zwei Kinder, ausser eventu- ell im untersten Level des Baums. Das unterste Level ist von links nach rechts aufgef¨ullt. 2. Heap-Bedingung: Der Schl¨ussel jedes Knotens ist gr¨osser oder gleich den Schl¨usseln seiner Kinder. Abbildung 2.1 zeigt ein Beispiel f¨ur einen Max-Heap. Terminologie Wir sind B¨aumen schon kurz begegnet. Da dieses Konzept zunehmend wichtiger wird, wollen wir hier etwas mehr Terminologie daf¨ur einf¨uhren. In einem Baum werden die Knoten ¨uber Kanten miteinander verbunden. • Elternknoten, Kindknoten: Ein Knoten, der direkt ¨uber einem anderen Knoten liegt, wird als Elternknoten bezeichnet; der darunterliegende Knoten ist der Kindknoten. Jeder Knoten ausser dem obersten hat genau einen Elternknoten, aber kann verschieden viele Kindknoten haben. 5Der Schl¨ussel wird auch Wert oder Inhalt genannt. 2.6 Heapsort 19 • Vorfahren, Nachkommen: Ein Knoten v ist ein Vorfahr eines Knotens w, wenn es einen Pfad von v nach w gibt, der nur abw¨arts verl¨auft. In diesem Fall ist w ein Nachkomme von v. • Wurzel : Der Knoten, der ganz oben im Baum steht und keinen Elternknoten hat, ist die Wurzel des Baums. • Bl¨atter : Die Knoten, die keine Kinder haben, sind die Bl¨atter des Baums. Eigenschaften des Max-Heaps Aus der Heap-Bedingung folgt direkt, dass das gr¨osste Element eines Max-Heaps stets in der Wurzel des Baums steht. Ausserdem gilt: Der Schl¨ussel eines beliebigen Vorfahren ist gr¨osser oder gleich dem Schl¨ussel eines beliebigen Nachkommen. 2.6.3 Heapsort Um Heapsort zu implementieren, ben¨otigen wir zwei Operationen auf Max-Heaps: 1. Daten in Heap umwandeln: Gegeben ein Array A mit n Elementen, konstruiere einen Max-Heap, dessen Knoten die Elemente aus A enthalten. 2. Maximum aus Heap l¨oschen (ExtractMax): Finde das gr¨osste Element des Heaps (also den Schl¨ussel der Wurzel), entferne es aus dem Heap, und stelle die Heap-Bedingung wieder her. Sobald wir diese beiden Operationen implementiert haben, l¨asst sich Heapsort wie folgt beschreiben: Heapsort(A[1..n]) Heapsort 1 H ← Heapify(A) ▷ Wandle Array in Heap um. 2 for i ← n, n − 1, . . . , 1 do ▷ Entferne Elemente aus Heap 3 A[i] ← ExtractMax(H) Beachten Sie, dass wir noch nicht diskutiert haben, wie man einen Heap im Rechner ¨uberhaupt darstellt. Das ist auch v¨ollig in Ordnung. Unsere Analyse und unser Code wird unabh¨angig von der genauen Darstellung sein. Wir treffen nur einige Annahmen: • Wir wissen, wo im Rechner die Wurzel abgespeichert ist. • Wir kennen den Speicherort des letzten Blatts, und wir haben weiterhin einen g¨ultigen Heap, wenn wir dieses Blatt aus dem Speicher l¨oschen. • F¨ur einen gegebenen Knoten k¨onnen wir in Zeit O(1) feststellen, wie viele Kinder der Knoten hat (0, 1 oder 2) und in Zeit O(1) auf den Speicherort dieser Kindknoten zugreifen. Ebenso k¨onnen wir in Zeit O(1) auf den Speicherort des Elternknotens zugreifen (ausser bei der Wurzel, die ja keinen Elternknoten hat). 20 Sortieren • Wenn wir den Speicherort eines Knotens kennen, k¨onnen wir seinen Schl¨ussel in Zeit O(1) ¨uberschreiben. Tats¨achlich empfehlen wir, die abstrakte Ebene eines Heaps nicht mit der Imple- mentierungsebene zu vermischen. Zum Verstehen eines Heaps und der darauf ba- sierenden Algorithmen ist die Baumstruktur die bessere Abstraktionsebene. Fragen der Implementierungen lenken eher ab, als dass sie helfen. 2.6.4 Maximum aus Heap l¨oschen (ExtractMax procedure) Die Operation ExtractMax besteht aus zwei Schritten: 1. Entferne die Wurzel des Heaps. Dadurch entsteht eine L¨ucke an der Wurzel- position, und die Baumstruktur des Heaps wird verletzt. 2. Verschiebe das letzte Blatt des Heaps an die Wurzelposition.6 Dadurch wird die Baumstruktur wieder hergestellt, allerdings ist die Heap-Bedingung m¨ogli- cherweise verletzt. Stelle die Heap-Bedingung wieder her, indem du den neuen Wurzelknoten so lange mit dem gr¨osseren seiner beiden Kinder vertauschst (“versickern l¨asst”), bis die Heap-Bedingung erf¨ullt ist. Wir ¨uberlegen uns nun, dass das Versickern in Schritt 2. einen g¨ultigen Heap erzeugt. Wenn ein Knoten v die Heapbedingung verletzt und wir v mit dem gr¨osseren seiner beiden Kinder u vertauschen, so ist die Heapbedingung an u (an der fr¨uheren Position von v) danach erf¨ullt. Es gibt zwei Orte, wo danach die Heapbedingung verletzt sein k¨onnte: Am Elternknoten von u, sofern vorhanden (denn dieser hat u als neues Kind bekommen), und am Knoten v selbst. Der Elternknoten von u ist kein Problem, denn der war schon vor Aufrufen von ExtractMin Elternknoten von u und hat damit einen gr¨osseren Schl¨ussel als u. Also ist die Heapbedingung weiterhin ¨uberall ausser eventuell an v erf¨ullt. In jedem Schritt wird v ein Level weiter nach unten geschoben, bis die Heapbedingung an v erf¨ullt ist, und zu diesem Zeitpunkt ist die Heapbedingung an jedem Knoten erf¨ullt. Da Bl¨atter trivialerweise die Heapbedingung erf¨ullen, endet dieser Prozess sp¨atestens, wenn v ein Blatt ist. Abbildung 2.2 veranschaulicht die Operation Extract Max anhand eines Bei- spiels. Laufzeit von Extract Max Die Laufzeit von Extract Max wird von der Anzahl der Vertauschungen dominiert, die notwendig sind, um die Heap-Bedingung wiederherzustellen. Im schlimmsten Fall m¨ussen wir den neuen Wurzelknoten bis zum untersten Level des Baums verschieben. Die Anzahl der Vertauschungen ist dann proportional zur Tiefe des Baums. Wie schon fr¨uher gesehen, ist die Tiefe eines vollst¨andigen Bin¨arbaums mit n Knoten O(log n). Folglich ben¨otigt ExtractMax Zeit O(log n). 2.6.5 Daten in Heap umwandeln Es gibt verschiedene M¨oglichkeiten, ein Array in einen Heap umzuwandeln. Eine ein- fache Methode besteht darin, mit einem leeren Heap zu beginnen und die Elemente des Arrays nacheinander in den Heap einzuf¨ugen. 6Wir realisieren 1. und 2. zusammen dadurch, dass wir den Schl¨ussel der Wurzel mit dem Schl¨ussel des letzten Blattes ¨uberschreiben, und das letzte Blatt l¨oschen. 2.6 Heapsort 21 ∑ . 89 87 42 41 32 72 67 8 28 5 2 0 25 Ersetze Wurzel durch letztes Blatt −→ 2 87 42 41 32 72 67 8 28 5 25 Tausche Knoten mit gr¨osstem Kind −→ 87 2 42 41 32 72 67 8 28 5 25 Problem an der Wurzel behoben Problem an Knoten 2 neu geschaffen ∫ −→ 87 72 42 41 32 2 67 8 28 5 25 Erneut behoben und geschaffen −→ 87 72 42 41 32 67 2 8 28 5 25 Problem gel¨ost Abb. 2.2 Beispiel f¨ur die Operation Extract Max. ∫ 89 87 42 41 32 72 67 8 28 5 2 60 25 60 wird als neuer Knoten hinzugef¨ugt −→ 89 87 42 41 32 72 67 8 28 60 2 5 25 Vertausche bis die Heapbedingung erf¨ullt ist −→ 89 87 42 41 32 72 67 8 60 28 2 5 25 Heapbedingung ist wieder erf¨ullt Abb. 2.3 Beispiel f¨ur die Operation insert. Einf¨ugen in Heap (insert) Die Operation insert(H, p) f¨ugt einen neuen Knoten mit Schl¨ussel p in den Heap H ein. Sie besteht aus zwei Schritten: 1. F¨uge einen neuen Knoten v an der n¨achsten freien Stelle im Heap ein. Dadurch wird die Baumstruktur des Heaps erhalten. 2. Stelle die Heap-Bedingung wieder her, indem du den neuen Knoten v so lange mit seinem Elternknoten vertauschst, bis die Heap-Bedingung erf¨ullt ist. Analog zum Versickern ¨uberlegt man sich, dass Operation 2. die Heapbedingung wieder herstellt. Der einzige Knoten, der die Heapbedingung m¨oglicherweise verletzt, ist der jeweilige Elternknoten von v. Das Problem verschwindet sp¨atestens, wenn v zur Wurzel des Baumes aufsteigt, da es dann keinen Elternknoten mehr gibt. Abbildung 2.3 veranschaulicht die Operation insert anhand eines Beispiels. Laufzeit von insert Die Laufzeit von insert wird von der Anzahl der Vertauschungen dominiert, die notwendig sind, um die Heap-Bedingung wiederherzustellen. Im schlimmsten Fall m¨ussen wir den neuen Knoten bis zur Wurzel des Baums verschieben. Die Anzahl der Vertauschungen ist daher proportional zur Tiefe des Baums, also O(log n). Folglich ben¨otigt insert f¨ur einen Heap mit n Knoten Zeit O(log n). 22 Sortieren Laufzeit des Umwandelns Um ein Array mit n Elementen in einen Heap umzuwandeln, m¨ussen wir n mal die Operation insert ausf¨uhren. Da insert f¨ur einen Heap mit k ≤ n Knoten Zeit O(log k) ≤ O(log n) ben¨otigt, betr¨agt die Gesamtlaufzeit des Umwandelns O(n log n). Alternatives Verfahren zum Umwandeln Eine zweite M¨oglichkeit zum Umwandeln eines Arrays in einen Heap besteht darin, die Schl¨ussel in beliebiger Reihenfolge in die Knoten des Heaps zu schreiben, und dann jeden Knoten einmal wie bei ExtractMax versickern zu lassen, wobei wir von unten nach oben vorgehen. Wir lassen also zun¨achst die Knoten auf dem unters- ten Level versickern (was trivial ist, weil diese die Heapbedingung nicht verletzen k¨onnen), dann die Knoten des zweituntersten Level, und so weiter bis hin zur Wur- zel. Dabei f¨uhren wir f¨ur jeden Knoten den kompletten Versickerungsvorgang bis m¨oglicherweise zur untersten Ebene aus, und nicht etwa nur einen Schritt. Der Vorteil dieses Verfahrens ist, dass wir f¨ur die meisten Knoten nicht viele Schritte ausf¨uhren m¨ussen, weil die meisten Knoten schon in der untersten Ebene starten. Wir skizzieren hier nur grob die Analyse. Hat der Baum Tiefe t ≤ O(log n), so gibt es einen Knoten (die Wurzel) in Tiefe 0, zwei Knoten in Tiefe 1, und allgemein 2i Knoten in Tiefe i. F¨ur einen Knoten in Tiefe i m¨ussen wir bis zu t − i Vertau- schungen durchf¨uhren, bis er bis zur Ebene der Bl¨atter gesickert ist. Die gesamte Zahl an Vertauschungen ist damit h¨ochstens t∑ i=0 2 i · (t − i). Sie lernen n¨achsten Semester in Analysis mehr dar¨uber, wie man solche Summen auswertet. Hier geben wir nur das Ergebnis an: die Summe ist in O(2t) = O(n).7 Damit ist dies ein effizienteres Verfahren, ein Array in einen Heap umzuwandeln. F¨ur die Gesamtlaufzeit von Heapsort bringt das aber keine Vorteile, weil dort das n- malige Ausf¨uhren von ExtractMax dort immer noch Zeit O(n log n) dauert. Deshalb verzichten wir auch auf eine Diskussion der Korrektheit dieses Verfahrens. 2.6.6 Laufzeitanalyse von Heapsort Wir fassen zur ¨Ubersicht die Laufzeit von Heapsort zusammen: 1. Umwandeln des Arrays in einen Heap: O(n log n) bzw. O(n) 2. n-maliges Ausf¨uhren von ExtractMax: n · O(log n) = O(n log n) Die Gesamtlaufzeit von Heapsort betr¨agt daher O(n log n). 7Man k¨onnte dieses Ergebnis herleiten, indem man sich f¨ur jedes Level ¨uberlegt, wie viele Knoten dieses Level beim Versickern ¨uberqueren k¨onnten. F¨ur einen voll besetzten Baum kann jeder Knoten im untersten Level landen, aber nur h¨ochstens n/2 Knoten k¨onnen beim Versickern das zweitunters- ten Level durchqueren, n¨amlich die, die oberhalb dieses Levels starten. Daher werden h¨ochsten n/2 Knoten vom zweituntersten zum untersten Level getauscht. Ebenso k¨onnen h¨ochstens n/4 Knoten vom drittuntersten zum zweituntersten Level getauscht werden, weil nur so viele Knoten oberhalb des drittuntersten Levels starten, und so weiter. Damit ist die Zahl der Vertauschungen h¨ochstens n/2 + n/4 + n/8 + . . . ≤ n · ∑t i=1 2−i ≤ n. 2.6 Heapsort 23 93 40 8 39 1 4 6 11 1 2 3 4 5 6 7 8 Abb. 2.4 Darstellung des Heaps aus Abbildung 2.1 in einem Array. 2.6.7 Darstellung eines Bin¨arbaums im Speicher Bisher haben wir uns nicht darum gek¨ummert, wie ein Bin¨arbaum (und damit auch ein Heap) im Speicher eines Computers dargestellt wird. Eine effiziente M¨oglichkeit besteht darin, die Knoten des Baums in einem Array zu speichern. Die Knoten werden Level f¨ur Level von links nach rechts im Array abgelegt. Die Wurzel des Baums steht an Position 1 des Arrays. Die beiden Kinder der Wurzel stehen an Position 2 und 3, und so weiter. Allgemein stehen die Kinder des Knotens an Position k an den Positionen 2k und 2k + 1, sofern diese Indizes h¨ochstens n sind. Sind eine oder beide dieser Positionen gr¨osser als n, so hat der Knoten nur ein bzw. gar kein Kind. Diese Aussage kann man leicht per Induktion nach k beweisen, was wir den Lesern als ¨Ubung ¨uberlassen. Abbildung 2.4 zeigt die Darstellung des Heaps aus Abbildung 2.1 in einem Array. Mit dieser Darstellung lassen sich alle Heap-Operationen direkt im Array imple- mentieren. Beispielsweise l¨asst sich die Heap-Bedingung f¨ur den Knoten an Position k wie folgt testen: SatisfiesHeapCondition(k) Test der Heap-Bedingung 1 if 2k > n then return true ▷ A[k] ist ein Blatt 2 if 2k = n and A[2k] ≤ A[k] then return true ▷ A[k] hat nur ein Kind 3 if 2k < n and A[2k] ≤ A[k] and A[2k + 1] ≤ A[k] then return true ▷ A[k] hat zwei Kinder 4 return false 2.6.8 Vorteile und Nachteile von Heapsort Heapsort bietet gegen¨uber anderen Sortieralgorithmen wie Mergesort und Quicksort einige Vor- und Nachteile: Vorteile • Laufzeit O(n log n): Heapsort hat die gleiche asymptotische Laufzeit wie Mer- gesort und randomisiertes Quicksort. • In-place: Heapsort ben¨otigt keinen zus¨atzlichen Speicherplatz (ausser einer konstanten Anzahl von Hilfsvariablen). Nachteile • Schlechte Lokalit¨at: Heapsort springt im Speicher hin und her, da die Kin- der eines Knotens aus den tieferen Leveln im Speicher nicht in der N¨ahe des 24 Sortieren Abb. 2.5 Beispiel f¨ur einen Entscheidungsbaum eines vergleichsbasierten Sortieralgorith- mus. Elternknotens stehen. Dies kann die Performance des Algorithmus negativ be- einflussen, da der Zugriff auf benachbarte Speicherzellen in der Regel schneller ist als der Zugriff auf weit voneinander entfernten Speicherzellen. In der Praxis wird Heapsort daher seltener verwendet als Mergesort und Quick- sort. 2.7 Untere Schranke f¨ur vergleichsbasiertes Sortieren Wir haben nun drei Sortieralgorithmen kennengelernt – Mergesort, Quicksort und Heapsort –, die alle eine Laufzeit von O(n log n) erreichen. Die Frage ist nun, ob es einen Sortieralgorithmus gibt, der eine noch bessere Laufzeit erreicht. Unter der Annahme, dass der Sortieralgorithmus ausschliesslich auf Vergleichen basiert, ist die Antwort nein. Jeder vergleichsbasierte Sortieralgorithmus ben¨otigt im schlechtesten Fall mindestens Ω(n log n) viele Vergleiche. 2.7.1 Beweis Um diese Aussage zu beweisen, betrachten wir einen beliebigen vergleichsbasierten Sortieralgorithmus und stellen ihn als Entscheidungsbaum dar. Jeder Knoten des Entscheidungsbaums repr¨asentiert einen Vergleich von zwei Schl¨usseln, also eine if-Abfrage. Die beiden Kanten, die von einem Knoten ausgehen, entsprechen den beiden m¨oglichen Ergebnissen des Vergleichs (true oder false). Die Bl¨atter des Entscheidungsbaums entsprechen den m¨oglichen Permutationen des Eingabearrays. Eine Permutation ist eine bestimmte Anordnung der Elemente des Arrays. Beispielsweise sind (2, 1, 3) und (3, 1, 2) zwei verschiedene Permutationen des Arrays (1, 2, 3). Die Anzahl der Vergleiche, die der Algorithmus im schlechtesten Fall ausf¨uhrt, entspricht der H¨ohe des Entscheidungsbaums. Anzahl der Permutationen Ein Array mit n Elementen hat n! verschiedene Permutationen. Dies l¨asst sich wie folgt begr¨unden: F¨ur das erste Element des Arrays gibt es n m¨ogliche Positionen. F¨ur das zweite Element gibt es dann nur noch n − 1 m¨ogliche Positionen, da eine Position bereits belegt ist. F¨ur das dritte Element gibt es n − 2 m¨ogliche Positionen, und so weiter. Insgesamt erhalten wir n · (n − 1) · (n − 2) · · · 2 · 1 = n! m¨ogliche Anordnungen. H¨ohe des Entscheidungsbaums Da der Entscheidungsbaum f¨ur jede m¨ogliche Permutation des Eingabearrays min- destens ein Blatt enthalten muss, hat er mindestens n! Bl¨atter. Wir haben schon zuvor gesehen, dass ein bin¨arer Baum der H¨ohe h weniger als 2h+1 Knoten hat, also insbesondere weniger als 2h+1 Bl¨atter. 2.7 Untere Schranke f¨ur vergleichsbasiertes Sortieren 25 Aus diesen beiden Tatsachen folgt: 2 h+1 > n! ⇒ h > log2(n!) − 1. In einem fr¨uheren Teil der Vorlesung haben wir schon gezeigt, dass log2(n!) ∈ Ω(n log n) gilt. Alternativ kann man das aus der Stirling-Formel herleiten. Folglich ben¨otigt jeder vergleichsbasierte Sortieralgorithmus im schlechtesten Fall mindestens Ω(n log n) viele Vergleiche. 2.7.2 Nicht vergleichsbasiertes Sortieren: Bucketsort Die untere Schranke von Ω(n log n) gilt f¨ur alle vergleichsbasierten Sortierverfahren. Sie gilt sogar allgemeiner f¨ur alle Programme, die sich nur aufgrund von if-Abfragen verzweigen. Es ist auch sonst kein Sortierverfahren bekannt, dass f¨ur beliebige In- puts schneller als Θ(n log n) w¨are. Es gibt aber durchaus Sortierverfahren, die nicht vergleichsbasiert sind. Wir werden hier als Beispiel den Algorithmus Bucket-Sort skizzieren. Dazu nehmen wir an, dass das Inputarray A nicht beliebige ganze Zahlen enth¨alt, sondern nur Zahlen aus {1, 2, . . . , n}. Es darf Zahlen aber mehrfach enthal- ten. Ein solches Array k¨onnen wir wie folgt sortieren. Wir z¨ahlen, wie h¨aufig jede der Zahlen 1 bis n in A vorkommt. Dazu benutzen wir ein Array B aus n Variablen, wir z¨ahlen also in B[i], wie h¨aufig die Zahl i in A auftritt. Sobald wir diese Zahlen haben, f¨ullen wir das Array A von links nach rechts: Wir schreiben B[1]-mal den Wert 1 in A, dann B[2]-mal den Wert 2, und so weiter. Bucket-Sort(A[1..n]) ▷ alle Eintr¨age in {1, .., n} Bucket-Sort 1 B[1..n] ← [0, 0, . . . , 0] 2 for j ← 1, 2, . . . , n do 3 B[A[j]] ← B[A[j]] + 1 ▷ Z¨ahle in B[i], wie oft i vor- kommt 4 k ← 1 5 for i ← 1, 2, . . . , n do 6 A[k, . . . , k + B[i] − 1] ← [i, i, .., i] ▷ Schreibe B[i]-mal den Wert i in A 7 k ← k + i ▷ A ist bis Position k − 1 gef¨ullt. Bucket-Sort hat Laufzeit Θ(n), ist also schneller als Mergesort, Quicksort oder Heapsort. Andererseits funktioniert Bucket-Sort nicht f¨ur beliebige Arrays, sondern nur f¨ur Arrays mit Eintr¨agen aus {1, ..n}.8 Bucket-Sort ist nicht vergleichsbasiert und operiert fundamental anders als die bisherigen Algorithmen. Der entscheidende Unterschied ist in Zeile 3. Dort ¨andern wir die Speicherzelle mit Index A[j]. Da es n verschiedene M¨oglichkeiten f¨ur den Wert von A[j] gibt, kann diese einzelne Operation je nach Wert von A[j] zu n verschiedenen Ergebnissen f¨uhren, obwohl der Computer sie in Zeit O(1) ausf¨uhren kann. Im Entscheidungsbaum entspricht diese 8Man kann Bucket-Sort leicht auf Arrays mit Eintr¨agen aus {1, .., Cn} f¨ur eine Konstante C erweitern und erh¨alt immer noch Laufzeit Θ(n). Das bleibt aber immer noch eine starke Ein- schr¨ankung. Mit einer n-stelligen Bin¨arzahl kann man 2 n verschiedene Zahlen darstellen, und schon mit O(log n) vielen Stellen kann man Zahlen darstellen, die viel gr¨osser sind als O(n). 26 Sortieren Operation also einem Knoten mit n Kindern. Dagegen hat bei vergleichsbasierten Algorithmen jeder Knoten h¨ochstens Grad 2, da diese sich nur durch if-Abfragen verzweigen. Deshalb gilt die untere Schranke von oben nicht f¨ur Bucket-Sort.9 9Tats¨achlich m¨usste man f¨ur die untere Schranke auch ber¨ucksichtigen, dass es aufgrund des eingeschr¨ankten Inputs keine n! m¨oglichen Outputs gibt. Genauer gibt es nur ( 2n−1 n ) viele sortier- te Arrays mit Eintr¨agen aus {1, .., n}, die als Output in Frage kommen. Diese Zahl wird in der Kombinatorik als Zahl der Kombinationen mit Wiederholung genannt, der Prozess auch als Zie- hen mit Zur¨ucklegen ohne Ber¨ucksichtigung der Reihenfolge. Da log(( 2n−1 n ) ) = Θ(n), k¨onnen auch vergleichsbasierte Algrithmen unter solchen starken Voraussetzungen schneller sein.","libVersion":"0.3.2","langs":""}