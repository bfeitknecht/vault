{"path":"HS23/LinAlg/UE/s/LinAlg-u06-s.pdf","text":"D-INFK Linear Algebra HS 2023 Afonso Bandeira Bernd G¨artner Solution for Assignment 6 1. a) By using the elimination procedure on A we bring the matrix into reduced row echolon form R: A =   −1 2 5 −2 −3 3 12 −3 1 −14 −7 −6   →   1 −2 −5 2 0 −3 −3 3 0 −12 −2 −8   →   1 0 −3 0 0 1 1 −1 0 0 10 −20   →   1 0 0 −6 0 1 0 1 0 0 1 −2   =: R. Performing the same row operations on b as well yields b =   −6 −15 8   →   6 3 2   →   4 −1 −10   →   1 0 −1   =: c. From the lecture, we know that Ax = b ⇐⇒ Rx = c for all x ∈ R4. The only free variable is x4. In particular, we can rewrite our system as I3   x1 x2 x3   = c −   −6 1 −2   [x4] = c − x4   −6 1 −2   =   1 + 6x4 −x4 −1 + 2x4   where   −6 1 −2   =: F (so that we can compare with the explanation in the blackboard notes). There- fore, the full set of solutions is L = {     1 + 6x4 −x4 −1 + 2x4 x4     ∈ R4 : x4 ∈ R}. b) The nullspace of A contains the solutions to Ax = 0. Equivalently, these are the solutions to Rx = 0 with the R from the previous subtask. As above, we can rearrange this system to I3   x1 x2 x3   = 0 −   −6 1 −2   [x4] = −x4   −6 1 −2   =   6x4 −x4 2x4   where we again have [ −6 1 −2]⊤ = F . Following the blackboard notes, we can obtain one basis vector of N(A) from each free variable. In this case, we only have a single free variable. Setting it to 1 yields the solution x = [6 −1 2 1]⊤. We conclude that this single vector is a basis of N(A). In words, this means that every vector in N(A) can be obtained from this basis vector. Now consider the column space of A. By definition, it is spanned by the columns of A. In order to find a basis for it, we have to find an independent subset of these columns that still spans the same space. Equivalently, we have to find the CR decomposition of A. Luckily, we already found R. Moreover, we know from the lecture (Section 3.2.2 in the blackboard notes) that C can now be found by taking those columns in A that have a pivot in R. Concretely, in our case R has pivots in the first three columns. Hence, we get C =   −1 2 5 −3 3 12 1 −14 −7   . The columns v1 = [−1 −3 1]⊤ , v2 = [ 2 3 −14 ]⊤ , v3 = [5 12 −7 ]⊤ of C are a basis of C(A). c) Recall that the dimension of a subspace is the size of its basis. For N(A), we got 1 basis vector and hence N(A) has dimension 1. Similarly, C(A) has dimension 3. In particular, A has rank r = 3 and this allows us to calculate the dimensions of C(A⊤) and N(A⊤) using the formulas from the lecture as well: the dimension of C(A⊤) is r = 3 while the dimension of N(A⊤) is m − r = 0 where m is the number of rows of A. d) Recall that row operations preserve the row space and hence we have R(A) = R(R). Moreover, all rows of R are linearly independent by construction. Hence, the rows of R form a basis of R(A). Concretely, a basis of R(A) is given by the three vectors     1 0 0 −6     ,     0 1 0 1     ,     0 0 1 −2     ∈ R 4. 2. a) Let v1, v2 denote the columns of A. We can rewrite A [1 0 ] =   1 1 2   to 1v1 + 0v2 =   1 1 2   which immediately yields v1 = [ 1 1 2 ]⊤. Similarly, we get A [1 1 ] = 1v1 + 1v2 =   2 3 2   and hence v2 =   2 3 2   − v1 =   1 2 0   . We conclude that A =   1 1 1 2 2 0   . b) Observe that A has m = 3 rows, n = 2 columns, and rank r = 2 (since the two columns of A are linearly independent). From the lecture, we know that the dimension of C(A) is r = 2, the dimension of C(A⊤) is r = 2, the dimension of N(A) is n − r = 0, and the dimension of N(A⊤) is m − r = 1. 3. a) Let x ∈ Rn be arbitrary. We prove x ∈ R(A) ⇐⇒ x ∈ R(B) by arguing both directions separately. “ =⇒ ” Assume x ∈ R(A), i.e. there exist coefficients a1, . . . , am ∈ R such that x = a1v1 + · · · + amvm. Consider adding the two terms cajvi and −cajvi to x (we’re effectively adding 0). We then get x = x + cajvi − cajvi = (a1v1 + · · · + amvm) + cajvi − cajvi = a1v1 + · · · + ai−1vi−1 + (ai + caj)vi + ai+1vi+1 + . . . + aj−1vj−1 + aj(vj − cvi) + aj+1vj+1 + · · · + amvm where we assumed j > i (but the other case is symmetric). In particular, defining bk = ak for all k ∈ [m] \\ {i} and bi = ai + caj we get x = b1w1 + · · · + bmwm and therefore x ∈ R(B). “ ⇐= ” Assume x ∈ R(B), i.e. there exist coefficients b1, . . . , bm ∈ R such that x = b1w1 + · · · + bmwm. No new ideas are needed for this direction (i.e. we just reverse the argument above and in particular, one could prove both directions at once). Define ak = bk for all k ∈ [m] \\ {i} and ai = bi − cbj. We then get x = b1w1 + · · · + bmwm = (b1v1 + · · · + bmvm) − cbjvi = a1v1 + · · · + amvm and therefore x ∈ R(A). b) Concretely, switching rows i and j implies that vi = wj, vj = wi, and vk = wk for all k ∈ [m] \\ {i, j}. Let x ∈ Rn be arbitrary. If there exist coefficients a1, . . . , am ∈ R such that x = a1v1 + · · · + amvm, then the coefficients defined as bk = ak for all k ∈ [m] \\ {i, j}, bi = aj, and bj = ai, yield x = b1w1 + · · · + bmwm. The converse is also true, i.e. from coefficients b1, . . . , bm ∈ R with x = b1w1 + · · · + bmwm we can derive coefficients a1, . . . , am ∈ R with x = a1v1 + · · · + amvm by switching bi and bj. We conclude x ∈ R(A) if and only if x ∈ R(B) and hence R(A) = R(B). c) Concretely, we have vk = wk for all k ∈ [m] \\ {i} and wi = cvi. Now let x ∈ Rn be arbitrary. Recall that c ̸= 0. Thus, given coefficients a1, . . . , am ∈ R with x = a1v1 + · · · + amvm, we define bk = ak for k ∈ [m] \\ {i} and bi = ai c to get x = b1w1 + · · · + bmwm. Conversely, if we are given coefficients b1, . . . , bm ∈ R with x = b1w1 + · · · + bmwm, then defining ak = bk for k ∈ [m] \\ {i} and ai = cbi yields x = a1v1 + · · · + amvm. In other words, we have x ∈ R(A) if and only if x ∈ R(B) and hence R(A) = R(B). d) Recall that the elimination procedure uses exactly the three row operations from above, i.e. in each step we can either add a multiple of one row to another, switch two rows, or multiply a row with a scalar. As we proved above, each of these operations preserves the row space. Hence, any sequence of these operations also preserves the row space. We conclude R(A) = R(R). 4. We want to prove that U ∪ W is a subspace of V if and only if U ⊆ W or W ⊆ U . “ ⇐= ” If U ⊆ W , then U ∪ W = W is a subspace of V by assumption. The same reasoning applies in the case W ⊆ U . “ =⇒ ” We provide an indirect proof, i.e. assuming that we have U ̸⊆ W and W ̸⊆ U , we want to prove that U ∪ W is not a subspace of V . Consider arbitrary vectors u ∈ U \\ W and w ∈ W \\ U that must exist by assumption. Clearly, we have u, w ∈ U ∪ W . We claim that u + w /∈ U ∪ W which implies that U ∪ W is not closed under vector addition and hence it is not a subspace. Indeed, if we had u + w ∈ U ∪ W then either u + w ∈ U or u + w ∈ W . Assume first u + w ∈ U . Since U is a subspace containing both u and u + w we get (u + w) − u = w ∈ U since U must be closed under vector addition. But this is impossible by our assumption w ∈ W \\ U . Analogously, the case u + w ∈ W is impossible as well. Hence, u + w is not in U ∪ W , as desired. 5. Recall that the dimension of a subspace is defined as the size of a basis of that subspace. So to solve this exercise, it suffices to come up with a basis of Sn. We already saw a basis of S2 in the lecture. In particular, the three matrices [1 0 0 0 ] , [ 0 0 0 1 ] , [0 1 1 0 ] are such a basis. None of the matrices can be obtained from the others because each of the three matrices has a non-zero entry at a place where none of the other matrices has a non-zero entry (i.e. the three matrices are linearly independent). Moreover, every symmetric 2 × 2 matrix can be obtained as linear combination of those three matrices because it must have the form [a b b d ] = a [ 1 0 0 0 ] + b [ 0 1 1 0 ] + d [0 0 0 1 ] for some a, b, d ∈ R (i.e. the three matrices span all of S2). This idea generalizes to Sn. In particular, for i, j ∈ [n] with i ≤ j, define the n × n matrix B(ij) by B(ij) ℓk =    1 if ℓ = i and k = j 1 if ℓ = j and k = i 0 otherwise for all ℓ, k ∈ [n]. Observe that for i = j, B(ij) contains a single 1 on its diagonal and is zero everywhere else. For i < j, we find exactly two 1s in B(ij) and zeroes everywhere else. We claim that the set of matrices B = {B(ij) : i, j ∈ [n], i ≤ j} is a basis of Sn. We first check linear independence. Let i, j ∈ [n] with i ≤ j be arbitrary. Then B(ij) ij = 1 but none of the other matrices in the set has a non-zero entry at position (i, j). So B(ij) cannot be obtained as a linear combination of the other matrices. We conclude that set of matrices B is independent. Let now S ∈ Sn be an arbitrary symmetric n × n matrix. For all i, j ∈ [n], we must have Sij = Sji by symmetry. Thus, we can write S = ∑ i,j∈[n]:i≤j SijB(ij) and therefore we conclude that B spans all of Sn. Finally, observe that |B| = 1 + 2 + 3 + · · · + n = n(n+1) 2 . Hence, the dimension of Sn is n(n+1) 2 . 6. 1. Which of the following statements is true for all n × n matrices A? √ (a) N(A) = N(2A) Explanations: We know from the lecture that the nullspace of an n × n matrix is a subspace of Rn. In particular, any nullspace is closed under scalar multiplication. Therefore, N(A) = N(2A). (b) N(A) = N(A2) Explanations: Let x ∈ Rn be arbitrary. If we have Ax = 0, we also get A2x = 0. But the converse is not necessarily true. Consider the 2 × 2 matrix A = [ 0 1 0 0 ]. Clearly, there exists x ∈ R2 with Ax ̸= 0 and hence N(A) ̸= R2. But we have A2 = 0 and therefore N(A2) = R2. (c) N(A) = N(A + I) Explanations: Consider again the matrix A = [ 0 1 0 0 ]. The matrix A + I has rank 2 while the matrix A has rank 1. Therefore, their nullspaces cannot be the same. (d) N(A) = N(A⊤) Explanations: For A = [0 1 0 0 ] we get A⊤ = [0 0 1 0 ] . Now consider the standard unit vector e2 ∈ R2. We have Ae2 = e1 ̸= 0 and A⊤e2 = 0 and therefore N(A) ̸= N(A⊤). 2. Which of the following statements is true for all square matrices A? √ (a) C(A) = C(2A) Explanations: We know that the column space of an n × n matrix is a subspace of Rn. In particular, any column space is closed under scalar multiplication. Therefore, C(A) = C(2A). (b) C(A) = C(A2) Explanations: For A = [0 1 0 0 ] we get A2 = 0. As before, this implies that A has rank 1 while A2 has rank 0 and hence they have different column spaces (the dimensions of the column spaces must be different, so the spaces themselves must be different as well). (c) C(A) = C(A + I) Explanations: For A = [ 0 1 0 0 ] we get that A has rank 1 while A + I has rank 2. Therefore, the two column spaces must be different. (d) C(A) = C(A⊤) Explanations: The column space of A = [0 1 0 0 ] is spanned by the standard unit vector e1 while the column space of A⊤ is spanned by the standard unit vector e2. In particular, we have e1 ∈ C(A) but e1 /∈ C(A⊤). 3. The following equations each describe a plane in R3: x − y − z = 0 2x − 5y + 3z = 0 3x + 4z = 0. Which of the following statements is true? (a) The intersection of all three planes is empty. √ (b) The intersection of all three planes contains exactly one element. (c) The intersection of all three planes is a line. Explanations: For a point to be in the intersection of all three planes, it has to be a solution to all three equations. Thus, we want to understand the set of solutions of the linear system A   x y z   =   1 −1 −1 2 −5 3 3 0 4     x y z   = 0. As it turns out, A has full rank (i.e. its rank is 3) and hence this linear system has a unique solution (which is the zero-vector). Therefore, the intersection of all three planes contains exactly one element. 4. Consider the linear system x1 + (b − 1)x2 = 3 −3x1 − (2b − 8)x2 = −5 with variables x1, x2 and parameter b ∈ R. For which values of b is the set of solutions to the above system empty (i.e. there is no solution)? (a) Only for b = 0. √ (b) Only for b = −5. (c) For all possible values of b (i.e. for all of R). (d) The system always has a solution regardless of the value of b. Explanations: Adding the first equation 3 times to the second equation, we get (3b − 3 − 2b + 8)x2 = 4 and thus (b + 5)x2 = 4. This equation has a solution whenver b ̸= −5. But there is no solution if b = −5.","libVersion":"0.3.2","langs":""}