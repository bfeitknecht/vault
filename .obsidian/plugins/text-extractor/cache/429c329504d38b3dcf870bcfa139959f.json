{"path":"sem2/DDCA/VRL/extra/slides/DDCA-L21-GPU-SIMD-memory.pdf","text":"Digital Design & Computer Arch. Lecture 21: SIMD/GPU Architectures Frank K. Gürkaynak Mohammad Sadrosadati Prof. Onur Mutlu ETH Zürich Spring 2024 16 May 2024 Readings for this Week  Required  Lindholm et al., \"NVIDIA Tesla: A Unified Graphics and Computing Architecture,\" IEEE Micro 2008.  Recommended  Peleg and Weiser, “MMX Technology Extension to the Intel Architecture,” IEEE Micro 1996. 2 Exploiting Data Parallelism: SIMD Architectures and GPUs SIMD Processing: Exploiting Regular (Data) Parallelism Flynn’s Taxonomy of Computers  Mike Flynn, “Very High-Speed Computing Systems,” Proc. of IEEE, 1966  SISD: Single instruction operates on single data element  SIMD: Single instruction operates on multiple data elements  Array processor  Vector processor  MISD: Multiple instructions operate on single data element  Closest form: systolic array processor, streaming processor  MIMD: Multiple instructions operate on multiple data elements (multiple instruction streams)  Multiprocessor  Multithreaded processor 5 Flynn’s Taxonomy of Computers  Mike Flynn, “Very High-Speed Computing Systems,” Proc. of IEEE, 1966 6 MISD Example from Flynn 7Mike Flynn, “Very High-Speed Computing Systems,” Proc. of IEEE, 1966 Similar to a “generalized” systolic array Lecture 18b: Systolic Array Architectures https://www.youtube.com/watch?v=Ayo8uVPvjyw&list=PL5Q2soXY2Zi-EImKxYYY1SZuGiOAOBKaf&index=22 SIMD Example from Flynn 9Mike Flynn, “Very High-Speed Computing Systems,” Proc. of IEEE, 1966 Similar to an “array processor” Flynn’s Taxonomy of Computers  Mike Flynn, “Very High-Speed Computing Systems,” Proc. of IEEE, 1966  SISD: Single instruction operates on single data element  SIMD: Single instruction operates on multiple data elements  Array processor  Vector processor  MISD: Multiple instructions operate on single data element  Closest form: systolic array processor, streaming processor  MIMD: Multiple instructions operate on multiple data elements (multiple instruction streams)  Multiprocessor  Multithreaded processor 10 Data Parallelism  Concurrency arises from performing the same operation on different pieces of data  Single instruction multiple data (SIMD)  E.g., dot product of two vectors  Contrast with data flow  Concurrency arises from executing different operations in parallel (in a data driven manner)  Contrast with thread (“control”) parallelism  Concurrency arises from executing different threads of control in parallel  SIMD exploits operation-level parallelism on different data  Same operation concurrently applied to different pieces of data  A form of ILP where instruction happens to be the same across data 11 SIMD Processing Paradigm  Single instruction operates on multiple data elements  In time or in space  Multiple processing elements (PEs), i.e., execution units  Time-space duality  Array processor: Instruction operates on multiple data elements at the same time using different spaces (PEs)  Vector processor: Instruction operates on multiple data elements in consecutive time steps using the same space (PE) 12 Storing Multiple Data Elements: Vector Registers  Each vector data register holds N M-bit values  Each register stores a vector  Not a (single) scalar value as we saw before 13 V0,0 V0,1 V0,N-1 V1,0 V1,1 V1,N-1 V0 M-bit wide V1 V2 M-bit wide PE Array vs. Vector Processors 14 ARRAY PROCESSOR VECTOR PROCESSOR LD VR  A[3:0] ADD VR  VR, 1 MUL VR  VR, 2 ST A[3:0]  VR Instruction Stream Time LD0 LD1 LD2 LD3 AD0 AD1 AD2 AD3 MU0 MU1 MU2 MU3 ST0 ST1 ST2 ST3 LD0 LD1 AD0 LD2 AD1 MU0 LD3 AD2 MU1 ST0 AD3 MU2 ST1 MU3 ST2 ST3 Space Space Same op @ same time Different ops @ same space Different ops @ time Same op @ space SIMD Array Processing vs. VLIW  VLIW: Multiple independent operations packed together into a “long inst.” 15 SIMD Array Processing vs. VLIW  Array processor: Single operation on multiple (different) data elements 16 Lecture 18a: VLIW Architectures https://www.youtube.com/watch?v=Ayo8uVPvjyw&list=PL5Q2soXY2Zi-EImKxYYY1SZuGiOAOBKaf&index=22 Vector Processors (I)  A vector is a one-dimensional array of numbers  Many scientific/commercial programs use vectors for (i = 0; i<=49; i++) C[i] = (A[i] + B[i]) / 2  A vector processor is one whose instructions operate on vectors rather than scalar (single data) values  Basic requirements  Need to load/store vectors  vector registers (contain vectors)  Need to operate on vectors of different lengths  vector length register (VLEN)  Elements of a vector might be stored apart from each other in memory  vector stride register (VSTR)  Stride: distance in memory between two elements of a vector 18  A and B matrices, both stored in memory in row-major order  Load A’s row 0 (A0 through A5) into vector register V1  Each time, increment address by 1 to access the next column  Accesses have a stride of 1  Load B’s column 0 (B0 through B50) into vector register V2  Each time, increment address by 10 to access the next row  Accesses have a stride of 10 Vector Stride Example: Matrix Multiply A4x6 B6x10 → C4x10 Dot product of each row vector of A with each column vector of B A Linear Memory B 0 1 2 3 4 5 6 0 1 2 3 4 5 6 7 8 9 10 Vector Processors (II)  A vector instruction performs an operation on each element in consecutive cycles  Vector functional units are pipelined  Each pipeline stage operates on a different data element  Vector instructions allow deeper pipelines  No intra-vector dependencies  no hardware interlocking needed within a vector  No control flow within a vector  Known stride allows easy address calculation for all vector elements  Enables easy loading (or even early loading, i.e., prefetching) of vectors into registers/cache/memory 20 Vector Processor Advantages + No dependencies within a vector  Pipelining & parallelization work really well  Can have very deep pipelines (without the penalty of deep pipelines) + Each instruction generates a lot of work (i.e., operations)  Reduces instruction fetch bandwidth requirements  Amortizes instruction fetch and control overhead over many data --> Leads to high energy efficiency per operation + No need to explicitly code loops  Fewer branches in the instruction sequence + Highly regular memory access pattern 21 Vector Processor Disadvantages -- Works (only) if parallelism is regular (data/SIMD parallelism) ++ Vector operations -- Very inefficient if parallelism is irregular -- How about searching for a key in a linked list? 22Fisher, “Very Long Instruction Word architectures and the ELI-512,” ISCA 1983. Recommended Paper: VLIW 23Fisher, “Very Long Instruction Word architectures and the ELI-512,” ISCA 1983. Amdahl’s Law  Amdahl’s Law  f: Parallelizable fraction of a program  N: Number of processors  Amdahl, “Validity of the single processor approach to achieving large scale computing capabilities,” AFIPS 1967.  Maximum speedup limited by serial portion: Serial bottleneck  All parallel machines “suffer from” the serial bottleneck 24 Speedup = 1 +1 - f f N Recommended Paper: Amdahl’s Law 25 Lecture on Parallelism & Heterogeneity I 26https://youtu.be/GLzG_rEDn9A?t=7211 Lecture on Parallelism & Heterogeneity II 27https://youtu.be/P8l3SMAbyYw Vector Processor Limitations -- Memory (bandwidth) can easily become a bottleneck, especially if 1. compute/memory operation balance is not maintained 2. data is not mapped appropriately to memory banks 28 Vector Processing in More Depth Vector Registers  Each vector data register holds N M-bit values  Vector control registers: VLEN, VSTR, VMASK  Maximum VLEN can be N  Maximum number of elements stored in a vector register  Vector Mask Register (VMASK)  Indicates which elements of vector to operate on  Set by vector test instructions  e.g., VMASK[i] = (Vk[i] == 0) 30 V0,0 V0,1 V0,N-1 V1,0 V1,1 V1,N-1 M-bit wide M-bit wide Vector Functional Units  Use a deep pipeline to execute element operations  fast clock cycle  Control of deep pipeline is simple because elements in vector are independent 31 V 1 V 2 V 3 V1 * V2  V3 Six stage multiply pipeline Slide credit: Krste Asanovic Vector Machine Organization (CRAY-1)  CRAY-1  Russell, “The CRAY-1 computer system,” CACM 1978.  Scalar and vector modes  8 64-element vector registers  64 bits per element  16 memory banks  8 64-bit scalar registers  8 24-bit address registers 32 Recommended Paper Russell, “The CRAY-1 computer system,” CACM 1978. 33 CRAY X-MP-28 @ ETH (CAB, E Floor) 34 CRAY X-MP System Organization 35 Cray Research Inc., “The CRAY X-MP Series of Computer Systems,” 1985 CRAY X-MP Design Detail 36 Cray Research Inc., “The CRAY X-MP Series of Computer Systems,” 1985 CRAY X-MP CPU Functional Units 37 Cray Research Inc., “The CRAY X-MP Series of Computer Systems,” 1985 CRAY X-MP System Configuration 38 Cray Research Inc., “The CRAY X-MP Series of Computer Systems,” 1985 Seymour Cray, Leader in Supercomputer Design 39 \"If you were plowing a field, which would you rather use: Two strong oxen or 1024 chickens?\" © amityrebecca / Pinterest. https://www.pinterest.ch/pin/473018767088408061/ © Scott Sinklier / Corbis. http://america.aljazeera.com/articles/2015/2/20/the-short-brutal-life-of-male-chickens.html https://en.wikipedia.org/wiki/Seymour_Cray Vector Machine Organization (CRAY-1)  CRAY-1  Russell, “The CRAY-1 computer system,” CACM 1978.  Scalar and vector modes  8 64-element vector registers  64 bits per element  16 memory banks  8 64-bit scalar registers  8 24-bit address registers 40 Loading/Storing Vectors from/to Memory  Requires loading/storing multiple elements  Elements separated from each other by a constant distance (stride)  Assume stride = 1 for now  Elements can be loaded in consecutive cycles if we can start the load of one element per cycle  Can sustain a throughput of one element per cycle  Question: How do we achieve this with a memory that takes more than 1 cycle to access?  Answer: Bank the memory; interleave the elements across banks 41 Memory Banking  Memory is divided into banks that can be accessed independently; banks share address and data buses (to reduce memory chip pins)  Can start and complete one bank access per cycle  Can sustain N concurrent accesses if all N go to different banks 42 Bank 0 Bank 1 MDR MAR Bank 2 Bank 15 MDR MAR MDR MAR MDR MAR Data bus Address bus CPU Picture credit: Derek Chiou Vector Memory System  Next address = Previous address + Stride  If (stride == 1) && (consecutive elements interleaved across banks) && (number of banks >= bank latency), then  we can sustain 1 element/cycle throughput 43 0 1 2 3 4 5 6 7 8 9 A B C D E F + Base Stride Vector Registers Memory Banks Address Generator Picture credit: Krste Asanovic Scalar Code Example: Element-Wise Avg.  For I = 0 to 49  C[i] = (A[i] + B[i]) / 2  Scalar code (instruction and its latency in clock cycles) MOVI R0 = 50 1 MOVA R1 = A 1 MOVA R2 = B 1 MOVA R3 = C 1 X: LD R4 = MEM[R1++] 11 ;autoincrement addressing LD R5 = MEM[R2++] 11 ADD R6 = R4 + R5 4 SHFR R7 = R6 >> 1 1 ST MEM[R3++] = R7 11 DECBNZ R0, X 2 ;decrement and branch if NZ 44 304 dynamic instructions Scalar Code Execution Time (In Order) 45  Scalar execution time on an in-order processor with 1 bank  First two loads in the loop cannot be pipelined: 2*11 cycles  4 + 50*40 = 2004 cycles  Scalar execution time on an in-order processor with 1 bank with 2 memory ports (two different memory accesses can be serviced concurrently) OR 2 banks (where arrays B and C are stored in different banks)  First two loads in the loop can be pipelined: 1 + 11 cycles  4 + 50*30 = 1504 cycles Vectorizable Loops  A loop is vectorizable if each iteration is independent of any other  For I = 0 to 49  C[i] = (A[i] + B[i]) / 2  Vectorized loop (each instruction and its latency): MOVI VLEN = 50 1 MOVI VSTR = 1 1 VLD V0 = A 11 + VLEN – 1 VLD V1 = B 11 + VLEN – 1 VADD V2 = V0 + V1 4 + VLEN – 1 VSHFR V3 = V2 >> 1 1 + VLEN – 1 VST C = V3 11 + VLEN – 1 46 7 dynamic instructions Basic Vector Code Performance  Assume no chaining (no vector data forwarding)  i.e., output of a vector functional unit cannot be used as the direct input of another  The entire vector register needs to be ready before any element of it can be used as part of another operation  1 memory port (one address generator) per bank  16 memory banks (word-interleaved: consecutive elements of an array are stored in consecutive banks)  285 cycles 47 1 1 11 49 11 49 4 49 1 49 11 49 V0 = A[0..49] V1 = B[0..49] ADD SHIFT STORE VLD V0=A VLD V1=B VADD V2=V0+V1 VSHFR V3=V2>>1 VST C=V3 Basic Vector Code Performance (II)  Why 16 banks?  11-cycle memory access latency  Having 16 (>11) banks ensures there are enough banks to overlap enough memory operations to cover memory latency  The above assumes a unit stride (i.e., stride = 1)  Correct for our example program  What if stride > 1?  How do you ensure we can access 1 element per cycle when memory latency is 11 cycles? 48 Recall: Vector Memory System  Next address = Previous address + Stride  If (stride == 1) && (consecutive elements interleaved across banks) && (number of banks >= bank latency), then  we can sustain 1 element/cycle throughput 49 0 1 2 3 4 5 6 7 8 9 A B C D E F + Base Stride Vector Registers Memory Banks Address Generator Picture credit: Krste Asanovic Vector Chaining  Vector chaining: Data forwarding from one vector functional unit to another 50 Memory V 1 Load Unit Mult. V 2 V 3 Chain Add V 4 V 5 Chain LV v1 MULV v3,v1,v2 ADDV v5, v3, v4 Slide credit: Krste Asanovic Vector Code Performance - Chaining  Vector chaining: Data forwarding from one vector functional unit to another  182 cycles 51 1 1 11 49 11 49 4 49 1 49 11 49 These two VLDs cannot be pipelined. WHY? VLD and VST cannot be pipelined. WHY? Strict assumption: Each memory bank has a single port (memory bandwidth bottleneck) VLD V0=A VLD V1=B VADD V2=V0+V1 VSHFR V3=V2>>1 VST C=V3 Vector Code Performance – Multiple Memory Ports  Chaining and 2 load ports, 1 store port in each bank  79 cycles  19X perf. improvement! 52 1 1 11 49 4 49 1 49 11 49 11 491 VLD V0=A VLD V1=B VADD V2=V0+V1 VSHFR V3=V2>>1 VST C=V3 Conditional Operations in a Loop  What if some operations should not be executed on a vector (based on a dynamically-determined condition)? loop: for (i=0; i<N; i++) if (a[i] != 0) then b[i]=a[i]*b[i]  Idea: Masked operations  VMASK register is a bit mask determining which data element should not be acted upon VLD V0 = A VLD V1 = B VMASK = (V0 != 0) VMUL V1 = V0 * V1 VST B = V1  This is predicated execution. Execution is predicated on mask bit. 53 Another Example with Masking 54 for (i = 0; i < 64; ++i) if (a[i] >= b[i]) c[i] = a[i] else c[i] = b[i] A B VMASK 1 2 0 2 2 1 3 2 1 4 10 0 -5 -4 0 0 -3 1 6 5 1 -7 -8 1 Steps to execute the loop in SIMD code 1. Compare A, B to get VMASK 2. Masked store of A into C 3. Complement VMASK 4. Masked store of B into C Masked Vector Instructions 55 C[4] C[5] C[1] Write data port A[7] B[7] M[3]=0 M[4]=1 M[5]=1 M[6]=0 M[2]=0 M[1]=1 M[0]=0 M[7]=1 Density-Time Implementation – scan mask vector and only execute elements with non-zero masks C[1] C[2] C[0] A[3] B[3] A[4] B[4] A[5] B[5] A[6] B[6] M[3]=0 M[4]=1 M[5]=1 M[6]=0 M[2]=0 M[1]=1 M[0]=0 Write data portWrite Enable A[7] B[7]M[7]=1 Simple Implementation – execute all N operations, turn off result writeback according to mask Slide credit: Krste Asanovic Which one is better? Tradeoffs? Some Issues  Stride and bank count  As long as stride and bank count are relatively prime to each other and there are enough banks to cover bank access latency, we can sustain 1 element/cycle throughput  Storage format of a matrix  Row major: Consecutive elements in a row are laid out consecutively in memory  Column major: Consecutive elements in a column are laid out consecutively in memory  You need to change the stride when accessing a row versus column 56  A and B matrices, both stored in memory in row-major order  Load A’s row 0 into vector register V1  Each time, increment address by 1 to access the next column  Accesses have a stride of 1  Load B’s column 0 into vector register V2  Each time, increment address by 10  Accesses have a stride of 10 Bank Conflicts in Matrix Multiplication 57 A4x6 B6x10 → C4x10 Dot product of each row vector of A with each column vector of B Different strides can lead to bank conflicts How do we minimize them? Minimizing Bank Conflicts  More banks  More ports in each bank  Better data layout to match the access pattern  Is this always possible?  Better mapping of address to bank  E.g., randomized mapping  Rau, “Pseudo-randomly interleaved memory,” ISCA 1991. 58 Recommended Reading: Minimizing Bank Conflicts 59Rau, “Pseudo-randomly Interleaved Memory,” ISCA 1991. Array vs. Vector Processors, Revisited  Array vs. vector processor distinction is a “purist’s” distinction  Most “modern” SIMD processors are a combination of both  They exploit data parallelism in both time and space  GPUs are a prime example we will cover in more detail 60 GPUs (Graphics Processing Units) GPUs are SIMD Engines Underneath  The instruction pipeline operates like a SIMD pipeline (e.g., an array processor)  However, the programming is done using threads, NOT SIMD instructions  To understand this, let’s go back to our parallelizable code example  But, before that, let’s distinguish between  Programming Model (Software) vs.  Execution Model (Hardware) 62 Programming Model vs. Hardware Execution Model  Programming Model refers to how the programmer expresses the code  E.g., Sequential (von Neumann), Data Parallel (SIMD), Dataflow, Multi-threaded (MIMD, SPMD), …  Execution Model refers to how the hardware executes the code underneath  E.g., Out-of-order execution, Vector processor, Array processor, Dataflow processor, Multiprocessor, Multithreaded processor, …  Execution Model can be very different from the Programming Model  E.g., von Neumann model implemented by an OoO processor  E.g., SPMD model implemented by a SIMD processor (a GPU) 63 How Can You Exploit Parallelism Here? 64 for (i=0; i < N; i++) C[i] = A[i] + B[i]; load load add store load load add store Iter. 1 Iter. 2 Scalar Sequential Code Let’s examine three programming options to exploit instruction-level parallelism present in this sequential code: 1. Sequential (SISD) 2. Data-Parallel (SIMD) 3. Multithreaded (MIMD/SPMD) Prog. Model 1: Sequential (SISD) 65 load load add store load load add store Iter. 1 Iter. 2 Scalar Sequential Code  Can be executed on a:  Pipelined processor  Out-of-order execution processor  Independent instructions executed when ready  Different iterations are present in the instruction window and can execute in parallel in multiple functional units  In other words, the loop is dynamically unrolled by the hardware  Superscalar or VLIW processor  Can fetch and execute multiple instructions per cycle for (i=0; i < N; i++) C[i] = A[i] + B[i]; load load add store load load add store Iter. 1 Iter. 2 Scalar Sequential Code Prog. Model 2: Data Parallel (SIMD) 66 for (i=0; i < N; i++) C[i] = A[i] + B[i]; Vector Instruction load load add store load load add store Iter. 1 Iter. 2 Vectorized Code Realization: Each iteration is independent Idea: Programmer or compiler generates a SIMD instruction to execute the same instruction from all iterations across different data Best executed by a SIMD processor (vector, array) VLD A  V1 VLD B  V2 VADD V1 + V2  V3 VST V3  C load load add store load load add store Iter. 1 Iter. 2 Scalar Sequential Code Prog. Model 3: Multithreaded 67 for (i=0; i < N; i++) C[i] = A[i] + B[i]; load load add store load load add store Iter. 1 Iter. 2 Realization: Each iteration is independent Idea: Programmer or compiler generates a thread to execute each iteration. Each thread does the same thing (but on different data) Can be executed on a MIMD machine Prog. Model 3: Multithreaded 68 for (i=0; i < N; i++) C[i] = A[i] + B[i]; load load add store load load add store Iter. 1 Iter. 2 Realization: Each iteration is independent Idea: Programmer or compiler generates a thread to execute each iteration. Each thread does the same thing (but on different data) Can be executed on a MIMD machine This particular model is also called: SPMD: Single Program Multiple Data Can be executed on a SIMD machineCan be executed on a SIMT machine Single Instruction Multiple Thread A GPU is a SIMD (SIMT) Machine  Except it is not programmed using SIMD instructions  It is programmed using threads (SPMD programming model)  Each thread executes the same code but operates a different piece of data  Each thread has its own context (i.e., can be treated/restarted/executed independently)  A set of threads executing the same instruction are dynamically grouped into a warp (wavefront) by the hardware  A warp is essentially a SIMD operation formed by hardware! 69 Warp 0 at PC X+3 Warp 0 at PC X+2 Warp 0 at PC X+1 SPMD on SIMT Machine 70 for (i=0; i < N; i++) C[i] = A[i] + B[i]; load load add store load load add store Iter. 1 Iter. 2 Realization: Each iteration is independent Idea: Programmer or compiler generates a thread to execute each iteration. Each thread does the same thing (but on different data) Can be executed on a MIMD machine This particular model is also called: SPMD: Single Program Multiple Data Can be executed on a SIMD machineA GPU executes it using the SIMT model: Single Instruction Multiple Thread Warp 0 at PC X Warp: A set of threads that execute the same instruction (i.e., at the same PC) Graphics Processing Units SIMD not Exposed to Programmer (SIMT) SIMD vs. SIMT Execution Model  SIMD: A single sequential instruction stream of SIMD instructions  each instruction specifies multiple data inputs  [VLD, VLD, VADD, VST], VLEN  SIMT: Multiple instruction streams of scalar instructions  threads grouped dynamically into warps  [LD, LD, ADD, ST], NumThreads  Two Major SIMT Advantages:  Can treat each thread separately  i.e., can execute each thread independently (on any type of scalar pipeline)  MIMD processing  Can group threads into warps flexibly  i.e., can group threads that are supposed to truly execute the same instruction  dynamically obtain and maximize benefits of SIMD processing 72 Fine-Grained Multithreading of Warps 73 for (i=0; i < N; i++) C[i] = A[i] + B[i]; load load add store load load add store Iter. 0 Iter. 1 Warp 0 at PC X  Assume a warp consists of 32 threads  If you have 32K iterations, and 1 iteration/thread  1K warps  Warps can be interleaved on the same pipeline  Fine grained multithreading of warps Warp 1 at PC X Iter. 32 Iter. 33 Warp 20 at PC X+2 Iter. 20*32 Iter. 20*32 + 1 Fine-Grained Multithreading of Warps 74 for (i=0; i < N; i++) C[i] = A[i] + B[i]; load load add store load load add store Iterations 0-31 Warp 0 at PC X+3  Assume a warp consists of 32 threads  If you have 32K iterations, and 1 iteration/thread  1K warps  Warps can be interleaved on the same pipeline  Fine grained multithreading of warps Warp 1 at PC X+2 Warp 2 at PC X+1 Warp 3 at PC X Iterations 32-63 Iterations 64-95 Iterations 96-127 All threads in a warp are independent of each other  They be executed seamlessly in a fine-grained multithreaded pipeline Fine-Grained Multithreading: Basic Idea SignImmE CLK A RD Instruction Memory+ 4 A1 A3 WD3 RD2 RD1 WE3 A2 CLK Sign Extend Register File 0 1 0 1 A RD Data Memory WD WE 0 1 PCF0 1 PC' InstrD 25:21 20:16 15:0 5:0 SrcBE 20:16 15:11 RtE RdE <<2+ ALUOutM ALUOutW ReadDataW WriteDataE WriteDataM SrcAE PCPlus4D PCBranchM WriteRegM4:0 ResultW PCPlus4EPCPlus4F 31:26 RegDstD BranchD MemWriteD MemtoRegD ALUControlD ALUSrcD RegWriteD Op Funct Control Unit ZeroM PCSrcM CLK CLK CLK CLK CLK WriteRegW4:0 ALUControlE2:0ALU RegWriteE RegWriteM RegWriteW MemtoRegE MemtoRegM MemtoRegW MemWriteE MemWriteM BranchE BranchM RegDstE ALUSrcE WriteRegE4:0 Each pipeline stage has an instruction from a different, completely-independent thread We need a PC and a register file for each thread + muxes and control Lecture on Fine-Grained Multithreading 76https://youtu.be/XaW_O9nKPe0?t=5070 Warps and Warp-Level FGMT  Warp: A set of threads that execute the same instruction (on different data elements)  SIMT (Nvidia-speak)  All threads run the same code  Warp: The threads that run lengthwise in a woven fabric … Thread Warp 3 Thread Warp 8 Thread Warp 7 Thread Warp Scalar Thread W Scalar Thread X Scalar Thread Y Scalar Thread Z Common PC SIMD Pipeline Lindholm et al., \"NVIDIA Tesla: A Unified Graphics and Computing Architecture,\" IEEE Micro 2008. High-Level View of a GPU Lindholm et al., \"NVIDIA Tesla: A Unified Graphics and Computing Architecture,\" IEEE Micro 2008. Latency Hiding via Warp-Level FGMT  Warp: A set of threads that execute the same instruction (on different data elements)  Fine-grained multithreading  One instruction per thread in pipeline at a time (No interlocking)  Interleave warp execution to hide latencies  Register values of all threads stay in register file  FGMT enables simple pipeline & long latency tolerance  Millions of threads operating on the same large image/video 79 DecodeRFRFRFALUALUALU D-Cache Thread Warp 6 Thread Warp 1 Thread Warp 2DataAll Hit? Miss? Warps accessing memory hierarchy Thread Warp 3 Thread Warp 8 Writeback Warps available for scheduling Thread Warp 7 I-Fetch SIMD Pipeline Slide credit: Tor Aamodt  Same instruction in different threads uses thread id to index and access different data elements SIMT Memory Access (Loads and Stores) Let’s assume N=16, 4 threads per warp  4 warps 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15+ + + + + Slide credit: Hyesoon Kim Threads Data elements Warp 0 Warp 1 Warp 2 Warp 3 80 For maximum performance, memory should provide enough bandwidth (i.e., elements per cycle throughput to match computation unit throughput)  CPU threads and GPU kernels  Sequential or modestly parallel sections on CPU  Massively parallel sections on GPU: Blocks of threads Serial Code (host) . . . . . . Parallel Kernel (device) KernelA<<<nBlk, nThr>>>(args); Serial Code (host) Parallel Kernel (device) KernelB<<<nBlk, nThr>>>(args); Warps not Exposed to GPU Programmers 81 Slide credit: Hwu & Kirk Sample GPU SIMT Code (Simplified) for (ii = 0; ii < 100000; ++ii) { C[ii] = A[ii] + B[ii]; } // there are 100000 threads __global__ void KernelFunction(…) { int tid = blockDim.x * blockIdx.x + threadIdx.x; int varA = aa[tid]; int varB = bb[tid]; C[tid] = varA + varB; } CPU code CUDA code Slide credit: Hyesoon Kim 82 Sample GPU Program (Less Simplified) 83Slide credit: Hyesoon Kim Lecture on GPU Programming 84https://youtu.be/AkYnuqVpCug Heterogeneous Systems Course (Spring 2022) https://safari.ethz.ch/projects_and_seminars/spring2022/doku.php ?id=heterogeneous_systems https://youtube.com/playlist?list=PL5Q2soXY2Zi9XrgXR38IM_FTjmY6h7Gzm  Short weekly lectures  Hands-on projects 85 Heterogeneous Systems Course (Spring 2023) https://safari.ethz.ch/projects_and_seminars/spring2023/doku.php ?id=heterogeneous_systems https://www.youtube.com/watch?v=8JGo2zylE80&list=PL5Q2soXY2Zi- qSKahS4ofaEwYl7_qp9mw  Short weekly lectures  Hands-on projects From Blocks to Warps  GPU core: A SIMD pipeline  Streaming Processor (SP)  Many such SIMD Processors  Streaming Multiprocessor (SM)  Blocks are divided into warps  SIMD/SIMT unit (32 threads) … t0 t1 t2 … t31 … … t0 t1 t2 … t31 … Block 0’s warps Block 1’s warps … t0 t1 t2 … t31 … Block 2’s warps 87 NVIDIA Fermi architecture SIMD vs. SIMT Execution Model  SIMD: A single sequential instruction stream of SIMD instructions  each instruction specifies multiple data inputs  [VLD, VLD, VADD, VST], VLEN  SIMT: Multiple instruction streams of scalar instructions  threads grouped dynamically into warps  [LD, LD, ADD, ST], NumThreads  Two Major SIMT Advantages:  Can treat each thread separately  i.e., can execute each thread independently on any type of scalar pipeline  MIMD processing  Can group threads into warps flexibly  i.e., can group threads that are supposed to truly execute the same instruction  dynamically obtain and maximize benefits of SIMD processing 88 Threads Can Take Different Paths in Warp-based SIMD  Each thread can have conditional control flow instructions  Threads can execute different control flow paths 89 Thread Warp Common PC Thread 2 Thread 3 Thread 4 Thread 1 B C D E F A G Slide credit: Tor Aamodt Control Flow Problem in GPUs/SIMT  A GPU uses a SIMD pipeline to save area on control logic  Groups scalar threads into warps  Branch divergence occurs when threads inside warps branch to different execution paths 90 Branch Path A Path B Branch Path A Path B Slide credit: Tor Aamodt This is the same as conditional/predicated/masked execution. Recall the Vector Mask and Masked Vector Operations Remember: Each Thread Is Independent  Two Major SIMT Advantages:  Can treat each thread separately  i.e., can execute each thread independently on any type of scalar pipeline  MIMD processing  Can group threads into warps flexibly  i.e., can group threads that are supposed to truly execute the same instruction  dynamically obtain and maximize benefits of SIMD processing  If we have many threads  We can find individual threads that are at the same PC  And, group them together into a single warp dynamically  This reduces “divergence”  improves SIMD utilization  SIMD utilization: fraction of SIMD lanes executing a useful operation (i.e., executing an active thread) 91 Dynamic Warp Formation/Merging  Idea: Dynamically merge threads executing the same instruction, i.e., at the same PC (after branch divergence)  Form new warps from warps that are waiting  Enough threads branching to each path enables the creation of full new warps 92 Warp X Warp Y Warp Z Dynamic Warp Formation/Merging  Idea: Dynamically merge threads executing the same instruction, i.e., at the same PC (after branch divergence)  Fung et al., “Dynamic Warp Formation and Scheduling for Efficient GPU Control Flow,” MICRO 2007. 93 Branch Path A Path B Branch Path A Analyzing GPUs is Fun! 94 https://safari.ethz.ch/digitaltechnik/spring2023/lib/exe/fetch.php?media=digitaltechnik-s22-en.pdf Analyzing GPUs is Fun! 95 https://safari.ethz.ch/digitaltechnik/spring2023/lib/exe/fetch.php?media=digitaltechnik-s22-en.pdf An Example GPU NVIDIA GeForce GTX 285  NVIDIA-speak:  240 stream processors  “SIMT execution”  Generic speak:  30 cores  8 SIMD functional units per core  NVIDIA, “NVIDIA GeForce GTX 200 GPU. Architectural Overview. White Paper,” 2008. Slide credit: Kayvon Fatahalian 97 NVIDIA GeForce GTX 285 Core … = instruction stream decode= SIMD functional unit, control shared across 8 units = execution context storage = multiply-add = multiply 64 KB of storage for thread contexts (registers) Slide credit: Kayvon Fatahalian 98 NVIDIA GeForce GTX 285 Core … 64 KB of storage for thread contexts (registers)  Groups of 32 threads share instruction stream (each group is a Warp): they execute the same instruction on different data  Up to 32 warps are interleaved in an FGMT manner  Up to 1024 thread contexts can be stored Slide credit: Kayvon Fatahalian NVIDIA GeForce GTX 285 Tex Tex Tex Tex Tex Tex Tex Tex Tex Tex … … … ……… ……… ……… ……… ……… ……… ……… ……… ……… 30 cores on the GTX 285: 30,720 threads Slide credit: Kayvon Fatahalian 100 0.0 5000.0 10000.0 15000.0 20000.0 25000.0 0 1000 2000 3000 4000 5000 6000 7000 8000 GTX 285 (2009) GTX 480 (2010) GTX 780 (2013) GTX 980 (2014) P100 (2016) V100 (2017) A100 (2020)GFLOPS#Functional Units Functional units (stream processors) GFLOPS Evolution of NVIDIA GPUs 101 NVIDIA V100  NVIDIA-speak:  5120 stream processors  “SIMT execution”  Generic speak:  80 cores  64 SIMD functional units per core  Tensor cores for Machine Learning  NVIDIA, “NVIDIA Tesla V100 GPU Architecture. White Paper,” 2017. 102 NVIDIA V100 Block Diagram 80 cores on the V100 https://devblogs.nvidia.com/inside-volta/ 103 NVIDIA V100 Core 15.7 TFLOPS Single Precision 7.8 TFLOPS Double Precision 125 TFLOPS for Deep Learning (Tensor cores) 104 https://devblogs.nvidia.com/inside-volta/ Tensor Core Microarchitecture (Volta)  Each warp utilizes two tensor cores  Each tensor core contains two “octets”  16 SIMD units per tensor core (8 per octet)  4x4 matrix-multiply and accumulate each cycle per tensor core 105* M. A. Raihan, N. Goli and T. M. Aamodt, \"Modeling Deep Learning Accelerator Enabled GPUs,\" ISPASS 2019. Proposed* tensor core microarchitecture SIMD unit Unlike conventional SIMD, register contents are not private to each thread, but shared inside the warp Edge TPU: Baseline Accelerator DRAM ML Model PE ArrayBuffer Dataflow 64x64 array 2TFLOP/s 4MB on-chip buffer Output ActivationParameter Input Activation =* 106 Introduction TPU and Model Characterization Mensa Framework Mensa-G Evaluation Conclusion ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● Research Lecture on Edge TPU 107https://youtu.be/KPPfRRPENgQ?t=2999 Lecture 18b: Systolic Array Architectures https://www.youtube.com/watch?v=Ayo8uVPvjyw&list=PL5Q2soXY2Zi-EImKxYYY1SZuGiOAOBKaf&index=22 NVIDIA A100  NVIDIA-speak:  6912 stream processors  “SIMT execution”  Generic speak:  108 cores  64 SIMD functional units per core  Tensor cores for Machine Learning  Support for sparsity  New floating point data type (TF32)  https://developer.nvidia.com/blog/nvidia-ampere-architecture-in-depth/ 109 NVIDIA A100 Block Diagram 108 cores on the A100 (Up to 128 cores in the full-blown chip) 40MB L2 cache https://developer.nvidia.com/blog/nvidia-ampere-architecture-in-depth/ 110 NVIDIA A100 Core 19.5 TFLOPS Single Precision 9.7 TFLOPS Double Precision 312 TFLOPS for Deep Learning (Tensor cores) 111 https://developer.nvidia.com/blog/nvidia-ampere-architecture-in-depth/ 0 10000 20000 30000 40000 50000 60000 0 2000 4000 6000 8000 10000 12000 14000 16000 GTX 285 (2009) GTX 480 (2010) GTX 780 (2013) GTX 980 (2014) P100 (2016) V100 (2017) A100 (2020) H100 (2022)GFLOPS#Functional Units Functional Units (Stream Processors) GFLOPS Evolution of NVIDIA GPUs (Updated) 112 NVIDIA H100 Block Diagram 144 cores on the full GH100 60MB L2 cache https://developer.nvidia.com/blog/nvidia-hopper-arc hitecture-in-dept h/ 113 NVIDIA H100 Core 48 TFLOPS Single Precision* 24 TFLOPS Double Precision* 800 TFLOPS (FP16, Tensor Cores)* 114 https://developer.nvidia.com/blog/nvidia-hopper-arc hitecture-in-dept h/ * Preliminary performance estimates Digital Design & Computer Arch. Lecture 21: SIMD/GPU Architectures Frank K. Gürkaynak Mohammad Sadrosadati Prof. Onur Mutlu ETH Zürich Spring 2024 16 May 2024 Backup SlidesGPU vs. Array/Vector Processor Recall: Vector Instruction Execution 118 VADD A,B  C C[1] C[2] C[0] A[3] B[3] A[4] B[4] A[5] B[5] A[6] B[6] Execution using one pipelined functional unit C[4] C[8] C[0] A[12] B[12] A[16] B[16] A[20] B[20] A[24] B[24] C[5] C[9] C[1] A[13] B[13] A[17] B[17] A[21] B[21] A[25] B[25] C[6] C[10] C[2] A[14] B[14] A[18] B[18] A[22] B[22] A[26] B[26] C[7] C[11] C[3] A[15] B[15] A[19] B[19] A[23] B[23] A[27] B[27] Execution using four pipelined functional units Slide credit: Krste Asanovic Time Space Time Warp Execution (Recall the Previous Slide) 119 32-thread warp executing ADD A[tid],B[tid]  C[tid] C[1] C[2] C[0] A[3] B[3] A[4] B[4] A[5] B[5] A[6] B[6] Execution using one pipelined functional unit C[4] C[8] C[0] A[12] B[12] A[16] B[16] A[20] B[20] A[24] B[24] C[5] C[9] C[1] A[13] B[13] A[17] B[17] A[21] B[21] A[25] B[25] C[6] C[10] C[2] A[14] B[14] A[18] B[18] A[22] B[22] A[26] B[26] C[7] C[11] C[3] A[15] B[15] A[19] B[19] A[23] B[23] A[27] B[27] Execution using four pipelined functional units Slide credit: Krste Asanovic Time Space Time Recall: Vector Unit Structure 120 Lane Functional Unit Partitioned Vector Registers Memory Subsystem Elements 0, 4, 8, … Elements 1, 5, 9, … Elements 2, 6, 10, … Elements 3, 7, 11, … Slide credit: Krste Asanovic 121 Lane Functional Unit Registers for each Thread Memory Subsystem Registers for thread IDs 0, 4, 8, … Registers for thread IDs 1, 5, 9, … Registers for thread IDs 2, 6, 10, … Registers for thread IDs 3, 7, 11, … Slide credit: Krste Asanovic GPU SIMD Execution Unit StructureRecall: Vector Instruction Level Parallelism Can overlap execution of multiple vector instructions  Example machine has 32 elements per vector register and 8 lanes  Example with 24 operations/cycle (steady state) while issuing 1 vector instruction/cycle 122 load load mul mul add add Load Unit Multiply Unit Add Unit time Instruction issue Slide credit: Krste Asanovic Warp Instruction Level Parallelism Can overlap execution of multiple instructions  Example machine has 32 threads per warp and 8 lanes  Completes 24 operations/cycle (steady state) while issuing 1 warp/cycle 123 W3 W0 W1 W4 W2 W5 Load Unit Multiply Unit Add Unit time Warp issue Slide credit: Krste Asanovic Warp-based SIMD vs. Traditional SIMD  Traditional SIMD contains a single thread  Sequential instruction execution; lock-step operations in a SIMD instruction  Programming model is SIMD (no extra threads)  SW needs to know vector length  ISA contains vector/SIMD instructions  Warp-based SIMD consists of multiple scalar threads executing in a SIMD manner (i.e., same instruction executed by all threads)  Does not have to be lock step  Each thread can be treated individually (i.e., placed in a different warp)  programming model not SIMD  SW does not need to know vector length  Enables multithreading and flexible dynamic grouping of threads  ISA is scalar  SIMD operations can be formed dynamically  Essentially, it is SPMD programming model implemented on SIMD hardware 124 SPMD  Single procedure/program, multiple data  This is a programming model rather than computer organization  Each processing element executes the same procedure, except on different data elements  Procedures can synchronize at certain points in program, e.g. barriers  Essentially, multiple instruction streams execute the same program  Each program/procedure 1) works on different data, 2) can execute a different control-flow path, at run-time  Many scientific applications are programmed this way and run on MIMD hardware (multiprocessors)  Modern GPUs programmed in a similar way on a SIMD hardware 125 Warp Scheduling Large Warps and Two-Level Warp Scheduling (II)  Two main reasons for GPU resources be underutilized  Branch divergence  Long latency operations 127 Narasiman et al., “Improving GPU Performance via Large Warps and Two-Level Warp Scheduling,” MICRO 2011. Large Warps and Two-Level Warp Scheduling  Two main reasons for GPU resources be underutilized  Branch divergence  Long latency operations 128 time Core Memory System All Warps Compute Req Warp 0 All Warps Compute Req Warp 1 Req Warp 15 Round Robin Scheduling, 16 total warps Narasiman et al., “Improving GPU Performance via Large Warps and Two-Level Warp Scheduling,” MICRO 2011. Long-latency load Two-Level Scheduling of Warps  Scheduling smaller warp groups reduces stalls due to long latency operations Narasiman et al., “Improving GPU Performance via Large Warps and Two-Level Warp Scheduling,” MICRO 2011. time Core Memory System All Warps Compute Req Warp 0 All Warps Compute Req Warp 1 Req Warp 15 Round Robin Scheduling, 16 total warps time Core Memory System Compute Req Warp 0 Req Warp 1 Req Warp 7 Two Level Round Robin Scheduling, 2 fetch groups, 8 warps each Group 0 Compute Group 1 Req Warp 8 Req Warp 9 Req Warp 15 Compute Group 0 Compute Group 1 Saved Cycles Large Warp Microarchitecture Example Decode Stage 1 0 0 1 0 1 0 0 0 0 1 1 1 0 0 0 0 0 1 0 0 1 0 0 1 0 0 1 0 1 0 0 0 0 0 0 1 1 1 1 0 0 0 0 1 1 1 1 0 0 0 1 1 1 11 1 0 1 Sub-warp 0 mask Sub-warp 0 maskSub-warp 1 mask Sub-warp 0 maskSub-warp 1 maskSub-warp 2 mask 1 1 1 1 1 1 1 1  Idea: Reduce branch divergence by having large warps  Dynamically break down a large warp into sub-warps Narasiman et al., “Improving GPU Performance via Large Warps and Two-Level Warp Scheduling,” MICRO 2011. Dynamic Warp Formation Example Dynamic Warp Formation Example 132 A A B B G G A AC C D D E E F F Time A A B B G G A AC D E E F Time A x/1111 y/1111 B x/1110 y/0011 C x/1000 y/0010 D x/0110 y/0001 F x/0001 y/1100 E x/1110 y/0011 G x/1111 y/1111 A new warp created from scalar threads of both Warp x and y executing at Basic Block D D Execution of Warp x at Basic Block A Execution of Warp y at Basic Block A Legend AA Baseline Dynamic Warp Formation Slide credit: Tor Aamodt Hardware Constraints Limit Flexibility of Warp Grouping 133 Lane Functional Unit Registers for each Thread Memory Subsystem Registers for thread IDs 0, 4, 8, … Registers for thread IDs 1, 5, 9, … Registers for thread IDs 2, 6, 10, … Registers for thread IDs 3, 7, 11, … Slide credit: Krste Asanovic Can you move any thread flexibly to any lane?","libVersion":"0.3.2","langs":""}