{"path":"sem3/TI/VRL/extra/hromkovic-TI.pdf","text":"Theoretische InformatikJuraj Hromkovič Theoretische Informatik Formale Sprachen, Berechenbarkeit, Komplexitätstheorie, Algorithmik, Kommunikation und Kryptographie 5., überarbeitete Aufl age Juraj Hromkovič ETH Zürich Zürich, Schweiz ISBN 978-3-658-06432-7 ISBN 978-3-658-06433-4 (eBook) DOI 10.1007/978-3-658-06433-4 Die Deutsche Nationalbibliothek verzeichnet diese Publikation in der Deutschen Nationalbibliografi e; detaillierte bibliografi sche Daten sind im Internet über http://dnb.d-nb.de abrufb ar. Springer Vieweg Die erste Aufl age erschien unter dem Titel „Algorithmische Konzepte der Informatik“. © Springer Fachmedien Wiesbaden 2001, 2004, 2007, 2011, 2014 Das Werk einschließlich aller seiner Teile ist urheberrechtlich geschützt. Jede Verwertung, die nicht aus- drücklich vom Urheberrechtsgesetz zugelassen ist, bedarf der vorherigen Zustimmung des Verlags. Das gilt insbesondere für Vervielfältigungen, Bearbeitungen, Übersetzungen, Mikroverfi lmungen und die Ein- speicherung und Verarbeitung in elektronischen Systemen. Die Wiedergabe von Gebrauchsnamen, Handelsnamen, Warenbezeichnungen usw. in diesem Werk be- rechtigt auch ohne besondere Kennzeichnung nicht zu der Annahme, dass solche Namen im Sinne der Warenzeichen- und Markenschutz-Gesetzgebung als frei zu betrachten wären und daher von jedermann benutzt werden dürft en. Gedruckt auf säurefreiem und chlorfrei gebleichtem Papier Springer Vieweg ist eine Marke von Springer DE. Springer DE ist Teil der Fachverlagsgruppe Springer Science+Business Media. www.springer-vieweg.de Mit einer Weisheit, die keine Träne kennt, mit einer Philosophie, die nicht zu lachen versteht, und einer Größe, die sich nicht vor Kindern verneigt, will ich nichts zu tun haben. Khalil Gibran Meinen Eltern Vorwort Dieses Buch ist eine einfache Einführung in algorithmische Grundkonzepte der Theore- tischen Informatik. Die Theoretische Informatik ist weltweit ein fester Bestandteil des Informatikstudiums. Im Unterschied zu den ingenieursmäßig geprägten Gebieten der Praktischen und der Technischen Informatik hebt die Theoretische Informatik mehr die naturwissenschaftlichen und mathematischen Aspekte der Informatik hervor. Gerade die mathematische Prägung ist oft ein Grund dafür, dass die Theoretische Informatik für zu schwer gehalten wird und dadurch ein nicht gerade beliebter Teil der Ausbildung ist. Der Schwierigkeitsgrad der Theoretischen Informatik ist aber meiner Meinung nach nicht der einzige Grund ihrer Unbeliebtheit, insbesondere wenn die Studierenden in ihrer Beurteilung außerdem noch das Prädikat „schwach motiviert“ oder sogar „langweilig“ verwenden. Das könnte auch damit zusammenhängen, dass sich die Einführung in die Theoretische Informatik im Grundstudium an vielen deutschen Hochschulen auf den klassischen Stoﬀ der Berechenbarkeit, der Theorie der formalen Sprachen und der ab- strakten Komplexitätstheorie beschränkt. Dass man dabei überwiegend nur die Konzepte und Ansichten, die vor dem Jahr 1970 entstanden sind, vermittelt, dürfte alleine nicht schlimm sein. Es führt aber oft dazu, dass man mit einer einzigen Motivation zu viele Vorlesungen der Art Deﬁnition – Satz – Beweis absolvieren muss und so halbiert sich die Wichtigkeit dieser Motivation in den Augen der Studierenden mit jeder weiteren Vorlesung, die anknüpft, ohne eine eigene Motivation zu bringen. Um Abhilfe von diesem Zustand zu schaﬀen, muss man sich die Entwicklung der Theoretischen Informatik in den letzten 30 Jahren ansehen. Es geht dabei nicht nur darum, dass man in dieser Zeit tiefere Resultate und viele neue Konzepte entwickelt hat, sondern insbesondere darum, dass die Theorie immer mehr auf die Bedürfnisse der Praxis eingegangen ist. Dadurch sind die Anwendungen der Theorie direkter geworden und die Anschaulichkeit der Motivationen ist stark gestiegen. Die Theoretische Informatik liefert nicht-triviales Know-How, das in vielen Fällen faszinierende und überraschende Anwendungen ermöglicht. Es ist nicht möglich, in einem Einführungskurs alle derartigen spektakulären Erkenntnisse zu präsentieren, weil einige ein zu tiefes Verständnis der Materie fordern, als dass sie im Vordiplom als Ziel gestellt werden können. Aber es viii gibt genügend Ideen, die in einer Einführung darstellbar sind, und die wesentlich die Denkweise eines Informatikers prägen können und sollten. Dieses Buch ist ein Versuch des Autors, zumindest teilweise seine Vision einer modernen Einführung in die algorithmischen Gebiete der Theoretischen Informatik zu realisieren. Dabei folgt er der Richtung, die in der englischsprachigen Literatur Mike Sipser und in der deutschsprachigen Literatur Ingo Wegener eingeschlagen haben, und die im Wesentlichen auf den oben präsentierten Gedanken basiert. Die klassischen Teile der Berechenbarkeit und Komplexitätstheorie sind hier reduziert und dafür einige wichtige Konzepte aus dem Bereich der Algorithmik, Randomisierung, Kommunikation und Kryptographie eingesetzt. Die Strategien dieses Buches heißen „Einfachheit“ und „Weniger ist manchmal mehr“. Für uns ist die Prägung des intuitiven, informellen Verständnisses der Materie genau so wichtig wie präzise Formalisierung, detaillierte Beweisführungen und Begründungen. Didaktisch geht das Buch immer langsam vom Einfachen zum Komplizierten vor. Wir versuchen, die Anzahl der Begriﬀe und Deﬁnitionen zu minimieren, auch wenn wir dadurch auf die Präsentation einiger wichtiger Konzepte und Resultate verzichten müssen. Die Idee dahinter ist, dass es wichtiger ist, die unterschiedlichen konzeptuellen Denkweisen und Methoden zu präsentieren, als ein „vollständiges“ Bild einer abgeschlossenen mathe- matischen Theorie zu zeichnen. Da die Argumentation in diesem Buch nicht nur auf der formalen mathematischen Ebene geführt wird, sondern insbesondere auf das intuitive Verständnis der Materie baut, ist das Buch auch als Lehrmaterial für Fachhochschulen und Studierende, die Informatik nicht als Hauptfach studieren, gut geeignet. Hilfreiche Unterstützung Anderer hat zu der Entstehung dieses Lehrbuches wesent- lich beigetragen. Besonderer Dank gilt Dirk Bongartz, Hans-Joachim Böckenhauer und Alexander Ferrein für sorgfältiges Korrekturlesen und zahlreiche Verbesserungsvorschlä- ge. Herzlicher Dank geht an Dietmar Berwanger, Volker Claus, Georg Schnitger, Erich Valkema und Peter Widmayer für Bemerkungen und anregende Diskussionen. Alexander Ferrein und Manuel Wahle danke ich für die sorgfältige Einbettung des Manuskripts in LATEX. Mein tiefster Dank gilt Frau Stefanie Laux vom Teubner Verlag für die hervor- ragende und konstruktive Zusammenarbeit. Herzlichst danke ich Ingrid Zámečníková für die Illustrationen, den einzigen vollkommenen Teil des Buches, und Tanja für ihre Zitatensammlung. Aachen, 2001 Juraj Hromkovič Die Köpfe von Menschen soll man nicht mit Fakten, Namen und Formeln füllen. Um so etwas zu lernen, braucht man nicht in die Schule zu gehen. Der Zweck der Erziehung ist, dem Menschen das Denken beizubringen, und so eine Ausbildung, die keine Lehrbücher ersetzen können. A. Einstein Vorwort zur zweiten Auﬂage Es freut mich sehr, dass diese alternative Einführung in die Theoretische Informatik so gut von Kollegen und Studenten angenommen wurde. Ich möchte hier zunächst allen denen herzlich danken, die Zeit gefunden haben, ihre Meinungen über das Buch sowie einige Verbesserungswünsche und Erweiterungsvorschläge zu äußern. Ich habe dieses Buch mit Sorgfalt geschrieben, weil ich wusste, dass der Inhalt des Buches den Rahmen der traditionellen Vorstellung der Grundlagen der Theoretischen Informatik sprengt. Deshalb habe ich befürchtet, dass es viele Kollegen als einen Verstoß gegen überwiegend akzeptierte Darstellungen der Informatikgrundlagen im Grundstudium bewerten würden. Außer den vielen positiven Kommentaren, die meine Sorgen schrumpfen ließen, fand ich am Erfreulichsten (als eine echte Bestätigung des Buchkonzeptes), dass die Leser sich überwiegend gerade die Erweiterung der nicht-klassischen Gebiete wie Randomisierung und Kommunikation gewünscht haben. Ich nahm diese Herausforderung mit Freude an und habe versucht, gleich die erste Möglichkeit zu nutzen, für die zweite Auﬂage das Buch sorgfältig zu überarbeiten und zu erweitern. Das Kapitel Randomisierung wurde um einen neuen Abschnitt ergänzt, der die Methode der Fingerabdrücke zum Entwurf von zufallsgesteuerten Algorithmen als einen wichtigen Spezialfall der Anwendung der Methode der häuﬁgen Zeugen präsentiert. Die Methode der Fingerabdrücke wurde an dem randomisierten Äquivalenztest für zwei Polynome illustriert. Im Kapitel Kommunikation und Kryptographie kamen zwei neue Abschnitte dazu. Der Abschnitt Digitale Unterschriften zeigt eine sehr wichtige kommerzielle Anwendung des vorgestellten Konzeptes der Public-Key-Kryptosysteme. Dabei machen wir darauf aufmerksam, dass im Rahmen der klassischen Kryptographie keine Möglichkeit besteht, digitale Unterschriften ohne hohes Fälschungsrisiko zu leisten. Damit ist das kryptogra- phische Konzept der öﬀentlichen Schlüssel eine Basis, ohne die man sich heute den sich dynamisch entwickelnden Bereich des E-Commerce gar nicht vorstellen kann. Der zweite neue Abschnitt zeigt einen Entwurf eines eﬃzienten Telefonnetzes. Die Zielsetzung ist dabei, die Problemstellungen und geeignete Methoden zur Lösung der gestellten Probleme im Bereich der Kommunikation in Netzen am Beispiel eines eleganten Netzentwurfes zu x illustrieren. Einige Kollegen haben sich gewünscht, in diesem Lehrmaterial auch den Beweis des Satzes von Cook zu lesen. Obwohl es sich um eines der grundlegendsten Resultate der Informatik handelt, vertrete ich weiter die Meinung, dass dieses Resultat einen zu schweren Beweis für das Grundstudium besitzt. Trotzdem habe ich mich entschieden, den Beweis einzufügen, um dem besonders interessierten Studierenden die Möglichkeit zu geben, diese grundsätzlich wichtige Beweistechnik kennenzulernen. Da ich diesen Beweis nun einbetten wollte, habe ich versucht, durch eine langsame Entwicklung der Beweisidee den Schwierigkeitsgrad des Beweises abzumildern. Deswegen entstanden drei Seiten über die Beschreibung von Texten und Spielkonﬁgurationen durch Boole’sche Formeln, die als eine Vorbereitung für den Beweis des Satzes von Cook dienen sollen. Nach dieser Einführung konzentriert sich der Beweis nur auf eines, und zwar wie man die Semantik der Berechnungen durch die Formeln eﬃzient ausdrücken kann. Außer den drei oben beschriebenen Erweiterungen wurden an vielen Stellen Korrekturen und Verbesserungen eingearbeitet, um die Anschaulichkeit und die Verständlichkeit der Beweise zu erhöhen. Nennenswert sind die neuen graphischen Darstellungen zu den Beweisen des Satzes von Rice und der Sätze über die Anwendung der Kolmogorov- Komplexität in der Theorie der Berechenbarkeit. Aachen, im Oktober 2003 Juraj Hromkovič Wenn du nicht auf das Unerwartete wartest, ﬁndest du nichts Edles, nichts, was schwer zu ﬁnden ist. Heraklit Vorwort zur dritten Auﬂage Gleich am Anfang möchte ich mich bei den zahlreichen Kollegen bedanken, die sich Zeit für das Lesen und Kommentieren der zweiten Auﬂage genommen haben. Dabei war der am häuﬁgsten geäußerte Wunsch, zusätzlich das Thema Grammatiken zu behandeln. Obwohl dieses Thema bereits sehr gut in mehreren Lehrbüchern behandelt wurde,1 enthält die dritte Auﬂage nun ein neues Kapitel über Grammatiken und die Chomsky-Hierarchie. Somit bietet dieses Lehrbuch den Dozenten ein fast vollständiges Angebot an klassischen Themen, das durch einige neuere Konzepte bereichert wird. Auf diese Weise ist dieses Material sowohl für die Kollegen geeignet, die sich auf die klassischen Grundlagen der Informatik konzentrieren, als auch für diejenigen, die in der Einführungsveranstaltung über „Theoretische Informatik“ nicht-klassische Gebiete vorstellen wollen. Das neue Kapitel stellt Grammatiken als Mechanismen zur Erzeugung von Wörtern vor, und somit als eine Alternative zur endlichen Darstellung von Sprachen. Die Schwerpunkte liegen auf dem Studium von kontextfreien Sprachen, die von zentralem Interesse für den Compilerbau sind, und auf der Äquivalenz zwischen Grammatiken und Turingmaschinen. Das Kapitel über Grammatiken steht ganz am Ende des Buches, aber dies bedeutet nicht, dass man dieses Thema am Ende der Veranstaltung präsentieren müsste. Die Teile über reguläre und kontextfreie Grammatiken können direkt im Anschluss an Kapitel 3 über endliche Automaten behandelt werden. Nach Kapitel 4 über Turingmaschinen hat man schon alle notwendigen Vorkenntnisse, um das ganze neue Kapitel zu meistern. Wir sind der Meinung, dass es für eine erfolgreiche Vorbereitung auf eine Prüfung notwendig ist, beliebige Teile des Lerntextes inhaltlich korrekt wiedergeben zu können und leichte Abwandlungen der im Buch vorkommenden Aufgaben lösen zu können. Um die wichtigsten Grundkenntnisse hervorzuheben und dem Studierenden eine Selbstkontrolle zu ermöglichen, haben wir die Zusammenfassungen am Ende der Kapitel um eine Liste von Kontrollfragen erweitert. Wir empfehlen allen Dozenten zusätzlich, die genauen Anforderungen für das Studium gemäß ihrer eigenen Zielsetzungen schriftlich festzuhalten. 1Dies war auch der Hauptgrund, warum wir auf die Darstellung dieser Thematik in den ersten zwei Auﬂagen verzichtet haben. xii Außer der oben beschriebenen Erweiterung wurden an einigen Stellen Korrekturen und verbesserte Erklärungen eingearbeitet. In diesem Zusammenhang möchte ich mich bei Christoph Zimmerli und Laurent Zimmerli für ausführliches Korrekturlesen der zweiten Auﬂage herzlich bedanken. Bester Dank geht an Hans-Joachim Böckenhauer und Julian Tschannen für sorgfältige Korrekturen des neuen Kapitels und an Martin Jaggi und Philipp Zumstein für einige Verbesserungsvorschläge. Bei Nicolas Born und Manuel Wahle bedanke ich mich für ihre Unterstützung bei der LATEX-Bearbeitung der dritten Auﬂage. Besonderer Dank geht an Ulrich Sandten vom Teubner Verlag für eine sehr gute und konstruktive Zusammenarbeit, in der keine Tabus für neue Wege gegolten haben. Mein herzlichster Dank geht an Karl Frey. Seine didaktischen Konzepte waren für mich die interessanteste fachdidaktische Auseinandersetzung und die größte didaktische Bereicherung in den letzten drei Jahren. Zürich, im Juni 2007 Juraj Hromkovič Die Kreativität ist eine Abweichung von der Norm bei einer vollständigen Beherrschung der Norm. Juraj Popovňák Vorwort zur vierten Auﬂage Die vierte Auﬂage beinhaltet keine wesentlichen Änderungen im Vergleich zur dritten Auﬂage. Dafür wurden aber zahlreiche kleine Verbesserungen vorgenommen. Hier möchte ich meinen besten Dank an Bernhard Brodowsky, Martin Kaufmann, Noah Heusser, Enrico Kravina, Oli Frank, Simon Eugster, Ilya Vassilenko, Jan-Filip Zagalak und Philipp Zumstein für das aufmerksame Lesen und Kommentieren der verbesserungswürdigen Stellen richten. Ein besonders herzlicher Dank geht an Emo Welzl für mehrere Jahre erfolgreicher Zusammenarbeit bei der gemeinsamen Durchführung der Vorlesung „Theoretische In- formatik“ an der ETH und an Dennis Komm und Björn Steﬀen für die unaufhörliche Hilfe bei den ständigen Versuchen, die Lehrunterlagen und somit auch dieses Buch zu verbessern. Zürich, im September 2010 Juraj Hromkovič Die Vollkommenheit besteht aus Kleinigkeiten, doch die Vollkommenheit selbst ist keine Kleinigkeit. Michelangelo Vorwort zur fünften Auﬂage In dieser Auﬂage wurden viele kleinere Verbesserungen eingearbeitet, während sich der Inhalt des Buches im Wesentlichen nicht geändert hat. Ferner wurde die Darstel- lung vollständig überarbeitet. Was hinzugekommen ist, ist überwiegend kontextuelles Wissen, das es ermöglicht, die Informatik und die Mathematik als starke Forschungs- instrumente zur Wissensgenerierung mit ihren Möglichkeiten und Grenzen anzusehen. Die Darstellung der Informatik im Rahmen der Entwicklung der gesamten Wissenschaft ermöglicht es, die Einführung in die Informatikgrundlagen, die vorher von manchen als zu mathematisch-technisch und somit oftmals langweilig empfunden wurde, als eine faszinierende Entdeckungsreise zu gestalten. Herzlich bedanken möchte ich mich bei Knut Baganz, Joël Bohnes, Jérôme Dohrau, Manuela Fischer, Manuel Kohler, Sacha Krug, Benjamin Richner, Hannes Rösti, Jasmin Smula, Björn Steﬀen, Emo Welzl, Jochen Zehnder und ganz besonders Daniel Schmitter für unzählige Verbesserungsvorschläge. Mein besonderer Dank gilt Hans-Joachim Böckenhauer und Dennis Komm, die enorm viel Zeit und große Sorgfalt aufgewendet haben, um mir viele wertvolle Vorschläge für qualitative Verbesserungen vieler Erklärungen zu unterbreiten, und die bei ihrer Umsetzung mitgewirkt haben. Zürich, im August 2014 Juraj Hromkovič Inhaltsverzeichnis 1 Einleitung 1 1.1 Informatik als wissenschaftliche Disziplin ................... 1 1.2 Eine faszinierende Theorie . . ......................... 5 1.3 Für die Studierenden .............................. 8 1.4 Aufbau des Lehrmaterials ........................... 10 2 Alphabete, Wörter, Sprachen und die Darstellung von Problemen 13 2.1 Zielsetzung ................................... 13 2.2 Alphabete, Wörter und Sprachen ....................... 14 2.3 Algorithmische Probleme . .......................... 24 2.4 Kolmogorov-Komplexität ........................... 33 2.5 Zusammenfassung und Ausblick ........................ 45 3 Endliche Automaten 49 3.1 Zielsetzung ................................... 49 3.2 Die Darstellungen der endlichen Automaten ................. 49 3.3 Simulationen .................................. 63 3.4 Beweise der Nichtexistenz ........................... 68 3.5 Nichtdeterminismus .............................. 75 3.6 Zusammenfassung ............................... 86 4 Turingmaschinen 91 4.1 Zielsetzung ................................... 91 4.2 Auszug aus der Geschichte ........................... 92 4.3 Das Modell der Turingmaschine . ....................... 94 4.4 Mehrband-Turingmaschinen und Church’sche These ............ 103 4.5 Nichtdeterministische Turingmaschinen .................... 113 4.6 Kodierung von Turingmaschinen ....................... 118 4.7 Zusammenfassung ............................... 120 5 Berechenbarkeit 125 5.1 Zielsetzung ................................... 125 5.2 Die Methode der Diagonalisierung ...................... 126 5.3 Die Methode der Reduktion . . . ....................... 134 5.4 Der Satz von Rice ............................... 145 5.5 Das Post’sche Korrespondenzproblem ..................... 149 5.6 Die Methode der Kolmogorov-Komplexität . ................. 157 5.7 Folgen für die Forschung ............................ 160 5.8 Zusammenfassung ............................... 163 xviii Inhaltsverzeichnis 6 Komplexitätstheorie 167 6.1 Zielsetzung ................................... 167 6.2 Komplexitätsmaße ............................... 168 6.3 Komplexitätsklassen und die Klasse P .................... 174 6.4 Nichtdeterministische Komplexitätsmaße ................... 182 6.5 Die Klasse NP und Beweisveriﬁkation .................... 190 6.6 NP-Vollständigkeit . . . ............................ 194 6.7 Zusammenfassung ............................... 213 7 Algorithmik für schwere Probleme 217 7.1 Zielsetzung ................................... 217 7.2 Pseudopolynomielle Algorithmen ....................... 219 7.3 Approximationsalgorithmen . ......................... 225 7.4 Lokale Suche .................................. 231 7.5 Simulated Annealing .............................. 236 7.6 Zusammenfassung ............................... 239 8 Randomisierung 241 8.1 Zielsetzung ................................... 241 8.2 Elementare Wahrscheinlichkeitstheorie .................... 242 8.3 Ein randomisiertes Kommunikationsprotokoll ................ 246 8.4 Die Methode der häuﬁgen Zeugen und der randomisierte Primzahltest . . 249 8.5 Die Methode der Fingerabdrücke und die Äquivalenz von zwei Polynomen 254 8.6 Zusammenfassung ............................... 259 9 Kommunikation und Kryptographie 263 9.1 Zielsetzung ................................... 263 9.2 Klassische Kryptosysteme ........................... 264 9.3 Public-Key-Kryptosysteme und RSA ..................... 265 9.4 Digitale Unterschriften ............................. 270 9.5 Interaktive Beweissysteme und Zero-Knowledge-Beweise .......... 273 9.6 Entwurf eines Kommunikationsnetzes .................... 277 9.7 Zusammenfassung ............................... 285 10 Grammatiken und Chomsky-Hierarchie 287 10.1 Zielsetzung ................................... 287 10.2 Das Konzept der Grammatiken ........................ 289 10.3 Reguläre Grammatiken und endliche Automaten .............. 299 10.4 Kontextfreie Grammatiken und Kellerautomaten .............. 310 10.5 Allgemeine Grammatiken und Turingmaschinen ............... 332 10.6 Zusammenfassung ............................... 335 Wenn du ein Schiﬀ bauen willst, so trommle nicht Männer zusammen, um Holz zu beschaﬀen, Aufträge zu vergeben oder Arbeit zu verteilen, sondern wecke in ihnen die Sehnsucht nach dem weiten, endlosen Meer! A. de Saint-Exupéry 1 Einleitung 1.1 Informatik als wissenschaftliche Disziplin Jeder, der Informatik studiert oder ausübt, sollte sich von Zeit zu Zeit die Frage stellen, wie er die Informatik deﬁnieren würde, und wie er die Rolle der Informatik im Kontext aller Wissenschaft, allgemeiner Bildung und der täglichen Praxis sieht. Wichtig ist dabei, dass sich mit tieferem Verständnis der Natur der Informatik auch unsere Vorstellung über die Informatik weiterentwickelt. Deswegen ist es insbesondere für die Studierenden im Grundstudium sehr wichtig, dass sie sich nach jedem Semester mit ihrer eigenen Vorstellung über Informatik beschäftigen. Eine Hilfe zu einer solchen Auseinandersetzung zwischen eigener Vorstellung und neu erworbenen Kenntnissen soll diese Einleitung bieten. 1 Versuchen wir zuerst die Frage „Was ist Informatik?“ zu beantworten. Eine genaue Speziﬁkation einer wissenschaftlichen Disziplin zu geben, ist eine schwierige Aufgabe, die man selten vollständig erfüllen kann. Üblicherweise versucht man, Informatik mit der folgenden allgemeinen Aussage zu beschreiben. Informatik ist die Wissenschaft der algorithmischen Darstellung, Verarbeitung, Speicherung und Übertragung von Information. Obwohl diese meist akzeptierte Deﬁnition der Informatik die Information und die Algorithmen als die Hauptobjekte der Untersuchung der Informatik festlegt und den Umgang mit diesen als Ziel der Untersuchung formuliert, sagt sie nicht genug über die Natur der Informatik und über die in der Informatik benutzten Methoden aus. Eine weiterführende Frage für die Klärung der Substanz der Informatik ist die folgende: Welchen Wissenschaften kann man die Informatik zuordnen? Ist sie Metawis- senschaft (wie Philosophie und Mathematik), Geisteswissenschaft, Naturwis- senschaft oder Ingenieurwissenschaft? 1Man beachte, dass das Folgende nur den persönlichen Ansichten und dem Wissensstand des Autors entspricht und keinen Anspruch auf absolute Wahrheit erhebt. J. Hromkovič, Theoretische Informatik, DOI 10.1007/978-3-658-06433-4_1, © Springer Fachmedien Wiesbaden 2014 2 1 Einleitung Die Antwort auf diese Frage klärt nicht nur das Objekt der Untersuchung, sondern sie legt auch die Methodik und die Beiträge der Informatik fest. Die Antwort ist, dass die Informatik keiner dieser Wissenschaftsgruppen vollständig zugeordnet werden kann. Die Informatik besitzt sowohl Aspekte einer Metawissenschaft, einer Naturwissenschaft als auch einer Ingenieurwissenschaft. Wir geben hier eine kurze Begründung für diese Behauptung. Wie die Philosophie und die Mathematik studiert die Informatik allgemeine Kategorien wie Determinismus, Nichtdeterminismus, Zufall, Information, Wahrheit, Unwahr- heit, Komplexität, Sprache, Beweis, Wissen, Kommunikation, Approximation, Algorithmus, Simulation usw. und trägt zu ihrem Verständnis bei. Mehreren dieser Kategorien hat die Informatik einen neuen Inhalt und eine neue Bedeutung gegeben. Eine Naturwissenschaft (im Unterschied zur Philosophie und Mathematik) studiert konkrete natürliche Objekte und Prozesse, bestimmt die Grenze zwischen Möglichem und Unmöglichem und erforscht die quantitativen Gesetze der Naturprozesse. Sie modelliert, analysiert und überprüft die Glaubwürdigkeit erzeugter Modelle durch Experimente. Alle diese Aspekte einer Naturwissenschaft ﬁnden sich auch in der Informatik. Die Objekte sind Information und Algorithmen (Programme, Rechner) und die Prozesse sind die physikalisch existierenden Prozesse der Informationsverarbeitung. Versuchen wir dies an der Entwicklung der Informatik zu dokumentieren. Die historisch erste wichtige Forschungsfrage der Informatik war die folgende Frage von philosophischer Bedeutung: Existieren wohldeﬁnierte Aufgaben, die man automatisch (durch einen Rechner, fernab der Leistungsfähigkeit heutiger oder zukünftiger Rechner) nicht lösen kann? Die Bemühungen, diese Frage zu beantworten, führten zur Gründung der Informatik als eigenständiger Wissenschaft. Die Antwort auf diese Frage ist positiv, und wir kennen heute viele praktisch relevante Aufgaben, die man gerne algorithmisch (automatisch) lösen möchte, die man aber algorithmisch nicht lösen kann. Das liegt aber nicht daran, dass bisher niemand einen Algorithmus (ein Programm) zur Lösung dieser Aufgaben entwickelt hat, sondern daran, dass man solche Programme nie entwickeln wird, weil ihre Nichtexistenz mathematisch bewiesen wurde. Nachdem man Methoden entwickelt hat, um Aufgaben danach zu klassiﬁzieren, ob für sie ein Programm als algorithmische Lösung existiert oder nicht, stellt man sich folgende naturwissenschaftliche Frage: Wie schwer sind konkrete algorithmische Aufgaben? Die Schwierigkeit einer Aufgabe misst man aber nicht darin, wie schwer ein Programm für die Aufgabe zu entwickeln ist oder wie umfangreich das Programm ist. Die Schwierigkeit einer Aufgabe misst man als den Aufwand, den man leisten muss, um die Aufgabe für konkrete Eingaben algorithmisch zu lösen. Man stellte fest, dass es beliebig schwere Aufgaben gibt, sogar solche, für deren Lösung man mehr Energie braucht, als im ganzen bekannten Universum zur Verfügung steht. Es existieren also Aufgaben, für deren Lösung man zwar 1.1 Informatik als wissenschaftliche Disziplin 3 Programme schreiben kann, was aber nichts hilft, weil ein Lauf eines solchen Programms mehr Zeit benötigt, als seit dem Urknall bis heute vergangen ist. Die bloße Existenz eines Programms für eine untersuchte Aufgabe bedeutet also nicht, dass diese Aufgabe praktisch algorithmisch lösbar ist. Die Bemühungen, die Aufgaben in praktisch lösbare und praktisch unlösbare zu un- terteilen, führten zu einigen der faszinierendsten mathematisch-naturwissenschaftlichen Erkenntnissen, die in der Informatik entdeckt worden sind. Als ein Beispiel solcher Resultate können wir zufallsgesteuerte Algorithmen betrachten. Die Programme (Algorithmen), wie wir sie benutzen, sind deterministisch. Die Bedeutung des Wortes „deterministisch“ ist, dass das Programm und die Problemeingabe vollständig die Arbeit des Programms determinieren. In jedem Augenblick ist in Abhängigkeit der aktuellen Daten eindeutig, was die nächste Aktion des Programms sein wird. Zufallsge- steuerte Programme dürfen mehrere Möglichkeiten für die Umsetzung ihrer Arbeit haben; welche Möglichkeit ausgewählt wird, wird zufällig entschieden. Es sieht so aus, als ob ein zufallsgesteuertes Programm von Zeit zu Zeit eine Münze wirft und abhängig davon, ob Kopf oder Zahl gefallen ist, eine entsprechende Strategie für die Suche nach dem richtigen Resultat wählt. Ein zufallsgesteuertes Programm hat daher mehrere unterschiedliche Berechnungen für eine Eingabe. Im Unterschied zu deterministischen Programmen, die immer eine zuverlässige Berechnung des richtigen Resultats liefern, dürfen einige Berech- nungen des zufallsgesteuerten Programms auch falsche Resultate liefern. Das Ziel ist, die Wahrscheinlichkeit einer falschen Berechnung nach unten zu drücken, was in gewissem Sinne bedeuten könnte, dass man versucht, den proportionalen Anteil der Berechnungen mit falschem Resultat klein zu halten. Auf den ersten Blick sieht ein zufallsgesteuertes Programm unzuverlässig gegenüber deterministischen Programmen aus, und man kann fragen, wozu es gut sein sollte. Es existieren aber Probleme von großer praktischer Bedeutung, bei denen der schnellste deterministische Algorithmus auf dem schnellsten heutigen Rechner mehr Zeit zur Berech- nung der Lösung brauchen würde als die Zeit, die seit dem Urknall bis heute vergangen ist. Die Aufgabe scheint also praktisch unlösbar zu sein. Und dann passiert ein „Wunder“: ein zufallsgesteuerter Algorithmus, der die Aufgabe in ein paar Minuten auf einem gewöhnli- chen Personalcomputer mit einer Fehlerwahrscheinlichkeit von 1 durch 1000 Milliarden löst. Kann man ein solches Programm für unzuverlässig halten? Ein deterministisches Programm, das eine Aufgabe in einem Tag berechnet, ist unzuverlässiger als unser zufalls- gesteuertes Programm, weil die Wahrscheinlichkeit des Auftretens eines Hardwarefehlers während einer 24-stündigen Arbeit viel höher ist als die Wahrscheinlichkeit einer fehlerhaf- ten Ausgabe des schnellen zufallsgesteuerten Programms. Ein konkretes Beispiel von hoher praktischer Bedeutung ist das Problem des Primzahltests. In der alltäglichen Anwendung kryptographischer Public-Key-Protokolle muss man große Primzahlen generieren, wobei groß rund 700 Dezimalziﬀern bedeutet. Alle klassischen deterministischen Algorithmen für den Primzahltest basieren auf der Überprüfung der Teilbarkeit der gegebenen Zahl n. Aber schon die Anzahl der Primzahlen kleiner als √n für eine so große Zahl n ist größer als die Anzahl der Protonen im bekannten Universum und deshalb sind solche Algorithmen praktisch uninteressant. Die Weiterentwicklung der Algorithmik hat in die- sem Jahrhundert zu einem neuen deterministischen Algorithmus geführt, der rund n6 4 1 Einleitung Operationen braucht, um eine Zahl mit n Dezimalstellen erfolgreich als Primzahl zu identiﬁzieren. Bei einer gegebenen Zahl mit 700 Dezimalziﬀern würde dies bei heutigen Standardrechnern noch immer einen Aufwand von mehreren Jahren bedeuten. Aber es gibt mehrere zufallsgesteuerte Algorithmen, die den Primzahltest in einigen Minuten auf einem PC realisieren. Ein anderes spektakuläres Beispiel ist ein Kommunikationsprotokoll für den Vergleich der Inhalte zweier Datenbanken, die auf zwei weit entfernten Rechnern gespeichert sind. Man kann mathematisch beweisen, dass jedes deterministische Kom- munikationsprotokoll, das die Äquivalenz der Inhalte überprüft, ungefähr so viele Bits bei der Kommunikation zwischen beiden Rechnern austauschen muss, wie die Datenbank enthält. Betrachten wir jetzt den Fall, dass die Datenbanken eine Größenordnung von 10 12 Bits haben. So viele Bits zu versenden ist ein großer Aufwand. Ein zufallsgesteuertes Kommunikationsprotokoll kann diesen Äquivalenztest mit der Versendung einer Nachricht, die kürzer als 2000 Bits ist, realisieren. Die Fehlerwahrscheinlichkeit des Tests ist dabei kleiner als 1 zu 1000 Milliarden. Warum so etwas überhaupt möglich ist, ist ohne Informatikvorkenntnisse nur schwer zu erklären. Die Suche nach den wahren Gründen für die Stärke der Zufallssteuerung ist eine faszinierende mathematisch-naturwissenschaftliche Forschungsaufgabe. Wichtig zu bemerken ist, dass auch hier die Natur unser bester Lehrmeister sein kann, weil in der Natur mehr zufallsgesteuert abläuft als man glauben würde. Informatiker können viele Beispiele für Systeme geben, bei denen die gewünschten Eigenschaften und Verhaltensweisen nur durch das Konzept der Zufallssteuerung erreicht werden können. In solchen Beispielen muss jedes deterministische „hundert Prozent zuverlässige“ System mit dem erwünschten Verhalten aus Milliarden Teilsystemen bestehen, die alle richtig miteinander kooperieren müssen. Ein solch komplexes System, bei dem viele Teilsysteme immer korrekt arbeiten, kann praktisch nicht realisiert werden, und falls ein Fehler auftritt, ist es eine fast unlösbare Aufgabe, ihn zu ﬁnden. Man braucht gar nicht darüber nachzudenken, wie hoch die Entwicklungs- und Herstellungskosten eines solchen Systems sein würden. Auf der anderen Seite kann man ein solches System zu geringen Kosten mit dem gewünschten Verhalten durch ein kleines zufallsgesteuertes System realisieren, bei dem alle Funktionen jederzeit überprüfbar sind und die Wahrscheinlichkeit eines fehlerhaften Verhaltens so klein ist, dass man sich in der Anwendung keine Sorgen darüber machen muss. Trotz der naturwissenschaftlichen Aspekte der Informatik, die wir gerade illustriert haben, ist die Informatik für die meisten Informatiker eine typische anwendungs- und problemorientierte Ingenieurwissenschaft. Die Informatik umfasst nicht nur die technischen Aspekte des Engineering, wie Organisation des Entwicklungsprozesses (Phasen, Meilensteine, Dokumentati- on), Formulierung strategischer Ziele und Grenzen, Modellierung, Beschrei- bung, Speziﬁkation, Qualitätssicherung, Testen, Einbettung in existierende Systeme, Wiederverwendung und Werkzeugunterstützung, sondern auch die Managementaspekte wie zum Beispiel Teamorganisation und -leitung, Kostenvoranschlag und Kostenaufschlüsselung, Planung, Produktivität, Qualitätsmanagement, Abschätzung von Zeitrahmen und Fristen, Zeit zur Markteinführung, Vertragsabschluss und Marketing. 1.2 Eine faszinierende Theorie 5 Eine Informatikerin oder ein Informatiker sollte auch ein echter pragmatischer Praktiker sein. Bei der Konstruktion sehr komplexer Software- oder Hardwaresysteme muss man oft die Entscheidung gefühlsmäßig bezüglich eigener Erfahrung treﬀen, weil man keine Chance hat, die komplexe Realität vollständig zu modellieren und zu analysieren. Wenn man sich das, was wir bisher über die Informatik geschildert haben, durch den Kopf gehen lässt, könnte man den Eindruck bekommen, dass das Studium der Informatik zu schwer ist. Mathematikkenntnisse sind erforderlich und die naturwissenschaftlichen sowie die ingenieurmäßigen Aspekte des Denkens sind gleichermaßen erwünscht. Das mag stimmen, aber das ist auch der größte Vorteil dieser Ausbildung. Die größte Schwäche heutiger Wissenschaft ist eine zu enge Spezialisierung, die dazu führt, dass sich viele Wis- senschaften zu unabhängig voneinander entwickelt haben. Die Wissenschaften entwickelten eigene Sprachen, die oft sogar für benachbarte Wissenschaften nicht mehr verständlich sind. Es geht so weit, dass die standardisierte Art der Argumentation in einer Wissen- schaft in einer anderen Wissenschaft als eine oberﬂächliche und unzulässige Begründung eingestuft wird. Das macht die propagierte interdisziplinäre Forschung ziemlich schwierig. Die Informatik ist in ihrem Kern interdisziplinär. Sie orientiert sich an der Suche nach Problemlösungen in allen Bereichen des wissenschaftlichen und alltäglichen Lebens, bei denen man Rechner anwendet oder anwenden könnte, und bedient sich dabei eines breiten Spektrums von Verfahren, das von präzisen formalen Methoden der Mathematik bis hin zum erfahrungsmäßigen „Know-How“ der Ingenieurdisziplinen variiert. Die Möglichkeit, gleichzeitig unterschiedliche Wissenschaftssprachen und Arten des Denkens zusammen- hängend in einer Disziplin zu erlernen, ist das Wichtigste, was die Informatikabsolventen in ihrer Ausbildung bekommen sollen. 1.2 Eine faszinierende Theorie Dieses Buch ist der elementaren Einführung in die algorithmischen Gebiete der Theoreti- schen Informatik gewidmet. Die Theoretische Informatik ist eine faszinierende Wissen- schaftsdisziplin, die durch spektakuläre Ergebnisse und hohe Interdisziplinarität wesentlich zur Entwicklung unserer Weltanschauung beigetragen hat. Statistisch gesehen gehört aber die Theoretische Informatik nicht gerade zu den Lieblingsfächern der Studierenden. Viele Studierende bezeichnen die Theoretische Informatik sogar als eine Hürde, die man überwinden muss, um einen Informatik-Universitätsabschluss zu erhalten. Für diese Ein- stellung gibt es sicherlich mehrere Gründe. Ein Grund dafür ist, dass die Theoretische Informatik von allen Informatikbereichen am stärksten mathematisch geprägt ist und so einen höheren Schwierigkeitsgrad besitzt. Dazu kommt oft noch, dass die Studierenden mit einer falschen Vorstellung des Informatikstudiums angetreten sind und dass wir, die Dozenten der Informatik, die theoretischen Veranstaltungen nicht immer attraktiv genug verkaufen. Zuviel Druck auf eine saubere Darstellung der kleinsten technischen Details mathematischer Beweise und zu wenig Raum für Motivationen, Zusammenhänge, infor- melle Ideenentwicklung in geschichtlichem Rahmen und direkte Anwendungsmöglichkeiten können das Studium auch der faszinierendsten Wissenschaftsgebiete versäuern. In der obigen Beschreibung der Informatik als einer Wissenschaft mit vielen Gesich- tern haben wir schon indirekt die Wichtigkeit der Theoretischen Informatik dargestellt. 6 1 Einleitung Weil es aber noch weitere wichtige Gründe für die Unverzichtbarkeit des Studiums der Theoretischen Informatik in der Informatikausbildung gibt, möchten wir die wichtigsten systematisch auﬂisten. 1. Philosophische Tiefe Die Theoretische Informatik erforscht Kenntnisse und bildet neue Konzepte und Begriﬀe, die die ganze Wissenschaft in ihren tiefsten Grundlagen beeinﬂussen. Die Theoretische Informatik gibt partielle oder vollständige Antworten auf Fragen philosophischer Tiefe wie: • Existieren Probleme, die nicht automatisch (algorithmisch) lösbar sind? Falls ja, wo liegt die Grenze zwischen automatisch Lösbarem und automatisch Unlösbarem? • Wie kann man ein zufälliges Objekt deﬁnieren? • Können nichtdeterministische und zufallsgesteuerte Prozesse etwas, was die deter- ministischen nicht können? Ist Nichtdeterminismus und Zufall stärker (eﬃzienter) als Determinismus? • Wie kann man die Schwierigkeit von Aufgaben deﬁnieren? • Wo ist die Grenze der „praktischen“ automatischen Lösbarkeit? • Was ist ein mathematischer Beweis, und ist es schwerer, mathematische Beweise algorithmisch zu ﬁnden, als gegebene Beweise algorithmisch auf Korrektheit zu überprüfen? Wichtig ist dabei zu bemerken, dass viele dieser Fragestellungen eigentlich ohne die for- malen Konzepte der Algorithmen und Berechnungen nicht formulierbar waren. So hat die Theoretische Informatik die Sprache der Wissenschaften auch um neue Begriﬀe bereichert und dadurch zur Entwicklung der Wissenschaftssprache beigetragen. Mehrere bekannte Grundkategorien der Wissenschaft wie Determinismus, Zufall und Nichtdeterminismus haben neue Bedeutung gewonnen, und so wurden unsere allgemeinen Ansichten über die Welt beeinﬂusst. 2. Praxisnähe und spektakuläre Ergebnisse Die Theoretische Informatik ist praxisrelevant. Auf der einen Seite liefert sie Erkenntnisse methodologischer Natur, die unsere ersten groben Entscheidungen bei der Bearbeitung algorithmischer Aufgaben steuern. Auf der anderen Seite liefert sie Konzepte und Me- thoden, die direkt praktisch umsetzbar sind und ohne die viele Anwendungen unmöglich wären. Neben dem schon erwähnten Konzept zufallsgesteuerter Algorithmen existieren noch viele andere „Wunder“, die in der Theoretischen Informatik entstanden sind. Es gibt schwere Optimierungsprobleme, bei denen man durch eine Abschwächung der Forderung, eine optimale Lösung zu liefern, einen gewaltigen Sprung von einer unrealisierbaren Menge Rechnerarbeit zu einer Angelegenheit von ein paar Minuten machen kann. Dabei braucht die Abmilderung der Forderung oft nicht groß zu sein. Wir dürfen die Berechnung einer Lösung fordern, deren Qualität nur sehr wenig von der Qualität einer optimalen Lösung abweicht. Würden Sie glauben, dass es möglich ist, jemanden vom Besitz eines Geheimnisses (zum Beispiel eines Passwortes) zu überzeugen, ohne ein einziges Bit der 1.2 Eine faszinierende Theorie 7 Information über dieses Geheimnis zu verraten? Würden Sie glauben, dass zwei Personen in einem Gespräch bestimmen können, wer von ihnen älter ist, ohne dem anderen das eigene Alter zu verraten? Würden Sie glauben, dass man mathematische Beweise von mehreren tausend Seiten Länge fast mit hundertprozentiger Zuverlässigkeit auf Korrekt- heit überprüfen kann, obwohl man diese gar nicht liest und nur einige wenige zufällig ausgewählte Bits (Buchstaben) des Beweises ansieht? Alles das, was wir oben erwähnt haben, ist möglich. Dies zeigt nicht nur, dass man dank der Theorie Dinge realisieren kann, die man vorher vielleicht für unmöglich gehalten hat, sondern auch, dass die Forschung in der Theoretischen Informatik voller Spannung und Überraschungen ist und dass man auch mit Theoretischer Informatik begeistern kann. 3. Lebensdauer von Kenntnissen Durch die schnelle Entwicklung der Technologien ändert sich die Welt der berufspraktischen Informatik ständig. Die Hälfte der dort erworbenen Kenntnisse über Produkte ist in fünf Jahren so veraltet, dass man mit ihnen nichts mehr anfangen kann. Daher gäbe ein Studi- um, das sich überproportional dem Wissen über Produkte widmet, keine hinreichende Berufsperspektive. Andererseits haben die Konzepte und Methoden der Theoretischen In- formatik im Durchschnitt eine wesentlich längere Lebensdauer von mehreren Jahrzehnten. Ein Absolvent der Informatik kann auf dieses Know-How lange aufbauen. 4. Interdisziplinarität Die Theoretische Informatik ist stark interdisziplinär und kann sich in vielen spannenden Gebieten an Forschung und Entwicklung beteiligen. Genomprojekte, medizinische Dia- gnostik, Optimierung in allen Gebieten der Wirtschaft und technischen Wissenschaften, automatische Spracherkennung und Weltraumforschung sind nur einige Beispiele eines großen Spektrums von Möglichkeiten. Neben diesen wesentlichen Beiträgen der Informatik für andere Wissenschaften gibt es auch Möglichkeiten faszinierender Beiträge anderer Wissenschaften für die Informatik. Das Studium der Berechnungen auf der Ebene der Mikroteilchen, die den Gesetzen der Quantenmechanik folgen, hat als Hauptthema die Fragestellung, ob man in der Mikrowelt gewisse Berechnungen (Aufgabenlösungen) eﬃzi- ent realisieren kann, die in der Makrowelt nicht eﬃzient realisierbar sind. Das theoretische Modell eines Quantenrechners ist bereits ausgearbeitet, aber die Möglichkeit einer prakti- schen Realisierung ist für Physiker eine große Herausforderung mit unbekanntem Ausgang. Unabhängig davon, ob dieses Projekt Erfolg haben wird, sind die Gesetze der Mikrowelt so überraschend und so kontraintuitiv für die Menschen, die ihre Erfahrungen in der Makrowelt gesammelt haben, dass man noch mit vielen „Wundern“ durch die Anwendung der Kenntnisse der Quantentheorie rechnen muss. Schon heute ist klar, dass man in der Mikrowelt sicher kommunizieren kann, da jeder Versuch, die Kommunikation abzuhören, sofort vom Sender entdeckt und abgewehrt werden kann. Ein anderes spannendes Gebiet ist das Rechnen mit DNA-Molekülen. DNA-Moleküle sind Informationsträger und daher sollte es nicht überraschen, dass man sie zur Informationsspeicherung und -übertragung benutzen kann. Heute wissen wir schon, dass man durch chemische Operationen auf DNA- Molekülen die Arbeit von Rechnern nachahmen kann. Dies ist nicht nur theoretisch klar, mehrere solcher Simulationen von Berechnungen durch DNA-Moleküle wurden schon in Laboratorien realisiert. Es ist nicht auszuschließen, dass eines Tages einige DNA-Moleküle die ganze Arbeit eines Rechners übernehmen können. 8 1 Einleitung 5. Denkweise Die Mathematiker begründen die Sonderrolle der Mathematik in der Ausbildung mit der Entwicklung, Bereicherung und Prägung einer Denkweise, die der allgemeinen Entwicklung einer Persönlichkeit zugute kommen sollte. Falls dieser Beitrag der Mathematik anerkannt wird, dann muss man die Wichtigkeit der Informatik für die allgemeine Bildung und die Bereicherung um neue Denkweisen in gleichem Maße akzeptieren. Während sich in letzter Zeit die Mathematikausbildung leider zu stark auf das Erlernen korrekter Beweisführung reduziert hat, fördert die Theoretische Informatik auch die Modellierung der Realität und die Suche nach Konzepten und Methoden zur Lösung konkreter Probleme. In Verbindung mit dem pragmatischen Denken eines guten Praktikers ist dies das Wesentliche, was das Studium den Informatikern bieten kann. 1.3 Für die Studierenden Dieses Buch ist in erster Linie für Sie bestimmt. Der Sinn des Buches ist nicht nur, Ihnen einige grundlegende Konzepte der Informatik zu vermitteln, sondern es ist auch ein Versuch, Sie für die Informatik zu begeistern. Wie weit diese Zielsetzungen erfüllt werden, bleibt Ihnen zu beurteilen. In den ersten Teilen dieses Kapitels habe ich versucht, Sie davon zu überzeugen, dass Informatik eine faszinierende Wissenschaft ist, eine Wissenschaft, die einem sehr viel Spaß, Spannung und Freude bereiten kann. Dass es viele Leute gibt, die die Arbeit in der Theoretischen Informatik richtig genießen, möchte ich an einigen mündlich überlieferten (möglichen) Ausschnitten aus den Lehr- und Forschungsveranstaltungen eines meiner Freunde illustrieren. • „Und wenn sich dieser verdammte Fehlerfaktor dann immer noch nicht unter 0.5 drücken lässt, dann werden wir andere Saiten aufziehen. Der wird es noch bereuen, sich in mein Lernmodell getraut zu haben.“ • „Also heute haben wir so ein Epsilon in die Knie gezwungen. Da war was am Dampfen, sag ich Euch. Erst haben wir eine Approximation draufgehetzt. Erst hat es ja noch ein bisschen gezuckt. Aber dann haben wir es mit einer semideﬁniten Matrix stumpf erschlagen. Es war herrlich!“ • „Ihr sagt es, kein popliges, dahergelaufenes Lambda vergreift sich an unserem My, ohne dafür zu bezahlen. Wir machen es so richtig fertig. Es wird gar nicht wissen, wie ihm geschieht.“ Halten Sie solche Emotionen zu einem mathematisch geprägten Thema für übertrieben? Ich nicht. Es ist ein riesiger Spaß, in einer solchen Vorlesung sitzen zu dürfen, und man begreift gleich, wo die Schwierigkeiten liegen, mit denen man zu kämpfen hat. Die Emotionen in Lehre und Forschung sind der Hauptantrieb. Wenn Sie eine emotionale Beziehung zu einem Lehrthema entwickeln können, sind sie schon auf der Erfolgstour. Wenn Sie bisher noch keine emotionale Beziehung zu irgendeinem Gebiet der Informatik gefunden haben, ist es höchste Zeit, sich auf die Suche zu begeben. Falls die Suche erfolglos bleiben sollte, wäre es angemessen, die Suche auf Wissenschaftsdisziplinen oder Aktivitäten außerhalb der Informatik zu erweitern. 1.3 Für die Studierenden 9 Dieses Buch ist einigen Teilen der Theoretischen Informatik gewidmet. Warum hält man das Studium der Theoretischen Informatik für schwer? Es existiert kein leichter Weg zu einem tieferen Verständnis und zur Beherrschung von Methoden, die eindrucksvolle Anwendungen haben. Das sollte aber niemanden überraschen. Wenn jemand 100 Meter unter 10 Sekunden laufen oder über 8 Meter weit springen möchte, muss er auch lange Jahre ein ziemlich hartes Training absolvieren. Immer wenn man etwas Besonderes erreichen will, muss man etwas Besonderes dafür tun. Eine Wissenschaft zu erlernen ist keine Ausnahme, und vielleicht haben Sie noch zusätzlich die Motivationsschwierigkeit, dass Sie das Ziel während der Bemühung noch nicht genau sehen und seinen Wert nicht abschätzen können. Es wird also viel Ausdauer gefordert und insbesondere Bereitschaft, jedes Thema viele Male zu iterieren, um ein immer tiefer gehendes Verständnis für die Zusammenhänge zu entwickeln. Dieses Buch soll Ihnen den Einstieg in die algorithmischen Teile der Theoretischen Informatik erleichtern. Dazu benutzen wir folgende drei Konzepte: 1. Einfachheit und Anschaulichkeit Wir erklären alles, was einfach zu erklären ist, auch einfach. Wir vermeiden unnötige mathematische Abstraktionen, wir versuchen also so konkret zu sein, wie es nur geht. Dadurch schaﬀen wir den Einstieg mit elementaren Kenntnissen der Mathematik. Bei allen komplizierteren Überlegungen und Beweisen erklären wir zuerst die Zusammen- hänge und Ideen auf eine anschauliche und informelle Weise und gehen erst dann zu formalen Begründungen über. An den mit ∗ gekennzeichneten Stellen empfehlen wir den Fachhochschulstudenten und Nicht-Informatikern, auf die formalen Beweise zu verzichten. Außerdem können diese Gruppen zum Beispiel auf den technischen Umgang mit dem formalen Modell der Turingmaschine im Teil Berechenbarkeit verzichten, und das Ver- ständnis der Grundlagen der Theorie der Berechenbarkeit über die Argumentation auf der Ebene von Programmen (in einer beliebigen Programmiersprache) erreichen. Die Anschaulichkeit ist uns wichtiger als die Präsentation der besten bekannten Er- gebnisse. Wenn wir die Methoden und Beweisideen leichter mit schwächeren Resultaten erklären können, dann ziehen wir die Darstellung solcher Resultate der technischen und schwer durchschaubaren Präsentation stärkerer Resultate vor. Im ganzen Buch folgen wir der Linie, stufenweise mit kleinen Schritten vom Einfachen zum Komplizierten zu gehen, und vermeiden so Gedankensprünge. 2. „Weniger ist manchmal mehr“ oder eine kontextsensitive Darstellung Viele Studienpläne und Lehrbücher gehen von der falschen Vorstellung aus, dem Leser in erster Linie ein gewisses Quantum an Information liefern zu müssen. In der Vorlesung oder in den Lehrmaterialien spielt man dann ein falsches Optimierungsspiel – in minimaler Zeit so viele Kenntnisse und Resultate wie möglich zu vermitteln. Dies führt oft zur Präsentation einer großen Menge von einzelnen Resultaten, die zu isoliert wirken. Dabei geht der Kontext der ganzen Veranstaltung verloren. Die Philosophie dieses Buches ist eine andere. Wir wollen die Denkweise der Studierenden prägen. Deswegen ist uns die Anzahl der präsentierten Resultate nicht so wichtig. Wir konzentrieren uns auf die historische Entwicklung der informatischen Konzepte und Denkweisen, und die Präsentation von Deﬁnitionen, Resultaten, Beweisen und Methoden ist nur ein Mittel, um dieses Ziel zu erreichen. Deswegen nehmen wir gerne die Reduktion 10 1 Einleitung der Masse des Lernstoﬀes um 20–30 % im Vergleich zu standardisierten Vorlesungen in Kauf. Dafür widmen wir mehr Zeit den Motivationen, Zielsetzungen und Zusammenhängen zwischen der Praxis und den theoretischen Konzepten und insbesondere dem inneren Kontext der entwickelten Theorie. Einen besonderen Schwerpunkt legen wir auf die Bildung neuer Begriﬀe. Es ist nicht so, wie es vielleicht in einigen Vorlesungen mathematischer Natur aussehen könnte, dass die Terminologie vom Himmel gefallen ist. Die formal deﬁnierten Terme sind immer eine Approximation oder eine Abstraktion intuitiver Begriﬀe, deren Formalisierung wir brauchen, um überhaupt exakte Aussagen über gewisse Objekte und Erscheinungen formulieren zu können und um die Realisierung einer formal sauberen und eindeutigen Argumentation (Beweisführung) zu ermöglichen. Wir bemühen uns hier, die Gründe zu erläutern, warum man die Begriﬀe so wie hier und nicht anders formalisiert hat und wo die Grenzen ihres Nutzens liegen. 3. Unterstützung iterativer Arbeitsweise An diese Strategie ist auch der Aufbau des Buches angepasst, der wiederholtes Nachdenken über präsentierte Konzepte fördert. Jedes Kapitel fängt mit dem Unterkapitel „Zielset- zungen“ an, in dem das Lernziel des Kapitels in allen Zusammenhängen erläutert wird. Der Kern des Kapitels ist dann der Formalisierung der Ideen durch theoretische Konzepte und dem Studium im Rahmen dieser Konzepte gewidmet. Bei jedem wesentlichen neuen Schritt wird auf die wichtigsten Zusammenhänge mit deren Zielsetzungen aufmerksam gemacht. Jedes Kapitel endet mit einer kurzen Zusammenfassung und einem Ausblick. Dort werden noch einmal die wichtigsten Kenntnisse des Kapitels informell wiederholt und in Zusammenhang mit anderen Teilen der Theorie gebracht. Hier werden auch weitere Entwicklungen der theoretischen Konzepte und aus der Literatur bekannte tiefgreifen- de Resultate kurz erwähnt. Die Entfernung des Wissenstandes von den angestrebten Forschungszielen wird auch diskutiert. Wie üblich wird das Erlernen des Buchinhaltes durch Übungen unterstützt. Die Übungen beﬁnden sich hier nicht in einem gesonderten Kapitel, sondern sind direkt an den Stellen des Textes eingefügt, wo es am passendsten ist, über sie nachzudenken. Sie dienen zur Übung der Anwendungen der präsentierten Methoden sowie zur Vertiefung des Verständnisses des Lernstoﬀes. Das Ziel des Autors ist nicht nur, die Informatik als eine faszinierende Wissenschaft vorzustellen, sondern Ihnen auch eine preiswerte Eintrittskarte in die algorithmischen Gebiete der Informatik anzubieten. Die Voraussetzungen für die Anwendung dieser Karte sind minimal. Erfahrungen mit der Programmierung im Umfang eines Semesters und elementare Kenntnisse der Mathematik sind hinreichend. Die Kenntnisse von Standard- vorlesungen wie Rechnerstrukturen und Algorithmen und Datenstrukturen sind zwar nicht notwendig, aber sehr hilfreich für das Verständnis konzeptioneller Zusammenhänge. 1.4 Aufbau des Lehrmaterials Außer dieser Einleitung umfasst das Buch neun weitere Kapitel. Kapitel 2 und 3 dienen als Einstieg. In ihnen lernt man die Sprache und Ausdrucksweisen der Theoretischen Infor- matik. Das Resultat der Arbeit eines Rechners kann man immer als eine Transformation eines Textes in einen anderen Text verstehen, weil wir die Eingaben und Ausgaben als 1.4 Aufbau des Lehrmaterials 11 Texte darstellen. Kapitel 2 legt die Grundlagen für den Umgang mit Texten und benutzt sie, um die formale Speziﬁkation algorithmischer Aufgaben zu entwickeln. Außerdem beschäftigt sich Kapitel 2 mit der Frage, wie man die Menge des Informationsgehaltes eines Textes messen kann, und wann man einen Text (ein Objekt) als zufällig betrachten kann. Kapitel 3 stellt die endlichen Automaten als das einfachste Berechnungsmodell vor. Das Ziel ist dabei nicht der Einstieg in die Automatentheorie, sondern nur eine Vorbereitung für die Deﬁnition eines formalen Modells von Algorithmen (Programmen). Wir nutzen endliche Automaten, um eine erste einfache Vorstellung der Kernbegriﬀe der Informatik wie Zustand und Konﬁguration eines Berechnungsmodells, Berechnung, Berechnungs- schritt, Determinismus, Nichtdeterminismus, Beschreibungskomplexität und Simulation zu vermitteln. Diese erleichtert es uns später, das Verständnis dieser Begriﬀe für das allgemeine Modell der Turingmaschine zu gewinnen. Kapitel 4 ist der Turingmaschine als dem theoretischen Grundmodell des intuitiven Begriﬀes Algorithmus gewidmet. Weil der technische Umgang mit Turingmaschinen einer Programmierung im Maschinencode eines Rechners entspricht, versuchen wir diesen Teil auf das Notwendigste für das Verständnis und den Umgang mit den oben erwähnten Grundbegriﬀen zu beschränken. Kapitel 5 beschäftigt sich mit der Theorie der Berechenbarkeit. Hier stellen wir uns die Frage, welche Aufgaben man algorithmisch (automatisch) lösen kann und welche nicht, und präsentieren einige Methoden, mit deren Hilfe wir diese Frage für einige konkrete Aufgaben beantworten. Hier arbeiten wir auf zwei Ebenen. Die erste Ebene führt die Argumentation nur anhand eines intuitiven Verständnisses des Begriﬀes Programm, die zweite Ebene argumentiert formal über Turingmaschinen. Kapitel 6 ist der Komplexitätstheorie gewidmet. Hier stellt man sich die Frage, wie man die Schwierigkeit algorithmischer Aufgaben misst, und ob es beliebig schwere algorithmisch lösbare Aufgaben gibt. Die Schwierigkeit (Berechnungskomplexität) messen wir als die zur Lösung algorithmischer Aufgaben notwendige Menge der Rechnerarbeit. Zuerst präsen- tieren wir einige wichtige Grundkenntnisse, auf deren formale Beweise wir wegen ihres Schwierigkeitsgrades in diesem Kurs verzichten. Danach zeigen wir das Konzept der NP- Vollständigkeit als eine Methode zur Klassiﬁzierung algorithmischer Aufgaben in einfache (praktisch lösbare) und schwere Aufgaben. Das Konzept basiert auf dem Studium der Beziehung zwischen nichtdeterministischen und deterministischen Berechnungen, welches eines der Kernforschungsthemen der Theoretischen Informatik ist. In gewissem Rahmen zeigen wir, dass die Schwierigkeit (Komplexität), etwas nichtdeterministisch zu lösen, der Schwierigkeit einer Korrektheitsprüfung (Veriﬁkation) eines mathematischen Beweises und die deterministische Art, Lösungen zu suchen, der Erzeugung mathematischer Beweise entspricht. Daher ist die Frage, ob nichtdeterministische Berechnungen eﬃzienter als deterministische sein können, äquivalent zu der Frage, ob es einfacher ist, einen gegebenen Beweis zu veriﬁzieren, als ihn zu erzeugen. Kapitel 7 beschäftigt sich mit der Algorithmentheorie.2 Dieses Kapitel ist eine Fort- setzung von Kapitel 6 und stellt folgende Frage: Was kann man mit Aufgaben machen, 2Dabei handelt es sich nicht um Teile der Vorlesung Algorithmen und Datenstrukturen oder einer klassischen Vorlesung über Algorithmen. 12 1 Einleitung die so schwer sind, dass der beste Algorithmus Jahre bräuchte, um diese zu lösen? Hier präsentieren wir Ansätze wie pseudopolynomielle Algorithmen, lokale Suche, Approxi- mationsalgorithmen und Heuristiken wie Simulated Annealing, die in der Praxis solche Probleme oft bewältigen. Wir erklären die methodologischen Grundlagen solcher Metho- den, die darauf basieren, dass man durch kleine Änderungen der Lösungsanforderungen oder Aufgabenspeziﬁkation einen gewaltigen Sprung macht von einer physikalisch un- realisierbaren Menge Arbeit zu einer Angelegenheit von ein paar Minuten. Zum Beispiel können Approximationsalgorithmen gewaltige Komplexitätssprünge ermöglichen. Dort weicht man von der Forderung ab, eine optimale Lösung zu berechnen, und geht über zur Berechnung einer Lösung, deren Qualität sich von der Qualität einer optimalen Lösung nicht allzu viel unterscheidet. Im Fall der schon in Abschnitt 1.1 erwähnten zufallsgesteuerten Algorithmen ist man von der Forderung nach richtigen Lösungen mit hundertprozentiger Sicherheit zu der schwächeren Forderung der Korrektheit mit großer Wahrscheinlichkeit übergegangen. Das Konzept der Randomisierung (Zufallssteuerung) gehört zweifellos zu den Basiskonzepten der in Kapitel 7 präsentierten Algorithmik. Weil es heute von essentieller Bedeutung für viele theoretische und praktische Kernbereiche der Informatik ist, widmen wir diesem Konzept aber ein eigenes Kapitel 8. Hier beschränken wir uns aber nicht nur auf die Präsentation einiger eindrucksvoller Beispiele eﬃzienter zufallsgesteuerter Algorithmen wie zum Beispiel den randomisierten Primzahltest. Wir versuchen dem Leser auch die Gründe für den Erfolg der zufallsgesteuerten Algorithmen wenigstens teilweise zu erläu- tern. In diesem Rahmen stellen wir die Methode der häuﬁgen Zeugen und die Methode der Fingerabdrücke als grundlegende Paradigmen des Entwurfs von zufallsgesteuerten Algorithmen vor. Kapitel 9 ist Kommunikationsaufgaben gewidmet. Besonders in den letzten Jahren hat die Entwicklung der Technologie die Übertragung riesiger Mengen von Daten und Gesprächen ermöglicht. Deswegen sind die algorithmischen Ansätze zur Lösung von Kommunikationsaufgaben ein fester und sich dynamisch entwickelnder Bestandteil der Informatik geworden. Kapitel 9 bietet zunächst einen Ausblick auf die Thematik der sicheren Kommunikation. Wir beginnen mit ein paar Beispielen von Kryptosystemen und stellen das Konzept der Public-Key-Kryptosysteme vor. Wir verwenden dieses Konzept, um zu zeigen, wie man ohne Fälschungsrisiko digitale Unterschriften leisten kann. Dann präsentieren wir das Konzept der Zero-Knowledge-Beweissysteme, das auch eine wichtige Rolle in kryptographischen Anwendungen spielt. Wir schließen dieses Kapitel mit dem Entwurf eines leistungsfähigen Telefonnetzes ab, um die Problemstellungen in dem Bereich der Netzwerkkommunikation zu illustrieren. Wie im Vorwort zur dritten Auﬂage schon vorgestellt wurde, ist das letzte Kapitel Grammatiken als Generierungsmechanismen von Sprachen gewidmet. Die Typen von Grammatiken klassiﬁzieren wir nach Chomsky und setzen sie mit den entsprechenden Maschinenmodellen in Verbindung. Das Tiefste, das wir erfahren können, sind die Oﬀenbarungen der Mystik. Sie sind das fundamentalste Gefühl, das an der Wiege aller wahren Kunst und Wissenschaft steht. Wer es nicht kennt, kann sich nicht mehr wundern; er erlebt das tiefe Staunen nicht mehr: Er ist so gut wie tot . . . Wie eine erloschene Kerze . . . A. Einstein 2 Alphabete, Wörter, Sprachen und die Darstellung von Problemen 2.1 Zielsetzung Die Rechner arbeiten im Prinzip mit Texten, die nichts anderes sind als Folgen von Symbolen aus einem bestimmten Alphabet. Die Programme sind Texte über dem Alphabet der Rechnertastatur, alle Informationen sind im Rechner als Folgen von Nullen und Einsen gespeichert, Eingaben und Ausgaben sind im Wesentlichen auch Texte (oder können zumindest als Texte dargestellt werden) über einem geeignet gewählten Alphabet. Aus dieser Sicht realisiert jedes Programm eine Transformation von Eingabetexten in Ausgabetexte. Das erste Ziel von Kapitel 2 ist, den Formalismus für den Umgang mit Texten als Infor- mationsträger einzuführen. Dieser liefert die notwendige Grundlage, um überhaupt formal die Grundbegriﬀe der Informatik wie algorithmisches Problem (Aufgabe), Algorithmus (Programm), Rechner, Berechnung, Eingabe, Ausgabe usw. deﬁnieren zu können. Die Grundbegriﬀe, die hier eingeführt werden, sind Alphabet, Wort und Sprache. Ein Teil unserer Aufgabe hier ist es, auch den Umgang mit diesen Begriﬀen zu üben und so einige grundlegende Operationen auf Texten zu erlernen. Das zweite Ziel dieses Kapitels ist zu lernen, wie der eingeführte Formalismus zur formalen Darstellung algorithmischer Aufgaben genutzt werden kann. Dabei betrachten wir überwiegend zwei Klassen von Aufgaben, Entscheidungsprobleme und Optimie- rungsprobleme. Das dritte und letzte Ziel dieses Kapitels ist, sich mit der Komprimierbarkeit von Texten zu beschäftigen. Wir führen hier den Begriﬀ der Kolmogorov-Komplexität ein. Dank diesem können wir nicht nur über die kürzeste Darstellung von Objekten (Texten) und die Komprimierbarkeit von Darstellungen sprechen, sondern auch den Informationsinhalt von Texten messen und eine sinnvolle Deﬁnition des Attributs zufällig für Texte geben. Dies ist ein Beitrag der Informatik auf der philosophischen Ebene, weil er sinnvoll erklärt, wann ein Objekt oder eine Erscheinung als zufällig eingeordnet werden kann. Ein anderer J. Hromkovič, Theoretische Informatik, DOI 10.1007/978-3-658-06433-4_2, © Springer Fachmedien Wiesbaden 2014 14 2 Alphabete, Wörter, Sprachen und die Darstellung von Problemen wichtiger Punkt ist, dass die Kolmogorov-Komplexität ein wertvolles Instrument zur Untersuchung von Berechnungen ist, was auch an mehreren Stellen in diesem Buch deutlich gemacht wird. 2.2 Alphabete, Wörter und Sprachen Bei der algorithmischen Datenverarbeitung repräsentieren wir die Daten und betrachteten Objekte durch Folgen von Symbolen. Genau wie bei der Entwicklung natürlicher Sprachen fangen wir mit der Festlegung von Symbolen an, die wir zur Darstellung der Daten verwenden wollen. Im Folgenden bezeichnet N = {0, 1, 2,...} die Menge der natürlichen Zahlen. Deﬁnition 2.1. Eine endliche nichtleere Menge Σ heißt Alphabet. Die Elemente eines Alphabets werden Buchstaben (Zeichen, Symbole) genannt. Die Bedeutung ist die gleiche wie bei natürlichen Sprachen: Das Alphabet wird verwen- det, um eine schriftliche Darstellung einer Sprache zu erzeugen. Für uns ist nur wichtig zu wissen, dass wir uns beliebige, aber nur endlich viele Symbole aussuchen dürfen, um eine Darstellung untersuchter Objekte zu realisieren. Wir präsentieren jetzt einige der hier am häuﬁgsten benutzten Alphabete. • Σbool = {0, 1} ist das Boole’sche Alphabet, mit dem die Rechner arbeiten. • Σlat = {a, b, c, . . . , z} ist das lateinische Alphabet. • ΣTastatur =Σlat ∪{A,B, ...,Z, ␣,>,<, (, ),..., !} ist das Alphabet aller Symbole der Rechnertastatur, wobei ␣ das Leersymbol ist. • Σm = {0, 1, 2,...,m − 1} für jedes m ≥ 1 ist ein Alphabet für die m-adische Darstellung von Zahlen. • Σlogic = {0, 1,x, (, ), ∧, ∨, ¬} ist ein Alphabet, in dem man Boole’sche Formeln gut darstellen kann. Im Folgenden deﬁnieren wir Wörter als Folgen von Buchstaben. Man bemerke, dass der Begriﬀ Wort in der Fachsprache der Informatik einem beliebigen Text entspricht und nicht nur der Bedeutung des Begriﬀs Wort in natürlichen Sprachen. Deﬁnition 2.2. Sei Σ ein Alphabet. Ein Wort über Σ ist eine endliche (eventuell leere) Folge von Buchstaben aus Σ. Das leere Wort λ ist die leere Buchstabenfolge. (Manchmal benutzt man ε statt λ.) Die Länge |w| eines Wortes w ist die Länge des Wortes als Folge, d. h. die Anzahl der Vorkommen von Buchstaben in w. Σ∗ ist die Menge aller Wörter über Σ, Σ +=Σ ∗ −{λ}. Die Folge 0, 1, 0, 0, 1, 1 ist ein Wort über Σbool und über ΣTastatur, |0, 1, 0, 0, 1, 1| =6. Das leere Wort λ ist ein Wort über jedem Alphabet, |λ| =0. 2.2 Alphabete, Wörter und Sprachen 15 Verabredung. Wir werden Wörter ohne Komma schreiben, das heißt, statt der Folge x1,x2,...,xn schreiben wir x1x2 ...xn. Statt 0, 1, 0, 0, 1, 1 benutzen wir also im Folgenden die Darstellung 010011. Das Leersymbol ␣ über ΣTastatur ist unterschiedlich von λ,esgilt |␣| = 1. Somit kann der Inhalt eines Buches oder ein Programm als ein Wort über ΣTastatur betrachtet werden. Es gilt (Σbool) ∗ = {λ, 0, 1, 00, 01, 10, 11, 000, 001, 010, 100, 011,...} = {λ}∪{x1x2 ...xi | i ∈ N,xj ∈ Σbool für j =1,...,i}. Wir sehen an diesem Beispiel, dass eine Möglichkeit, alle Wörter über einem Alphabet aufzuzählen, darin besteht, alle Wörter der Länge i =0, 1, 2,... hintereinander zu schreiben. Aufgabe 2.1. Bestimmen Sie für jedes i ∈ N, wie viele Wörter der Länge i über einem Alphabet Σ existieren. Aufgabe 2.2. Gegeben sei das Alphabet Σ = {0, 1, #}. Seien k, n positive ganze Zahlen mit k ≤ n. (a) Bestimmen Sie die Anzahl der verschiedenen Wörter der Länge n mit genau k Vorkommen des Symbols #. (b) Bestimmen Sie die Anzahl der verschiedenen Wörter der Länge n mit höchstens k Vor- kommen des Symbols #. Wörter können wir benutzen, um unterschiedliche Objekte wie zum Beispiel Zahlen, Formeln, Graphen und Programme darzustellen. Ein Wort x = x1x2 ...xn ∈ (Σbool)∗, xi ∈ Σbool für i =1,...,n, kann als die binäre Darstellung der Zahl Nummer(x)= n∑ i=1 xi · 2n−i betrachtet werden. Für eine natürliche Zahl m ∈ N −{0} bezeichnen wir mit Bin(m) ∈ (Σbool)∗ die kürzeste1 binäre Darstellung von m, also Nummer(Bin(m))= m. Wir deﬁnieren Bin(0) = 0. Aufgabe 2.3. Eine binäre Darstellung jeder positiven Zahl beginnt mit einer 1. Wie lang ist Bin(m) bei einer gegebenen Zahl m? Aufgabe 2.4. Sei x ∈ Σ ∗ m für ein m ≥ 1. Betrachten Sie x als m-adische Darstellung einer Zahl Nummerm(x). Wie berechnet man Nummerm(x)? Eine Zahlenfolge a1,a2,...,am, m ∈ N, ai ∈ N für i =1,...,m, kann man als Bin(a1)#Bin(a2)# ··· #Bin(am) ∈{0, 1, #}∗ darstellen. 1Dies bedeutet lediglich, dass das erste Symbol von Bin(m) eine 1 ist. 16 2 Alphabete, Wörter, Sprachen und die Darstellung von Problemen v1 v2 v3 v4 Abbildung 2.1 Sei G =(V, E) ein gerichteter Graph mit der Knotenmenge V und der Kantenmenge E ⊆{(u, v) | u, v ∈ V, u ̸= v}.Sei n = |V | die Kardinalität von V . Wir wissen, dass wir G durch eine Adjazenzmatrix MG repräsentieren können. MG =[aij] hat die Größe n × n und aij =1 ⇐⇒ (vi,vj) ∈ E. Daher bedeutet aij = 1, dass die Kante (vi,vj)in G vorhanden ist, und aij =0 bedeutet, dass die Kante (vi,vj)in G nicht vorhanden ist. Eine Matrix können wir als ein Wort über dem Alphabet Σ = {0, 1, #} repräsentieren. Wir schreiben einfach die Zeilen von MG hintereinander und das Symbol # benutzen wir, um das Ende einer Zeile zu markieren. Für den Graphen in Abbildung 2.1 ist die entsprechende Adjazenzmatrix ⎛ ⎜ ⎜ ⎝ 0011 0011 0101 0000 ⎞ ⎟ ⎟ ⎠ . Die vorgeschlagene Kodierung als Wort über {0, 1, #} ist 0011#0011#0101#0000#. Es ist klar, dass diese Darstellung eindeutig ist, was bedeutet, dass man aus der gegebenen Darstellung den Graphen eindeutig bestimmen kann. Aufgabe 2.5. Die vorgeschlagene Darstellung eines Graphen als Wort über {0, 1, #} hat die Länge n(n + 1) für einen Graphen mit n Knoten. Überlegen Sie sich eine kürzere eindeutige Darstellung von Graphen über dem Alphabet {0, 1, #}. Aufgabe 2.6. Entwerfen Sie eine Darstellung für Graphen über dem Alphabet Σbool. Bei algorithmischen Aufgaben sind die Eingaben oft gewichtete Graphen G =(V, E, h), wobei h eine Funktion von E nach N −{0} ist. Informell bedeutet dies, dass jeder Kante e ∈ E ein Gewicht (manchmal auch Kosten genannt) h(e) zugeordnet ist. Wir wissen, dass auch solche Graphen durch Adjazenzmatrizen darstellbar sind. Auch in diesem Fall 2.2 Alphabete, Wörter und Sprachen 17 v1 v2 v3v4 v5 1 1 1 6 6 5 7 Abbildung 2.2 bedeutet aij = 0, dass die Kante2 {vi,vj} nicht vorhanden ist. Falls {vi,vj}∈ E, dann ist aij = h({vi,vj}) das Gewicht der Kante {vi,vj}. In diesem Fall können wir die Gewichte aij = h({vi,vj}) binär darstellen und durch #-Symbole abtrennen. Um das Ende einer Zeile zu bezeichnen, kann man zwei # hinterein- ander benutzen. Damit hat der gewichtete Graph in Abbildung 2.2 mit der Adjazenzmatrix ⎛ ⎜ ⎜ ⎜ ⎜ ⎝ 07061 70110 01060 61605 10050 ⎞ ⎟ ⎟ ⎟ ⎟ ⎠ die folgende Darstellung über {0, 1, #}: 0#111#0#110#1##111#0#1#1#0##0#1#0#110#0##110# 1#110#0#101##1#0#0#101#0##. Weil der betrachtete Graph ungerichtet ist, gilt aij = aji für alle i, j. In unserer Darstellung bedeutet dies, dass die Information über das Gewicht jeder Kante in dem Wort doppelt vorkommt. Deswegen reicht es aus, nur die Matrixelemente oberhalb der Hauptdiagonalen zu betrachten. Das resultierende Wort über {0, 1, #} ist dann 111#0#110#1##1#1#0##110#0##101##. Als letztes Beispiel betrachten wir die Darstellung Boole’scher Formeln, die nur die Boole’schen Operationen Negation (¬), Disjunktion (∨) und Konjunktion (∧) benutzen. Im Folgenden bezeichnen wir Boole’sche Variablen in Formeln als x1,x2,.... Die Anzahl der möglichen Variablen ist unendlich und deswegen können wir x1,x2,... nicht als Buchstaben unseres Alphabets benutzen. Wir benutzen daher das Alphabet Σlogic = {0, 1,x, (, ), ∧, ∨, ¬} und kodieren die Boole’sche Variable xi als das Wort xBin(i) für 2Die ungerichtete Kante zwischen u und v bezeichnen wir hier als {u, v}. Für eine gerichtete Kante von u nach v benutzen wir die übliche Bezeichnung (u, v). 18 2 Alphabete, Wörter, Sprachen und die Darstellung von Problemen jedes i ∈ N. Die restlichen Symbole der Formel übernehmen wir eins zu eins. Damit hat die Formel (x1 ∨ x7) ∧¬(x12) ∧ (x4 ∨ x8 ∨¬(x2)) die folgende Darstellung (x1 ∨ x111) ∧¬(x1100) ∧ (x100 ∨ x1000 ∨¬(x10)). Eine nützliche Operation über Wörtern ist die einfache Verkettung zweier Wörter. Deﬁnition 2.3. Die Verkettung (Konkatenation) für ein Alphabet Σ ist eine Abbil- dung Kon : Σ ∗ × Σ∗ → Σ ∗, so dass Kon(x, y)= x · y = xy für alle x, y ∈ Σ∗. Sei Σ = {0, 1,a,b} und seien x =0aa1bb und y = 111b. Dann ist Kon(x, y)= x · y = 0aa1bb111b. Bemerkung 2.1. Die Verkettung Kon über Σ ist eine assoziative Operation über Σ ∗, weil Kon(u, Kon(v, w)) = u · (v · w)= uvw =(u · v) · w = Kon(Kon(u, v),w) für alle u, v, w ∈ Σ∗. Ferner gilt für jedes x ∈ Σ∗: x · λ = λ · x = x. Also ist (Σ ∗, Kon) eine Halbgruppe (Monoid) mit dem neutralen Element λ. Es ist klar, dass die Konkatenation nur für einelementige Alphabete kommutativ ist. Bemerkung 2.2. Für alle x, y ∈ Σ ∗ gilt: |xy| = |x · y| = |x| + |y|. Im Folgenden werden wir die einfache Notation xy an Stelle der Notationen Kon(x, y) und x · y bevorzugen. Deﬁnition 2.4. Für ein Wort a = a1a2 ...an, wobei ai ∈ Σ für i ∈{1, 2,...,n},be- zeichnet a R = anan−1 ...a1 die Umkehrung (Reversal) von a. Aufgabe 2.7. Sei Σ ein Alphabet und seien u, v ∈ Σ ∗. Beweisen oder widerlegen Sie: (uv) R = vRuR. Deﬁnition 2.5. Sei Σ ein Alphabet. Für alle x ∈ Σ∗ und alle i ∈ N deﬁnieren wir die i-te Iteration xi von x als x 0 = λ, x 1 = x und x i = xx i−1. 2.2 Alphabete, Wörter und Sprachen 19 Teilwort SuﬃxPräﬁx ab ... bb ...c ac ...b ab ...b b .. .a a ...b bb . ..ab Abbildung 2.3 So ist zum Beispiel Kon(aabba, aaaaa)= aabbaaaaaa = a 2b2a 6 = a 2b2(aa) 3. Wir sehen, dass uns die eingeführte Notation eine kürzere Darstellung von Wörtern ermöglicht. Im Folgenden deﬁnieren wir Teilwörter eines Wortes x als zusammenhängende Teile von x (Abbildung 2.3). Deﬁnition 2.6. Seien v, w ∈ Σ∗ für ein Alphabet Σ. • v heißt ein Teilwort von w ⇐⇒ ∃x, y ∈ Σ∗ : w = xvy. • v heißt ein Präﬁx von w ⇐⇒ ∃y ∈ Σ∗ : w = vy. • v heißt ein Suﬃx von w ⇐⇒ ∃x ∈ Σ∗ : w = xv. • v ̸= λ heißt ein echtes Teilwort (Präﬁx, Suﬃx) von w genau dann, wenn v ̸= w und v ein Teilwort (Präﬁx, Suﬃx) von w ist. Es gilt (abc) 3 = abcabcabc, und das Wort abc ist ein echtes Präﬁx von (abc) 3. Das Wort bc ist ein echtes Suﬃx von (abc) 3. Aufgabe 2.8. Sei Σ ein Alphabet und sei x ∈ Σ n für ein n ∈ N −{0}. Wie viele unterschiedliche Teilwörter kann x höchstens haben? Zählen Sie alle unterschiedlichen Teilwörter des Wortes abbcbbab auf. Deﬁnition 2.7. Seien x ∈ Σ∗ und a ∈ Σ. Dann ist |x|a deﬁniert als die Anzahl der Vorkommen von a in x. Für jede Menge A bezeichnet |A| die Kardinalität von A und P(A) = {S | S ⊆ A} die Potenzmenge von A. Also ist |abbab|a =2, |11bb0|0 = 1. Für alle x ∈ Σ∗ gilt |x| = ∑ a∈Σ |x|a. In diesem Buch brauchen wir oft eine feste Ordnung aller Wörter über einem gegebenen Alphabet. Die günstigste Möglichkeit für uns ist, die folgende kanonische Ordnung zu betrachten. Deﬁnition 2.8. Sei Σ= {s1,s2,...,sm}, m ≥ 1, ein Alphabet und sei s1 <s2 < ··· < sm eine Ordnung auf Σ. Wir deﬁnieren die kanonische Ordnung auf Σ ∗ für u, v ∈ Σ ∗ wie folgt: u<v ⇐⇒ |u| < |v| ∨|u| = |v|∧ u = x · si · u′ ∧ v = x · sj · v′ für irgendwelche x, u ′,v′ ∈ Σ∗ und i<j. 20 2 Alphabete, Wörter, Sprachen und die Darstellung von Problemen Unter dem Begriﬀ Sprache verstehen wir jede Menge von Wörtern über einem festen Alphabet. Deﬁnition 2.9. Eine Sprache L über einem Alphabet Σ ist eine Teilmenge von Σ ∗. Das Komplement L ∁ der Sprache L bezüglich Σ ist die Sprache Σ∗ − L. L∅ = ∅ ist die leere Sprache. Lλ = {λ} ist die einelementige Sprache, die nur aus dem leeren Wort besteht. Sind L1 und L2 Sprachen über Σ,soist L1 · L2 = L1L2 = {vw | v ∈ L1 und w ∈ L2} die Konkatenation von L1 und L2.Ist L eine Sprache über Σ, so deﬁnieren wir L 0 := Lλ und L i+1 = L i · L für alle i ∈ N, L∗ = ⋃ i∈N L i und L + = ⋃ i∈N−{0} L i = L · L ∗. L ∗ nennt man den Kleene’schen Stern von L. Die folgenden Mengen sind Sprachen über dem Alphabet Σ = {a, b}: • L1 = ∅, • L2 = {λ}, • L3 = {λ, ab, abab}, • L4 =Σ ∗ = {λ, a, b, aa, . . .}, • L5 =Σ + = {a, b, aa, . . .}, • L6 = {a} ∗ = {λ, a, aa, aaa, . . .} = {a i | i ∈ N}, • L7 = {a p | p ist eine Primzahl}, • L8 = {a ib2ia i | i ∈ N}, • L9 =Σ, • L10 =Σ 3 = {aaa, aab, aba, abb, baa, bab, bba, bbb}. Die Menge aller grammatisch korrekten Texte im Deutschen ist eine Sprache über ΣTastatur und die Menge aller syntaktisch korrekten Programme in C++ ist eine Sprache über ΣTastatur. Man bemerke, dass Σi = {x ∈ Σ∗ ||x| = i}, und dass L∅L = L∅ = ∅, Lλ · L = L. Aufgabe 2.9. Sei L1 = {λ, ab, b3a4} und L2 = {ab, b, ab2,b4}. Welche Wörter liegen in der Sprache L1L2? 2.2 Alphabete, Wörter und Sprachen 21 Unser nächstes Ziel ist, den Umgang mit Sprachen ein wenig zu üben. Weil Sprachen Mengen sind, haben die üblichen Operationen Vereinigung (∪) und Schnitt (∩) eine klare Bedeutung. Zu diesen Operationen haben wir die Konkatenation und den Kleene’schen Stern hinzugefügt. Die erste Frage, die wir uns stellen, ist, ob Distributivgesetze bezüglich ∪ und Konkatenation bzw. bezüglich ∩ und Konkatenation gelten. Für die Konkatenation und ∪ gibt uns das nächste Lemma 2.1 eine positive Antwort. Um den Beweis der Gleichheit von zwei Mengen (Sprachen) A und B zu führen, benutzen wir die üblichen Methoden der Mengentheorie. Wir zeigen nacheinander A ⊆ B und B ⊆ A,was A = B impliziert. Um A ⊆ B zu zeigen, reicht es zu beweisen, dass für jedes Element x ∈ A gilt, dass x ∈ B. Lemma 2.1. Seien L1,L2 und L3 Sprachen über einem Alphabet Σ. Dann gilt L1L2 ∪ L1L3 = L1(L2 ∪ L3). Beweis. Zuerst zeigen wir L1L2 ∪ L1L3 ⊆ L1(L2 ∪ L3). Die Bemerkungen in geschweiften Klammern sind die Begründung für den vorangegangenen Schritt. Es gilt: L1L2 ⊆ L1(L2 ∪ L3), weil L1L2 = {xy | x ∈ L1 ∧ y ∈ L2} {Deﬁnition der Konkatenation} ⊆{xy | x ∈ L1 ∧ y ∈ L2 ∪ L3} {weil L2 ⊆ L2 ∪ L3} = L1 · (L2 ∪ L3). {Deﬁnition der Konkatenation} Analog zeigt man L1L3 ⊆ L1(L2 ∪ L3). Daraus folgt L1L2 ∪ L1L3 ⊆ L1(L2 ∪ L3). Jetzt zeigen wir die Inklusion L1(L2 ∪ L3) ⊆ L1L2 ∪ L1L3. Sei im Folgenden x ∈ L1(L2 ∪ L3). Dann gilt x ∈{yz | y ∈ L1 ∧ z ∈ L2 ∪ L3} {Deﬁnition der Konkatenation} =⇒∃y ∈ L1 ∧∃z ∈ L2 ∪ L3, so dass x = yz =⇒∃y ∈ L1 ∧ (∃z ∈ L2 ∨∃z ∈ L3), so dass x = yz {Deﬁnition von ∪} ⇐⇒ (∃y ∈ L1 ∧∃z ∈ L2 : x = yz) ∨ (∃y ∈ L1 ∧∃z ∈ L3 : x = yz) {Distributivgesetz für ∧, ∨} ⇐⇒ (x ∈{yz | y ∈ L1 ∧ z ∈ L2} ︸ ︷︷ ︸ L1L2 ) ∨ (x ∈{yz | y ∈ L1 ∧ z ∈ L3} ︸ ︷︷ ︸ L1L3 ) {Deﬁnition der Konkatenation} ⇐⇒ x ∈ L1L2 ∪ L1L3. {Deﬁnition von ∪} \u0002 Jetzt wollen wir uns mit der Frage nach der Gültigkeit des Distributivgesetzes für die Konkatenation und den Schnitt beschäftigen. Vielleicht ist es auf den ersten Blick überraschend, dass hier die Antwort im allgemeinen negativ ist. Es gilt nur eine Inklusion, die wir zuerst zeigen. 22 2 Alphabete, Wörter, Sprachen und die Darstellung von Problemen Lemma 2.2. Seien L1,L2,L3 Sprachen über einem Alphabet Σ. Dann gilt L1(L2 ∩ L3) ⊆ L1L2 ∩ L1L3. Beweis. Sei x ∈ L1(L2 ∩ L3). Dies ist äquivalent zu x ∈{yz | y ∈ L1 ∧ z ∈ L2 ∩ L3} {Deﬁnition der Konkatenation} ⇐⇒ ∃y, z ∈ Σ∗,y ∈ L1 ∧ (z ∈ L2 ∧ z ∈ L3), so dass x = yz {Deﬁnition von ∩} ⇐⇒ ∃y, z ∈ Σ∗, (y ∈ L1 ∧ z ∈ L2) ∧ (y ∈ L1 ∧ z ∈ L3): x = yz =⇒∃y, z ∈ Σ∗, (yz ∈ L1L2) ∧ (yz ∈ L1L3): x = yz {Deﬁnition der Konkatenation} ⇐⇒ x ∈ L1L2 ∩ L1L3. {Deﬁnition von ∩} \u0002 Um zu zeigen, dass L1(L2 ∩ L3) ⊇ L1L2 ∩ L1L3 nicht im allgemeinen gilt, reicht es, drei konkrete Sprachen U1, U2 und U3 zu ﬁnden, so dass U1(U2 ∩ U3) ⊊ U1U2 ∩ U1U3. Die Idee für die Suche nach solchen U1, U2, U3 beruht auf der Tatsache, dass die einzige Implikation in dem Beweis von Lemma 2.2 nicht umgekehrt werden kann. Wenn ein Wort x in L1L2 und auch in L1L3 liegt, bedeutet das noch nicht x = yz,wobei y ∈ L1 und z ∈ L2 ∩ L3. Es kann passieren, dass es zwei verschiedene Zerlegungen von x gibt, so dass gilt x = y1z1 = y2z2 mit y1 ̸= y2 und y1 ∈ L1, z1 ∈ L2 und y2 ∈ L1, z2 ∈ L3. Lemma 2.3. Es existieren U1, U2, U3 ∈ (Σbool) ∗, so dass U1(U2 ∩ U3) ⊊ U1U2 ∩ U1U3. Beweis. Zuerst wählen wir U2 = {0} und U3 = {10}. Damit ist U2 ∩ U3 = ∅ und damit auch U1(U2 ∩ U3)= ∅ für jede Sprache U1. Jetzt reicht es aus, U1 so zu wählen, dass U1U2 ∩ U1U3 nicht leer wird. Wir setzen U1 = {λ, 1}. Dann ist U1U2 = {0, 10}, U1U3 = {10, 110} und damit U1U2 ∩ U1U3 = {10}. \u0002 Aufgabe 2.10. Seien L1, L2 und L3 Sprachen über dem Alphabet {0}. Gilt L1(L2 ∩ L3)= L1L2 ∩ L1L3? Aufgabe 2.11. Seien L1 ⊆ Σ ∗ 1 und L2,L3 ⊊ Σ ∗ 2 für zwei Alphabete Σ1 und Σ2 mit Σ1 ∩ Σ2 = ∅. Gilt L1(L2 ∩ L3)= L1L2 ∩ L1L3? Aufgabe 2.12. Existieren Sprachen L1, L2 und L3, so dass L1(L2 ∩L3) endlich und L1L2 ∩L1L3 unendlich ist? Im Folgenden üben wir noch den Umgang mit dem Kleene’schen Stern. 2.2 Alphabete, Wörter und Sprachen 23 Beispiel 2.1. Wir wollen beweisen, dass {a} ∗{b}∗ = {a ibj | i, j ∈ N} ist. Zuerst zeigen wir {a} ∗{b}∗ ⊆{a ibj | i, j ∈ N}.Sei x ∈{a}∗{b}∗. Dann gilt x = yz, wobei y ∈{a}∗ ∧ z ∈{b}∗ {Deﬁnition der Konkatenation} =⇒ x = yz, wobei (∃k ∈ N : y ∈{a}k) ∧ (∃m ∈ N : z ∈{b}m) {Deﬁnition des Kleene’schen Sterns} ⇐⇒ x = yz, wobei (∃k ∈ N : y = ak) ∧ (∃m ∈ N : z = bm) ⇐⇒ ∃k, m ∈ N, so dass x = a kbm =⇒ x ∈{aibj | i, j ∈ N}. Jetzt zeigen wir {a ibj | i, j ∈ N}⊆{a} ∗{b}∗.Sei x ∈{a ibj | i, j ∈ N}. Dann gilt x = a rbl für irgendwelche Zahlen r, l ∈ N =⇒ x ∈{a} ∗{b}∗, weil a r ∈{a}∗,b l ∈{b}∗. ♦ Aufgabe 2.13. Beweisen oder widerlegen Sie: ({a}∗{b}∗) ∗ = {a, b}∗. Deﬁnition 2.10. Seien Σ1 und Σ2 zwei beliebige Alphabete. Ein Homomorphismus von Σ∗ 1 nach Σ∗ 2 ist jede Funktion h :Σ∗ 1 → Σ ∗ 2 mit den folgenden Eigenschaften: (i) h(λ)= λ und (ii) h(uv)= h(u) · h(v) für alle u, v ∈ Σ ∗ 1. Wir beobachten leicht, dass es zur Speziﬁkation eines Homomorphismus reicht, h(a) für alle Buchstaben a ∈ Σ1 festzulegen. Aufgabe 2.14. Sei h ein Homomorphismus von Σ ∗ 1 nach Σ ∗ 2. Beweisen Sie mit Hilfe der Induk- tion, dass für jedes Wort x = x1x2 ...xm, xi ∈ Σ1 für i =1,...,m, h(x)= h(x1)h(x2) ...h(xm). Betrachten wir h(#)=10, h(0) = 00 und h(1) = 11. Es ist klar, dass h ein Homomor- phismus von {0, 1, #}∗ nach (Σbool) ∗ ist. Zum Beispiel gilt h(011#101#) = h(0)h(1)h(1)h(#)h(1)h(0)h(1)h(#) = 0011111011001110. Wir können h benutzen, um jede eindeutige Darstellung irgendwelcher Objekte über {0, 1, #} in eine neue eindeutige Darstellung dieser Objekte über Σbool zu überführen. Aufgabe 2.15. Deﬁnieren Sie einen Homomorphismus von {0, 1, #} ∗ nach (Σbool) ∗, der unend- lich viele unterschiedliche Wörter aus {0, 1, #}∗ auf das gleiche Wort aus (Σbool) ∗ abbildet. Aufgabe 2.16. Deﬁnieren Sie einen injektiven Homomorphismus von (Σlogic) ∗ nach (Σbool) ∗, um eine eindeutige Darstellung Boole’scher Formeln über Σbool zu erzeugen. 24 2 Alphabete, Wörter, Sprachen und die Darstellung von Problemen Aufgabe 2.17. Seien Σ1 und Σ2 zwei Alphabete. Sei h ein Homomorphismus von Σ ∗ 1 nach Σ ∗ 2. Für jede Sprache L ⊆ Σ ∗ 1 deﬁnieren wir h(L) = {h(w) | w ∈ L}. Seien L1,L2 ⊆ Σ∗ 1. Beweisen oder widerlegen Sie die folgende Aussage: h(L1)h(L2)= h(L1L2). 2.3 Algorithmische Probleme Bevor wir den intuitiven Begriﬀ Algorithmus durch das Modell der Turingmaschine formal deﬁnieren, werden wir statt Algorithmus den Begriﬀ Programm benutzen. Wir setzen voraus, dass der Leser weiß, was ein Programm ist. In welcher Programmiersprache es geschrieben ist, spielt hier keine Rolle. Wenn wir Programme als Algorithmen betrachten, fordern wir jedoch, dass ein solches Programm für jede zulässige Eingabe hält und eine Ausgabe liefert. Daher ist es für einen Algorithmus unzulässig, in eine Endlosschleife zu laufen. Mit dieser Voraussetzung realisiert ein Programm (Algorithmus) A typischerweise eine Abbildung A :Σ∗ 1 → Σ ∗ 2 für irgendwelche Alphabete Σ1 und Σ2. Dies bedeutet, dass (i) die Eingaben als Wörter kodiert sind, (ii) die Ausgaben als Wörter kodiert sind und (iii) A für jede Eingabe eine eindeutige Ausgabe bestimmt. Für jeden Algorithmus A und jede Eingabe x bezeichnen wir mit A(x) die Ausgabe des Algorithmus A für die Eingabe x. Wir sagen, dass zwei Algorithmen (Programme) A und B äquivalent sind, falls beide über dem gleichen Eingabealphabet Σ arbeiten und A(x)= B(x) für alle x ∈ Σ∗. Im Folgenden präsentieren wir einige grundlegende Klassen algorithmischer Probleme. Wir beginnen mit den Entscheidungsproblemen, die man üblicherweise benutzt, um die Theorie der Berechenbarkeit und die Komplexitätstheorie aufzubauen. Deﬁnition 2.11. Das Entscheidungsproblem (Σ,L) für ein gegebenes Alphabet Σ und eine gegebene Sprache L ⊆ Σ∗ ist, für jedes x ∈ Σ∗ zu entscheiden, ob x ∈ L oder x/∈ L. Ein Algorithmus A löst das Entscheidungsproblem (Σ,L), falls für alle x ∈ Σ∗ gilt: A(x)= { 1, falls x ∈ L, 0, falls x/∈ L. Wir sagen auch, dass A die Sprache L erkennt. 2.3 Algorithmische Probleme 25 Wenn für eine Sprache L ein Algorithmus existiert, der L erkennt, werden wir sagen, dass L rekursiv ist. 3 Wir benutzen häuﬁg eine Sprache L ⊆ Σ∗, um eine gewisse Eigenschaft von Wörtern aus Σ∗ (oder von Objekten, die durch die Wörter dargestellt sind) zu speziﬁzieren. Die Wörter, die in L sind, haben diese Eigenschaft und alle Wörter aus L ∁ =Σ∗ − L haben diese Eigenschaft nicht. Üblicherweise stellen wir ein Entscheidungsproblem (Σ,L) wie folgt dar: Eingabe: x ∈ Σ∗. Ausgabe: A(x) ∈ Σbool = {0, 1}, wobei A(x)= { 1, falls x ∈ L (Ja, x hat die Eigenschaft), 0, falls x/∈ L (Nein, x hat die Eigenschaft nicht). Beispielsweise ist ({a, b}, {anbn | n ∈ N}) ein Entscheidungsproblem, das man auch folgendermaßen darstellen kann: Eingabe: x ∈{a, b}∗. Ausgabe: Ja, falls x = a nbn für ein n ∈ N. Nein, sonst. Beispiel 2.2. Ein bekanntes und praktisch wichtiges Entscheidungsproblem ist der Prim- zahltest (Σbool, {x ∈ (Σbool) ∗ | Nummer(x) ist eine Primzahl}). Die übliche Darstellung ist Eingabe: x ∈ (Σbool) ∗. Ausgabe: Ja, falls Nummer(x) eine Primzahl ist. Nein, sonst. ♦ Beispiel 2.3. Sei L = {x ∈ (ΣTastatur)∗ | x ist ein syntaktisch korrektes Programm in C++}. Wir können folgendes Problem betrachten, das eine Teilaufgabe des Compilers ist. Eingabe: x ∈ (ΣTastatur) ∗. Ausgabe: Ja, falls x ∈ L. Nein, sonst. ♦ Beispiel 2.4. Das Problem des Hamiltonschen Kreises (HK)ist(Σ, HK), wobei Σ= {0, 1, #} und HK = {x ∈ Σ∗ | x kodiert einen ungerichteten Graphen, der einen Hamiltonschen Kreis enthält. 4}. ♦ 3Die Rekursivität ist ein wichtiger Begriﬀ und deswegen geben wir später mit dem formalen Modell der Berechnung einer Turingmaschine eine formal präzise Deﬁnition dieses Begriﬀes. 4Zur Erinnerung: Ein Hamiltonscher Kreis eines Graphen G ist ein geschlossener Weg (Kreis), der jeden Knoten von G genau einmal enthält. 26 2 Alphabete, Wörter, Sprachen und die Darstellung von Problemen Beispiel 2.5. Das Erfüllbarkeitsproblem aussagenlogischer Formeln (ERF)ist (Σlogic, ERF) mit ERF = {x ∈ (Σlogic) ∗ | x kodiert eine erfüllbare Boole’sche Formel}. ♦ Eine wichtige Teilklasse von Entscheidungsproblemen ist die Klasse der Äquivalenz- probleme. Das Äquivalenzproblem für Programme besteht zum Beispiel darin, für zwei Programme A und B in irgendeiner festen Programmiersprache (also für die Eingabe (A, B) ∈ (ΣTastatur)∗) zu entscheiden, ob A und B äquivalent sind. Ein anderes Beispiel ist, für zwei Boole’sche Formeln zu entscheiden, ob beide Formeln die gleiche Boole’sche Funktion darstellen. Deﬁnition 2.12. Seien Σ und Γ zwei Alphabete. Wir sagen, dass ein Algorithmus A eine Funktion (Transformation) f :Σ ∗ → Γ∗ berechnet (realisiert), falls A(x)= f (x) für alle x ∈ Σ∗. Entscheidungsprobleme sind spezielle Fälle von Funktionsberechnungen, weil das Lösen eines Entscheidungsproblems bedeutet, die charakteristische Funktion 5 einer Sprache zu berechnen. Auf den ersten Blick könnte man den Eindruck bekommen, dass die Berechnung von Funktionen die allgemeinste Darstellung algorithmischer Probleme ist. Die folgende Deﬁnition zeigt, dass das nicht der Fall ist. Deﬁnition 2.13. Seien Σ und Γ zwei Alphabete, und sei R ⊆ Σ ∗ × Γ ∗ eine Relation in Σ∗ und Γ∗. Ein Algorithmus A berechnet R (oder löst das Relationsproblem R), falls für jedes x ∈ Σ∗, für das ein y ∈ Γ∗ mit (x, y) ∈ R existiert, gilt: (x, A(x)) ∈ R. Man bemerke, dass es hinreichend ist, für jedes gegebene x eins von potenziell unendlich vielen y mit (x, y) ∈ R zu ﬁnden, um ein Relationsproblem zu lösen. Die folgenden Beispiele zeigen, dass die Relationsprobleme nicht nur eine mathematische Verallgemeinerung der Berechnung von Funktionen darstellen, sondern dass wir viele derartige Probleme in der Praxis haben. Sei Rfac ⊆ (Σbool)∗ × (Σbool)∗, wobei (x, y) ∈ Rfac genau dann, wenn entweder Nummer(y) ein Faktor 6 von Nummer(x) ist, oder y =1, wenn Nummer(x) eine Primzahl ist, oder y =0, wenn x ∈{0, 1}. Eine anschauliche Darstellung dieses Relationsproblems könnte wie folgt aussehen. Eingabe: x ∈ (Σbool) ∗. Ausgabe: y ∈ (Σbool) ∗,wobei Nummer(y)= ⎧ ⎪⎪⎨ ⎪⎪⎩ 0, falls x =0 oder x =1, 1, falls x ist eine Primzahl, k, sonst, wobei k ein Faktor von Nummer(x)ist. 5Die charakteristische Funktion fL einer Sprache L ⊆ Σ∗ ist eine Funktion von Σ∗ nach {0, 1} mit fL(x) = 1 genau dann, wenn x ∈ L. 6Eine Zahl a ist ein Faktor einer Zahl b, falls a die Zahl b teilt und a/∈{1,b}. 2.3 Algorithmische Probleme 27 Ein anderes schweres Problem ist die Beweiserstellung. Es sei RBeweis ⊆ (ΣTastatur) ∗ × (ΣTastatur)∗, wobei (x, y) ∈ RBeweis,wennentweder x eine wahre Aussage in einer be- stimmten mathematischen Theorie kodiert und y einen Beweis der durch x kodierten Aussage darstellt oder y = ␣,wenn x keine wahre Aussage darstellt. Für uns sind aber die Optimierungsprobleme, die einen Spezialfall von Relationsproble- men darstellen, von zentralem Interesse. Um die Struktur von Optimierungsproblemen anschaulich darzustellen, benutzen wir folgende Beschreibung statt der Relationsdarstel- lung. Informell bestimmt eine Eingabe x eines Optimierungsproblems eine Menge M(x) zulässiger Lösungen für x. Damit bekommen wir auch eine Relation R mit (x, y) ∈ R, wenn y eine zulässige Lösung von x ist. Aber dieses R ist nicht das zu lösende Rela- tionsproblem. Die Eingabe x bestimmt zusätzlich noch den Preis für jedes y mit der Eigenschaft (x, y) ∈ R. Die Ausgabe zu x muss eine zulässige Lösung mit dem günstigsten (je nach Aufgabenspeziﬁzierung minimalen oder maximalen) Preis sein. Deﬁnition 2.14. Ein Optimierungsproblem ist ein 6-Tupel U =(ΣI , ΣO,L, M, cost, goal), wobei: (i) ΣI ist ein Alphabet (genannt Eingabealphabet), (ii) ΣO ist ein Alphabet (genannt Ausgabealphabet), (iii) L ⊆ Σ ∗ I ist die Sprache der zulässigen Eingaben (als Eingaben kommen nur Wör- ter in Frage, die eine sinnvolle Bedeutung haben).Ein x ∈ L wird ein Problemfall (Instanz) von U genannt. (iv) M ist eine Funktion von L nach P(Σ ∗ O), und für jedes x ∈ L ist M(x) die Menge der zulässigen Lösungen für x, (v) cost ist eine Funktion, cost : ⋃ x∈L(M(x) ×{x}) → R +, genannt Kostenfunktion, (vi) goal ∈{Minimum, Maximum} ist das Optimierungsziel. Eine zulässige Lösung α ∈M(x) heißt optimal für den Problemfall x des Optimie- rungsproblems U , falls cost(α, x)= Opt U (x) = goal{cost(β, x) | β ∈M(x)}. Ein Algorithmus A löst U, falls für jedes x ∈ L (i) A(x) ∈M(x), {A(x) ist eine zulässige Lösung des Problemfalls x von U.} (ii) cost(A(x),x) = goal{cost(β, x) | β ∈M(x)}. Falls goal = Minimum, ist U ein Minimierungsproblem; falls goal = Maximum, ist U ein Maximierungsproblem. 28 2 Alphabete, Wörter, Sprachen und die Darstellung von Problemen In der Deﬁnition eines Optimierungsproblems als 6-Tupel hat das Eingabealphabet ΣI die gleiche Bedeutung wie das Alphabet von Entscheidungsproblemen, d. h., ΣI benutzt man, um die Instanzen von U darzustellen. Analog benutzt man das Ausgabealphabet ΣO für die Darstellung von Ausgaben (zulässigen Lösungen). Die Sprache L ⊆ Σ∗ I ist die Menge der Darstellungen von Problemfällen. Dies bedeutet, dass wir uns auf das Optimierungsproblem und nicht auf das Entscheidungsproblem (ΣI ,L) konzentrieren und dass wir voraussetzen, dass eine Eingabe aus Σ∗ I − L nie vorkommen wird. Ein Problemfall x ∈ L formuliert meistens eine Menge von Einschränkungen und M(x) ist die Menge von Objekten (zulässigen Lösungen zu x), die diese Einschränkungen erfüllen. In der Regel bestimmt der Problemfall x auch, wie hoch die Kosten cost(α, x) für jedes α ∈M(x) sind. Die Aufgabe ist, in der Menge der zulässigen Lösungen M(x) zu x eine optimale zu ﬁnden. Die typische Schwierigkeit ist, dass die Menge M(x) eine so große Mächtigkeit hat, dass es unmöglich ist, alle zulässigen Lösungen aus M(x)zu generieren und deren Kosten zu vergleichen. Um die Speziﬁkation konkreter Optimierungsprobleme zu veranschaulichen, lassen wir oft die Speziﬁkation von ΣI ,ΣO und die Darstellung von Daten über ΣI und ΣO aus. Wir setzen einfach voraus, dass die typischen Daten wie Zahlen, Graphen oder Formeln in der oben präsentierten Darstellung vorkommen. Dadurch reduzieren wir die Deﬁnition eines Optimierungsproblems auf die Speziﬁkation folgender vier Objekte: • die Menge der Problemfälle L, also die zulässigen Eingaben, • die Menge der Einschränkungen, gegeben durch jeden Problemfall x ∈ L, und damit M(x) für jedes x ∈ L, • die Kostenfunktion, • das Optimierungsziel. Beispiel 2.6. Das Traveling-Salesman-Problem (TSP). Die Aufgabe besteht darin, in einem gegebenen kantengewichteten, vollständigen Graphen einen Hamiltonschen Kreis mit minimalen Kosten zu ﬁnden. Eingabe: Ein kantengewichteter vollständiger Graph (G, c), wobei G =(V, E), V = {v1,...,vn} für ein n ∈ N −{0}, und c : E → N −{0}. {Strikt formal ist die Eingabe jedes Wort x ∈{0, 1, #} ∗, so dass x einen gewichteten, vollständigen Graphen (G, c) darstellt} Einschränkungen: Für jeden Problemfall (G, c)ist M(G, c) die Menge aller Hamil- tonschen Kreise von G. Jeder Hamiltonsche Kreis lässt sich durch ein (n +1)-Tupel von Knoten vi1,vi2,...,vin,vi1 darstellen, wobei (i1,...,in) eine Permutation von {1, 2,...,n} ist. Man beachte, dass diese Darstellung nicht eindeutig ist. {Eine streng formale Darstellung von M(G, c) wäre die Menge aller Wörter y1#y2# ... #yn ∈{0, 1, #} ∗ =Σ∗ O mit yi ∈{0, 1}+ für i =1, 2,...,n und {Nummer(y1), Nummer(y2),..., Nummer(yn)} = {1, 2,...,n}} Kosten: Für jeden Hamiltonschen Kreis H = vi1,vi2,...,vin,vi1 ∈M(G, c)ist cost((vi1,...,vin,vi1), (G, c)) = n∑ j=1 c({vij ,vi(j mod n)+1}), 2.3 Algorithmische Probleme 29 v1 1 1 1 1 1 22 3 v2 7 8 v3 v4 v5 Abbildung 2.4 d. h., die Kosten jedes Hamiltonschen Kreises entsprechen der Summe der Gewichte aller seiner Kanten. Ziel: Minimum. Für den Problemfall von TSP aus Abbildung 2.4 gilt: cost((v1,v2,v3,v4,v5,v1), (G, c)) = 8 +1+7+2+1 = 19, cost((v1,v5,v3,v2,v4,v1), (G, c)) = 1+1+1+1+1 = 5. Der Hamiltonsche Kreis v1, v5, v3, v2, v4, v1 ist die einzige optimale Lösung zu diesem Problemfall von TSP. ♦ Das TSP ist ein schweres Optimierungsproblem. In der Anwendung erscheinen aber oft nur Problemfälle, die gewisse gute Eigenschaften haben und für die man bei der Suche nach einer guten Lösung bessere Chancen hat. Wir sagen, dass ein Optimierungs- problem U 1 =(ΣI , ΣO,L ′, M, cost, goal)ein Teilproblem des Optimierungsproblems U 2 =(ΣI , ΣO,L, M, cost, goal) ist, falls L ′ ⊆ L ist. Auf diese Weise deﬁnieren wir auch das metrische TSP (Δ-TSP) als ein Teilproblem von TSP. Das bedeutet, dass die Einschränkungen, die Kosten und das Ziel genauso deﬁniert sind wie bei TSP, nur die Menge der Eingaben (Problemfälle) wird folgendermaßen eingeschränkt: Jeder Problemfall (G, c) von Δ-TSP erfüllt die sogenannte Dreiecksungleichung, was bedeutet, dass c({u, v}) ≤ c({u, w})+ c({w, v}) für alle Knoten u, v, w von G. Dies ist eine natürliche Eigenschaft, die besagt, dass die direkte Verbindung zwischen u und v nicht teurer sein darf als beliebige Umwege (Verbindungen über andere Knoten). Man bemerke, dass der Problemfall in Abbildung 2.4 die Dreiecksungleichung nicht erfüllt. Aufgabe 2.18. Beweisen Sie, dass |M((G, c))| =(n − 1)!/2, wenn G ein Graph mit n Knoten ist, wobei n> 2. 30 2 Alphabete, Wörter, Sprachen und die Darstellung von Problemen v1 v2 v3v4 v5 Abbildung 2.5 Eine Knotenüberdeckung eines Graphen G =(V, E) ist jede Knotenmenge U ⊆ V , so dass jede Kante aus E zu mindestens einem Knoten aus U inzident7 ist. Die Menge {v2,v4,v5} ist zum Beispiel eine Knotenüberdeckung des Graphen aus Abbildung 2.5, weil jede Kante mindestens zu einem dieser drei Knoten inzident ist. Die Menge {v1,v2,v3} ist keine Knotenüberdeckung des Graphen aus Abbildung 2.5, weil die Kante {v4,v5} durch keinen der Knoten v1, v2 und v3 bedeckt wird. Aufgabe 2.19. Das Knotenüberdeckungsproblem (minimum vertex cover problem, MIN-VC) ist ein Minimierungsproblem, bei dem man für einen gegebenen (ungerichteten) Graphen G eine Knotenüberdeckung mit minimaler Kardinalität sucht. (a) Bestimmen Sie die Menge aller Knotenüberdeckungen des Graphen aus Abbildung 2.5. (b) Geben Sie die formale Beschreibung des MIN-VC an. Speziﬁzieren Sie dabei die Darstellung der Eingaben und der Ausgaben über dem Alphabet {0, 1, #}. Beispiel 2.7. Das Problem der maximalen Clique (MAX-CL). Eine Clique eines Graphen G =(V, E) ist jede Teilmenge U ⊆ V , so dass {{u, v}| u, v ∈ U, u ̸= v}⊆ E (die Knoten von U bilden einen vollständigen Teilgraphen von G). Das Problem der maximalen Clique ist, eine Clique mit maximaler Kardinalität zu ﬁnden. Eingabe: Ein (ungerichteter) Graph G =(V, E). Einschränkungen: M(G)= {S ⊆ V |{{u, v}| u, v ∈ S, u ̸= v}⊆ E}. Kosten: Für jedes S ∈M(G) ist cost(S, G)= |S|. Ziel: Maximum. ♦ Aufgabe 2.20. Ein Graph T =(V, E′) heißt Spannbaum eines Graphen G =(V, E), falls T ein Baum (ein kreisfreier zusammenhängender Graph) und E′ ⊆ E ist. Das Gewicht eines Spannbaumes T =(V, E′) eines gewichteten Graphen (G, c) entspricht der Summe ∑ e∈E′ c(e) aller Gewichte der Kanten in E′. Das Problem des minimalen Spannbaumes ist, für einen gewichteten Graphen (G, c) einen Spannbaum mit minimalem Gewicht zu bestimmen. Geben Sie die formale Darstellung des Problems des minimalen Spannbaumes als Optimierungsproblem an. 7Eine Kante {u, v} ist inzident zu ihren Endpunkten u und v. 2.3 Algorithmische Probleme 31 Tabelle 2.1 x1 x2 x3 x1 ∨ x2 x1 ∨ x2 ∨ x3 x2 x2 ∨ x3 x3 x1 ∨ x3 # erfüllte Klauseln 000 0 1 1 0 0 1 3 001 0 1 1 1 1 1 5 010 1 1 0 1 0 1 4 011 1 1 0 1 1 1 5 100 1 1 1 0 0 1 4 101 1 1 1 1 1 0 5 110 1 1 0 1 0 1 4 111 1 0 0 1 1 0 3 Beispiel 2.8. Maximale Erfüllbarkeit (MAX-SAT). Sei X = {x1,x2,...} die Menge der Boole’schen Variablen. Die Menge der Literale über X ist LitX = {x, x | x ∈ X},wobei x die Negation von x bezeichnet. Eine Klausel ist eine beliebige endliche Disjunktion von Literalen (zum Beispiel x1 ∨ x3 ∨ x4 ∨ x7). Eine Formel ist in konjunktiver Normalform (KNF), wenn sie eine Konjunktion von Klauseln ist. Ein Beispiel einer Formel über X in KNF ist Φ=(x1 ∨ x2) ∧ (x1 ∨ x2 ∨ x3) ∧ x2 ∧ (x2 ∨ x3) ∧ x3 ∧ (x1 ∨ x3). Das Problem der maximalen Erfüllbarkeit ist, für eine gegebene Formel Φ in KNF eine Belegung ihrer Variablen zu ﬁnden, die die maximal mögliche Anzahl von Klauseln von Φ erfüllt. Eingabe: Eine Formel Φ = F1 ∧ F2 ∧ ··· ∧ Fm über X in KNF,wobei Fi eine Klausel für i =1,...,m ist, m ∈ N −{0}. Einschränkungen: Für jede Formel Φ über {xi1,xi2,...,xin} ist M(Φ) = {0, 1} n. {Jedes α = α1 ...αn ∈M(Φ), αj ∈{0, 1} für j =1,...,n, stellt eine Belegung dar, die xij den Wert αj zuordnet.} Kosten: Für jedes Φ und jedes α ∈M(Φ) ist cost(α, Φ) die Anzahl der Klauseln, die durch α erfüllt werden. Ziel: Maximum. Zu der oben gegebenen Formel Φ können wir in Tabelle 2.1 alle acht Belegungen der Va- riablen x1, x2, x3 betrachten und dadurch die optimale Lösung bestimmen. Die optimalen Lösungen sind die Belegungen 001, 011 und 101, die 5 Klauseln von Φ erfüllen. ♦ Beispiel 2.9. Ganzzahlige Lineare Programmierung (integer linear program- ming, ILP). Hier besteht die Aufgabe darin, für ein gegebenes System linearer Gleichun- gen und eine lineare Funktion von Unbekannten des linearen Systems eine Lösung dieses Systems zu berechnen, die minimal bezüglich der linearen Funktion ist. Eingabe: Eine (m × n)-Matrix A =[aij]i=1,...m,j=1,...,n und zwei Vektoren b = (b1,...,bm) T und c = (c1,...,cn) für m, n ∈ N −{0},wobei aij, bi, cj ganze Zahlen für i =1,...,m und j =1,...,n sind. 32 2 Alphabete, Wörter, Sprachen und die Darstellung von Problemen Einschränkungen: M(A, b, c)= {X =(x1,...,xn) T ∈ Nn | AX = b}. {M(A, b, c) ist die Menge aller Lösungsvektoren des linearen Systems AX = b, deren Elemente nur aus natürlichen Zahlen bestehen.} Kosten: Für jedes X =(x1,...,xn) T ∈M(A, b, c)ist cost(X, (A, b, c)) = n∑ i=1 cixi. Ziel: Minimum. ♦ Außer Entscheidungsproblemen und Optimierungproblemen betrachten wir auch algo- rithmische Probleme anderer Natur. Diese haben keine Eingabe, sondern die Aufgabe ist, ein Wort oder eine unendliche Reihenfolge von Symbolen zu generieren. Deﬁnition 2.15. Sei Σ ein Alphabet, und sei x ∈ Σ ∗. Wir sagen, dass ein Algorithmus A das Wort x generiert, falls A für die Eingabe λ die Ausgabe x liefert. Das folgende Programm A generiert das Wort 100111. A: begin write(100111); end Das folgende Programm An generiert das Wort (01) n für jedes n ∈ N −{0}. An: begin for i =1 to n do write(01); end Ein Programm, das x generiert, kann man als eine alternative Darstellung von x betrachten. Damit kann man einige Wörter im Rechner als Programme, die diese Wörter generieren, speichern. Deﬁnition 2.16. Sei Σ ein Alphabet, und sei L ⊆ Σ ∗. A ist ein Aufzählungsalgorith- mus für L, falls A für jede Eingabe n ∈ N −{0} die Wortfolge x1,x2,...,xn ausgibt, wobei x1,x2,...,xn die kanonisch n ersten Wörter in L sind. Beispiel 2.10. Seien Σ = {0} und L = {0p | p ist eine Primzahl}. Eingabe: n. Ausgabe: 02, 03, 0 5, 0 7,..., 0pn,wobei pn die n-te kleinste Primzahl ist. ♦ Aufgabe 2.21. Beweisen Sie, dass eine Sprache L genau dann rekursiv ist, wenn ein Aufzäh- lungsalgorithmus für L existiert. 2.4 Kolmogorov-Komplexität 33 2.4 Kolmogorov-Komplexität In diesem Abschnitt wollen wir Wörter als Informationsträger betrachten und einen sinnvollen Weg zur Messung des Informationsgehaltes von Wörtern ﬁnden. Wir beschrän- ken uns dabei auf Wörter über dem Alphabet Σbool. Die intuitive Grundidee könnte sein, einem Wort einen kleinen Informationsgehalt zuzuordnen, falls das Wort eine kurze Darstellung hat (komprimierbar ist), und einen großen Informationsgehalt, wenn es so unregelmäßig (irregulär) aussieht, dass man keine kürzere Darstellung des Wortes ﬁnden kann. So scheint das Wort 011011011011011011011011 mit der Darstellung (011) 8 einen kleineren Informationsgehalt als 0101101000101101001110110010 zu haben. Die Erzeugung einer kürzeren Darstellung eines Wortes x nennen wir eine Komprimierung von x. Die erste Idee wäre, sich eine feste Komprimierungsmethode zu überlegen und dann die Länge des resultierenden komprimierten Wortes als Maß für den Informationsgehalt zu nehmen. Natürlich muss dabei gefordert werden, dass die resultierende Darstellung wieder einWortüberΣbool ist, weil es sicherlich keine Kunst ist, mit mächtigeren Alphabeten kürzere Darstellungen zu erzeugen. Aufgabe 2.22. Finden Sie eine injektive Abbildung H von (Σbool) ∗ nach {0, 1, 2, 3, 4} ∗ =(Σ5) ∗, so dass |x|≥ 2 ·|H(x)|− 1 für jedes x ∈ (Σbool)∗, |x|≥ 4. Welchen Komprimierungsfaktor kann man erreichen, wenn man statt Σ5 das Alphabet Σm für ein m> 5 nimmt? Wenn wir als mögliche Komprimierung die Ausnutzung der Wiederholungen der Teil- wörter betrachten wollen, bekommen wir zuerst Wörter über dem Alphabet {0, 1, (, )} wie zum Beispiel (011)1000 für (011)8 oder (0)1010(010)1(01)1101 für (0) 10(010) 1(01) 13. Um diese wieder über Σbool zu kodieren, können wir einen Homomorphismus von {0, 1, (, )}∗ nach (Σbool) ∗ wie folgt deﬁnieren: h(0)=00,h(1)=11,h( ( )=10 und h( ) )=01. So bekommt das Wort (011) 8 die Darstellung 100011110111000000. Das Problem ist, dass man unendlich viele mögliche Komprimierungsmechanismen be- trachten kann. Welcher soll dann aber der richtige sein? Wir können zum Beispiel die vorgestellte Komprimierung noch so verbessern, dass wir die Darstellung von Potenzen 34 2 Alphabete, Wörter, Sprachen und die Darstellung von Problemen noch komprimieren. So kann ein Wort (011)1048576 kürzer als (011)220 dargestellt wer- den. Diese Strategie kann man natürlich beliebig fortsetzen, um kurze Darstellungen für Wörter wie (01)122n , (01)222n , ... zu generieren. Dies bedeutet, dass, egal welche dieser Komprimierungen wir nehmen, immer eine weitere Komprimierung existiert, die für unendlich viele Wörter bessere Resultate liefert. Daher können wir, um ein objektives Maß für den Informationsgehalt von Wörtern zu bekommen, keine dieser Strategien verfolgen. Es kommt noch schlimmer. Betrachten wir folgende Komprimierungsmethode. Für jedes x ∈ (Σbool) ∗ können wir Nummer(x) eindeutig als seine Faktorisierung pi1 1 · pi2 2 ·· · ·· pik k für Primzahlen p1 <p2 < ··· <pk, i1,i2,...,ik ∈ N −{0} für j =1, 2,...,k ausdrücken. Eine mögliche Darstellung von pi1 1 · pi2 2 ·· · ·· pik k über {0, 1, (, )} ist Bin(p1)(Bin(i1))Bin(p2)(Bin(i2)) ... Bin(pk)(Bin(ik)). Mit der Anwendung des Homomorphismus h bekommen wir wieder eine binäre Darstellung. Die schlechte Nachricht ist, dass diese Komprimierungsmethode unvergleichbar mit der vorherigen Methode der Wörterpotenzen ist. Also komprimiert die erste Methode für einige Wörter besser als die zweite und umgekehrt. Aufgabe 2.23. Finden Sie zwei Wörter x, y ∈ (Σbool) ∗, so dass (i) die Komprimierungsmethode der Teilwörterpotenzen eine wesentlich kürzere Darstellung für x liefert als die Methode der Primzahlzerlegung und (ii) die Komprimierungsmethode der Primzahlzerlegung für y zu einer wesentlich kürzeren Darstellung führt als die Methode der Teilwörterpotenzen. Eine Deﬁnition des Komplexitätsmaßes muss robust sein in dem Sinne, dass die gemesse- ne Komplexität eine breite Gültigkeit hat und daher unter unterschiedlichen Rahmenbedin- gungen genutzt werden kann. Bei einer Variierung der Komplexität in Abhängigkeit von der Komprimierungsmethode würde eine Festlegung auf eine Methode keine Möglichkeit zu allgemeinen Aussagen über den Informationsgehalt von Wörtern geben. Ein Ausweg aus dieser scheinbaren Sackgasse hat Kolmogorov durch folgende Deﬁnition gefunden. Es ist dabei wichtig zu beobachten, dass die Lösung unseres Komprimierungsproblems ohne die vorherige Einführung des Begriﬀs Algorithmus (Programm) nicht möglich gewesen wäre. Deﬁnition 2.17. Für jedes Wort x ∈ (Σbool) ∗ ist die Kolmogorov-Komplexität K (x) des Wortes x das Minimum der binären Längen der Pascal-Programme, die x generieren. Wir wissen, dass uns ein Übersetzer für Pascal zu jedem Pascal-Programm seinen Maschinencode generiert, der ein Wort über (Σbool)∗ ist. Für jedes Wort x betrachten wir also alle (unendlich vielen) Maschinencodes von Programmen, die x generieren und die Länge eines kürzesten ist die Kolmogorov-Komplexität von x.8 Ist K (x)ein guter Kandidat für die Deﬁnition des Informationsgehaltes von x?Wennesumdie 8Man bemerke, dass es mehrere unterschiedliche kürzeste Programme für x geben kann. 2.4 Kolmogorov-Komplexität 35 Einbeziehung aller möglichen Komprimierungsmethoden geht, sicherlich ja. Wir können zu jeder Komprimierungsmethode, die zu x eine komprimierte Darstellung y produziert, ein Programm schreiben, das y als Parameter (Konstante des Programms) beinhaltet, und das x aus y (basierend auf der Komprimierungsmethode) erzeugt. Bevor wir uns aber in diese Deﬁnition vertiefen, zeigen wir ein paar grundlegende Resultate über die Kolmogorov-Komplexität, um mehr Verständnis für dieses Komplexitätsmaß zu gewinnen. Das erste Resultat stellt sicher, dass K (x) nicht wesentlich länger sein darf als |x|,was natürlich erwünscht ist. Lemma 2.4. Es existiert eine Konstante d, so dass für jedes x ∈ (Σbool) ∗ K (x) ≤|x| + d. Beweis. Für jedes x ∈ (Σbool) ∗ nehmen wir folgendes Programm 9 Ax: Ax: begin write(x); end Die Teile begin, write, end und Kommata des Programms Ax sind gleich für jedes x ∈ (Σbool)∗ und die Länge ihrer Kodierung im Maschinencode bestimmt die (nicht zu große) Konstante d. Das Wort x ist im Programm als x binär dargestellt und leistet deswegen zur binären Darstellung von Ax nur den Beitrag |x|. Die letzte Frage, die noch zu beantworten bleibt, ist die folgende: „Wie erkennt man in der binären Darstellung des Programms Ax den Teil, der x entspricht?“ Das Problem liegt darin, dass wir zum Beispiel die Symbole des benutzten Alphabets ΣTastatur durch Folgen von acht Nullen und Einsen binär darstellen, beispielsweise in ASCII-Kodierung. Das gilt natürlich auch für die Darstellung der Symbole 0 und 1 aus ΣTastatur.Wennwir aber die Nullen und Einsen von x so repräsentieren würden, hätte Ax die Länge 8 ·|x| + d. Um x eins zu eins in Ax darzustellen, geben wir am Anfang der binären Kodierung von Ax zwei Zahlen k und l an. Diese werden wiederum mit einer ASCII-Kodierung dargestellt, voneinander beispielsweise durch ein Komma und vom eigentlichen Programm mit einem Semikolon getrennt. Die Zahl k gibt die Länge des Präﬁxes der Kodierung bis zu der Stelle an, an der x geschrieben wird. Die Zahl l gibt die Länge des Suﬃxes der binären Kodierung von Ax ab der Stelle an, an der das Wort x steht. Damit ist k die Länge der Kodierung von begin write( und l ist die Länge der Kodierung von ); end Das entsprechende Programm kann nun nicht mehr von einem Standard-Pascal-Compiler übersetzt werden, sondern erfordert einen Compiler, der diese Zahlen zunächst auswertet und das Programm anschließend übersetzt. Eine Alternative zur Angabe von k und l ist 9Zur Vereinfachung benutzen wir im Folgenden eine Pascal-ähnliche Programmiersprache, die zum Beispiel ohne Variablendeklaration auskommt. 36 2 Alphabete, Wörter, Sprachen und die Darstellung von Problemen es, dem Compiler in einer anderen Form mitzuteilen, dass das vorliegende Programm genau die gegebene Form besitzt. Wichtig ist noch zu beobachten, dass die Zahlen k und l unabhängig von x sind. Deswegen dürfen wir nicht statt l die Länge |x| von x am Anfang der Kodierung angeben, weil Bin(|x|) die Länge ⌈log2(|x| +1)⌉ hat, die von x abhängig ist. \u0002 Regelmäßige Wörter haben natürlich eine kleinere Kolmogorov-Komplexität. Sei yn = 0n ∈{0, 1} ∗ für jedes n ∈ N −{0}. Das folgende Programm Yn generiert yn. Yn: begin for I =1 to n do write(0); end Alle Programme Yn sind gleich bis auf das n. Die Kosten für die binäre Kodierung Bin(n)von n sind ⌈log2(n +1)⌉. Also existieren Konstanten c und c ′, so dass K (yn) ≤⌈log2(n +1)⌉ + c ′ = ⌈log2 n⌉ + c = ⌈log2 |yn|⌉ + c für alle n ∈ N −{0}. Betrachten wir jetzt zn =0 n2 ∈{0, 1} ∗ für jedes n ∈ N −{0}. Das folgende Programm Zn generiert das Wort zn. Zn: begin M := n; M := M × M ; for I =1 to M do write(0); end Alle Programme Zn sind gleich bis auf das n. Beachte, dass n die einzige Zahl (Konstante) ist, die in Zn vorkommt. Die Symbole M und I bezeichnen die Variablen, und für die Kodierung des Programms spielt es keine Rolle, wie groß die Zahlen sind, die den Variablen während der Ausführung des Programms zugewiesen werden. Wenn d die Größe der Kodierungen von Zn bis auf das n ist, erhalten wir K(zn) ≤⌈log2(n +1)⌉ + d ≤⌈log2( √ |zn|)⌉ + d +1. Aufgabe 2.24. Beweisen Sie die folgende Behauptung: Es existiert eine Konstante c, so dass für jedes n ∈ N −{0} K ((01) 2n) ≤⌈log2(n +1)⌉ + c = ⌈ log2 log2 (∣ ∣ ∣(01) 2n∣ ∣ ∣ /2 )⌉ + c. Aufgabe 2.25. Geben Sie eine unendliche Folge von Wörtern y1,y2,y3,... über Σbool mit folgenden Eigenschaften an: (i) |yi| < |yi+1| für alle i ∈ N −{0} und (ii) es existiert eine Konstante c, so dass K(yi) ≤⌈log2 log2 log2 |yi|⌉ + c für alle i ∈ N −{0} gilt. 2.4 Kolmogorov-Komplexität 37 Aufgabe 2.26. Beweisen Sie, dass für jede positive Zahl m ein Wort wm existiert, so dass |wm|− K (wm) >m. Wir können auch den Informationsgehalt von Zahlen messen, indem wir die Kolmogo- rov-Komplexität ihrer Binärdarstellung messen. Deﬁnition 2.18. Die Kolmogorov-Komplexität einer natürlichen Zahl n ist K (n)= K (Bin(n)). Das nächste Resultat zeigt, dass es auch nichtkomprimierbare Wörter im Sinne der Kolmogorov-Komplexität gibt. Lemma 2.5. Für jede Zahl n ∈ N −{0} existiert ein Wort wn ∈ (Σbool) n, so dass K(wn) ≥|wn| = n, d. h., es existiert für jede Zahl n ein nichtkomprimierbares Wort der Länge n. Beweis. Der Beweis basiert auf einer einfachen kombinatorischen Idee, die oft Anwendung ﬁndet. Wir haben genau 2n Wörter x1,...,x2n in (Σbool)n. Sei, für i =1, 2,..., 2n, C-Prog(xi) ∈{0, 1} ∗ der Maschinencode eines Programms Prog(xi), das xi generiert und K (xi)= |C-Prog(xi)| (also ist Prog(xi) eines der kürzesten Programme, die xi generieren). Es ist klar, dass für zwei unterschiedliche Wörter xi und xj die Kodierungen C-Prog(xi) und C-Prog(xj) unterschiedlich sein müssen. Das bedeutet, dass wir 2n unterschiedliche Maschinencodes C-Prog(x1), C-Prog(x2),..., C-Prog(x2n) von kürzesten Programmen für x1,x2,...,x2n haben. Es genügt zu zeigen, dass mindestens einer der Maschinencodes eine Länge von mindestens n haben muss. Das kombinatorische Argument für die gewünschte Behauptung sagt einfach, dass es unmöglich ist, 2 n unterschiedliche Maschinencodes der Länge kleiner als n zu haben. Jeder Maschinencode ist ein nichtleeres Wort über (Σbool) ∗. Die Anzahl der Wörter der Länge i über Σbool ist genau 2i. Daher ist die Anzahl aller unterschiedlichen nichtleeren Wörter über Σbool mit einer Länge von höchstens n − 1 n−1∑ i=1 2 i =2 n − 2 < 2 n. Also muss es unter den 2n Wörtern C-Prog(x1),..., C-Prog(x2n) mindestens eines mit Länge von mindestens n geben. Sei C-Prog(xj) ein solches Wort mit |C-Prog(xj)|≥ n. Weil |C-Prog(xj)| = K (xj), ist xj nicht komprimierbar. \u0002 Aufgabe 2.27. Beweisen Sie, dass für alle i, n ∈ N −{0}, i<n,2n − 2n−i unterschiedliche Wörter x in (Σbool) n existieren, so dass K(x) ≥ n − i. Aufgabe 2.28. Beweisen Sie, dass es unendlich viele Zahlen m gibt, so dass K (m) ≥⌈log2(m +1)⌉− 1. 38 2 Alphabete, Wörter, Sprachen und die Darstellung von Problemen Kehren wir jetzt zurück zur Frage, ob die Kolmogorov-Komplexität ein genügend robustes Maß für den Informationsgehalt von Wörtern ist. Statt einer speziﬁschen Kom- primierungsmethode haben wir ein formales Modell von Programmen genommen, das in sich alle möglichen Komprimierungsansätze implizit beinhaltet. Man kann aber die Festlegung auf eine Programmiersprache als zu große Einschränkung sehen. Wäre es für die Komprimierung einiger Wörter nicht günstiger, C++ oder Java statt Pascal zu betrachten? Spielt nicht die Festlegung einer Programmiersprache in der Deﬁnition der Kolmogorov-Komplexität eine ähnlich negative Rolle für die Robustheit der Messung des Informationsgehaltes wie die Festlegung einer Komprimierungsmethode? Die Antwort auf diese Fragen ist: Nein. Wir zeigen im Folgenden einen Satz, der aussagt, dass die Festlegung auf eine Pro- grammiersprache nur einen beschränkten Einﬂuss auf die Kolmogorov-Komplexität von Wörtern hat und daher unsere Festlegung auf Pascal in der Formalisierung des intuitiven Begriﬀs des Informationsgehalts akzeptiert werden kann. Für jedes Wort x über Σbool und jede Programmiersprache A sei KA(x) die Kolmogorov- Komplexität von x bezüglich der Programmiersprache A (die Länge des kürzesten Ma- schinencodes eines Programms in A, das x generiert). Satz 2.1. Seien A und B Programmiersprachen. Es existiert eine Konstante cA,B, die nur von A und B abhängt, so dass |KA(x) − KB(x)|≤ cA,B für alle x ∈ (Σbool) ∗. Beweis. Wir wissen, dass wir für beliebige Programmiersprachen A und B einen Inter- preter als ein Programm UA→B in B erstellen können, das jedes Programm aus A in ein äquivalentes Programm in B umschreibt und dann auf einer aktuellen Eingabe arbeiten lässt. Zur Vereinfachung kann man sich für die Wortgenerierung die Situation so vorstellen, dass UA→B ein Programm P in A als einen Eingabeparameter bekommt und die gleiche Arbeit wie P realisiert. Sei cA→B die binäre Länge des Programms UA→B.Sei Px ein Programm in A, das x generiert. Dann generiert das Programm UA→B mit Eingabe Px das Wort x.Weil UA→B mit Eingabe Px ein Programm in der Programmiersprache B ist, gilt KB(x) ≤ KA(x)+ cA→B. (2.1) Wenn man einen Übersetzer UB→A von B nach A mit einer binären Länge cB→A nimmt, bekommt man für jedes Wort x ∈ (Σbool) ∗ KA(x) ≤ KB(x)+ cB→A. (2.2) Wenn wir cA,B als das Maximum von cA→B und cB→A nehmen, implizieren (2.1) und (2.2), dass |KA(x) − KB(x)|≤ cA,B. \u0002 2.4 Kolmogorov-Komplexität 39 Nachdem wir die Kolmogorov-Komplexität als Maß des Informationsgehalts von Wörtern akzeptiert haben, wollen wir uns mit der Nützlichkeit dieses Begriﬀes beschäftigen. Die Kolmogorov-Komplexität hat viele Anwendungen, meistens als eine eﬃziente Methode für die Erstellung anschaulicher mathematischer Beweise. Im Folgenden präsentieren wir drei Anwendungen, für deren Verständnis keine besonderen Vorkenntnisse benötigt werden. Die erste Anwendung ist auf der grundlegenden Ebene der Begriﬀserzeugung. Der Zufall ist einer der grundlegenden Begriﬀe der Wissenschaft und wir werden uns damit in Kapitel 8 über zufallsgesteuerte Algorithmen beschäftigen. Hier stellen wir uns die Frage, wann man ein Objekt oder seine Darstellung als Wort als zufällig bezeichnen kann. Dabei kann die klassische Wahrscheinlichkeitstheorie nicht helfen, weil sie nur den Zufallserscheinungen die Wahrscheinlichkeit ihres Eintreﬀens zuordnet. Wenn man zum Beispiel Wörter aus (Σbool) n zufällig bezüglich einer uniformen Wahrscheinlichkeitsvertei- lung über (Σbool)n ziehen möchte, dann hat jedes Wort die gleiche Wahrscheinlichkeit gezogen zu werden. Könnte man diese Wahrscheinlichkeiten mit der Zufälligkeit in Ver- bindung bringen? Dann wäre 0 n genauso zufällig wie ein unregelmäßiges Wort und das entspricht nicht der intuitiven Vorstellung des Attributs zufällig. Nach dem Wörterbuch sollte zufällig bedeuten: „nach keinem Plan gebaut“. Ein zufälliges Objekt hat also eine chaotische Struktur, die keine Regularität besitzt. Und da sind wir mit unserem Begriﬀ der Komprimierung und des Informationsgehalts gerade richtig. Ein Wort ist zufällig, wenn für dieses Wort keine komprimierte Darstellung existiert, wenn es keinen anderen Plan zu seiner Erzeugung gibt als seine vollständige Beschreibung. Deswegen ist die folgende Deﬁnition bisher die beste bekannte Formalisierung des informellen Begriﬀes „zufällig“. Deﬁnition 2.19. Ein Wort x ∈ (Σbool)∗ heißt zufällig, falls K (x) ≥|x|. Eine Zahl n heißt zufällig, falls K(n)= K(Bin(n)) ≥⌈log2(n +1)⌉− 1. Die nächste Anwendung zeigt, dass die Existenz eines Programms, das ein Entschei- dungsproblem (Σbool,L) löst, zu gewissen Aussagen über die Kolmogorov-Komplexität der Wörter in L führen kann. Wenn L zum Beispiel keine zwei Wörter gleicher Länge enthält, dann ist die Kolmogorov-Komplexität jedes Wortes x aus L nicht viel größer als log2 |x|. Satz 2.2. Sei L eine Sprache über Σbool. Sei, für jedes n ∈ N −{0}, zn das n-te Wort in L bezüglich der kanonischen Ordnung. Wenn ein Programm AL existiert, das das Entscheidungsproblem (Σbool,L) löst, dann gilt für alle n ∈ N −{0}, dass K (zn) ≤⌈log2(n +1)⌉ + c, wobei c eine von n unabhängige Konstante ist. Beweis. Für jedes n entwerfen wir ein Programm Cn, das zn generiert. Jedes Cn enthält AL als Teilprogramm. 40 2 Alphabete, Wörter, Sprachen und die Darstellung von Problemen Cn: begin i =0; x := λ; while i< n do begin Berechne AL(x) mit dem Programm AL; if AL(x)=1 then begin i := i +1; z := x; end; x := Nachfolger von x in kanonischer Ordnung; end; write(z); end Wir sehen, dass Cn nacheinander die Wörter aus (Σbool)∗ in kanonischer Ordnung generiert. Für jedes Wort x überprüft Cn mit dem Teilprogramm AL,ob x in L ist oder nicht. Das Programm Cn zählt die Anzahl der von AL akzeptierten Wörter. Es ist klar, dass die Ausgabe von Cn das n-te Wort zn in L ist. Wir bemerken wieder, dass alle Programme Cn gleich sind bis auf den Parameter n. Wenn wir die binäre Länge von Cn bis auf den Teil, der n darstellt, mit c bezeichnen, dann ist die Länge von Cn genau c + ⌈log2(n +1)⌉ für jedes n ∈ N. Das Resultat des Satzes folgt direkt aus dieser Beobachtung. \u0002 Um ein typisches Missverständnis zu vermeiden, bemerken wir, dass die Länge des Programms Cn nichts damit zu tun hat, wie viel Speicherplatz die Variablen x, i und z während der Arbeit von Cn verbrauchen. Dass die Inhalte der Variablen von Cn mit der Zeit wesentlich längere Darstellungen als der Maschinencode von Cn haben können, spielt dabei keine Rolle, weil diese Werte nicht in die Beschreibung des Programms gehören. Der einzige Wert, der in die Beschreibung des Programms gehört, ist n. Aus der Sicht des Programms Cn ist aber n eine feste Konstante (Zahl) und wird daher als Teil des Maschinencodes von Cn dargestellt. Im nächsten Kapitel werden wir die Beweisidee von Satz 2.2 vertiefen. Aufgabe 2.29. Sei p ein Polynom einer Variablen. Sei L ⊆ (Σbool)∗ eine unendliche rekursive Sprache mit der Eigenschaft |L ∩ (Σbool) m|≤ p(m) für alle m ∈ N −{0}. Sei, für alle n ∈ N −{0}, zn das n-te Wort in L bezüglich der kanonischen Ordnung. Wie kann man K (zn)in |zn| nach oben beschränken? Die dritte und letzte Anwendung bezieht sich auf ein Resultat der Zahlentheorie, das enorm wichtig ist für den Entwurf zufallsgesteuerter Algorithmen. Für jede positive ganze Zahl n sei Prim(n) die Anzahl der Primzahlen kleiner gleich n. Der folgende grundlegende Satz der Zahlentheorie sagt, dass die Primzahlen relativ dicht zwischen den natürlichen Zahlen verstreut sind. Bei Unterstreichung der Primzahlen können wir diese Häuﬁgkeit der Primzahlen in folgender Folge kleiner Zahlen 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23,... 2.4 Kolmogorov-Komplexität 41 Tabelle 2.2 n Prim(n) Prim(n) n/ ln n 10 3 168 ≈ 1,161 10 6 78498 ≈ 1,084 10 9 50847478 ≈ 1,053 direkt beobachten. Im Folgenden bezeichnet pi immer die i-te kleinste Primzahl. Satz 2.3 (Primzahlsatz). lim n→∞ Prim(n) n/ ln n =1. Der Primzahlsatz ist eine der bemerkenswertesten Entdeckungen der Mathematik. Er sagt, dass die Anzahl der Primzahlen ungefähr so schnell wächst wie die Funktion n/ ln n. Für „kleine“ Werte von n kann man Prim(n) genau berechnen. Tabelle 2.2 zeigt einige solcher Werte. Wie nah Prim(n)an n/ ln n herankommt, zeigt die Ungleichung ln n − 3 2 < n Prim(n) < ln n − 1 2 für alle n ≥ 67. Aufgabe 2.30. Sei pi die i-te kleinste Primzahl für alle i ∈ N −{0}. Benutzen Sie Satz 2.3, um zu beweisen, dass lim n→∞ pn n ln n =1. Die ursprünglichen Beweise dieses Satzes (diese sind vor mehr als 100 Jahren entstanden) waren sehr kompliziert und kamen nicht ohne die Benutzung komplexer Zahlen aus. Auch die später vereinfachten Beweise sind zu schwer, um sie hier zu präsentieren. Dies sollte nicht überraschen, denn die Frage, wie schnell die Funktion Prim(n) /(n/ ln n) mit wachsendem n zu 1 konvergiert, hängt sehr stark mit der Riemann’schen Hypothese zusammen, einem der spektakulärsten oﬀenen Probleme der Mathematik. An dieser Stelle wollen wir die Nützlichkeit der Argumentation über die Kolmogorov- Komplexität zeigen, indem wir einen einfachen Beweis für eine schwächere Version des Primzahlsatzes geben. Diese einfachere Version des Primzahlsatzes ist aber stark genug, um für die meisten Anwendungen für den Entwurf zufallsgesteuerter Algorithmen hinreichend zu sein. Wir zeigen zuerst ein fundamentales Resultat, das besagt, dass es unendlich viele Primzahlen mit bestimmten Eigenschaften gibt. Lemma 2.6. Sei n1,n2,n3,... eine steigende unendliche Folge natürlicher Zahlen mit K (ni) ≥⌈log2 ni⌉/2. Für jedes i ∈ N −{0} sei qi die größte Primzahl, die die Zahl ni teilt. Dann ist die Menge Q = {qi | i ∈ N −{0}} unendlich. 42 2 Alphabete, Wörter, Sprachen und die Darstellung von Problemen Beweis. Wir beweisen Lemma 2.6 indirekt. Angenommen, Q = {qi | i ∈ N −{0}} ist eine endliche Menge. Sei pm die größte Primzahl in Q. Dann kann man jede Zahl ni eindeutig als ni = p ri,1 1 · pri,2 2 ·· · ·· pri,m m für irgendwelche ri,1,ri,2,...,ri,m ∈ N darstellen. Somit kann man ein einfaches Pro- gramm A entwerfen, das für gegebene Parameter ri,1,ri,2,...,ri,m die binäre Darstellung von ni erzeugt. Sei c die binäre Länge des Programms A außer der Darstellung der Parameter ri,1,...,ri,m (also die Darstellung des Teils von A, der für alle i ∈ N gleich ist). Dann gilt K (ni) ≤ c +8 · (⌈log2(ri,1 +1)⌉ + ⌈log2(ri,2 +1)⌉ + ··· + ⌈log2(ri,m +1)⌉) für alle i ∈ N −{0}. Die multiplikative Konstante 8 kommt daher, dass wir für die Zahlen ri,1,ri,2,...,ri,m dieselbe Kodierung verwenden wie für den Rest des Programms (zum Beispiel ASCII-Kodierung), damit ihre Darstellungen eindeutig voneinander getrennt werden können. Weil ri,j ≤ log2 ni für alle j ∈{1, 2,...,m} erhalten wir K (ni) ≤ c +8m ·⌈log2(log2 ni +1)⌉ für alle i ∈ N −{0}.Weil m und c Konstanten bezüglich i sind, kann ⌈log2 ni⌉/2 ≤ c +8m ·⌈log2(log2 ni +1)⌉ nur für endlich viele Zahlen i gelten. Dies ist aber ein Widerspruch zu unserer Vorausset- zung K (ni) ≥⌈log2 ni⌉/2 für alle i ∈ N −{0}. \u0002 Aufgabe 2.31. In Lemma 2.6 setzen wir eine unendliche steigende Folge natürlicher Zahlen n1,n2,n3,... mit der Eigenschaft K(ni) ≥⌈log2 ni⌉/2 voraus. Wie weit kann man diese Vor- aussetzung abschwächen (d. h. ⌈log2 ni⌉/2 verkleinern), ohne die Aussage von Lemma 2.6 zu verletzen? Lemma 2.6 zeigt nicht nur, dass es unendlich viele Primzahlen geben muss, sondern sogar, dass die Menge der größten Primzahlfaktoren einer beliebigen unendlichen Folge natürlicher Zahlen mit nichttrivialer Kolmogorov-Komplexität unendlich ist. Wir werden dieses Resultat im Beweis der folgenden unteren Schranke für Prim(n) benutzen. Satz 2.4. ∗ Für unendlich viele k ∈ N gilt Prim(k) ≥ k 217 log2 k · (log2 log2 k)2 . Beweis. Sei pj die j-te kleinste Primzahl für alle j ∈ N−{0}. Betrachten wir ein beliebiges n ≥ 2. Sei pm der größte Primfaktor von n. Es ist klar, dass man aus pm und n/pm durch Multiplikation die Zahl n generieren kann. Es geht auch mit noch weniger Information, nämlich m und n/pm, weil man für ein gegebenes m einfach pm mit einem Programm bestimmen kann, das Primzahlen aufzählt. Überlegen wir uns jetzt eine eindeutige und dabei kurze Darstellung des Paares (m, n/pm) als ein Wort über Σbool. Wir können nicht einfach Bin(m) und Bin(n/pm) 2.4 Kolmogorov-Komplexität 43 konkatenieren, weil man dann nicht weiß, wo das Wort Bin(m)in Bin(m)Bin(n/pm) endet, und somit die Darstellung nicht eindeutig wird. Versuchen wir jetzt, m anders zu kodieren. Sei Bin(m)= a1a2 ...a⌈log2(m+1)⌉ für ai ∈ Σbool für i =1, 2,..., ⌈log2(m +1)⌉. Dann setzen wir Bin(m)= a10a20a30 ...a⌈log2(m+1)⌉−10a⌈log2(m+1)⌉1. Das Wort Bin(m)Bin(n/pm) kodiert eindeutig (m, n/pm), weil das Ende der Kodierung von m eindeutig durch die erste 1 an einer geraden Stelle bestimmt ist. Die Länge dieser Darstellung ist 2 ·⌈log2(m +1)⌉ + ⌈log2((n/pm)+1)⌉. Das ist uns aber noch zu lang und deswegen kodieren wir (m, n/pm) als Bin(⌈log2(m +1)⌉)Bin(m)Bin(n/pm). Diese Darstellung ist eindeutig, weil das Ende von Bin(⌈log2(m +1)⌉) eindeutig zu erkennen ist und Bin(⌈log2(m +1)⌉) aussagt, dass die nächsten ⌈log2(m +1)⌉ Bits zum Teilwort Bin(m) gehören. Die Länge dieser Kodierung ist 2 ⌈log2(⌈log2(m +1)⌉ +1)⌉ + ⌈log2(m +1)⌉ + ⌈log2((n/pm)+1)⌉ . Auf diesem Prinzip aufbauend kann man unendlich viele Verbesserungen durchführen, für den Satz reicht eine weitere. Wir repräsentieren deﬁnitiv das Paar (m, n/pm) als Wort(m, n/pm) = Bin(⌈log2(⌈log2(m +1)⌉ +1)⌉)Bin(⌈log2(m +1)⌉)Bin(m)Bin(n/pm). Damit ist oﬀenbar |Wort(m, n/pm)| =2 ·⌈log2(⌈log2(⌈log2(m +1)⌉ +1)⌉ +1)⌉ + ⌈log2(⌈log2(m +1)⌉ +1)⌉ (2.3) + ⌈log2(m +1)⌉ + ⌈log2((n/pm)+1)⌉. Die Darstellung Wort(m, n/pm) betrachten wir jetzt als die Komprimierung von Bin(n). Weil wir eine feste Komprimierungsstrategie betrachten, bekommen wir mit demselben Argument wie in Lemma 2.5 und in Aufgabe 2.27, dass für alle i ∈ N −{0} für mehr als die Hälfte aller Zahlen n aus {2i, 2i +1,..., 2i+1 − 1} die Länge von Wort(m, n/pm) mindestens i − 1= ⌈log2(n +1)⌉− 2 ist. Analog hat mehr als die Hälfte der Zahlen aus {2 i, 2 i +1,..., 2 i+1 − 1} eine Kolmogorov-Komplexität von mindestens ⌈log2(n +1)⌉− 2. Folglich existiert für alle i ∈ N −{0} eine Zahl ni,2 i ≤ ni ≤ 2 i+1 − 1, mit |Wort(m, ni/pm)|≥|Bin(ni)|− 2= ⌈log2(ni +1)⌉− 2 und K (ni) ≥⌈log2(ni +1)⌉− 2. Also gibt es unendlich viele n mit den Eigenschaften |Wort(m, n/pm)|≥⌈log2(n +1)⌉− 2 (2.4) 44 2 Alphabete, Wörter, Sprachen und die Darstellung von Problemen und K (n) ≥⌈log2(n +1)⌉− 2. (2.5) Wenn wir (2.4) für ein solches n in (2.3) einsetzen, erhalten wir ⌈log2(n +1)⌉− 2 ≤ 2 ·⌈log2(⌈log2(⌈log2(m +1)⌉ +1)⌉ +1)⌉ + ⌈log2(⌈log2(m +1)⌉ +1)⌉ + ⌈log2(m +1)⌉ + ⌈log2((n/pm)+1)⌉ ≤ 2 ·⌈log2⌈log2⌈log2 m⌉⌉⌉ +6 + ⌈log2⌈log2 m⌉⌉ +2 + ⌈log2 m⌉ + ⌈log2(n/pm)⌉ +2 ≤ 2 ·⌈log2 log2 log2 m⌉ + ⌈log2 log2 m⌉ + ⌈log2 m⌉ +10 + ⌈log2(n/pm)⌉ {Es gilt ⌈log2 x⌉ = ⌈log2⌈x⌉⌉, für den Beweis siehe [GKP94].} ≤ 2 · log2 log2 log2 m + log2 log2 m + log2 m + log2(n/pm)+15 und somit log2 n ≤ 2 · log2 log2 log2 m + log2 log2 m + log2 m + log2(n/pm)+17. Weil log2(n/pm)=log2 n − log2 pm folgt daraus log2 pm ≤ 2 · log2 log2 log2 m + log2 log2 m + log2 m +17. Daher ist pm ≤ 2 17 · m · log2 m · (log2 log2 m) 2. (2.6) Wegen (2.5) erfüllt unsere unendliche Folge n1,n2,n3,... die Voraussetzung von Lem- ma 2.6. Damit gilt (2.6) für unendlich viele m. Das bedeutet, dass die m-te Primzahl höchstens 2 17 · m · log2 m · (log2 log2 m) 2 sein darf für unendlich viele m. Daher gibt es m Primzahlen unter den ersten 217 · m · log2 m · (log2 log2 m)2 natürlichen Zahlen. Anders ausgedrückt ist Prim(217 · m · log2 m · (log2 log2 m) 2) ≥ m (2.7) für unendlich viele m. Setzen wir jetzt die Substitution k =217 · m · log2 m · (log2 log2 m)2 in (2.7) ein. Weil k ≥ m ist, erhalten wir Prim(k) ≥ m = k 217 · log2 m · (log2 log2 m)2 ≥ k 217 · log2 k · (log2 log2 k)2 . \u0002 2.5 Zusammenfassung und Ausblick 45 Aufgabe 2.32. Betrachten Sie Bin(⌈log2(m +1)⌉) · Bin(m) · Bin(n/pm) als die Komprimie- rung von n = pm(n/pm). Welche untere Schranke auf Prim(k) würde die Anwendung dieser Komprimierung im Beweis von Satz 2.3 liefern? Aufgabe 2.33. Was für ein Resultat würde die Darstellung Bin(⌈log2(⌈log2(⌈log2(m +1)⌉ +1)⌉ +1)⌉)Bin(⌈log2(⌈log2(m +1)⌉ +1)⌉) · Bin(⌈log2(m +1)⌉)Bin(m)Bin(n/pm) von n = pm · (n/pm) im Beweis von Satz 2.3 bringen? Aufgabe 2.34.∗ Was ist die bestmögliche untere Schranke auf Prim(k), die man mit der Beweismethode von Satz 2.3 bekommen kann? Aufgabe 2.35.∗ Sei p1,p2,p3,... die aufsteigende Folge aller Primzahlen. Zeigen Sie mit Hilfe des Primzahlsatzes, dass eine Konstante c existiert, so dass für alle m ∈ N −{0} K(pm) ≤⌈log2 pm⌉−⌈log2 log2 pm⌉ + c gilt. 2.5 Zusammenfassung und Ausblick In diesem Kapitel haben wir die Grundbegriﬀe Alphabet, Wort und Sprache eingeführt. Ein Alphabet ist eine beliebige nichtleere endliche Menge von Symbolen, genau wie bei der Schrift natürlicher Sprachen. Der Begriﬀ Wort über einem Alphabet entspricht einem beliebigen Text, der aus den Symbolen des Alphabets besteht. Jede Menge von Wörtern über dem gleichen Alphabet nennt man Sprache. Diese Grundbegriﬀe benutzt man in allen Bereichen der Datenverarbeitung. Dies fängt an bei der Speziﬁkation algorithmischer Aufgaben, wo die Eingaben und Ausgaben immer als Wörter dargestellt werden. Ein Entscheidungsproblem ist im Prinzip durch eine Sprache L speziﬁziert. Die Aufgabe ist es, zu entscheiden, ob ein gegebenes Wort in L ist oder nicht. Ein Optimierungsproblem hat eine kompliziertere Struktur. Ein Eingabewort kodiert eine Menge von Anforderungen und die Kosten der Objekte (der zulässigen Lösungen), die die Anforderungen erfüllen. Meistens erfüllen sehr viele Lösungen diese Menge von Anforderungen, und man soll eine der optimalen bezüglich des Optimierungskriteriums (zum Beispiel die Lösung mit minimalen Kosten) als Ausgabe berechnen. Eine vernünftige Möglichkeit, den Informationsgehalt von Wörtern über {0, 1} zu messen, ist die Kolmogorov-Komplexität. Die Kolmogorov-Komplexität eines Wortes x ist die binäre Länge des kürzesten Pascal-Programms, das x generiert. Ein Programm, das x generiert, kann man als eine komprimierte Darstellung von x betrachten. Die Kolmogorov-Komplexität von x ist also die Länge der kürzesten Komprimierung von x. Die Wörter, die man nicht komprimieren kann, kann man als zufällig betrachten. Es gibt unendlich viele zufällige Wörter über {0, 1}, für jede Länge mindestens eins. Die Kolmogorov-Komplexität hilft nicht nur, den Informationsgehalt von Wörtern und deren Grad der Zufälligkeit zu messen, sondern man kann sie als eine eﬃziente Methode zum Beweisen neuer Resultate verwenden. 46 2 Alphabete, Wörter, Sprachen und die Darstellung von Problemen Das Studium von Alphabeten, Wörtern, Sprachen und deren Darstellung ist das Thema der Theorie der formalen Sprachen. Diese Theorie ist eines der ältesten Gebiete der Informatik. Weil in der Informatik die zentralen Objekte wie Informationen, Programme, Aussagen (Sätze), Nachrichten, Speicherinhalte, Beweise und Berechnungen als Wörter dargestellt sind, liefert die Theorie der formalen Sprachen die Bausteine für viele andere Grundgebiete der Informatik wie Berechenbarkeit, Komplexitätstheorie, Algorithmik, Kryptographie, Compilerbau usw. In diesem Buch haben wir uns auf minimale Kennt- nisse der Theorie der formalen Sprachen beschränkt, die hinreichend für die nächsten Kapitel sind. Ein Grund dafür ist auch, dass dem an der Theorie der formalen Sprachen Interessierten hervorragende Lehrbücher zur Verfügung stehen, und wir keinen Grund dafür sehen, das „hundertste“ Buch zu diesem Thema zu liefern. Wärmstens empfehlen wir das erfolgreichste klassische Lehrbuch der Theoretischen Informatik, „Introduction to Automata Theory, Languages, and Computation“ von Hopcroft und Ullman [HU79] und seine erweiterte neue Version [HMU06]. Ein anderes schön geschriebenes klassisches Buch ist [Sal73] von Salomaa. Von den neuesten deutschen Lehrbüchern können wir zum Beispiel Erk und Priese [EP00] und Wegener [Weg05] empfehlen. Die Idee, die Kolmogorov-Komplexität zur Messung des Informationsgehaltes von Wörtern einzuführen, entstand in den 60er Jahren unabhängig bei Kolmogorov [Kol65, Kol68] und Chaitin [Cha66, Cha69, Cha74]. Den umfassendsten Überblick über dieses Thema bietet das Buch von Li und Vitányi [LV93], das leider für Anfänger sehr schwer lesbar ist. Anschauliche Beispiele ihrer Anwendung kann man bei Schöning [Sch95] ﬁnden. Kontrollaufgaben 1. Sei Σ = {0, 1, 2, 3} ein Alphabet. Seien k, l und n positive Zahlen, k + l ≤ n. Wie viele Wörter über Σ der Länge n mit k Symbolen 1 und l Symbolen 0 gibt es? 2. Deﬁnieren Sie die Konkatenation von zwei Sprachen und den Kleene’schen Stern einer Sprache. 3. Seien Σ1 und Σ2 zwei Alphabete. Deﬁnieren Sie den Begriﬀ des Homomorphismus von Σ ∗ 1 nach Σ∗ 2. Warum ist jeder Homomorphismus h durch h(x) für jedes x ∈ Σ1 vollständig determiniert? 4. Seien L1,L2,L3 und L4 beliebige Sprachen über einem Alphabet Σ. Beweisen oder wider- legen Sie die folgenden Behauptungen. (a) (L1 ∪ L2) · (L3 ∪ L4)= L1L3 ∪ L1L4 ∪ L2L3 ∪ L2L4, (b) (L1 ∩ L2) · (L3 ∩ L4)= L1L3 ∩ L1L4 ∩ L2L3 ∩ L2L4, (c) (L1 ∪ L2) · (L3 ∩ L4)=(L1L3 ∩ L1L4) ∪ (L2L3 ∩ L2L4), (d) (L1 ∪ L2) · (L3 ∩ L4)= L1(L3 ∩ L4) ∪ L2(L3 ∩ L4). Wenn eine Gleichung nicht für alle Sprachen gelten sollte, überprüfen Sie, ob mindestens eine Inklusion gilt. 5. Gilt die Gleichung {a, b} ∗ = {a{b}∗} ∗ ·{b{a}∗} ∗? 2.5 Zusammenfassung und Ausblick 47 6. Beschreiben Sie informell die Bedeutung des Begriﬀes „Algorithmus“. 7. Deﬁnieren Sie folgende algorithmische Grundaufgabenstellungen: Entscheidungsproblem, Optimierungsproblem und Aufzählungsproblem. Erklären Sie, was es bedeutet, diese Probleme zu lösen. 8. Wie hängt die Existenz eines Algorithmus für ein Entscheidungsproblem (Σ,L) mit der Existenz eines Aufzählungsalgorithmus für L zusammen? 9. Suchen Sie sich ein Optimierungsproblem aus und geben Sie seine vollständige formale Beschreibung an. 10. Deﬁnieren Sie die Kolmogorov-Komplexität von Wörtern über {0, 1} und erklären Sie, warum man sich gerade auf diese Art der Messung des Informationsgehaltes von Wörtern geeinigt hat. 11. Wie stark hängt die Deﬁnition der Kolmogorov-Komplexität von der Wahl der Program- miersprache ab? 12. Wie deﬁniert man die Kolmogorov-Komplexität natürlicher Zahlen? 13. Sei Σ = {0, 1, 2, 3, 4, 5, 6, 7, 8, 9}. Wie würden Sie die Kolmogorov-Komplexität von Wörtern über Σ deﬁnieren? Betrachten Sie mehrere Möglichkeiten und diskutieren Sie ihre Vor- und Nachteile. 14. Warum ist die Kolmogorov-Komplexität eines Wortes x nie wesentlich größer als |x| und warum ist dies erwünscht? 15. Bei der Bestimmung einer oberen Schranke für die Kolmogorov-Komplexität K(x) eines Wortes x geht man meistens so vor, dass man ein Programm Px zur Generierung von x entwickelt, um dann die binäre Länge des Programms zu messen. Hängt diese Messung von der Größe des Speichers ab, den Px während der Generierung von x benutzt? Gibt es Unterschiede in der Messung der binären Länge der folgenden zwei Programme, die das gleiche Wort 0 n 3 generieren? P 1 n: begin M := n; M := M × M × M ; for I := 1 to M do write(0); end P 2 n: begin M := n × n × n; for I := 1 to M do write(0); end 16. Geben Sie eine unendliche Folge von Wörtern y1,y2,y3,... über Σbool mit folgenden Eigenschaften an: (i) |yi| < |yi+1| für alle i ∈ N −{0} und 48 2 Alphabete, Wörter, Sprachen und die Darstellung von Problemen (ii) es existiert eine Konstante c, so dass K(yi) ≤⌈log2√ |yi|⌉ + c für alle i ∈ N −{0} gilt. 17. Wie sieht es mit der Verteilung der Wörter aus {0, 1} n bezüglich deren Kolmogorov- Komplexität aus? Wie viele wenig komprimierbare Wörter gibt es? 18. Erklären Sie ohne die formalen Beweise zu wiederholen, warum die Annahme der Existenz zu weniger Primzahlen (zum Beispiel Prim(k) ≤ k/(log2 k) 2) unter den natürlichen Zahlen zu einem Widerspruch führt. Was hat das mit der Darstellung der Zahlen durch ihre Primfaktorzerlegung zu tun? Der Beweis einer hohen Bildung ist die Fähigkeit, über komplexe Sachen so einfach wie möglich zu sprechen. R. Emerson 3 Endliche Automaten 3.1 Zielsetzung Endliche Automaten sind das einfachste Berechnungsmodell, das man in der Informatik betrachtet. Im Prinzip entsprechen endliche Automaten speziellen Programmen, die gewisse Entscheidungsprobleme lösen und dabei keine Variablen benutzen. Endliche Automaten arbeiten in Echtzeit in dem Sinne, dass sie die Eingabe nur einmal von links nach rechts lesen, das Resultat steht sofort nach dem Lesen des letzten Buchstabens fest. Der Grund, endliche Automaten hier zu behandeln, ist nicht der Einstieg in die Automatentheorie. Wir nutzen die endlichen Automaten zu didaktischen Zwecken, um auf einfache und anschauliche Weise die Modellierung von Berechnungen zu erläutern. Daher führen wir einige Grundbegriﬀe der Informatik wie Konﬁguration, Berechnungsschritt, Berechnung, Simulation, Determinismus und Nichtdeterminismus für das Modell der endlichen Automaten mit dem Ziel ein, das grobe Verständnis für die allgemeine Bedeutung dieser Begriﬀe zu gewinnen. Dies sollte uns später die Entwicklung des Verständnisses dieser Begriﬀe im Zusammenhang mit einem allgemeinen Modell algorithmischer Berechnungen erleichtern. Wir lernen also in diesem Kapitel, wie man eine Teilklasse von Algorithmen (Program- men) formal und dabei anschaulich modellieren und untersuchen kann. Neben dem ersten Kontakt mit den oben erwähnten Grundbegriﬀen der Informatik lernen wir auch, was es bedeutet, einen Beweis zu führen, der zeigt, dass eine konkrete Aufgabe in einer gegebenen Teilklasse von Algorithmen nicht lösbar ist. 3.2 Die Darstellungen der endlichen Automaten Wenn man ein Berechnungsmodell deﬁnieren will, muss man folgende Fragen beantworten: 1. Welche elementaren Operationen, aus denen man die Programme zusammenstellen kann, stehen zur Verfügung? 2. Wie viel Speicher steht zur Verfügung und wie geht man mit dem Speicher um? J. Hromkovič, Theoretische Informatik, DOI 10.1007/978-3-658-06433-4_3, © Springer Fachmedien Wiesbaden 2014 50 3 Endliche Automaten 3. Wie wird die Eingabe eingegeben? 4. Wie wird die Ausgabe bestimmt (ausgegeben)? Bei endlichen Automaten hat man keinen Speicher zur Verfügung außer dem Speicher, in dem das Programm gespeichert wird und dem Zeiger, der auf die angewendete Zeile des Programms zeigt. Das bedeutet, dass das Programm keine Variablen benutzen darf. Das mag überraschend sein, weil man fragen kann, wie man ohne Variablen überhaupt rechnen kann. Die Idee dabei ist, dass der Inhalt des Zeigers, also die Nummer der aktuellen Programmzeile, die einzige wechselnde Information ist, und dass man mit dieser Pseudovariablen auskommen muss. Wenn Σ = {a1,a2,...,ak} das Alphabet ist, über dem die Eingaben dargestellt sind, dann darf der endliche Automat nur den folgenden Operationstyp benutzen: select input = a1 goto i1 input = a2 goto i2 ... input = ak goto ik Die Bedeutung dieser Operation (dieses Befehls) ist, dass man das nächste Eingabe- symbol liest und mit a1,a2,...,ak vergleicht. Wenn es gleich aj ist, setzt das Programm die Arbeit in der Zeile ij fort. Die Realisierung dieses Befehls bedeutet automatisch, dass das gelesene Symbol gelöscht wird und man daher in der Zeile ij das nächste Symbol liest. Jede Zeile des Programms enthält genau einen Befehl der oben angegebenen Form. Wir nummerieren die Zeilen mit natürlichen Zahlen 0, 1, 2, 3,... und die Arbeit des Programms beginnt immer in der Zeile 0. Wenn Σ nur aus zwei Symbolen (zum Beispiel 1 und 0) besteht, kann man statt des Befehls select den folgenden Befehl if ... then ... else benutzen. if input =1 then goto i else goto j. Solche Programme benutzt man, um Entscheidungsprobleme zu lösen. Die Antwort ist durch die Zeilennummer bestimmt. Wenn ein Programm aus m Zeilen besteht, wählt man eine Teilmenge F von {0, 1, 2,...,m − 1} aus. Wenn dann nach dem Lesen der gesamten Eingabe das Programm in der j-ten Zeile endet, und j ∈ F , dann akzeptiert das Programm die Eingabe. Wenn j ∈{0, 1, 2,...,m − 1}− F , dann akzeptiert das Programm die Eingabe nicht. Die Menge aller akzeptierten Wörter ist die von dem Programm akzeptierte (erkannte) Sprache. Betrachten wir als Beispiel folgendes Programm A, das Eingaben über dem Alphabet Σbool bearbeitet. 0: if input =1 then goto 1 else goto 2 1: if input =1 then goto 0 else goto 3 2: if input =0 then goto 0 else goto 3 3: if input =0 then goto 1 else goto 2 Setzen wir F = {0, 3}. Das Programm A arbeitet auf einer Eingabe 1011 wie folgt: Es startet in der Zeile 0, und geht in die Zeile 1, nachdem es eine 1 gelesen hat. Es liest eine 3.2 Die Darstellungen der endlichen Automaten 51 0000 111 Programm Lesekopf Eingabeband... Abbildung 3.1 0 in der ersten Zeile und geht in die dritte Zeile. In der dritten Zeile liest es eine 1 und geht in die zweite Zeile, um in die dritte Zeile zurückzukehren beim Lesen einer weiteren 1. Die Berechnung ist beendet, und weil 3 ∈ F gilt, wird das Wort 1011 akzeptiert. Mit endlichen Automaten verbindet man oft die schematische Darstellung aus Abbil- dung 3.1. In dieser Abbildung sehen wir die drei Hauptkomponenten des Modells: ein gespeichertes Programm,ein Band mit dem Eingabewort und einen Lesekopf, der sich auf dem Band nur von links nach rechts bewegen kann. 1 Das Band (auch Eingabeband genannt) betrachtet man als einen linearen Speicher für die Eingabe. Das Band besteht aus Feldern (Zellen). Ein Feld ist eine elementare Speichereinheit, die ein Symbol aus dem betrachteten Alphabet beinhalten kann. Die oben beschriebene Klasse von Programmen benutzt man heute fast gar nicht mehr, um endliche Automaten zu deﬁnieren, weil diese Programme wegen des goto-Befehls keine schöne Struktur haben. Daher ist diese Modellierungsart nicht sehr anschaulich und für die meisten Zwecke auch sehr unpraktisch. Die Idee einer umgangsfreundlicheren Deﬁnition endlicher Automaten basiert auf folgender visueller Darstellung unseres Programms. Wir ordnen jedem Programm A einen gerichteten markierten Graphen G(A)zu. G(A) hat genau so viele Knoten wie das Programm A Zeilen hat, und jeder Zeile von A ist genau ein Knoten zugeordnet, der durch die Nummer der Zeile markiert wird. Falls das Programm A aus einer Zeile i in die Zeile j beim Lesen eines Symbols b übergeht, dann enthält G(A) eine gerichtete Kante (i, j) mit der Markierung b. Weil unsere Programme ohne Variablen für jedes a ∈ Σ in jeder Zeile einen goto-Befehl haben, 2 hat jeder Knoten von G(A) genau den Ausgangsgrad3 |Σ|. Abbildung 3.2 enthält den Graphen G(A) für das oben beschriebene vierzeilige Programm A. Die Zeilen aus F sind durch Doppelkreise als besondere Knoten von G(A) gekennzeichnet. Der Knoten, der der Zeile 0 entspricht, wird durch einen zusätzlichen Pfeil (Abbildung 3.2) als der Anfangsknoten aller Berechnungen markiert. Aus dieser graphischen Darstellung entwickeln wir jetzt die standardisierte formale Deﬁnition endlicher Automaten. Die graphische Darstellung werden wir aber weiterhin benutzen, weil sie eine sehr anschauliche Beschreibung endlicher Automaten bietet. Die 1Die komponentenartige Darstellung allgemeiner Berechnungsmodelle beinhaltet außerdem noch einen Speicher, Schreib- und Lesezugriﬀsmöglichkeiten auf diesen Speicher und eventuell ein Ausgabemedium. 2Jede Zeile ist ein select über alle Symbole des Alphabets. 3Der Ausgangsgrad eines Knotens ist die Anzahl der gerichteten Kanten, die den Knoten verlassen. 52 3 Endliche Automaten 0 2 1 3 0 1 0 1 1 0 1 0 Abbildung 3.2 folgende formale Deﬁnition ist andererseits besser für das Studium der Eigenschaften endlicher Automaten und für die formale Beweisführung geeignet. Hierfür ändern wir teilweise die Terminologie. Was wir bisher als Zeile des Programms oder als Knoten des Graphen bezeichnet haben, werden wir im Weiteren als Zustand des endlichen Automaten bezeichnen. Die Kanten des Graphen, die den goto-Befehlen des Programms entsprechen, werden durch die sogenannte Übergangsfunktion beschrieben. Man beachte, dass der folgenden Deﬁnition ein allgemeines Schema zugrunde liegt, das man bei der Deﬁnition beliebiger Rechnermodelle anwenden kann. Zuerst deﬁniert man eine Struktur, die die exakte Beschreibung jedes Objekts aus der Modellklasse ermöglicht. Dann beschreibt man die Bedeutung (Semantik) dieser Struktur. Dies geschieht in folgender Reihenfolge. Zunächst deﬁniert man den Begriﬀ der Konﬁguration. Eine Konﬁguration ist die vollständige Beschreibung einer Situation (eines allgemeinen Zustands), in der sich das Modell beﬁndet. Dann deﬁniert man einen Schritt als einen Übergang aus einer Konﬁguration in eine andere Konﬁguration, wobei dieser Übergang durch eine elementare Aktion des Rechnermodells realisierbar sein muss. Eine Berechnung kann dann als eine Folge solcher Schritte gesehen werden. Wenn man eine Berechnung deﬁniert hat, kann man jeder Eingabe das Resultat der Arbeit des Rechnermodells als Ausgabe zuordnen. Deﬁnition 3.1. Ein (deterministischer) endlicher Automat (EA) ist ein Quintupel M =(Q, Σ,δ,q0,F ), wobei (i) Q eine endliche Menge von Zuständen ist, {vorher die Menge von Zeilen eines Programms ohne Variablen} (ii) Σ ein Alphabet, genannt Eingabealphabet, ist, {die Bedeutung ist, dass die zulässigen Eingaben alle Wörter über Σ sind} (iii) q0 ∈ Q der Anfangszustand ist, {vorher die Zeile 0 des Programms ohne Variablen} (iv) F ⊆ Q die Menge der akzeptierenden Zustände4 ist und 4In der deutschsprachigen Literatur auch Endzustände genannt. Der Begriﬀ „Endzustand“ kann aber auch zu Missverständnissen führen, weil die Berechnungen in einem beliebigen Zustand enden können. Außerdem entspricht der Begriﬀ „akzeptierender Zustand“ der wahren Bedeutung dieser Zustände und der Bezeichnung bei anderen Berechnungsmodellen wie bei Turingmaschinen. 3.2 Die Darstellungen der endlichen Automaten 53 (v) δ eine Funktion von Q × Σ nach Q ist, die Übergangsfunktion genannt wird. {δ(q, a)= p bedeutet, dass M in den Zustand p übergeht, falls M im Zustand q das Symbol a gelesen hat} Eine Konﬁguration von M ist ein Element aus Q × Σ ∗. {Wenn M sich in einer Konﬁguration (q, w) ∈ Q × Σ ∗ beﬁndet, bedeutet das, dass M im Zustand q ist und noch das Suﬃx w eines Eingabewortes lesen soll.} Die Konﬁguration (q0,x) ∈{q0}× Σ∗ heißt die Startkonﬁguration von M auf x. {Die Arbeit (Berechnung) von M auf x muss in der Startkonﬁguration (q0,x)von x anfangen.} Jede Konﬁguration aus Q ×{λ} nennt man eine Endkonﬁguration. Ein Schritt von M ist eine Relation (auf Konﬁgurationen) M ⊆ (Q × Σ ∗) × (Q × Σ ∗), deﬁniert durch (q, w) M (p, x) ⇐⇒ w = ax, a ∈ Σ und δ(q, a)= p. {Ein Schritt entspricht einer Anwendung der Übergangsfunktion auf die aktuelle Konﬁgu- ration, in der sich M in einem Zustand q beﬁndet und ein Eingabesymbol a liest.} Eine Berechnung C von M ist eine endliche Folge C = C0,C1,...,Cn von Konﬁgu- rationen, so dass Ci M Ci+1 für alle 0 ≤ i ≤ n − 1. C ist die Berechnung von M auf einer Eingabe x ∈ Σ∗, falls C0 =(q0,x) und Cn ∈ Q ×{λ} eine Endkonﬁguration ist. Falls Cn ∈ F ×{λ}, sagen wir, dass C eine akzeptierende Berechnung von M auf x ist, und dass M das Wort x akzeptiert.Falls Cn ∈ (Q − F ) ×{λ}, sagen wir, dass C eine verwerfende Berechnung von M auf x ist, und dass M das Wort x verwirft (nicht akzeptiert). {Man bemerke, dass M für jede Eingabe x ∈ Σ∗ genau eine Berechnung hat.} Die von M akzeptierte Sprache L(M ) ist deﬁniert als L(M )= {w ∈ Σ∗ | die Berechnung von M auf w endet in einer Endkonﬁguration (q, λ) mit q ∈ F }. LEA = {L(M ) | M ist ein EA} ist die Klasse der Sprachen, die von endlichen Automaten akzeptiert werden. LEA bezeichnet man auch als die Klasse der regulären Sprachen, und jede Sprache L aus LEA wird regulär genannt. Benutzen wir noch einmal das Programm A, um die gegebene Deﬁnition der endlichen Automaten zu illustrieren. Der zum Programm A äquivalente EA ist M =(Q, Σ,δ,q0,F ) mit Q = {q0,q1,q2,q3}, Σ= {0, 1},F = {q0,q3} und δ(q0, 0) = q2,δ(q0, 1) = q1,δ(q1, 0) = q3,δ(q1, 1) = q0, δ(q2, 0) = q0,δ(q2, 1) = q3,δ(q3, 0) = q1,δ(q3, 1) = q2. Anschaulicher kann man die Übergangsfunktion δ durch Tabelle 3.1 beschreiben. Die anschaulichste Darstellung eines EA ist aber die schon angesprochene graphische Form (Abbildung 3.2), die man für den EA in die in Abbildung 3.3 gegebene Form umwandeln kann. 54 3 Endliche Automaten Tabelle 3.1 Zustand Eingabe 01 q0 q2 q1 q1 q3 q0 q2 q0 q3 q3 q1 q2 q0 q2 q1 q3 0 1 0 1 1 0 1 0 Abbildung 3.3 Die Berechnung von M auf der Eingabe 1011 ist (q0, 1011) M (q1, 011) M (q3, 11) M (q2, 1) M (q3,λ). Weil q3 ∈ F , ist 1011 ∈ L(M ). Die folgende Deﬁnition führt Bezeichnungen ein, die den formalen Umgang mit endlichen Automaten erleichtern. Deﬁnition 3.2. Sei M =(Q, Σ,δ,q0,F ) ein endlicher Automat. Wir deﬁnieren M ∗ als die reﬂexive und transitive Hülle der Schrittrelation M von M ; daher ist (q, w) M ∗ (p, u) ⇐⇒ (q = p und w = u) oder ∃k ∈ N −{0}, so dass (i) w = a1a2 ...aku, ai ∈ Σ für i =1, 2,...,k, und (ii) ∃r1,r2,...,rk−1 ∈ Q, so dass (q, w) M (r1,a2 ...aku) M (r2,a3 ...aku) M ··· (rk−1,aku) M (p, u). Wir deﬁnieren ˆδ : Q × Σ∗ → Q durch: (i) ˆδ(q, λ)= q für alle q ∈ Q und (ii) ˆδ(q, wa)= δ(ˆδ(q, w),a) für alle a ∈ Σ, w ∈ Σ∗, q ∈ Q. Die Bedeutung von (q, w) M ∗ (p, u) ist, dass es eine Berechnung von M gibt, die ausgehend von der Konﬁguration (q, w) zu der Konﬁguration (p, u) führt. Die Aussage ˆδ(q, w)= p bedeutet, dass, wenn M im Zustand q das Wort w zu lesen beginnt, dann endet M im Zustand p (also (q, w) M ∗ (p, λ)). Daher können wir schreiben: L(M )= {w ∈ Σ∗ | (q0,w) M ∗ (p, λ) mit p ∈ F } = {w ∈ Σ∗ | ˆδ(q0,w) ∈ F }. 3.2 Die Darstellungen der endlichen Automaten 55 Kl[q0] Kl[q1] Kl[q2] Kl[q3] Kl[qn] ... Σ∗ ... Abbildung 3.4 Betrachten wir jetzt den EA M aus Abbildung 3.3. Versuchen wir, die Sprache L(M )zu bestimmen. Wir können leicht beobachten, dass für Wörter, die eine gerade [ungerade] Anzahl von Einsen haben, M die Berechnung in q0 oder q2 [q1 oder q3] beendet. Wenn die Anzahl der Nullen in einem x ∈ Σ ∗ gerade [ungerade] ist, dann ist ˆδ(q0,x) ∈{q0,q1} [ˆδ(q0,x) ∈{q2,q3}]. Diese Beobachtung führt zu folgender Behauptung. Lemma 3.1. L(M )= {w ∈{0, 1}∗ ||w|0 + |w|1 ≡ 0mod 2}. Beweis. Zuerst bemerken wir, dass jeder EA die Menge Σ ∗ in |Q| Klassen Kl[p] = {w ∈ Σ ∗ | ˆδ(q0,w)= p} = {w ∈ Σ∗ | (q0,w) M ∗ (p, λ)}, aufteilt, und es ist klar, dass ⋃p∈Q Kl[p]=Σ∗ und Kl[p] ∩ Kl[q]= ∅ für alle p, q ∈ Q, p ̸= q. In dieser Terminologie gilt L(M )= ⋃ p∈F Kl[p]. Anders ausgedrückt ist die Relation Rδ, die durch xRδy ⇐⇒ ˆδ(q0,x)= ˆδ(q0,y) deﬁniert wird, eine Äquivalenzrelation auf Σ ∗, die die endlich vielen Klassen Kl[p] bestimmt (Abbildung 3.4). Daher ist ein sicherer Weg, L(M ) zu bestimmen, die Bestimmung von Kl[q0], Kl[q1], Kl[q2] und Kl[q3] unseres EA M . Zu dieser Bestimmung stellen wir die folgende Indukti- onsannahme auf: Kl[q0]= {w ∈{0, 1} ∗ ||w|0 und |w|1 sind gerade}, Kl[q1]= {w ∈{0, 1} ∗ ||w|0 ist gerade, |w|1 ist ungerade}, Kl[q2]= {w ∈{0, 1}∗ ||w|0 ist ungerade, |w|1 ist gerade} und Kl[q3]= {w ∈{0, 1}∗ ||w|0 und |w|1 sind ungerade}. 56 3 Endliche Automaten Weil Kl[q0] ∪ Kl[q3]= {w ∈{0, 1}∗ ||w|0 + |w|1 ≡ 0mod 2}, ist die Behauptung von Lemma 3.1 eine direkte Folge unserer Induktionsannahme. Um den Beweis von Lemma 3.1 zu vervollständigen, reicht es also, die Induktionsannahme zu beweisen. Wir zeigen dies durch Induktion bezüglich der Eingabelänge. 1. Induktionsanfang. Wir beweisen die Induktionsannahme für alle Wörter der Länge kleiner gleich zwei. ˆδ(q0,λ)= q0 und daher ist λ ∈ Kl[q0], ˆδ(q0, 1) = q1 und daher ist 1 ∈ Kl[q1], ˆδ(q0, 0) = q2 und daher ist 0 ∈ Kl[q2], (q0, 00) M (q2, 0) M (q0,λ) und daher ist 00 ∈ Kl[q0], (q0, 01) M (q2, 1) M (q3,λ) und daher ist 01 ∈ Kl[q3], (q0, 10) M (q1, 0) M (q3,λ) und daher ist 10 ∈ Kl[q3], (q0, 11) M (q1, 1) M (q0,λ) und daher ist 11 ∈ Kl[q0]. Daher gilt die Induktionsannahme für die Wörter der Länge 0, 1 und 2. 2. Induktionsschritt. Wir setzen voraus, dass die Induktionsannahme für alle x ∈ (Σbool)∗, |x|≤ i, gilt. Wir wollen beweisen, dass sie auch für Wörter der Länge i + 1 gilt. Den Induktionsschritt beweisen wir für alle i ≥ 2, daher gilt die Induktionsannahme für alle Wörter aus (Σbool) ∗. Sei w ein beliebiges Wort aus (Σbool) i+1. Dann ist w = za,wobei z ∈ (Σbool) i und a ∈ Σbool. Wir unterscheiden vier Möglichkeiten bezüglich der Paritäten von |z|0 und |z|1. (a) Seien |z|0 und |z|1 beide gerade. Weil die Induktionsannahme ˆδ(q0,z)= q0 für z impliziert (daher ist z ∈ Kl[q0]), erhalten wir ˆδ(q0,za)= δ(ˆδ(q0,z),a)= Ind. δ(q0,a)= { q1, falls a =1, q2, falls a =0. Weil |z1|0 gerade und |z1|1 ungerade ist, entspricht ˆδ(q0,z1) = q1 der Indukti- onsannahme z1 ∈ Kl[q1]. Weil |z0|0 ungerade und |z0|1 gerade ist, stimmt das Resultat ˆδ(q0,z0) = q2 mit der Induktionsannahme z0 ∈ Kl[q2] überein. (b) Seien |z|0 und |z|1 beide ungerade. Weil ˆδ(q0,z)= q3 (daher ist z ∈ Kl[q3]) bezüglich der Induktionsannahme für z, erhalten wir ˆδ(q0,za)= δ(ˆδ(q0,z),a)= Ind. δ(q3,a)= { q2, falls a =1, q1, falls a =0. 3.2 Die Darstellungen der endlichen Automaten 57 Dies entspricht der Induktionsannahme, dass z0 ∈ Kl[q1] und z1 ∈ Kl[q2]. Die Fälle (c) und (d) sind analog und wir überlassen sie dem Leser als Übung. \u0002 Aufgabe 3.1. Vervollständigen Sie den Beweis der Induktionsannahme aus Lemma 3.1 (Fälle (c) und (d)). Aufgabe 3.2. Sei L = {w ∈ (Σbool)∗ ||w|0 ist ungerade}. Entwerfen Sie einen EA M mit L(M )= L und beweisen Sie L(M )= L. Aufgabe 3.3. Entwerfen Sie endliche Automaten für die Sprachen ∅,Σ∗ und Σ+ für ein beliebiges Alphabet Σ. Geben Sie die formale Darstellung der Automaten als Quintupel, sowie die graphische Darstellung an. Wenn ein EA A genügend anschaulich und strukturiert dargestellt wird, kann man die Sprache L(A) auch ohne eine Beweisführung bestimmen. In den meisten Fällen verzichtet man also auf einen formalen Beweis, wie er in Lemma 3.1 geführt wurde. 5 Aus Lemma 3.1 haben wir aber etwas Wichtiges für den Entwurf eines EA gelernt. Eine gute Entwurfsstrategie ist es, die Menge aller Wörter aus Σ∗ in Teilklassen von Wörtern mit gewissen Eigenschaften zu zerlegen und „Übergänge“ zwischen diesen Klassen bezüglich der Konkatenation eines Symbols aus Σ zu deﬁnieren. Betrachten wir diese Strategie für den Entwurf eines EA für die Sprache U = {w ∈ (Σbool) ∗ ||w|0 = 3 und (|w|1 ≥ 2 oder |w|1 =0)}. Um |w|0 = 3 veriﬁzieren zu können, muss jeder EA B mit L(B)= U für die Anzahl der bisher gelesenen Nullen die Fälle |w|0 =0, |w|0 =1, |w|0 =2, |w|0 = 3 und |w|0 ≥ 4 unterscheiden können. Gleichzeitig muss B auch die Anzahl der bisher gelesenen Einsen zählen, um mindestens die Fälle |w|1 =0, |w|1 = 1 und |w|1 ≥ 2 unterscheiden zu können. Daraus resultiert die Idee, die Zustandsmenge Q = {qi,j | i ∈{0, 1, 2, 3, 4},j ∈{0, 1, 2}} zu wählen. Die Bedeutung sollte folgende sein: Für alle j ∈{0, 1} und alle i ∈{0, 1, 2, 3} ist Kl[qi,j]= {w ∈ (Σbool) ∗ ||w|0 = i und |w|1 = j}. Für alle i ∈{0, 1, 2, 3} ist Kl[qi,2]= {w ∈ (Σbool) ∗ ||w|0 = i und |w|1 ≥ 2}. Für j ∈{0, 1} ist Kl[q4,j]= {w ∈ (Σbool) ∗ ||w|0 ≥ 4 und |w|1 = j}, Kl[q4,2]= {w ∈ (Σbool) ∗ ||w|0 ≥ 4 und |w|1 ≥ 2}. 5Man bemerke, dass dies der Situation entspricht, in der man gewöhnlicherweise des Aufwands wegen auf den formalen Beweis der Korrektheit eines entworfenen Programms verzichtet. 58 3 Endliche Automaten q0,0 q1,0 q2,0 q3,0 q4,0 q0,1 q1,1 q2,1 q3,1 q4,1 q0,2 q1,2 q2,2 q3,2 q4,2 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0, 1 0 0 Abbildung 3.5 Es ist klar, dass q0,0 der Anfangszustand ist. Die Übergangsfunktion von B kann man direkt aus der Bedeutung der Zustände qi,j bestimmen, wie in Abbildung 3.5 gezeigt. Wir bemerken, dass U =Kl[q3,0] ∪ Kl[q3,2] und setzen daher F = {q3,0,q3,2}. Aufgabe 3.4. (a) Beweisen Sie L(B)= U für den EA B aus Abbildung 3.5. (b) Entwerfen Sie einen EA A mit L(A)= U , so dass A weniger Zustände als B hat, und geben Sie zu jedem Zustand q dieses Automaten die Klasse Kl[q] an. Die Methode zum Automatenentwurf, die auf der Bestimmung der Bedeutung der Zu- stände basiert, ist die grundlegendste Entwurfsstrategie für endliche Automaten. Deswegen präsentieren wir noch zwei anschauliche Anwendungen dieser Methode. Beispiel 3.1. Unsere nächste Aufgabe ist, einen EA für die Sprache L(0010) = {x0010y | x, y ∈{0, 1}∗} zu entwerfen. Hier ist der Entwurf ein bisschen schwieriger als bei einer Sprache, bei der alle Wörter mit 0010 anfangen müssen. Wir müssen feststellen, ob 0010 irgendwo in dem Eingabewort liegt. Die erste Idee ist die, dass der EA sich merken muss, welches Präﬁx von 0010 er gerade in den zuletzt gelesenen Buchstaben gefunden hat. Wenn das bisher gelesene Wort zum Beispiel 011001 war, dann muss er sich merken, dass er schon 001 als Kandidaten gefunden hat, und wenn jetzt das nächste Symbol 0 ist, dann muss er akzeptieren. Wenn der EA das Präﬁx 1100 einer Eingabe gelesen hat, muss er sich merken, dass die letzten 3.2 Die Darstellungen der endlichen Automaten 59 p0 p1 p2 p3 p4 1 0 0 1 0 0, 1 Abbildung 3.6 p0 p1 p2 p3 p4 1 0 0 1 0 0, 1 1 0 1 Abbildung 3.7 zwei Symbole 00 waren.6 Damit ergeben sich fünf mögliche Zustände entsprechend den fünf Präﬁxen von 0010. Wir ziehen jetzt vor, deren Bedeutung anschaulich statt genau formal zu beschreiben. Kl[p0]: Kein Präﬁx von 0010 ist ein nichtleeres Suﬃx des bisher gelesenen Wortes x (zum Beispiel x = λ oder x endet mit 11). Kl[p1]: Die Wörter enden mit 0 und enthalten keinen längeren Präﬁx von 0010 als ein Suﬃx (sie enden zum Beispiel mit 110). Kl[p2]: Die Wörter enden mit 00. Kl[p3]: Die Wörter enden mit 001. Kl[p4]: Die Wörter enden mit 0010 oder beinhalten 0010 als Teilwort. Aus diesen Klassenbeschreibungen erhalten wir die in Abbildung 3.6 dargestellte Teilstruktur des zu konstruierenden EA. Dass die Folge 0010 gelesen werden muss, um den einzigen akzeptierenden Zustand p4 zu erreichen, ist oﬀensichtlich. Wenn der EA p4 erreicht hat, hat er schon 0010 in der Eingabe gefunden und bleibt somit im akzeptierenden Zustand (δ(p4, 0) = δ(p4, 1) = p4), egal was noch kommt. Das Lesen einer 1 in p0 ändert nichts daran, dass wir noch kein Präﬁx von 0010 in den letzten Buchstaben gesehen haben. Um den EA zu vervollständigen, fehlen uns drei Pfeile aus den Zuständen p1,p2 und p3 für das Lesen von 1 aus p1 und p3 und das Lesen von 0 aus p2. Es gibt nur eine eindeutige Möglichkeit, dies korrekt zu machen (um L(0010) zu erkennen) und diese ist in Abbildung 3.7 dargestellt. In p1 ist 0 das längste Suﬃx des gelesenen Wortes, das einem Präﬁx von 0010 entspricht. Wenn wir nun eine 1 lesen, müssen wir unsere Suche nach 0010 neu beginnen, weil eine 1 am Ende bedeutet, dass wir aktuell mit 01 kein Präﬁx von 0010 haben. Damit ist δ(p1, 1) = p0. 6Im Prinzip reicht es, die Zahl 2 zu speichern, weil das Wort 0010 bekannt ist und klar ist, dass 00 der Präﬁx der Länge 2 ist. 60 3 Endliche Automaten Wenn man in p2 eine 0 liest, ändert sich nichts an der Tatsache, dass 00 das Suﬃx des gerade gelesenen Wortes ist und somit bleiben wir in p2, also ist δ(p2, 0) = p2. Wenn in p3 eine 1 kommt, endet das gelesene Wort mit 11 und somit kann das Wort am Ende kein nichtleeres Suﬃx von 0010 enthalten. Der einzige mögliche Schluss ist δ(p3, 1) = p0. Damit ist der EA vervollständigt. ♦ Aufgabe 3.5. Geben Sie eine genaue formale Beschreibung der Klassen der Zustände des EA in Abbildung 3.7 an. In Beispiel 3.1 haben wir einen EA in Abbildung 3.7 entworfen, der genau die Wörter akzeptiert, die das Wort 0010 als Teilwort enthalten. Der anstrengendste Teil des Entwurfs bestand in der Bestimmung der Kanten (Transitionen), wenn die Suche nach 0010 wegen einer Unstimmigkeit unterbrochen wurde. Dann musste man entscheiden, ob man die Suche neu anfangen soll oder ob das zuletzt gelesene Suﬃx noch ein kürzeres Präﬁx von 0010 enthält, und die weitere Suche musste dann von entsprechender Stelle fortgesetzt werden. Dass man da wirklich aufpassen muss, zeigt die Schwierigkeit der Klassenbeschreibung in Aufgabe 3.5. Man kann das Risiko in diesem Entwurfsprozess vermeiden, indem man sich entscheidet, einfach mehr Informationen über das gelesene Wort zu speichern. Eine Idee wäre, durch die Namen der Zustände alle kompletten Suﬃxe der Länge 4 zu speichern. Zum Beispiel sollte ein Zustand q0110 alle Wörter enthalten, die mit 0110 enden. In dem Augenblick ist alles übersichtlich und die Beschreibung der Zustandsklassen einfach, aber wir bezahlen diese Transparenz mit der Automatengröße. Wir haben 24 = 16 Zustände, um alle Suﬃxe der Länge 4 über {0, 1} zu speichern. Das ist noch nicht alles. Es gibt auch kürzere Wörter, die natürlich kein Präﬁx der Länge 4 enthalten. Alle Wörter kürzer als 4 brauchen dann eigene Zustände, was 23 +22 +21 + 1=15 weitere Zustände ergibt. Weil das Zeichnen eines EA mit 16 + 15 = 31 Zuständen sehr aufwendig ist, stellen wir eine Anwendung dieser Idee für die Suche nach einem kürzeren Teilwort vor. Beispiel 3.2. Betrachten wir die Sprache L = {x110y | x, y ∈{0, 1} ∗}. Wie oben angedeutet, führen wir die Zustände pabc ein, wobei in pabc die Wörter landen, die mit dem Suﬃx abc für a, b, c ∈{0, 1} enden. Dies ist noch nicht ganz genau, denn für abc ̸= 110 nehmen wir ein Wort mit Suﬃx abc in Kl[pabc] nur dann, wenn das Wort das Teilwort 110 nicht enthält. (In einem solchen Fall müsste aber die Akzeptanz eines solchen Wortes längst entschieden worden sein.) Wir beschreiben jetzt genau die Bedeutung aller Zustände: 3.2 Die Darstellungen der endlichen Automaten 61 Kl[p110]= L = {x ∈{0, 1}∗ | x enthält das Teilwort 110}, Kl[p000]= {x000 | x ∈{0, 1}∗ und x000 enthält das Teilwort 110 nicht}, Kl[p001]= {x001 | x ∈{0, 1}∗ und x001 enthält das Teilwort 110 nicht}, Kl[p010]= {x010 | x ∈{0, 1}∗ und x010 enthält das Teilwort 110 nicht}, Kl[p011]= {x011 | x ∈{0, 1}∗ und x011 enthält das Teilwort 110 nicht}, Kl[p100]= {x100 | x ∈{0, 1}∗ und x100 enthält das Teilwort 110 nicht}, Kl[p101]= {x101 | x ∈{0, 1}∗ und x101 enthält das Teilwort 110 nicht}, Kl[p111]= {x111 | x ∈{0, 1}∗ und x111 enthält das Teilwort 110 nicht}. Dann brauchen wir die Zustände pλ,p0,p1,p00,p01,p10 und p11 für kürzere Wörter, daher gilt Kl[pλ]= {λ}, Kl[p0]= {0}, Kl[p1]= {1}, Kl[p00]= {00}, Kl[p01]= {01}, Kl[p10]= {10} und Kl[p11]= {11}. Der Startzustand ist oﬀensichtlich der Zustand pλ und p110 ist der einzige akzeptierende Zustand. Der resultierende EA ist in Abbildung 3.8 gezeichnet. Die Verzweigungsstruktur (in der Informatik Baumstruktur genannt) oben gibt jedem Wort der Länge höchstens 3 einen anderen Zustand. Alle anderen Kanten (Transitionen) führen zu den untersten Zuständen pabc. Wenn ein Wort x001 zum Beispiel um eine 0 zu x0010 verlängert wird, dann muss man aus p001 in p010 übergehen (d. h. δ(p001, 0) = p010). Deswegen sind die Kanten so gelegt, dass durch das Lesen eines weiteren Symbols immer der dem neuen Suﬃx der Länge 3 entsprechende Zustand erreicht wird. Die einzige Ausnahme ist der Zustand p110. Da bleiben wir, unabhängig davon, welche Symbole noch gelesen werden, weil wir schon das Teilwort 110 in der Eingabe festgestellt haben und alle solchen Eingaben akzeptiert werden müssen. Der Entwurf des EA in Abbildung 3.8 riecht auch nach viel Arbeit, aber die könnte lohnenswert sein, wenn man auf eine übersichtliche Weise einen EA für eine Sprache wie L = {x ∈{0, 1}∗ | x enthält mindestens eines der Wörter 000, 011 oder 110 als Teilwörter} entwerfen möchte. Da reicht es aus, den EA aus Abbildung 3.8 zu nehmen, die aus p000 und p011 ausgehenden Kanten durch δ(p000, 0) = δ(p000, 1) = p000 und δ(p011, 0) = δ(p011, 1) = p011 zu ersetzen und F = {p000,p011,p110} zu wählen. ♦ 62 3 Endliche Automaten pλ p0 p00 p000 0 p001 1 0 p01 p010 0 p011 1 1 0 p1 p10 p100 0 p101 1 0 p11 p110 0 p111 1 1 1 0 1 1 0 0 1 0 1 0, 1 1 0 0 1 1 0 Abbildung 3.8 Aufgabe 3.6. Modiﬁzieren Sie den Automaten aus Abbildung 3.8, um folgende Sprachen zu akzeptieren: (a) {xyz ∈{0, 1} ∗ | y ∈{001, 010, 100},x,z ∈{0, 1}∗}, (b) ∗ {xyz ∈{0, 1} ∗ | y ∈{01, 100, 111},x,z ∈{0, 1}∗}, (c) {x011 | x ∈{0, 1}∗}, (d) {xz ∈{0, 1} ∗ | x ∈{0, 1}∗,z ∈{001, 010, 100}}. Aufgabe 3.7. Entwerfen Sie EA für folgende Sprachen: (a) {x1111y | x, y ∈{0, 1, 2}∗}, (b) {x110110y | x, y ∈{0, 1, 2}∗}, (c) {x010101y | x, y ∈{0, 1}∗}, (d) {x0y | x, y ∈{0, 1}∗}, (e) {x0y1z | x, y, z ∈{0, 1} ∗}, (f) {x ∈{0, 1}∗ ||x|0 ≥ 3}, (g) {x ∈{0, 1}∗ | 3 ≤|x|1 ≤ 5}, (h) ∗ {x001y101z | x, y, z ∈{0, 1}∗}, (i) {x0011 | x ∈{0, 1}∗}, 3.3 Simulationen 63 (j) {x10011 | x ∈{0, 1}∗}, (k) {1x11001 | x ∈{0, 1}∗}, (l) {x11001y0 | x, y ∈{0, 1} ∗}. Aufgabe 3.8. Wenden Sie die Entwurfsstrategie aus Abbildung 3.8 an, um einen EA für folgende Sprachen zu entwerfen: (a) {x ∈{0, 1} ∗ | x enthält 11 als Teilwort oder endet mit dem Suﬃx 10}, (b) {λ, 0, 11,x000 | x ∈{0, 1} ∗}, (c) {x ∈{0, 1} ∗ | x enthält 00 oder 11 als Teilwörter}. Bestimmen Sie die entsprechenden Zustandsklassen für alle entworfenen Automaten. Aufgabe 3.9. Entwerfen Sie für jede der folgenden Sprachen einen EA: (a) {w ∈{0, 1, 2} ∗ | w = 002122x, x ∈ (Σbool) ∗}, (b) {w ∈{a, b, c}∗ | w = yabcabc, y ∈{a, b, c}∗}, (c) {w ∈{a, b, c}∗ | w = xaabby, x, y ∈{a, b, c}∗}, (d) {w ∈{0, 1} ∗ ||w|0 ≡ 1 mod 3 und w = x111y für x, y ∈{0, 1} ∗}, (e) {abbxb3y | x, y ∈{a, b}∗}, (f) {w ∈{a, b}∗ | w = abbz für ein z ∈{a, b} ∗ und w = ub3v mit u, v ∈{a, b} ∗}, (g) {xy ∈{0, 1} ∗ | y ∈{000, 010, 100, 101}}, (h) {xyz ∈{0, 1}∗ | y ∈{001, 010, 101, 110}}. Die graphische Darstellung der Automaten ist zur Lösung der Aufgabe ausreichend. Geben Sie dazu zu jedem Zustand q dieser Automaten die Klasse Kl[q] an. 3.3 Simulationen Die Simulation ist einer der meist benutzten Begriﬀe der Informatik. Trotzdem wurde dieser Begriﬀ nie durch eine formale Deﬁnition festgelegt. Der Grund dafür ist, dass man in unterschiedlichen Bereichen den Begriﬀ der Simulation unterschiedlich auslegt. Die engste Interpretation dieses Begriﬀes fordert, dass jeder elementare Schritt der simulierten Berechnung durch einen Schritt der simulierenden Berechnung nachgemacht wird. Eine etwas schwächere Forderung ist, dass man einen Schritt der simulierten Berechnung durch mehrere Schritte simulieren darf. Eine noch schwächere Form verzichtet auf die Simulation einzelner Schritte und fordert nur, dass man gewisse wichtige Teile der Berechnung nachahmt. Die allgemeinste Deﬁnition fordert nur das gleiche Eingabe-Ausgabe-Verhalten und verzichtet vollständig auf die Simulation der Wege, die von den Eingaben zur entsprechenden Ausgabe führen. In diesem Teilkapitel zeigen wir eine Simulation im engen Sinne. Wir zeigen, wie man die Berechnungen von zwei endlichen Automaten mit einem EA simultan Schritt für Schritt nachahmen kann. Lemma 3.2. Sei Σ ein Alphabet und seien M1 =(Q1, Σ,δ1,q01,F1) und M2 =(Q2, Σ,δ2, q02,F2) zwei EA. Für jede Mengenoperation ⊙∈{∪, ∩, −} existiert ein EA M , so dass L(M )= L(M1) ⊙ L(M2). 64 3 Endliche Automaten x2 x2x2 x1 x1x1 xi xi xiM1: M2: M : (q, p) q p Abbildung 3.9 Beweis. Die Idee des Beweises ist, den EA M so zu konstruieren, dass M gleichzeitig die Arbeit der beiden Automaten M1 und M2 simulieren kann. 7 Die Idee der Simulation ist einfach. Die Zustände von M werden Paare (q, p), wobei q ein Zustand von M1 und p ein Zustand von M2 ist. Das erste Element von (q, p) soll q sein genau dann, wenn sich M1 gerade im Zustand q beﬁndet. Analog soll das zweite Element des Zustandes von Mp sein genau dann, wenn M2 sich im Zustand p beﬁndet (Abbildung 3.9). Den formalen Beweis führen wir in zwei Schritten durch. Zuerst geben wir eine formale Konstruktion des EA M und beweisen dann, dass M beide EA M1 und M2 simuliert. Konstruktion von M . Sei M =(Q, Σ,δ,q0,F⊙), wobei (i) Q = Q1 × Q2, (ii) q0 =(q01,q02), (iii) für alle q ∈ Q1, p ∈ Q2 und a ∈ Σ, δ((q, p),a)=(δ1(q, a),δ2(p, a)), (iv) falls ⊙ = ∪, dann ist F = F1 × Q2 ∪ Q1 × F2, {Mindestens einer von M1 und M2 endet in einem akzeptierenden Zustand.} falls ⊙ = ∩, dann ist F = F1 × F2, und {M1 und M2 müssen beide akzeptieren.} falls ⊙ = −, dann ist F = F1 × (Q2 − F2). {M1 muss akzeptieren und M2 darf nicht akzeptieren.} Beweis der Behauptung L(M )= L(M1) ⊙ L(M2). Um die Behauptung für jedes ⊙∈{∪, ∩, −} zu beweisen, reicht es, die folgende Gleichheit zu zeigen: ˆδ((q01,q02),x)=(ˆδ1(q01,x), ˆδ2(q02,x)) für alle x ∈ Σ∗. (3.1) 7Im Prinzip hat M auch keine andere Möglichkeit, weil die Möglichkeit, zuerst M1 und dann M2 zu simulieren, nicht besteht (die Eingabe steht nur einmal zum Lesen zur Verfügung). 3.3 Simulationen 65 q0 q1 1 0 1 0 Abbildung 3.10. M1 Wir beweisen (3.1) durch Induktion bezüglich |x|. (a) Induktionsanfang. Falls x = λ, ist (3.1) oﬀenbar erfüllt. (b) Induktionsschritt. Wir beweisen für jedes i ∈ N, dass, wenn (3.1) erfüllt ist für jedes x ∈ Σ∗ mit |x|≤ i, dann ist (3.1) erfüllt für jedes w ∈ Σi+1. Sei w ein beliebiges Wort aus Σi+1. Dann ist w = za für irgendwelche z ∈ Σi und a ∈ Σ. Aus der Deﬁnition der Funktion ˆδ erhalten wir ˆδ((q01,q02),w)= ˆδ((q01,q02),za) = δ(ˆδ((q01,q02),z),a) = (3.1) δ((ˆδ1(q01,z), ˆδ2(q02,z)),a) = Def. δ (δ1(ˆδ1(q01,z),a),δ2(ˆδ2(q02,z),a)) =(ˆδ1(q01,za), ˆδ2(q02,za)) =(ˆδ1(q01,w), ˆδ2(q02,w)). \u0002 Aufgabe 3.10. Sei L ⊆ Σ∗ eine reguläre Sprache. Beweisen Sie, dass auch L∁ =Σ∗ − L eine reguläre Sprache ist. Die vorgestellte Simulation bietet uns eine modulare Technik zum Entwurf endlicher Automaten. Dieser strukturierte Ansatz ist besonders für größere und komplexere Au- tomaten geeignet, weil er nicht nur den Entwurfsprozess veranschaulicht, sondern auch die Veriﬁkation des entworfenen Automaten vereinfacht. Die Idee der modularen Ent- wurfsstrategie ist, zuerst einfache Automaten für einfache Sprachen zu bauen und dann aus diesen „Bausteinen“ den gesuchten EA zusammenzubauen. Wir illustrieren diese Entwurfsmethode durch das folgende Beispiel. Beispiel 3.3. Seien L1 = {x ∈{0, 1}∗ ||x|0 ist gerade}, und L2 = {x ∈{0, 1} ∗ ||x|1 = 0 oder |x|1 ≥ 3}. Wir bauen zuerst zwei endliche Automaten M1 und M2 mit L(M1)= L1 und L(M2)= L2, die in Abbildungen 3.10 und 3.11 dargestellt sind. Wie unsere Idee besagt, hat M die Zustandsmenge {(q0,p0), (q0,p1), (q0,p2), (q0,p3), (q1,p0), (q1,p1), (q1,p2), (q1,p3)}. 66 3 Endliche Automaten p0 p1 p2 p31 1 1 0 0 0 0,1 Abbildung 3.11. M2 (q0,p0) (q0,p1) (q0,p2) (q0,p3) (q1,p0) (q1,p1) (q1,p2) (q1,p3) 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 Abbildung 3.12. M Um M übersichtlich zu zeichnen, legen wir die Zustände von M matrixartig auf ein Blatt Papier (Abbildung 3.12). Die erste Zeile beinhaltet Zustände mit der ersten Komponente q0 und die zweite Zeile Zustände mit q1 als erste Komponente. Die i-te Spalte für i =0, 1, 2, 3 beinhaltet die Zustände mit der zweiten Komponente pi. Jetzt muss man anhand von M1 und M2 die Kanten (Übergänge) bestimmen. Zum Beispiel geht M1 aus q0 bei0in q1 über und M2 bleibt beim Lesen von 0 in p0. Deswegen geht M aus (q0,p0) beim Lesen von 0 in (q1,p0)über. Nachdem auf diese Weise alle Kanten bezeichnet worden sind, beobachten wir in Ab- bildung 3.12, dass wir in den Spalten die Anzahl der Nullen modulo 2 rechnen und in den Zeilen die Anzahl Einsen von 0 bis 3 zählen. Also bedeutet der Zustand (qi,pj) für i ∈{0, 1},j ∈{0, 1, 2, 3}, dass das bisher gelesene Präﬁx x genau j Einsen beinhaltet (wenn j = 3 mindestens 3 Einsen) und |x|0 mod 2= i. Damit sind alle für uns wichtigen Merkmale der gelesenen Wörter in M beobachtet (gespeichert). Welche sind die akzeptierenden Zustände? Das hängt davon ab, was für eine Sprache wir akzeptieren wollen. Wenn wir zum Beispiel die Sprache L1 ∩ L2 = L(M1) ∩ L(M2) akzeptieren wollen, dann müssen wir genau dann akzeptieren wenn M1 und M2 beide akzeptieren. Das bedeutet, die akzeptierenden Zustände von M sind genau die Zustände, bei denen beide Komponenten akzeptierende Zustände von M1 und M2 sind. Weil q0 der 3.3 Simulationen 67 einzige akzeptierende Zustand von M1 ist und die akzeptierenden Zustände von M2 die Zustände p0 und p3 sind, sind die akzeptierenden Zustände von M die Zustände (q0,p0) und (q0,p3). Wenn M die Sprache L1 ∪ L2 = L(M1) ∪ L(M2) akzeptieren sollte, dann akzeptiert M genau dann, wenn mindestens einer der endlichen Automaten M1 und M2 akzeptiert. Das bedeutet, dass die akzeptierenden Zustände von M genau die Zustände sind, bei denen mindestens eine Komponente einem akzeptierenden Zustand von M1 oder M2 entspricht. Somit sind die akzeptierenden Zustände für L1 ∪ L2 die folgenden Zustände: (q0,p0), (q0,p1), (q0,p2), (q0,p3), (q1,p0), (q1,p3). ♦ Aufgabe 3.11. Betrachten Sie den EA M aus Beispiel 3.3. Bestimmen Sie die Menge der akzeptierenden Zustände, wenn M die folgenden Sprachen akzeptieren soll: (a) L(M1) − L(M2)= {x ∈{0, 1} ∗ | x ∈ L(M1) und x/∈ L(M2)}, (b) L(M2) − L(M1), (c) {0, 1} ∗ − (L(M1) ∩ L(M2)), (d) {0, 1}∗ − (L(M1) ∪ L(M2)), (e) ({0, 1} ∗ − L(M1)) ∪ L(M2), (f) ({0, 1} ∗ − L(M2)) ∩ L(M1). Aufgabe 3.12. Nutzen Sie die Methode des modularen Entwurfs, um endliche Automaten für folgende Sprachen zu bauen: (a) {x ∈{0, 1} ∗ ||x|0 mod3=1 und |x|1 mod3=2}, (b) {x ∈{0, 1}∗ ||x|0 mod2=1 und 1 ≤|x|1 ≤ 3}, (c) {x ∈{0, 1, 2} ∗ ||x|0 mod 3 ∈{0, 1} und (|x|1 + |x|2)mod 2=0}, (d) {x ∈{0, 1} ∗ ||x|1 ist gerade und x enthält das Teilwort 0101}, (e) {x ∈{0, 1}∗ | x = y00z11v für y, z, v ∈{0, 1}∗ und |x|0 mod3=2}, (f) {x ∈{0, 1,a,b}∗ | x enthält das Teilwort 111 oder das Teilwort aba}, (g) {x ∈{0, 1}∗ | x beginnt mit dem Präﬁx 011 und x enthält 100 als Teilwort}, (h) {x ∈{0, 1}∗ | x enthält entweder 0101 als Teilwort oder endet mit dem Suﬃx 111}, (i) {x ∈{0, 1, 2} ∗ ||x|0 ist gerade ∧|x|1 ist ungerade ∧|x|1 + |x|2 ≥ 2}, (j) {x ∈{0, 1} ∗ | x enthält mindestens eines der folgenden Teilwörter: 0011, 110}. Aufgabe 3.13. Betrachten Sie die folgenden Sprachen: L1 = {x ∈{0, 1, 2} ∗ ||x|0 ist gerade}, L2 = {x ∈{0, 1, 2} ∗ ||x|1 ist ungerade}, L3 = {x ∈{0, 1, 2} ∗ ||x|2 ist gerade}. Bauen Sie drei endliche Automaten M1,M2 und M3, so dass L(M1)= L1,L(M2)= L2 und L(M3)= L3. Können Sie jetzt mit der modularen Entwurfstechnik einen EA M konstruieren, so dass L(M )=(L1 ∩ L2) ∪ L3? 68 3 Endliche Automaten 3.4 Beweise der Nichtexistenz Um zu zeigen, dass eine Sprache L nicht regulär ist (L/∈LEA), genügt es zu beweisen, dass es keinen EA gibt, der die Sprache akzeptiert. Im Allgemeinen zu zeigen, dass von einer gewissen Klasse von Programmen (Algorithmen) eine konkrete Aufgabe nicht lösbar ist (kein Programm aus der Klasse löst die Aufgabe), gehört zu den schwersten Problemstellungen in der Informatik. Beweise solcher Aussagen nennen wir Beweise der Nichtexistenz. Im Unterschied zu konstruktiven Beweisen, bei denen man die Existenz eines Objekts mit gewissen Eigenschaften direkt durch eine Konstruktion eines solchen Objekts beweist (wir konstruieren zum Beispiel einen EA M mit vier Zuständen, der eine gegebene Sprache akzeptiert), kann man bei den Beweisen der Nichtexistenz mit einer unendlichen Menge von Kandidaten (zum Beispiel allen endlichen Automaten) nicht so vorgehen, dass man alle Kandidaten einen nach dem anderen betrachtet und überprüft, dass keiner die gewünschten Eigenschaften hat. Um die Nichtexistenz eines Objekts mit gegebenen Eigenschaften in einer unendlichen Klasse von Kandidaten zu beweisen, muss man für gewöhnlich eine tiefgreifende Kenntnis über diese Klasse haben, die im Widerspruch zu den gewünschten Eigenschaften steht. Weil die Klasse der endlichen Automaten eine Klasse sehr stark eingeschränkter Pro- gramme ist, sind die Beweise der Nichtexistenz der Art „es gibt keinen EA, der die gegebene Sprache L akzeptiert“ relativ leicht. Wir nutzen dies hier, um eine einfache Einführung in die Methodik der Erstellung von Beweisen der Nichtexistenz zu geben. Wir wissen, dass endliche Automaten keine andere Speichermöglichkeit als den aktuellen Zustand (Nummer der aufgerufenen Programmzeile) besitzen. Das bedeutet für einen EA A, der nach dem Lesen zweier unterschiedlicher Wörter x und y im gleichen Zustand endet (also ˆδ(q0,x)= ˆδ(q0,y)), dass A in Zukunft nicht mehr zwischen x und y unterscheiden kann. Mit anderen Worten bedeutet dies, dass für alle z ∈ Σ∗ gilt, dass ˆδA(q0,xz)= ˆδA(q0,yz). Formulieren wir diese wichtige Eigenschaft im folgenden Lemma. Lemma 3.3. Sei A =(Q, Σ,δA,q0,F ) ein EA. Seien x, y ∈ Σ∗, x ̸= y, so dass (q0,x) A ∗ (p, λ) und (q0,y) A ∗ (p, λ) für ein p ∈ Q (also ˆδA(q0,x)= ˆδA(q0,y)= p (x, y ∈ Kl[p])). Dann existiert für jedes z ∈ Σ∗ ein r ∈ Q, so dass xz und yz ∈ Kl[r], also gilt insbesondere xz ∈ L(A) ⇐⇒ yz ∈ L(A). Beweis. Aus der Existenz der Berechnungen (q0,x) A ∗ (p, λ) und (q0,y) A ∗ (p, λ) von A folgt die Existenz folgender Berechnung auf xz und yz: (q0,xz) A ∗ (p, z) und (q0,yz) A ∗ (p, z) 3.4 Beweise der Nichtexistenz 69 für alle z ∈ Σ ∗.Wenn r = ˆδA(p, z) (also wenn (p, z) A ∗ (r, λ) die Berechnung von A auf z ausgehend vom Zustand p) ist, dann ist die Berechnung von A auf xz (q0,xz) A ∗ (p, z) A ∗ (r, λ) und die Berechnung von A auf yz (q0,yz) A ∗ (p, z) A ∗ (r, λ). Wenn r ∈ F , dann sind beide Wörter xz und yz in L(A). Falls r/∈ F , dann sind xz, yz /∈ L(A). \u0002 Lemma 3.3 ist ein Spezialfall einer Eigenschaft, die für jedes (deterministische) Rechner- modell gilt. Wenn man einmal in den Berechnungen auf zwei unterschiedlichen Eingaben die gleiche Konﬁguration 8 erreicht, dann ist der weitere Verlauf beider Berechnungen iden- tisch. Im Fall eines Entscheidungsproblems bedeutet dies, dass entweder beide Eingaben akzeptiert werden oder beide verworfen werden. Lemma 3.3 kann man leicht anwenden, um die Nichtregularität mehrerer Sprachen zu zeigen. Sei L = {0n1n | n ∈ N}. Intuitiv sollte L für jeden EA deswegen schwer sein, weil man die Anzahl der vorkommenden Nullen speichern sollte, um diese dann mit der Anzahl nachfolgender Einsen vergleichen zu können. Aber die Anzahl Nullen im Präﬁx 0 n kann beliebig groß sein und jeder EA hat eine feste Größe (Anzahl der Zustände). Also kann kein EA die vorkommenden Nullen in jedem Eingabewort zählen und wir brauchen nur formal auszudrücken, dass dieses Zählen erforderlich ist, um L zu akzeptieren. Wir zeigen indirekt, dass L/∈LEA.Sei A =(Q, Σbool,δA,q0,F ) ein EA mit L(A)= L. Man betrachte die Wörter 01, 0 2, 03,..., 0|Q|+1. Weil die Anzahl dieser Wörter |Q| + 1 ist, existieren i, j ∈{1, 2,..., |Q| +1}, i<j,so dass ˆδA(q0, 0i)= ˆδA(q0, 0 j). Nach Lemma 3.3 gilt 0iz ∈ L ⇐⇒ 0jz ∈ L für alle z ∈ (Σbool) ∗. Dies gilt aber nicht, weil für z =1 i das Wort 0 i1 i in L liegt und das Wort 0 j1i nicht in L ist. Aufgabe 3.14. Zeigen Sie mit Hilfe von Lemma 3.3, dass folgende Sprachen nicht in LEA sind: (a) {w ∈{a, b} ∗ ||w|a = |w|b}, (b) {a nbmc n | n, m ∈ N}, (c) {w ∈{0, 1, #} ∗ | w = x#x für ein x ∈{0, 1} ∗}, (d) {x1y ∈{0, 1} ∗ ||x| = |y|}. 8Dies gilt nur, wenn man eine Konﬁguration als vollständige Beschreibung des allgemeinen Zustandes des Rechnermodells (einschließlich des noch erreichbaren Teils der Eingabe) betrachtet. 70 3 Endliche Automaten xxx x y y z z q0 q0 pp pp pp r r Abbildung 3.13 Um die Beweise der Nichtregularität konkreter Sprachen anschaulich und einfach zu machen, sucht man nach leicht überprüfbaren Eigenschaften, die jede reguläre Sprache erfüllen muss. Wenn eine Sprache L eine solche Eigenschaft nicht besitzt, weiß man direkt, dass L nicht regulär ist. Im Folgenden zeigen wir zwei solcher Methoden zum Beweis von Aussagen der Form L/∈LEA. Die erste Methode nennt man Pumping. Sie basiert auf folgender Idee. Wenn für ein Wort x und einen Zustand p eines EA A (p, x) A ∗ (p, λ) gilt, dann gilt auch (p, x i) A ∗ (p, λ) für alle i ∈ N. Also kann A nicht unterscheiden, wie viele x gelesen worden sind. Daher ist, wenn ˆδA(q0,y)= p für ein y ∈ Σ∗ und ˆδA(p, z)= r für ein z ∈ Σ∗ (Abbildung 3.13), (q0,yx iz) A ∗ (p, x iz) A ∗ (p, z) A ∗ (r, λ) die Berechnung von A auf yx iz für alle i ∈ N (also {yx iz | i ∈ N}⊆ Kl[r] für ein r ∈ Q). Dies bedeutet, dass A entweder alle Wörter yx iz für i ∈ N akzeptiert (falls r ∈ F ), oder dass A kein Wort aus {yx iz | i ∈ N} akzeptiert. Lemma 3.4 (Pumping-Lemma für reguläre Sprachen). Sei L regulär. Dann exis- tiert eine Konstante n0 ∈ N, so dass sich jedes Wort w ∈ Σ ∗ mit |w|≥ n0 in drei Teile y, x und z zerlegen lässt, das heißt w = yxz, wobei (i) |yx|≤ n0, (ii) |x|≥ 1 und (iii) entweder {yx kz | k ∈ N}⊆ L oder {yx kz | k ∈ N}∩ L = ∅. Beweis. Sei L ⊆ Σ ∗ regulär. Dann existiert ein EA A =(Q, Σ,δA,q0,F ), so dass L(A)= L. Wir setzen n0 = |Q|.Sei w ∈ Σ ∗ mit |w|≥ n0. Dann ist w = w1w2 ...wn0u,wobei wi ∈ Σ für i =1,...,n0 und u ∈ Σ∗. Betrachten wir die Berechnung (q0,w1w2w3 ...wn0) A (q1,w2w3 ...wn0) A (q2,w3 ...wn0) A ··· A (qn0−1,wn0) A (qn0,λ) (3.2) 3.4 Beweise der Nichtexistenz 71 von A auf w1w2 ...wn0 .Inden n0 + 1 Konﬁgurationen dieser Berechnung kommen n0 +1 Zustände q0,q1,q2,...,qn0 vor. Weil |Q| = n0, existieren i, j ∈{0, 1,...,n0}, i<j,so dass qi = qj. Daher lässt sich (3.2) als (q0,w1 ...wn0) A ∗ (qi,wi+1 ...wn0) A ∗ (qi,wj+1 ...wn0) A ∗ (qn0,λ) (3.3) darstellen. Wir setzen jetzt y = w1 ...wi,x = wi+1 ...wj und z = wj+1 ...wn0u. Es ist klar, dass w = yxz. Wir überprüfen die Eigenschaften (i), (ii) und (iii). (i) yx = w1 ...wiwi+1 ...wj und daher |yx| = j ≤ n0. (ii) Weil i<j und |x| = j − i,ist x ̸= λ (|x|≥ 1). (iii) Wenn man die Notation y und x statt w1 ...wi und wi+1 ...wj in (3.3) verwendet, sieht die Berechnung von A auf yx wie folgt aus: (q0,yx) A ∗ (qi,x) A ∗ (qi,λ). (3.4) Die Berechnung (3.3) impliziert (qi,x k) A ∗ (qi,λ) für alle k ∈ N. Dann ist für alle k ∈ N (q0,yxkz) A ∗ (qi,x kz) A ∗ (qi,z) A ∗ (ˆδA(qi,z),λ) die Berechnung von A auf yx kz. Wir sehen, dass für alle k ∈ N die Berechnungen im gleichen Zustand ˆδA(qi,z) enden. Falls also ˆδA(qi,z) ∈ F , dann akzeptiert A alle Wörter aus {yx kz | k ∈ N}. Falls ˆδA(qi,z) /∈ F , dann akzeptiert A kein Wort aus {yx kz | k ∈ N}. \u0002 Wie wendet man Lemma 3.4 an, um zu zeigen, dass eine Sprache nicht regulär ist? Führen wir dies wieder am Beispiel der Sprache L = {0 n1 n | n ∈ N} vor. Wir führen den Beweis indirekt. Sei L regulär. Dann existiert eine Konstante n0 mit den in Lemma 3.4 beschriebenen Eigenschaften, also muss jedes Wort mit einer Länge von mindestens n0 eine Zerlegung besitzen, die die Eigenschaften (i), (ii) und (iii) erfüllt. Um zu zeigen, dass L/∈LEA, reicht es aus, ein hinreichend langes Wort zu ﬁnden, für das keine seiner Zerlegungen die Eigenschaften (i), (ii) und (iii) erfüllt. Wir wählen jetzt w =0 n01 n0. Es ist klar, dass |w| =2n0 ≥ n0. Es muss eine Zerlegung w = yxz von w mit den Eigenschaften (i), (ii) und (iii) geben. Weil nach (i) |yx|≤ n0 gilt, ist y =0l und x =0m für irgendwelche l, m ∈ N. Nach (ii) ist m ̸=0. Weil w =0n01n0 ∈ L,ist {yx kz | k ∈ N} = {0n0−m+km1n0 | k ∈ N}⊆ L. Das ist aber ein Widerspruch, weil yx 0z = yz =0n0−m1n0 /∈ L. (Es ist sogar so, dass 0n01n0 das einzige Wort aus {yx kz | k ∈ N} ist, das in L liegt.) 72 3 Endliche Automaten Bei der Anwendung des Pumping-Lemmas ist es wichtig, dass wir das Wort w frei wählen können, weil das Lemma für alle ausreichend langen Wörter gilt. Die Wahl des Wortes w ist insbesondere wichtig aus folgenden zwei Gründen. Erstens kann man für w eine „schlechte“ Wahl treﬀen in dem Sinne, dass man mit diesem Wort und der Pumping-Methode die Tatsache L/∈LEA nicht beweisen kann. Für die Sprache L = {0 n1n | n ∈ N} ist ein Beispiel einer schlechten Wahl das Wort w =0 n0 /∈ L. Es besteht die Möglichkeit, das Wort w wie folgt zu zerlegen: w = yxz mit y =0,x =0,z =0 n0−2. Es ist oﬀensichtlich, dass das Pumping-Lemma für dieses w gilt, weil keines der Wörter in {yx kz | k ∈ N} = {0n0−1+k | k ∈ N} zu L gehört und so die Eigenschaften (i), (ii) und (iii) erfüllt sind. Zweitens kann man Wörter wählen, die den Beweis von L/∈LEA zwar ermöglichen, aber nicht als günstig anzusehen sind, weil eine Menge Arbeit notwendig ist, um zu beweisen, dass keine Zerlegung des gewählten Wortes alle drei Eigenschaften (i), (ii) und (iii) erfüllt. Als Beispiel betrachten wir die Wahl des Wortes w =0 ⌈n0/2⌉1 ⌈n0/2⌉ für die Sprache L = {0n1n | n ∈ N}. Während Eigenschaft (i) des Pumping-Lemmas für das Wort 0n01n0 garantiert, dass x nur aus Nullen besteht, kann x für das Wort 0⌈n0/2⌉1⌈n0/2⌉ ein beliebiges nichtleeres Teilwort von w sein. Wir können zwar ebenfalls 0⌈n0/2⌉1⌈n0/2⌉ benutzen, um L/∈LEA zu zeigen, müssen dabei aber mindestens die folgenden drei Fälle möglicher Zerlegungen betrachten. (i) y =0 i, x =0 m, z =0 ⌈n0/2⌉−m−i1 ⌈n0/2⌉ für ein i ∈ N und ein m ∈ N −{0}, das heißt, x besteht nur aus Nullen. {In diesem Fall kann man das gleiche Argument wie für das Wort w =0n01n0 nutzen, um zu zeigen, dass die Eigenschaft (iii) des Pumping-Lemmas nicht gilt.} (ii) y =0 ⌈n0/2⌉−m, x =0 m1j, z =1 ⌈n0/2⌉−j für positive ganze Zahlen m und j, daher enthält x mindestens eine 0 und mindestens eine 1. {In diesem Fall ist (iii) nicht erfüllt, denn w = yxz ∈ L und yx 2z/∈ L (yx 2z = 0 ⌈n0/2⌉1 j0 m1 ⌈n0/2⌉ hat nicht die Form 0 ∗1∗).} (iii) y =0 ⌈n0/2⌉1i, x =1 m, z =1 ⌈n0/2⌉−i−m für ein i ∈ N und ein m ∈ N −{0}. {In diesem Fall kann man die Anzahl der Einsen erhöhen, ohne die Anzahl der Nullen zu ändern, und somit ist yx lz/∈ L für alle l ∈ N −{1}.} Also sehen wir, dass die Wahl des Wortes 0⌈n0/2⌉1⌈n0/2⌉ für den Beweis von L/∈LEA zu viel mehr Arbeit führt als die Wahl des Wortes 0 n01 n0. 3.4 Beweise der Nichtexistenz 73 Aufgabe 3.15. Beweisen Sie durch Anwendung des Pumping-Lemmas, dass die folgenden Sprachen nicht regulär sind: (a) {ww | w ∈{0, 1}∗}, (b) {anbnc n | n ∈ N}, (c) {w ∈{0, 1} ∗ ||w|0 = |w|1}, (d) {an 2 | n ∈ N}, (e) {a2n | n ∈ N}, (f) {w ∈{0, 1} ∗ ||w|0 =2|w|1}, (g) {x1y | x, y ∈{0, 1} ∗, |x| = |y|}. Aufgabe 3.16. Beweisen Sie folgende Version des Pumping-Lemmas: Sei L ⊆ Σ∗ regulär. Dann existiert eine Konstante n0 ∈ N, so dass sich jedes Wort w ∈ Σ∗ mit |w|≥ n0 in drei Teile w = yxz zerlegen lässt, wobei (i) |xz|≤ n0, (ii) |x|≥ 1 und (iii) entweder {yx kz | k ∈ N}⊆ L oder {yx kz | k ∈ N}∩ L = ∅. Aufgabe 3.17. ∗ Formulieren und beweisen Sie eine allgemeinere Form des Pumping-Lemmas, die Lemma 3.4 und das Lemma aus Aufgabe 3.16 als Spezialfälle beinhaltet. Im Folgenden stellen wir eine Methode zum Beweis der Nichtregularität vor, die auf der Kolmogorov-Komplexität beruht. Sie zeigt auf eine andere Weise, dass endliche Automaten nicht beliebig lange zählen können (oder nicht so viele Informationen über das bisher gelesene Wort speichern können). Diese Methode basiert auf dem nächsten Satz, der zeigt, dass nicht nur alle Wörter einer regulären Sprache eine kleine Kolmogorov-Komplexität haben, sondern auch alle Suﬃxe von Wörtern einer regulären Sprache. Somit ist Satz 3.1 eine Verfeinerung von Satz 2.2 für reguläre Sprachen. Satz 3.1.∗ Sei L ⊆ (Σbool) ∗ eine reguläre Sprache. Sei Lx = {y ∈ (Σbool) ∗ | xy ∈ L} für jedes x ∈ (Σbool) ∗. Dann existiert eine Konstante const, so dass für alle x, y ∈ (Σbool) ∗ K (y) ≤⌈log2(n +1)⌉ + const, falls y das n-te Wort in der Sprache Lx ist. Beweis. Weil L regulär ist, existiert ein EA M mit L(M )= L. Die Idee des Beweises ist ähnlich dem Beweis von Satz 2.2, aber nicht gleich. Wenn wir den Beweis von Satz 2.2 genau verfolgen würden, würden wir in kanonischer Reihenfolge alle Wörter z aus (Σbool) ∗ nacheinander generieren und für jedes Wort xz durch die Simulation von M auf xz bestimmen, ob xz ∈ L = L(M ). Das n-te akzeptierte Wort der Form xz bestimmt, dass z das n-te Wort in Lx ist. Die Schwäche dieses Ansatzes ist, dass man dem Programm zur Generierung von y nicht nur n und M geben muss, sondern auch x. Das Wort x kann aber eine beliebig große Komplexität K (x) im Vergleich zu K (y) haben. 74 3 Endliche Automaten Die Kernidee hier ist, dass wir einem Programm zur Generierung von y das Wort x gar nicht geben müssen. Es genügt, ihm den Zustand ˆδ(q0,x) zu geben und dann die Simulation auf den generierten Wörtern z immer aus ˆδ(q0,x) zu starten. Sei y das n-te Wort in Lx für ein x ∈ (Σbool)∗. Das Programm Ax,y zur Generierung von y arbeitet wie folgt: Ax,y: begin z := λ; i := 0; while i<n do begin Simuliere die Arbeit von M aus dem Zustand ˆδ(q0,x) auf z; if ˆδ(ˆδ(q0,x),z) ∈ F then begin i := i +1; y := z; end; z := kanonischer Nachfolger von z; end; write(y); end Für alle x, y ist Ax,y gleich bis auf n und den Zustand ˆδ(q0,x). Aber den Zustand ˆδ(q0,x) können wir in die Beschreibung von M mit einem speziellen Zeiger auf ˆδ(q0,x) einbetten. Weil es nur |Q| viele Möglichkeiten für den Zeiger gibt, existiert eine Konstante constM , die die Länge der vollständigen Beschreibung von M mit dem Zeiger auf einen der Zustände (egal welchen) beinhaltet. Die Länge der Beschreibung von Ax,y außer den Parametern n, M und ˆδ(q0,x) ist eine Konstante d bezüglich x und y. Die Zahl n kann man mit ⌈log2(n +1)⌉ Bits darstellen. Also gilt K (y) ≤⌈log2(n +1)⌉ + constM + d. \u0002 Wir wenden Satz 3.1 an, um noch einmal L = {0n1n | n ∈ N} /∈LEA indirekt zu beweisen. Sei L regulär. Für jedes m ∈ N ist 1 m das erste Wort in der Sprache L0m = {y | 0 my ∈ L} = {0j1 m+j | j ∈ N}. Satz 3.1 folgend existiert eine Konstante c, die unabhängig von x =0 m und y =1 m und somit von m ist, so dass K (1m) ≤⌈log2(1+1)⌉ + c =1 + c. Somit gilt für eine Konstante d =1 + c K (1m) ≤ d (3.5) für alle m ∈ N. Dies ist aber nicht möglich, weil 3.5 Nichtdeterminismus 75 (i) die Anzahl aller Programme, deren Länge kleiner oder gleich d sind, endlich ist (höchstens 2d) und (ii) die Menge {1 m | m ∈ N} unendlich ist. Eine andere Argumentation als zu sagen, dass endlich viele unterschiedliche Programme nicht unendlich viele unterschiedliche Wörter generieren können, ist, die Aussage aus Aufgabe 2.28 anzuwenden. Diese Aussage garantiert die Existenz unendlich vieler Zahlen m mit K (m) ≥⌈log2(m +1)⌉− 1. Weil es ein b ∈ N gibt, so dass |K (1 m) − K (m)|≤ b für alle m ∈ N,steht K (m) ≥⌈log2(m +1)⌉− 1 für unendlich viele m ∈ N im Widerspruch zu (3.5). Aufgabe 3.18. Wenden Sie Satz 3.1 an, um die Nichtregularität folgender Sprachen zu beweisen: (a) {0 n 2 | n ∈ N}, (b) {022n | n ∈ N}, (c) {w ∈{0, 1} ∗ ||w|0 =2 ·|w|1}, (d) {w ∈{0, 1} ∗ | w = xx für ein x ∈{0, 1} ∗}. 3.5 Nichtdeterminismus Die standardmäßigen Programme sowie die bisher betrachteten endlichen Automaten sind Modelle deterministischer Berechnungen. Determinismus bedeutet in diesem Kontext, dass in jeder Konﬁguration eindeutig festgelegt ist (determiniert ist), was im nächsten Schritt passieren wird. Daher bestimmen ein Programm (oder ein EA) A und seine Eingabe x vollständig und eindeutig die Berechnung von A auf x. Nichtdeterminismus erlaubt in gewissen Konﬁgurationen eine Auswahl von mehreren Aktionen (Möglichkeiten, die Arbeit fortzusetzen). Dabei reicht es aus, wenn mindestens eine der Möglichkeiten zu dem richtigen Resultat führt. Dies entspricht einem auf den ersten Blick künstlichen Spiel, so, als ob ein nichtdeterministisches Programm immer die richtige Möglichkeit wählt. Diese Wahl einer von mehreren Möglichkeiten nennen wir nichtdeterministische Entscheidung.Für ein Entscheidungsproblem (Σ,L) bedeutet dies, dass ein nichtdeterministisches Programm (nichtdeterministischer EA) A eine Sprache L akzeptiert, falls für jedes x ∈ L mindestens eine akzeptierende Berechnung von A auf x existiert und für jedes y ∈ Σ∗ − L alle Berechnungen nicht-akzeptierend sind. Obwohl ein nichtdeterministisches Programm nicht zum praktischen Einsatz geeignet scheint (und wir kein Orakel haben, das uns helfen würde, die richtige nichtdeterministische Entscheidung zu treﬀen), hat das Studium nichtdeterministischer Berechnungen einen wesentlichen Beitrag geliefert zum Verständnis deterministischer Berechnungen und zur Untersuchung der Grenze der Möglichkeiten, Probleme algorithmisch zu lösen. Die Zielsetzung dieses Teilkapitels ist es, Nichtdeterminismus im Modell der endlichen Automaten einzuführen und zu untersuchen. Dabei interessieren uns die für allgemei- ne Berechnungsmodelle zentralen Fragen, ob man nichtdeterministische Berechnungen deterministisch simulieren kann und falls ja, mit welchem Berechnungsaufwand. Für Programme könnte man Nichtdeterminismus zum Beispiel mit einem Befehl „choose goto i or goto j“ einführen. Bei endlichen Automaten führen wir Nichtdeterminismus so 76 3 Endliche Automaten a a a Abbildung 3.14 ein, dass wir einfach aus einem Zustand mehrere Übergänge für das gleiche Eingabesymbol erlauben (Abbildung 3.14). Deﬁnition 3.3. Ein nichtdeterministischer endlicher Automat (NEA) ist ein Quintupel M =(Q, Σ,δ,q0,F ). Dabei ist (i) Q eine endliche Menge, Zustandsmenge genannt, (ii) Σ ein Alphabet, Eingabealphabet genannt, (iii) q0 ∈ Q der Anfangszustand, (iv) F ⊆ Q die Menge der akzeptierenden Zustände und (v) δ eine Funktion9 von Q × Σ nach P(Q), Übergangsfunktion genannt. {Wir beobachten, dass Q,Σ, q0 und F die gleiche Bedeutung wie bei einem EA haben. Ein NEA kann aber zu einem Zustand q und einem gelesenen Zeichen a auch mehrere Nachfolgezustände oder gar keinen haben.} Eine Konﬁguration von M ist ein Element aus Q × Σ ∗. Die Konﬁguration (q0,x) ist die Startkonﬁguration für das Wort x. Ein Schritt von M ist eine Relation M ⊆ (Q × Σ ∗) × (Q × Σ∗), deﬁniert durch (q, w) M (p, x) ⇐⇒ w = ax für ein a ∈ Σ und p ∈ δ(q, a). Eine Berechnung von M ist eine endliche Folge D1,D2,...,Dk von Konﬁgurationen, wobei Di M Di+1 für i =1,...,k − 1. Eine Berechnung von M auf x ist eine Berechnung C0,C1,...,Cm von M , wobei C0 =(q0,x) und entweder Cm ∈ Q ×{λ} oder Cm =(q, ay) für ein a ∈ Σ, ein y ∈ Σ ∗ und ein q ∈ Q, so dass δ(q, a)= ∅. Die Berechnung C0,C1,...,Cm ist eine akzeptierende Berechnung von M auf x, wenn Cm =(p, λ) für ein p ∈ F . Falls eine akzeptierende Berechnung von M auf x existiert, sagen wir auch, dass M das Wort x akzeptiert. Die Relation M ∗ ist die reﬂexive und transitive Hülle von M , genau wie bei einem EA. Die Sprache L(M ) = {w ∈ Σ∗ | (q0,w) M ∗ (p, λ) für ein p ∈ F } 9Alternativ kann man δ als Relation auf (Q × Σ) × Q deﬁnieren. 3.5 Nichtdeterminismus 77 q0 q1 q2 0, 1 1 1 0, 1 Abbildung 3.15 ist die von M akzeptierte Sprache. Zu der Übergangsfunktion δ deﬁnieren wir die Funktion ˆδ von Q × Σ∗ in P(Q) wie folgt: (i) ˆδ(q, λ)= {q} für jedes q ∈ Q, (ii) ˆδ(q, wa)= {p ∈ Q | es existiert ein r ∈ ˆδ(q, w), so dass p ∈ δ(r, a)} = ⋃ r∈ˆδ(q,w) δ(r, a) für alle q ∈ Q, a ∈ Σ,w ∈ Σ∗. Wir sehen, dass ein Wort x in L(M ) ist, wenn M mindestens eine akzeptierende Berechnung auf x hat. Bei einer akzeptierenden Berechnung auf x wird wie bei einem EA gefordert, dass das ganze Wort x gelesen wird und M nach dem Lesen des letzten Buchstabens in einem akzeptierenden Zustand ist. Im Unterschied zu endlichen Automaten kann eine nicht akzeptierende Berechnung enden, auch wenn die Eingabe nicht vollständig gelesen wurde. Dies passiert, wenn der NEA in einem Zustand q das Symbol a liest und δ(q, a)= ∅, d. h., wenn keine Möglichkeit existiert, die Berechnung fortzusetzen. Der Deﬁnition von ˆδ folgend sehen wir, dass ˆδ(q0,w) die Menge aller Zustände aus Q ist, die aus q0 durch das vollständige Lesen des Wortes w erreichbar sind. Daher ist L(M )= {w ∈ Σ∗ | ˆδ(q0,w) ∩ F ̸= ∅} eine alternative Deﬁnition der von M akzeptierten Sprache. Betrachten wir folgendes Beispiel eines NEA. Sei M =(Q, Σ,δ,q0,F ), wobei Q = {q0,q1,q2}, Σ= {0, 1},F = {q2} und δ(q0, 0) = {q0},δ(q0, 1) = {q0,q1}, δ(q1, 0) = ∅,δ(q1, 1) = {q2}, δ(q2, 0) = {q2},δ(q2, 1) = {q2}. Auf die gleiche Weise wie bei endlichen Automaten stellen wir eine graphische Darstel- lung von M (Abbildung 3.15) vor. Das Wort 10110 ist in L(M ), weil (q0, 10110) M (q0, 0110) M (q0, 110) M (q1, 10) M (q2, 0) M (q2,λ) eine akzeptierende Berechnung von M auf x ist. Um entscheiden zu können, ob ein NEA M ein Wort x akzeptiert, muss man alle Berechnungen von M auf x verfolgen. Eine anschauliche Darstellung aller Berechnungen von M auf x ist durch den sogenannten Berechnungsbaum BM (x)von M auf x 78 3 Endliche Automaten M M M M M M MM M M (q0, 10110) (q1, 0110) (q0, 0110) (q0, 110) (q0, 10) (q1, 10) (q1, 0) (q0, 0) (q2, 0) (q0,λ) (q2,λ) {q0} {q0} {q0,q1} {q0,q1} {q0,q2} {q0,q1,q2} Abbildung 3.16 gegeben. Die Knoten des Baums sind Konﬁgurationen von M . Die Wurzel von BM (x) ist die Anfangskonﬁguration von M auf x. Die Kinder eines Knotens (q, α) sind alle Konﬁgurationen, die man von (q, α) in einem Schritt erreichen kann (d. h. alle (p, β), so dass (q, α) M (p, β)). Ein Blatt von BM (x) ist entweder eine Konﬁguration (r, λ) oder eine Konﬁguration (s, aβ) mit a ∈ Σ, wobei δ(s, a)= ∅ (daher sind die Blätter Konﬁgurationen, aus denen kein weiterer Berechnungschritt möglich ist). Bei dieser Darstellung entspricht jeder Weg von der Wurzel zu einem Blatt einer Berechnung von M auf x. Daher entspricht die Anzahl der Blätter von BM (x) genau der Anzahl unterschiedlicher Berechnungen von M auf x. Aufgabe 3.19. Entwerfen Sie einen NEA, der für jede Eingabe x ∈ (Σbool)∗ genau 2|x| unter- schiedliche Berechnungen hat. Ein Berechnungsbaum BM (x)von M aus Abbildung 3.15 auf dem Wort x = 10110 ist in Abbildung 3.16 dargestellt. Der Berechnungsbaum BM (10110) hat vier Blätter. Die zwei Blätter (q1, 0110) und (q1, 0) entsprechen den Berechnungen, in denen es M nicht gelungen ist, das Eingabewort vollständig zu lesen, weil δ(q1, 0) = ∅. Somit sind diese Berechnungen nicht akzeptierend. Das Blatt (q0,λ) und das Blatt (q2,λ) entsprechen zwei Berechnungen, in denen das Eingabewort 10110 vollständig gelesen wurde. Weil q2 ∈ F , ist die Berechnung, die in (q2,λ) endet, eine akzeptierende Berechnung. Die Schlussfolgerung ist, dass 10110 ∈ L(M ). Weil q2 der einzige akzeptierende Zustand von M ist, und die einzige Möglichkeit, von q0 zu q2 zu gelangen, darin besteht, zwei Einsen hintereinander zu lesen, liegt die Vermutung nahe, dass L(M ) genau die Wörter der Form x11y, x, y ∈ (Σbool)∗, enthält. Das folgende Lemma bestätigt unsere Vermutung. 3.5 Nichtdeterminismus 79 Lemma 3.5. Sei M der NEA aus Abbildung 3.15. Dann ist L(M )= {x11y | x, y ∈ (Σbool) ∗}. Beweis. Wir beweisen diese Gleichheit zweier Mengen durch zwei Inklusionen. (i) Zuerst beweisen wir {x11y | x, y ∈ (Σbool) ∗}⊆ L(M ). Sei w ∈{x11y | x, y ∈ (Σbool)∗},d. h. w = x11y für irgendwelche x, y ∈ (Σbool)∗. Es reicht, die Existenz einer akzeptierenden Berechung von M auf w zu beweisen. Da q0 ∈ δ(q0, 0) ∩ δ(q0, 1), existiert für jedes x ∈ (Σbool)∗ die folgende Berechnung von M auf x: (q0,x) M ∗ (q0,λ). (3.6) Da q2 ∈ δ(q2, 0) ∩ δ(q2, 1), existiert für jedes y ∈ (Σbool)∗ die folgende Berechnung von M auf y: (q2,y) M ∗ (q2,λ). (3.7) Daher ist die folgende Berechnung eine akzeptierende Berechnung von M auf x11y: (q0,x11y) M ∗ (q0, 11y) M (q1, 1y) M (q2,y) M ∗ (q2,λ). (ii) Wir beweisen L(M ) ⊆{x11y | x, y ∈ (Σbool) ∗}. Sei w ∈ L(M ). Daher existiert eine akzeptierende Berechnung von M auf w.Weil die Berechnung auf w in q0 anfangen und in q2 enden muss 10 und der einzige Weg von q0 zu q2 über q1 führt, sieht eine akzeptierende Berechnung von M auf w wie folgt aus: (q0,w) M ∗ (q1,z) M ∗ (q2,λ). (3.8) Jede Berechnung von M kann höchstens eine Konﬁguration mit dem Zustand q1 beinhalten, weil q1 in keiner Schleife von M liegt. Wenn man einmal q1 verlässt, kann man nicht wieder zu q1 zurückkehren. Daher kann die Berechnung (3.8) genauer wie folgt dargestellt werden: (q0,w) M ∗ (q0,az) M (q1,z) M (q2,u) M ∗ (q2,λ), (3.9) wobei z = bu für ein b ∈ Σbool. Die einzige Möglichkeit, in den Zustand q1 zu gelangen, ist die Anwendung der Transition q1 ∈ δ(q0, 1), d. h. durch Lesen einer 1 im Zustand q0, daher ist a = 1 in (3.9). Weil δ(q1, 0) = ∅ und δ(q1, 1) = {q2}, ist die einzige Möglichkeit, einen Berechnungschritt aus q1 zu realisieren, eine 1 zu lesen, das heißt b = 1. Folglich entspricht (3.9) der Berechnung (q0,w) M ∗ (q0, 11u) M (q1, 1u) M (q2,u) M ∗ (q2,λ). Daher muss w das Teilwort 11 beinhalten und somit gilt w ∈{x11y | x, y ∈ (Σbool) ∗}. \u0002 10Der Zustand q2 ist der einzige akzeptierende Zustand von M . 80 3 Endliche Automaten Aufgabe 3.20. Entwerfen Sie für jede der folgenden Sprachen einen NEA: (a) {1011x00y | x, y ∈ (Σbool) ∗}, (b) {01, 101}∗, (c) {x ∈ (Σbool) ∗ | x enthält als Teilwörter 01011 und 01100}, (d) {x ∈ (Σ10) ∗ | Nummer10(x) ist teilbar durch 3}. Bemühen Sie sich, den NEA so einfach wie möglich zu gestalten, d. h., minimieren Sie die Anzahl der Zustände (Knoten) und Transitionen (Kanten). Sei LNEA = {L(M ) | M ist ein NEA}. Die zentrale Frage dieses Teilkapitels ist, ob LNEA = LEA, genauer, ob endliche Automaten die Arbeit nichtdeterministischer endlicher Automaten simulieren können. Diese Frage ist auch zentral für allgemeinere Berechnungs- modelle. Bisherige Erfahrungen zeigen, dass die Simulation von Nichtdeterminismus durch Determinismus nur dann realisierbar ist, wenn die Möglichkeit besteht, alle nichtdetermi- nistischen Berechnungen eines Modells M auf einer Eingabe x durch eine deterministische Berechnung nachzuahmen. Dies gilt auch für endliche Automaten. Die Idee der Simula- tion eines NEA M durch einen EA A basiert auf dem Prinzip der Breitensuche in den Berechnungsbäumen von M . Die erste wichtige Beobachtung für eine derartige Simulation ist, dass alle Konﬁgurationen eines Berechungsbaumes in einer Entfernung i von der Wurzel das gleiche zweite Element haben (weil sie alle nach dem Lesen von genau den ersten i Symbolen des Eingabewortes erreicht worden sind). Daher unterscheiden sich die Konﬁgurationen in der gleichen Entfernung von der Wurzel nur in den Zuständen. Obwohl die Anzahl der Konﬁgurationen in einer festen Entfernung i von der Wurzel exponentiell in i sein kann, bedeutet dies nicht, dass wir exponentiell viele Berechnungen simulieren müssen. Es gibt nur endlich viele Zustände des NEA und daher gibt es nur endlich viele unterschiedliche Konﬁgurationen in der Entfernung i. Wenn zwei unterschiedliche Knoten u und v des Berechnungsbaumes mit der gleichen Konﬁguration C markiert sind, sind die Teilbäume mit der Wurzel u und v identisch, und es reicht daher aus, nur in einem der beiden Teilbäume nach einer akzeptierenden Berechnung zu suchen (Abbildung 3.17). Mit anderen Worten reicht es für eine Simulation aus, für jede Entfernung i von der Wurzel des Berechnungsbaumes BM (x) die Menge der dort auftretenden Zustände zu bestimmen. Diese Menge ist aber nichts anderes als ˆδ(q0,z), wobei z das Präﬁx des Eingabewortes mit |z| = i ist. Am rechten Rand von Abbildung 3.16 sind die Zustandsmengen ˆδ(q0,λ)= {q0}, ˆδ(q0, 1) = {q0,q1}, ˆδ(q0, 10) = {q0}, ˆδ(q0, 101) = {q0,q1}, ˆδ(q0, 1011) = {q0,q1,q2} und ˆδ(q0, 10110) = {q0,q2} angegeben, die den erreichbaren Mengen von Zuständen nach dem Lesen des Präﬁxes einer Länge i für i ∈{0, 1,..., 5} entsprechen. Diese Beobachtung führt dazu, dass man als Zustände des simulierenden (determinis- tischen) EA A beliebige Teilmengen der Zustandsmenge des NEA M =(Q, Σ,δ,q0,F ) verwendet. Dies führt dazu, dass man die folgende Konstruktion des endlichen Automaten A als Potenzmengenkonstruktion in der Automatentheorie bezeichnet. Ein Zustand ⟨P ⟩ von A für P ⊆ Q wird die Bedeutung haben, dass nach der gegebenen Anzahl von Berechnungsschritten genau die Zustände aus P in den Berechnungen von M auf gegebener Eingabe erreichbar sind 11 (P = ˆδ(q0,z)). Ein Berechnungsschritt A aus einem 11Wir benutzen die Bezeichnung ⟨P ⟩ statt P , um immer deutlich zu machen, ob wir einen Zustand von A, der einer Menge von Zuständen von M entspricht, oder eine Menge von Zuständen von M betrachten. 3.5 Nichtdeterminismus 81 i qq BM (x): Tq Tq Abbildung 3.17 Zustand ⟨P ⟩ für ein gelesenes Symbol a bedeutet die Bestimmung der Menge ⋃ p∈P δ(p, a), also aller Zustände, die aus irgendeinem Zustand p ∈ P beim Lesen von a erreichbar sind. Eine Formalisierung dieser Idee liefert der Beweis des nächsten Satzes. Satz 3.2. Zu jedem NEA M existiert ein EA A, so dass L(M )= L(A). Beweis. Sei M =(Q, Σ,δM ,q0,F ) ein NEA. Wir konstruieren einen EA A =(QA, ΣA,δA, q0A,FA) wie folgt: (i) QA = {⟨P ⟩| P ⊆ Q}, (ii) ΣA =Σ, (iii) q0A = ⟨{q0}⟩, (iv) FA = {⟨P ⟩| P ⊆ Q und P ∩ F ̸= ∅}, (v) δA ist eine Funktion von QA × ΣA nach QA, die wie folgt deﬁniert ist. Für jedes ⟨P ⟩∈ QA und jedes a ∈ ΣA ist δA(⟨P ⟩ ,a)= 〈 ⋃ p∈P δM (p, a) 〉 = ⟨{q ∈ Q |∃p ∈ P, so dass q ∈ δM (p, a)}⟩ . Es ist klar, dass A ein EA ist. Abbildung 3.18 zeigt den EA A, der sich nach dieser Potenzmengenkonstruktion aus dem NEA M aus Abbildung 3.15 ergibt. 12 12Man bemerke, dass die Zustände ⟨∅⟩, ⟨{q1}⟩, ⟨{q2}⟩ und ⟨{q1,q2}⟩ in A nicht aus ⟨{q0}⟩ erreichbar sind, d. h., es gibt kein Wort, dessen Bearbeitung in einem dieser Zustände endet. Wenn wir also diese Zustände aus A herausnehmen, wird das keinen Einﬂuss auf die von A akzeptierte Sprache haben. 82 3 Endliche Automaten ⟨{q0}⟩ ⟨{q0,q1}⟩ ⟨{q0,q2}⟩ ⟨{q0,q1,q2}⟩ ⟨{q1,q2}⟩⟨{q2}⟩ ⟨{q1}⟩⟨∅⟩ 0 1 0 1 0 1 0 1 0, 1 0 1 0, 1 0, 1 Abbildung 3.18 Um L(M )= L(A) zu zeigen, reicht es, folgende Äquivalenz zu beweisen: ∀x ∈ Σ ∗ : ˆδM (q0,x)= P ⇐⇒ ˆδA(q0A,x)= ⟨P ⟩ . (3.10) Wir beweisen (3.10) mittels Induktion bezüglich |x|. (i) Induktionsanfang. Sei |x| = 0, das heißt x = λ. Weil ˆδM (q0,λ)= {q0} und q0A = ⟨{q0}⟩, gilt (3.10) für x = λ. (ii) Induktionsschritt. Sei (3.10) gültig für alle z ∈ Σ ∗ mit |z|≤ m, m ∈ N. Wir beweisen, dass (3.10) auch für alle Wörter aus Σ m+1 gilt. Sei y ein beliebiges Wort aus Σ m+1. Dann ist y = xa für ein x ∈ Σ m und ein a ∈ Σ. Nach der Deﬁnition der Funktion ˆδA gilt ˆδA(q0A,xa)= δA(ˆδA(q0A,x),a). (3.11) Wenn wir die Induktionsannahme (3.10) für x anwenden, erhalten wir ˆδA(q0A,x)= ⟨R⟩⇐⇒ ˆδM (q0,x)= R, daher gilt ˆδA(q0A,x)= ⟨ˆδM (q0,x)⟩. (3.12) 3.5 Nichtdeterminismus 83 Nach Deﬁnition (v) von δA gilt δA(⟨R⟩ ,a)= 〈 ⋃ p∈R δM (p, a) 〉 (3.13) für alle R ⊆ Q und alle a ∈ Σ. Zusammenfassend ist ˆδA(q0A,xa)= (3.11) δA(ˆδA(q0A,x),a) = (3.12) δA(⟨ˆδM (q0,x)⟩,a) = (3.13) 〈 ⋃ p∈ˆδM (q0,x) δM (p, a) 〉 = ⟨ˆδM (q0,xa)⟩. \u0002 Im Folgenden sagen wir, dass zwei Automaten A und B äquivalent sind, falls L(A)= L(B). Aufgabe 3.21. Wenden Sie die Konstruktion aus Satz 3.2 an, um einen äquivalenten EA zu dem NEA aus Abbildung 3.19 zu erzeugen. Aufgabe 3.22. Konstruieren Sie mittels Potenzmengenkonstruktion endliche Automaten zu den nichtdeterministischen endlichen Automaten, die Sie in der Bearbeitung der Aufgabe 3.20 (b) und (d) entworfen haben. Die Folge von Satz 3.2 ist, dass LEA = LNEA, d. h., die (deterministischen) endlichen Automaten sind bezüglich der Sprachakzeptierung genauso stark wie die nichtdetermi- nistischen endlichen Automaten. Wir bemerken aber, dass die durch Potenzmengen- konstruktion erzeugten endlichen Automaten wesentlich (exponentiell) größer sind als die gegebenen nichtdeterministischen endlichen Automaten. Die nächste Frage ist also, ob es Sprachen gibt, bei denen man für die Simulation von Nichtdeterminismus durch Determinismus unausweichlich mit einem exponentiellen Wachstum der Automatengröße bezahlen muss, oder ob eine andere Konstruktion existiert, die die Erzeugung kleinerer äquivalenter deterministischer Automaten sicherstellt. Wir zeigen jetzt, dass man die Potenzmengenkonstruktion im Allgemeinen nicht verbessern kann. Betrachten wir die folgende reguläre Sprache Lk = {x1y | x ∈ (Σbool) ∗,y ∈ (Σbool) k−1} für jedes k ∈ N −{0}. Der NEA Ak in Abbildung 3.19 akzeptiert Lk auf die Weise, dass er für jedes Symbol 1 der Eingabe nichtdeterministisch im Zustand q0 rät, ob dieses Symbol das k-te Symbol vor dem Ende der Eingabe ist. Ak veriﬁziert dann deterministisch, ob diese Entscheidung korrekt war. Aufgabe 3.23. Geben Sie eine formale Beschreibung des NEA Ak aus Abbildung 3.19 und beweisen Sie, dass Ak die Sprache Lk akzeptiert. Konstruieren Sie für jedes k ∈ N −{0} einen EA Bk mit Lk = L(Bk). 84 3 Endliche Automaten q0 q1 q2 qk−1 qk 0, 1 1 1 0 1 0 1 0 1 0 Abbildung 3.19 Der NEA Ak hat k + 1 Zustände. Wir beweisen jetzt, dass jeder EA, der Lk akzeptiert, exponentiell viele Zustände bezüglich der Größe von Ak haben muss. Lemma 3.6. Für alle k ∈ N −{0} muss jeder EA, der Lk akzeptiert, mindestens 2k Zustände haben. Beweis. Sei Bk =(Qk, Σbool,δk,q0k,Fk) ein EA mit L(Bk)= Lk. Um zu zeigen, dass Bk mindestens 2k viele Zustände haben muss, verwenden wir die gleiche grundlegende Idee, die wir in Abschnitt 3.4 für die Beweise der Nichtexistenz13 benutzt haben. Wenn ˆδk(q0k,x)= ˆδk(q0k,y) für irgendwelche Wörter x und y über Σbool, dann gilt für alle z ∈ (Σbool) ∗ xz ∈ L(Bk) ⇐⇒ yz ∈ L(Bk). (3.14) Die Idee des Beweises ist es, eine Menge Sk von Wörtern zu ﬁnden, so dass für keine zwei unterschiedlichen Wörter x, y ∈ Sk die Gleichung ˆδk(q0k,x)= ˆδk(q0k,y) gelten darf. Dann müsste Bk mindestens |Sk| viele Zustände haben. 14 Wir wählen Sk =(Σbool)k und zeigen, dass ˆδk(q0k,u) paarweise unterschiedliche Zustände für alle Wörter u aus Sk sind. Beweisen wir es indirekt. Seien x = x1x2 ...xk und y = y1y2 ...yk, mit xi,yi ∈ Σbool für i =1,...,k, zwei unterschiedliche Wörter aus Sk =(Σbool) k. Setzen wir ˆδk(q0k,x)= ˆδk(q0k,y) voraus. Weil x ̸= y, existiert ein j ∈{1,...,k}, so dass xj ̸= yj. Ohne Beschränkung der Allgemeinheit setzen wir xj = 1 und yj = 0 voraus. Betrachten wir jetzt z =0 j−1. Dann ist xz = x1 ...xj−11xj+1 ...xk0 j−1 und yz = y1 ...yj−10yj+1 ...yk0 j−1 und daher xz ∈ Lk und yz /∈ Lk. Dies ist aber ein Widerspruch zu (3.14). Daher hat Bk mindestens |Sk| =2k viele Zustände. \u0002 13Dies sollte nicht überraschend sein, da wir im Prinzip auch einen Nichtexistenzbeweis führen. Wir beweisen, dass kein EA mit weniger als 2k Zuständen für Lk existiert. 14Wenn |Sk| unendlich wäre, dann würde dies die Nichtexistenz eines EA für die gegebene Sprache bedeuten. 3.5 Nichtdeterminismus 85 In dem Beweis des obigen Lemmas haben wir eine einfache Beweismethode eingeführt, mit der man untere Schranken für die Größe endlicher Automaten zur Akzeptierung konkreter regulärer Sprachen zeigen kann. Um diese Methode noch an einem einfachen Beispiel zu veranschaulichen, zeigen wir ihre Anwendung für die reguläre Sprache L = {x11y | x, y ∈{0, 1}∗}, die wir schon bei der Illustrierung der Potenzmengenkonstruktion in Abbildung 3.15 betrachtet haben. Um zu zeigen, dass jeder endliche Automat für L mindestens drei Zustände hat, suchen wir uns die folgenden drei Wörter aus: λ, 1, 11. Jetzt sollen wir für jedes Paar (x, y) aus unterschiedlichen Wörtern x, y aus S = {λ, 1, 11} zeigen, dass ein Wort z existiert, so dass genau eines der Wörter xz und yz in L ist (d. h., x und y erfüllen die Äquivalenz (3.14) nicht). Für x = λ und y = 1 wählen wir z = 1. Dann gilt xz =1 /∈ L und yz =11 ∈ L. Daher ist ˆδ(q0,λ) ̸= ˆδ(q0, 1) für jeden endlichen Automaten, der L akzeptiert. Für x = λ und y = 11 wählen wir z = 0. Dann gilt xz =0 /∈ L und yz = 110 ∈ L und deshalb müssen ˆδ(q0,λ) und ˆδ(q0, 11) auch unterschiedlich sein. Für x = 1 und y = 11 wählen wir z = λ. Dann gilt xz =1 /∈ L und yz =11 ∈ L und deshalb müssen ˆδ(q0, 1) und ˆδ(q0, 11) ebenfalls unterschiedlich sein. Zusammengefasst müssen ˆδ(q0,λ), ˆδ(q0, 1) und ˆδ(q0, 11) paarweise unterschiedlich sein für jeden EA A =(Q, Σbool,δ,q0,F ), der L akzeptiert. Aufgabe 3.24. Betrachten Sie die Sprache L = {u11v | u, v ∈{0, 1} ∗} und die drei Wörter λ, 1, 11. Bestimmen Sie für alle Paare x, y von unterschiedlichen Wörtern aus S = {λ, 1, 11} die Menge Z(x, y) ⊆ (Σbool) ∗, so dass für jedes z ∈ Z(x, y) (xz /∈ L und yz ∈ L)oder(xz ∈ L und yz /∈ L). Aufgabe 3.25. Betrachten Sie L = {x11y | x, y ∈{0, 1} ∗}. Wählen Sie S′ disjunkt zu S = {λ, 1, 11} und wenden Sie S′ an, um zu zeigen, dass jeder EA für L mindestens drei Zustände hat. Aufgabe 3.26. Sei L = {x011y | x, y ∈{0, 1} ∗}. (i) Konstruieren Sie einen NEA M mit vier Zuständen für L und beweisen Sie L = L(M ). (ii) Wenden Sie auf M die Potenzmengenkonstruktion an, um so einen (deterministischen) EA zu erhalten. 86 3 Endliche Automaten Aufgabe 3.27. ∗ Ein EA A heißt minimal für die reguläre Sprache L(A), wenn kein kleinerer (bezüglich der Kardinalität der Zustandsmenge) EA B mit L(A)= L(B) existiert. Konstruieren Sie minimale EA für die Sprachen aus Aufgabe 3.9 und beweisen Sie ihre Minimalität. Aufgabe 3.28. Entwerfen Sie einen NEA M mit höchstens sechs Zuständen, so dass L(M )= {0x | x ∈{0, 1} ∗ und x enthält die Wörter 11 oder 100 als Teilwörter}. 3.6 Zusammenfassung In diesem Kapitel haben wir die endlichen Automaten als ein Modell sehr einfacher Berechnungen vorgestellt, die keine Variablen und damit keinen Speicher benutzen. Die Zielsetzung war aber nicht, die Automaten zu studieren, sondern an einem einfachen Beispiel das Deﬁnieren eines Berechungsmodells zu zeigen. Die Deﬁnition beginnt mit der Beschreibung der Komponenten und mit der Festlegung der grundlegenden Operationen (Aktionen) des Modells. Danach deﬁniert man den Begriﬀ der Konﬁguration, die eine vollständige Beschreibung des allgemeinen Zustands des betrachteten Berechnungsmodells zu einem gegebenen Zeitpunkt ist. Die Durchführung eines Berechnungsschrittes entspricht der Anwendung einer elementaren Operation des Modells auf die aktuelle Konﬁguration. So deﬁniert man einen Schritt als einen Übergang aus einer Konﬁguration C zu einer anderen Konﬁguration D. Der Unterschied zwischen C und D ist durch eine elementare Operation erzeugbar. Eine Berechnung beschreibt man als eine Folge von Konﬁgurationen mit der Eigenschaft, dass man aus jeder Konﬁguration zu ihrer Nachfolgekonﬁguration in einem Berechnungsschritt übergehen kann. Eine Berechnung auf einer Eingabe x startet in einer initialen Konﬁguration, in der x als Eingabe zur Verfügung steht. Die letzte Konﬁguration der Berechnung bestimmt das Resultat. Bei endlichen Automaten handelt es sich um einfache Algorithmen zur Spracherkennung (zur Lösung von Entscheidungsproblemen), die sich beim Lesen von Eingabesymbolen zwischen (inneren) Zuständen aus einer endlichen Zustandsmenge bewegen. Ein endlicher Automat akzeptiert ein Eingabewort, wenn er sich nach den Lesen des letzten Buchstabens des Eingabewortes in einem akzeptierenden Zustand beﬁndet. Zu zeigen, dass ein Problem durch Algorithmen aus einer bestimmten Algorithmenklasse nicht lösbar ist, bedeutet, einen Nichtexistenzbeweis zu führen. Dies erfordert meistens ein tieferes Verständnis der Natur der betrachteten Algorithmenklasse. Im Fall der endlichen Automaten basieren die Beweise von L/∈LEA auf der Tatsache, dass die Vielfältigkeit der Wörter in L so groß ist, dass man sie durch die Zerlegung von Σ∗ in endlich viele Klassen nicht charakterisieren kann. Mit anderen Worten reichen endlich viele Zustände als Speicher nicht aus, um alle wichtigen Eigenschaften (Charakteristika) der bisher gelesenen Präﬁxe von Eingabewörtern zu speichern. Diese Art der Argumentation kann man auch benutzen, um für eine gegebene Sprache U ∈LEA eine untere Schranke für die Größe eines jeden EA A mit L(A)= U zu beweisen. Nichtdeterministische Algorithmenmodelle (Rechnermodelle) erlauben im Unterschied zu deterministischen eine Auswahl von mehreren (endlich vielen) Aktionen in jedem Berechnungsschritt. Dadurch kann ein nichtdeterministischer Algorithmus exponentiell viele (bezüglich der Eingabelänge) unterschiedliche Berechnungen auf einer Eingabe haben. Die Interpretation der Arbeit eines nichtdeterministischen Algorithmus A ist optimistisch 3.6 Zusammenfassung 87 – wir setzen voraus, dass A in jeder nichtdeterministischen Wahl eine richtige Entscheidung triﬀt, falls eine solche existiert. Dies bedeutet, dass ein nichtdeterministischer Algorithmus erfolgreich bei der Lösung eines Problems ist, wenn für jede Eingabeinstanz x mindestens eine der Berechnungen von A auf x das richtige Resultat liefert, zusammen mit dem Beweis, dass dieses Resultat richtig ist. Bei einem Entscheidungsproblem (Σ,L) bedeutet dies, dass A für alle x ∈ L mindestens eine akzeptierende Berechnung (mit der Aussage „x ∈ L“) auf x hat und alle Berechnungen von A auf y für jedes y/∈ L nichtakzeptierend sind (mit dem Resultat „y/∈ L“ enden). Im Allgemeinen kennen wir keine eﬃzientere Art, nichtdeterministische Algorithmen A durch deterministische Algorithmen B zu simulieren, als alle möglichen Berechnungen von A durch B zu simulieren. Dies ist auch der Fall bei endlichen Automaten, wo die Simulation einer Breitensuche im Berechnungsbaum von A auf gegebener Eingabe entspricht. Weil B dabei nach dem Lesen eines Präﬁxes z des Eingabewortes alle Zustände speichert, die A beim Lesen von z erreichen kann, nennt man die Konstruktion des EA B aus dem NEA A die Potenzmengenkonstruktion. Dieses Kapitel ist nur einigen elementaren Aspekten der Automatentheorie gewidmet und erhebt deswegen keinen weiteren Anspruch, als eine Einführung in dieses Gebiet zu sein. Was die Kenntnisse der Klasse regulärer Sprachen anbelangt, liefert dieses Ka- pitel einen noch unwesentlicheren Beitrag. Reguläre Sprachen können nicht nur durch andere Automatenmodelle charakterisiert werden, sondern auch durch weitere wichtige Mechanismen, die nicht auf Maschinenmodellen (Berechnungsmodellen) basieren. Die wichtigsten Beispiele solcher Formalisierungen sind die regulären Grammatiken als Gene- rierungsmechanismen und die algebraische Darstellung durch reguläre Ausdrücke. Für die Erweiterung der Kenntnisse über die Klasse der regulären Sprachen empfehlen wir wärmstens Hopcroft, Motwani und Ullman [HMU06]. Kontrollaufgaben 1. Wie kann man das Modell des endlichen Automaten mit der Hilfe von Programmen erklären? Warum benutzen wir einen endlichen Automaten nicht zur Modellierung von Rechnern, obwohl die Rechner (wie auch alles andere, was wir kennen) endlich sind? 2. Deﬁnieren Sie einen EA formal als ein Quintupel und erklären Sie die Bedeutung der Grundbegriﬀe der Theorie der Berechnungen wie Konﬁguration, Berechnungsschritt und Berechnung. 3. Erklären Sie, wie die Zustände eines endlichen Automaten A die Menge aller Eingaben über Σ ∗ in endlich viele Klassen zerlegen. Wie kann dies beim Entwurf und der Veriﬁkation eines endlichen Automaten helfen? Zeigen Sie ein paar Beispiele. 4. Entwerfen Sie basierend auf der Bedeutung der Zustände endliche Automaten für folgende Sprachen: (a) {x1111y | x, y ∈{0, 1, 2, 3} ∗}, (b) {x110110y | x, y ∈{0, 1, 2} ∗}, (c) {x010101y | x, y ∈{0, 1} ∗}, (d) {x0y | x, y ∈{0, 1} ∗}, (e) {x0y1z | x, y, z ∈{0, 1} ∗}, 88 3 Endliche Automaten (f) {x ∈{0, 1}∗ ||x|0 ≥ 3}, (g) {x ∈{0, 1}∗ | 3 ≤|x|0 ≤ 5}, (h) {x001y101z | x, y, z ∈{0, 1}∗}, (i) {x0011 | x ∈{0, 1}∗}, (j) {x10011 | x ∈{0, 1} ∗}, (k) {1x11001 | x ∈{0, 1}∗}, (l) {x11001y0 | x, y ∈{0, 1}∗}. Wählen Sie sich drei dieser Sprachen aus und beweisen Sie mit Induktion die Korrektheit der von Ihnen entworfenen Automaten. 5. Entwerfen Sie Automaten für die folgenden Sprachen: (a) {x ∈{0, 1} ∗ | x enthält 11 als Teilwort oder endet mit dem Suﬃx 10}, (b) {λ, 0, 11,x00 | x ∈{0, 1} ∗}, (c) {x ∈{0, 1} ∗ | x enthält 00 oder 11 als Teilwort}. 6. Erklären Sie, wie ein EA simultan mehrere endliche Automaten simulieren kann. Wenden Sie diese Simulationstechnik an, um den modularen Entwurf endlicher Automaten für folgende Sprachen zu realisieren: (a) {x ∈{0, 1} ∗ ||x|1 ist gerade und x enthält das Teilwort 001}, (b) {x ∈{0, 1}∗ | x enthält das Teilwort 000 und x enthält das Teilwort 111}, (c) {x ∈{0, 1, 2} ∗ | x enthält die Teilwörter 021 und 2110}, (d) {x ∈{0, 1}∗ ||x|1 mod3=1 und x endet mit dem Suﬃx 0011}. 7. Erklären Sie die drei Methoden zum Beweis, dass eine Sprache keine reguläre Sprache ist. 8. Beweisen Sie, dass folgende Sprachen nicht regulär sind. Wenden Sie dabei jede der drei gelernten Methoden mindestens einmal an: (a) {0 n 3 | n ∈ N}, (b) {0n1 2n0 n | n ∈ N}, (c) {xy ∈{0, 1} ∗ | x ist ein nichtleeres Präﬁx von y}, (d) {x2y | x, y ∈{0, 1}∗, |x|1 = |y|0}, (e) {03n | n ∈ N}. 9. Erklären Sie das Konzept nichtdeterministischer endlicher Automaten. Geben Sie ein Beispiel eines NEA an, der auf jeder Eingabe der Länge n exponentiell viele unterschiedliche Berechnungen hat. 10. Erklären Sie, warum deterministische endliche Automaten nichtdeterministische endliche Automaten (trotz der Möglichkeit vieler Berechnungen auf einer Eingabe) simulieren können. 11. Für welche Sprachen ist es wesentlich einfacher einen NEA als einen EA zu entwerfen? 3.6 Zusammenfassung 89 12. Entwerfen Sie nichtdeterministische endliche Automaten für folgende Sprachen: (a) {x1010y | x, y ∈{0, 1} ∗}∪{z010 | z ∈{0, 1} ∗}, (b) {x ∈{0, 1} ∗ | x enthält 001 oder 110 als ein Teilwort}, (c) {x ∈{0, 1}∗ ||x|1 mod3=0 oder x = y101z für y, z ∈{0, 1} ∗}, (d) {x ∈{0, 1, 2} ∗ | x enthält eines der Wörter 012, 00, 22, 111 als Teilwort}. Wenden Sie die Potenzmengenkonstruktion für mindestens einen der entworfenen NEAs an, um einen äquivalenten EA zu bauen. 13. Beweisen Sie, dass jeder EA für eine der folgenden Sprachen mindestens 4 Zustände hat. (a) {x ∈{0, 1} ∗ ||x| mod4=2}, (b) {x0011y | x, y ∈{0, 1} ∗}, (c) {01x10 | x ∈{0, 1}}, (d) {x ∈{0, 1, 2}∗ ||x|0 ist gerade und |x|1 ist teilbar durch 3}. Können Sie für einige dieser Sprachen sogar eine höhere untere Schranke für die Anzahl der Zustände beweisen? Die Menschen, die die Geduld haben, auch einfache Sachen vollkommen zu machen, gewinnen die Fähigkeit, auch schwere Sachen einfach zu meistern. F. Schiller 4 Turingmaschinen 4.1 Zielsetzung Wenn man ursprünglich in der Mathematik einen Ansatz zur Lösung gewisser Probleme vermitteln wollte, hat man ihn als eine mathematische Methode formal genau beschrieben. Eine sorgfältige Beschreibung einer Methode hatte die Eigenschaft, dass ein Anwender gar nicht verstehen musste, warum die Methode funktioniert, und trotzdem die Methode erfolgreich zur Lösung seiner Probleminstanz verwenden konnte. Die einzige Voraussetzung für eine erfolgreiche Anwendung war das Verständnis des mathematischen Formalismus, in dem die Methode dargestellt wurde. Die Entwicklung des Rechners führte dazu, dass man Methoden zur Lösung von Problemen durch Programme beschreibt. Der mathematische Formalismus ist hier durch die benutzte Programmiersprache gegeben. Das wichtigste Merkmal aber bleibt. Der Rechner, der keinen Intellekt besitzt und daher kein Verständnis für das Problem sowie für die Methode zu seiner Lösung besitzt, kann das Programm ausführen und dadurch das Problem lösen. Deswegen können wir über automatische oder algorithmische Lösbarkeit von Problemen sprechen. Um zu zeigen, dass ein Problem automatisch lösbar ist, reicht es aus, eine Methode zu seiner Lösung zu ﬁnden und diese in Form eines Programms (Algorithmus) darzustellen. Deswegen kommen positive Aussagen über algorithmische (automatische) Problemlösbarkeit gut ohne eine Festlegung auf eine Formalisierung des Begriﬀs Algorithmus aus. Es reicht oft, eine Methode halb informell und grob zu beschreiben, und jedem wird klar, dass sich die Methode in die Form eines Programms umsetzen lässt. Daher ist es nicht verwunderlich, dass die Mathematik schon lange vor der Existenz von Rechnern die Lösbarkeit mathematischer Probleme mit der Existenz allgemeiner Lösungsmethoden (heute würden wir Algorithmen sagen) im Sinne von „automatischer Lösbarkeit“ verknüpft hat. Die Notwendigkeit der Formalisierung des Begriﬀs Algorithmus (Lösungsmethode) kam erst mit dem Gedanken, mathematisch die automatische Unlösbarkeit konkreter Probleme zu beweisen. Zu diesem Zweck hat man mehrere Formalismen entwickelt, die sich alle als äquivalent bezüglich des Begriﬀs „automatische (algorithmische) Lösbarkeit“ erwiesen haben. Auch jede vernünftige Programmiersprache ist eine zulässige Formalisierung der J. Hromkovič, Theoretische Informatik, DOI 10.1007/978-3-658-06433-4_4, © Springer Fachmedien Wiesbaden 2014 92 4 Turingmaschinen automatischen Lösbarkeit. Aber solche Formalisierungen sind nicht so gut geeignet für Beweise der Nichtexistenz von Algorithmen für konkrete Probleme, weil sie wegen der Anwenderfreundlichkeit eine Menge komplexer Operationen (Befehle) enthalten. Daher braucht man sehr einfache Modelle, die nur ein paar elementare Operationen erlauben und trotzdem die volle Berechnungsstärke von Programmen beliebiger Programmiersprachen besitzen. Ein solches Modell, das sich in der Theorie zum Standard entwickelt hat, ist die Turingmaschine. Die Zielsetzung dieses Kapitels ist, dieses Modell vorzustellen und so die Basis für die Theorie der Berechenbarkeit (der algorithmischen Lösbarkeit) und für die Komplexitätstheorie in den nächsten Kapiteln zu schaﬀen. Der Stoﬀ dieses Kapitels ist in fünf Abschnitte unterteilt. Abschnitt 4.2 skizziert die historische Entwicklung, die schliesslich zur Formalisierung des Begriﬀs Algorithmus führte. Abschnitt 4.3 stellt das grundlegende Modell der Turingmaschine vor und übt den Umgang mit Turingmaschinen. Abschnitt 4.4 präsentiert die Mehrband-Varianten von Turingmaschinen, die das grundlegende Modell der abstrakten Komplexitätstheorie sind. In diesem Abschnitt wird auch die Äquivalenz zwischen Programmen in einer be- liebigen Programmiersprache und Turingmaschinen diskutiert. Abschnitt 4.5 führt die nichtdeterministische Turingmaschine ein und untersucht die Möglichkeiten der Simulati- on nichtdeterministischer Turingmaschinen durch (deterministische) Turingmaschinen. Abschnitt 4.6 präsentiert eine mögliche Kodierung von Turingmaschinen als Wörter über dem Alphabet Σbool. Bevor wir den Begriﬀ des Algorithmus in der Form der Turingmaschine axiomatisch deﬁ- nieren, präsentieren wir die Überlegungen von Alan Turing, die zu dieser mathematischen Deﬁnition des Algorithmus geführt haben. 4.2 Auszug aus der Geschichte Die Automatisierung im weiteren Sinne dieses Wortes begleitet die Entwicklung der menschlichen Zivilisation seit jeher. Das durch Beobachtungen, Experimente und Nach- denken erwobene Wissen hat man zur Erzeugung unterschiedlicher Produkte und zur Entwicklung diverser Vorgehensweisen in allen Bereichen des menschlichen Lebens ge- nutzt (zum Beispiel für Heilungsprozeduren, Bauverfahren, Werkzeugherstellung etc.). Es war wichtig, dass man all diese Vorgehensweisen erfolgreich verwenden konnte, ohne zu verstehen, warum sie funktionieren und wie man das dazu nötige Wissen entdeckt hat. Zum Beispiel wusste man dank Pythagoras, dass ein durch drei Seiten der Längen 3, 4 und 5 Einheiten aufgespanntes Dreieck einen rechten Winkel zwischen den Katheten besitzt. Also erzeugten die Bauarbeiter rechte Winkel auf diese Weise. Sie brauchten dafür den Satz des Pythagoras gar nicht zu verstehen, von seinem Beweis mussten sie keine Ahnung haben. Diese Tatsache brachte der menschlichen Gesellschaft die Eﬃzienz, die für ihre Weiterentwicklung maßgeblich war. Die Technik und die Informatik sind heute der Höhepunkt dieser Entwicklung. Es werden Produkte hergestellt, die auf Befehl (zum Beispiel auf Knopfdruck) eine gewünschte Tätigkeit ausüben. Am Anfang des zwanzigsten Jahrhunderts war es noch so, dass zur Ausführung eines Algorithmus (einer Vorgehensweise) in der Regel ein Mensch notwendig war. Zum Beispiel gab es menschliche Rechner, die die entwickelten Methoden zur Berechnung ballistischer 4.2 Auszug aus der Geschichte 93 Kurven umgesetzt haben. Die menschlichen Rechner wurden aus einer Gruppe von Personen zusammengesetzt, die all diejenigen mathematischen Operationen realisieren konnten, die in der Beschreibung des Algorithmus verwendet wurden. Dies war die Zeit, zu der Alan Turing versuchte, den Begriﬀ des Algorithmus mathe- matisch festzulegen. Er wollte keinen Computer bauen. Er wollte die Klasse derjenigen Methoden als Algorithmen deﬁnieren, die jeder dazu ausgebildete Mensch durchführen konnte. Seine Überlegungen beruhten auf der folgenden Strategie. Die atomaren Bausteine eines Algorithmus sind Operationen (elementare Basisinstruktionen wie zum Beispiel arithmetische Operationen). Man wählt nun einfach eine endliche Menge von Operationen aus, und alle Verfahren, die man aus diesen Operationen zusammensetzen kann, sind Algorithmen über dieser Klasse von Basisoperationen. Das Problem ist natürlich, dass man unbegrenzt viele solche Mengen von Operationen zu betrachten hat. Welche Eigenschaften muss eine solche Menge von Basisoperationen sinnvollerweise erfüllen? 1. Alle gewählten Basisoperationen sind eindeutig interpretierbar und jede von ihnen ist für einen Menschen vollständig erlernbar. 2. Alles, was wir intuitiv unter dem Begriﬀ Algorithmus verstehen, muss man aus dieser Menge von Operationen zusammensetzen können. Die erste Anforderung ist leicht zu erfüllen. Aus heutiger Sicht kann man die Befehle einer beliebigen Assemblersprache oder sogar einer höheren Programmiersprache nehmen und man ist sich sicher, dass Anforderung 1 erfüllt ist. Anforderung 2 kann man als die Forderung nach der Vollständigkeit der Menge von Operationen verstehen, sie ist nicht so einfach zu erfüllen. Egal, wie groß die Menge von Basisoperationen ist, die man nimmt, es kann immer vorkommen, dass jemand einen Algorithmus (etwas, das wir intuitiv als Algorithmus anerkennen, weil uns die Umsetzung klar ist) vorschlägt, den man nicht aus diesen Operationen zusammensetzen kann. Und dies war auch das Hauptproblem von Alan Turing. Er versuchte, die Vorgehensweise eines Mathematikers zu beschreiben, der in der formalen Logik arbeitet. Dieser benutzt ein Alphabet, um logische Behauptungen zu formulieren. Man kann sich dies so vorstellen, dass jede solche Behauptung in eine Zeile geschrieben wird. Die Länge dieser Zeilen darf man nicht durch eine Konstante beschränken, weil es für jede Zahl n Behauptungen gibt, die man nicht als Text der Länge höchstens n darstellen kann. Um jetzt aus bereits bestehenden Behauptungen etwas Neues zu folgern, darf der Mathematiker in einem Rechenschritt einen Teil des Textes durch etwas Äquivalentes ersetzen. Weil das menschliche Gehirn endlich ist, gibt es eine feste Konstante d, die beschränkt, wie lang der Teil eines Textes ist, den er auf einmal anschauen und somit bearbeiten kann. Die Anzahl solcher Textoperationen, die er durchführen will, ist endlich, aber nicht durch eine feste Konstante beschränkbar. Daraus schloss Alan Turing, dass der Mathematiker außer seinem endlichen Gehirn noch ein Blatt Papier als Hardware braucht, das in beiden Dimensionen unendlich ist, also eine unbegrenzte Anzahl von Rechenschritten (Anzahl von Zeilen) auf unbegrenzt langen Texten ermöglicht. Danach vereinfachte Turing dieses Modell in zwei Schritten. Zunächst kann der Mathe- matiker mit einer Zeile Text auskommen, wenn er einen Radiergummi zur Hilfe nimmt, 94 4 Turingmaschinen und damit die vorgenommenen Ersetzungen direkt an den entsprechenden Stellen durch- führen kann. Damit ist anstatt des in zwei Dimensionen unbegrenzten Papierblatts nur noch ein Papierband nötig, ähnlich dem Band eines endlichen Automaten. Allerdings ist dieses Band unendlich lang. Das Gehirn ist endlich und somit immer in einem von endlich vielen Zuständen, die man wie bei einem endlichen Automaten mit einer Zustandsmenge und einer Übergangsfunkti- on auf dieser Zustandsmenge simulieren kann. Die unbekannte Konstante d, die bestimmt, wie groß ein Teil des Textes sein darf, den der Mathematiker in einem Schritt austau- schen kann, machte Turing aber auch noch Sorgen, weil dadurch die Anzahl möglicher Operationen exponentiell groß in d war. Da entdeckte er, dass man jede solche Operation zum Textaustausch eines endlich langen Textes auch als eine Folge kleinerer Operationen ausführen kann, die nur ein Symbol lesen und danach das Symbol löschen oder gegen ein anderes Symbol austauschen. Auf diese Weise hat er alle semantischen Probleme beseitigt, die durch die Auswahl der benötigten Basisoperationen auftraten, weil man alle Operationen durch die Textoperationen auf einzelnen Symbolen und Herumlaufen mit dem Stift auf dem Band erledigen kann. 4.3 Das Modell der Turingmaschine Eine Turingmaschine kann als eine Verallgemeinerung eines EA gesehen werden. Informell besteht sie (Abbildung 4.1) aus (i) einer endlichen Kontrolle, die das Programm enthält, (ii) einem unendlichem Band, das als Eingabeband, aber auch als Speicher (Arbeitsband) zur Verfügung steht, und (iii) einem Lese-/Schreibkopf, der sich in beiden Richtungen auf dem Band bewegen kann. Die Ähnlichkeit zu einem EA besteht in der Kontrolle über einer endlichen Zustands- menge und dem Band, das am Anfang das Eingabewort enthält. Der Hauptunterschied zwischen Turingmaschinen und endlichen Automaten besteht darin, dass eine Turingma- schine das Band auch als Speicher benutzen kann und dass dieses Band unendlich lang ist. Das erreicht man dadurch, dass der Lesekopf des EA durch einen Lese-/Schreibkopf ersetzt wird und dass man diesem Kopf auch die Bewegung nach links erlaubt. Eine elementare Operation von Turingmaschinen kann also als folgende Aktion beschrieben werden. Die Argumente sind (i) der Zustand, in dem sich die Maschine beﬁndet, und (ii) das Symbol auf dem Feld des Bandes, auf dem sich der Lese-/Schreibkopf gerade beﬁndet. Abhängig von diesen Argumenten macht die Turingmaschine Folgendes. Sie (i) ändert den Zustand, 4.3 Das Modell der Turingmaschine 95 0 1 2 3 i i +1i − 1 n n +1 n +2 x1 x2 x3 xi xnxi−1 xi+1 ␣␣··· ······¢ endliche Kontrolle (Programm) unendliches Band Lese-/Schreibkopf Abbildung 4.1 (ii) schreibt ein Symbol auf das Feld des Bandes, von dem gerade gelesen wurde 1 (dies kann man also als ein Ersetzen des gelesenen Symbols durch ein neues Symbol sehen), und (iii) bewegt den Lese-/Schreibkopf ein Feld nach links oder rechts oder sie bewegt ihn gar nicht. Wichtig dabei ist, dass 1. das Band auf der linken Seite das Randsymbol ¢ enthält, über das die Turingmaschine nach links nicht weiter gehen darf und welches nie durch ein anderes Symbol ersetzt werden darf (die Einführung eines linken Randes ermöglicht die Nummerierung der Felder des Bandes von links nach rechts, wobei wir dem Bandfeld mit dem Symbol ¢ die Nummer 0 zuordnen), und 2. das Band nach rechts unendlich ist,2 wobei die nicht beschrifteten Felder das Leerfeld-Symbol ␣ (manchmal auch Blanksymbol genannt) enthalten. Jetzt geben wir die formale Beschreibung einer Turingmaschine in der Weise an, wie wir es bei der Deﬁnition von endlichen Automaten gelernt haben. Zuerst beschreiben wir die Komponenten und die elementaren Operationen. Dann wählen wir eine Darstellung von Konﬁgurationen und deﬁnieren den Berechnungsschritt als eine Relation auf Konﬁgu- rationen. Danach folgen die Deﬁnitionen der Berechnung einer Turingmaschine und der von einer Turingmaschine akzeptierten Sprache. Deﬁnition 4.1. Eine Turingmaschine (TM ) ist ein 7-Tupel M =(Q, Σ, Γ,δ,q0, qaccept,qreject). Dabei ist (i) Q eine endliche Menge, die Zustandsmenge von M genannt wird, (ii) Σ das Eingabealphabet, wobei ¢ und das Blanksymbol ␣nicht in Σ sind, {Σ dient genau wie bei einem EA zur Darstellung der Eingabe.} 1Wo sich der Lese-/Schreibkopf gerade beﬁndet. 2Man bemerke, dass man in endlicher Zeit höchstens endlich viele Felder des Bandes beschriften kann und damit die aktuelle Speichergröße (die Anzahl der Nicht-␣-Felder) immer endlich ist. Der Sinn der Forderung eines unbeschränkten Speichers ist nur, dass man nach Bedarf einen beliebig großen endlichen Speicher zur Verfügung hat. 96 4 Turingmaschinen (iii) Γ ein Alphabet, Arbeitsalphabet genannt, wobei Σ ⊆ Γ, ¢, ␣ ∈ Γ, Γ ∩ Q = ∅, {Γ enthält alle Symbole, die in Feldern des Bandes auftreten dürfen, d. h. die Symbole, die M als Speicherinhalte (variable Werte) benutzt.} (iv) δ : (Q −{qaccept,qreject}) × Γ → Q × Γ ×{L, R, N} eine Abbildung, Übergangsfunk- tion von M genannt, mit der Eigenschaft δ(q, ¢) ∈ Q ×{¢}×{R, N} für alle q ∈ Q, {δ beschreibt die elementare Operation von M . M kann eine Aktion (q, X, Z) ∈ Q × Γ ×{L, R, N} aus einem aktuellen Zustand p beim Lesen eines Symbols Y ∈ Γ durchführen, falls δ(p, Y )=(q, X, Z). Dies bedeutet den Übergang von p nach q, das Ersetzen von Y durch X und die Bewegung des Kopfes entsprechend Z. Z =L bedeutet die Bewegung des Kopfes nach links, Z = R nach rechts und Z =N bedeutet keine Bewegung. Die Eigenschaft δ(q, ¢) ∈ Q ×{¢}×{R, N} verbietet das Ersetzen des Symbols ¢ durch ein anderes Symbol und die Bewegung des Kopfes nach links über die Randbezeichnung ¢.} (v) q0 ∈ Q der Anfangszustand, (vi) qaccept ∈ Q der akzeptierende Zustand, {M hat genau einen akzeptierenden Zustand. Wenn M den Zustand qaccept erreicht, akzeptiert M die Eingabe, egal wo sich dabei der Kopf auf dem Band beﬁndet. Aus qaccept ist keine Aktion von M mehr möglich.} (vii) qreject ∈ Q −{qaccept} der verwerfende Zustand. {Wenn M den Zustand qreject erreicht, dann endet damit die Berechnung und M verwirft die Eingabe. Das heißt insbesondere auch, dass M nicht die komplette Eingabe lesen muss, um sie zu akzeptieren beziehungsweise zu verwerfen.} Eine Konﬁguration C von M ist ein Element aus Konf (M ) = {¢}· Γ∗ · Q · Γ + ∪ Q ·{¢}· Γ +. {Eine Konﬁguration ¢w1qaw2, w1 ∈ Γ∗, w2 ∈ Γ∗, a ∈ Γ, q ∈ Q, (Abbildung 4.2) ist eine vollständige Beschreibung folgender Situation. M ist im Zustand q, der Inhalt des Bandes ist ¢w1aw2␣␣␣ ... und der Kopf steht auf dem Feld |w1| + 1 des Bandes und liest das Symbol a. Eine Konﬁguration p¢w, p ∈ Q, w ∈ Γ∗, beschreibt die Situation, in der der Inhalt des Bandes ¢w␣␣␣ ... ist und der Kopf auf dem 0-ten Feld des Bandes steht und das Randsymbol ¢ liest.3} Eine Startkonﬁguration für ein Eingabewort x ist q0¢x. Ein Schritt von M ist eine Relation M auf der Menge der Konﬁgurationen ( M ⊆ Konf(M ) × Konf(M )) deﬁniert durch 3Man bemerke, dass man für die Darstellung der Konﬁgurationen zwischen mehreren guten Möglichkeiten wählen kann. Zum Beispiel könnte man die Darstellung (q, w, i) ∈ Q × Γ∗ × N verwenden, um die Situation zu beschreiben, in der M im Zustand q ist, w␣␣␣ ... auf dem Band steht und der Kopf auf dem i-ten Feld des Bandes steht. 4.3 Das Modell der Turingmaschine 97 ¢ w1 w2 ␣␣ ... q a Abbildung 4.2 (i) x1x2 ...xi−1qxixi+1 ...xn M x1x2 ...xi−1pyxi+1 ...xn, falls δ(q, xi)=(p, y, N) (Abbildung 4.3 (a)), (ii) x1x2 ...xi−1qxixi+1 ...xn M x1x2 ...xi−2pxi−1yxi+1 ...xn, falls δ(q, xi)=(p, y, L) (Abbildung 4.3 (b)), (iii) x1x2 ...xi−1qxixi+1 ...xn M x1x2 ...xi−1ypxi+1 ...xn, falls δ(q, xi)=(p, y, R) für i<n (Abbildung 4.3 (c)) und (iv) x1x2 ...xn−1qxn M x1x2 ...xn−1yp␣, falls δ(q, xn)=(p, y, R) (Abbildung 4.3 (d)). Eine Berechnung von M ist eine (potentiell unendliche) Folge von Konﬁgurationen C0,C1,C2,..., so dass Ci M Ci+1 für alle i =0, 1, 2,.... Wenn C0 M C1 M ··· M Ci für ein i ∈ N, dann C0 M ∗ Ci. Die Berechnung von M auf einer Eingabe x ist eine Berechnung, die mit der Startkonﬁguration C0 = q0¢x beginnt und entweder unendlich ist oder in einer Konﬁgura- tion w1qw2 endet, wobei q ∈{qaccept,qreject}. Die Berechnung von M auf x heißt akzeptierend, falls sie in einer akzeptierenden Konﬁguration w1qacceptw2 endet. Die Berechnung von M auf x heißt verwerfend, wenn sie in einer verwerfenden Konﬁguration w1qrejectw2 endet. Eine nicht-akzeptierende Berechnung von M auf x ist entweder eine verwerfende oder eine unendliche Berechnung von M auf x. Die von der Turingmaschine M akzeptierte Sprache ist L(M ) = {w ∈ Σ∗ | q0¢w M ∗ yqacceptz, für irgendwelche y, z ∈ Γ∗}. Wir sagen, dass M eine Funktion F :Σ∗ → Γ ∗ berechnet, falls für alle x ∈ Σ∗ : q0¢x M ∗ qaccept¢F (x). Eine Sprache L ⊆ Σ∗ heißt rekursiv aufzählbar, falls eine TM M existiert, so dass L = L(M ). LRE = {L(M ) | M ist eine TM} ist die Klasse aller rekursiv aufzählbaren Sprachen. 4 Eine Sprache L ⊆ Σ∗ heißt rekursiv (entscheidbar),5 falls L = L(M ) für eine TM M , für die für alle x ∈ Σ∗ gilt: 4Die Bezeichnung LRE kommt vom englischen „recursively enumerable“. 5Genauer müsste man sagen, dass das Entscheidungsproblem (Σ,L) entscheidbar ist. 98 4 Turingmaschinen x2x2 x2x2 x2x2 x2x2 xi−1xi−1 xi−1xi−1 xi−1xi−1 xn−1xn−1 xi+1xi+1 xi+1xi+1 xi+1xi+1 xi xi xi xn xnxn xnxn xnxn x1x1 x1x1 x1x1 x1x1 ¢¢ ¢¢ ¢¢ ¢¢ q q q q p p p p y y y y .. . .. . .. . .. . .. . .. ... . .. ... ... ... ... . .. ... ... ... ... . .. ... ... ... ... . ␣ (a) (b) (c) (d) Abbildung 4.3 (i) q0¢x M ∗ yqacceptz, y, z ∈ Γ∗, falls x ∈ L und (ii) q0¢x M ∗ uqrejectv, u, v ∈ Γ∗, falls x/∈ L. {Dies bedeutet, dass M keine unendlichen Berechnungen besitzt.} Wenn (i) und (ii) gelten, sagen wir, dass M auf jeder Eingabe hält oder dass M immer hält. {Eine TM, die immer hält, ist ein formales Modell des Begriﬀs Algorithmus.} LR = {L(M ) | M ist eine TM, die immer hält} ist die Klasse der rekursiven (algorithmisch erkennbaren) Sprachen. Eine Funktion F : Σ∗ 1 → Σ∗ 2 für zwei Alphabete Σ1, Σ2 heißt total berechenbar, falls eine TM M existiert, die F berechnet. 6 Die Turingmaschinen, die immer halten, repräsentieren die Algorithmen, die immer terminieren und die richtige Ausgabe liefern. So sind gerade die rekursiven Sprachen 6Beachten Sie, dass die TM M immer hält. 4.3 Das Modell der Turingmaschine 99 (entscheidbaren Entscheidungsprobleme) die Sprachen (Entscheidungsprobleme), die algo- rithmisch erkennbar (lösbar) sind. Aufgabe 4.1. Ändern Sie die Deﬁnition einer TM, indem Sie als Konﬁguration die Tripel (q, ¢w, i) ∈ Q ×{¢}Γ ∗ × N nehmen. Ein Tripel (q, ¢w, i) beschreibt die Situation, wenn die TM im Zustand q ist, der Bandinhalt ¢w␣␣␣ ... ist und der Kopf auf das i-te Feld des Bandes zeigt. Geben Sie dann eine Deﬁnition des Schrittes und der Berechnung, die mit dieser Darstellung von Konﬁgurationen arbeitet. Im Folgenden zeigen wir ein paar konkrete Turingmaschinen und ähnlich wie bei endlichen Automaten entwickeln wir eine anschauliche graphische Darstellung von Tu- ringmaschinen. Sei LMitte = {w ∈ (Σbool) ∗ | w = x1y, wobei |x| = |y|}. Somit enthält LMitte alle Wörter ungerader Länge, die in der Mitte eine 1 haben. Aufgabe 4.2. Beweisen Sie, dass LMitte /∈LEA. Wir beschreiben eine TM M , so dass L(M )= LMitte. Die Idee ist, zuerst zu überprüfen, ob die Eingabelänge ungerade ist und dann das mittlere Symbol zu bestimmen. Sei M =(Q, Σ, Γ,δ,q0,qaccept,qreject), wobei Q = {q0,qeven,qodd,qaccept,qreject,qA,qB,q1,qleft,qright,qmiddle}, Σ= {0, 1}, Γ=Σ ∪{¢, ␣}∪ (Σ ×{A, B}) und δ(q0, ¢)=(qeven, ¢, R), δ(q0,a)=(qreject,a, N) für alle a ∈{0, 1, ␣}, δ(qeven,b)=(qodd,b, R) für alle b ∈{0, 1}, δ(qeven, ␣)=(qreject, ␣, N), δ(qodd,b)=(qeven,b, R) für alle b ∈{0, 1}, δ(qodd, ␣)=(qB, ␣, L). Wir beobachten, dass nach dem Lesen eines Präﬁxes gerader (ungerader) Länge M im Zustand qeven (qodd) ist. Wenn also M das Symbol ␣ im Zustand qeven liest, ist das Eingabewort gerader Länge und muss verworfen werden. Wenn M das Symbol ␣ im Zustand qodd liest, geht M in den Zustand qB über, in dem die zweite Phase der Berechnung anfängt. In dieser Phase bestimmt M die Mitte des Eingabewortes, indem M abwechselnd das am weitesten rechts stehende Symbol a ∈{0, 1} in ( a B) umwandelt und das am weitesten links stehende Symbol b ∈{0, 1} durch ( b A ) ersetzt. Dieses kann 100 4 Turingmaschinen qp a → b, X Abbildung 4.4 man mit folgenden Transitionen (Übergängen) realisieren: δ(qB,a)=(q1, ( a B), L) für alle a ∈{0, 1}, δ(q1,a)=(qleft,a, L) für alle a ∈{0, 1}, δ(q1,c)=(qmiddle,c,R) für alle c ∈{¢, ( 0 A), ( 1 A)}, δ(qmiddle, ( 0 B))=(qreject, 0, N), δ(qmiddle, ( 1 B))=(qaccept, 1, N), δ(qleft,a)=(qleft,a, L) für alle a ∈{0, 1}, δ(qleft,c)=(qA,c, R) für alle c ∈{ ( 0 A), ( 1 A), ¢}, δ(qA,b)=(qright, ( b A ), R) für alle b ∈{0, 1}, δ(qright,b)=(qright,b, R) für alle b ∈{0, 1}, δ(qright,d)=(qB,d, L) für alle d ∈{ ( 0 B), ( 1 B)}. Die fehlenden Argumentpaare wie zum Beispiel (qright, ¢) können nicht auftreten, und deswegen kann man die Deﬁnitionen von δ so vervollständigen, dass man für alle fehlenden Argumente den Übergang nach qreject hinzunimmt. Wenn man einen Befehl δ(q, a)=(p, b, X) für q, p ∈ Q, a, b ∈ Σ und X ∈{L, R, N} graphisch wie in Abbildung 4.4 darstellt, dann kann man die konstruierte TM M wie in Abbildung 4.5 darstellen. Diese graphische Darstellung ist ähnlich zu der bei endlichen Automaten. Der Unterschied liegt nur in der Kantenbeschriftung, bei der zusätzlich zu dem gelesenen Symbol a noch das neue Symbol b und die Bewegungsrichtung X notiert wird. Betrachten wir jetzt die Arbeit von M auf dem Eingabewort x = 1001101. Die erste Phase der Berechnung von M auf x läuft wie folgt: q0¢1001101 M ¢qeven1001101 M ¢1qodd001101 M ¢10qeven01101 M ¢100qodd1101 M ¢1001qeven101 M ¢10011qodd01 M ¢100110qeven1 M ¢1001101qodd␣ M ¢100110qB1 Das Erreichen des Zustandes qB bedeutet, dass x eine ungerade Länge hat. Jetzt wird M abwechselnd die Symbole a am rechten Rand durch ( a B) und am linken Rand durch( a A) ersetzen. ¢100110qB1 M ¢10011q10( 1 B) M ¢1001qleft10( 1 B) M ¢100qleft110( 1 B) M ¢10qleft0110( 1 B) 4.3 Das Modell der Turingmaschine 101 q0 q1 qA qB qreject qaccept qeven qrightqleft qmiddle qodd 1 → 1, R 1 → 1, R 1 → 1, R 0 → 0, R 0 → 0, R 0 → 0, R ¢ → ¢, R ¢ → ¢, R ¢ → ¢, R ␣ → ␣, L 0 → 0, L 0 → 0, L 1 → 1, L 1 → 1, L ␣ → ␣, N ␣ → ␣, N 0 → 0, N 1 → 1, N 0 → ( 0 B), L 1 → ( 1 B), L ( 0 A) → ( 0 A), R ( 0 A) → ( 0 A), R ( 1 A) → ( 1 A), R ( 1 A) → ( 1 A), R ( 1 B) → 1, N 0 → ( 0 A), R 1 → ( 1 A), R ( 0 B) → ( 0 B), L ( 1 B) → ( 1 B), L ( 0 B) → 0, N Abbildung 4.5 M ¢1qleft00110( 1 B) M ¢qleft100110( 1 B) M qleft¢100110( 1 B) M ¢qA100110( 1 B) M ¢( 1 A)qright00110( 1 B) M ¢( 1 A)0qright0110( 1 B) M ∗ ¢( 1 A)00110qright( 1 B) M ¢( 1 A)0011qB0( 1 B) M ¢( 1 A)001q11( 0 B)( 1 B) M ¢( 1 A)00qleft11( 0 B)( 1 B) M ∗ ¢qleft( 1 A)0011( 0 B)( 1 B) M ¢( 1 A)qA0011( 0 B)( 1 B) M ¢( 1 A)( 0 A)qright011( 0 B)( 1 B) M ∗ ¢( 1 A)( 0 A)011qright( 0 B)( 1 B) M ¢( 1 A)( 0 A)01qB1( 0 B)( 1 B) M ¢( 1 A)( 0 A)0q11( 1 B)( 0 B)( 1 B) 102 4 Turingmaschinen M ∗ ¢( 1 A)qleft( 0 A)01( 1 B)( 0 B)( 1 B) M ¢( 1 A)( 0 A)qA01( 1 B)( 0 B)( 1 B) M ¢( 1 A)( 0 A)( 0 A)qright1( 1 B)( 0 B)( 1 B) M ¢( 1 A)( 0 A)( 0 A)1qright( 1 B)( 0 B)( 1 B) M ¢( 1 A)( 0 A)( 0 A)qB1( 1 B)( 0 B)( 1 B) M ¢( 1 A)( 0 A)q1( 0 A)( 1 B)( 1 B)( 0 B)( 1 B) M ¢( 1 A)( 0 A)( 0 A)qmiddle( 1 B)( 1 B)( 0 B)( 1 B) M ¢( 1 A)( 0 A)( 0 A)qaccept1( 1 B)( 0 B)( 1 B). Aufgabe 4.3. Schreiben Sie die Berechnung der TM M aus Abbildung 4.5 auf den Wörtern 010011 und 101 auf. Aufgabe 4.4. Die TM M aus Abbildung 4.5 arbeitet so, dass sie die ganze Zeit die Information über die ursprüngliche Eingabe behält. Nutzen Sie diese Eigenschaft aus, um M zu einer TM M ′ zu erweitern, so dass L(M ′)= {w ∈{0, 1} ∗ | w = x1x für ein x ∈{0, 1} ∗}. Betrachten wir jetzt die Sprache LP = {02n | n ∈ N −{0}}⊆{0} ∗. Eine TM, die LP akzeptiert, kann folgende Strategie verfolgen: 1. Laufe über das Band von ¢ bis zum ersten ␣ (von links nach rechts) und „lösche“ jede zweite 0, d. h., ersetze sie durch a. Falls die Anzahl Nullen auf dem Band ungerade ist, halte im Zustand qreject. Sonst, fahre fort mit Schritt 2. 2. Laufe über das Band von dem am weitesten links stehenden ␣ bis zum ¢ und überprüfe, ob auf dem Band genau eine Null oder mehrere Nullen stehen. • Falls auf dem Band genau eine Null steht, akzeptiere. • Falls auf dem Band mindestens zwei Nullen stehen, wiederhole Schritt 1. Die Idee dieser Strategie ist, dass man eine Zahl 2i mit i ≥ 1 solange ohne Rest durch 2 teilen kann, bis man 1 erhält. Eine mögliche Realisierung dieser Strategie in Form einer TM ist A =({q0,qeven,qodd,q1,q2,q3,qaccept,qreject}, {0}, {0,a, ¢, ␣},δA,q0,qaccept,qreject) mit der graphischen Darstellung in Abbildung 4.6. Aufgabe 4.5. Eine andere Strategie, die Sprache LP zu erkennen, ist, die Eingabe 0 i im ersten Lauf durch das Band in 0 j1j umzuwandeln, falls i =2j gerade ist. Im nächsten Lauf überprüft man, ob j gerade ist, indem man im positiven Fall 0j1j durch 0 j 2 1 j 2 1j ersetzt. Akzeptiert wird nur, wenn durch diese Art des Halbierens auf dem Band das Wort 01i−1 erzeugt wurde. Dieses Halbieren könnte man mit der Strategie zur Suche der Mitte einer Eingabe realisieren. Konstruieren Sie eine TM, die als eine Implementierung des beschriebenen Verfahrens gesehen werden kann. 4.4 Mehrband-Turingmaschinen und Church’sche These 103 qreject qaccept q0 q1 q2 q3 qeven qodd ¢ → ¢, N ¢ → ¢, N 0 → a, R ␣ → ␣, L ␣ → ␣, L 0 → 0, R 0 → 0, L 0 → 0, L 0 → 0, L a → a, Ra → a, R a → a, L a → a, L a → a, L ¢ → ¢, R ¢ → ¢, R Abbildung 4.6 Aufgabe 4.6. Entwerfen Sie Turingmaschinen für folgende Sprachen: (a) {anbn | n ∈ N}, (b) {x#y | x, y ∈{0, 1}∗, Nummer(x) = Nummer(y)+1}, (c) {w#w | w ∈{0, 1} ∗}, (d) ∗ {0 n 2 | n ∈ N}. Aufgabe 4.7. Entwerfen Sie Turingmaschinen, die für jedes Wort x ∈ (Σbool)∗ im Zustand qaccept mit folgendem Bandinhalt halten: (a) y ∈ (Σbool) ∗, so dass Nummer(y) = Nummer(x)+1, (b) x#x, (c) z ∈ (Σbool) ∗, so dass Nummer(z)=2 · Nummer(x), (d) ###x. 4.4 Mehrband-Turingmaschinen und Church’sche These Die Turingmaschinen sind das Standardmodell der Theorie der Berechenbarkeit, wenn es um die Klassiﬁzierung der Entscheidungsprobleme in rekursive (rekursiv aufzählbare) und nicht rekursive (nicht rekursiv aufzählbare) geht. Dieses Modell ist aber nicht so gut für die 104 4 Turingmaschinen ¢ ¢ ¢ ¢ $w ␣␣ ␣␣ ␣␣ ··· ··· ··· ... Eingabeband 1. Arbeitsband 2. Arbeitsband k-tes Arbeitsband Lesekopf Lese-/Schreibköpfe Programm Abbildung 4.7 Komplexitätstheorie geeignet. Der Hauptpunkt der Kritik ist, dass dieses Modell nicht dem des allgemein akzeptierten Modells des Von-Neumann-Rechners entspricht. Dieses fordert, dass alle Komponenten des Rechners – Speicher für Programm und Daten, CPU und das Eingabemedium – physikalisch unabhängige Teile sind. In dem Modell der Turingmaschine sind das Eingabemedium und der Speicher eins – das Band. Der zweite Kritikpunkt ist die zu starke Vereinfachung in Bezug auf die benutzten Operationen und die Linearität des Speichers. Wenn man nämlich zwei Feldinhalte, die weit voneinander entfernt liegen, vergleichen möchte, braucht man mindestens so viele Operationen (Berechnungsschritte) wie die zwei Felder voneinander entfernt liegen. Das folgende Modell der Mehrband-Turingmaschine ist das grundlegende Modell der Komplexitätstheorie. Für jede positive ganze Zahl k hat eine k-Band-Turingmaschine folgende Komponenten (Abbildung 4.7): • eine endliche Kontrolle (Programm), • ein endliches Band mit einem Lesekopf, • k Arbeitsbänder, jedes mit eigenem Lese-/Schreibkopf. Am Anfang jeder Berechnung auf einem Wort w ist die k-Band-Turingmaschine in folgender Situation. • Das Eingabeband enthält ¢w$, wobei ¢ und $ die linke bzw. die rechte Seite der Eingabe markieren. • Der Lesekopf des Eingabebandes zeigt auf ¢. 4.4 Mehrband-Turingmaschinen und Church’sche These 105 • Der Inhalt aller Arbeitsbänder ist ¢␣␣␣ ... und deren Lese-/Schreibköpfe zeigen auf ¢. • Die endliche Kontrolle ist im Anfangszustand q0. Während der Berechnung dürfen sich alle k + 1 Köpfe in beide Richtungen links und rechts bewegen, nur darf sich kein Kopf über das Randsymbol ¢ nach links bewegen und der Lesekopf des Eingabebandes darf sich nicht über das rechte Randsymbol $ nach rechts bewegen. Der Lesekopf darf nicht schreiben, und deswegen bleibt der Inhalt ¢w$ des Eingabebandes während der ganzen Berechnung gleich. Für die Feldinhalte der Arbeitsbänder betrachtet man wie bei einer TM ein Arbeitsalphabet Γ. Die Felder aller k + 1 Bänder kann man von links nach rechts nummerieren, beginnend mit 0 bei ¢.So kann man eine Konﬁguration einer k-Band-TM M wie folgt darstellen. Eine Konﬁguration (q, w, i, u1,i1,u2,i2,...,uk,ik) ist ein Element aus Q × Σ ∗ × N × (Γ ∗ × N)k mit folgender Bedeutung: • M ist im Zustand q, • der Inhalt des Eingabebandes ist ¢w$ und der Lesekopf des Eingabebandes zeigt auf das i-te Feld des Eingabebandes (d. h., falls w = a1a2 ...an, dann liest der Lesekopf das Symbol ai), • für j ∈{1, 2,...,k} ist der Inhalt des j-ten Bandes ¢uj␣␣␣ ... , und ij ≤|uj| ist die Position des Feldes, auf das der Kopf des j-ten Bandes zeigt. Die Berechnungsschritte von M können mit einer Transitionsfunktion δ : Q × (Σ ∪{¢, $}) × Γk → Q ×{L, R, N}× (Γ ×{L, R, N}) k beschrieben werden. Die Argumente (q, a, b1,...,bk) ∈ Q × (Σ ∪{¢, $}) × Γk sind der aktuelle Zustand q, das gelesene Symbol a ∈ Σ ∪{¢, $} auf dem Eingabeband und die k Symbole b1,...,bk ∈ Γ, auf denen die Köpfe der Arbeitsbänder stehen. Diese k Symbole werden von den Köpfen der k Arbeitsbänder durch andere Symbole ersetzt, und die Position aller k + 1 Köpfe wird um maximal 1 geändert. Die Eingabe w wird von M akzeptiert, falls M in der Berechnung auf w den Sonderzustand qaccept erreicht. Die Eingabe w wird nicht akzeptiert, falls M die Eingabe w im Zustand qreject verwirft oder die Berechnung von M auf w nicht terminiert (unendlich ist). Wir verzichten auf die formale Deﬁnition der k-Band-Turingmaschine. Der oben gege- benen Beschreibung folgend ist die Erstellung einer solchen Deﬁnition eine Routinearbeit und wir überlassen dies dem Leser zum Training. Aufgabe 4.8. Geben Sie eine exakte formale Deﬁnition einer k-Band-Turingmaschine. Folgen Sie dabei allen Schritten der Deﬁnition von Turingmaschinen. 106 4 Turingmaschinen ¢ ¢¢ ¢ $$x x x ## yy ␣ ······ Abbildung 4.8 qp a, b → X, d, Y Abbildung 4.9 Für jedes k ∈ N −{0} nennen wir eine k-Band-Turingmaschine (k-Band-TM) auch Mehrband-Turingmaschine (MTM). Weil die Operationen einer MTM ein bisschen komplexer als die elementaren Operationen einer TM sind, kann man erwarten, dass Mehrband-Turingmaschinen gewisse Probleme einfacher oder schneller als Turingmaschi- nen lösen können. Betrachten wir die Sprache Lgleich = {w#w | w ∈ (Σbool) ∗}. Eine TM, die das erste w mit dem zweiten w vergleicht, muss mit dem Kopf viele Male über lange Entfernungen auf dem Band hin und her laufen. Eine 1-Band-TM A kann Lgleich mit folgender Strategie einfach erkennen: 1. A überprüft, ob die Eingabe die Form x#y mit x, y ∈ (Σbool)∗ hat.7 Falls nicht, verwirft A die Eingabe. 2. Für die Eingabe x#y kopiert A das Wort x auf das Arbeitsband (d. h., nach diesem Schritt enthält das Arbeitsband ¢x (Abbildung 4.8)). 3. A positioniert den Kopf des Arbeitsbandes auf ¢. Dann bewegt A simultan beide Köpfe nach rechts und vergleicht x und y. Falls x ̸= y, liest A in einem der Schritte zwei unterschiedliche Symbole. In diesem Fall verwirft A die Eingabe. Falls alle Paare von Symbolen gleich sind und beide Köpfe gleichzeitig ␣ erreichen, akzeptiert A die Eingabe. Eine Transition δ(p, a, b)=(q, X, d, Y ) einer 1-Band-TM kann man graphisch wie in Abbildung 4.9 darstellen. Der Übergang von p zu q ﬁndet beim Lesen von a auf dem Eingabeband und gleichzeitigem Lesen von b auf dem Arbeitsband statt. Das Symbol b wird durch d ersetzt, X ∈{L, R, N} bestimmt die Bewegung des Lesekopfes auf dem Eingabeband und Y ∈{L, R, N} bestimmt die Bewegung des Kopfes auf dem Arbeitsband. 7Dies bedeutet, dass A wie ein EA einfach veriﬁzieren kann, ob die Eingabe genau ein # hat. 4.4 Mehrband-Turingmaschinen und Church’sche These 107 Entsprechend dieser graphischen Darstellung (Abbildung 4.9) gibt Abbildung 4.10 die Beschreibung einer 1-Band-TM M , die eine Implementierung der oben beschriebenen Strategie zum Erkennen von Lgleich ist. Die Zustände q0,q1,q2 und qreject benutzt man zur Realisierung des ersten Schrittes der Strategie. Falls die Eingabe genau ein # enthält, erreicht M den Zustand q2 mit dem Lesekopf auf dem letzten Symbol der Eingabe. Sonst endet M in qreject. Der Zustand q2 wird benutzt, um den Lesekopf zurück auf das linke Randsymbol ¢ zu bringen. Im Zustand qcopy kopiert M das Präﬁx der Eingabe bis zum # auf das Arbeitsband (Abbildung 4.8) und im Zustand qadjust kehrt der Kopf des Arbeitsbandes zurück auf ¢. Der Vergleich von x und y des Eingabewortes x#y ﬁndet im Zustand qcompare statt. Falls x = y, endet M im Zustand qaccept. Falls x und y sich im Inhalt auf irgendeiner Position unterscheiden oder unterschiedliche Längen haben, endet M im Zustand qreject. Aufgabe 4.9. Beschreiben Sie informell und auch in der Form eines Diagramms 1-Band- Turingmaschinen, die folgende Sprachen akzeptieren: (a) L = {a nbn | n ∈ N}, (b) L = {w ∈ (Σbool) ∗ ||w|0 = |w|1}, (c) L = {a nbnc n | n ∈ N}, (d) L = {www | w ∈ (Σbool) ∗}, (e) L = {an 2 | n ∈ N}. Wir haben jetzt zwei unterschiedliche Modelle – die TM und die MTM. Um beide gleichzeitig benutzen zu dürfen, müssen wir deren Äquivalenz bezüglich der Klasse der akzeptierten Sprachen beweisen. Seien A und B zwei Maschinen (TM, MTM), die mit dem gleichen Eingabealphabet Σ arbeiten. Im Folgenden sagen wir, dass eine Maschine A äquivalent zu einer Maschine B ist, falls für jede Eingabe x ∈ (Σbool) ∗ gilt: (i) A akzeptiert x ⇐⇒ B akzeptiert x, (ii) A verwirft x ⇐⇒ B verwirft x, (iii) A arbeitet unendlich lange auf x ⇐⇒ B arbeitet unendlich lange auf x. Es ist klar, dass L(A)= L(B), wenn A und B äquivalent sind. Die Tatsache L(A)= L(B) garantiert aber nicht, dass A und B äquivalent sind, denn es kann beispielsweise sein, dass A auf einem Wort w/∈ L(A)= L(B) unendlich lange läuft, während B dieses verwirft. Lemma 4.1. Zu jeder TM A existiert eine zu A äquivalente 1-Band-TM B. Beweis. Wir beschreiben die Simulation von A durch B, ohne die formale Konstruktion von B aus A anzugeben. B arbeitet in zwei Phasen: 1. B kopiert die ganze Eingabe w auf das Arbeitsband. 2. B simuliert Schritt für Schritt die Arbeit von A auf dem Arbeitsband. (Dies bedeutet, dass B auf dem unendlichen Arbeitsband genau das macht, was A auf seinem unendlichen Eingabeband gemacht hätte.) Es ist klar, dass A und B äquivalent sind. \u0002 108 4 Turingmaschinen qreject qacceptqcompareqadjust qcopy q0 q1 q2 1, ¢ → R, ¢, N 1, ¢ → R, ¢, N 0, ¢ → R, ¢, N 0, ¢ → R, ¢, N ¢, ¢ → R, ¢, N $, ¢ → N, ¢, N #, ¢ → R, ¢, N #, ¢ → N, ¢, N $, ¢ → L, ¢, N 1, ␣ → R, 1, R 0, ␣ → R, 0, R ¢, ¢ → R, ¢, R #, ␣ → N, ␣, L #, 0 → N, 0, L #, 1 → N, 1, L 0, 0 → R, 0, R 1, 1 → R, 1, R #, ¢ → R, ¢, R $, ␣ → N, ␣, N 1, 0 → N, 0, N 0, 1 → N, 1, N $, 0 → N, 0, N $, 1 → N, 1, N 1, ¢ → L, ¢, N 0, ¢ → L, ¢, N #, ¢ → L, ¢, N 0, ␣ → N, ␣, N 1, ␣ → N, ␣, N Abbildung 4.10 4.4 Mehrband-Turingmaschinen und Church’sche These 109 wwA: B: ¢ ¢ ¢¢ ¢ ¢ ¢ ¢ ¢ $$ x1 x1 x2 x2 xk xk ... ... ↑ ↑ ↑ ↑ ... ␣␣ ␣ ␣␣ ␣ ␣ ␣ ␣ ␣ ␣␣ ␣ ␣␣ ␣ ␣ ␣ ␣␣ ␣ Abbildung 4.11 Aufgabe 4.10. Geben Sie eine formale Konstruktion einer 1-Band-TM B an, die äquivalent zu TM A =(Q, Σ, Γ,δ,q0,qaccept,qreject) ist. Im Folgenden werden wir meistens auf formale Konstruktionen von Turingmaschinen und formale Beweise der Äquivalenzen (oder von L(A)= L(B)) verzichten. Der Grund ist ähnlich wie bei der Beschreibung von Algorithmen oder bei formalen Beweisen der Korrektheit von Programmen. Wir ersparen uns viel Kleinarbeit, wenn uns intuitiv klar ist, dass ein Programm (eine TM) die gewünschte Tätigkeit realisiert, und wir auf einen formalen Beweis verzichten. Lemma 4.2. Für jede Mehrband-TM A existiert eine zu A äquivalente TM B. Beweis. Sei A eine k-Band-Turingmaschine für ein k ∈ N −{0}. Wir zeigen, wie sich eine TM B konstruieren lässt, die Schritt für Schritt A simulieren kann. Eine gute Strategie, um so eine Simulation zu erklären, ist, zuerst die Darstellung von Konﬁgurationen der zu simulierenden Maschine A festzulegen, und dann erst die Simulation der einzelnen Schritte zu erklären. Die Idee der Darstellung der aktuellen Konﬁguration von A und B ist in Abbildung 4.11 anschaulich beschrieben. B speichert die Inhalte aller k + 1 Bänder von A auf ihrem einzigen Band. Anschaulich gesehen zerlegt B ihr Band in 2(k + 1) Spuren. Dies kann man wie folgt erreichen. Falls ΓA das Arbeitsalphabet von A ist, so ist ΓB =(ΣA ∪{¢, $, ␣}) ×{␣, ↑} × (ΓA ×{␣, ↑}) k ∪ ΣA ∪{␣, ¢} das Arbeitsalphabet von B. Für ein Symbol α =(a0,a1,a2,...,a2k+1) ∈ ΓB sagen wir, dass ai auf der i-ten Spur liegt. Daher bestimmen die i-ten Elemente der Symbole auf dem 110 4 Turingmaschinen Band von B den Inhalt der i-ten Spur. Eine Konﬁguration (q, w, i, x1,i1,x2,i2,...,xk,ik) von A ist dann in B wie folgt gespeichert. Der Zustand q ist in der endlichen Kontrolle von B gespeichert. Die 0-te Spur des Bandes von B enthält ¢w$, d. h. den Inhalt des Eingabebandes von A. Für alle i ∈{1,...,k} enthält die (2i)-te Spur des Bandes von B das Wort ¢xi, d. h. den Inhalt des i-ten Arbeitsbandes von A. Für alle i ∈{1,...,k} bestimmt die (2i + 1)-te Spur mit dem Symbol ↑ die Position des Kopfes auf dem i-ten Arbeitsband von A. Ein Schritt von A kann jetzt durch folgende Prozedur von B simuliert werden: 1. B liest einmal den Inhalt ihres Bandes von links nach rechts, bis sie alle k +1 Kopfpositionen von A gefunden hat, und speichert dabei in ihrem Zustand die k +1 Symbole,8 die sie bei den k + 1 Köpfen von A gelesen hat. (Das sind genau die Symbole der geraden Spuren, auf die die Symbole ↑ auf den ungeraden Spuren zeigen.) 2. Nach der ersten Phase kennt B das ganze Argument (der Zustand von A ist auch in dem Zustand von B gespeichert) der Transitionsfunktion von A und kann also genau die entsprechenden Aktionen (Köpfe bewegen, Ersetzen von Symbolen) von A bestimmen. Diese Änderungen führt B in einem Lauf über ihr Band von rechts nach links durch. \u0002 Aufgabe 4.11. Geben Sie eine formale Konstruktion einer TM an, die die Phase 1 der Simulation eines Schrittes von A aus Lemma 4.2 realisiert. Aufgabe 4.12. Zeigen Sie, dass für jede TM A eine zu A äquivalente TM B existiert, die in jedem Schritt ihren Kopf bewegt. Deﬁnition 4.2. Zwei Maschinenmodelle (Maschinenklassen) A und B für Entschei- dungsprobleme sind äquivalent, falls (i) für jede Maschine A ∈A eine zu A äquivalente Maschine B ∈B existiert, und (ii) für jede Maschine C ∈B eine zu C äquivalente Maschine D ∈A existiert. Aufgabe 4.13. Geben Sie eine formale Deﬁnition der Äquivalenz von zwei Maschinen an, die Funktionen von Σ ∗ nach Γ ∗ berechnen. Formulieren Sie dazu die Deﬁnition der Äquivalenz von zwei Maschinenklassen zur Berechnung von Funktionen. Lemmata 4.1 und 4.2 implizieren direkt das folgende Resultat. Satz 4.1. Die Maschinenmodelle von Turingmaschinen und Mehrband-Turingmaschinen sind äquivalent. Die Kenntnis, dass diese beiden Maschinenmodelle als gleichberechtigt zur Algorith- menmodellierung betrachtet werden dürfen, erleichtert unsere Arbeit. Wenn wir beweisen 8Dies bedeutet, dass die Zustandsmenge von B die Menge Q × (Σ ∪{¢, $}) × Γk enthält, was kein Problem ist, da diese Menge endlich ist. 4.4 Mehrband-Turingmaschinen und Church’sche These 111 wollen, dass eine Sprache rekursiv oder rekursiv aufzählbar ist, reicht es, eine Mehrband- TM für diese Sprache zu konstruieren (was meistens einfacher ist, als eine TM zu konstru- ieren). Wenn wir aber zeigen wollen, dass eine Sprache nicht rekursiv oder nicht rekursiv aufzählbar ist, werden wir mit der Nichtexistenz einer entsprechenden TM argumentieren. Die Situation ist vergleichbar damit, dass man eine höhere Programmiersprache zum Beweis algorithmischer Lösbarkeit eines gegebenen Problems benutzt und Assembler oder Maschinencode zum Beweis algorithmischer Unlösbarkeit benutzt. Dies ist genau das, was im nächsten Kapitel behandelt wird. Deswegen könnte es für uns auch hilfreich sein, die Äquivalenz zwischen Turingmaschinen und einer höheren Programmiersprache zu beweisen. Ein formaler Beweis erfordert eine große Menge an technischer Kleinarbeit, die sehr zeitaufwendig ist. Deshalb erklären wir nur die Idee, wie man eine solche Äquivalenz zeigen kann. Dass man für jede TM ein äquivalentes Programm schreiben kann, glaubt hoﬀentlich jeder in der Programmierung ein bisschen erfahrene Leser. Man kann sogar noch etwas Besseres – einen Interpreter für Turingmaschinen schreiben. Ein Interpreter CTM für Turingmaschinen bekommt eine Beschreibung einer TM M in einem festgelegten Forma- lismus und ein Eingabewort über dem Eingabealphabet von M . Danach simuliert CTM die Arbeit von M auf w. Wie kann man jetzt zu einem Programm einer komplexeren Programmiersprache eine TM bauen? Dazu sollte man sich den Weg der Entwicklung der Programmiersprachen anschauen. Am Anfang hat man nur in Assembler oder sogar noch in Maschinencode programmiert. Die einzigen erlaubten Operationen waren Vergleiche von zwei Zahlen und die arithmetischen Operationen. Alle anderen Befehle wurden als kleine Programme aus diesem Grundrepertoire zusammengesetzt, um dem Programmierer die Arbeit zu vereinfachen. Deswegen werden wir keine Zweifel an der Äquivalenz von Assembler und beliebigen Programmiersprachen haben; insbesondere weil wir wissen, dass die Compiler Programme in höheren Programmiersprachen in Assembler oder Maschinencode übersetzen. Daher reicht es aus, die Äquivalenz zwischen Assembler und Turingmaschinen zu beweisen. Den Assembler kann man durch sogenannte Registermaschinen modellieren und dann diese durch Turingmaschinen simulieren lassen. Diesen Weg werden wir jetzt aber nicht gehen. Wir können unsere Aufgabe noch vereinfachen. Wir können Multiplikation und Division von zwei Zahlen a und b durch Programme realisieren, die nur mit den Grundoperationen Addition und Subtraktion arbeiten. Aufgabe 4.14. Schreiben Sie ein Programm, das für zwei gegebene Zahlen a und b das Produkt a · b berechnet. Dabei darf das Programm nur die Addition, die Subtraktion und Vergleich von zwei Zahlen in Verbindung mit if ... then ... else benutzen. Danach kann man einen Vergleich von zwei Zahlen durch Programme durchführen, die nur die Operation +1 (I := I +1), −1(I := I − 1) und den Test auf 0 (if I =0 then ... else ...) benutzen. Aufgabe 4.15. Schreiben Sie ein Programm, das den Befehl if I ≥ J then goto 1 else goto 2 nur mit Hilfe der Operationen +1, −1 und dem Test auf 0 realisiert. Am Ende können wir auch noch auf Addition und Subtraktion verzichten. 112 4 Turingmaschinen Aufgabe 4.16. Schreiben Sie Programme, die für zwei gegebene Zahlen a, b ∈ N die Addition a + b und die Subtraktion a − b nur mit Hilfe der Operationen +1, −1 und dem Test auf 0 berechnen. Die Aufgabe, die Programme mit Operationen +1, −1 und if I =0 then ... else ... auf Mehrband-Turingmaschinen zu simulieren, ist nicht mehr so schwer. Die Variablen werden auf den Arbeitsbändern in der Form x#y gespeichert und durch ## voneinander getrennt, wobei x die binäre Kodierung des Namens der Variablen Ix und y die binäre Kodierung des Wertes von Ix ist. Die Operationen +1, −1 und den Test y = 0 kann eine MTM einfach realisieren. Der einzige größere Aufwand entsteht, wenn ein Band zum Beispiel ¢x#y##z#u## ... enthält und der Platz zur Speicherung des Wertes von Ix in y nicht mehr reicht (wenn mehr Felder für y gebraucht werden). In einem solchen Fall muss die Mehrband-TM den Inhalt ##z#u## ... rechts von y nach rechts verschieben, um mehr Platz für die Speicherung von y zu gewinnen. In der Theoretischen Informatik hat man Hunderte formaler Modelle (nicht nur in Form von Maschinenmodellen) zur Speziﬁkation der algorithmischen Lösbarkeit entwickelt. Alle vernünftigen Modelle sind äquivalent zu Turingmaschinen. Dies führte zu der Formulierung der sogenannten Church’schen These: Church’sche These Die Turingmaschinen sind die Formalisierung des Begriﬀes „Algorithmus“, d. h., die Klasse der rekursiven Sprachen (der entscheidbaren Entscheidungs- probleme) stimmt mit der Klasse der algorithmisch (automatisch) erkennbaren Sprachen überein. Die Church’sche These ist nicht beweisbar, weil sie einer Formalisierung des intuitiven Begriﬀes Algorithmus entspricht. Es ist nicht möglich zu beweisen, dass keine andere formale Modellierung des intuitiven Begriﬀes Algorithmus existiert, die (i) unserer Intuition über diesen Begriﬀ entspricht und (ii) die algorithmische Lösung von Entscheidungsproblemen ermöglicht, die man mit der Hilfe von Turingmaschinen nicht entscheiden kann. Das einzige, was passieren könnte, ist, dass jemand ein solches stärkeres Modell ﬁndet. In diesem Fall wären die Grundlagen der Theoretischen Informatik zu revidieren. Die Suche nach einem solchen Modell war aber bisher vergeblich, und wir wissen heute, dass sogar das physikalische Modell des Quantenrechners 9 zu Turingmaschinen äquivalent ist. Die Situation ist also ähnlich wie in der Mathematik und der Physik. Wir akzeptieren die Church’sche These, weil sie unserer Erfahrung entspricht, und postulieren sie als ein Axiom. Wie wir schon bemerkt haben, hat sie die Eigenschaften mathematischer Axiome – sie kann nicht bewiesen werden, aber man kann nicht ausschließen, dass sie eines Tages widerlegt wird. 10 Die Church’sche These ist das einzige informatikspeziﬁsche Axiom, auf 9Quantenrechner arbeiten nach dem Prinzip der Quantenmechanik. 10Die Widerlegung eines Axioms oder einer These sollte man nicht als „Katastrophe“ betrachten. Solche Resultate gehören zur Entwicklung der Wissenschaften dazu. Die bisherigen Resultate und Kenntnisse muss man deswegen nicht verwerfen, nur relativieren. Sie gelten einfach weiter unter der Voraussetzung, dass das Axiom gilt. 4.5 Nichtdeterministische Turingmaschinen 113 welchem die Theoretische Informatik aufgebaut wird. Alle anderen benutzten Axiome sind die Axiome der Mathematik. 4.5 Nichtdeterministische Turingmaschinen Den Nichtdeterminismus kann man in das Modell der Turingmaschinen auf gleichem Wege einführen wie wir den Nichtdeterminismus bei den endlichen Automaten eingeführt haben. Für jedes Argument besteht die Möglichkeit einer Auswahl aus endlich vielen Aktionen. Auf der Ebene der Transitionsregeln bedeutet dies, dass die Transitionsfunktion δ nicht von Q × Γ nach Q × Γ ×{L, R, N} geht, sondern von Q × Γ nach P(Q × Γ ×{L, R, N}). Eine andere formale Möglichkeit ist, δ als Relation auf (Q × Γ) × (Q × Γ ×{L, R, N})zu betrachten. Eine nichtdeterministische Turingmaschine M akzeptiert ein Eingabewort w genau dann, wenn es mindestens eine akzeptierende Berechnung von M auf w gibt. Die formale Deﬁnition einer nichtdeterministischen Turingmaschine folgt. Deﬁnition 4.3. Eine nichtdeterministische Turingmaschine (NTM ) ist ein 7- Tupel M =(Q, Σ, Γ,δ,q0,qaccept,qreject), wobei (i) Q, Σ, Γ,q0,qaccept,qreject die gleiche Bedeutung wie bei einer TM haben, und (ii) δ : (Q −{qaccept,qreject}) × Γ →P(Q × Γ ×{L, R, N}) die Übergangsfunktion von M ist und die folgende Eigenschaft hat: δ(p, ¢) ⊆{(q, ¢,X) | q ∈ Q, X ∈{R, N}} für alle p ∈ Q. {Das Randsymbol darf nicht durch ein anderes Symbol ersetzt werden, und der Kopf darf sich nicht von ¢ aus nach links bewegen.} Eine Konﬁguration von M ist ein Element aus Konf (M ) =({¢}· Γ∗ · Q · Γ ∗) ∪ (Q ·{¢}· Γ ∗). {mit der gleichen Bedeutung wie bei einer TM} Die Konﬁguration q0¢w ist die Anfangskonﬁguration für das Wort w ∈ Σ ∗. Eine Konﬁ- guration heißt akzeptierend, falls sie den Zustand qaccept enthält. Eine Konﬁguration heißt verwerfend, falls sie den Zustand qreject enthält. Ein Schritt von M ist eine Relation M , die auf der Menge der Konﬁgurationen ( M ⊆ Konf(M )×Konf(M )) wie folgt deﬁniert ist. Für alle p, q ∈ Q und alle x1,x2,...,xn,y ∈ Γ gilt • x1x2 ...xi−1qxixi+1 ...xn M x1x2 ...xi−1pyxi+1 ...xn, falls (p, y, N) ∈ δ(q, xi), • x1x2 ...xi−2,xi−1qxixi+1 ...xn M x1x2 ...xi−2pxi−1yxi+1 ...xn, falls (p, y, L) ∈ δ(q, xi), • x1x2 ...xi−1qxixi+1 ...xn M x1x2 ...xi−1ypxi+1 ...xn, falls (p, y, R) ∈ δ(q, xi) für i<n und 114 4 Turingmaschinen • x1x2 ...xn−1qxn M x1x2 ...xn−1yp␣, falls (p, y, R) ∈ δ(q, xn). Die Relation M ∗ ist die reﬂexive und transitive Hülle von M . Eine Berechnung von M ist eine Folge von Konﬁgurationen C0,C1,... , so dass Ci M Ci+1 für i =0, 1, 2,... . Eine Berechnung von M auf einer Eingabe x ist eine Berechnung, die mit der Anfangskonﬁguration q0¢x beginnt und die entweder unendlich ist oder in einer Konﬁguration w1qw2 endet, wobei q ∈{qaccept,qreject}. Eine Berechnung von M auf x heißt akzeptierend, falls sie in einer akzeptierenden Konﬁguration endet. Eine Berechnung von M auf x heißt verwerfend, falls sie in einer verwerfenden Konﬁguration endet. Die von der NTM M akzeptierte Sprache ist L(M ) = {w ∈ Σ ∗ | q0¢w M ∗ yqacceptz für irgendwelche y, z ∈ Γ∗}. Aufgabe 4.17. Beschreiben und deﬁnieren Sie formal eine nichtdeterministische k-Band-Tu- ringmaschine für jedes k ∈ N −{0}. Aufgabe 4.18. Sei M eine nichtdeterministische Mehrband-Turingmaschine. Beschreiben Sie eine NTM M ′, so dass L(M )= L(M ′). Ähnlich wie im Fall der endlichen Automaten können auch bei Turingmaschinen nicht- deterministische Strategien die Berechnungen vereinfachen. Betrachten wir die Sprache Lungleich = {x#y | x, y ∈ (Σbool) ∗,x ̸= λ, x ̸= y}. Eine (deterministische) TM müsste Buchstabe für Buchstabe x und y vergleichen, um den Unterschied feststellen zu können. Eine NTM kann die Position i, an der sich x = x1 ...xn und y = y1 ...ym unterscheiden, nichtdeterministisch raten und dann die Korrektheit des Ratens durch den Vergleich von xi und yi veriﬁzieren. Im Folgenden beschreiben wir eine nichtdeterministische 1-Band-TM A, die Lungleich akzeptiert (die formale Darstellung ﬁndet sich in Abbildung 4.12). A arbeitet auf einer Eingabe w wie folgt: 1. A überprüft deterministisch mit einem Lauf des Lesekopfes über das Eingabeband, ob w genau ein Symbol # enthält (Zustände q0,q1,qreject in Abbildung 4.12). Falls das nicht der Fall ist, verwirft A die Eingabe w. Falls w = x#y für x, y ∈ (Σbool) ∗, setzt A die Arbeit mit Phase 2 fort (im Zustand q2 in Abbildung 4.12). 2. A stellt die Köpfe auf beiden Bändern auf ¢ (Zustand q2 in Abbildung 4.12). 3. A bewegt beide Köpfe simultan nach rechts (dabei ersetzt der Kopf auf dem Arbeitsband die Symbole ␣ durch die Symbole a) und in jedem Schritt triﬀt sie nichtdeterministisch die Entscheidung, ob der Unterschied in der Position des gelesenen Feldes vorkommt oder nicht (Zustand qguess in Abbildung 4.12). Falls b ∈{0, 1} auf dem Eingabeband gelesen wird und A rät, dass der Unterschied an dieser Position auftritt, speichert A das Symbol b in seinem Zustand und geht zur Phase 4 über (A geht in Zustand pb über in Abbildung 4.12). Falls A das Symbol # liest (|x| < |y| rät), geht A in p# über. 4.5 Nichtdeterministische Turingmaschinen 115 qreject qaccept qguess q0 q1 p0 p1 s0 s1 q2 p# 0, ¢ → N, ¢, N 0, ¢ → N, ¢, N $,a → N,a, N $,a → N,a, N 1, ¢ → N, ¢, N 1, ¢ → N, ¢, N 1,a → R, ␣, L1,a → R, ␣, L 1,a → R, ␣, L 0,a → R, ␣, L0,a → R, ␣, L 0,a → R, ␣, L #,a → R, ␣, L #,a → R, ␣, L 0,a → R,a, N0,a → R,a, N 1,a → R,a, N 1,a → R,a, N 1, ␣ → R,a, N #, ␣ → R,a, N 0, ␣ → R,a, N ¢, ¢ → R, ¢, R 0, ␣ → R,a, R 1, ␣ → R,a, R #, ¢ → L, ¢, N 0, ¢ → L, ¢, N 1, ¢ → L, ¢, N $, ¢ → L, ¢, N $, ¢ → N, ¢, N $, ¢ → N, ¢, N$, ¢ → N, ¢, N #, ¢ → N, ¢, N#, ¢ → R, ¢, N 1, ¢ → R, ¢, N 1, ¢ → R, ¢, N0, ¢ → R, ¢, N 0, ¢ → R, ¢, N ¢, ¢ → R, ¢, N Abbildung 4.12 116 4 Turingmaschinen 4. Jetzt ist die Entfernung des Kopfes von ¢ auf dem Arbeitsband gleich der Position des gespeicherten Symboles b ∈{0, 1, #} in x#. A geht zuerst mit dem Lesekopf auf #, ohne den Kopf auf dem Arbeitsband zu bewegen (Zustände p0 und p1 in Abbildung 4.12). Danach bewegt sich in jedem weiteren Schritt der Lesekopf nach rechts, und der Kopf auf dem Arbeitsband nach links (Zustände s0 und s1 in Abbildung 4.12). Wenn der Kopf auf dem Arbeitsband ¢ erreicht, steht der Kopf des Eingabebandes auf der geratenen Position von y. Falls das gespeicherte Symbol b in sb ungleich dem gelesenen Symbol des Eingabebandes ist, akzeptiert A das Eingabewort w = x#y. A akzeptiert w auch, wenn |x| < |y| ((qaccept, N, ¢, N) ∈ δ(p#,c, N) für alle c ∈{0, 1} in Abbildung 4.12) oder wenn |x| > |y| (δ(sb, $,d)= {(qaccept, N,d, N)} für alle b ∈{0, 1},d ∈{a, ¢} in Abbildung 4.12). Die Strategie des nichtdeterministischen Ratens und nachfolgenden deterministischen Veriﬁzierens der Korrektheit des Geratenen ist typisch für nichtdeterministische Berech- nungen. 11 Ein anderes Beispiel ist das Akzeptieren der Sprache Lquad = {a n2 | n ∈ N} von einer nichtdeterministischen 2-Band-TM B. B kann zuerst für jede Eingabe w eine Zahl n raten (n kann durch die Positionen der Köpfe auf den Arbeitsbändern gespeichert werden) und dann |w| = n 2 überprüfen. Aufgabe 4.19. Beschreiben Sie die Berechnungsstrategie einer nichtdeterministischen 2-Band- TM, die Lquad akzeptiert und implementieren Sie ihre Strategie in der Form eines Diagramms. Die wichtigste Frage ist nun, ob die nichtdeterministischen Turingmaschinen Sprachen akzeptieren, die man mit (deterministischen) Turingmaschinen nicht akzeptieren kann. Ähnlich wie bei endlichen Automaten ist die Antwort auf diese Frage negativ, und die Simulationsstrategie basiert auf der Breitensuche in den Berechnungsbäumen der nichtdeterministischen TM. Deﬁnition 4.4. Sei M =(Q, Σ, Γ,δ,q0,qaccept,qreject) eine NTM und sei x ein Wort über dem Eingabealphabet Σ von M .Ein Berechnungsbaum TM,x von M auf x ist ein (potentiell unendlicher) gerichteter Baum mit einer Wurzel, der wie folgt deﬁniert wird. (i) Jeder Knoten von TM,x ist mit einer Konﬁguration beschriftet. (ii) Die Wurzel ist der einzige Knoten von TM,x mit dem Eingangsgrad 0 und ist mit der Startkonﬁguration q0¢x beschriftet. (iii) Jeder Knoten des Baumes, der mit einer Konﬁguration C beschriftet ist, hat genauso viele Kinder wie C Nachfolgekonﬁgurationen hat, und diese Kinder sind mit diesen Nachfolgekonﬁgurationen von C markiert. Die Deﬁnition von Berechnungsbäumen kann man natürlich auch für nichtdeterministi- sche Mehrband-Turingmaschinen verwenden. 11Wie stark man dadurch den Nichtdeterminismus charakterisieren kann, erfahren wir in Kapitel 6. 4.5 Nichtdeterministische Turingmaschinen 117 Aufgabe 4.20. Zeichnen Sie die Berechnungsbäume der nichtdeterministischen 1-Band-TM aus Abbildung 4.12 für die Eingaben 01#01#1 und 01#0. Es gibt zwei wesentliche Unterschiede zwischen den Berechnungsbäumen eines NEA und einer NTM. Die Berechnungsbäume nichtdeterministischer endlicher Automaten sind immer endlich, was bei nichtdeterministischen Turingmaschinen nicht immer der Fall sein muss. Zweitens müssen die Konﬁgurationen in der gleichen Entfernung zur Wurzel eines Berechnungsbaumes TM,x einer NTM M auf x keine Ähnlichkeiten haben und deshalb können im Unterschied zu nichtdeterministischen endlichen Automaten die Positionen der Köpfe auf dem Eingabeband unterschiedlich sein. Satz 4.2. Sei M eine NTM. Dann existiert eine TM A, so dass (i) L(M )= L(A) und (ii) falls M keine unendlichen Berechnungen auf Wörtern aus (L(M )) ∁ hat, dann hält A immer. Beweis. Nach Lemma 4.2 genügt es, eine 2-Band TM A mit den Eigenschaften (i) und (ii) zu konstruieren. Wir beschränken uns auf eine Beschreibung der Arbeit von A und verzichten auf die formale Konstruktion. Die Strategie von A heißt Breitensuche in den Berechnungsbäumen von M . Eingabe: Ein Wort w. Phase 1. A kopiert die Anfangskonﬁguration q0¢w auf das erste Arbeitsband. Phase 2. A überprüft, ob das erste Band eine akzeptierende Konﬁguration enthält. Falls ja, hält A und akzeptiert w. Sonst setzt A die Berechnung mit Phase 3 fort. Phase 3. A schreibt alle Nachfolgekonﬁgurationen der Konﬁgurationen aus dem ersten Arbeitsband auf das zweite Arbeitsband (man beachte, dass eine NTM nur endlich viele Aktionen für ein gegebenes Argument zur Wahl hat und A somit immer alle Möglichkeiten realisieren kann). Falls es keine Nachfolgekonﬁgurationen gibt (das zweite Arbeitband leer bleibt), hält A im Zustand qreject. Phase 4. A löscht den Inhalt des ersten Arbeitsbandes und kopiert den Inhalt des zweiten Arbeitsbandes auf das erste. Danach löscht A den Inhalt des zweiten Bandes und fährt mit Phase 2 fort. Wir bemerken, dass nach dem i-ten Durchlauf der Phasen 3 und 4 das erste Arbeitsband alle Konﬁgurationen des Berechnungsbaumes TM,w mit der Entfernung i von der Wurzel (alle in i Schritten erreichbaren Konﬁgurationen) enthält. Falls w ∈ L(M ), dann existiert eine akzeptierende Berechnung von M auf w von einer Länge j für ein j ∈ N, und somit wird w nach j Durchläufen der Phasen 3 und 4 in der Phase 2 akzeptiert. Falls w/∈ L(M ), wird w von A nicht akzeptiert. Falls TM,x endlich ist, hält A im Zustand qreject. \u0002 Aufgabe 4.21. Sei A die NTM aus Abbildung 4.13. (i) Geben Sie die ersten sechs Ebenen (alle Konﬁgurationen nach höchstens fünf Berechnungs- schritten) der Berechnungsbäume TA(x) für x = 01 und für x = 0010 an. (ii) Bestimmen Sie die Sprache L(A). 118 4 Turingmaschinen q0 q1 q2 q3 qaccept qreject ¢ → ¢, R ¢ → ¢, R0 → 0, R 0 → 0, R 0 → 0, N 0 → 0, N 1 → 1, N 1 → 1, N 1 → 1, N 1 → 1, L 1 → 1, L Abbildung 4.13 4.6 Kodierung von Turingmaschinen Jedes Programm hat eine binäre Darstellung in Form des Maschinencodes. Für die Trans- formation eines Programms, das über dem Alphabet ΣTastatur entsprechend der Syntax der Programmiersprache gegeben ist, in den Maschinencode sorgen Übersetzer (Compiler). Das Ziel dieses Kapitels ist, eine einfache binäre Kodierung von Turingmaschinen zu entwickeln. Wir beginnen zunächst damit, dass wir eine Kodierung von Turingmaschinen über {0, 1, #} entwickeln. Sei M =(Q, Σ, Γ,δ,q0,qaccept,qreject) eine TM, wobei Q = {q0,q1,...,qm,qaccept,qreject} und Γ = {A1,A2,...,Ar}. Wir deﬁnieren zuerst die Kodierung der einzelnen Symbole wie folgt: Code(qi)=10 i+11 für i =0, 1,...,m, Code(qaccept)=10 m+21, Code(qreject)=10 m+31, Code(Aj) = 110 j11 für j =1,...,r, Code(N) = 1110111, Code(R) = 1110 2111, Code(L) = 11103111. Diese Kodierung von Symbolen nutzen wir zu folgender Darstellung einzelner Transitionen. Code(δ(p, Al)=(q, Am,α)) = #Code(p)Code(Al)Code(q)Code(Am)Code(α)# für jede Transition δ(p, Al)=(q, Am,α), p ∈{q0,q1,...,qm}, q ∈ Q, l, m ∈{1,...,r}, α ∈{L, R, N}. Die Kodierung der Turingmaschine M gibt zuerst die globalen Daten – die Anzahl der Zustände (|Q|) und die Anzahl der Symbole aus dem Arbeitsalphabet (|Γ|). Danach folgt die Liste aller Transitionen. Daher Code(M )=#0 m+3#0r##Code(Transition1)#Code(Transition2)# ... 4.6 Kodierung von Turingmaschinen 119 q0 q1 q2 qacceptqreject ¢ → ¢, R ¢ → ¢, R 0 → 0, R 0 → 0, R 1 → 1, R ␣ → ␣, L a → a, N b → b, N a ∈{0, 1, ␣} b ∈{1, ¢, ␣} Abbildung 4.14 Aufgabe 4.22. Sei M die TM aus Abbildung 4.14. (i) Geben Sie Code(M )von M an. Gliedern Sie den Code übersichtlich mit Hilfe von Kom- mentaren. (ii) Geben Sie die Sprache L(M ) an. (iii) Ist L(M ) regulär oder nicht? Beweisen Sie Ihre Behauptung. Um eine Kodierung über Σbool zu erhalten, benutzen wir folgenden Homomorphismus h : {0, 1, #}∗ → (Σbool) ∗: h(#)=01,h(0)=00,h(1)=11. Deﬁnition 4.5. Für jede Turingmaschine M wird Kod(M ) = h(Code(M )) die Kodierung der TM M genannt. KodTM = {Kod(M ) | M ist eine TM} bezeichnet die Menge der Kodierungen aller Turingmaschinen. Es ist klar, dass die Abbildung von M auf Kod(M ) injektiv ist und daher bestimmt Kod(M ) eindeutig eine Turingmaschine. Aufgabe 4.23. Beschreiben Sie ein Programm, dass jede formale Beschreibung einer TM M nach Deﬁnition 4.5 in Code(M ) umwandelt. Aufgabe 4.24. Beschreiben Sie ein Programm, das für jedes Wort x ∈{0, 1} ∗ entscheidet, ob x =Kod(M ) für eine TM M . Im Folgenden bezeichnet Aver ein Programm (eine TM), das für jedes x ∈ (Σbool)∗ entscheidet, ob x die Kodierung einer TM ist. Die wichtigste Beobachtung ist, dass die Festlegung auf eine Kodierung Kod(M )von Turingmaschinen eine lineare Ordnung auf den Turingmaschinen wie folgt deﬁniert. 120 4 Turingmaschinen Deﬁnition 4.6. Sei x ∈ (Σbool)∗. Für jedes i ∈ N −{0} sagen wir, dass x die Kodie- rung der i-ten TM ist, falls (i) x =Kod(M ) für eine TM M und (ii) die Menge {y ∈ (Σbool) ∗ | y ist vor x in kanonischer Ordnung} enthält genau i − 1 Wörter, die Kodierungen von Turingmaschinen sind. Falls x = Kod(M ) die Kodierung der i-ten TM ist, dann ist M die i-te Turingmaschine Mi. Die Zahl i ist die Ordnung der TM Mi. Wir beobachten, dass es nicht schwer ist, für eine gegebene Zahl i die Kodierung Kod(Mi)der i-ten Turingmaschine zu berechnen. Sei Gen eine Funktion von N −{0} nach (Σbool) ∗ deﬁniert durch Gen(i)=Kod(Mi). Lemma 4.3. Die Funktion Gen ist total rekursiv, d. h., es existiert eine Turingmaschine (Programm) AGen, die für eine gegebene Zahl i die Kodierung Kod(Mi) berechnet. Beweis. Ein Programm, das Gen berechnet, kann wie folgt arbeiten. Eingabe: Ein i ∈ N −{0}. Schritt 1. x := λ; {x ist ein Wort über (Σbool) ∗} I := 0; Schritt 2. while I< i do begin Benutze Aver um zu entscheiden, ob x ∈ KodTM; if x ∈ KodTM then begin I := I +1; y := x; end; x := Nachfolger von x in kanonischer Ordnung auf (Σbool) ∗; end; Ausgabe: y. \u0002 Aufgabe 4.25. Schreiben Sie ein Programm, das für eine gegebene Kodierung Kod(M ) ∈ (Σbool) ∗ die Ordnung der TM M berechnet. 4.7 Zusammenfassung Die Turingmaschine ist ein abstraktes Rechnermodell mit der Berechnungsstärke realer Rechner. Die Komponenten einer TM sind ein unendliches Band, eine endliche Kontrolle und ein Lese-/Schreibkopf. Das Band ist aufgeteilt in Felder. Jedes Feld enthält ein Symbol des Arbeitsalphabets (ein Computerwort). Das Band wird als Eingabemedium der TM sowie als Speicher der TM benutzt. Die elementaren Aktionen (Transitionen) einer TM hängen vom aktuellen Zustand der TM und dem Symbol ab, das der Kopf auf dem 4.7 Zusammenfassung 121 Band liest. In einer Aktion darf die TM den Zustand ändern, das gelesene Symbol durch ein neues ersetzen und den Kopf um ein Feld auf dem Band bewegen. Eine Berechnung entsteht durch eine Folge elementarer Aktionen. Eine TM akzeptiert (verwirft) ein Wort x, wenn sie in einem Sonderzustand qaccept (qreject) die Berechnung auf x beendet. Ein Wort x wird von der TM nicht akzeptiert, wenn x verworfen wird oder wenn die Berechnung der TM auf x unendlich ist. Die von einer TM M akzeptierte Sprache L(M ) ist die Menge aller Wörter, die M akzeptiert. Eine Sprache L heißt rekursiv aufzählbar, falls L = L(M ) für eine TM M . Eine Sprache L heißt rekursiv (oder entscheidbar), falls L = L(M ) für eine TM M , die keine unendliche Berechnung hat (d. h., alle Berechnungen enden entweder in qaccept oder in qreject). Das Modell der Mehrband-TM hat anstatt eines unendlichen Bandes (das für die Eingabe sowie für die Speicherung von Daten bei einer TM dient) ein endliches Band, das nur die Eingabe enthält, und eine endliche positive Anzahl unendlicher Arbeitsbänder. Die Modelle der Turingmaschine und der Mehrband-Turingmaschine sind äquivalent in dem Sinne, dass jede TM durch eine MTM simuliert werden kann und umgekehrt. Diese Turingmaschinenmodelle sind äquivalent zu Programmen in jeder geläuﬁgen Program- miersprache. Die Church’sche These besagt, dass eine Turingmaschine, die keine unendliche Berech- nung ausführt, die Formalisierung des intuitiven Begriﬀs „Algorithmus“ ist. Daher sind alle Probleme, die man mit Hilfe von Turingmaschinen lösen kann, algorithmisch (automatisch) lösbar und alle Probleme, die auf Turingmaschinen nicht lösbar sind, sind algorithmisch (automatisch) unlösbar. Die Church’sche These ist ein Axiom der Informatik, daher kann sie nie bewiesen werden. Es besteht nur die Möglichkeit, sie zu widerlegen. Auf die gleiche Art wie bei endlichen Automaten kann man Turingmaschinen zu nichtdeterministischen Turingmaschinen verallgemeinern. Eine nichtdeterministische TM kann mehrere Berechnungen auf einer Eingabe haben. Die Eingabe wird akzeptiert, falls mindestens eine dieser Berechnungen in qaccept endet. Durch eine Breitensuche in den Berechnungsbäumen (dies entspricht der Potenzmengenkonstruktion bei endli- chen Automaten) können nichtdeterministische Turingmaschinen durch deterministische Turingmaschinen simuliert werden. Wie Programme können auch Turingmaschinen eindeutig als Wörter über Σbool darge- stellt werden. Weil die Wörter über Σbool durch die kanonische Ordnung linear geordnet sind, gewinnt man durch diese Darstellung eine lineare Ordnung für die Menge der Turing- maschinen. Für eine gegebene Zahl i kann man die Kodierung der i-ten TM berechnen. Ebenfalls kann man zu einer gegebenen TM ihre Ordnung berechnen. Die Einführung eines formalen Modells von Algorithmen war der erste Schritt, der zur Gründung der Theoretischen Informatik führte. Diesen Fortschritt hat Kurt Gödel mit seiner grundlegenden Arbeit [Göd31] initiiert. In dieser Arbeit wurde erstmals bewiesen, dass es mathematische Probleme gibt, die man nicht „automatisch“ lösen kann. Dies motivierte Church [Chu36], Kleene [Kle36], Post [Pos36] und Turing [Tur36] zum Ent- wurf formaler Modelle des algorithmischen Rechnens. Alle diese Modelle und auch viele andere, die später deﬁniert wurden, sind äquivalent. Die Folge dieser Äquivalenzen ist die Church’sche These. Das Modell der Turingmaschine aus [Tur36] wurde zum Basismodell des Rechners in der Theoretischen Informatik, obwohl die ursprünglichen Gedanken von 122 4 Turingmaschinen Turing nicht gerade mit einem Rechner verknüpft waren. Turing wollte die Verfahren (Algorithmen) zur Symbolmanipulation präzise formalisieren. Statt an einen Rechner dachte er an einen Menschen (menschlichen Rechner), der ein Rechenverfahren mit einem Stift auf einem Blatt Papier ausführt. Das eindimensionale Band als Speichermedium ist dadurch motiviert, dass man auf einem Blatt zeilenweise schreibt. Die endlich vielen Symbole, die man benutzen darf, bestimmen dann das Arbeitsalphabet. Um das alles systematisch zu machen, teilte Turing das Band in Felder ein, wobei jedes Feld genau ein Symbol beinhalten durfte. Der Inhalt des ganzen Bandes musste immer endlich sein, aber die Länge des Bandes (die Größe des Papiers) war unbeschränkt. Turing setzte voraus, dass der Rechner (das menschliche Gehirn) endlich groß ist, und sich somit nur in einem von endlich vielen möglichen Zuständen beﬁnden kann. Daher stammt die endliche Zustandsmenge für das Modell der Turingmaschine. Aus einem ähnlichen Grund setzte Turing voraus, dass eine Aktion eines Rechners in einem Augenblick nur durch einen Anteil des Bandes von beschränkter (konstanter) Größe beeinﬂusst werden kann. Weil man jede solche Aktion durch eine Folge elementarer Operationen, bei denen nur ein Symbol des Bandes betrachtet wird, simulieren kann, führte Turing die Festlegung der Berechnungsschritte durch die in Abschnitt 4.3 vorgestellte Transitionsfunktion ein. Die Mehrband-Turingmaschine wurde von Hartmanis und Stearns [HS65] eingeführt. Mehrband-Turingmaschinen wurden zum Basismodell für Berechnungen in der Komplexi- tätstheorie. Eine hinreißende Diskussion zu dem Thema dieses Kapitels kann man bei Harel [Har93] ﬁnden. Kontrollaufgaben 1. Beschreiben Sie das Modell der Turingmaschine. Was sind die wesentlichen Unterschiede zu einem endlichen Automaten? 2. Deﬁnieren Sie die Begriﬀe „Konﬁguration“, „Berechnungsschritt“ und „Berechnung“ einer Turingmaschine. Wie deﬁniert man die von einer TM M akzeptierte Sprache L(M )? 3. Was sind rekursive Sprachen und was sind rekursiv aufzählbare Sprachen? 4. Entwerfen Sie eine Turingmaschine für die Sprache {anbnc n | n ∈ N} und stellen Sie die TM in der Form eines Diagramms dar. 5. Was würde sich an der Berechnungsstärke der Turingmaschine ändern, wenn wir mehrere akzeptierende Zustände erlauben? 6. Erklären Sie das Konzept von Mehrband-Turingmaschinen. Warum führen wir dieses Konzept ein? 7. Wie deﬁniert man die Äquivalenz von zwei Maschinen und die Äquivalenz von zwei Maschinenmodellen? 8. Wie würden Sie vorgehen, um die Äquivalenz zwischen Java und Turingmaschinen zu zeigen? 9. Was sagt die Church’sche These aus? Warum betrachten wir die Church’sche These als ein Axiom? Warum glauben wir an dieses Axiom? 10. Erklären Sie das Konzept nichtdeterministischer Turingmaschinen. Wie viele unterschiedli- che Berechnungen kann eine NTM auf einer Eingabe der Länge n haben? 4.7 Zusammenfassung 123 11. Geben Sie ein Beispiel einer Sprache an, für die es einfacher ist, eine NTM anstatt einer TM zu entwerfen. 12. Wie kann man zu jeder NTM eine äquivalente TM bauen? 13. Wie kann man Turingmaschinen durch Wörter über {0, 1} kodieren? Entwerfen Sie eine eigene Kodierung für Turingmaschinen und schreiben Sie ein Programm, das aus Ihrer Kodierung eine konkrete TM erstellt. 14. Wie kann man Turingmaschinen linear ordnen? 15. Kann man für eine gegebene natürliche Zahl i algorithmisch die i-te TM konstruieren? Kann man für eine gegebene TM ihre Ordnung algorithmisch bestimmen? Hunderte von Talenten zeigen die Größe ihrer Epoche, aber nur ein Genie ahnt, was ihr fehlt. E. Geibel 5 Berechenbarkeit 5.1 Zielsetzung Die Theorie der Berechenbarkeit ist die erste Theorie, die in der Informatik entstanden ist. Sie hat Methoden zur Klassiﬁzierung von Problemen in algorithmisch lösbare und algorithmisch unlösbare entwickelt. Dies bedeutet, dass diese Theorie uns Techniken zum Beweisen der Nichtexistenz von Algorithmen zur Lösung konkreter Probleme liefert. Das Erlernen dieser Techniken ist das Hauptziel dieses Kapitels. Wir beschränken uns in diesem Kapitel auf Entscheidungsprobleme. Unser erstes Ziel ist es zu zeigen, dass es Sprachen gibt, die von keiner Turingmaschine akzeptiert werden. Dies ist einfach einzusehen, wenn man begreift, dass es viel mehr Sprachen gibt als Turingmaschinen. Aber von beiden gibt es unendlich viele, d. h., wir müssen lernen, wie man beweisen kann, dass eine unendliche Zahl größer ist als eine andere. Dazu präsentieren wir in Abschnitt 5.2 die Diagonalisierungstechnik aus der Mengenlehre. Die Methode der Diagonalisierung ermöglicht es uns, auch für eine konkrete Sprache, Diagonalsprache genannt, ihre Nichtzugehörigkeit zu LRE zu zeigen. Unser zweites Ziel ist es, die Methode der Reduktion vorzustellen. Diese Methode ermöglicht es, ausgehend von einer nichtentscheidbaren Sprache die Unentscheidbarkeit weiterer Sprachen zu beweisen, und stellt das Hauptinstrument zur Klassiﬁzierung der Sprachen bezüglich ihrer Entscheidbarkeit dar. Wir wenden diese Methode an, um die Unentscheidbarkeit einiger Entscheidungsprobleme über Turingmaschinen (Programmen) in Abschnitt 5.3 zu beweisen. Dabei lernen wir, dass die praktisch relevante Aufgabe der Korrektheitsüberprüfung (des Testens) von Programmen ein algorithmisch unlösbares Problem darstellt. In Abschnitt 5.4 stellen wir den Satz von Rice vor, der besagt, dass fast alle nicht-trivialen Probleme über Turingmaschinen (Programmen) unentscheidbar sind. In Abschnitt 5.5 zeigen wir, dass man die Methode der Reduktion auch zum Beweis der Unentscheidbarkeit anderer als nur Turingmaschinen-bezogener Probleme benutzen kann. Als ein Beispiel beweisen wir die Unentscheidbarkeit eines Entscheidungsproblems über einer Art von Dominospiel, das als Post’sches Korrespondenzproblem bezeichnet wird. In Abschnitt 5.6 zeigen wir eine andere Methode zum Beweisen der algorithmischen J. Hromkovič, Theoretische Informatik, DOI 10.1007/978-3-658-06433-4_5, © Springer Fachmedien Wiesbaden 2014 126 5 Berechenbarkeit Unlösbarkeit konkreter Probleme. Diese Methode basiert auf der Kolmogorov-Komplexität und kann als eine Alternative zur Diagonalisierungstechnik in dem Sinne gesehen werden, dass man mittels der Kolmogorov-Komplexität die Unlösbarkeit eines konkreten Problems beweisen und dadurch den Startpunkt für die Anwendung der Reduktionsmethode erhalten kann. In Abschnitt 5.7 verwenden wir die Kolmogorov-Komplexität, um zu zeigen, dass es Programme gibt, die zwar korrekt sind, deren Korrektheit man aber nicht beweisen kann. 5.2 Die Methode der Diagonalisierung Unser erstes Ziel ist es zu zeigen, dass es nicht rekursiv aufzählbare Sprachen gibt. Dazu wollen wir folgendes quantitative Argument benutzen. Wir wollen zeigen, dass die Mächtigkeit |KodTM| der Menge aller Turingmaschinen kleiner als die Mächtigkeit aller Sprachen über Σbool ist. Dabei bezeichnet KodTM die Menge der binären Kodierungen aller Turingmaschinen wie in Abschnitt 4.6 deﬁniert. Die Anzahl aller Turingmaschinen ist unendlich und kann von oben durch |(Σbool)∗| beschränkt werden, weil KodTM ⊆ (Σbool) ∗. Die Kardinalität aller Sprachen über Σbool ist |P((Σbool) ∗)|, was oﬀensichtlich auch eine unendliche Zahl ist. Um zu beweisen, dass ∣ ∣(Σbool) ∗∣ ∣ < ∣ ∣P((Σbool) ∗)∣ ∣ , benötigen wir eine Methode zum Vergleich der Größen unendlicher Zahlen (der Mächtig- keiten unendlicher Mengen). Das folgende Konzept von Cantor zum Vergleich der Mächtigkeiten von zwei (unendli- chen) Mengen berührt die philosophischen und axiomatischen Wurzeln der Mathematik und ist die Grundlage der modernen Mengenlehre. Die Idee des Konzeptes von Cantor ist wie folgt. Ein Hirte hat ein Herde weißer Schafe und eine Herde schwarzer Schafe. Er will feststellen, welche der Herden zahlreicher ist, also ob er mehr schwarze als weiße Schafe hat oder umgekehrt. Das Problem ist, dass er nur bis 5 zählen kann, was nicht ausreichend ist. Was kann er tun? Er nimmt ein weißes und ein schwarzes Schaf und führt dieses Paar auf eine andere Wiese. Dies macht er so lange, bis eine der beiden ursprünglichen Herden aufgelöst ist. Die Sorte, deren Herde noch nicht ausgeschöpft ist, ist zahlreicher. Genau diese Idee setzen wir in folgender Deﬁnition um. Deﬁnition 5.1. Seien A und B zwei Mengen. Wir sagen, dass |A|≤|B|, falls eine injektive Funktion f von A nach B existiert. Wir sagen, dass |A| = |B|, falls |A|≤|B| und |B|≤|A| (d. h., es existiert eine Bijektion 1 zwischen A und B). Wir sagen, dass |A| < |B|, 1Der Satz von Cantor und Bernstein sagt, dass aus der Existenz von injektiven Abbildungen F1 : A → B und F2 : B → A die Existenz einer Bijektion zwischen A und B folgt. 5.2 Die Methode der Diagonalisierung 127 A B f Abbildung 5.1 falls |A|≤|B| und keine injektive Abbildung von B nach A existiert. Zuerst beobachten wir, dass für endliche Mengen Deﬁnition 5.1 exakt mit unserem Ver- ständnis für den Vergleich der Mächtigkeiten von A und B übereinstimmt (Abbildung 5.1). Wenn für alle x, y ∈ A mit x ̸= y auch f (x) ̸= f (y) gilt, dann muss B mindestens so viele Elemente wie A haben. Wenn {f (x) | x ∈ A} = B, dann ist f eine Bijektion und |A| = |B|. Nach Deﬁnition 5.1 reicht es daher aus zu beweisen, dass keine injektive Abbildung von der Menge aller Sprachen über Σbool in die Menge aller Turingmaschinen existiert. Das heißt, es existiert keine Abbildung von P((Σbool) ∗) nach KodTM, die jeder Sprache L eine TM M mit L(M )= L zuordnet. 2 Aufgabe 5.1. Seien A, B und C Mengen. Beweisen Sie, dass |B|≤|A| und |C|≤|B| die Relation |C|≤|A| impliziert. Aufgabe 5.2. Seien A und B Mengen. Beweisen Sie, dass A ⊆ B die Ungleichung |A|≤|B| impliziert. Deﬁnition 5.1 hat aber auch ihre paradoxe Seite. Sei Ngerade = {2i | i ∈ N}. Nach Deﬁnition 5.1 gilt |N| = |Ngerade|, weil die Funktion f : N → Ngerade mit f (i)=2i für alle i ∈ N oﬀensichtlich eine Bijektion ist. So haben N und Ngerade die gleiche Mächtigkeit, obwohl Ngerade eine echte Teilmenge von N ist. Dies ist ein Paradoxon, das beim Vergleich der Mächtigkeiten endlicher Mengen nicht auftreten kann. Dieses Paradoxon widerspricht der Erfahrung aus der endlichen Welt, wo das Ganze immer größer ist als eines seiner Teile. Dies zeigt aber nicht, dass das Cantor’sche Konzept zum Vergleich der Mächtigkeit von Mengen falsch ist, sondern nur, dass die Welt unendlicher Mengen und Zahlen Gesetzen unterliegen könnte, die den Erfahrungen aus der Behandlung endlicher Objekte nicht entsprechen. Der Schluss |N| = |Ngerade| scheint in der „unendlichen Welt“ richtig zu sein, weil die Bijektion f (i)=2i die Elemente aus N und Ngerade paart, und so erscheinen beide Mengen gleich mächtig zu sein. Vielleicht ist es wichtig, an dieser Stelle zu bemerken, dass dieses Konzept 2Weil jede solche Abbildung injektiv sein muss. 128 5 Berechenbarkeit auf der axiomatischen Ebene liegt. Daher kann man nicht beweisen, dass dies die einzige sinnvolle (korrekte) Möglichkeit zum Vergleich der Mächtigkeit unendlicher Mengen ist. Deﬁnition 5.1 ist nur ein Versuch, das intuitive Verständnis der Mathematiker für den Mächtigkeitsvergleich zu formalisieren. Es ist nicht auszuschließen, dass jemand noch eine andere, geeignetere Formalisierung ﬁndet. 3 Für uns ist aber nur wichtig, dass das Konzept aus Deﬁnition 5.1 dazu geeignet ist, die Existenz nicht rekursiv aufzählbarer Sprachen zu beweisen. Aufgabe 5.3. Das Hilbert’sche Hotel besteht aus unendlich vielen Zimmern, die mit den natürlichen Zahlen 1, 2, 3,... nummeriert sind. Das Hotel ist besetzt, in jedem Zimmer verweilt also ein Gast. Jetzt kommt ein neuer Gast und fragt, ob er ein Zimmer bekommen kann. Der Portier sagt locker „Kein Problem“. Er lässt für alle i =1, 2, 3,... den Gast aus Zimmer i in Zimmer i + 1 umziehen und gibt Zimmer 1 dem neuen Gast. (a) Gibt es auch eine Lösung, wenn der Gast unbedingt Zimmer 7 haben will? (b) Es kommen 32 neue Gäste. Was macht der Portier jetzt? (c) Es kommt ein voll besetzter, unendlich großer Bus an. Die Plätze sind mit positiven ganzen Zahlen 1, 2, 3,... nummeriert. Kann der Portier alle unterbringen? Im Folgenden betrachten wir N als die „kleinste“ unendliche Menge und stellen die Frage, welche unendlichen Mengen die gleiche Mächtigkeit wie N haben, und ob eine unendliche Menge A mit |A| > |N| existiert. Deﬁnition 5.2. Eine Menge A heißt abzählbar, 4 falls A endlich ist oder |A| = |N|. Die intuitive Bedeutung der Abzählbarkeit einer Menge A ist, dass man die Elemente aus A als erstes, zweites, drittes, ... nummerieren kann. Dies ist oﬀensichtlich, weil jede injektive Funktion f : A → N eine lineare Ordnung auf A bestimmt, die eine Nummerierung5 ist, (ein Objekt a ∈ A ist vor dem Objekt b ∈ A ⇐⇒ f (a) <f (b)). Deswegen überrascht es nicht, dass (Σbool) ∗ und KodTM abzählbar sind. Lemma 5.1. Sei Σ ein beliebiges Alphabet. Dann ist Σ ∗ abzählbar. Beweis. Sei Σ = {a1,...,am} eine beliebige endliche Menge. Man deﬁniere eine lineare Ordnung a1 <a2 < ··· <am auf Σ. Diese lineare Ordnung bestimmt die kanonische Ordnung auf Σ∗ (vergleiche Deﬁnition 2.8). Die kanonische Ordnung auf Σ∗ ist eine Nummerierung und bestimmt daher eine injektive Funktion von Σ ∗ nach N. \u0002 Satz 5.1. Die Menge KodTM der Turingmaschinenkodierungen ist abzählbar. 3Dies ähnelt unserer Diskussion über die Church’sche These. 4Eine äquivalente Deﬁnition der Abzählbarkeit einer Menge A ist folgende: A ist abzählbar ⇐⇒ es existiert eine injektive Funktion f : A → N Dies bedeutet, dass keine unendliche Menge B mit |B| < |N| existiert. Auf den Beweis dieses Fakts verzichten wir hier. 5Eine Nummerierung auf einer Menge A determiniert eine lineare Ordnung, in der zwischen zwei beliebigen Elementen höchstens endlich viele Elemente aus A liegen. Umgekehrt ist eine lineare Ordnung mit dieser Eigenschaft eine Nummerierung. 5.2 Die Methode der Diagonalisierung 129 123456 ··· 1 (1, 1) (1, 2) (1, 3) (1, 4) (1, 5) (1, 6) ··· 2 (2, 1) (2, 2) (2, 3) (2, 4) (2, 5) (2, 6) ··· 3 (3, 1) (3, 2) (3, 3) (3, 4) (3, 5) (3, 6) ··· 4 (4, 1) (4, 2) (4, 3) (4, 4) (4, 5) (4, 6) ··· 5 (5, 1) (5, 2) (5, 3) (5, 4) (5, 5) (5, 6) ··· 6 (6, 1) (6, 2) (6, 3) (6, 4) (6, 5) (6, 6) ··· ... ... ... ... ... ... ... . . . Abbildung 5.2 Beweis. Satz 5.1 ist eine direkte Folge von Lemma 5.1 und der Tatsache KodTM ⊆ (Σbool) ∗. \u0002 Aufgabe 5.4. Geben Sie explizit die Nummerierung von (Σbool)∗ an, die der kanonischen Ordnung auf (Σbool) ∗ entspricht. Aufgabe 5.5. Beweisen Sie, dass die Menge Z der ganzen Zahlen abzählbar ist. Aufgabe 5.6. Sei A eine abzählbare Menge und a ein Element mit a/∈ A. Beweisen Sie, dass A ∪{a} auch abzählbar ist. Aufgabe 5.7. Seien A und B zwei abzählbare Mengen. Beweisen Sie, dass A ∪ B abzählbar ist. Das nächste Resultat kann ein bisschen überraschend wirken. Wir beweisen, dass die Mächtigkeit der Menge Q + der positiven rationalen Zahlen der Mächtigkeit von N entspricht. Dabei weiss man, dass die rationalen Zahlen sehr dicht nebeneinander auf der reellen Achse liegen (zwischen beliebigen rationalen Zahlen a und b mit a<b liegen unendlich viele rationale Zahlen c mit der Eigenschaft a<c<b), und dass die natürlichen Zahlen auf der reellen Achse den Abstand 1 besitzen. Weil man die positiven rationalen Zahlen als p q mit p, q ∈ N −{0} darstellen kann, würde man erwarten, dass |Q +| ungefähr |N×N| ist, was nach „unendlich mal unendlich“ aussieht. In der endlichen Welt würde man über den Vergleich von n2 mit n sprechen. Trotzdem ist |Q +| = |N|, weil die Elemente aus N × N sich nummerieren lassen. Die folgende Methode zur Nummerierung von positiven rationalen Zahlen ist einfach und elegant und ﬁndet auch Anwendung in der Theorie der Berechenbarkeit. Lemma 5.2. (N −{0}) × (N −{0}) ist abzählbar. Beweis. Wir konstruieren eine unendliche Matrix MN×N wie in Abbildung 5.2. Die Matrix hat unendlich viele Zeilen und unendlich viele Spalten, die durch natürliche Zahlen 1, 2, 3,... nummeriert sind. An der Kreuzung der i-ten Zeile und der j-ten Spalte beﬁndet sich das Element (i, j) ∈ (N −{0}) × (N −{0}). Es ist oﬀensichtlich, dass MN×N alle Elemente aus (N −{0}) × (N −{0}) enthält. Der Versuch, die Elemente aus (N −{0}) × (N −{0}) so zu ordnen, dass man mit den Elementen der ersten Zeile beginnt und danach die Elemente weiterer Zeilen nummeriert, 130 5 Berechenbarkeit 12345 6 ··· 1 2 3 4 5 6 ... ... Abbildung 5.3 muss scheitern, weil man wegen der Unendlichkeit der ersten Zeile nie zu der Nummerierung der zweiten Zeile gelangt. Eine passende Möglichkeit, die Elemente der Matrix MN×N zu nummerieren, ist durch die Zick-Zack-Linie in Abbildung 5.3 gegeben. Auf diese Weise nimmt man eine endliche Nebendiagonale nach der anderen, und es ist oﬀensichtlich, dass jedes Element von MN×N eine Nummer bekommt. Die resultierende Folge ist a1 =(1, 1), a2 =(2, 1), a3 =(1, 2), a4 =(3, 1), a5 =(2, 2), a6 =(1, 3), a7 =(4, 1), ... Formal deﬁniert man dabei die lineare Ordnung (a, b) < (c, d) ⇐⇒ a + b<c + d oder (a + b = c + d und b<d). Die entsprechende Nummerierung ist f ((a, b)) = (a + b − 1 2 ) + b, weil das Element (a, b) das b-te Element auf der (a + b − 1)-ten Nebendiagonalen ist und die Anzahl der Elemente auf den ersten a + b − 2 Nebendiagonalen a+b−2∑ i=1 i = (a + b − 2) · (1 + a + b − 2) 2 = (a + b − 1 2 ) ist. Es ist oﬀensichtlich, dass f eine Bijektion von (N −{0}) × (N −{0}) nach N −{0} ist. \u0002 5.2 Die Methode der Diagonalisierung 131 12345 6 ··· 1 2 3 4 5 6 ... Abbildung 5.4 Aufgabe 5.8. Um die Abzählbarkeit der Menge (N −{0}) × (N −{0}) zu beweisen, kann man auch die Nummerierung aus Abbildung 5.4 verwenden. Bestimmen Sie die entsprechende injektive Abbildung von (N −{0}) × (N −{0}) nach N. Wie viele unterschiedliche injektive Abbildungen von (N −{0}) × (N −{0}) nach N können Sie auﬂisten? Satz 5.2. Q+ ist abzählbar. Beweis. Sei h folgende Abbildung von Q + nach (N −{0}) × (N −{0}): h ( p q ) =(p, q) für alle p, q mit dem größten gemeinsamen Teiler 1. Oﬀensichtlich ist h injektiv. Weil (N −{0}) × (N −{0}) nach Lemma 5.2 abzählbar ist, ist auch Q+ abzählbar. \u0002 Aufgabe 5.9. In das voll besetzte unendliche Hilbert’sche Hotel kommen unendlich viele un- endlich große Busse. Die Busse haben Nummern 1, 2, 3,... . Alle Busse sind voll besetzt und die Plätze in jedem Bus haben die Nummern 1, 2, 3,... . Kann der Portier jetzt alle unterbringen? Aufgabe 5.10. Beweisen Sie, dass N × N × N abzählbar ist. Aufgabe 5.11. Seien A und B zwei abzählbare Mengen. Beweisen Sie, dass A×B eine abzählbare Menge ist. Trotz ihrer Dichte auf der reellen Achse sind die positiven rationalen Zahlen abzählbar. Nach der Aussage von Aufgabe 5.11 ist auch (Q +) i für jedes i ∈ N −{0} abzählbar. Man könnte denken, dass alle unendlichen Mengen abzählbar sind. Wir zeigen jetzt, dass die Menge R der reellen Zahlen nicht abzählbar ist. Dazu reicht es zu zeigen, dass die Menge [0, 1] der reellen Zahlen größer gleich 0 und kleiner gleich 1 nicht abzählbar ist. Also besitzt R einen anderen, höheren Typ von Unendlichkeit als N und Q +. 132 5 Berechenbarkeit f (x) x ∈ [0, 1] 1 0. a11 a12 a13 a14 ... 2 0. a21 a22 a23 a24 ... 3 0. a31 a32 a33 a34 ... 4 0. a41 a42 a43 a44 ... ... ... ... ... ... ... i 0. ai1 ai2 ai3 ai4 ... aii ... ... ... Abbildung 5.5 Satz 5.3. [0, 1] ist nicht abzählbar. Beweis. Wir müssen zeigen, dass keine injektive Funktion von [0, 1] nach N −{0} existiert. Wir führen einen indirekten Beweis dieser Behauptung. Wir setzen voraus, dass [0, 1] abzählbar ist und dass f eine injektive Abbildung von [0, 1] nach N −{0} ist. Daher bestimmt f eine Nummerierung der reellen Zahlen aus [0, 1], die wir in Abbildung 5.5 verdeutlichen können. Die i-te Zahl aus [0, 1] ist ai =0.ai1ai2ai3 ... (das heißt f (ai)= i), wobei aij Ziﬀern aus {0, 1, 2,..., 9} für j =1, 2,... sind. Jetzt wenden wir die sogenannte Diagonalisierungsmethode an, um zu zeigen, dass in der Tabelle in Abbildung 5.5 mindestens eine reelle Zahl aus [0, 1] fehlt, und daher f keine Nummerierung von [0, 1] ist. Die Auﬂistung der reellen Zahlen in Abbildung 5.5 kann man als unendliche Matrix M =[aij]i=1,...,∞,j=1,...,∞ interpretieren. Wir konstruieren jetzt eine reelle Zahl c =0.c1c2c3 ... so, dass ci ̸= aii und ci /∈{0, 9} für alle i ∈ N −{0}. Im Prinzip schauen wir die Diagonale a11a22a33 ... von M an und wählen für jedes i ein festes ci ∈{1, 2, 3, 4, 5, 6, 7, 8}− aii. Damit unterscheidet sich c von jeder Darstellung einer reellen Zahl in Abbildung 5.5 in mindestens einer Ziﬀer j, genauer, die Darstellung von c unterscheidet sich von der Darstellung von ai mindestens in der i-ten Dezimalstelle hinter dem Punkt für alle i ∈ N −{0}. Weil die Darstellung von c keine 0 und 9 als Ziﬀer enthält,6 ist sie eindeutig, und wir können schließen, dass c ̸= ai für alle i ∈ N −{0}. Daher ist c nicht in Abbildung 5.5 dargestellt und f keine Nummerierung der Zahlen aus [0, 1]. \u0002 Wir haben gezeigt, dass die Menge aller Turingmaschinen (Algorithmen) aufzählbar ist. Um zu zeigen, dass es Probleme gibt, die algorithmisch nicht lösbar sind, reicht es, die Nichtabzählbarkeit der Menge aller Sprachen (Entscheidungsprobleme) über {0, 1} zu beweisen. Wir beweisen dies auf zwei unterschiedliche Arten. Zuerst zeigen wir |[0, 1]|≤|P((Σbool) ∗)| und dann konstruieren wir direkt mit der Diagonalisierungsmethode eine Sprache, die von keiner Turingmaschine akzeptiert wird. Satz 5.4. P((Σbool) ∗) ist nicht abzählbar. 6Man bemerke, dass 1.000 und 0.999 zwei unterschiedliche Darstellungen der Zahl 1 sind. 5.2 Die Methode der Diagonalisierung 133 Beweis. Wir zeigen |P((Σbool) ∗)|≥|[0, 1]|. Jede reelle Zahl aus [0, 1] kann man binär wie folgt darstellen. Durch a =0.a1a2a3 ... mit ai ∈ Σbool für i =1, 2, 3,... wird die Zahl Nummer(a)= ∞∑ i=1 ai2 −i dargestellt. Falls eine Zahl mehrere Darstellungen hat (es gibt höchstens 2), dann wählen wir die in lexikographischer Ordnung letzte. 7 Wir benutzen diese Darstellung, um folgende Abbildung f von [0, 1] nach P((Σbool)∗) zu deﬁnieren. Für jede binäre Darstellung a =0.a1a2a3 ... einer reellen Zahl aus [0, 1] ist f (a)= {a1,a2a3,a4a5a6,...,a( n 2)+1a( n 2)+2 ...a( n+1 2 ),...}. Oﬀensichtlich ist f (a) eine Sprache über Σbool, die genau ein Wort der Länge n für alle n ∈ N −{0} enthält. Deswegen führt jeder Unterschied in einer Ziﬀer zweier bi- närer Darstellungen b und c zu f (b) ̸= f (c). Daher ist f injektiv und P((Σbool)∗) nicht abzählbar. \u0002 Korollar 5.1. |KodTM| < |P((Σbool)∗)| und somit existieren unendlich viele nicht re- kursiv aufzählbare Sprachen über Σbool. Aufgabe 5.12. Beweisen Sie |[0, 1]| = |P((Σbool) ∗)|. Wir benutzen jetzt die Diagonalisierungsmethode, um eine konkrete nicht rekursiv aufzählbare Sprache zu konstruieren. Sei w1, w2, w3, ... die kanonische Ordnung aller Wörter über Σbool, und sei M1, M2, M3, ... die Folge aller Turingmaschinen. Wir deﬁnieren eine unendliche Boole’sche Matrix A =[dij]i,j=1,...,∞ (Abbildung 5.6) mit dij =1 ⇐⇒ Mi akzeptiert wj. Damit bestimmt die i-te Zeile di1di2di3 ... der Matrix A die Sprache L(Mi)= {wj | dij = 1 für alle j ∈ N −{0}}. Analog zum Beweis von Satz 5.3, in dem wir eine reelle Zahl konstruieren, die nicht in der Nummerierung in Abbildung 5.5 enthalten ist, konstruieren wir jetzt eine Sprache Ldiag, Diagonalsprache genannt, die keiner der Sprachen L(Mi) entspricht. Wir deﬁnieren Ldiag = {w ∈ (Σbool) ∗ | w = wi für ein i ∈ N −{0} und Mi akzeptiert wi nicht} = {w ∈ (Σbool) ∗ | w = wi für ein i ∈ N −{0} und dii =0}. Satz 5.5. Ldiag /∈LRE. Beweis. Wir beweisen Ldiag /∈LRE indirekt. Sei Ldiag ∈LRE. Dann ist Ldiag = L(M ) für eine TM M .Weil M eine der Turingmaschinen in der Nummerierung aller Turingmaschinen sein muss, existiert ein i ∈ N −{0}, so dass M = Mi.Aber Ldiag kann nicht gleich L(Mi) sein, weil folgende Äquivalenz gilt: wi ∈ Ldiag ⇐⇒ dii =0 ⇐⇒ wi /∈ L(Mi), d. h., wi ist in genau einer der Sprachen Ldiag oder L(Mi). \u0002 7Zum Beispiel hat 1 2 die Darstellungen 0.01 und 0.10, wobei wir letztere wählen würden. 134 5 Berechenbarkeit w1 w2 w3 ... wi ... M1 d11 d12 d13 ... d1i ... M2 d21 d22 d23 ... d2i ... M3 d31 d32 d33 ... d3i ... ... ... ... ... ... Mi di1 di2 di3 ... dii ... ... ... ... ... ... Abbildung 5.6 Aufgabe 5.13. Betrachten Sie die Sprache L2diag = {w ∈ (Σbool) ∗ | w = w2i für ein i ∈ N −{0} und Mi akzeptiert w2i nicht (di,2i =0)}. Beweisen Sie, dass L2diag /∈LRE. Aufgabe 5.14. Sei für jedes k ∈ N Lk,diag = {w ∈ (Σbool) ∗ | w = wi+k für ein i ∈ N −{0} und Mi akzeptiert wi+k nicht (di,i+k =0)}. Beweisen Sie, dass Lk,diag /∈LRE für jedes k ∈ N. 5.3 Die Methode der Reduktion Die Methode der Reduktion ist die am häuﬁgsten benutzte Methode zur Klassiﬁkation der Entscheidungsprobleme bezüglich der algorithmischen Lösbarkeit. Die Idee ist sehr einfach. Sei A ein Problem, für das wir beweisen wollen, dass es nicht algorithmisch lösbar ist. Wenn wir jetzt ein Problem B ﬁnden, von dem schon bekannt ist, dass es nicht algorithmisch lösbar ist, aber die algorithmische Lösbarkeit von A die algorithmische Lösbarkeit von B implizieren würde, dann können wir schließen, dass A nicht algorithmisch lösbar ist. Dies nennen wir eine Reduktion von B auf A. Deﬁnition 5.3. Seien L1 ⊆ Σ∗ 1 und L2 ⊆ Σ∗ 2 zwei Sprachen. Wir sagen, dass L1 auf L2 rekursiv reduzierbar ist, L1 ≤R L2, falls L2 ∈LR =⇒ L1 ∈LR. Die Bezeichnung L1 ≤R L2 entspricht der intuitiven Bedeutung, dass L2 bezüglich der algorithmischen Lösbarkeit mindestens so schwer wie L1 ist, 5.3 Die Methode der Reduktion 135 TM B mit L(B)= L1 x ∈ Σ∗ 1 TM M M (x) TM A „x∈L1⇔M (x)∈L2“ A(M (x)) Abbildung 5.7 denn wenn L2 algorithmisch lösbar wäre (das heißt L2 = L(A) für einen Algorithmus A), dann wäre auch L1 algorithmisch lösbar (das heißt L1 = L(B) für einen Algorithmus B). Wir kennen bereits die Diagonalsprache Ldiag, von der wir wissen, dass sie nicht in LRE und somit auch nicht in LR liegt. Wir brauchen jetzt konkrete Techniken für die Beweise von Resultaten der Art L1 ≤R L2, um weitere nichtrekursive Sprachen zu ﬁnden. Wir stellen jetzt zwei solcher Techniken vor, die der Deﬁnition der rekursiven Reduzierbarkeit entsprechen. Die erste Technik wird EE-Reduktion (Eingabe-zu-Eingabe-Reduktion) genannt. Das Schema der EE-Reduktion ist in Abbildung 5.7 dargestellt. Die Idee ist, eine TM (einen Algorithmus) M zu ﬁnden, die für jede Eingabe x für das Entscheidungsproblem (Σ1,L1) eine Eingabe y für das Entscheidungsproblem (Σ2,L2) konstruiert, so dass die Lösung des Problems (Σ2,L2) für y der Lösung des Problems (Σ1,L1) für x entspricht. Das bedeutet, wenn eine TM (ein Algorithmus) A für (Σ2,L2) existiert, dann ist die Hintereinander- schaltung von M und A eine TM (ein Algorithmus) für (Σ1,L1). Deﬁnition 5.4. Seien L1 ⊆ Σ∗ 1, L2 ⊆ Σ∗ 2 zwei Sprachen. Wir sagen, dass L1 auf L2 EE-reduzierbar 8 ist, L1 ≤EE L2, wenn eine TM M existiert, die eine Abbildung fM :Σ∗ 1 → Σ ∗ 2 mit der Eigenschaft x ∈ L1 ⇐⇒ fM (x) ∈ L2 für alle x ∈ Σ∗ 1 berechnet. Wir sagen auch, dass die TM M die Sprache L1 auf die Sprache L2 reduziert. Das nächste Lemma besagt, dass die Relation ≤EE ein Spezialfall der Relation ≤R ist, was bedeutet, dass es reicht, L1 ≤EE L2 zu zeigen, um L1 ≤R L2 zu beweisen. Lemma 5.3. Seien L1 ⊆ Σ∗ 1, L2 ⊆ Σ∗ 2 zwei Sprachen. Falls L1 ≤EE L2, dann auch L1 ≤R L2. Beweis. Sei L1 ≤EE L2.Um L1 ≤R L2 zu beweisen, reicht es zu zeigen, dass die Existenz eines Algorithmus A (einer TM A, die immer hält), der L2 entscheidet (L2 ∈LR), die Existenz eines Algorithmus B garantiert, der L1 entscheidet. Sei A eine TM, die immer hält und L(A)= L2. Die Voraussetzung L1 ≤EE L2 impliziert die Existenz einer TM M , die für jedes x ∈ Σ∗ 1 ein Wort M (x) ∈ Σ∗ 2 mit der Eigenschaft x ∈ L1 ⇐⇒ M (x) ∈ L2 berechnet. Wir konstruieren eine TM B, die immer hält und für die L(B)= L1 gilt. Die TM B arbeitet auf einer Eingabe x ∈ Σ∗ 1 wie folgt: 8Die EE-Reduzierbarkeit wird in der englischsprachigen Literatur „many-one-reducibility“ genannt. 136 5 Berechenbarkeit x ∈ L1 x/∈ L1 x ∈ Σ∗ 1 TM A TM B mit L(B)= L1 L(A)= L2 Abbildung 5.8 (i) B simuliert die Arbeit von M auf x, bis auf dem Band das Wort M (x) steht. (ii) B simuliert die Arbeit von A auf M (x). Wenn A das Wort M (x) akzeptiert, dann akzeptiert B das Wort x. Wenn A das Wort M (x) verwirft, dann verwirft B das Wort x. Weil A immer hält, hält auch B immer und somit gilt L1 ∈LR. \u0002 Aufgabe 5.15. Beweisen Sie, dass ≤EE eine transitive Relation ist (d. h., beweisen Sie, dass L1 ≤EE L2 und L2 ≤EE L3 =⇒ L1 ≤EE L3). Wir bemerken, dass die TM B mit L(B)= L1 die TM A als ein Teilprogramm benutzt (Abbildung 5.7), und zwar so, dass B die TM A für jede Eingabe einmal laufen lässt und die Ausgabe von A als eigene Ausgabe übernimmt. Dies ist aber eine unnötige Einschränkung. Mit einer weiteren, allgemeineren Technik kann man B so bauen, dass B die TM A mehrmals auf unterschiedlichen Eingaben aufruft und abhängig von den Resultaten der Arbeit von A die Entscheidung bezüglich x ∈ L1 triﬀt (Abbildung 5.8). Wir sind jetzt imstande, Beweise für Aussagen der Form L/∈LR für konkrete Sprachen zu führen. Wir fangen mit einer allgemeinen Bemerkung an, und zwar, dass eine Sprache L genau dann in LR ist, wenn das Komplement von L in LR ist. Lemma 5.4. Sei Σ ein Alphabet. Für jede Sprache L ⊆ Σ ∗ gilt: L ≤R L ∁ und L ∁ ≤R L. Beweis. Es reicht aus, L ∁ ≤R L für jede Sprache L zu beweisen, weil (L ∁)∁ = L, und daher impliziert L ∁ ≤R L für alle Sprachen L die Relation (L ∁) ∁ ≤R L ∁ (wenn man statt L die Sprache L ∁ in die Relation L ∁ ≤R L einsetzt). Sei A ein Algorithmus, der (Σ,L) entscheidet. Ein Algorithmus B,der(Σ,L ∁) entschei- det, ist in Abbildung 5.9 dargestellt. B übergibt seine Eingabe x ∈ Σ ∗ als Eingabe an A und invertiert dann die Entscheidung von A. Im Formalismus der Turingmaschinen wird man einfach zu einer TM A =(Q, Σ, Γ,δ,q0, qaccept,qreject), die immer hält und L akzeptiert, die TM B =(Q, Σ, Γ,δ,q0,q′ accept,q′ reject) mit q′ accept = qreject, q′ reject = qaccept bauen. Weil A immer hält, ist es oﬀensichtlich, dass B auch immer hält und L(B)=(L(A))∁. \u0002 5.3 Die Methode der Reduktion 137 x ∈ L∁ x/∈ L∁ x ∈ L x/∈ L x ∈ Σ∗ TM A TM B mit L(B)= L∁ Abbildung 5.9 Korollar 5.2. (Ldiag) ∁ /∈LR. Beweis. Wir haben gezeigt, dass Ldiag /∈LRE und damit auch Ldiag /∈LR. Nach Lem- ma 5.4 ist Ldiag ≤R (Ldiag)∁. Weil nach Lemma 5.4 (Ldiag)∁ ∈LR auch Ldiag ∈LR implizieren würde, können wir schließen, dass (Ldiag) ∁ /∈LR. \u0002 Aus den bisher präsentierten Tatsachen können wir aber nicht schließen, dass (Ldiag) ∁ /∈ LRE. Das Gegenteil ist wahr, und somit beweisen wir LR ⊊ LRE. Lemma 5.5. (Ldiag) ∁ ∈LRE. Beweis. Nach der Deﬁnition von Ldiag erhalten wir (Ldiag) ∁ = {x ∈ (Σbool) ∗ | x = wi für ein i ∈ N −{0} und Mi akzeptiert das Wort wi}. Eine TM D, die (Ldiag) ∁ akzeptiert, kann wie folgt arbeiten. Eingabe: Ein x ∈ (Σbool) ∗. (i) Berechne i, so dass x das i-te Wort wi in der kanonischen Ordnung über Σbool ist. (ii) Generiere die Kodierung Kod(Mi)der i-ten TM Mi. (iii) Simuliere die Berechnung von Mi auf dem Wort wi = x. Falls Mi das Wort wi akzeptiert, dann akzeptiert auch D die Eingabe x. Falls Mi das Wort wi verwirft (d. h. in qreject hält), dann hält D und verwirft x = wi auch. Falls Mi auf wi unendlich lange arbeitet (d. h. wi /∈ L(Mi)), simuliert D die unendliche Arbeit von Mi auf wi. Daher hält D auf x nicht und somit x/∈ L(D). Es ist oﬀensichtlich, dass L(D)=(Ldiag) ∁. \u0002 Korollar 5.3. (Ldiag) ∁ ∈LRE −LR, und daher LR ⊊ LRE. 138 5 Berechenbarkeit Im Folgenden präsentieren wir weitere Sprachen, die nicht rekursiv sind, aber in LRE liegen (Abbildung 5.10). Deﬁnition 5.5. Die universelle Sprache ist die Sprache LU = {Kod(M )#w | w ∈ (Σbool) ∗ und M akzeptiert w}. Satz 5.6. Es gibt eine TM U, universelle TM genannt, so dass L(U )= LU. Daher gilt LU ∈LRE. Beweis. Es reicht aus, eine 1-Band-TM U mit L(U )= LU zu konstruieren. U kann wie folgt auf einer Eingabe z ∈{0, 1, #}∗ arbeiten. (i) U überprüft, ob z genau ein # enthält. Falls nicht, verwirft U das Wort z. (ii) Sei z = y#x mit y, x ∈ (Σbool)∗. U überprüft, ob y eine Kodierung einer TM ist. Falls y keine TM kodiert, verwirft U die Eingabe y#x. (iii) Falls y = Kod(M ) für eine TM M , schreibt U die Anfangskonﬁguration von M auf x auf das Arbeitsband. (iv) U simuliert schrittweise die Berechnung von M auf x wie folgt. while der Zustand der Konﬁgurationen von M auf dem Arbeitsband ist unterschied- lich von qaccept und qreject do begin Simuliere einen Schritt von M auf dem Arbeitsband; {Die Aktion, die zu simulieren ist, kann U aus der Kodierung Kod(M ) auf dem Arbeitsband leicht ablesen.} end; if der Zustand von M ist qaccept then U akzeptiert z =Kod(M )#x; else U verwirft z =Kod(M )#x; Man bemerke, dass eine unendliche Berechnung von M auf x zu einer unendlichen Berechnung von U auf Kod(M )#x führt und U deshalb in diesem Fall die Eingabe Kod(M )#x nicht akzeptiert. Somit ist L(U )= LU. \u0002 Was bedeutet eigentlich die Tatsache, dass LU ∈LRE? Dies bedeutet die Existenz einer TM (eines Programms) ohne Haltegarantie, die eine beliebige Turingmaschine auf einer gegebenen Eingabe simulieren kann. Dies ist aber eine ganz natürliche Forderung, die man als ein Axiom an jede formale Deﬁnition einer allgemeinen Algorithmenklasse stellt. Die Betriebssysteme mit ihren Interpretern erfüllen gerade diese Aufgabe für die Programmiersprachen als Berechnungsmodelle. Es wäre unzulässig, einen Interpreter (ein Betriebssystem) zu erstellen, der unfähig wäre, jedes syntaktisch korrekte Programm in gegebener Programmiersprache auf einer Eingabe laufen zu lassen. 5.3 Die Methode der Reduktion 139 LU LH (Lempty)∁ Ldiag (Ldiag)∁ LR LRE Abbildung 5.10 Im Folgenden beweisen wir, dass LU /∈LR. Die Bedeutung dieser Behauptung ist, dass man im Allgemeinen nicht das Resultat der Berechnung einer TM M auf einer Eingabe x anders berechnen kann, als die Berechnung von M auf x zu simulieren. Wenn aber M auf x unendlich lange arbeitet, wissen wir zu keinem Zeitpunkt der Simulation, ob die Berechnung von M auf x unendlich ist oder ob M in den nächsten Schritten halten und entscheiden wird. Deswegen können wir in endlicher Zeit keine Entscheidung über die Zugehörigkeit von x zu L(M ) treﬀen. Die folgenden Beweise basieren auf der Reduktionsmethode. Die meisten Beweise präsen- tieren wir zweimal. Einmal anschaulich auf der Ebene von Algorithmen als Programme in irgendeiner Programmiersprache und das zweite Mal im Formalismus der Turingmaschinen und der EE-Reduktion. Satz 5.7. LU /∈LR. Beweis. Es reicht zu zeigen, dass (Ldiag)∁ ≤R LU. Nach Korollar 5.2 gilt (Ldiag)∁ /∈LR und damit folgt LU /∈LR. Sei A ein Algorithmus (Programm), der LU entscheidet. Wir bauen einen Algorithmus B, der mit A als Teilprogramm die Sprache (Ldiag)∁ entscheidet. Algorithmus B ist so strukturiert wie in Abbildung 5.11 dargestellt. Für eine Eingabe x ∈ (Σbool) ∗ berechnet das Teilprogramm C ein i, so dass x = wi, und die Kodierung Kod(Mi). Die Wörter Kod(Mi) und x = wi bekommt A als Eingabe. Die Entscheidung „akzeptiere“ oder „verwerfe“ von A für Kod(Mi)#x wird als das Resultat von B für die Eingabe x übernommen. Oﬀensichtlich gilt L(B)=(Ldiag)∁ und B hält immer, weil A nach der Voraussetzung immer hält und seine Entscheidung liefert. Jetzt beweisen wir die Aussage im Formalismus der Turingmaschinen. Es reicht zu beweisen, dass (Ldiag)∁ ≤EE LU. Wir beschreiben eine TM M , die eine Abbildung fM von (Σbool) ∗ nach {0, 1, #}∗ berechnet, so dass x ∈ (Ldiag) ∁ ⇐⇒ fM (x) ∈ LU. M arbeitet wie folgt. Für eine Eingabe x berechnet M zuerst ein i, so dass x = wi. Danach berechnet M die Kodierung Kod(Mi)der i-ten TM. M hält mit dem Inhalt Kod(Mi)#x 140 5 Berechenbarkeit x ∈ (Σbool)∗ C x = wi A L(A)= LU Kod(Mi) wi ∈ L(Mi) wi /∈ L(Mi) Algorithmus B mit L(B)= (Ldiag)∁ wi ∈ (Ldiag)∁ wi /∈ (Ldiag)∁ Abbildung 5.11 auf dem Band. Weil x = wi, ist es nach der Deﬁnition von (Ldiag) ∁ oﬀensichtlich, dass x = wi ∈ (Ldiag) ∁ ⇐⇒ Mi akzeptiert wi ⇐⇒ wi ∈ L(Mi) ⇐⇒ Kod(Mi)#x ∈ LU. \u0002 Aufgabe 5.16. Zeigen Sie, dass die folgende Sprache {Kod(M )#x#0i | x ∈{0, 1}∗,i ∈ N,M hat mindestens i + 1 Zustände und während der Berechnung von M auf x wird der i-te Zustand von M mindestens einmal erreicht} keine rekursive Sprache ist. Wir sehen, dass eines der zentralen Probleme in der Theorie der Berechenbarkeit stark mit dem Halten der Turingmaschinen (mit der Endlichkeit der Berechnungen) zusammenhängt. Für die Sprachen (Ldiag) ∁ und LU gibt es Turingmaschinen (Programme), die diese Sprachen akzeptieren, aber es gibt keine Turingmaschinen, die (Ldiag) ∁ und LU entscheiden (d. h. keine unendlichen Berechnungen machen). Deswegen betrachten wir jetzt das folgende Problem. Deﬁnition 5.6. Das Halteproblem ist das Entscheidungsproblem ({0, 1, #},LH), wobei LH = {Kod(M )#x | x ∈{0, 1}∗ und M hält auf x}. Aufgabe 5.17. Beweisen Sie, dass LH ∈LRE gilt. Das folgende Resultat besagt, dass es keinen Algorithmus gibt, der testen kann, ob ein gegebenes Programm immer terminiert. Satz 5.8. LH /∈LR. Beweis. Wir führen zuerst einen Beweis auf der Ebene von Programmen. Wir zeigen LU ≤R LH.Sei LH ∈LR, d. h., es existiert ein Algorithmus A,der LH akzeptiert. Wir beschreiben jetzt einen Algorithmus B, der mit A als Teilprogramm die Sprache LU 5.3 Die Methode der Reduktion 141 Kod(M ) y = x w A C U L(A)= LH Kod(M )#x M hält auf x M akzeptiert x M verwirft x Algorithmus B für LU y#x ∈ LU y#x/∈ LU M hält nicht auf x w/∈ LU, falls w nicht die Form y#x besitzt oder y keine gültige Kodierung einer TM ist Abbildung 5.12 entscheidet (Abbildung 5.12). B bekommt eine Eingabe w und benutzt ein Teilprogramm C zum Testen, ob die Eingabe die Form y#x mit y =Kod(M ) für eine TM M hat. Falls nicht, dann verwirft B die Eingabe. Falls w =Kod(M )#x, dann gibt B dem Teilprogramm A diese Eingabe. Falls A „M hält nicht auf x“ antwortet, dann weiß B, dass x/∈ L(M ) und verwirft die Eingabe Kod(M )#x. Falls A „M hält auf x“ antwortet, dann simuliert B im Teilprogramm U die Arbeit von M auf x.Weil M auf x eine endliche Berechnung hat, wird die Simulation von U in endlicher Zeit durchgeführt. Falls die Ausgabe von U „M akzeptiert x“ ist, dann akzeptiert B seine Eingabe y#x = Kod(M )#x. Falls die Ausgabe von U „M verwirft x“ ist, dann verwirft B die Eingabe Kod(M )#x. Oﬀensichtlich gilt L(B)= LU und B hält immer. Daher erhalten wir LU ∈LR und somit gilt LU ≤R LH. Wir beweisen jetzt LU ≤R LH in dem Formalismus der Turingmaschinen. Es reicht zu zeigen, dass LU ≤EE LH. Wir beschreiben eine TM M , die LU auf LH reduziert. Für eine Eingabe w arbeitet M wie folgt. M überprüft, ob die Eingabe die Form w = Kod(M )#x für eine TM M und ein x ∈ (Σbool) ∗ hat. (i) Falls w diese Form nicht hat, generiert M die Kodierung Kod(M1) einer TM M1, die für jede Eingabe in einer endlosen Schleife im Zustand q0 läuft (δ(q0,a)=(q0,a, N) für alle a ∈{0, 1}). Dann hält M mit dem Bandinhalt M (w)=Kod(M1)#x. (ii) Falls w = Kod(M )#x, dann modiﬁziert M die Kodierung der TM M zu folgender TM M2 mit L(M2)= L(M ). M2 arbeitet genau wie M , nur dass alle Transitionen zum Zustand qreject von M zu einem neuen Zustand p umgeleitet werden, in dem M2 für jede Eingabe in einer endlosen Schleife läuft. Daher führt M2 für jede Eingabe y/∈ L(M )= L(M2) eine unendliche Berechnung durch. M beendet seine Arbeit mit dem Bandinhalt M (w)=Kod(M2)#x. 142 5 Berechenbarkeit Wir beweisen jetzt für alle w ∈{0, 1, #}∗, dass w ∈ LU ⇐⇒ M (w) ∈ LH. Sei w ∈ LU. Daher ist w = Kod(M )#x für eine TM M und ein Wort x ∈{0, 1}∗ und x ∈ L(M ). Dann ist M (w)=Kod(M2)#x mit L(M2)= L(M ) ein Wort in LH. Sei w/∈ LU. Wir unterscheiden zwei Möglichkeiten. Wenn w nicht die Form Kod(M )#x für eine TM M hat, dann ist M (w)= Kod(M1)#x,wobei M1 auf keiner Eingabe hält. Daher ist M (w) nicht in LH.Wenn w die Form Kod(M )#x für eine TM M hat und Kod(M )#x/∈ LU, bedeutet das, dass x/∈ L(M ). In diesem Fall ist aber M (w)= Kod(M2)#x,wobei M2 auf keiner Eingabe aus (Σbool)∗ − L(M ) hält. Weil x/∈ L(M ), gilt Kod(M )#x/∈ LH. \u0002 Aufgabe 5.18. Beweisen Sie die folgenden Aussagen: (a) LU ≤R (Ldiag) ∁, (b) LH ≤R (Ldiag) ∁, (c) Ldiag ≤R LU, (d) (Ldiag) ∁ ≤R LH, (e) LH ≤R LU. Betrachten wir jetzt die Sprache Lempty = {Kod(M ) | L(M )= ∅}, die die Kodierungen aller Turingmaschinen enthält, die die leere Menge (kein Wort) akzeptieren. Oﬀensichtlich ist (Lempty) ∁ = {x ∈ (Σbool) ∗ | x ̸=Kod(M ) für alle TM M oder x =Kod(M ) und L(M ) ̸= ∅}. Lemma 5.6. (Lempty) ∁ ∈LRE. Beweis. Wir führen zwei unterschiedliche Beweise für (Lempty) ∁ ∈LRE. Im ersten Beweis zeigen wir, dass es nützlich ist, das Modell der nichtdeterministischen Turingmaschinen zur Verfügung zu haben. Der zweite Beweis zeigt, wie man die Ideen aus der Mengenlehre, speziell aus dem Beweis |N| = |Q +|, in der Theorie der Berechenbarkeit anwenden kann. Weil für jede NTM M1 eine TM M2 existiert, so dass L(M1)= L(M2), reicht es zu zeigen, dass eine NTM M1 mit L(M1)=(Lempty) ∁ existiert. M1 arbeitet auf einer Eingabe x wie folgt. (i) M1 überprüft deterministisch, ob x =Kod(M ) für eine TM M . Falls x keine TM kodiert, akzeptiert M1 das Wort x. (ii) Falls x = Kod(M ) für eine TM M , wählt M1 nichtdeterministisch ein Wort y ∈ (Σbool) ∗ und simuliert deterministisch die Berechnung von M auf y. 5.3 Die Methode der Reduktion 143 (iii) Falls M das Wort y akzeptiert (das heißt L(M ) ̸= ∅), so akzeptiert M1 die Eingabe x =Kod(M ). Falls M das Wort y verwirft, akzeptiert M1 in diesem Lauf die Eingabe x nicht. Falls die Berechnung von M auf y unendlich ist, rechnet M1 auch in diesem Lauf auf x unendlich lange und akzeptiert so dass Wort x = Kod(M ) in diesem Lauf nicht. Gemäß Schritt (i) akzeptiert M1 alle Wörter, die keine TM kodieren. Falls x = Kod(M ) und L(M ) ̸= ∅, dann existiert ein Wort y mit y ∈ L(M ). Deswegen existiert eine Berechnung von M1 auf x =Kod(M ), in der x akzeptiert wird. Falls x ∈ Lempty, dann existiert keine akzeptierende Berechnung von M1 auf x, und so schließen wir, dass L(M1)=(Lempty) ∁. Im zweiten Beweis von (Lempty) ∁ ∈LRE konstruieren wir direkt eine (deterministische) TM A, die (Lempty) ∁ akzeptiert. A arbeitet auf einer Eingabe w wie folgt. (i) Falls w keine Kodierung einer Turingmaschine ist, so akzeptiert A die Eingabe w. (ii) Falls w =Kod(M ) für eine TM M , arbeitet A wie folgt. A konstruiert systematisch (in der Reihenfolge aus Abbildung 5.3 oder Abbildung 5.4) alle Paare (i, j) ∈ (N −{0}) × (N −{0}). Für jedes Paar (i, j) generiert A das i-te Wort wi über dem Eingabealphabet der TM M und simuliert j Berechnungsschritte von M auf wi. Falls für ein (k, l) die TM M das Wort wk in l Schritten akzeptiert, hält A und ak- zeptiert ihre Eingabe w = Kod(M ). Sonst arbeitet A unendlich lange und akzeptiert damit die Eingabe w nicht. Der Kernpunkt der Beweisidee ist, dass, wenn ein Wort y mit y ∈ L(M ) existiert, dann ist y = wk für ein k ∈ N −{0} und die akzeptierende Berechnung von M auf y hat eine endliche Länge l. Damit garantiert die systematische Überprüfung aller Paare (i, j)in Phase (ii) der Arbeit von A das Akzeptieren von Kod(M ), falls L(M ) ̸= ∅. Somit ist L(A)=(Lempty) ∁. \u0002 Im Folgenden zeigen wir, dass (Lempty) ∁ /∈LR. Dies entspricht der Nichtexistenz eines Algorithmus zur Überprüfung, ob ein gegebenes Programm die leere Menge akzeptiert. Daher können wir im Allgemeinen nicht die Korrektheit von Programmen testen. Dies geht sogar in den Fällen nicht, in denen es sich um triviale Aufgabenstellungen handelt (zum Beispiel eine konstante Funktion zu berechnen). Lemma 5.7. (Lempty) ∁ /∈LR. Beweis. Wir zeigen LU ≤EE (Lempty) ∁. Wir beschreiben eine TM A, die LU auf (Lempty) ∁ reduziert (Abbildung 5.13). Für jede Eingabe x ∈{0, 1, #}∗ arbeitet A wie folgt. (i) Falls x nicht die Form Kod(M )#w für eine TM M und ein w ∈ (Σbool) ∗ hat, dann schreibt A das Wort A(x)= Kod(Bx) auf das Band, wobei Bx eine TM ist, die über Σbool arbeitet und die leere Menge ∅ akzeptiert (L(Bx)= ∅). 144 5 Berechenbarkeit x w Kod(M ) Kod(Bx) TM E A L(E)= (Lempty)∁ L(Bx) ̸= ∅ L(Bx)= ∅ M akzeptiert w M akzeptiert w nicht Abbildung 5.13 (ii) Falls x = Kod(M )#w für eine TM M und ein Wort w ∈ (Σbool)∗, dann generiert A die Kodierung Kod(Bx) einer TM Bx als ihre Ausgabe. Die TM Bx arbeitet für jede Eingabe y (unabhängig vom Inhalt der eigenen Eingabe y) wie folgt. (a) Bx löscht y und generiert x =Kod(M )#w auf dem Band. {Das Wort x kann durch die Zustände und die δ-Funktion von Bx beschrieben werden.} (b) Bx simuliert die Arbeit von M auf w. Falls M das Wort w akzeptiert, akzeptiert Bx ihre Eingabe y. Falls M das Wort w verwirft, verwirft Bx ihre Eingabe y. Falls M auf w nicht hält, dann arbeitet auch Bx unendlich lange. Folglich akzeptiert Bx ihre Eingabe y nicht. Wir beweisen jetzt, dass x ∈ LU ⇐⇒ A(x)=Kod(Bx) ∈ (Lempty) ∁ für alle x ∈{0, 1, #}∗. Sei x ∈ LU. Daher gilt x = Kod(M )#w für eine TM M und w ∈ L(M ). In diesem Fall gilt L(Bx)=(Σbool) ∗ ̸= ∅ und somit Kod(Bx) ∈ (Lempty) ∁. Sei x/∈ LU. Dann hat x entweder nicht die Form Kod(M ′)#z für eine TM M ′ und ein z ∈{0, 1}∗ oder x = Kod(M )#w für eine TM M und w/∈ L(M ). In beiden Fällen gilt L(Bx)= ∅ und somit Kod(Bx) /∈ (Lempty) ∁. \u0002 Korollar 5.4. Lempty /∈LR. Beweis. Wenn Lempty ∈LR wäre, müsste nach Lemma 5.4 auch (Lempty) ∁ ∈LR sein. \u0002 Aufgabe 5.19.∗ Beweisen Sie, dass die folgenden Sprachen nicht in LRE sind: (a) Lempty, (b) (LH) ∁, (c) (LU) ∁. Die nächste Folgerung aus Lemma 5.7 ist, dass das Äquivalenzproblem für zwei Tu- ringmaschinen unentscheidbar ist. Man kann also kein Programm entwerfen, das für zwei gegebene Programme testen kann, ob die Programme das gleiche Problem lösen (die gleiche semantische Bedeutung haben). 5.4 Der Satz von Rice 145 Korollar 5.5. Die Sprache LEQ = {Kod(M )#Kod(M ) | L(M )= L(M )} ist nicht entscheidbar (das heißt LEQ /∈LR). Beweis. Die Beweisidee ist einfach, weil Lempty als ein Spezialfall von LEQ betrachtet werden kann. Formal reicht es zu zeigen, dass Lempty ≤EE LEQ. Es ist einfach, eine TM A zu konstruieren, die für eine Eingabe Kod(M ) die Ausgabe Kod(M )#Kod(C) generiert, wobei C eine feste triviale TM mit L(C)= ∅ ist. Es ist klar, dass Kod(M )#Kod(C) ∈ LEQ ⇐⇒ L(M )= L(C)= ∅ ⇐⇒ Kod(M ) ∈ Lempty. \u0002 Aufgabe 5.20. Beweisen Sie, dass die Sprache {Kod(M )#Kod(M ) | L(M ) ⊆ L(M )} nicht in LR ist. 5.4 Der Satz von Rice Im letzten Abschnitt haben wir festgestellt, dass das Testen von Programmen ein sehr schweres Problem ist. Für ein Programm A und eine Eingabe des Programms x ist es unentscheidbar, ob A auf x terminiert. Daher können wir Programme nicht algorithmisch testen, um zu veriﬁzieren, ob ein gegebenes Programm für jede Eingabe terminiert (einem Algorithmus entspricht). Das triviale Entscheidungsproblem, ob ein gegebenes Programm keine Eingabe akzeptiert (ob L(M )= ∅ für eine TM M ) ist auch unentscheidbar. Dies lässt uns ahnen, dass es nicht viele Testprobleme über Programmen gibt, die entscheidbar sind. Die Zielsetzung dieses Abschnitts ist zu zeigen, dass alle in gewissem Sinne nichttrivialen Probleme über Programmen (Turingmaschinen) unentscheidbar sind. Was mit dem Begriﬀ nichttrivial gemeint ist, speziﬁert die folgende Deﬁnition. Deﬁnition 5.7. Eine Sprache L ⊆ KodTM heißt semantisch nichttriviales Ent- scheidungsproblem über Turingmaschinen, falls folgende Bedingungen gelten: (i) Es gibt eine TM M1, so dass Kod(M1) ∈ L (daher L ̸= ∅), (ii) Es gibt eine TM M2, so dass Kod(M2) /∈ L (daher enthält L nicht die Kodierungen aller Turingmaschinen), (iii) für zwei Turingmaschinen A und B impliziert L(A)= L(B) Kod(A) ∈ L ⇐⇒ Kod(B) ∈ L. Bevor wir zum Beweis der Unentscheidbarkeit semantisch nichttrivialer Entscheidungs- probleme übergehen, betrachten wir aus technischen Gründen noch die folgende Sprache LH,λ = {Kod(M ) | M hält auf λ} als ein speziﬁsches Halteproblem. Lemma 5.8. LH,λ /∈LR. 146 5 Berechenbarkeit Beweis. Wir zeigen LH ≤EE LH,λ.EineTM A kann LH auf LH,λ wie folgt reduzieren. Für eine Eingabe x, die nicht die Form Kod(M )#w hat, generiert A eine einfache TM Hx, die auf jeder Eingabe unendlich lange läuft. Falls x = Kod(M )#w für eine TM M und ein Wort w, generiert A eine Kodierung Kod(Hx) einer TM Hx, die wie folgt arbeitet: 1. Ohne die eigene Eingabe y anzuschauen, generiert die TM Hx das Wort x = Kod(M )#w auf dem Band. 2. Hx simuliert die Berechnung von M auf w. Falls M auf w hält, dann hält Hx auch und akzeptiert die eigene Eingabe. Falls die Berechnung von M auf w unendlich ist, dann arbeitet Hx auch unendlich lange. Oﬀensichtlich gilt x ∈ LH ⇐⇒ x =Kod(M )#w und M hält auf w ⇐⇒ Hx hält immer (für jede eigene Eingabe y) ⇐⇒ Hx hält auf λ ⇐⇒ Kod(Hx) ∈ LH,λ für jedes x ∈{0, 1, #}∗. \u0002 Satz 5.9∗ (Satz von Rice). Jedes semantisch nichttriviale Entscheidungsproblem über Turingmaschinen ist unentscheidbar. Beweis. Sei L ein beliebiges semantisch nichttriviales Entscheidungsproblem über Turing- maschinen. Wir zeigen entweder LH,λ ≤EE L oder LH,λ ≤EE L ∁. Sei M∅ eine TM mit der Eigenschaft, dass L(M∅)= ∅. Wir unterscheiden jetzt zwei Möglichkeiten bezüglich der Zugehörigkeit von Kod(M∅)zu L: I. Sei Kod(M∅) ∈ L. In diesem Fall beweisen wir LH,λ ≤EE L ∁. Nach Bedingung (ii) von Deﬁnition 5.7 existiert eine TM M , so dass Kod(M ) /∈ L. Wir beschreiben jetzt die Arbeit einer TM S, die LH,λ auf L ∁ reduziert. Für jede Eingabe x ∈ (Σbool) ∗ berechnet S (i) entweder S(x)= Kod(M ′) mit L(M ′)= L(M∅)= ∅ (wenn x/∈ LH,λ), also Kod(M ′) /∈ L ∁. (ii) oder S(x)= Kod(M ′) mit L(M ′)= L(M )(wenn x ∈ LH,λ), also Kod(M ′) ∈ L ∁. S führt die Berechnung auf folgende Weise durch (Abbildung 5.14): Eingabe: Ein x ∈ (Σbool) ∗. 1. S überprüft, ob x = Kod(M ) für eine TM M . Falls x keine Kodierung einer TM ist, schreibt S das Wort S(x)=Kod(M∅) auf das Band. 5.4 Der Satz von Rice 147 x ∈{0, 1}∗ x S Falls x =Kod(M ) für eine TM M , dann generiert S eine TM M ′. M ′ simuliert M auf λ. Falls M auf λ hält, simuliert M ′ die Arbeit von M auf eigener Eingabe. S(x)=Kod(M ′) {L(M ′)= ∅ = L(M∅), wenn M auf λ nicht hält} {L(M ′)= L(M )mit Kod(M ) ∈ L∁, wenn M auf λ hält} Eine TM, die L∁ akzeptiert JA S(x) ∈ L∁ NEIN S(x) /∈ L∁ M hält auf λ M hält nicht auf λ x ist keine Ko- dierung einer TM S(x)= Kod(M∅) {M∅ /∈ L∁} Eine TM für LH,λ Abbildung 5.14 148 5 Berechenbarkeit 2. Falls x = Kod(M ) für eine TM M , dann generiert S die Kodierung Kod(M ′) einer TM M ′, die wie folgt arbeitet. (a) Das Eingabealphabet von M ′ ist ΣM , also das Eingabealphabet der TM M mit Kod(M ) /∈ L, das heißt Kod(M ) ∈ L ∁. (b) Für jedes y ∈ (ΣM )∗ generiert M ′ das Wort x = Kod(M ) auf dem Band hinter y (also wird y dabei nicht überschrieben) und simuliert die Berechnung von M auf λ. Falls M auf λ nicht hält (das heißt Kod(M ) /∈ LH,λ), dann hält auch M ′ auf y nicht, und deshalb y/∈ L(M ′). {Weil diese Simulation der Berechnung von M auf λ unabhängig von der Eingabe y von M ′ läuft, ist L(M ′)= ∅ = L(M∅) und damit Kod(M ′) ∈ L, also Kod(M ′) /∈ L ∁.} Falls M auf λ hält (das heißt Kod(M ) ∈ LH,λ), dann generiert M ′ die Kodierung Kod(M )derTM M auf dem Band. Dann simuliert M ′ die Arbeit von M auf der eigenen Eingabe y ∈ (ΣM ) ∗. M ′ akzeptiert y genau dann, wenn M das Wort y akzeptiert. {Damit ist L(M ′)= L(M ) und daher Kod(M ′) /∈ L, also Kod(M ′) ∈ L ∁.} Wir sehen, dass (Abbildung 5.14) x ∈ LH,λ ⇐⇒ S(x) ∈ L ∁ für alle x ∈ (Σbool) ∗ und somit LH,λ ≤EE L ∁. II. Sei Kod(M∅) /∈ L. Nach Bedingung (i) von Deﬁnition 5.7 muss eine TM ˜M existieren, so dass Kod( ˜M ) ∈ L. Jetzt kann auf die gleiche Weise LH,λ ≤EE L bewiesen werden wie LH,λ ≤EE L ∁ in Teil I bewiesen wurde. ˜M spielt dabei die Rolle von M . \u0002 Aufgabe 5.21. Führen Sie einen detaillierten Beweis von LH,λ ≤EE L, falls Kod(M∅) /∈ L. Der Satz von Rice hat folgende Konsequenz. Sei L eine beliebige rekursive Sprache und sei KodL = {Kod(M ) | M ist eine TM und L(M )= L} die Sprache der Kodierungen aller Turingmaschinen, die die Sprache L akzeptieren. Weil L rekursiv ist, ist KodL ̸= ∅. Oﬀensichtlich existieren Turingmaschinen, deren Kodierungen nicht in KodL sind, und KodL erfüllt die Bedingung (iii) von Deﬁnition 5.7. Damit ist KodL ein semantisch nichttriviales Entscheidungsproblem über Turingmaschinen, und nach dem Satz von Rice ist KodL /∈LR. Dies bedeutet, dass wir für kein algorithmisch lösbares Problem algorithmisch testen können, ob ein entworfener Algorithmus eine korrekte Lösung des Problems ist. Die Veriﬁkation von Programmen ist also eine schwere Aufgabe, und deswegen ist ein gut strukturierter und modularer Entwurf von Programmen so wichtig für die Zuverlässigkeit des entstehenden Softwareproduktes. 5.5 Das Post’sche Korrespondenzproblem 149 x1 y1 x2 y2 xn yn ... Abbildung 5.15 1 0 0 #0 #0 01 # 01# s1 s2 s3 s4 Abbildung 5.16 Aufgabe 5.22. Beweisen Sie die folgende Aussage. Sei Σ ein Alphabet und sei L ⊆ Σ∗. L, L∁ ∈LRE ⇐⇒ L ∈LR. 5.5 Das Post’sche Korrespondenzproblem In den letzten Abschnitten haben wir gezeigt, dass fast alle Probleme über Turingmaschi- nen (Programmen) unentscheidbar sind. Eine berechtigte Frage ist aber, ob man auch natürliche unentscheidbare Probleme außerhalb der Welt der Turingmaschinen hat. Die Antwort auf diese Frage ist positiv, und die Zielsetzung dieses Abschnitts ist zu zeigen, wie man mit Hilfe von Reduktionen die Beweise der Unentscheidbarkeit aus der Welt der Turingmaschinen in die Welt der Spiele übertragen kann. Wir betrachten jetzt folgendes Dominospiel. Man hat eine endliche Menge von Domino- steintypen (Abbildung 5.15), jeweils dargestellt als Paar (x, y), wobei x und y nichtleere Wörter über demselben Alphabet Σ sind. Von jedem Dominosteintyp stehen unbegrenzt viele Dominosteine zur Verfügung. Die Frage ist, ob es möglich ist, einige Steine so nebeneinander zusammenzusetzen, dass oben (durch das erste Element der Paare be- stimmt) das gleiche Wort wie unten (durch das zweite Element der Paare bestimmt) steht. Veranschaulichen wir uns das an folgendem Beispiel. Seien s1 =(1, #0), s2 =(0, 01), s3 = (#0, 0), s4 = (01#, #) die Dominosteintypen eines Dominospieles über {0, 1, #}. Die graphische Darstellung der Dominosteintypen s1, s2, s3 und s4 ist in Abbildung 5.16 gegeben. Dieses Dominospiel hat eine Lösung, und zwar die Folge s2,s1,s3,s2,s4. Mit dieser Folge bekommen wir die Darstellung des Wortes 01#0001# (Abbildungen 5.17 und 5.18). Für das Dominospiel s1 = (00, 001), s2 =(0, 001), s3 =(1, 11) gibt es keine Lösung, weil alle zweiten Elemente der Dominosteintypen länger sind als die entsprechenden ersten Elemente. 150 5 Berechenbarkeit 1 0 00 #0 #0 0101 # 01# s1 s2s2 s3 s4 Abbildung 5.17 11 11 0000 0000 ## ## s1 s2s2 s3 s4 Abbildung 5.18 Dieses Dominospiel nennt man das Post’sche Korrespondenzproblem, und seine formale Deﬁnition als Entscheidungsproblem ist wie folgt. Deﬁnition 5.8. Sei Σ ein Alphabet. Eine Instanz des Post’schen Korrespondenz- problems über Σ ist ein Paar (A, B), wobei A = w1,...,wk, B = x1,...,xk für ein k ∈ N −{0}, wi,xi ∈ Σ+ für i =1,...,k. Für jedes i ∈{1,...,k} wird (wi,xi) ein Dominosteintyp genannt. Wir sagen, dass die Instanz (A, B) des Post’schen Korrespondenzproblems eine Lösung hat, falls m ∈ N −{0} und i1,i2,...,im ∈{1,...,k} existieren, so dass wi1wi2 ...wim = xi1xi2 ...xim . Das Post’sche Korrespondenzproblem (PKP) ist zu entscheiden, ob eine gegebene Instanz des PKP eine Lösung hat oder nicht. Lemma 5.9. Falls eine Instanz (A, B) des PKP eine Lösung hat, dann hat (A, B) un- endlich viele Lösungen. Beweis. Falls i1,i2,...,ik eine Lösung der Instanz (A, B) ist, dann ist (i1,i2,...,ik)j eine Lösung für jedes j ∈ N −{0}. \u0002 Aufgabe 5.23. Deﬁnieren Sie das PKP als ein Entscheidungsproblem (Σbool,LPKP) für eine Sprache LPKP ⊆ (Σbool) ∗. Beweisen Sie LPKP ∈LRE. Aufgabe 5.24. Beweisen Sie, dass die Instanz ((10, 011, 101), (101, 11, 011)) des PKP keine Lösung hat. Aufgabe 5.25. Hat die Instanz ((1, 1110111, 101), (111, 1110, 01)) des PKP eine Lösung? 5.5 Das Post’sche Korrespondenzproblem 151 Unser Ziel ist es zu zeigen, dass das PKP unentscheidbar ist. Die Idee des Beweises basiert auf der Tatsache, dass unser Dominospiel eine genügende Ausdrucksstärke hat, um Berechnungen von Turingmaschinen simulieren zu können. Aus rein technischen Gründen betrachten wir zuerst eine Spezialform des PKP, bei der das Dominospiel immer mit dem ersten Dominosteintyp anfangen muss. Deﬁnition 5.9. Sei Σ ein Alphabet. Eine Instanz des modiﬁzierten Post’schen Korrespondenzproblems über Σ ist ein Paar (C, D), wobei C = u1,...,uk, D = v1,...,vk für ein k ∈ N −{0}, ui,vi ∈ Σ+ für i =1,...,k. Wir sagen, dass die Instanz (C, D) des modiﬁzierten Post’schen Korrespondenzproblems eine Lösung hat, falls m ∈ N und j1,j2,...,jm ∈{1,...,k} existieren, so dass u1uj1uj2 ...ujm = v1vj1vj2 ...vjm. Das modiﬁzierte Post’sche Korrespondenzproblem (MPKP) ist zu entscheiden, ob eine gegebene Instanz des MPKP über einem beliebigen Σ eine Lösung hat. Betrachten wir folgende Instanz (A, B)des PKP. Seien A =0, 11, 1, B = 001, 1, 11, das heißt s1 =(0, 001), s2 = (11, 1) und s3 =(1, 11). Oﬀensichtlich ist s2s3 eine Lösung des PKP. Wenn wir (A, B) als eine Instanz des MPKP betrachten, dann stellen wir fest, dass (A, B) keine Lösung hat. Dies bedeutet, dass es für ein gegebenes Dominospiel (A, B) wichtig ist, ob (A, B) als eine Instanz des PKP oder des MPKP zu betrachten ist. Aufgabe 5.26. Beweisen Sie, dass die Instanz ((0, 11, 1), (001, 1, 11)) des MPKP keine Lösung hat. Lemma 5.10. Falls das PKP entscheidbar ist, dann ist auch das MPKP entscheidbar. Beweis. Sei (A, B) eine Instanz des MPKP. Der Übersicht halber schreiben wir diese als MPKP(A, B). Wir konstruieren nun eine Instanz (C, D)des PKP (die wir von nun an als PKP(C, D) schreiben), so dass MPKP(A, B) hat eine Lösung ⇐⇒ PKP(C, D) hat eine Lösung. Sei A = w1,...,wk, B = x1,...,xk, k ∈ N −{0}, wi,xi ∈ Σ∗ für i =1,...,k und ein Alphabet Σ. Seien ¢, $ /∈ Σ. Wir konstruieren PKP(C, D) über dem Alphabet Σ1 = Σ ∪{¢, $}. Zuerst deﬁnieren wir zwei Homomorphismen hL und hR von Σ ∗ nach Σ ∗ 1 wie folgt. Für alle a ∈ Σist hL(a)= ¢a und hR(a)= a¢. Wir sehen, dass hL das Symbol ¢ links neben jeden Buchstaben setzt und hR das Symbol ¢ rechts neben jeden Buchstaben setzt. Zum Beispiel ist für das Wort 0110 hL(0110) = ¢0¢1¢1¢0 und hR(0110) = 0¢1¢1¢0¢. 152 5 Berechenbarkeit Wir setzen C = y1,y2,...,yk+2 und D = z1,z2,...,zk+2,wobei y1 = ¢hR(w1),z1 = hL(x1), y2 = hR(w1),z2 = hL(x1), y3 = hR(w2),z3 = hL(x2), ... ... yi+1 = hR(wi),zi+1 = hL(xi), ... ... yk+1 = hR(wk),zk+1 = hL(xk), yk+2 =$,zk+2 = ¢$. Zum Beispiel ist für die Instanz ((0, 11, 1), (001, 1, 11)) des MPKP die entsprechende Instanz des PKP ((¢0¢, 0¢, 1¢1¢, 1¢, $), (¢0¢0¢1, ¢0¢0¢1, ¢1, ¢1¢1, ¢$)). Es ist oﬀensichtlich, dass eine TM (ein Algorithmus) die Konstruktion von PKP(C, D) aus MPKP(A, B) realisieren kann. Es bleibt also zu zeigen, dass entweder sowohl MPKP(A, B) als auch PKP(C, D) Lösungen besitzen oder beide keine Lösung haben. 1. Zuerst beweisen wir, dass man aus jeder Lösung für MPKP(A, B) eine Lösung für PKP(C, D) bestimmen kann. Sei i1,i2,...,im eine Lösung für MPKP(A, B). Daher ist u = w1wi1wi2 ...wim = x1xi1xi2 ...xim . Die entsprechende Indexfolge 2,i1 +1,i2 +1,...,im + 1 für PKP(C, D) entspricht der Anwendung von hR auf w1wi1wi2 ...wim und hL auf x1xi1xi2 ...xim . Damit ist ¢hR(u)= ¢y2yi1+1 ...yim+1 = z2zi1+1 ...zim+1¢ = hL(u)¢. Daher unterscheiden sich hR(u)= y2yi1+1 ...yim+1 und hL(u)= z2zi1+1 ...zim+1 nur in dem ersten Symbol ¢ und dem letzten Symbol ¢. Weil in der Konstruktion y1 = ¢y2 und z1 = z2 gilt, ersetzen wir den ersten Index 2 durch 1. So bekommen wir die Folge 1,i1 +1,i2 +1,...,im + 1, für die y1yi1+1yi2+1 ...yim+1 = z1zi1+1zi2+1 ...zim+1¢ gilt. Um eine Lösung des PKP zu erhalten, konkatenieren wir von rechts den (k + 2)-ten Dominostein ($, ¢$). Damit gilt y1yi1+1yi2+1 ...yim+1yk+2 = z1zi1+1zi2+1 ...zim+1zk+2 und somit ist 1,i1 +1,i2 +1,...,im +1,k + 2 eine Lösung von PKP(C, D). 2. Wir müssen noch beweisen, dass die Existenz einer Lösung für PKP(C, D) die Existenz einer Lösung für MPKP(A, B) impliziert. Zuerst bemerken wir, dass 5.5 Das Post’sche Korrespondenzproblem 153 alle zi mit ¢ anfangen und von den yi’s nur y1 mit ¢ anfängt. Damit muss jede Lösung für PKP(C, D) mit dem ersten Dominostein beginnen. Weil die einzigen zwei Dominosteine mit gleichem letzten Symbol die (k + 2)-ten Dominosteintypen sind, muss jede Lösung von PKP(C, D) mit dem Index k + 2 enden. Sei 1,j1,j2,...,jm,k + 2 eine Lösung für PKP(C, D). Wir behaupten, dass 1,j1 − 1,j2 − 1,...,jm − 1 die Lösung von MPKP(A, B) ist. Die Begründung ist, dass die Entfernung der Symbo- le ¢ und $ aus y1yj1yj2 ...yjm yk+2 zum Wort w1wj1−1wj2−1 ...wjm−1 führt, und die Entfernung der Symbole ¢ und $ aus z1zj1zj2 ... zjm zk+2 in x1xj1−1xj2−1 ...xjm−1 resultiert. Weil 1,j1,...,jm,k + 2 eine Lösung von PKP(C, D) ist, gilt y1yj1yj2 ...yjmyk+2 = z1zj1zj2 ...zjmzk+2. Dies impliziert w1wj1−1wj2−1 ...wjm−1 = x1xj1−1xj2−1 ...xjm−1. Damit ist 1,j1 − 1,j2 − 1,...,jm − 1 eine Lösung von MPKP(A, B). \u0002 Aufgabe 5.27. Beweisen Sie, dass die Entscheidbarkeit von MPKP die Entscheidbarkeit von PKP impliziert. Wir beweisen jetzt die Unentscheidbarkeit von MPKP, indem wir zeigen, dass man mit diesem Dominospiel die Berechnung einer TM auf einer Eingabe simulieren kann. Lemma 5.11. ∗ Die Entscheidbarkeit von MPKP impliziert die Entscheidbarkeit von LU. Beweis. Sei x ∈{0, 1, #}∗. Wir konstruieren eine Instanz (A, B) des MPKP, so dass x ∈ LU ⇐⇒ (A, B) hat eine Lösung. Wenn x nicht die Form Kod(M )#w hat (das heißt x/∈ LU), dann setzen wir A = 0 und B = 1. Es ist klar, dass ein Dominospiel mit einem einzigen Dominosteintyp (0, 1) keine Lösung hat. Sei x = Kod(M )#w für eine TM M =(Q, Σ, Γ,δ,q0,qaccept,qreject) und ein Wort w ∈ (Σbool) ∗. Gemäß Aufgabe 4.12 dürfen wir voraussetzen, dass M in jedem Schritt den Kopf bewegt. Wir beschreiben die Konstruktion von (A, B)zu M und w in vier Schritten. Jeder Schritt bestimmt eine Gruppe von Dominosteintypen mit speziﬁscher Bedeutung. Die grobe Idee ist, mit der B-Liste die Berechnung von M auf w zu simulieren und mit der A-Liste die Berechnung mit einer Verspätung von einer Konﬁguration zu simulieren. Dadurch kann man Dominosteintypen benutzen, bei denen das erste Element das Argument (den Zustand und das gelesene Symbol) bestimmt und das zweite Element die Nachfolgekonﬁguration entsprechend der δ-Funktion beschreibt. Durch spezielle Dominosteintypen wird die Möglichkeit gegeben, die Verspätung von A nachzuholen, wenn die Berechnung in qaccept endet. Sei \u0002 ein neues Symbol, das nicht in Γ enthalten ist. 154 5 Berechenbarkeit 1. Die erste Gruppe enthält nur einen Dominosteintyp (\u0002, \u0002q0¢w\u0002). Dieser Dominostein startet die Simulation der Berechnung von M auf w.Unten steht die Ausgangskonﬁguration von M auf w und oben haben wir die Verspätung q0¢w\u0002. Da dieser Dominosteintyp der erste ist, muss jede zulässige Lösung mit ihm beginnen. 2. Die zweite Gruppe enthält folgende |Γ| + 1 Dominosteintypen: (X, X) für alle X ∈ Γ, (\u0002, \u0002). Diese Gruppe benutzt man zum Kopieren der Teile von Konﬁgurationen, die sich in einem Schritt nicht ändern. 3. Die dritte Gruppe dient zur Simulation der Berechnungsschritte von M auf w. Für alle q ∈ Q −{qaccept,qreject}, p ∈ Q, X, Y, Z ∈ Γ konstruieren wir folgende Dominosteintypen: (qX, Y p), falls δ(q, X)=(p, Y, R), (ZqX, pZY ), falls δ(q, X)=(p, Y, L), (q \u0002,Y p\u0002), falls δ(q, ␣)=(p, Y, R), (Zq \u0002,pZY \u0002), falls δ(q, ␣)=(p, Y, L). Dies ermöglicht es uns, oben die Konﬁguration von unten zu kopieren und unten mit dem Vorsprung die Nachfolgekonﬁguration zu generieren. 4. Die vierte Gruppe von Dominosteinen ermöglicht es, die unten entwickelte Berech- nung oben nachzuholen, falls unten die Berechnung in dem akzeptierenden Zustand qaccept geendet hat. Mit folgenden Dominosteintypen kann der Zustand qaccept die Symbole aus Γ „schlucken“. Für alle X, Y ∈ Γ konstruieren wir (XqacceptY, qaccept), (Xqaccept,qaccept), (qacceptY, qaccept). Wenn qaccept unten alle Bandsymbole geschluckt hat, ist der Vorsprung unten auf qaccept \u0002 geschrumpft. Der folgende Dominosteintyp (qaccept \u0002\u0002, \u0002) ermöglicht dann das deﬁnitive Nachholen des Wortinhaltes von unten bei dem Wort oben. Illustrieren wir die Konstruktion einer MPKP-Instanz (A, B) für die TM M =({q0,q1, qaccept,qreject}, {0, 1}, {0, 1, ¢, ␣},δM ,q0,qaccept,qreject), die in Abbildung 5.19 dargestellt ist. Die konstruierten Dominosteintypen für die Eingabe 01 sind die folgenden: 5.5 Das Post’sche Korrespondenzproblem 155 0 → 0, R 0 → 0, R 1 → 1, R1 → 1, R ¢ → ¢, R ␣ → ␣, L␣ → ␣, L q0 q1 qaccept qreject Abbildung 5.19 (i) Die erste Gruppe: (\u0002, \u0002q0¢01\u0002). (ii) Die zweite Gruppe: (0, 0), (1, 1), (¢, ¢), (␣, ␣), (\u0002, \u0002). (iii) Die dritte Gruppe: (q01, 1q0), (q00, 0q1), (q0¢, ¢q1), (0q0␣,qaccept0␣), (1q0␣,qaccept1␣), (¢q0␣,qaccept¢␣), (␣q0␣,qaccept␣␣), (0q0 \u0002,qaccept0␣\u0002), (1q0 \u0002,qaccept1␣\u0002), (¢q0 \u0002,qaccept¢␣\u0002), (␣q0 \u0002,qaccept␣␣\u0002), (q11, 1q1), (q10, 0q0), (0q1␣,qreject0␣), (1q1␣,qreject1␣), (¢q1␣,qreject¢␣), (␣q1␣,qreject␣␣), (0q1 \u0002,qreject0␣\u0002), (1q1 \u0002,qreject1␣\u0002), (¢q1 \u0002,qreject¢␣\u0002), (␣q1 \u0002,qreject␣␣\u0002). (iv) Die vierte Gruppe: (0qaccept0,qaccept), (0qaccept1,qaccept), (0qaccept¢,qaccept), (0qaccept␣,qaccept), (1qaccept0,qaccept), (1qaccept1,qaccept), (1qaccept¢,qaccept), (1qaccept␣,qaccept), (¢qaccept0,qaccept), (¢qaccept1,qaccept), (¢qaccept¢,qaccept), (¢qaccept␣,qaccept), (␣qaccept0,qaccept), (␣qaccept1,qaccept), (␣qaccept¢,qaccept), (␣qaccept␣,qaccept), 156 5 Berechenbarkeit q0q0q0 q0q0q0 q1 q1 00000 0000 11111 1111 ¢¢¢¢¢ ¢¢¢¢ \u0002 \u0002 \u0002\u0002\u0002\u0002\u0002 \u0002\u0002\u0002\u0002 qaccept ··· Abbildung 5.20 0 0 1 1¢ ¢¢ ¢ \u0002 \u0002 \u0002 \u0002 \u0002 \u0002 \u0002 \u0002 \u0002 \u0002 qaccept qaccept qaccept qaccept qaccept qaccept ··· Abbildung 5.21 (0qaccept,qaccept), (1qaccept,qaccept), (¢qaccept,qaccept), (␣qaccept,qaccept), (qaccept0,qaccept), (qaccept1,qaccept), (qaccept¢,qaccept), (qaccept␣,qaccept), (qaccept \u0002\u0002, \u0002). Der Anfang der Simulation der Berechnung von M auf 01 durch das Dominospiel (A, B) ist in Abbildung 5.20 gezeigt. In Abbildung 5.21 ist die Phase des Schrumpfens gezeigt, in der der Wortinhalt oben den Wortinhalt unten nachholt. Es ist oﬀensichtlich, dass (A, B) aus einer gegebenen TM M und einem Wort w algorithmisch erzeugt werden kann. Es bleibt zu beweisen, dass die folgende Äquivalenz gilt: M akzeptiert w ⇐⇒ (A, B) hat eine Lösung. Um unnötige technische Details zu vermeiden, bleiben wir bei einer halbinformellen Begründung. Wenn w ∈ L(M ), dann existiert eine akzeptierende Berechnung von M auf w. Zu dieser Berechnung kann man eine Lösung für (A, B) wie folgt bauen. Die Lösung beginnt mit dem Dominostein (\u0002, \u0002q0¢w\u0002) der ersten Gruppe. Dann benutzen wir Dominosteine aus Gruppe 2, um die Symbole der Konﬁgurationen zu kopieren, die im nächsten Schritt unverändert bleiben. Die Dominosteine aus Gruppe 3 werden benutzt, um die Nachfolgekonﬁguration unten zu erzeugen. Nach jedem Symbol \u0002 ist das Wort unten um genau eine Konﬁguration länger als das Wort oben, das ein Präﬁx des Wortes unten ist. Wenn man unten die akzeptierende Konﬁguration mit qaccept erreicht, ermöglichen die Dominosteine der vierten Gruppe dem Wort oben, das Wort unten nachzuholen und so eine Lösung für (A, B) zu erhalten. Sei w/∈ L(M ). Jede Lösung für (A, B) muss mit dem Dominostein (\u0002, \u0002q0¢w\u0002)der ersten Gruppe anfangen. Weil die Steine der Gruppe 2 und 3 nur solche Änderungen der letzten Konﬁguration unten ermöglichen, die einem Berechnungsschritt entsprechen, wird der Zustand qaccept nie als Symbol im Wort unten auftreten. Dann bleibt aber der untere Teil immer länger als der obere Teil der gelegten Dominosteine. Also gibt es keine Lösung für (A, B). \u0002 5.6 Die Methode der Kolmogorov-Komplexität 157 Aufgabe 5.28. Schreiben Sie eine Instanz (A, B) des MPKP zu der TM in Abbildung 4.6. Aufgabe 5.29. Sei (A, B) eine Instanz des MPKP für eine TM M und ein Wort w nach der Konstruktion von Lemma 5.11. Beweisen Sie durch Induktion folgende Aussage: Falls w ∈ L(M ), dann existiert eine Folge von Indizes, so dass (i) der untere Teil der gelegten Dominosteine die komplette Berechnung von M auf w enthält, wobei die einzelnen Konﬁgurationen durch das Symbol # getrennt sind, und (ii) der obere Teil der gelegten Dominosteine ein echtes Präﬁx des unteren Teils ist, der die ganze Berechnung bis auf die letzte Konﬁguration enthält. Satz 5.10. Das PKP ist unentscheidbar. Beweis. Nach Lemma 5.11 ist das MPKP unentscheidbar und nach Lemma 5.10 ist das PKP mindestens so schwer wie das MPKP. \u0002 Aufgabe 5.30.∗ Betrachten Sie eine beschränkte Version des PKP, bei dem alle Dominosteinty- pen über dem festen einelementigen Alphabet {0} deﬁniert sind. Ist dieses Problem entscheidbar? 5.6 Die Methode der Kolmogorov-Komplexität In Abschnitt 5.2 haben wir die Methode der Diagonalisierung angewendet, um das erste algorithmisch unlösbare Problem Ldiag zu ﬁnden. Damit haben wir einen Startpunkt für den Aufbau der Theorie der Berechenbarkeit erzeugt, von dem aus man mittels der Methode der Reduktion (Abschnitte 5.3 bis 5.5) die Unentscheidbarkeit weiterer Probleme beweisen konnte. Die Zielsetzung dieses Abschnitts ist es, einen alternativen Weg für den Aufbau der Theorie der Berechenbarkeit anzudeuten. Ohne jede Voraussetzung über die Existenz eines algorithmisch unlösbaren Problems benutzen wir die Theorie der Kolmogorov-Komplexität, um zu zeigen, dass kein Algorithmus existiert, der die Kolmogorov-Komplexität K (x) des Wortes x für jedes x ∈ (Σbool)∗ berechnen kann. Ausgehend von diesem Startpunkt kann man dann wieder mittels der Reduktionsmethode die Unentscheidbarkeit aller in den vorigen Abschnitten vorgestellten unentscheidbaren Sprachen beweisen. Satz 5.11. Das Problem, für jedes x ∈ (Σbool)∗ die Kolmogorov-Komplexität K (x) von x zu berechnen, ist algorithmisch unlösbar. 9 Beweis. Wir beweisen dies indirekt. Sei A ein Algorithmus, der für jedes x ∈ (Σbool) ∗ das Wort K (x) berechnet. Sei xn das erste Wort bezüglich der kanonischen Ordnung über Σbool mit K (xn) ≥ n. Wir beschreiben nun einen Algorithmus Bn, n ∈ N −{0},der (i) A als ein Teilprogramm benutzt, und (ii) für die leere Eingabe λ das Wort xn generiert. 9Wenn man K als eine Funktion von (Σbool)∗ nach (Σbool)∗ statt als Funktion von (Σbool)∗ nach N betrachtet (dies bedeutet, dass K (x) die binäre Kodierung der Kolmogorov-Komplexität von x wäre), dann besagt Satz 5.11, dass die Funktion K nicht total rekursiv ist. 158 5 Berechenbarkeit Bn arbeitet wie folgt. Bn: begin x := λ; Berechne K (x) mit dem Algorithmus A; while K (x) <n do begin x := Nachfolger von x in kanonischer Ordnung; Berechne K (x) mit dem Algorithmus A; end; output(x); end Oﬀensichtlich berechnet Bn das Wort xn für alle n ∈ N. Wir beobachten, dass alle Algorithmen Bn bis auf die Zahl n identisch sind. Sei c die Länge des Maschinencodes von Bn bis auf n. Dann ist die binäre Länge von Bn höchstens ⌈log2 n⌉ + c für alle n ∈ N und für eine Konstante c unabhängig von n, da der Algorithmus A eine konstante Größe hat. Weil Bn das Wort xn generiert, gilt K (xn) ≤⌈log2 n⌉ + c für alle n ∈ N −{0}. Nach der Deﬁnition von xn gilt aber K (xn) ≥ n für alle n ∈ N −{0}. Die Ungleichung ⌈log2 n⌉ + c ≥ K (xn) ≥ n kann höchstens für endlich viele Zahlen n ∈ N −{0} gelten und somit haben wir einen Widerspruch zu der Voraussetzung, dass ein Algorithmus A zur Berechnung von K (x) existiert. \u0002 Aufgabe 5.31. Beweisen Sie, dass das Problem, für jede Zahl n ∈ N −{0} das erste Wort xn mit K (xn) ≥ n zu generieren, ein algorithmisch unlösbares Problem ist. Um zu zeigen, dass man mittels der Reduktionsmethode zum Beweis der Unentscheid- barkeit der grundlegenden Sprachen wie LU, LH, Lempty usw. übergehen kann, reduzieren wir die Berechnung der Funktion K auf das Halteproblem. Das folgende Lemma bietet einen alternativen Beweis der Tatsache LH /∈LR aus Satz 5.8. Lemma 5.12. Falls LH ∈LR, dann existiert ein Algorithmus zur Berechnung der Kolmogorov-Komplexität K (x) für jedes x ∈ (Σbool) ∗. Beweis. Sei LH ∈LR und sei H ein Algorithmus, der LH entscheidet. Der folgende Algorithmus A (Abbildung 5.22) berechnet K (x) für jedes x ∈ (Σbool) ∗. 5.6 Die Methode der Kolmogorov-Komplexität 159 x ∈{0, 1}∗ x Ein Generator für wi Testen, ob Testen, ob wi =Kod(M )für ein Pascal-Programm M λ Kod(M )= wi TM H w1,w2,w3,... L(H)= LH NEIN NEIN NEIN JA JA Simulation von M auf λ M (λ) M (λ)= x |Kod(M )| i = i +1 M hält nicht auf λ Ein Algorithmus A mit A(x)= K(x) für alle x ∈{0, 1}∗ Abbildung 5.22 160 5 Berechenbarkeit A generiert in kanonischer Folge die Wörter w1,w2,w3,.... Für jedes i überprüft A,ob wi die binäre Kodierung eines Pascal-Programms M ist (Abbildung 5.22). Falls wi = Kod(M ) für ein Pascal-Programm M ,wendet A das Teilprogramm H an, um festzustellen, ob M auf dem Wort λ hält. Falls M auf dem Wort λ hält, simuliert A die Arbeit von M auf λ (Abbildung 5.22). Falls M mit der Ausgabe u = M (λ) endet, überprüft A,ob u = x. Falls u = x, dann gibt A die Ausgabe K (x)= |wi| aus. Wenn |wi| die Ausgabe von A für die Eingabe x ist, dann ist wi das kürzeste Wort mit der Eigenschaft wi = Kod(M ) für ein Pascal-Programm M , und M generiert x. Damit hat A die Kolmogorov-Komplexität von x tatsächlich bestimmt. \u0002 Aufgabe 5.32. Beweisen Sie folgende Aussagen: (a) Falls LU ∈LR, dann existiert ein Algorithmus zur Berechnung von K (x) für jedes x ∈ (Σbool) ∗. (b) Falls Lempty ∈LR, dann existiert ein Algorithmus zur Berechnung von K (x) für jedes x ∈ (Σbool) ∗. 5.7 Folgen für die Forschung In der Theorie der Berechenbarkeit beweisen wir, dass für einige, leider sogar für ziemlich viele Probleme keine Algorithmen existieren. Was bedeutet dies für die Praxis? Es existiert kein Algorithmus, der die Korrektheit eines gegebenen Programms überprüft. Wenn wir ein Programm zur Steuerung einer Rakete oder eines Flugzeugs haben, müssen wir es aber überprüfen. Damit verlassen wir das Gebiet der Automatisierung und stehen vor der Forschungsaufgabe, einen mathematischen Beweis der Korrektheit dieses Programms zu erstellen. Somit werden die Instanzen algorithmisch unlösbarer Probleme zu Forschungs- objekten. Jede einzelne Instanz eines unlösbaren Problems kann man als eine einzelne Forschungsaufgabe betrachten, zum Beispiel, einen Fehler in einem gegebenen Compiler zu ﬁnden oder zu beweisen, dass dieser Compiler immer korrekt arbeitet. Natürlich ist hier die Schwierigkeit, dass es sehr viel länger dauern kann, einen Korrektheitsbeweis eines langen Programms herzuleiten, als das Programm selbst zu erstellen. Dies ist eine der Herausforderungen in dem Gebiet der Programmveriﬁkation. Was wir hier ansprechen wollen, ist etwas viel Problematischeres, das sogar als überra- schend betrachtet werden kann. Ist es möglich, dass ein Programm zwar korrekt ist, aber nicht beweisbar korrekt, weil kein mathematischer Beweis seiner Korrektheit existiert? Wenn dies möglich wäre, dann hätten wir eine Grenze des mittels der Mathematik Er- forschbaren, weil die Mathematik zu schwach wäre, um festzustellen, ob ein Programm korrekt ist oder nicht. Die Antwort auf diese Frage ist leider „Ja“, und wir zeigen hier, dass es konkrete gültige mathematische Aussagen gibt, für die in der Mathematik kein Beweis der Gültigkeit existiert und somit auch nicht gefunden werden kann. Dies unterstreicht die Aussage von Gödel: Man muss die Mathematik durch die axiomatische Einführung neuer Begriﬀe so stärken, dass man in ihr Tatsachen beweisen kann, die vorher nicht beweisbar waren. Also ist die Mathematik ein sich ständig entwickelndes Forschungsinstrument. Wir nutzen nun die Kolmogorov-Komplexität, um zu zeigen, dass man in der Mathematik konkrete korrekte Behauptungen (Sätze) formulieren kann, für die in der Mathematik 5.7 Folgen für die Forschung 161 keine Beweise ihrer Gültigkeit existieren. Bevor wir damit anfangen, müssen wir noch einmal darüber nachdenken, was wir unter der Mathematik verstehen. Die Mathematik als Forschungsinstrument ermöglicht es uns, die Gültigkeit oder die Ungültigkeit von Aussagen zu beweisen. Der Kernpunkt ist, dass die Korrektheit von Beweisen als Darstellungen der korrekten Argumentation immer überprüfbar ist. Jeden Beweis kann man detailliert auf der Ebene einer formalen Logik führen, in der nur erlaubte und allgemein akzeptierte Argumentationsschritte verwendet werden. Unter dieser Voraussetzung gilt die folgende Aussage: Es existiert ein Algorithmus, der für einen gegebenen Text entscheidet, ob der Text ein gültiger mathematischer Beweis ist oder nicht. (5.1) Die Hypothese (5.1) ist die Folge des Strebens der Mathematik nach absoluter Ex- aktheit und Korrektheit innerhalb ihrer Sprache, also aufbauend auf der Festlegung der begriﬀsbildenden Axiome. Die Mathematiker schreiben Beweise selten auf der Ebene einer elementaren Logik, in der sie algorithmisch überprüfbar wären. Obwohl sie dabei die Metasprache verwenden und größere Gedankenschritte machen, gibt es nie Zweifel unter Mathematikern, ob etwas bewiesen ist oder nicht. Also gilt für diejenigen, denen (5.1) zu formal oder zu eingeschränkt ist, mindestens die folgende Annahme: Die mathematische Community kann sich immer darauf einigen, ob ein gegebener Text ein Beweis einer Behauptung ist oder nicht. (5.2) Jetzt beweisen wir, dass es für alle bis auf endlich viele Wörter keine mathematischen Beweise zur Bestimmung ihrer Kolmogorov-Komplexität gibt. Im Folgenden sei ΣMath ein Alphabet, in dem man alle mathematischen Beweise ausdrücken kann. Satz 5.12. Es existiert eine natürliche Zahl d, so dass für alle n ≥ d kein Beweis für die Behauptung „K (x) ≥ n“ existiert für irgendein Wort x. Beweis. Nehmen wir an, es gibt eine unendliche Folge von natürlichen Zahlen n1,n2,... mit ni <ni+1 für i ∈{1, 2,...}, so dass es jeweils einen Beweis der Behauptung „K(yi) ≥ ni“ für irgendwelche Wörter y1,y2,... gibt. Sei w1,w2,... die Folge von Wörtern aus {0, 1} ∗, für die Folgendes gilt: (i) K (wi) ≥ ni und (ii) wi ist das Wort mit dem in kanonischer Reihenfolge kürzesten Beweis der Tatsache „K (yi) ≥ ni“ für irgendein yi. Wir konstruieren jetzt eine Folge von Algorithmen A1,A2,..., so dass Ai das Wort wi generiert. 162 5 Berechenbarkeit Ai: begin z := λ; I := ni; T := 0; while T =0 do begin Überprüfe (gemäß Annahme (5.1)), ob z ein Beweis der Aussage „K (y) ≥ I“ für ein y ∈{0, 1} ∗ ist; if z ist ein Beweis von „K (y) ≥ I“ then T := 1; else z := kanonischer Nachfolger von z über ΣMath; end; write(y); {Es gilt y = wi.} end Man bemerke, dass es nach Annahme (5.1) für alle i ∈{1, 2,...} mindestens einen Beweis von „K (y) ≥ ni“ gibt für ein y ∈{0, 1} ∗, also terminiert Ai immer. Die unendlich vielen Algorithmen Ai unterscheiden sich nur in dem Parameter ni. Somit gibt es eine Konstante c, so dass K (wi) ≤ log2(ni)+ c für alle i ∈{1, 2,...} gilt. Weil nach der Deﬁnition von wi die untere Schranke K (wi) ≥ ni für alle i ∈{1, 2,...} gilt, erhalten wir ni ≤ K (wi) ≤ log2(ni)+ c (5.3) für alle i ≥ 1. Weil die Ungleichung ni ≤ log2(ni)+ c nur für endlich viele Werte von i gelten kann, erhalten wir einen Widerspruch. Somit kann es nur endlich viele Zahlen ni geben, für die ein Beweis von „K (y) ≥ ni“ für irgendein Wort y existiert. \u0002 Im Beweis von Satz 5.12 haben wir die Annahme (5.1) über die Mathematik verwendet. Wir können aber einen ähnlichen Beweis auch nur mit Hilfe der schwächeren Annahme (5.2) führen. Hierfür deﬁnieren wir für jedes x ∈{0, 1}∗ den Wert MC (x) als die minimale Anzahl von Bits, die die mathematische Community braucht, um x eindeutig zu bestimmen. Wenn wir dann überall in dem Beweis von Satz 5.12 die Kolmogorov-Komplexität K (x) gegen MC (x) austauschen, erhalten wir das Ergebnis, dass MC (x) höchstens für endlich viele Wörter bestimmbar ist. Was haben wir gelernt? Die Mathematik als Forschungsinstrument hat ihre Grenzen. Wir sehen jetzt, dass wir nicht überprüfen können, ob eine genügend lange Folge von Bits zufällig ist. Nun zeigen wir, dass diese Überlegungen auch Grenzen für die Veriﬁkation von Programmen aufzeigen. Satz 5.13. Falls für jedes Programm P ein Beweis der Tatsache „P hält auf λ“ oder „P hält nicht auf λ“ existiert, dann gibt es einen Algorithmus zur Bestimmung von K (x) für jedes x ∈{0, 1}∗. 5.8 Zusammenfassung 163 Aufgabe 5.33. Beweisen Sie Satz 5.13. Aufgabe 5.34. ∗ Beweisen Sie unter der Annahme (5.1), dass eine TM M und ein Wort x ∈ L(M ) existieren, so dass es keinen Beweis der Aussage „x ∈ L(M )“ gibt. 5.8 Zusammenfassung Die beweistechnischen Grundlagen der Theorie der Berechenbarkeit liegen in der Mengen- lehre. Die grundlegenden Konzepte sind • der Vergleich unendlicher Zahlen, • die Methode der Diagonalisierung und • die Methode der Reduktion. Eine Menge A hat eine Mächtigkeit, die mindestens so groß ist wie die Mächtigkeit einer Menge B, wenn eine injektive Abbildung von B nach A existiert. Für jede unendliche Menge A gilt |A| = |A × A| und daher |N| = |Q +|. Die kleinste unendliche Menge ist N, und jede Menge C mit |C|≤|N| heißt abzählbar. Jede Menge B mit |B| > |N| heißt überabzählbar. Die Menge aller Turingmaschinen (Programme) ist abzählbar, und die Menge aller Spra- chen (Probleme) ist nicht abzählbar. Damit sind die meisten Probleme nicht algorithmisch lösbar. Eine Sprache L, die von einer Turingmaschine akzeptiert wird, heißt rekursiv aufzählbar. Eine Sprache L mit L = L(M ) für eine TM M , die auf jeder Eingabe in qaccept oder qreject hält, heißt rekursiv oder auch entscheidbar. Die Diagonalisierungsmethode ermöglicht es uns, eine Sprache Ldiag, Diagonalsprache genannt, zu konstruieren, die nicht rekursiv aufzählbar ist. Mit Hilfe der Methode der Reduktion kann man zeigen, dass einige Probleme bezüglich Rekursivität mindestens so schwer wie Ldiag und damit nicht rekursiv sind. Die wichtigsten Beispiele nichtrekursiver Entscheidungsprobleme sind die universelle Sprache und das Halteproblem. Die universelle Sprache enthält alle Wörter, die eine TM M und ein Wort w kodieren mit w ∈ L(M ). Das Halteproblem ist zu entscheiden, ob für eine TM M und ein Wort w die TM M auf w hält. Der Satz von Rice besagt, dass jedes semantisch nichttriviale Entscheidungsproblem über Turingmaschinen nicht rekursiv ist. Daher gibt es keine Algorithmen zum Testen von Programmen auf Korrektheit und Terminierung. Mit der Methode der Reduktion kann man Beweise der Unentscheidbarkeit auch außerhalb der Welt der Probleme über Turingmaschinen führen. Ein Beispiel hierfür ist das Post’sche Korrespondenzproblem. Die Motivation, die Entscheidbarkeit und Unentscheidbarkeit zu studieren, stammt ursprünglich von dem bekannten Mathematiker David Hilbert. Anfang des 20. Jahrhun- derts formulierte er einen Forschungsplan, dessen Ziel die Entwicklung eines Formalismus war, in dem man alle mathematischen Probleme lösen könnte. Kurt Gödel [Göd31] bewies 1931, dass der Hilbert’sche Plan nicht realisierbar ist, weil jede mathematische Theorie, 164 5 Berechenbarkeit die mindestens die Prädikatenlogik erster Stufe enthält, unentscheidbar ist. Diese funda- mentale Arbeit [Göd31] führte zur Formalisierung des Begriﬀs Algorithmus und legte die Grundlage der Theorie der Berechenbarkeit. Die Unentscheidbarkeit der universellen Sprache hat Turing in [Tur36] bewiesen. In [Ric53] veröﬀentliche Rice den nach ihm benannten Satz. Die Unentscheidbarkeit des Post’schen Korrespondenzproblems wurde von Post in [Pos46] gezeigt. Als weiterführende Literatur über die elementaren Grundlagen der Theorie der Be- rechenbarkeit hinaus empfehlen wir wärmstens die entsprechenden Kapitel in [HU79, HMU06, Sip97]. Wenn man die EE-Reduktion benutzt, um die Äquivalenz zwischen Problemen bezüg- lich der Rekursivität zu deﬁnieren, zerfällt die Menge der nicht-rekursiven Sprachen in unendlich viele Klassen Li, i =1, 2,.... Wenn eine Sprache L in Li ist, bedeutet dies, dass L unentscheidbar bleiben würde, auch wenn wir für alle unentscheidbaren Probleme aus den Klassen L1, L2,..., Li−1 Algorithmen hätten. Interessanterweise gibt es praktische Probleme, die in höhere Klassen dieser Hierarchie gehören. Für eine Vertiefung der Theorie der Berechenbarkeit empfehlen wir die klassischen Bücher von Trakhtenbrot [Tra63] und Rogers [Rog67]. Kontrollaufgaben 1. Erklären Sie das Cantor’sche Konzept zum Vergleich der Mächtigkeiten zweier Mengen. Warum betrachten wir dieses Konzept als natürlich? 2. Was ist der Hauptunterschied zwischen endlichen und unendlichen Mengen? (Oder wie würden Sie eine unendliche Menge deﬁnieren?) 3. Wann ist eine Menge abzählbar? Geben Sie mehrere Beispiele abzählbarer Mengen an. Wann ist ein Sprache rekursiv und wann ist sie rekursiv aufzählbar? 4. Warum ergibt das kartesische Produkt zweier abzählbarer Mengen eine abzählbare Menge? 5. Erklären Sie die Diagonalisierungsmethode. Beweisen Sie, dass es Sprachen gibt, die keine endliche Darstellung haben. 6. Die reellen sowie die rationalen Zahlen liegen auf der Zahlenachse. Beide Zahlenmengen Q und R haben die Eigenschaft, dass zwischen zwei beliebigen Zahlen unendlich viele Zahlen aus dieser entsprechenden Menge liegen. Trotzdem gibt es mehr reelle Zahlen als rationale Zahlen (|Q| < |R|). Wie ist das möglich? Was ist der Hauptunterschied zwischen rationalen und reellen Zahlen bezüglich ihrer Darstellungsmöglichkeiten? 7. Wann sagen wir, dass eine Sprache L1 auf eine Sprache L2 reduzierbar ist? Wie wendet man das Konzept der Reduzierbarkeit an, um zu zeigen, dass eine Sprache nicht rekursiv ist? 8. Sei f (x)= c eine konstante Funktion von N nach N für eine Konstante c ∈ N. Beweisen Sie, dass es für eine gegebene TM (ein gegebenes Programm) nicht entscheidbar ist, ob sie die Funktion f berechnet. 9. Wie baut man eine Instanz des modiﬁzierten Post’schen Korrespondenzproblems für eine gegebene Turingmaschine? 5.8 Zusammenfassung 165 10. Um das erste nichtrekursive Problem zu ﬁnden, wendet man üblicherweise die Diagonali- sierungsmethode an. Wie kann man dies ohne die Diagonalisierungsmethode erreichen? 11. Die Kolmogorov-Komplexität eines Wortes ist wohldeﬁniert. Warum kann man sie nicht algorithmisch für jedes Wort bestimmen? 12. Beweisen Sie folgende Aussagen: (a) Wenn LH ∈LR wäre, dann könnte man für jede Zahl n ∈ N die Kolmogorov- Komplexität K(n) bestimmen. (b) Falls LU rekursiv (entscheidbar) wäre, so wäre K(n) für jede Zahl n ∈ N algorithmisch berechenbar. 13. Betrachten Sie die Sprache LKol = {(w, x) | K(w) ≤ Nummer(x),w, x ∈{0, 1}∗}. Beweisen Sie, dass LKol unentscheidbar ist. Ist LKol rekursiv aufzählbar? Es gibt keinen größeren Schaden als verlorene Zeit. Michelangelo 6 Komplexitätstheorie 6.1 Zielsetzung Die Theorie der Berechenbarkeit liefert uns Methoden zur Klassiﬁzierung von Proble- men bezüglich ihrer algorithmischen Lösbarkeit. Danach ist ein Problem algorithmisch lösbar, wenn ein Algorithmus zur Lösung dieses Problems existiert, und ein Problem ist algorithmisch unlösbar, wenn kein Algorithmus für dieses Problem existiert. Die Komplexitätstheorie ist eine Fortsetzung der Theorie der Berechenbarkeit in dem Sinne, dass sie eine feinere Einteilung der Klasse algorithmisch lösbarer Probleme anstrebt. In den sechziger Jahren, als der Einsatz von Rechnern zur Lösung von Problemen nicht mehr nur auf ein paar Forschungsinstitute beschränkt war, stellte man fest, dass alleine die Existenz eines Algorithmus zur Lösung eines Problems noch keine Garantie für eine erfolgreiche rechnerunterstützte Lösung des Problems ist. Es gab viele praxisrelevante, algorithmisch lösbare Probleme, für die alle entworfenen Programme tagelang auf gegebe- nen Eingaben ohne Erfolg arbeiteten. In solchen Fällen stürzte der Rechner ab, bevor er zu irgendeinem Resultat kam. Dies warf die Frage auf, ob dieser Misserfolg eine Folge unserer Unfähigkeit ist, einen eﬃzienten Ansatz zur Lösung des betrachteten Problems zu ﬁnden, oder aber, ob es sich um eine inhärente Eigenschaft des Problems handelt, die keine eﬃziente algorithmische Lösung des Problems zulässt. Dies führte dazu, dass man anﬁng, die Schwierigkeit der algorithmisch lösbaren Probleme bezüglich des Rechenaufwandes zu messen, um so die Probleme nach deren Schwierigkeitsgrad zu klassiﬁzieren. Die Komplexitätstheorie ist die Theorie der quantitativen Gesetze und Grenzen der algorithmischen Informationsverarbeitung. Diese Theorie hat auch eine physikalische Dimension. Zum Beispiel hält man ein lösbares Problem für „praktisch unlösbar“, wenn die praktische Ausführung eines Algorithmus zur Lösung des Problems mehr Energie bräuchte, als es im ganzen bekannten Universum gibt. Die Hauptziele der Komplexitätstheorie sind (i) die Bestimmung der Berechnungskomplexitäten (Zeitkomplexität als die Anzahl der Rechenoperationen, Speicherplatzkomplexität) konkreter Probleme, (ii) die Speziﬁkation der Klasse „praktisch“ (eﬃzient) lösbarer Probleme und die Ent- J. Hromkovič, Theoretische Informatik, DOI 10.1007/978-3-658-06433-4_6, © Springer Fachmedien Wiesbaden 2014 168 6 Komplexitätstheorie wicklung von Methoden zur Klassiﬁzierung der algorithmisch lösbaren Probleme in „praktisch lösbare“ und „praktisch unlösbare“ und (iii) Vergleiche der Eﬃzienz (der Berechnungsstärke) deterministischer, nichtdeterminis- tischer und zufallsgesteuerter (randomisierter) Algorithmen. In unserer ersten Begegnung mit der Komplexitätstheorie in diesem Kapitel beschränken wir uns auf folgende Zielsetzung. In Abschnitt 6.2 lernen wir, wie man die Komplexi- tätsmaße in dem Berechnungsmodell der Turingmaschine deﬁniert (d. h., wie man die Komplexität misst) und welche Eigenschaften diese Maße haben. In Abschnitt 6.3 deﬁnie- ren wir die grundlegenden Komplexitätsklassen als Klassen von Sprachen, die mit der gegebenen Komplexität entscheidbar sind. Hier diskutieren wir auch die Speziﬁkation der Klasse praktisch lösbarer Probleme. In Abschnitt 6.4 zeigen wir, wie man die Komple- xität nichtdeterministischer Berechnungen misst, und wir deﬁnieren die fundamentalen nichtdeterministischen Komplexitätsklassen. Abschnitt 6.5 ist dem Vergleich der Eﬃzienz deterministischer und nichtdeterministischer Berechnungen gewidmet. Dieser Vergleich be- rührt die philosophischen Grundlagen der Mathematik. Wir zeigen, dass die Komplexität nichtdeterministischer Berechnungen der Komplexität des deterministischen Veriﬁzierens (Korrektheitsprüfung) eines gegebenen mathematischen Beweises entspricht, und dass die deterministische Komplexität der Komplexität der Erstellung eines mathematischen Beweises entspricht. Damit ist die Frage, ob nichtdeterministische Algorithmen eﬃzienter als die deterministischen Algorithmen sein können, äquivalent zu der Frage, ob es einfacher ist, mathematische Beweise algorithmisch zu veriﬁzieren als sie algorithmisch herzustellen. Abschnitt 6.6 stellt das Konzept der NP-Vollständigkeit vor, das uns eine Methode zum Beweisen eines gewissen Schwierigkeitsgrades konkreter Probleme bezüglich praktischer Lösbarkeit liefert. 6.2 Komplexitätsmaße Zur Deﬁnition der Komplexitätsmaße benutzen wir das Modell der Mehrband-Turingma- schine. Die Gründe dafür sind, dass dieses Berechnungsmodell einerseits einfach genug ist und andererseits dem grundlegenden Rechnermodell der von-Neumann-Maschine entspricht. Wie wir später sehen werden, ist das Modell der Mehrband-Turingmaschine für die Komplexitätsmessung robust genug in dem Sinne, dass die fundamentalen Resultate über die derart deﬁnierte Komplexität auch für die Komplexität der Ausführung von Programmen in beliebigen Programmiersprachen gültig sind. Damit sind sie insbesondere von allgemeiner Gültigkeit für die Klassiﬁzierung von Problemen in „praktisch lösbare“ und „praktisch unlösbare“ Probleme. Hier deﬁnieren wir zwei grundlegende Komplexitätsmaße: die Zeitkomplexität und die Speicherplatzkomplexität. Die Zeitkomplexität einer Berechnung entspricht der Anzahl der elementaren Operationen, die in dieser Berechnung ausgeführt werden. Damit steht sie in linearer Beziehung zu der Energie, die die Ausführung der Berechnung auf einem Rechner kosten würde. Die Speicherplatzkomplexität ist die Größe des benutzten Speichers, ausgedrückt in der Anzahl der gespeicherten Rechnerwörter. In den auf Turingmaschinen basierenden Modellen bestimmt das Arbeitsalphabet die Größe des Rechnerwortes, weil die 6.2 Komplexitätsmaße 169 Symbole des Arbeitsalphabetes genau die erlaubten Inhalte der Rechnerwörter darstellen. Deﬁnition 6.1. Sei M eine Mehrband-Turingmaschine oder TM, die immer hält. Sei Σ das Eingabealphabet von M . Sei x ∈ Σ∗ und sei D = C1,C2,...,Ck die Berechnung von M auf x. Dann ist die Zeitkomplexität TimeM (x) der Berechnung von M auf x deﬁniert durch TimeM (x) = k − 1, also durch die Anzahl der Berechnungsschritte in D. Die Zeitkomplexität von M ist die Funktion TimeM : N → N, deﬁniert durch TimeM (n) = max{TimeM (x) | x ∈ Σn}. Wir bemerken, dass TimeM (n) für jedes n ∈ N die minimale Zeitkomplexität ist, mit der M jede Probleminstanz der Länge n löst. Anders ausgedrückt ist TimeM (n) die Zeitkomplexität der längsten Berechnung auf Eingaben der Länge n (auf der „schwersten“ Probleminstanz aus Σn). Deswegen nennt man diese Art der Komplexitätsmessung „die Komplexität im schlechtesten Fall“. Eine andere Möglichkeit wäre zum Beispiel, die durchschnittliche Komplexität auf Wörtern der Länge n zu betrachten. Die zwei Hauptgründe für die Art der Messung aus Deﬁnition 6.1 sind folgende. Man will in vielen Fällen eine maximale Sicherheit haben, und die Komplexität im schlechtesten Fall gibt uns die Garantie, dass jede Eingabe (Probleminstanz) der Größe n durch M in TimeM (n) gelöst wird. Der andere praktische Grund ist, dass die Analyse der Komplexität eines Algorithmus im schlechtesten Fall meist einfacher ist als eine Komplexitätsanalyse im durchschnittlichen Fall. Wir werden uns deswegen in diesem Buch auf die Komplexität im schlechtesten Fall beschränken. Deﬁnition 6.2. Sei k ∈ N −{0}. Sei M eine k-Band-Turingmaschine, die immer hält. Sei C =(q, x, i, α1,i1,α2,i2,...,αk,ik) mit 0 ≤ i ≤|x| +1 und 0 ≤ ij ≤|αj| für j =1,...,k eine Konﬁguration von M . Die Speicherplatzkomplexität von C ist 1 SpaceM (C) = max{|αi|| i =1,...,k}. Sei C1,C2,...,Cl die Berechnung von M auf x. Die Speicherplatzkomplexität von M auf x ist SpaceM (x) = max{SpaceM (Ci) | i =1,...,l}. Die Speicherplatzkomplexität von M ist die Funktion SpaceM : N → N, deﬁniert durch SpaceM (n) = max{SpaceM (x) | x ∈ Σn}. 1Man bemerke, dass die Speicherplatzkomplexität nicht von der Länge des Eingabewortes x abhängt. 170 6 Komplexitätstheorie Es könnte überraschen, dass wir die Speicherplatzkomplexität einer Konﬁguration als die maximale beschriftete Länge eines Arbeitsbandes statt der Summe der beschrifteten Längen aller Arbeitsbänder deﬁniert haben. Im Prinzip ist es egal, welche der beiden Möglichkeiten man zur Deﬁnition der Speicherplatzkomplexität verwendet. Lemma 4.2 sagt uns, dass man k Bänder mit einem Band simulieren kann. Dabei ist die beschriftete Länge des Bandes genau das Maximum der beschrifteten Längen der simulierten Bänder. 2 Diese Beobachtung führt uns zur folgenden Aussage. Lemma 6.1. Sei k eine positive ganze Zahl. Für jede k-Band-TM A, die immer hält, existiert eine äquivalente 1-Band-TM B, so dass SpaceB(n) ≤ SpaceA(n). Der Grund für diese Eigenschaft der Speicherplatzkomplexität ist, dass die Mächtigkeit des Arbeitsalphabetes von M (die Länge der Rechnerwörter) keinen Einﬂuss auf SpaceM (n) hat. Daher ist diese Deﬁnition nicht zur Untersuchung von Unterschieden in der Größe eines konstanten multiplikativen Faktors geeignet. Lemma 6.2. Sei k eine positive ganze Zahl. Für jede k-Band-TM A existiert eine k- Band-TM B so, dass L(A)= L(B) und SpaceB(n) ≤ SpaceA(n) 2 +2. Beweis. Wir liefern nur die Idee des Beweises. Sei ΓA das Arbeitsalphabet von A. Wir konstruieren das Arbeitsalphabet ΓB von B,sodassΓB alle Symbole aus ΓA × ΓA enthält. Wenn α1,α2,...,αm der Inhalt des i-ten Bandes von A ist, i ∈{1, 2,...,k}, und der Kopf auf αj zeigt, enthält das i-te Band von B das Wort ¢(α1 α2 )(α3 α4 ) ... (αm−1 αm ) der Länge m 2 + 1, falls m gerade ist, und ¢(α1 α2 )(α3 α4 ) ... (αm ␣ ) der Länge m+1 2 +1 ≤ m 2 +2, falls m ungerade ist. Der Kopf des Bandes zeigt auf das Tupel, das αi enthält, 3 und in dem Zustand speichert B, auf welches der zwei Symbole in dem Tupel der Kopf auf dem i-ten Band von A zeigt. Oﬀensichtlich ist es für B kein Problem, die Schritte von A einen nach dem anderen zu simulieren, und die Speicherplatzkomplexität von B ist höchstens SpaceA(n)/2+2. \u0002 2Genau genommen muss man wegen des neuen Randsymbols ¢ die Speicherplatzkomplexität um 1 erhöhen. Dies kann man jedoch umgehen, indem man die Simulation aus dem Beweis von Lemma 4.2 leicht modiﬁziert und ¢ in das erste Symbol auf dem Arbeitsband integriert. 3Dies ist ( αi αi+1), falls i ungerade, und (αi−1 αi ), falls i gerade ist. 6.2 Komplexitätsmaße 171 Durch iterative Anwendung von Lemma 6.2 ist es möglich, für jede Konstante d und jede Mehrband-Turingmaschine M eine äquivalente Mehrband-Turingmaschine zu bauen, die eine d-mal kleinere Speicherplatzkomplexität als M hat. Die Situation bei der Zeitkomplexität ist ähnlich. Bei ihrer Messung geht die Größe des Arbeitsalphabetes auch nicht ein. Transitionen über mächtigen Alphabeten entsprechen aber zweifellos komplizierteren Operationen als Transitionen über kleineren Alphabeten. Dies führt auch zu der Möglichkeit, die Arbeit von Mehrband-Turingmaschinen um einen konstanten Faktor zu beschleunigen, wie es in der folgenden Aufgabe formuliert wird. Aufgabe 6.1. Beweisen Sie die folgende Behauptung: Sei M eine Mehrband-Turingmaschine, die immer hält. Dann existiert eine zu M äquivalente Mehrband-Turingmaschine A mit TimeA(n) ≤ TimeM (n) 2 +2n. Die Aussagen von Lemma 6.2 und Aufgabe 6.1 zeigen, dass die vorgestellte Art der Komplexitätsmessung relativ grob ist. Für unsere Ziele entsteht dadurch kein Nachteil, weil die Unterschiede zwischen den Komplexitätsmaßen unterschiedlicher Berechnungsmodelle oft sogar größer sind, als man durch einen konstanten Faktor ausdrücken kann, und wir primär an Resultaten interessiert sind, die für alle vernünftigen Maschinenmodelle gelten. Deswegen ist uns das asymptotische Verhalten der Funktionen TimeM und SpaceM wichtiger als eine genaue Bestimmung ihrer Funktionswerte. Für die asymptotische Analyse von Komplexitätsfunktionen benutzen wir die übliche Ω-, O-, Θ- und o-Notation. Deﬁnition 6.3. Für jede Funktion f : N → R+ deﬁnieren wir O(f (n)) = {r : N → R + |∃n0 ∈ N, ∃c ∈ N, so dass für alle n ≥ n0 : r(n) ≤ c · f (n)}. Für jede Funktion r ∈ O(f (n)) sagen wir, dass r asymptotisch nicht schneller wächst als f . Für jede Funktion g : N → R+ deﬁnieren wir Ω(g(n)) = {s : N → R + |∃n0 ∈ N, ∃d ∈ N, so dass für alle n ≥ n0 : s(n) ≥ 1 d · g(n)}. FürjedeFunktion s ∈ Ω(g(n)) sagen wir, dass s asymptotisch mindestens so schnell wächst wie g. Für jede Funktion h : N → R + deﬁnieren wir Θ(h(n)) = {q : N → R + |∃c, d, n0 ∈ N, so dass für alle n ≥ n0 : 1 d · h(n) ≤ q(n) ≤ c · h(n)}. = O(h(n)) ∩ Ω(h(n)). Falls g ∈ Θ(h(n)) sagen wir, dass g und h asymptotisch gleich schnell wachsen. 172 6 Komplexitätstheorie Seien f und g zwei Funktionen von N nach R +.Falls lim n→∞ f (n) g(n) =0, dann sagen wir, dass g asymptotisch schneller wächst als f und wir schreiben f (n)=o(g(n)). Aufgabe 6.2. Welche der folgenden Aussagen sind korrekt? Begründen Sie Ihre Antwort. (a) 2 n ∈ Θ(2 n+a) für jede positive Konstante a ∈ N, (b) 2b·n ∈ Θ(2 n) für jede positive Konstante b ∈ N, (c) logb n ∈ Θ(logc n) für alle reellen Zahlen b, c > 1, (d) (n + 1)! ∈ O(n!), (e) log(n!) ∈ Θ(n · log n). Aufgabe 6.3. Beweisen Sie für alle Funktionen f und g : N → R +, dass folgende Aussagen gelten: (a) f ∈ O(g) und g ∈ O(h) impliziert f ∈ O(h), (b) f ∈ O(g) ⇐⇒ g ∈ Ω(f ), (c) f ∈ Θ(g) ⇐⇒ g ∈ Θ(f ) ⇐⇒ Θ(f )=Θ(g). Betrachten wir jetzt die TM M aus Abbildung 4.5, die die Sprache LMitte akzeptiert. M läuft vom linken bis zum rechten Rand des Bandes und zurück und bewegt dabei den linken Rand ein Feld nach rechts und den rechten Rand ein Feld nach links. Auf diese Weise bestimmt M das mittlere Feld. Die Zeitkomplexität TimeM von M ist oﬀensichtlich in O(n2). Aufgabe 6.4. Bestimmen Sie für die TM aus Abbildung 4.5 TimeM (n) genau. Aufgabe 6.5. Entwerfen Sie eine 1-Band-Turingmaschine B mit L(B)= LMitte und TimeB(n) ∈ O(n). Aufgabe 6.6. Analysieren Sie asymptotisch die Zeitkomplexität der TM A aus Abbildung 4.6, die die Sprache LP akzeptiert. Die 1-Band-Turingmaschine A aus Abbildung 4.10 akzeptiert die Sprache Lgleich = {w#w | w ∈ (Σbool) ∗}, indem das Präﬁx des Eingabewortes bis zum # auf das Arbeitsband kopiert und dann der Inhalt des Arbeitsbandes mit dem Suﬃx des Eingabewortes nach dem # verglichen wird. Oﬀensichtlich ist TimeA(n) ≤ 3·n ∈ O(n) und SpaceM (n) ∈ O(n). Aufgabe 6.7. Entwerfen Sie eine 3-Band-Turingmaschine M mit den Eigenschaften L(M )= Lgleich, TimeM (n) ∈ O( n 2 log2 n ) und SpaceM (n) ∈ O(log2 n). Bis jetzt haben wir die Zeitkomplexität und die Speicherplatzkomplexität von Mehrband- Turingmaschinen deﬁniert. Was uns aber primär interessiert, ist die Komplexität von Problemen, um sie nach dieser Komplexität klassiﬁzieren zu können. Intuitiv würde man gerne sagen, dass die Zeitkomplexität eines Problems U der Zeitkomplexität einer 6.2 Komplexitätsmaße 173 asymptotisch optimalen MTM M (eines optimalen Algorithmus) für U entspricht. Unter der (asymptotischen) Optimalität von M versteht man, dass für jede MTM A, die U löst, TimeA(n) ∈ Ω(TimeM (n)) gilt (d. h., es existiert keine MTM für U , die asymptotisch besser ist als M ). Die Idee wäre also, die Komplexität eines Problems als die Komplexität des „besten“ Algorithmus für dieses Problem zu betrachten. Obwohl diese Idee natürlich und vernünftig aussieht, zeigt folgender Satz, dass man sie nicht zur Deﬁnition der Komplexität im Allgemeinen benutzen kann. Wegen des hohen Schwierigkeitsgrades verzichten wir auf einen Beweis. Satz 6.1. Es existiert ein Entscheidungsproblem (Σbool,L), so dass für jede MTM A, die (Σbool,L) entscheidet, eine MTM B existiert, die auch (Σbool,L) entscheidet, und für die gilt TimeB(n) ≤ log2(TimeA(n)) für unendlich viele n ∈ N. Satz 6.1 besagt, dass es Probleme gibt, für die man jeden gegebenen Algorithmus für dieses Problem wesentlich verbessern kann. 4 Dies bedeutet, dass für solche Probleme keine optimalen Algorithmen existieren und man deshalb für diese Probleme die Komplexität nicht in dem oben beschriebenen Sinne deﬁnieren kann. Deswegen spricht man in der Komplexitätstheorie nur über obere und untere Schranken für die Komplexität eines Problems im Sinne der folgenden Deﬁnition. Deﬁnition 6.4. Sei L eine Sprache. Seien f und g zwei Funktionen von N nach R +. Wir sagen, dass O(g(n)) eine obere Schranke für die Zeitkomplexität von L ist, falls eine MTM A existiert, so dass A die Sprache L entscheidet und TimeA(n) ∈ O(g(n)). Wir sagen, dass Ω(f (n)) eine untere Schranke für die Zeitkomplexität von L ist, falls für jede MTM B, die L entscheidet, TimeB(n) ∈ Ω(f (n)). Eine MTM C heißt optimal für L, falls TimeC(n) ∈ O(f (n)) gilt und Ω(f (n)) eine untere Schranke für die Zeitkomplexität von L ist. Die Bestimmung einer oberen Schranke für die Zeitkomplexität eines Problems ist meistens nicht sehr schwer, weil es hinreichend ist, einen Algorithmus zur Lösung des Problems zu ﬁnden. Eine nichttriviale untere Schranke für die Komplexität konkreter Probleme zu beweisen, gehört zu den technisch schwierigsten Aufgaben der Informatik, weil dies einen Beweis der Nichtexistenz eines eﬃzienten Algorithmus für das betrachtete Problem erfordert. Wie schwer diese Aufgabe ist, kann man damit illustrieren, dass wir tausende Probleme kennen, für die die besten bekannten Algorithmen exponentielle Zeitkomplexität bezüglich der Eingabelänge haben, aber die höchsten uns bekannten unteren Schranken für diese Probleme auf Mehrband-Turingmaschinen nur Ω(n) sind. Wir haben jetzt die Messung der Komplexität für das Studium der abstrakten Komple- xitätstheorie vorgestellt. Am Ende dieses Abschnitts diskutieren wir noch die Arten der Messung der Komplexität von konkreten Programmen (Algorithmen). Wir unterscheiden 4Man bemerke, dass diese Verbesserungsmöglichkeit nach Satz 6.1 zu einer unendlichen Folge von Verbesserungen führt. 174 6 Komplexitätstheorie zwei fundamentale Arten der Messung – die Messung mit uniformem Kostenmaß und die Messung mit logarithmischem Kostenmaß. Die Messung mit dem unifor- men Kostenmaß ist eine grobe Messung. Die Zeitkomplexität ist einfach die Anzahl der durchgeführten Basisoperationen wie arithmetische Operationen und Zahlenvergleiche, und die Speicherplatzkomplexität entspricht der Anzahl der benutzten Variablen. Dies bedeutet, dass der Preis jeder Operation 1 ist, unabhängig von der Größe der Operanden. Der Hauptvorteil der Messung mit dem uniformen Kostenmaß liegt in ihrer Einfachheit. Diese Messung ist auch angemessen, wenn in der Berechnung die Operanden die Größe eines Rechnerwortes nicht überschreiten. Falls aber die Operandengröße während der Berechnung wächst, entspricht die Messung nicht mehr dem realen Rechenaufwand. Wir illustrieren dies an folgendem Beispiel. Seien a ≥ 2 und k zwei positive ganze Zahlen. Die Aufgabe ist, a 2k zu bestimmen. Dies kann man mit dem Programm for i =1 to k do a := a · a berechnen. Das Programm berechnet tatsächlich mit den folgenden k Multiplikationen a2 := a · a, a 4 = a 2 · a 2,a 8 = a 4 · a 4,. . . ,a 2k = a 2k−1 · a 2k−1 den Wert a 2k . Bei der Messung mit uniformem Kostenmaß ist die Speicherplatzkomplexität also 3 und die Zeitkomplexität O(k). Wir brauchen aber real mindestens 2 k Bits, um das Resultat zu speichern und mindestens Ω(2k) Operationen über Rechnerwörtern (Zahlen mit binärer Darstellung der Länge 64 oder 32) fester Größe, um die Berechnung auf einem Rechner zu realisieren. Weil diese Überlegung für jedes k stimmt, bekommen wir einen exponentiellen Unterschied zwischen der uniformen Komplexität und der tatsächlichen Berechnungskomplexität. Wenn die Größe der Operanden in den betrachteten Berechnungen wachsen kann, benutzt man die Messung mit dem logarithmischen Kostenmaß, die eine sehr genaue Messung auf der Ebene von Bits ist. Die Kosten einer Operation misst man als die Summe der Längen der binären Darstellungen der in den Operationen vorkommenden Operanden, und die Zeitkomplexität einer Berechnung ist die Summe der Preise der in der Berechnung durchgeführten Operationen. Die Speicherplatzkomplexität ist die Summe der Darstellungslängen der Inhalte der benutzten Variablen. Die Messung mit logarithmischem Kostenmaß ist immer realistisch, sie kann nur manchmal sehr aufwendig sein. 6.3 Komplexitätsklassen und die Klasse P Für die Deﬁnition von Komplexitätsklassen benutzen wir das Modell der Mehrband- Turingmaschine. Wir betrachten hier Komplexitätsklassen nur als Sprachklassen, also als Mengen von Entscheidungsproblemen. 6.3 Komplexitätsklassen und die Klasse P 175 Deﬁnition 6.5. Für alle Funktionen f , g von N nach R + deﬁnieren wir TIME(f ) = {L(B) | B ist eine MTM mit TimeB(n) ∈ O(f (n))}, SPACE(g) = {L(A) | A ist eine MTM mit SpaceA(n) ∈ O(g(n))}, DLOG = SPACE(log2 n), P = ⋃ c∈N TIME(n c), PSPACE = ⋃ c∈N SPACE(n c) und EXPTIME = ⋃ d∈N TIME(2 nd). Im Folgenden beschäftigen wir uns mit den grundlegenden Beziehungen zwischen den Komplexitätsklassen und den Eigenschaften der Zeitkomplexität und der Speicherplatz- komplexität. Lemma 6.3. Für jede Funktion t : N → R + gilt TIME(t(n)) ⊆ SPACE(t(n)). Beweis. Jede MTM M , die in der Zeit TimeM (n) arbeitet, kann nicht mehr als TimeM (n) Felder eines Arbeitsbandes beschriften. Also gilt SpaceM (n) ≤ TimeM (n) für jede MTM M . \u0002 Korollar 6.1. P ⊆ PSPACE. Für die anderen Vergleiche brauchen wir den Begriﬀ der konstruierbaren Funktionen. Die Idee dabei ist, Mehrband-Turingmaschinen zu bauen, die auf sich selbst aufpassen in dem Sinne, dass sie gewisse Komplexitätsschranken in keiner ihrer Berechnungen überschreiten. Deﬁnition 6.6. Eine Funktion s : N → N heißt platzkonstruierbar, falls eine 1-Band- TM M existiert, so dass (i) SpaceM (n) ≤ s(n) für alle n ∈ N und (ii) für jede Eingabe 0 n, n ∈ N, generiert M das Wort 0 s(n) auf ihrem Arbeitsband und hält in qaccept. Eine Funktion t : N → N heißt zeitkonstruierbar, falls eine MTM A existiert, so dass (i) T imeA(n) ∈ O(t(n)) und (ii) für jede Eingabe 0 n, n ∈ N, generiert A das Wort 0 t(n) auf dem ersten Arbeitsband und hält in qaccept. 176 6 Komplexitätstheorie Die meisten gewöhnlichen monotonen Funktionen mit f (n) ≥ log2(n +1) (f (n) ≥ n) sind platzkonstruierbar (zeitkonstruierbar). Zum Beispiel kann eine 1-Band-TM A die Funktion ⌈log2(n +1)⌉ wie folgt konstruieren. A liest einmal 0 n auf dem Eingabeband von links nach rechts und speichert dabei auf dem Arbeitsband jeweils die binäre Darstellung der aktuellen Position des Kopfes auf dem Eingabeband, indem A für jede Bewegung um ein Feld nach rechts auf dem Eingabeband zu dem Inhalt des Arbeitsbandes eine Eins addiert. Wenn der Kopf auf dem Eingabeband das $-Symbol erreicht, dann hat der beschriftete Teil des Arbeitsbandes die Länge ⌈log2(n +1)⌉. Wenn man dann alle Einsen auf dem Arbeitsband durch Nullen ersetzt, enthält das Arbeitsband das Wort 0 ⌈log2(n+1)⌉. Wir beobachten, dass die Benutzung einer 1-Band-TM M in der Deﬁnition der Platzkon- struierbarkeit unwesentlich ist, weil jede MTM mit der gleichen Speicherplatzkomplexität durch eine 1-Band-TM simuliert werden kann. Eine MTM M kann die Funktion ⌈√ n ⌉ wie folgt konstruieren. M versucht auf einer Eingabe 0 i für i =1, 2,... festzustellen, ob i 2 ≤ n< (i +1)2. Für ein festes i kann die Überprüfung wie folgt laufen. Das erste und das zweite Band enthalten 0 i. Dann bewegt man den Kopf auf dem Eingabeband i · i-mal nach rechts, um festzustellen, ob i · i>n ist. Diese i · i Schritte nach rechts auf dem Eingabeband realisiert man wie folgt. Der Kopf auf dem Eingabeband und der Kopf auf dem ersten Band bewegen sich immer simultan nach rechts, der Kopf auf dem zweiten Band bleibt stehen. Wenn der Kopf auf dem ersten Band das Symbol ␣ erreicht, geht er zurück an den linken Rand des Bandes und der Kopf auf dem zweiten Band geht einen Schritt nach rechts. Damit macht der Kopf auf dem Eingabeband so viele Schritte nach rechts, dass dies dem Produkt der Inhaltslängen der beiden Arbeitsbänder entspricht. Aufgabe 6.8. Beschreiben Sie in Form von Diagrammen die zwei oben beschriebenen Mehrband- Turingmaschinen zur Konstruktion der Funktionen ⌈log2(n +1)⌉ und ⌈ √n ⌉. Oﬀensichtlich funktioniert die Idee der Multiplikation der Inhaltslängen von zwei Bändern auch, um zu zeigen, dass die Funktion f (n)= nq für ein beliebiges q ∈ N zeitkonstruierbar ist. Aufgabe 6.9. Beweisen Sie, dass folgende Funktionen platzkonstruierbar sind: (a) ⌈ √n ⌉q für jede positive ganze Zahl q, (b) ⌈n 1 3 ⌉, (c) ⌈n q 2 ⌉ für jede positive ganze Zahl q ≥ 2, (d) 2 n. Aufgabe 6.10. Zeigen Sie, dass folgende Funktionen zeitkonstruierbar sind: (a) n j für jedes j ∈ N −{0}, (b) c · n für jedes c ∈ N −{0}, (c) 2 n, (d) cn für jedes c ∈ N −{0, 1}. Aufgabe 6.11. Beweisen Sie die folgende Aussage: Wenn s(n) und t(n) zwei platzkonstruierbare (zeitkonstruierbare) Funktionen sind, dann ist auch die Funktion t(n) · s(n) platzkonstruierbar (zeitkonstruierbar). 6.3 Komplexitätsklassen und die Klasse P 177 Das nächste Lemma zeigt, dass es für jede platzkonstruierbare Funktion ausreicht, eine MTM M mit L(M )= L mit s(n)-platzbeschränkten Berechnungen auf allen Eingaben aus L zu konstruieren, um die Existenz einer MTM A zu garantieren, die ebenfalls L akzeptiert und auf allen Eingaben (also auch auf Wörtern aus L ∁) die Schranke s(n) für die Platzkomplexität einhält. Lemma 6.4. Sei s : N → N eine platzkonstruierbare Funktion. Sei M eine MTM mit SpaceM (x) ≤ s(|x|) für alle x ∈ L(M ). Dann existiert eine MTM A mit L(A)= L(M ) und SpaceA(n) ≤ s(n), d. h., es gilt SpaceA(y) ≤ s(|y|) für alle y über dem Eingabealphabet von M . Beweis. Sei M eine k-Band-TM für ein k ∈ N −{0} mit SpaceM (x) ≤ s(|x|) für alle x ∈ L(M ). Sei B eine 1-Band-TM, die s konstruiert. Wir konstruieren eine (k + 1)-Band- TM A, die auf jeder Eingabe x folgendermaßen arbeitet. (i) A interpretiert x als 0 |x| und simuliert die Arbeit von B auf 0 |x| auf dem (k + 1)-ten Arbeitsband. Die Simulation endet damit, dass das Wort 0 s(|x|) auf das (k + 1)-te Band geschrieben wird. (ii) A schreibt ein spezielles Symbol # /∈ ΓM auf die s(|x|)-te Position von allen Arbeitsbändern. Dieses Symbol darf A nicht löschen. Falls A an dieser Position ein Symbol X schreiben will, schreibt sie stattdessen das Symbol (X #), sodass die Markierung des rechten Randes erhalten bleibt. (iii) A simuliert schrittweise die Arbeit von M auf x mit den ersten k Bändern. Falls M versucht, ein Feld rechts von # auf einem der k Arbeitsbändern zu betreten, beendet A seine Berechnung in qreject. Falls M hält, hält A auch in einem Zustand mit derselben Bedeutung. Oﬀensichtlich ist SpaceA(z) ≤ s(|z|) für alle Eingaben z. Wir zeigen jetzt, dass L(A)= L(M ). Falls x ∈ L(M ), dann gilt SpaceM (x) ≤ s(|x|). Daher simuliert A die ganze Berechnung von M auf x und endet in qaccept. Also x ∈ L(A). Wenn y/∈ L(M ), unterscheiden wir zwei Fälle. Falls SpaceM (y) ≤ s(|y|), dann simuliert A die ganze Berechnung von M auf y und akzeptiert dadurch y nicht. Falls SpaceM (y) > s(|y|), dann bricht A die Simulation der Berechnung von M auf y in dem Augenblick ab, in dem M mehr als s(|y|) Speicherzellen auf einem Arbeitsband nutzen möchte. Nach der Unterbrechung der Simulation geht A in den Zustand qreject über, also y/∈ L(A). \u0002 Eine zu Lemma 6.4 analoge Behauptung für die Zeitkomplexität gibt das folgende Lemma. Lemma 6.5. Sei t : N → N eine zeitkonstruierbare Funktion. Sei M eine MTM mit TimeM (x) ≤ t(|x|) für alle x ∈ L(M ). Dann existiert ein MTM A mit L(A)= L(M ) und TimeA(n) ∈ O(t(n)). 178 6 Komplexitätstheorie Aufgabe 6.12. Beweisen Sie Lemma 6.5. Die obigen Lemmata zeigen, dass es für die Deﬁnition der Komplexitätsklassen SPACE(s) und TIME(t) für eine platzkonstruierbare Funktion s und eine zeitkonstruierbare Funktion t unwesentlich ist, ob man SpaceM und TimeM einer TM M als SpaceM (n) = max{SpaceM (x) | x ∈ Σn} und TimeM (n) = max{TimeM (x) | x ∈ Σn} oder als SpaceM (n) = max({SpaceM (x) | x ∈ L(M ) und |x| = n}∪{0}) und TimeM (n) = max({TimeM (x) | x ∈ L(M ) und |x| = n}∪{0}) deﬁniert. Das nächste Resultat zeigt eine wichtige Relation zwischen Speicherplatzkomplexität und Zeitkomplexität. Satz 6.2. Für jede Funktion s mit s(n) ≥ log2 n gilt SPACE(s(n)) ⊆ ⋃ c∈N TIME(c s(n)). Beweis. Sei L ∈ SPACE(s(n)). Nach Lemma 6.1 existiert eine 1-Band-TM M =(Q, Σ, Γ, δ, q0,qaccept,qreject), so dass L = L(M ) und SpaceM (n) ≤ d · s(n) für eine geeignete Konstante d gelten und M immer hält. Für jede Konﬁguration C =(q, w, i, x, j)von M deﬁnieren wir die innere Konﬁguration von C als In(C)=(q, i, x, j). Eine innere Konﬁguration enthält nur die Teile der Konﬁguration, die sich während einer Berechnung ändern können. Wir haben also den Inhalt w des Eingabebandes ausgelassen, der während der ganzen Berechnung unverändert bleibt. Unsere Idee ist jetzt zu zeigen, dass es keinen Sinn ergibt, länger als die Anzahl aller möglichen inneren Konﬁgurationen zu arbeiten, weil eine Berechnung in eine Endlosschleife gerät, wenn sie auf einem Wort w zwei gleiche innere Konﬁgurationen enthält. Betrachten wir jetzt die Mengen InKonf(n) aller möglichen inneren Konﬁgurationen (q, i, x, j) der 1-Band-TM M auf allen Eingabewörtern der Länge n. Dann muss 0 ≤ i ≤ n +1, |x|≤ SpaceM (n) ≤ d · s(n) und 0 ≤ j ≤ SpaceM (n) ≤ d · s(n) gelten. Weil n +2 ≤ 4 log2 n ≤ 4 s(n) (für n ≥ 2) und SpaceM (n) ≤|Γ| SpaceM (n) gelten, ist |InKonf M (n)|≤|Q|· (n +2) ·|Γ| SpaceM (n) · SpaceM (n) ≤ (max{4, |Q|, |Γ|}) 4d·s(n) ≤ c s(n) für c = (max{4, |Q|, |Γ|}) 4d. 6.3 Komplexitätsklassen und die Klasse P 179 Es ist klar, dass jede Berechnung D = C1,C2,C3,... von M auf einem Wort w mit |w| = n, die länger als |InKonf M (n)| ist, zwei identische innere Konﬁgurationen In(Ci) und In(Cj) für i<j enthält. Aus der Deﬁnition der inneren Konﬁguration wissen wir aber, dass dann auch die entsprechenden Konﬁgurationen Ci und Cj identisch sind. Weil M deterministisch ist, ist D = C1,...,Ci−1,Ci,Ci+1,...,Cj−1,Ci,Ci+1,...,Cj−1,Ci ... eine unendliche Berechnung mit der endlosen Schleife Ci,Ci+1,...,Cj. Deswegen hat jede endliche Berechnung von M auf einer Eingabe w höchstens die Länge |InKonf M (|w|)|. Gemäß der Deﬁnition der Speicherplatzklassen sind alle Berechnungen von M endlich. Somit arbeitet M in der Zeitkomplexität c s(n). \u0002 Im Folgenden zeigen wir, dass Satz 6.2 auch dann gilt, wenn man den s(n)-platzbe- schränkten Turingmaschinen unendliche Berechnungen erlauben würde. In diesem Fall müssen wir allerdings voraussetzen, dass s eine platzkonstruierbare Funktion ist. Unter die- sen Voraussetzungen bedeutet unsere Beobachtung insbesondere, dass jede akzeptierende Berechnung auf einer Eingabe der Länge n höchstens |InKonf M (n)| lang ist. Jetzt beschreiben wir eine 3-Band-TM A mit L(A)= L(M ) und TimeA(n) ∈ O(ks(n)) für eine Konstante k. Für jede Eingabe w arbeitet A wie folgt. (i) A simuliert die Konstruktion von s und schreibt 0 s(|w|) auf das erste Arbeitsband. {Weil eine TM B, die s konstruiert, höchstens s(|w|) Felder des Arbeitsbandes benutzt, existiert eine Konstante d, so dass TimeB(n) ≤ ds(n) gilt. Damit generiert A das Wort 0s(n) auch in der Zeit ds(n).} (ii) A schreibt 0c s(|w|) auf das zweite Band in c s(|w|) Schritten (dabei wird das dritte Arbeitsband als Hilfsspeicher benutzt). (iii) A simuliert die Arbeit von M auf w Schritt für Schritt auf dem ersten Arbeitsband. Für jeden simulierten Schritt löscht A eine 0 auf dem zweiten Band. Falls alle Nullen auf dem zweiten Band gelöscht worden sind und die Berechnung von M auf w nicht endet, dann hält A in qreject. Falls M in höchstens c s(|w|) Schritten seine Berechnung auf w beendet, dann endet A entsprechend in qaccept oder qreject. Wir haben schon bemerkt, dass die Berechnungsphasen (i) und (ii) von A in O(ds(|w|)) bzw. O(c s(|w|)) Zeit durchführbar sind. Die Phase (iii) von A läuft oﬀensichtlich in Zeit O(c s(|w|)), also TimeA(n) ∈ O((max{c, d}) s(n)). Falls x ∈ L(M ), dann hat die Berechnung von M auf x höchstens die Länge c s(x) und damit akzeptiert A nach erfolgreicher Simulation der Berechnung von M auf x das Wort x auch. Wenn x/∈ L(M ), dann wird x von A oﬀensichtlich verworfen. Korollar 6.2. DLOG ⊆ P und PSPACE ⊆ EXPTIME. Die Korollare 6.1 und 6.2 ergeben zusammen die folgende fundamentale Hierarchie deterministischer Komplexitätsklassen: DLOG ⊆ P ⊆ PSPACE ⊆ EXPTIME. 180 6 Komplexitätstheorie Tabelle 6.1 n 10 50 100 300 f (n) 10n 100 500 1000 3000 2n2 200 5000 20000 180000 n3 1000 125000 1000000 27000000 2n 1024 16 Ziﬀern 31 Ziﬀern 91 Ziﬀern n! ≈ 3,6 · 10 6 65 Ziﬀern 158 Ziﬀern 615 Ziﬀern Zu den fundamentalsten Resultaten der Komplexitätstheorie gehören die folgenden Hierachiesätze. Die Beweise dieser Sätze basieren auf einer komplizierten Anwendung der Diagonalisierungsmethode, und wir verzichten deshalb auf die Beweise in dieser Einführung. Satz 6.3. ∗ Seien s1 und s2 zwei Funktionen von N nach N mit folgenden Eigenschaften: (i) s2(n) ≥ log2 n, (ii) s2 ist platzkonstruierbar und (iii) s1(n)= o(s2(n)). Dann gilt SPACE(s1) ⊊ SPACE(s2). Satz 6.4. ∗ Seien t1 und t2 zwei Funktionen von N nach N mit folgenden Eigenschaften: (i) t2 ist zeitkonstruierbar und (ii) t1(n) · log2(t1(n)) = o(t2(n)). Dann gilt TIME(t1) ⊊ TIME(t2). Die Hierarchiesätze zeigen, dass es bezüglich der Komplexität beliebig schwere Probleme gibt. Es gibt zum Beispiel Probleme, die nicht in TIME(2n) liegen (d. h., es existiert keine TM, die das Problem in der Zeit 2 n lösen kann). Dass man dabei an die Grenze des physikalisch Machbaren stößt, zeigt Tabelle 6.1, die das Wachstum der Funktionen 10n, 2n2, n3,2 n und n! für die Eingabelängen 10, 50, 100 und 300 zeigt. Wenn die Zahlen zu groß sind, geben wir nur die Anzahl der Ziﬀern ihrer Dezimaldarstellung an. Setzen wir voraus, dass unser Rechner 109 Operationen in einer Sekunde durchführt. Dann braucht ein Algorithmus A mit TimeA(n)= n3 nur 27 Millisekunden für die größte Eingabelänge 300. Wenn TimeA(n)=2n, dann braucht A schon für die Eingabelänge n = 50 mehr als 13 Tage und für n = 100 mehr als 4·10 13 Jahre. Die geschätzte Anzahl der abgelaufenen Sekunden seit dem Urknall ist eine Zahl mit 21 Dezimalziﬀern. Wenn man diese physikalische Größe als die Grenze für die Zeit praktisch realisierbarer Berechnungen ansieht, dann sind die Algorithmen mit der Zeitkomplexität 2n und n! nicht praktisch durchführbar für realistische Eingabelängen. Keine lineare Beschleunigung der Rechner durch neue Technologien kann daran etwas ändern, weil die exponentiell wachsenden 6.3 Komplexitätsklassen und die Klasse P 181 Funktionen ihren Funktionswert bei jeder Verlängerung der Eingabelänge um ein einziges Bit vervielfachen. So wurde klar, dass die Probleme, die nicht unterhalb von TIME(2n) liegen, nicht praktisch lösbar sind. Auf diese Weise entstand die fundamentale Zielsetzung der Komple- xitätstheorie, die Klasse der praktisch lösbaren Probleme zu speziﬁzieren und Techniken zur Klassiﬁzierung der Probleme in praktisch lösbare und praktisch unlösbare zu entwi- ckeln. Im Folgenden nennen wir einen Algorithmus A mit TimeA(n) ∈ O(nc) für eine Konstante c einen polynomiellen Algorithmus. In den sechziger Jahren haben sich die Informatiker auf folgende Speziﬁkation 5 geeinigt. Ein Problem ist praktisch lösbar genau dann, wenn ein polynomieller Algo- rithmus zu seiner Lösung existiert. Die Klasse P ist die Klasse der praktisch entscheidbaren Probleme. Zu dieser Entscheidung führten im Wesentlichen folgende zwei Gründe. 1. Der erste Grund basiert mehr oder weniger auf einer praktischen Erfahrung. Dass die Probleme, für die kein polynomieller Algorithmus existiert, als praktisch unlös- bare Probleme speziﬁziert werden sollen, ist aus Tabelle 6.1 ersichtlich. Aber ein Algorithmus mit der polynomiellen Zeitkomplexität n1000 ist für realistische Einga- begrößen noch weniger praktisch anwendbar als einer mit der Zeitkomplexität 2 n. Darf man also ein Problem, für das der beste Algorithmus in der Zeit n1000 läuft, als praktisch lösbar einordnen? Die Erfahrung auf dem Gebiet des Algorithmenentwurfs zeigt aber, dass solche Probleme in der Praxis nicht auftreten. Wenn man für ein Problem einen Algorithmus A mit TimeA(n) ∈ O(nc) für ein großes c gefunden hat, dann gelingt es fast immer, einen anderen Algorithmus B mit TimeB(n) ∈ O(n6), meistens sogar mit TimeB(n) ∈ O(n3), zu ﬁnden. Deswegen ist die Klasse P aus praktischer Sicht nicht zu groß, und die Probleme aus P werden als praktisch lösbar angesehen. 2. Der zweite Grund ist theoretischer Natur. Die Deﬁnition einer wichtigen Klasse wie die der praktisch lösbaren Probleme muss robust in dem Sinne sein, dass sie unabhängig von dem in der Deﬁnition benutzten Rechnermodell ist. Es darf nicht passieren, dass ein Problem aus Sicht der Programmiersprache JAVA praktisch lösbar ist, aber aus Sicht der Mehrband-Turingmaschinen nicht. Dies wäre der Fall, wenn man versuchen würde, die Klasse praktisch lösbarer Probleme als sol- che mit einer oberen Schranke von O(n5) für die Zeitkomplexität zu deﬁnieren. Aber der Begriﬀ der Polynomialzeit-Berechnungen und somit auch der der Klasse P ist robust genug. Die Klasse der in polynomieller Zeit lösbaren Probleme ist die gleiche für jedes bekannte Berechnungsmodell zur Symbolmanipulation mit einem realistischen Zeitkomplexitätsmaß. Formal drückt man das durch den Be- griﬀ der Polynomialzeit-Reduzierbarkeit zwischen Berechnungsmodellen aus. Ein Berechnungsmodell A ist auf ein Berechnungsmodell B polynomialzeit- reduzierbar, falls ein Polynom p existiert, so dass für jeden Algorithmus A ∈A 5Diese Speziﬁkation wird heutzutage nicht mehr in genau dieser Form akzeptiert. Dieses Thema werden wir aber ausführlicher im nächsten Kapitel besprechen. 182 6 Komplexitätstheorie ein Algorithmus B ∈B existiert, der das gleiche Problem wie A löst und für den TimeB(n) ∈ O(p(TimeA(n))) gilt. Aus der Erfahrung mit Beweisen von Behauptun- gen wie „A ist auf B polynomialzeit-reduzierbar“ wissen wir, dass p in der Regel nicht schneller wachsen muss als n3. Als Beispiel können wir die Modelle der Tu- ringmaschinen und der Mehrband-Turingmaschinen betrachten. Oﬀensichtlich sind Turingmaschinen polynomialzeit-reduzierbar auf Mehrband-Turingmaschinen und es reicht aus, p(n)= n zu wählen (Lemma 4.1). Die Simulation in Lemma 4.2 zeigt, dass Mehrband-Turingmaschinen auf Turingmaschinen polynomialzeit-reduzierbar sind, und wir überlassen die genaue Analyse dem Leser. Aufgabe 6.13. Analysieren Sie die Simulation aus dem Beweis von Lemma 4.2, um folgende Aussage zu beweisen. Für jede MTM A mit TimeA(n) ≥ n existiert eine äquivalente TM B, so dass TimeB(n) ∈ O((TimeA(n)) 2). 6.4 Nichtdeterministische Komplexitätsmaße Nichtdeterministische Turingmaschinen (Algorithmen) können viele6 unterschiedliche Berechnungen auf einer Eingabe durchführen. Diese können alle sehr unterschiedliche Komplexitäten haben. Was ist dann die Komplexität der Arbeit einer nichtdeterministi- schen Turingmaschine (eines nichtdeterministischen Algorithmus) M auf einer Eingabe w? Wir vertreten bei nichtdeterministischen Berechnungsmodellen die optimistische Ansicht, die besagt, dass eine nichtdeterministische Turingmaschine immer die beste Möglichkeit aus einer bestehenden Auswahl wählt. Die „beste Wahl“ bedeutet nicht nur die richtige Wahl, die zu dem richtigen Resultat führt, sondern auch die eﬃzienteste Wahl, die mit minimaler Komplexität zu dem richtigen Resultat führt. Deswegen deﬁniert man die nichtdeterministische Komplexität einer Turingmaschine M auf einer Eingabe x als die Komplexität der eﬃzientesten Berechnung von M auf x mit dem richtigen Resultat. Im Falle der Sprachenerkennung (der Entscheidungsprobleme) betrachtet man nur die Komplexität von Berechnungen auf Wörtern, die in der Sprache liegen. Deﬁnition 6.7. Sei M eine NTM oder eine nichtdeterministische MTM. Sei x ∈ L(M ) ⊆ Σ∗. Die Zeitkomplexität von M auf x, TimeM (x), ist die Länge einer kürzesten akzeptierenden Berechnung von M auf x.Die Zeitkomplexität von M ist die Funktion TimeM : N → N, deﬁniert durch TimeM (n) = max({TimeM (x) | x ∈ L(M ) und |x| = n}∪{0}). Sei C = C1,C2,...,Cm eine akzeptierende Berechnung von M auf x. Sei SpaceM (Ci) die Speicherplatzkomplexität der Konﬁguration Ci. Wir deﬁnieren SpaceM (C) = max{SpaceM (Ci) | i =1, 2,...,m}. 6Sogar unendlich (aber abzählbar) viele. 6.4 Nichtdeterministische Komplexitätsmaße 183 Die Speicherplatzkomplexität von M auf x ist SpaceM (x) = min{SpaceM (C) | C ist eine akzeptierende Berechnung von M auf x}. Die Speicherplatzkomplexität von M ist die Funktion SpaceM : N → N deﬁniert durch SpaceM (n) = max({SpaceM (x) | x ∈ L(M ) und |x| = n}∪{0}). Deﬁnition 6.8. Für alle Funktionen f, g : N → R+ deﬁnieren wir NTIME(f ) = {L(M ) | M ist eine nichtdeterministische MTM mit TimeM (n) ∈ O(f (n))}, NSPACE(g) = {L(M ) | M ist eine nichtdeterministische MTM mit SpaceM (n) ∈ O(g(n))}, NLOG = NSPACE(log2 n), NP = ⋃ c∈N NTIME(n c) und NPSPACE = ⋃ c∈N NSPACE(n c). Zuerst zeigen wir analog zu Lemma 6.3 und Satz 6.2 die Relation zwischen der Zeit- komplexität und der Speicherplatzkomplexität nichtdeterministischer Turingmaschinen. Lemma 6.6. Für alle Funktionen t und s mit s(n) ≥ log2 n gilt (i) NTIME(t) ⊆ NSPACE(t), (ii) NSPACE(s) ⊆ ⋃ c∈N NTIME(cs(n)). Beweis. Wir beweisen zuerst (i) und dann (ii). (i) Sei L ∈ NTIME(t). Dann existiert eine nichtdeterministische MTM M mit L(M )= L und TimeM (n) ≤ d · t(n) für eine Konstante d und alle genügend großen n. Dies bedeutet, dass für jedes genügend lange x ∈ L(M ) eine akzeptierende Berechnung Cx von M auf x von höchstens der Länge d · t(|x|) existiert. Weil in d · t(n) Schritten höchstens d · t(n) Felder eines Arbeitsbandes besucht werden, ist SpaceM (Cx) ≤ d · t(|x|). Damit ist aber SpaceM (n) ≤ d · t(n) für alle genügend großen n, also L ∈ NSPACE(t). (ii) Sei L ∈ NSPACE(s). Dann existiert eine nichtdeterministische MTM A mit L = L(A) und SpaceA(x) ≤ d · s(|x|) für eine Konstante d und alle genügend langen Wörter x ∈ L(A)= L. Dies bedeutet, dass für jedes genügend lange x ∈ L(A) eine akzeptierende Berechnung Cx mit SpaceA(Cx) ≤ d · s(|x|) existiert. Sei Cx die kürzeste akzeptierende Berechnung von A auf x mit dieser Eigenschaft. Mit dem 184 6 Komplexitätstheorie gleichen Argument wie im Beweis von Satz 6.2 erhalten wir, dass es eine Konstante k gibt, so dass die Länge von Cx höchstens |InKonf A(|x|)|≤ kd·s(|x|) für alle genügend langen x ∈ L(A) ist. Wenn nämlich die Länge von Cx größer als |InKonf A(|x|)| wäre, dann existierten i, j ∈ N −{0},i ̸= j, so dass Cx = C1,C2,...,Ci−1,Ci,Ci+1,...,Cj,Cj+1,...,Cm und Ci und Cj gleiche Konﬁgurationen wären. Dann ist aber die Berechnung C ′ x = C1,C2,...,Ci−1,Ci,Cj+1,...,Cm auch eine akzeptierende Berechnung von A auf x. Das ist ein Widerspruch zur Annahme, dass Cx die kürzeste akzeptierende Berechnung von A auf x ist. Weil A für alle genügend langen Wörter x ∈ L(A) akzeptierende Berechnungen von höchstens der Länge kd·s(|x|) =(kd) s(|x|) für Konstanten k und d hat, ist TimeA(n) ∈ O(c s(n)) für c = kd und somit L ∈ NTIME(cs(n)). \u0002 Aufgabe 6.14. Sei M eine nichtdeterministische MTM mit TimeM (n) ≤ t(n) für eine zeitkon- struierbare Funktion t. Beweisen Sie die Existenz einer nichtdeterministischen MTM A, so dass L(A)= L(M ) und eine Konstante d existiert, so dass für jede Eingabe w ∈ Σ ∗ alle Berechnungen von A auf w höchstens die Länge d · t(|w|) haben. Der folgende Satz zeigt die grundlegende Beziehung zwischen deterministischen und nichtdeterministischen Komplexitätsmaßen. Satz 6.5. Für jede Funktion t : N → R + und jede platzkonstruierbare Funktion s : N → N mit s(n) ≥ log2 n gilt (i) TIME(t) ⊆ NTIME(t), (ii) SPACE(t) ⊆ NSPACE(t) und (iii) NTIME(s(n)) ⊆ SPACE(s(n)) ⊆ ⋃ c∈N TIME(c s(n)). Beweis. Die Behauptungen (i) und (ii) sind oﬀensichtlich gültig, weil jede MTM auch eine nichtdeterministische MTM ist. Die Relation SPACE(s(n)) ⊆ ⋃ c∈N TIME(c s(n)) wurde in Satz 6.2 bewiesen. Um (iii) zu beweisen, reicht es aus, NTIME(s(n)) ⊆ SPACE(s(n)) zu zeigen. Sei L ∈ NTIME(s(n)). Also gibt es eine nichtdeterministische k-Band-TM M = (Q, Σ, Γ,δM ,q0,qaccept,qreject) mit L = L(M ) und TimeM (n) ∈ O(s(n)). Sei r = rM = max{|δM (U )|| U =(q, a, b1,...,bk) ∈ Q × (Σ ∪{¢, $}) × Γ k} die obere Schranke für die Anzahl der möglichen Aktionen von M aus einer Konﬁguration. Sei TM,x der Berechnungsbaum von M auf einer Eingabe x ∈ Σ∗. Wenn die nichtde- terministischen Entscheidungen von M auf jedem Argument mit 1, 2,...,r nummeriert 6.4 Nichtdeterministische Komplexitätsmaße 185 1 11 1111 1 111 1 12 2 2 222 2 2 2 3 3 3 34 .. . .. . ... ... ... ... ... ... r r TM,x Abbildung 6.1 werden, dann kann man jeder Kante von TM,x die entsprechende Nummer zuordnen (Abbildung 6.1). Wenn man dann eine Berechnung der Länge l als eine Folge von Kanten (statt einer Folge von Knoten) betrachtet, kann man jeder Berechnung C der Länge l eindeutig ein Wort z = z1z2 ...zl mit zi ∈{1, 2,...,r} zuordnen. Für eine gegebene Eingabe x von M bestimmt zum Beispiel die Folge 1,3,1,4 eindeutig das Präﬁx einer Berechnung, bei der M im ersten Schritt die erste Wahl triﬀt, im zweiten die dritte, im dritten die erste und im vierten Schritt die vierte Wahl. Wenn es zum Beispiel im vierten Schritt nur drei Möglichkeiten gab, entspricht die Folge 1,3,1,4 keiner Berechnung und wird als inkonsistent betrachtet. Dasselbe gilt, wenn die Berechnung nach 1,3,1 beendet ist und kein vierter Schritt existiert. Ohne Beschränkung der Allgemeinheit (siehe Aufgabe 6.14) setzen wir voraus, dass eine Konstante d existiert, so dass alle Berechnungen von M auf einer Eingabe w höchstens die Länge d · s(|w|) haben. Somit benutzt keine Berechnung von M auf w mehr als d · s(|w|) Speicherplätze. Jetzt beschreiben wir eine (k + 2)-Band-TM A, die alle Berechnungen von M von höchstens der Länge |InKonf M (n)| simuliert. Für jedes w ∈ Σ∗ arbeitet A wie folgt. (i) A schreibt 0s(|w|) auf das (k + 2)-te Band. (ii) A schreibt 0 d·s(|w|) auf das (k +1)-te Band und löscht dabei den Inhalt des (k +2)-ten Bandes. (iii) A generiert eins nach dem anderen alle Wörter z ∈{1, 2,...,rM }∗ der Länge höchstens d · s(|w|) auf dem (k + 2)-ten Band. Für jedes z simuliert A auf den ersten k Arbeitsbändern die entsprechende Berechnung (falls eine solche existiert) der nichtdeterministischen k-Band-TM M auf w. Falls M in irgendeiner dieser 186 6 Komplexitätstheorie simulierten Berechnungen seinen akzeptierenden Zustand qaccept erreicht, dann akzeptiert A die Eingabe w. Falls in keiner Berechnung von M auf w der Zustand qaccept erreicht wird, verwirft A das Wort w. Oﬀensichtlich ist L(A)= L(M ), weil M keine längeren Berechnungen als d · s(n) hat, und damit A im Fall des Verwerfens alle Berechnungen von M auf der Eingabe überprüft. SpaceM (n) ≤ d · s(n) gilt, weil SpaceM (n) ≤ TimeM (n). Somit überschreitet A auf den ersten k Arbeitsbändern, die zur Simulation von M benutzt werden, nie die Speicherplatzkomplexität SpaceM (n) ≤ d · s(n). Das (k + 1)-te Band benutzt genau d · s(n) Felder für 0 d·s(|w|). Weil auch das (k + 2)-te Band während der Simulation ein Wort aus {1, 2,...,r} ∗ der Länge höchstens d · s(n) enthält, gilt SpaceA(n) ≤ d · s(n). \u0002 Korollar 6.3. NP ⊆ PSPACE. Aufgabe 6.15. Analysieren Sie die Zeitkomplexität der (k + 2)-Band-TM A aus dem Beweis von Satz 6.5. Wir kennen keine eﬃzientere allgemeine deterministische Simulation nichtdetermi- nistischer Algorithmen, als systematisch alle Berechnungen eines gegebenen nichtde- terministischen Algorithmus zu simulieren.7 Dies führt dann aber beim Übergang von Nichtdeterminismus zu Determinismus zu exponentiellem Wachstum der Zeitkomplexi- tät. So ist es in Satz 6.5, und wir beobachten dies auch bei der Tiefensuche-basierten Simulation in Satz 4.2. Aufgabe 6.16. Analysieren Sie die Zeitkomplexität der deterministischen Simulation einer NTM aus Satz 4.2. Der nächste Satz gibt die beste bekannte zeiteﬃziente deterministische Simulation von nichtdeterministischem Speicherplatz. Satz 6.6. ∗ Für jede platzkonstruierbare Funktion s, s(n) ≥ log2 n, gilt NSPACE(s(n)) ⊆ ⋃ c∈N TIME(c s(n)). Beweis. Sei M eine nichtdeterministische MTM mit L(M )= L und SpaceM (n) ∈ O(s(n)). Ohne Beschränkung der Allgemeinheit setzen wir Folgendes voraus: (i) Es existiert eine Konstante d, so dass für jede Eingabe w der Länge n alle Berech- nungen von M auf w höchstens die Speicherplatzkomplexität d · s(n) haben. (ii) Für jede Eingabe w ∈ L(M ) hat M genau eine akzeptierende Konﬁguration Caccept(w)=(qaccept,w, 0,λ,...,λ, 0), d. h., bevor M in qaccept übergeht, löscht M die Inhalte aller Arbeitsbänder und stellt alle Köpfe auf das linke Randsymbol ¢. 7Heutzutage glauben die meisten Forscher nicht an die Existenz einer wesentlich eﬃzienteren Simulation nichtdeterministischer Algorithmen, aber die Nichtexistenz solcher Simulationsmethoden wurde noch nicht bewiesen. 6.4 Nichtdeterministische Komplexitätsmaße 187 Nach (i) wissen wir, dass es eine Konstante c gibt, so dass man für jede Eingabe w die Anzahl unterschiedlicher innerer Konﬁgurationen mit w auf dem Eingabeband durch |InKonf(|w|)|≤ c s(|w|) beschränken kann (siehe auch den Beweis von Satz 6.2). Diese Konﬁgurationen kann man als C0,C1,...,C|InKonf(|w|)|−1 in der kanonischen Ordnung nummerieren. Wir konstruieren jetzt eine MTM A mit L(A)= L. Für eine Eingabe w arbeitet A wie folgt: 1. A generiert alle inneren Konﬁgurationen C0,C1,...,C|InKonf(|w|)|−1 von M in kano- nischer Reihenfolge. 2. A konstruiert die Adjazenzmatrix M (w) des gerichteten Graphen G(w), dessen Knoten den |InKonf(|w|)| vielen verschiedenen Konﬁgurationen von Berechnungen von M auf w entsprechen, und in dem zwei Knoten Ci und Cj genau dann durch eine gerichtete Kante von Ci nach Cj verbunden sind, wenn die Konﬁguration Cj in einem Schritt von M aus Ci erreichbar ist. 3. Sei Ck = Caccept(w) die einzige akzeptierende Konﬁguration von M auf w.Sei C0 = Cstart(w) die Anfangskonﬁguration von M auf w. Oﬀensichtlich akzeptiert M das Wort w genau dann, wenn ein Weg von C0 nach Ck in G(w) existiert. Die MTM A überprüft, ob ein solcher Weg existiert. Falls ja, akzeptiert A die Eingabe w, sonst verwirft A die Eingabe w. Oﬀensichtlich gilt L(A)= L(M ). Wir analysieren jetzt die Zeitkomplexität von A. Wenn die inneren Konﬁgurationen geeignet kodiert werden, dann lässt sich Ci aus ihrer kanonischen Vorgängerkonﬁguration Ci−1 in der Zeit O(s(|w|)) konstruieren, also in der Zeit d1 · s(|w|) für eine geeignete Konstante d1. Dies gilt, weil s(|w|) ≥ log2(|w|) und damit s(|w|) der größte der Parameter der inneren Konﬁguration ist. Auch die kanonisch erste innere Konﬁguration ist in dieser Zeit bestimmbar. Insgesamt ist die Generierung aller inneren Konﬁgurationen von M , d. h. der erste Teil der Berechnung, also in |InKonf(|w|)|· d1 · s(|w|) ≤ cs(|w|) · d1 · s(|w|) Zeit möglich. Um M (w) zu konstruieren, muss man |InKonf(|w|)|·|InKonf(|w|)|≤ c s(|w|) · c s(|w|) ≤ c 2·s(|w|) Elemente mij der Matrix M (w) bestimmen. Um herauszuﬁnden, ob Cj aus Ci in einem Schritt von M erreichbar ist, muss A im Wesentlichen die Arbeitsbandinhalte, die Kopfpositionen und die Zustände von Ci und Cj miteinander vergleichen. Dies ist in linearer Zeit möglich, also in der Zeit d2 · s(|w|) für eine geeignete Konstante d2. Somit kann A den zweiten Teil der Berechnung in der Zeit c2·s(|w|) · d2 · s(|w|) durchführen. 188 6 Komplexitätstheorie Oﬀenbar gibt es eine Konstante c1 ≥ c, so dass c 2·s(|w|) · d2 · s(|w|) ≤ c s(|w|) 1 und c s(|w|) · d1 · s(|w|) ≤ c s(|w|) 1 gelten. Im dritten Schritt ihrer Berechnung bestimmt A,obein Wegvon C0 nach Ck in G(w) existiert. Dies ist in polynomieller Zeit bezüglich der Knotenanzahl |InKonf(|w|)| möglich. Auf einer MTM kann man diese Aufgabe in O(|InKonf(|w|)| 4) Schritten realisieren. Insgesamt lässt sich somit der Zeitaufwand von A nach oben abschätzen durch c s(|w|) 1 + c s(|w|) 1 +(c s(|w|)) 4 ≤ 2c s(|w|) 1 + c 4·s(|w|) 1 . Damit ist es oﬀensichtlich, dass TimeA(n) ∈ O((c4 1) s(n)) gilt. \u0002 Korollar 6.4. NLOG ⊆ P und NPSPACE ⊆ EXPTIME. Eine etwas anspruchsvollere Suche8 in dem Graphen G(w) aller potentiell möglichen Konﬁgurationen auf w führt zu folgendem Resultat. Satz 6.7∗ (Satz von Savitch). Sei s mit s(n) ≥ log2 n eine platzkonstruierbare Funk- tion. Dann gilt NSPACE(s(n)) ⊆ SPACE(s(n) 2). Beweis. Sei L ∈ NSPACE(s(n)). Somit gibt es eine nichtdeterministische MTM M mit L(M )= L und SpaceM (n) ≤ d · s(n) für eine geeignete Konstante d. Wie im Beweis von Satz 6.6 setzen wir voraus, dass für alle w ∈ L(M ) nur eine akzeptierende Konﬁguration Caccept(w) existiert. Caccept(w) enthält den akzeptierenden Zustand qaccept, das Eingabeband enthält w und alle Arbeitsbänder sind leer. Somit reicht es für eine deterministische Simulation von M aus, festzustellen, ob für die gegebene Eingabe w die Konﬁguration Caccept(w) aus der Startkonﬁguration Cstart(w)von M auf w erreichbar ist. Falls w ∈ L(M ) ist, kann die kürzeste akzeptierende Berechnung von M auf w nicht länger sein als |InKonf(|w|)|≤ c s(|w|) für eine geeignete Konstante c = cM (konstant bezüglich der Eingabelänge). Um festzu- stellen, ob Caccept(w) aus Cstart(w) in höchstens c s(|w|) Schritten erreichbar ist, benutzen wir die rekursive Methode „Teile und Herrsche“. 9 Diese Methode kann konkret wie folgt durch eine Prozedur beschrieben werden, die einen Boole’schen Wert zurückgibt. 8In der man G(w) nie komplett konstruiert, weil dies zu hohen Speicherbedarf verursachen würde. 9“divide and conquer” im Englischen 6.4 Nichtdeterministische Komplexitätsmaße 189 Prozedur REACHABLE Eingabe: (w, C, D, m) mit C, D Konﬁgurationen von M auf w, w ∈ Σ ∗ und m ∈ N. begin if m =1 then begin if C = D oder D ist in einem Schritt von C erreichbar then return TRUE; else return FALSE; end; else begin for jede Konﬁguration K von M auf w mit einer inneren Konﬁguration in InKonf(|w|) do begin if REACHABLE(w, C, K, ⌈ m 2 ⌉) und REACHABLE(w, K, D, ⌈ m 2 ⌉) then return TRUE; else return FALSE; end; end; end Weil s(n) platzkonstruierbar ist, kann eine deterministische MTM A den Wert c s(n) für jedes n ∈ N ausrechnen und speichern, ohne mehr als s(n) Speicherplatz zu verwenden. Das Teilen dieses Wertes durch 2 benötigt ebenfalls keinen größeren Speicherplatz als s(n). Die MTM A kann auch eine Nachfolgekonﬁguration einer gegebenen Konﬁguration ohne zusätzlichen Speicherbedarf erzeugen. Alle möglichen Nachfolgekonﬁgurationen einer Konﬁguration C können also nacheinander an derselben Stelle konstruiert werden. Damit wird für den Test, ob D von C aus erreichbar ist, lediglich der Platz für eine Konﬁguration benötigt. Für das Speichern einer inneren Konﬁguration der nichtdeterministischen MTM M braucht A höchstens d · s(|w|) Speicherplatz. A muss bei der Durchführung von REACHABLE(w, Cstart(w),Caccept(w),c s(|w|)) für eine geeignete Konstante k höchstens log2(2 · cs(|w|)) ≤ k · s(|w|) Konﬁgurationen auf einmal speichern, weil die Anzahl der verschachtelten Rekursionsauf- rufe (die Rekursionstiefe) höchstens log2(2 · c s(|w|)) ist. Die multiplikative Konstante 2 wird verwendet, um das Aufrunden beim Halbieren zu berücksichtigen. Somit ist der Speicherbedarf der MTM A höchstens d · s(|w|) · k · s(|w|) ∈ O(s(|w|)2). \u0002 190 6 Komplexitätstheorie Korollar 6.5. PSPACE = NPSPACE. Von keiner der oben angegebenen Simulationen nichtdeterministischer Berechnungen durch deterministische Berechnungen weiß man, ob es die eﬃzienteste mögliche Simulation ist. Die Zusammenfassung der vorgestellten Resultate führt zu der sogenannten funda- mentalen Komplexitätsklassenhierarchie der sequentiellen Berechnungen: DLOG ⊆ NLOG ⊆ P ⊆ NP ⊆ PSPACE ⊆ EXPTIME. Für jede dieser Inklusionen ist es unbekannt, ob es eine echte Inklusion ist. Einige echte Inklusionen müssen aber dabei sein, weil DLOG ⊊ PSPACE und P ⊊ EXPTIME direkte Folgerungen von Hierarchiesätzen sind. Die Bestimmung, welche der Inklusionen echt sind, ist seit über 40 Jahren das zentrale oﬀene Problem der Theoretischen Informatik. 6.5 Die Klasse NP und Beweisveriﬁkation In der fundamentalen Komplexitätsklassenhierarchie konzentriert sich das Interesse auf die Relation zwischen P und NP. Das Problem, ob P = NP oder P ⊊ NP gilt, ist das wohl bekannteste oﬀene Problem der Informatik, und heutzutage zählt es auch zu den wichtigsten oﬀenen Problemen der Mathematik. Für dieses große Interesse gibt es mehrere Gründe. Einerseits verbindet man polynomielle Zeit mit praktischer Lösbarkeit. Wir kennen heute über 3000 praktisch interessante Probleme, die in NP liegen, und für keines dieser Probleme ist ein deterministischer polynomieller Algorithmus bekannt. Wir würden gerne wissen, ob diese Probleme auch in P oder in NP − P liegen. Ein anderer Grund hängt mit dem fundamentalen Begriﬀ des mathematischen Beweises zusammen. Die Zeitkomplexität der deterministischen Berechnungen entspricht in gewissem Rahmen der Komplexität der algorithmischen Herstellung von mathematischen Beweisen, während die Zeitkomplexität nichtdeterministischer Berechnungen der Komplexität der algorithmischen Beweisveriﬁkation entspricht. Somit ist der Vergleich von P und NP äquivalent zu der Frage, ob es einfacher ist, gegebene Beweise zu veriﬁzieren, als sie herzustellen. Die Zielsetzung dieses Abschnitts ist es, den Zusammenhang zwischen der Klasse NP und der Polynomialzeit-beschränkten Beweisveriﬁkation zu zeigen. Skizzieren wir zuerst den Zusammenhang zwischen Berechnungen und Beweisen. Sei C eine akzeptierende Berechnung einer TM M auf einer Eingabe x. Dann kann C zweifellos auch als ein Beweis der Behauptung „x ∈ L(M )“ gesehen werden. Analog ist eine verwerfende Berechnung einer (deterministischen) TM M auf einem Wort x ein Beweis der Behauptung „x/∈ L(M )“. Von klassischen mathematischen Beweisen ist diese Vorstellung nicht weit entfernt. Betrachten wir L als eine Sprache, die alle korrekten Sätze (Aussagen) einer mathematischen Theorie enthält. Dann ist der Beweis von „x ∈ L(M )“ nichts anderes als der Beweis der Korrektheit (Gültigkeit) des Satzes x und der Beweis von „x/∈ L(M )“ ist der Beweis der Ungültigkeit von x. Wenn zum Beispiel L = SAT, wobei SAT = {x ∈ (Σlogic) ∗ | x kodiert eine erfüllbare Formel in KNF}, ist die Aussage „Φ ∈ SAT“ äquivalent zu der Behauptung „Φ ist eine erfüllbare Formel“. 6.5 Die Klasse NP und Beweisveriﬁkation 191 x1=0 x1=1 x2=0 x2=0 x2=1x2=1 xn=0xn=0 xn=1 xn=1 ⎫ ⎪⎪⎪⎬ ⎪⎪⎪⎭ deterministische Veriﬁkation ... ... ... ... .. . ⎫ ⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎬ ⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎭ Raten − 2n Berechnungen TM,Φ Abbildung 6.2 Versuchen wir jetzt den Zusammenhang zwischen nichtdeterministischen Berechnungen und Beweisveriﬁkation herzustellen. Die typische nichtdeterministische Berechnung startet mit dem Raten und veriﬁziert dann das Geratene. Das Geratene könnte ein Beweis von „x ∈ L“ sein, oder es könnte eine essentielle Information sein, mit deren Hilfe man den Beweis von „x ∈ L“ eﬃzient erstellen kann. Veranschaulichen wir dies wieder anhand des Erfüllbarkeitsproblems. Für eine Formel Φ über n Boole’schen Variablen x1,...,xn rät eine NTM M in den ersten n Schritten die Belegung α1,...,αn für x1,...,xn. Dann berechnet M den Wahrheitswert Φ(α1,...,αn), um zu veriﬁzieren, ob α1,...,αn die Formel Φ wirklich erfüllt (Abbildung 6.2). Falls α1,...,αn die Formel Φ erfüllt, ist es klar, dass wir den Beweis von „Φ ist erfüllbar“ eﬃzient herstellen können, wenn uns jemand α1,...,αn kostenlos gibt. Der Beweis ist nichts anderes als die Auswertung von Φ auf α1,...,αn. Deswegen nennen wir α1,...,αn ein Zertiﬁkat oder einen Zeugen für die Aussage „Φ ist erfüllbar“. Die Zeitkomplexität von M ist die Zeitkomplexität des Ratens plus die Komplexität des Veriﬁzierens. Der wesentliche Aufwand ist dabei dem Veriﬁzieren gewidmet. Unser Ziel ist jetzt zu zeigen, dass man alle polynomiellen nichtdeterministischen Turing- maschinen so umwandeln kann, dass sie am Anfang raten und dann das Geratene determi- nistisch veriﬁzieren. Dadurch reduziert sich die Zeitkomplexität der nichtdeterministischen Algorithmen auf die Zeitkomplexität der Beweisveriﬁkation (der Zertiﬁkatüberprüfung). Deﬁnition 6.9. Sei L ⊆ Σ∗ eine Sprache und sei p : N → N eine Funktion. Wir sagen, dass eine MTM (ein Algorithmus) A ein p-Veriﬁzierer für L ist, V (A)= L, falls A mit folgenden Eigenschaften auf allen Eingaben aus Σ∗ × (Σbool) ∗ arbeitet: (i) TimeA(w, x) ≤ p(|w|) für jede Eingabe (w, x) ∈ Σ ∗ × (Σbool) ∗. (ii) Für jedes w ∈ L existiert ein x ∈ (Σbool)∗, so dass |x|≤ p(|w|) und (w, x) ∈ L(A) 192 6 Komplexitätstheorie (d. h., A akzeptiert (w, x)).10 Das Wort x nennt man einen Beweis oder einen Zeugen der Behauptung w ∈ L. (iii) Für jedes y/∈ L gilt (y, z) /∈ L(A) für alle z ∈ (Σbool) ∗. Falls p(n) ∈ O(nk) für ein k ∈ N, so sagen wir, dass A ein Polynomialzeit-Veriﬁzierer ist. Wir deﬁnieren die Klasse der in Polynomialzeit veriﬁzierbaren Sprachen als VP = {V (A) | A ist ein Polynomialzeit-Veriﬁzierer}. Man bemerke, dass L(A) und V (A) unterschiedliche Sprachen für einen p-Veriﬁzierer A sind. Aus Deﬁnition 6.9 folgt V (A)= {w ∈ Σ∗ | es existiert ein x ∈ (Σbool) ∗ mit |x|≤ p(|w|), so dass (w, x) ∈ L(A)}. Ein p-Veriﬁzierer A für eine Sprache L ist also ein deterministischer Algorithmus, der für eine Eingabe (w, x) veriﬁziert, ob x ein Beweis (ein Zeuge) für „w ∈ L“ist. A veriﬁziert erfolgreich (w ∈ V (A)), wenn es einen Beweis x für „w ∈ L“ gibt mit |x|≤ p(|w|). Die Gleichheit V (A)= L fordert die Existenz eines Beweises x für „w ∈ L“ mit |x|≤ p(|w|) für jedes w ∈ L. Beispiel 6.1. Ein p-Veriﬁzierer A mit p(n) ∈ O(n2) für SAT kann wie folgt arbeiten. Für jede Eingabe (w, x) überprüft A zuerst, ob w die Kodierung einer Formel Φw in KNF ist. Falls nicht, verwirft A die Eingabe (w, x). Ansonsten berechnet A die Anzahl n der in Φw vorkommenden Variablen und überprüft ob die Länge von x ∈{0, 1} ∗ mindestens n ist. Falls x kürzer ist als n, verwirft A seine Eingabe. Falls |x|≥ n, interpretiert A die ersten n Bits von x als eine Belegung der Variablen in Φw und überprüft, ob diese Belegung die Formel Φw erfüllt. ♦ Beispiel 6.2. Eine k-Clique eines Graphen G mit n Knoten, k ≤ n, ist ein vollständiger Teilgraph von k Knoten in G.Sei CLIQUE = {x#y | x, y ∈{0, 1}∗,x kodiert einen Graphen Gx, der eine Nummer(y)-Clique enthält}. Ein Polynomialzeit-Veriﬁzierer B für CLIQUE arbeitet wie folgt. Für jede Eingabe (w, z) überprüft B,ob w = x#y,wobei x die Kodierung eines Graphen Gx und y ∈ (Σbool)∗ ist. Falls nicht, verwirft B seine Eingabe. Falls ja und Gx n Knoten v1,...,vn hat, überprüft B,ob Nummer(y) ≤ n und |z|≥⌈log2(n +1)⌉· Nummer(y) gelten. Falls nicht, verwirft B seine Eingabe (w, z). Falls ja, interpretiert B das Präﬁx von z der Länge ⌈log2(n +1)⌉· Nummer(y) als eine Kodierung von Nummer(y) Zahlen aus {1, 2,...,n}, die in Binärdarstellung mit führenden Nullen auf die Länge ⌈log2(n +1)⌉ erweitert sind. B veriﬁziert, ob es sich um Nummer(y) unterschiedliche Zahlen i1,i2,...,iNummer(y) handelt und ob die Knoten vi1,vi2,...,viNummer(y) einen vollständigen Graphen in Gx bilden. Falls ja, akzeptiert B die Eingabe (w, z), ansonsten verwirft B die Eingabe. ♦ 10Die Forderung, dass die Länge eines Zeugen polynomiell in |w| beschränkt ist, stellt keine notwendige Bedingung dar, da in polynomieller Zeit für einen beliebig langen Zeugen ohnehin nur ein Präﬁx polynomieller Länge gelesen werden kann. 6.5 Die Klasse NP und Beweisveriﬁkation 193 Aufgabe 6.17. Beschreiben Sie einen Polynomialzeit-Veriﬁzierer für die Sprache COMPOSITE = {x ∈ (Σbool) ∗ | Nummer(x) ist keine Primzahl}. Aufgabe 6.18. Beschreiben Sie einen Polynomialzeit-Veriﬁzierer und eine polynomielle NTM für die Sprache HK (das Problem des Hamiltonschen Kreises aus Beispiel 2.4). Die folgende Behauptung zeigt, dass man jede polynomielle NTM in eine äquivalente NTM umwandeln kann, die alle nichtdeterministischen Entscheidungen am Anfang macht und dann nur die Richtigkeit des Geratenen veriﬁziert. Satz 6.8. VP=NP. Beweis. Wir beweisen VP = NP durch die zwei Inklusionen NP ⊆ VP und VP ⊆ NP. (i) Zuerst zeigen wir NP ⊆ VP. Sei L ∈ NP, L ⊆ Σ ∗ für ein Alphabet Σ. Damit ist L = L(M ) für eine polynomielle NTM M mit TimeM (n) ∈ O(nk) für ein k ∈ N −{0}. Ohne Beschränkung der Allgemeinheit dürfen wir voraussetzen, dass M für jede nichtdeterministische Ent- scheidung eine Wahl aus höchstens zwei Möglichkeiten hat. Wir beschreiben jetzt einen Veriﬁzierer A, der für eine Eingabe (x, c) ∈ Σ ∗ × (Σbool) ∗ wie folgt arbeitet: (a) A interpretiert c als einen Navigator für die Simulation der nichtdeterministi- schen Entscheidungen von M . A simuliert schrittweise die Arbeit von M auf x. Falls M eine Wahl zwischen zwei Möglichkeiten hat, dann wählt A die erste Möglichkeit, falls das nächste Bit von c eine 0 ist, und A nimmt die zweite Möglichkeit, wenn das nächste Bit von c eine 1 ist. (Dies ist eine Strategie, um eine Berechnung von M auf x eindeutig durch ein Wort zu bestimmen, die ähnlich zu der Strategie im Beweis von Satz 6.5 ist.) (b) Falls M noch eine nichtdeterministische Wahl hat, aber schon alle Bits von c verbraucht sind, dann hält A und verwirft die Eingabe (x, c). (c) Falls A es schaﬀt, die durch c bestimmte Berechnung von M auf x vollständig zu simulieren, dann akzeptiert A seine Eingabe (x, c) genau dann, wenn M das Wort x in der durch c bestimmten Berechnung akzeptiert. Wir zeigen jetzt, dass A ein Polynomialzeit-Veriﬁzierer mit V (A)= L(M ) ist. Falls x ∈ L(M ), dann läuft die kürzeste akzeptierende Berechnung CM,x von M auf x in der Zeit O(|x|k). Dann aber existiert ein Zertiﬁkat (Navigator) c mit |c|≤|CM,x|, das die Berechnung CM,x bestimmt. Weil A jeden Schritt von M in einem Schritt simuliert, läuft die Berechnung von A auf (x, c) in der Zeit O(|x|k). Falls x/∈ L(M ), existiert keine akzeptierende Berechnung von M auf x, und somit verwirft A die Eingaben (x, d) für alle d ∈ (Σbool) ∗. Damit ist A ein O(nk)-Veriﬁzierer mit V (A)= L(M ). (ii) Wir zeigen jetzt VP ⊆ NP. Sei L ∈ VP, L ⊆ Σ∗ für ein Alphabet Σ. Dann existiert ein Polynomialzeit- Veriﬁzierer A mit V (A)= L. Wir betrachten eine NTM M , die auf jeder Eingabe x ∈ Σ∗ wie folgt arbeitet. 194 6 Komplexitätstheorie (a) M generiert nichtdeterministisch ein Wort c ∈ (Σbool) ∗. (b) M simuliert schrittweise die Arbeit von A auf (x, c). (c) M akzeptiert x genau dann, wenn A seine Eingabe (x, c) akzeptiert. Oﬀensichtlich ist L(M )= V (A) und TimeM (x) ≤ 2·TimeA(x, c) für jedes x ∈ L(M ) und einen kürzesten Zeugen c ∈ (Σbool)∗ von x ∈ L(M ). Somit arbeitet M in polynomieller Zeit und es gilt L ∈ NP. \u0002 Nach Satz 6.8 ist die Klasse NP die Klasse aller Sprachen L, die für jedes x ∈ L einen in |x| polynomiell langen Beweis von „x ∈ L“ haben, welchen man deterministisch in polynomieller Zeit bezüglich |x| veriﬁzieren kann. 6.6 NP-Vollständigkeit Im Unterschied zur Theorie der Berechenbarkeit, bei der man über gut ausgearbeitete Methoden zur Klassiﬁzierung der Probleme in algorithmisch lösbare und algorithmisch unlösbare verfügt, hat man in der Komplexitätstheorie keine mathematischen Methoden zur Klassiﬁzierung konkreter Probleme bezüglich der praktischen Lösbarkeit (der Zugehö- rigkeit zu P) gefunden. Es fehlen ausreichend starke Techniken, um untere Schranken für die Komplexität konkreter Probleme zu beweisen. Wie weit man von einem Beweis einer Aussage, dass ein konkretes Problem nicht in polynomieller Zeit lösbar ist, entfernt ist, zeigt die folgende Tatsache. Die höchste bekannte untere Schranke für die Zeitkomplexität von Mehrband-Turingmaschinen zur Lösung eines konkreten Entscheidungsproblems aus NP ist die triviale untere Schranke11 Ω(n) (man kann bisher noch nicht einmal eine untere Schranke Ω(n · log n) für irgendein Problem aus NP beweisen), obwohl für tausende Probleme aus NP die schnellsten bekannten Algorithmen in exponentieller Zeit laufen. Wir sind also bei vielen Problemen, von denen wir glauben, dass Ω(2 n) eine untere Schranke für die Zeitkomplexität ist, nicht imstande, eine höhere untere Schranke als Ω(n)zu beweisen. Um diese Lücke zwischen der beweistechnischen Realität und dem Gewünschten zu- mindests teilweise zu überwinden, überlegten die Forscher, ob man eine Methodik zur Klassiﬁzierung von Problemen bezüglich praktischer Lösbarkeit entwickeln könnte, wenn man sich eine zusätzliche, zwar unbewiesene, aber glaubwürdige Annahme erlaubt. Dies führte zu dem Konzept der NP-Vollständigkeit, das eine solche Klassiﬁzierung von Pro- blemen unter der Voraussetzung P ⊊ NP ermöglicht. Das Ziel dieses Abschnitts ist es, dieses Konzept vorzustellen. Zuerst diskutieren wir die Glaubwürdigkeit der Annahme P ⊊ NP. Den theoretischen Hintergrund für diese Annahme haben wir in Abschnitt 6.5 vermittelt. Man glaubt nicht, dass die Beweisveriﬁzierung den gleichen Schwierigkeitsgrad wie die Beweiserzeugung hat. Weiterhin sieht man trotz großer Mühe keine andere Möglichkeit, nichtdeterministische Algorithmen deterministisch zu simulieren, als systematisch alle nichtdeterministischen Berechnungen zu überprüfen. Weil aber die nichtdeterministischen Berechnungsbäume in ihrer Tiefe exponentiell viele Berechnungen beinhalten können und die Tiefe des Baumes 11Die Zeit, die man braucht, um überhaupt die ganze Eingabe einmal zu lesen. 6.6 NP-Vollständigkeit 195 der Zeitkomplexität entspricht, scheint ein exponentielles Wachstum der Zeitkomplexität bei einer solchen deterministischen Simulation unvermeidbar. Ein praktischer Grund für die Annahme P ⊊ NP basiert auf der 40-jährigen Erfahrung in der Algorithmik. Wir kennen mehr als 3000 Probleme in NP, viele davon sehr intensiv untersucht, für die die besten bekannten deterministischen Algorithmen eine exponentielle Komplexität haben. Die Algorithmiker halten es nicht für sehr wahrscheinlich, dass dieser Zustand nur eine Folge ihrer Unfähigkeit ist, existierende eﬃziente Algorithmen für diese Probleme zu ﬁnden. Wir setzen jetzt P ⊊ NP für den Rest dieses Abschnitts voraus. Wie kann uns das helfen, Resultate der Art L/∈ P zu zeigen? Die Idee ist, eine Klasse von schwersten Problemen in NP zu speziﬁzieren. Die Speziﬁkation muss so erfolgen, dass die Zugehörigkeit eines dieser schwersten Probleme zu P automatisch P = NP impliziert. Weil wir P ̸= NP vorausgesetzt haben, darf dann keines dieser schweren Probleme in P sein. Ähnlich wie in der Theorie der Berechenbarkeit nutzen wir jetzt den klassischen mathematischen Ansatz der Reduktion zur Deﬁnition der schwersten Probleme in NP. Ein Problem L aus NP ist schwer, wenn man jedes Problem aus NP eﬃzient auf L reduzieren kann. Deﬁnition 6.10. Seien L1 ⊆ Σ∗ 1 und L2 ⊆ Σ∗ 2 zwei Sprachen. Wir sagen, dass L1 polynomiell auf L2 reduzierbar ist, L1 ≤p L2, falls eine polynomielle TM (ein polynomieller Algorithmus) A existiert (Abbildung 6.3 ), die für jedes Wort x ∈ Σ∗ 1 ein Wort A(x) ∈ Σ∗ 2 berechnet, 12 so dass x ∈ L1 ⇐⇒ A(x) ∈ L2. A wird eine polynomielle Reduktion von L1 auf L2 genannt. Wir sehen, dass man die Reduktion ≤p aus der Reduktion ≤EE durch die zusätzliche Forderung der Eﬃzienz der Reduktion erhält (Abbildung 6.3). Wieder bedeutet L1 ≤p L2, dass L2 (in Bezug auf die Lösbarkeit in polynomieller Zeit) mindestens so schwer wie L1 ist. Deﬁnition 6.11. Eine Sprache L ist NP-schwer, falls für alle Sprachen L ′ ∈ NP gilt L ′ ≤p L. Eine Sprache L ist NP-vollständig, falls (i) L ∈ NP und (ii) L ist NP-schwer. Die Menge der NP-vollständigen Sprachen betrachten wir jetzt als die gesuchte Teil- klasse von schwersten Entscheidungsproblemen in NP. Die folgende Behauptung zeigt die gewünschte Eigenschaft der schwersten Probleme in NP – die leere Schnittmenge zwischen P und der Menge der NP-vollständigen Sprachen, falls P ⊊ NP (Abbildung 6.4). Lemma 6.7. Falls L ∈ P und L ist NP-schwer, dann gilt P=NP. 12Man bemerke, dass |A(x)| polynomiell in |x| ist, weil A in polynomieller Zeit arbeitet. 196 6 Komplexitätstheorie x ∈ Σ∗ 1 A mit polynomieller TimeA(n) A(x) ∈ Σ∗ 2 mit x ∈ L1 ⇐⇒ A(x) ∈ L2 C mit L(C)= L2 A(x) ∈ L2 A(x) /∈ L2 x ∈ L1 x/∈ L1 Algorithmus B mit L(B)= L1 Abbildung 6.3 Beweis. Sei L eine NP-schwere Sprache und sei L ∈ P. L ∈ P impliziert die Existenz einer polynomiellen TM M mit L = L(M ). Wir beweisen, dass für jede Sprache U ∈ NP, U ⊆ Σ∗ für ein Alphabet Σ, eine polynomielle MTM AU mit L(AU )= U existiert und deshalb U ∈ P. Da U ≤p L für jede Sprache U ∈ NP, existiert eine polynomielle TM BU , so dass x ∈ U ⇐⇒ BU (x) ∈ L. Wir beschreiben jetzt eine polynomielle MTM AU mit L(AU )= U . Für jede Eingabe x ∈ Σ∗ arbeitet AU wie folgt. (i) AU simuliert die Arbeit von BU auf x und berechnet BU (x). (ii) AU simuliert die Arbeit von M auf BU (x). AU akzeptiert x genau dann, wenn M das Wort BU (x) akzeptiert. Weil x ∈ U ⇐⇒ BU (x) ∈ L, gilt L(AU )= U .Da TimeAU (x)= TimeBU (x)+ TimeM (BU (x)), |BU (x)| polynomiell in x ist und die Turingmaschinen BU und M in polynomieller Zeit arbeiten, arbeitet auch AU in polynomieller Zeit. \u0002 6.6 NP-Vollständigkeit 197 P NP NP-vollständige Sprachen Abbildung 6.4 Wir haben jetzt die gewünschte Deﬁnition der schwersten Probleme in NP, vorausgesetzt, dass die Klasse der NP-vollständigen Sprachen nicht leer ist. Der nächste Satz zeigt, dass diese Gefahr nicht besteht, weil SAT NP-vollständig ist. Die NP-Schwere von SAT sagt aus, dass die Ausdrucksstärke Boole’scher Formeln in KNF sehr hoch ist, weil man für jede Sprache L ∈ NP die Frage „Ist x in L?“ als die Frage ausdrücken kann, ob eine bestimmte Formel erfüllbar ist. Die Ausdrucksstärke von Formeln alleine sollte aber nicht überraschend sein, weil man durch Formeln beliebige Texte beschreiben kann, und diese Texte können beliebige Objekte wie Sätze, Beweise, Berechnungen usw. darstellen. Wir illustrieren dies jetzt an einem kleinen Beispiel. Wir wollen eine Boole’sche Formel in KNF konstruieren, die es ermöglicht, Matrizen der Größe 3 × 3über {−1, 0, 1} zu beschreiben. Sie ist genau dann erfüllbar, wenn die Matrix in jeder Zeile und in jeder Spalte genau eine 1 enthält. Dazu wählen wir uns für alle i, j ∈{1, 2, 3} und alle k ∈{−1, 0, 1} die Variablen xi,j,k, mit deren Belegung wir dank folgender Bedeutung der 27 betrachteten Variablen eine beliebige (3 × 3)-Matrix A =(aij)i,j=1,2,3 über {−1, 0, 1} darstellen können. xi,j,1 =1 ⇐⇒ aij =1, xi,j,0 =1 ⇐⇒ aij =0, xi,j,−1 =1 ⇐⇒ aij = −1. Damit kann man die Matrix ⎛ ⎝ 10 0 −101 01 0 ⎞ ⎠ durch die folgende Belegung der 27 Variablen eindeutig bestimmen: x1,1,1 =1,x1,1,0 =0,x1,1,−1 =0,x1,2,1 =0,x1,2,0 =1,x1,2,−1 =0, x1,3,1 =0,x1,3,0 =1,x1,3,−1 =0,x2,1,1 =0,x2,1,0 =0,x2,1,−1 =1, x2,2,1 =0,x2,2,0 =1,x2,2,−1 =0,x2,3,1 =1,x2,3,0 =0,x2,3,−1 =0, x3,1,1 =0,x3,1,0 =1,x3,1,−1 =0,x3,2,1 =1,x3,2,0 =0,x3,2,−1 =0, x3,3,1 =0,x3,3,0 =1,x3,3,−1 =0. 198 6 Komplexitätstheorie Wir bemerken, dass es auch Belegungen gibt, die keine Matrix darstellen. Zum Beispiel ist die Belegung x1,1,1 =1= x1,1,0 so zu interpretieren, dass a11 beide Werte 1 und 0 annimmt, was nicht zulässig ist. Um dies auszuschließen, konstruieren wir zuerst eine Formel, die nur für die Belegungen erfüllt ist, die eine (3 × 3)-Matrix über {−1, 0, 1} bestimmen (d. h., jede Position der Matrix enthält genau einen Wert). Für alle i, j ∈ {1, 2, 3} garantiert die Formel Fi,j =(xi,j,1 ∨ xi,j,0 ∨ xi,j,−1) ∧ (¯xi,j,1 ∨ ¯xi,j,0) ∧ (¯xi,j,1 ∨ ¯xi,j,−1) ∧ (¯xi,j,0 ∨ ¯xi,j,−1) dass genau eine der Variablen xi,j,1, xi,j,0, xi,j,−1 den Wert 1 annimmt13 und somit der Inhalt der Position (i, j) der Matrix eindeutig bestimmt ist. Somit bestimmt jede Belegung, die die Formel Φ= ⋀ 1≤i,j≤3 Fi,j erfüllt, eindeutig eine (3 × 3)-Matrix über {−1, 0, 1}. Für i ∈{1, 2, 3} garantiert die Formel Zi =(xi,1,1 ∨ xi,2,1 ∨ xi,3,1) ∧ (¯xi,1,1 ∨ ¯xi,2,1) ∧ (¯xi,1,1 ∨ ¯xi,3,1) ∧ (¯xi,2,1 ∨ ¯xi,3,1) dass die i-te Zeile genau eine Eins enthält. Analog garantiert Sj =(x1,j,1 ∨ x2,j,1 ∨ x3,j,1) ∧ (¯x1,j,1 ∨ ¯x2,j,1) ∧ (¯x1,j,1 ∨ ¯x3,j,1) ∧ (¯x2,j,1 ∨ ¯x3,j,1) für j ∈{1, 2, 3}, dass die j-te Spalte genau eine Eins enthält. Somit ist Φ ∧ ⋀ i=1,2,3 Zi ∧ ⋀ j=1,2,3 Sj die gesuchte Boole’sche Formel in KNF. Aufgabe 6.19. Geben Sie eine Menge Boole’scher Variablen an, mit der man jede Situation (Konﬁguration) auf einem Schachbrett eindeutig bestimmen kann. Beschreiben Sie die Konstruk- tion einer Formel über diesen Variablen, die genau dann erfüllt ist, wenn auf dem Schachbrett genau acht Damen (und keine anderen Figuren) stehen, die sich gegenseitig nicht bedrohen. Mit der oben beschriebenen Strategie kann man Texte auf einem Blatt Papier beschrei- ben. Das Blatt kann man sich als eine passende (n × m)-Matrix über Elementen aus ΣTastatur vorstellen. Dann reichen n · m ·|ΣTastatur| Boole’sche Variablen, um dieses Ziel zu realisieren. 13Die elementare Disjunktion xi,j,1 ∨ xi,j,0 ∨ xi,j,−1 garantiert, dass mindestens eine der Variablen wahr ist. Die elementare Disjunktion (¯xi,j,1 ∨ ¯xi,j,0) garantiert, dass mindestens eine der Variablen xi,j,1 und xi,j,0 den Wert 0 annimmt. 6.6 NP-Vollständigkeit 199 Aufgabe 6.20. Bestimmen Sie eine Menge von Variablen, mit denen man jede Textseite der Größe 33 × 85 darstellen kann. Beschreiben Sie die Konstruktion einer Formel mit genau einer erfüllbaren Belegung, die dem Text auf der zweiten Seite des Buches „The Design and Analysis of Computer Algorithms“ von Aho, Hopcroft und Ullman entspricht. Wir sehen also, dass wir durch Formeln beliebige Texte und somit auch Konﬁgurationen einer TM darstellen können. Der Kern des Beweises des folgenden Satzes liegt darin, dass wir zusätzlich auch inhaltliche semantische Zusammenhänge des Textes, zum Beispiel wie eine Konﬁguration in einem Berechnungsschritt einer TM aus einer anderen Konﬁguration erreichbar ist, durch die Erfüllbarkeit einer Formel ausdrücken können. Wichtig ist dabei noch, dass man solche Formeln eﬃzient algorithmisch konstruieren kann, was auch bedeutet, dass die Formel nicht zu lang im Bezug auf das Eingabewort der beschriebenen Berechnung ist. Satz 6.9∗ (Satz von Cook). SAT ist NP-vollständig. Beweis. In Beispiel 6.1 (siehe auch Abbildung 6.2) haben wir schon bewiesen, dass SAT in VP = NP liegt. Es bleibt zu zeigen, dass alle Sprachen aus NP auf SAT polynomiell reduzierbar sind. Aus der Deﬁnition der Klasse NP folgt, dass für jede Sprache L ∈ NP eine NTM M mit L(M )= L und TimeM (n) ∈ O(nc) für ein c ∈ N existiert (d. h., M ist eine endliche Darstellung von L, die wir als Eingabe für die folgende polynomielle Reduktion benutzen dürfen). Es reicht also zu zeigen: Für jede polynomielle NTM M gilt L(M ) ≤p SAT. Sei M =(Q, Σ, Γ,δ,q0,qaccept,qreject) eine beliebige NTM mit TimeM (n) ≤ p(n) für ein Polynom p mit p(n) ≥ n.Sei Q = {q0,q1,...,qs−1,qs},wobei qs−1 = qaccept und qs = qreject, und sei Γ = {X1,...,Xm}, Xm = ␣. Wir entwerfen eine polynomielle Reduktion BM :Σ∗ → (Σlogic) ∗, so dass für alle x ∈ Σ∗: x ∈ L(M ) ⇐⇒ BM (x) ∈ SAT. Sei w ein beliebiges Wort aus Σ∗. BM soll eine Formel BM (w) konstruieren, so dass w ∈ L(M ) ⇐⇒ BM (w) ist erfüllbar. Das bedeutet, dass wir eine Formel konstruieren müssen, die alle Möglichkeiten der Arbeit von M auf w beschreibt. Die Idee ist, die Bedeutung der Variablen so zu wählen, dass eine Beschreibung einer beliebigen Konﬁguration zu einem beliebigen Zeitpunkt möglich ist. Wir wissen, dass jede Konﬁguration höchstens die Länge max{TimeM (|w|), |w|}+2 ≤ p(|w|)+2 hat, 14 was für die gegebene Eingabe w eine feste Zahl aus N ist. Um uns die Beschreibung zu vereinfachen, stellen wir jede Konﬁguration (¢Y1Y2 ...Yi−1qYi ...Yd) 14Die Konﬁguration enthält den Inhalt der Bandfelder 0,...,p(|w|) und den Zustand, dessen Position die Position des Kopfes markiert. 200 6 Komplexitätstheorie für d ≤ p(|w|) als (¢Y1Y2 ...Yi−1qYi ...YdYd+1 ...Yp(|w|)) dar, wobei Yd+1 = Yd+2 = ... = Yp(|w|) = ␣. Somit haben alle Konﬁgurationen die gleiche Bandlänge p(|w|) + 1. Um uns die Suche nach einer akzeptierenden Konﬁguration zu erleichtern, erweitern wir δ durch δ(qaccept,X)=(qaccept,X, N). Damit bleibt M nach dem Erreichen des akzeptierenden Zustandes qaccept weiter in qaccept, ohne eine Änderung vorzunehmen. Um die Zugehörigkeit von w zu L(M ) zu entscheiden, reicht es dann aus zu testen, ob eine der Konﬁgurationen, die nach p(|w|) Schritten von M erreicht werden, den Zustand qaccept enthält. Die Formel BM (w) wird aus folgenden Variablenklassen entstehen: • C⟨i, j, t⟩ für 0 ≤ i ≤ p(|w|), 1 ≤ j ≤ m,0 ≤ t ≤ p(|w|). Die Bedeutung von C⟨i, j, t⟩ ist wie folgt: C⟨i, j, t⟩ =1 ⇐⇒ Die i-te Position des Bandes von M enthält das Arbeitssymbol Xj zum Zeitpunkt t (also nach t Berechnungsschritten). Es gibt genau m · ((p(|w|)+1) 2 ∈ O((p(|w|)) 2) solcher Variablen. • S⟨k, t⟩ für 0 ≤ k ≤ s,0 ≤ t ≤ p(|w|). Die Bedeutung der Boole’schen Variablen S⟨k, t⟩ ist: S⟨k, t⟩ =1 ⇐⇒ Die NTM M ist im Zustand qk zum Zeitpunkt t. Die Anzahl solcher Variablen ist (s +1) · (p(|w|)+1) ∈ O(p(|w|)). • H⟨i, t⟩ für 0 ≤ i ≤ p(|w|), 0 ≤ t ≤ p(|w|). Die Bedeutung von H⟨i, t⟩ ist: H⟨i, t⟩ =1 ⇐⇒ Der Kopf von M ist auf der i-ten Position des Bandes zum Zeitpunkt t. Es gibt genau (p(|w|)+1) 2 ∈ O((p(|w|)) 2) solcher Variablen. Wir beobachten, dass wir durch die Belegung aller Variablen mit Werten für ein festes t die Beschreibung einer beliebigen Konﬁguration bekommen können. Zum Beispiel lässt sich die Konﬁguration (Xj0Xj1 ...Xji−1qrXji ...Xjp(|w|)) beschreiben durch: • C⟨0,j0,t⟩ = C⟨1,j1,t⟩ = ... = C⟨p(|w|),jp(|w|),t⟩ = 1 und C⟨k, l, t⟩ = 0 für alle restlichen Variablen aus dieser Klasse; • H⟨i, t⟩ = 1 und H⟨j, t⟩ = 0 für alle j ∈{0, 1,...,p(|w|)},j ̸= i; • S⟨r, t⟩ = 1 und S⟨l, t⟩ = 0 für alle l ∈{0, 1,...,s},l ̸= r. 6.6 NP-Vollständigkeit 201 Anfangskonﬁguration erste Konﬁguration zweite Konﬁguration dritte Konﬁguration p(|w|)-te Konﬁguration 0 0 1 1 2 3 Bandpositionen (Zustände, Kopfpositionen) p(|w|)+ 1 Zeit Abbildung 6.5 Wenn wir für jeden Zeitpunkt t eine Konﬁguration angeben, haben wir also die Möglich- keit, eine beliebige Berechnung von p(|w|) Schritten von M auf w durch die passende Belegung der Variablen zu beschreiben. Es ist wichtig, auch zu beobachten, dass es Belegungen der betrachteten Variablen gibt, die keine Interpretation auf der Ebene von Berechnungen von M auf w haben. So würde zum Beispiel S⟨1, 3⟩ = S⟨3, 3⟩ = S⟨7, 3⟩ =1 und C⟨2, 1, 3⟩ = C⟨2, 2, 3⟩ = 1 bedeuten, dass M im Zeitpunkt 3 (nach drei Berechnungs- schritten) gleichzeitig in drei Zuständen q1,q3,q7 ist und die zweite Position des Bandes das Symbol X1 sowie das Symbol X2 enthält. Diese Belegung für t = 3 entspricht also keiner Konﬁguration. Die Situation kann man sich auch so vorstellen, als ob man ein Blatt Papier der Größe (p(|w|)+2) × (p(|w|) + 1) hätte und die Belegung der Variablen bestimmen würde, welche Buchstaben auf jede Position (i, j), 0 ≤ i, j ≤ p(|w|), geschrieben werden. Es kann ein unsinniger Text entstehen, aber es gibt auch Belegungen, bei denen jede Zeile des Blattes eine Konﬁguration beschreibt und Belegungen, bei denen diese Folge von Konﬁgurationen einer Berechnung entspricht. Diese Situation ist in Abbildung 6.5 schematisch dargestellt. 202 6 Komplexitätstheorie Unsere Aufgabe ist jetzt, eine Formel BM (w)in KNF über den Variablen C⟨i, j, t⟩, S⟨k, t⟩ und H⟨i, t⟩ so zu konstruieren, dass gilt BM (w) ist erfüllbar ⇐⇒ es existiert eine akzeptierende Berechnung von M auf w. BM konstruiert die Formel BM (x)= A∧B∧C ∧D∧E∧F ∧G in KNF in folgenden sieben Schritten. Um die Konstruktion anschaulich und übersichtlich zu machen, beschreiben wir zuerst die Bedeutung der einzelnen Formeln A, B, C, D, E, F und G. A: A soll sicherstellen, dass der Kopf zu jedem Zeitpunkt genau auf einer Position des Bandes steht (d. h., A soll genau dann erfüllbar sein, wenn genau eine der Variablen H⟨i, t⟩ den Wert 1 hat für jedes feste t). B: Zu jedem Zeitpunkt ist genau ein Arbeitssymbol auf jeder Position des Bandes. C: Zu jedem Zeitpunkt ist M in genau einem Zustand. D: In jedem Schritt von M von einer Konﬁguration zur nächsten kann nur das Symbol geändert werden, auf dem der Lesekopf steht. E: Die Änderung des Zustandes, die Bewegung des Kopfes und die Änderung des Bandinhalts in jedem Schritt muss einer möglichen Aktion der NTM M (der Transitionsfunktion δ) entsprechen. F : F garantiert, dass die Variablen mit t = 0 genau die Anfangskonﬁguration von M auf w bestimmen. G: G garantiert, dass die letzte ((p(|w|) + 1)-te) Konﬁguration eine akzeptierende Konﬁguration ist. Wir sehen, dass die Erfüllbarkeit von A ∧ B ∧ C garantiert, dass unser Blatt in den Zeilen nur Konﬁgurationen enthält. Der Teil D ∧E von BM (x) soll sicherstellen, dass unser Blatt zusätzlich eine Berechnung von M beschreibt. F garantiert, dass diese Berechnung eine Berechnung von M auf w ist, und G garantiert, dass diese Berechnung in qaccept endet. In unserer Konstruktion werden wir öfter eine Formel von mehreren Variablen brauchen, die den Wert 1 genau dann annimmt, wenn genau eine der Variablen den Wert 1 hat. Seien x1,x2,...,xn Boole’sche Variablen. Die folgende Formel in KNF hat die gewünschte Eigenschaft bezüglich x1,x2,...,xn: U (x1,x2,...,xn)=(x1 ∨ x2 ∨ ... ∨ xn) ∧ (⋀ 1≤i,j≤n i̸=j (xi ∨ xj) ). Der erste Teil der Formel x1 ∨ x2 ∨ ... ∨ xn garantiert, dass mindestens eine der Variablen x1,x2,...xn auf 1 gesetzt werden muss, um U zu erfüllen. Weil der zweite Teil der Formel die elementare Disjunktion xi ∨ xj für alle Paare i, j ∈{1,...,n},i ̸= j enthält, dürfen xi und xj nicht zugleich den Wert 1 annehmen. Daher garantiert der zweite Teil der Formel, 6.6 NP-Vollständigkeit 203 dass höchstens eine Variable aus {x1,...,xn} den Wert 1 annehmen darf. Wir bemerken, dass die Länge der Formel U (x1,x2,...,xn) quadratisch in der Anzahl n der Variablen ist. Jetzt konstruieren wir nacheinander die Formeln A, B, C, D, E, F und G. (a) Für jedes t ∈{0, 1, 2,...,p(|w|)} deﬁnieren wir At = U (H⟨0,t⟩,H⟨1,t⟩,...,H⟨p(|w|),t⟩). At ist nur dann erfüllt, wenn sich der Lesekopf von M zum Zeitpunkt t genau auf einer Position i ∈{0, 1,...,p(|w|)} des Bandes beﬁndet. Die Erfüllung der Formel A = A0 ∧ A1 ∧ ... ∧ Ap(|w|) = ⋀ 0≤i≤p(|w|) Ai garantiert, dass sich der Lesekopf zu jedem Zeitpunkt t ∈{0, 1,...,p(|w|)} auf genau einer Position des Bandes beﬁndet. Die Anzahl der Literale in A ist in O((p(|w|)) 3), weil die Anzahl der Literale in At quadratisch in p(|w|) + 1 ist. (b) Für alle i ∈{0, 1, 2,...,p(|w|)}, t ∈{0, 1,...,p(|w|)} deﬁnieren wir Bi,t = U (C⟨i, 1,t⟩,C⟨i, 2,t⟩,...,C⟨i, m, t⟩). Bi,t ist erfüllt, wenn die i-te Position des Bandes nach t Berechnungsschritten von M genau ein Symbol enthält. Weil |Γ| = m eine Konstante ist, ist die Anzahl der Literale in Bi,t in O(1). Die Erfüllung der Formel B = ⋀ 0≤i,t≤p(|w|) Bi,t garantiert, dass alle Positionen des Bandes zu jedem Zeitpunkt genau ein Symbol enthalten. Die Anzahl der Literale in B ist oﬀensichtlich in O((p(|w|)) 2). (c) Wir deﬁnieren für alle t ∈{0, 1,...,p(|w|)} Ct = U (S⟨0,t⟩,S⟨1,t⟩,...,S⟨s, t⟩). Wenn eine Belegung von S⟨0,t⟩,...,S⟨s, t⟩ die Formel Ct erfüllt, dann ist M zum Zeitpunkt t in genau einem Zustand. Weil |Q| = s + 1 eine Konstante ist, ist die Anzahl der Literale in Ct in O(1). Oﬀensichtlich garantiert uns C = ⋀ 0≤t≤p(|w|) Ct, dass M zu jedem Zeitpunkt genau in einem Zustand ist. Die Anzahl der Literale in C ist in O(p(|w|)). (d) Die Formel Di,j,t =(C⟨i, j, t⟩↔ C⟨i, j, t +1⟩) ∨ H⟨i, t⟩ für 0 ≤ i ≤ p(|w|), 1 ≤ j ≤ m, 0 ≤ t ≤ p(|w|) − 1 sagt aus, dass ein nicht gelesenes Symbol nicht geändert werden darf (wenn H⟨i, t⟩ = 0, dann muss im nächsten Schritt das Symbol auf der i-ten Position unverändert bleiben). Oﬀensichtlich kann 204 6 Komplexitätstheorie man Di,j,t in eine KNF mit O(1) Literalen umwandeln. 15 Die gesuchte Formel ist dann D = ⋀ 0≤i≤p(|w|) 1≤j≤m 0≤t≤p(|w|)−1 Di,j,t und D enthält O((p(|w|)) 2) Literale, da m eine Konstante ist. (e) Wir betrachten für alle i ∈{0, 1, 2,...,p(|w|)}, j ∈{1,...,m}, t ∈{0, 1,...,p(|w|)}, k ∈{0, 1,...,s} die Formel Ei,j,k,t = C⟨i, j, t⟩∨ H⟨i, t⟩∨ S⟨k, t⟩∨ ⋁ l (C⟨i, jl,t +1⟩∧ S⟨kl,t +1⟩∧ H⟨il,t +1⟩) wobei l über alle möglichen Aktionen der NTM M für das Argument (qk,Xj) läuft, mit (qkl ,Xjl ,zl) ∈ δ(qk,Xj),zl ∈{L, R, N} und il = i + ϕ(zl), wobei ϕ(L) = −1,ϕ(R)=1,ϕ(N)=0. Ei,j,k,t kann man betrachten als die Disjunktion folgender vier Bedingungen: – C⟨i, j, t⟩, d. h., die i-te Position des Bandes enthält nicht Xj zum Zeitpunkt t. – H⟨i, t⟩, d. h., der Kopf ist nicht auf der i-ten Position zum Zeitpunkt t. – S⟨k, t⟩, d. h., M ist nicht im Zustand qk zum Zeitpunkt t. – Die Änderung der t-ten Konﬁguration entspricht einer möglichen Aktion bei dem Argument (qk,Xj) und der Kopfposition i. Die Idee der Konstruktion von Ei,j,k,t ist jetzt oﬀensichtlich: Wenn keine der ersten drei Bedingungen erfüllt ist, dann ist (qk,Xj) das aktuelle Argument für den (t +1)- ten Schritt und der Kopf ist auf der i-ten Position des Bandes. In diesem Fall müssen also die Änderungen genau nach der δ-Funktion von M für das Argument (qk,Xj) vorgenommen werden. Wenn man die l-te mögliche Aktion (qkl ,Xjl ,zl)beim Argument (qk,Xj) auswählt, dann muss Xj an der i-ten Position durch Xjl ersetzt werden, der neue Zustand muss qkl sein, und der Kopf muss sich entsprechend dem zl bewegen. Ei,j,k,t beinhaltet O(1) Literale, da l eine Konstante ist, und deshalb hat auch die Umwandlung in KNF O(1) Literale. Somit hat die gesuchte Formel E = ⋀ 0≤i,t≤p(|w|) 1≤j≤m 0≤k≤s Ei,j,k,t O((p(|w|)) 2) Literale. 15Die Formel x ↔ y ist äquivalent zu der Formel (x ∨ y) ∧ (x ∨ y). Somit ist Di,j,t ↔ (C⟨i, j, t⟩∨ C⟨i, j, t + 1⟩∨ H⟨i, t⟩) ∧ (C⟨i, j, t⟩∨ C⟨i, j, t +1⟩∨ H⟨i, t⟩). 6.6 NP-Vollständigkeit 205 (f) Die Anfangskonﬁguration von M auf w muss auf dem Band ¢w haben, der Kopf muss auf die 0-te Position des Bandes zeigen und M muss im Zustand q0 sein. Wenn w = Xj1Xj2 ...Xjn für jr ∈{1, 2,...,m}, n ∈ N und X1 = ¢, dann kann man die Anforderung, dass die Konﬁguration zum Zeitpunkt 0 die Anfangskonﬁguration von M auf w ist, wie folgt ausdrücken: F = S⟨0, 0⟩∧ H⟨0, 0⟩∧ C⟨0, 1, 0⟩ ∧ ⋀ 1≤r≤n C⟨r, jr, 0⟩∧ ⋀ n+1≤d≤p(|w|) C⟨d, m, 0⟩. Die Anzahl der Literale in F ist in O(p(|w|)) und F ist in KNF. (g) Die einfache Formel G = S⟨s − 1,p(|w|)⟩ garantiert, dass die letzte (also die p(|w|)-te) Konﬁguration den Zustand qaccept enthält. Gemäß der Konstruktion der Formel BM (w) ist es oﬀensichtlich, dass BM (w) genau dann erfüllbar ist, wenn eine akzeptierende Berechnung von M auf w existiert. Die Formel BM (w) kann algorithmisch aus den Daten M , w und p(|w|) generiert werden. Die Zeitkomplexität zur Berechnung von BM (w) ist asymptotisch linear in der Länge der Darstellung von BM (w). Wie wir ausgerechnet haben, ist die Anzahl der Literale in BM (w)in O((p(|w|)) 3). Wenn wir BM (w)überΣlogic darstellen, muss jede Variable binär kodiert werden. Weil die Anzahl der Variablen in O((p(|w|))2) liegt, kann jede Variable durch O(log2(|w|)) Bits repräsentiert werden. Damit ist die Länge von BM (w) und somit die Zeitkomplexität von BM in O((p(|w|))3 · log2(|w|)). Also ist BM eine polynomielle Reduktion von L(M ) auf SAT. \u0002 Im Beweis von Satz 6.9 haben wir gezeigt, dass alle Sprachen aus NP auf SAT reduzierbar sind. Dies bedeutet nichts anderes, als dass man jede Instanz eines Problems aus NP als das Problem der Erfüllbarkeit einer Formel darstellen kann. Daraus resultiert die Sichtweise, dass „die Sprache der Boole’schen Formeln“ stark genug ist, um jedes Problem aus NP darzustellen. Die NP-Vollständigkeit von SAT ist der Startpunkt 16 zur Klassiﬁzierung der Entschei- dungsprobleme bezüglich ihrer Zugehörigkeit zu P. Um die NP-Vollständigkeit anderer Probleme zu beweisen, benutzen wir die Methode der Reduktion, die auf folgender Beobachtung basiert. Lemma 6.8. Seien L1 und L2 zwei Sprachen. Falls L1 ≤p L2 und L1 ist NP-schwer, dann ist auch L2 NP-schwer. Aufgabe 6.21. Beweisen Sie Lemma 6.8. 16SAT spielt also in der Komplexitätstheorie eine ähnliche Rolle wie Ldiag in der Theorie der Berechen- barkeit. 206 6 Komplexitätstheorie Im Folgenden benutzen wir Lemma 6.8, um die NP-Vollständigkeit einiger Sprachen aus NP zu beweisen. Unser erstes Ziel ist zu zeigen, dass „die Sprache der Graphen (Relationen)“ auch ausdrucksstark genug ist, um jedes Problem aus NP darzustellen. Zur Veranschaulichung der Argumentation werden wir im Folgenden direkt mit Objekten wie Graphen und Formeln arbeiten statt streng formal über die Kodierungen von Graphen und Formeln zu sprechen. Somit gilt SAT = {Φ | Φ ist eine erfüllbare Formel in KNF}, CLIQUE = {(G, k) | G ist ein ungerichteter Graph, der eine k-Clique enthält}, VC = {(G, k) | G ist ein ungerichteter Graph mit einer Knotenüberdeckung (vertex cover) der Mächtigkeit höchstens k}, wobei eine Knotenüberdeckung eines Graphen G =(V, E) jede Menge von Knoten U ⊆ V ist, so dass jede Kante aus E mindestens einen Endpunkt in U hat. Sei Φ eine Formel und sei ϕ eine Belegung der Variablen von Φ. Im Folgenden bezeichnen wir durch ϕ(Φ) den Wahrheitswert von Φ bei der Belegung ϕ. Also ist Φ genau dann erfüllbar, wenn eine Belegung ϕ mit ϕ(Φ) = 1 existiert. Lemma 6.9. SAT ≤p CLIQUE. Beweis. Sei Φ = F1 ∧ F2 ∧ ... ∧ Fm eine Formel in KNF, Fi =(li1 ∨ li2 ∨· · · ∨ liki ),ki ∈ N −{0} für i =1, 2,...,m. Wir konstruieren eine Eingabe (G, k) des Cliquenproblems, so dass Φ ∈ SAT ⇐⇒ (G, k) ∈ CLIQUE. Wir setzen • k = m, • G =(V, E), wobei • V = {[i, j] | 1 ≤ i ≤ m, 1 ≤ j ≤ ki}, das heißt wir nehmen einen Knoten für jedes Auftreten eines Literals in Φ, • E = {{[i, j], [r, s]}| für alle [i, j], [r, s] ∈ V, mit i ̸= r und lij ̸= lrs},d. h., eine Kante {u, v} verbindet nur Knoten aus unterschiedlichen Klauseln, vorausgesetzt das Literal von u ist nicht die Negation des Literals von v. Betrachten wir die folgende Formel Φ=(x1 ∨ x2) ∧ (x1 ∨ x2 ∨ x3) ∧ (x1 ∨ x3) ∧ x2. Dann ist k = 4 und der konstruierte Graph ist in Abbildung 6.6 dargestellt. Es ist klar, dass (G, k) durch einen polynomiellen Algorithmus aus Φ konstruiert werden kann. Wir zeigen jetzt, Φ ist erfüllbar ⇐⇒ G enthält eine Clique der Größe k = m. (6.1) 6.6 NP-Vollständigkeit 207 x1 x1 x2 x3 x1 x2 x2 x3 [1, 1] [1, 2] [2, 1] [2, 2] [2, 3] [3, 1] [3, 2] [4, 1] F1 F2 F3 F4 Abbildung 6.6 Die Idee des Beweises ist folgende. Die Literale lij und lrs sind in G verbunden, wenn beide aus unterschiedlichen Klauseln kommen (i ̸= r) und beide gleichzeitig den Wert 1 annehmen können. Somit entspricht eine Clique in G den Belegungen von Variablen von Φ, die die Literale der Knoten der Clique erfüllen (zu 1 auswerten). Zum Beispiel bestimmt die Clique {[1, 1], [2, 1], [3, 2], [4, 1]} in Abbildung 6.6 die Belegung x1 =1, x3 = 1 und x2 =1 (x2 =0). Wir beweisen die Äquivalenz (6.1) durch zwei Implikationen. (i) „=⇒“: Sei Φ eine erfüllbare Formel. Dann existiert eine Belegung ϕ, so dass ϕ(Φ) = 1. Es gilt ϕ(Fi) = 1 für alle i ∈{1,...,m}. Also existiert für jedes i ∈{1,...,m} ein Index αi ∈{1,...,ki}, so dass ϕ(liαi) = 1. Wir behaupten, dass die Knotenmenge {[i, αi] | 1 ≤ i ≤ m} einen vollständigen Teilgraphen von G bildet. Es ist klar, dass [1,α1], [2,α2],..., [m, αm] aus unterschiedlichen Klauseln sind. Die Gleichheit liαi = ljαj für irgendwelche i, j, i ̸= j, impliziert ω(liαi) ̸= ω(ljαj ) für jede Belegung ω, und deshalb ist ϕ(liαi)= ϕ(ljαj ) = 1 nicht möglich. Also ist liαi ̸= ljαj für alle i, j ∈{1,...,m},i ̸= j, und {[i, αi], [j, αj]}∈ E für alle i, j ∈{1,...,m},i ̸= j. Somit ist {[i, αi] | 1 ≤ i ≤ m} eine Clique der Größe m. (ii) „⇐=“: Sei Q eine Clique von G mit k = m Knoten. Weil zwei Knoten durch eine Kante in G nur dann verbunden sind, wenn sie zwei Literalen aus unterschiedlichen Klauseln entsprechen, existieren α1,α2,...,αm mit αp ∈{1, 2,...,kp} für p = 1,...,m, so dass {[1,α1], [2,α2], . .., [m, αm]} die Knoten von Q sind. Folgend 208 6 Komplexitätstheorie v1v1 v2 v2 v3 v3v4 v4 v5 v5 G G Abbildung 6.7 der Konstruktion von G existiert eine Belegung ϕ der Variablen von Φ, so dass ϕ(l1α1)= ϕ(l2α2)= ··· = ϕ(lmαm) = 1. Das impliziert direkt ϕ(F1)= ϕ(F2)= ··· = ϕ(Fm) = 1 und somit erfüllt ϕ die Formel Φ. \u0002 Lemma 6.10. CLIQUE ≤p VC. Beweis. Sei G =(V, E) und k eine Eingabe des Clique-Problems. Wir konstruieren eine Eingabe (G, m) des Vertex-Cover-Problems wie folgt: • m = |V |− k, • G =(V, E), wobei E = {{u, v}| u, v ∈ V, u ̸= v, {u, v} /∈ E}. Abbildung 6.7 zeigt die Konstruktion des Graphen G aus einem gegebenen Graphen G. Es ist klar, dass man diese Konstruktion in linearer Zeit realisieren kann. Um „(G, k) ∈ CLIQUE ⇐⇒ (G, |V |− k) ∈ VC“ zu beweisen, reicht es zu zeigen: S ⊆ V ist eine Clique in G ⇐⇒ V − S ist eine Knotenüberdeckung in G. In Abbildung 6.7 sehen wir, dass die Clique {v1,v2,v5} von G die Knotenüberdeckung {v3,v4} in G bestimmt. Wir beweisen diese Behauptung durch zwei Implikationen. (i) „=⇒“: Sei S eine Clique in G. Also gibt es keine Kante zwischen den Knoten aus S in G. Daher ist jede Kante aus G adjazent zu mindestens einem Knoten in V − S. Also ist V − S eine Knotenüberdeckung in G. (ii) „⇐=“: Sei C ⊆ V eine Knotenüberdeckung in G. Gemäß der Deﬁnition einer Knotenüberdeckung ist jede Kante von G adjazent zu mindestens einem Knoten aus C. Also gibt es keine Kante {u, v} in E für u, v ∈ V − C. Deswegen gilt {u, v}∈ E für alle u, v ∈ V − C, u ̸= v. Somit ist V − C eine Clique in G. \u0002 6.6 NP-Vollständigkeit 209 Das folgende Resultat zeigt, dass das SAT-Problem schwer bleibt, auch wenn man sich auf eine Teilklasse von Formeln beschränkt. Wir sagen, dass eine Formel in 3KNF ist, falls sie in KNF ist und jede Klausel höchstens aus drei Literalen besteht. Das 3SAT- Problem ist zu bestimmen, ob eine Formel in 3KNF erfüllbar ist. Im Folgenden betrachten wir eine Belegung ϕ von Boole’schen Variablen aus einer Menge X = {x1,...,xn} als eine Abbildung ϕ : X →{0, 1}.Sei Y = {y1,...,yr} eine Menge Boole’scher Variablen. Wir sagen, dass ω : X ∪ Y →{0, 1} eine Erweiterung von ϕ : X →{0, 1} ist, falls ω(z)= ϕ(z) für alle z ∈ X. Lemma 6.11. SAT ≤p 3SAT. Beweis. Sei F = F1 ∧ F2 ∧· · ·∧ Fm eine Formel in KNF über einer Menge Boole’scher Variablen {x1,...,xn}. Wir konstruieren eine Formel C in 3KNF (alle Klauseln enthalten höchstens 3 Literale), so dass F ist erfüllbar (F ∈ SAT) ⇐⇒ C ist erfüllbar (C ∈ 3SAT). Die polynomielle Reduktion führen wir für jede der Klauseln F1,...,Fm einzeln wie folgt durch: Falls Fi weniger als 4 Literale enthält, dann setzen wir Ci = Fi.Sei Fi = z1 ∨ z2 ∨ ··· ∨ zk mit k ≥ 4,zi ∈{x1, x1,...,xn, xn}. Wir konstruieren Ci über Variablen {x1,...,xn,yi,1,yi,2,...,yi,k−3},wobei yi,1, yi,2, ..., yi,k−3 neue Variablen sind, die bei der Konstruktion von Cj mit j ̸= i nicht benutzt werden. Ci =(z1 ∨ z2 ∨ yi,1) ∧ (yi,1 ∨ z3 ∨ yi,2) ∧ (yi,2 ∨ z4 ∨ yi3) ∧· · · ∧ (yi,k−4 ∨ zk−2 ∨ yi,k−3) ∧ (yi,k−3 ∨ zk−1 ∨ zk). Für Fi = x1 ∨ x3 ∨ x2 ∨ x7 ∨ x9 erhalten wir zum Beispiel Ci =(x1 ∨ x3 ∨ yi,1) ∧ (yi,1 ∨ x2 ∨ yi,2) ∧ (yi,2 ∨ x7 ∨ x9). Um zu zeigen, dass F = F1 ∧· · ·∧ Fm genau dann erfüllbar ist, wenn C = C1 ∧· · ·∧ Cm erfüllbar ist, reicht es, die folgende Behauptung zu beweisen. Eine Belegung ϕ der Variablen aus {x1,...,xn} erfüllt Fi ⇐⇒ es existiert eine Erweiterung ϕ′ von ϕ auf {x1,...,xn,yi,1,...,yi,k−3}, die Ci erfüllt. (i) „=⇒“: Sei ϕ eine Belegung der Variablen in {x1,x2,...,xn}, so dass ϕ(Fi)= 1. Also existiert ein j ∈{1,...,k} mit ϕ(zj) = 1. Wir nehmen ϕ′ : {x1,...,xn, yi,1,...,yi,k−3}→ {0, 1}, so dass (a) ϕ′(xl)= ϕ(xl) für l =1,...,n, (b) ϕ ′(yi,1)= ··· = ϕ′(yi,j−2)=1 und (c) ϕ′(yi,j−1)= ··· = ϕ′(yi,k−3)=0. Weil ϕ′(zj) = 1, ist die (j − 1)-te Klausel von Ci erfüllt. ϕ′(yi,r) = 1 garantiert die Erfüllung der r-ten Klausel von Ci für r =1,...,j−2. ϕ′(yi,s) = 0 (das heißt yi,s =1) garantiert die Erfüllung der (s + 1)-ten Klausel von Ci für s = j − 1,j,...,k − 3. Damit erfüllt ϕ′ alle k − 2 Klauseln von Ci. 210 6 Komplexitätstheorie (ii) „⇐=“: Sei ϕ eine Belegung, so dass ϕ(Fi) = 0. Wir beweisen, dass keine Erweiterung ϕ′ von ϕ existiert, so dass ϕ′(Ci)=1. ϕ(Fi) = 0 impliziert ϕ(z1)= ϕ(z2)= ··· = ϕ(zk) = 0. Also muss man, um die erste Klausel zu erfüllen, für yi,1 den Wert 1 einsetzen. Dann ist ϕ′(yi,1)=0 und ϕ′(yi,2) muss 1 sein, um die zweite Klausel zu erfüllen. Auf diese Weise bekommen wir ϕ′(yi,1)= ϕ′(yi,2)= ··· = ϕ′(yi,k−3)=1, um diese ersten k − 3 Klauseln zu erfüllen. Dann ist aber ϕ′(yi,k−3) = 0, und weil ϕ(zk−1)= ϕ(zk) = 0, bleibt die letzte Klausel unerfüllt. \u0002 Aufgabe 6.22. Beweisen Sie folgende polynomielle Reduktionen: (a) VC ≤p CLIQUE. (b) 3SAT ≤p VC. Das Konzept der NP-Vollständigkeit entwickelte sich zu der Basismethode zur Klas- siﬁzierung der Schwierigkeit von algorithmischen Problemen. Wir kennen heute mehr als 3000 NP-vollständige Probleme. Das vorgestellte Konzept der NP-Vollständigkeit funktioniert aber nur für Entscheidungsprobleme. Im Folgenden wollen wir dieses Konzept so modiﬁzieren, dass es auch zur Klassiﬁzierung von Optimierungsproblemen geeignet ist. Dazu brauchen wir zuerst Klassen von Optimierungsproblemen, die eine ähnliche Bedeutung wie die Klassen P und NP für Entscheidungsprobleme haben. Wir beginnen mit der Klasse NPO als Analogie zur Klasse NP für Optimierungsprobleme. Deﬁnition 6.12. NPO ist die Klasse der Optimierungsprobleme, wobei U =(ΣI , ΣO,L, M, cost, goal) ∈ NPO, falls folgende Bedingungen erfüllt sind: (i) L ∈ P, {Es kann eﬃzient veriﬁziert werden, ob ein x ∈ Σ∗ I eine zulässige Eingabe ist.} (ii) es existiert ein Polynom pU , so dass (a) für jedes x ∈ L und jedes y ∈M(x), |y|≤ pU (|x|), {Die Größe jeder zulässigen Lösung ist polynomiell in der Eingabegröße.} (b) es existiert ein polynomieller Algorithmus A, der für jedes y ∈ Σ∗ O und jedes x ∈ L mit |y|≤ pU (|x|) entscheidet, ob y ∈M(x) oder nicht, (iii) die Funktion cost kann man in polynomieller Zeit berechnen. Wir sehen, dass ein Optimierungsproblem U in NPO ist, falls 1. man eﬃzient überprüfen kann, ob ein gegebenes Wort ein Problemfall (eine Instanz) von U ist, 2. die Größe der Lösungen polynomiell in der Größe der Eingabe (des Problemfalls) ist und man in polynomieller Zeit veriﬁzieren kann, ob ein y eine zulässige Lösung für einen gegebenen Problemfall ist, und 3. man die Kosten der zulässigen Lösung eﬃzient berechnen kann. Der Bedingung (ii.b) folgend sehen wir die Analogie zwischen NPO und VP. Das konzeptionell Wichtigste ist aber, dass die Bedingungen (i), (ii) und (iii) natürlich sind, 6.6 NP-Vollständigkeit 211 weil sie die Schwierigkeit von U auf den Bereich der Optimierung einschränken und damit die Einordnung der praktischen Lösbarkeit von U unabhängig machen von solchen Entscheidungsproblemen wie denen, ob eine Eingabe eine Instanz von U repräsentiert oder ob y eine zulässige Lösung für x ist. Damit liegt der Schwierigkeitsgrad von Problemen in NPO eindeutig in der Suche nach einer optimalen Lösung in der Menge aller zulässigen Lösungen. Die folgende Begründung zeigt, dass MAX-SAT in NPO liegt. 1. Man kann eﬃzient entscheiden, ob ein x ∈ (Σlogic)∗ eine Boole’sche Formel Φx in KNF kodiert. 2. Für jedes x hat jede Belegung α ∈{0, 1}∗ der Variablen der Formel Φx die Eigen- schaft |α| < |x|, und man kann in linearer Zeit veriﬁzieren, ob |α| gleich der Anzahl der Variablen in Φx ist. 3. Für jede gegebene Belegung α der Variablen von Φx kann man in linearer Zeit bezüglich |x| die Anzahl der erfüllten Klauseln berechnen und so die Kosten der zulässigen Lösung α bestimmen. Betrachten wir jetzt folgende Optimierungsprobleme. Das Problem des maximalen Schnitts, MAX-CUT, ist, für einen Graphen G =(V, E) einen maximalen Schnitt zu ﬁnden. Ein Schnitt von G =(V, E) ist jedes Paar (V1,V2) mit V1 ∪ V2 = V und V1 ∩ V2 = ∅. Der Preis eines Schnitts (V1,V2)von G ist die Anzahl der Kanten zwischen V1 und V2, d. h., cost(V1,V2)= |E ∩{{v, u}| v ∈ V1,u ∈ V2}|. Das Problem der minimalen Knotenüberdeckung, MIN-VC, ist, für einen Graphen G eine minimale Knotenüberdeckung zu ﬁnden. Aufgabe 6.23. Geben Sie formale Deﬁnitionen der Optimierungsprobleme MAX-CUT und MIN-VC an und zeigen Sie, dass beide in NPO liegen. Die folgende Deﬁnition deﬁniert auf natürliche Weise die Klasse PO von Optimie- rungsproblemen, die die gleiche Bedeutung wie die Klasse P für Entscheidungsprobleme hat. Deﬁnition 6.13. PO ist die Klasse von Optimierungsproblemen U =(ΣI , ΣO, L, M, cost, goal), so dass (i) U ∈ NPO und (ii) es existiert ein polynomieller Algorithmus A, so dass A(x) für jedes x ∈ L eine optimale Lösung für x ist. Die Deﬁnition der NP-Schwere eines Optimierungsproblems erhalten wir jetzt durch geschickte Reduktion zu einem NP-schweren Entscheidungsproblem. 212 6 Komplexitätstheorie Deﬁnition 6.14. Sei U =(ΣI , ΣO,L, M, cost, goal) ein Optimierungsproblem aus NPO. Die Schwellenwert-Sprache für U ist LangU = {(x, a) ∈ L × (Σbool) ∗ | OptU (x) ≤ Nummer(a)}, falls goal = Minimum, und LangU = {(x, a) ∈ L × (Σbool) ∗ | OptU (x) ≥ Nummer(a)}, falls goal = Maximum. Wir sagen, dass U NP-schwer ist, falls LangU NP-schwer ist. Zuerst zeigen wir, dass das in Deﬁnition 6.14 vorgestellte Konzept der NP-Schwere für Optimierungsprobleme zum Beweisen von Aussagen der Form U/∈ PO unter der Voraussetzung P ̸= NP geeignet ist. Lemma 6.12. Falls ein Optimierungsproblem U ∈ PO, dann LangU ∈ P. Beweis. Falls U ∈ PO, dann existiert ein polynomieller Algorithmus A, der für jedes x ∈ L eine optimale Lösung für x berechnet und damit OptU (x) bestimmt. Also kann man A benutzen, um LangU zu entscheiden. \u0002 Satz 6.10. Sei U ∈ NPO.Falls U NP-schwer ist und P ̸=NP, dann U/∈ PO. Beweis. Wir beweisen Satz 6.10 indirekt. Angenommen U ∈ PO. Nach Lemma 6.12 gilt LangU ∈ P. Weil U NP-schwer ist, ist auch LangU NP-schwer. Somit ist LangU eine NP-schwere Sprache in P, was P = NP impliziert. \u0002 Die folgenden Beispiele zeigen, dass Deﬁnition 6.14 eine einfache Methode zum Beweisen der NP-Schwere von Optimierungsproblemen bietet. Lemma 6.13. MAX-SAT ist NP-schwer. Beweis. Es reicht zu zeigen, dass LangMAX-SAT NP-schwer ist. Wir zeigen SAT ≤p LangMAX-SAT.Sei x ∈ L die Kodierung einer Formel Φx mit m Klauseln. Wir nehmen (x, Bin(m)) als Eingabe für das Entscheidungsproblem (Σlogic, LangMAX-SAT). Es ist klar, dass (x, Bin(m)) ∈ LangMAX-SAT ⇐⇒ Φx ist erfüllbar. \u0002 Das Problem der maximalen Clique, MAX-CL, ist, für einen gegebenen Graphen eine maximale Clique in G zu ﬁnden. Lemma 6.14. MAX-CL ist NP-schwer. Beweis. Man bemerke, dass CLIQUE = LangMAX-CL.Da CLIQUE NP-schwer ist, sind wir fertig. \u0002 Aufgabe 6.24. Beweisen Sie, dass MAX-CUT und MIN-VC NP-schwer sind. 6.7 Zusammenfassung 213 6.7 Zusammenfassung Das Hauptziel der Komplexitätstheorie ist die Klassiﬁzierung der algorithmischen Proble- me bezüglich der Menge der Rechnerressourcen, die benötigt werden, um die Probleme zu lösen. Dabei erforscht man quantitative Gesetze der Informationsverarbeitung und die Grenze der praktischen algorithmischen Lösbarkeit. Die wichtigsten Komplexitätsmaße sind die Zeitkomplexität und die Speicherplatzkom- plexität. Das Basismodell eines Rechners ist in der abstrakten Komplexitätstheorie die Mehrband-Turingmaschine. Die Komplexität einer MTM (eines Algorithmus) betrachtet man als eine Funktion f der Eingabelänge n,wobei f (n) die maximale Komplexität über alle Berechnungen auf Eingaben der Länge n ist. Diese Komplexitätsmessung nennt man die Messung im schlechtesten Fall. Es existieren algorithmisch lösbare Probleme von beliebig hohem Schwierigkeitsgrad. Algorithmen von exponentieller Zeitkomplexität hält man nicht für praktisch durchführbar. Die praktische Lösbarkeit von Problemen wird mit der polynomiellen Zeitkomplexität verbunden. Die Klasse P ist die Menge aller Entscheidungsprobleme, die man mit Algo- rithmen in polynomieller Zeit lösen kann. Die Deﬁnition der Klasse P ist robust in dem Sinne, dass sie unabhängig von der Wahl des Rechnermodells ist. Die Zeitkomplexität der Arbeit einer nichtdeterministischen MTM M auf einer Eingabe w ist die Länge einer kürzesten akzeptierenden Berechnung von M auf w. Die typische Arbeit eines nichtdeterministischen Algorithmus besteht aus nichtdeterministischem Raten und nachfolgender deterministischer Veriﬁkation des Geratenen. Die Klasse NP ist die Klasse aller Sprachen, die nichtdeterministisch in polynomieller Zeit entscheidbar sind. Die Frage, ob P eine echte Teilmenge von NP ist, ist wahrscheinlich die bekannteste bisher unbeantwortete Frage in der Theoretischen Informatik. Die Klasse NP enthält viele praktisch interessante Probleme, von denen man nicht weiss, ob sie in P liegen. Man kann zeigen, dass die Frage, ob P ungleich NP ist, äquivalent zu der Frage ist, ob es schwerer ist, mathematische Beweise algorithmisch zu ﬁnden als gegebene Beweise algorithmisch zu veriﬁzieren. Es ist uns nicht gelungen, Beweistechniken für untere Schranken der Zeitkomplexität konkreter Probleme zu entwickeln. Damit fehlt uns die methodologische Grundlage zur Klassiﬁzierung der Probleme in praktisch lösbare und praktisch unlösbare. Das Konzept der NP-Vollständigkeit ermöglicht uns aber, Resultate der Art L/∈ P unter der zusätzlichen Voraussetzung P ̸= NP zu zeigen. Die NP-vollständigen Probleme sind die schwersten Probleme in NP in dem Sinne, dass, wenn nur eines von ihnen in P wäre, P = NP gelten müsste. Die Hierarchiesätze wurden bei Hartmanis, Stearns und Lewis [HS65, HSL65] bewiesen. Die Begriﬀe der polynomiellen Reduktion und der NP-Vollständigkeit gehen auf die Arbeiten von Cook [Coo71], Levin [Lev73] und Karp [Kar72] zurück. Das klassische Buch von Garey und Johnson [GJ79] bietet eine detaillierte Darstellung der Theorie der NP-Vollständigkeit. Hervorragende Präsentationen des Themas „Praktische Lösbarkeit“ kann man bei Lewis und Papadimitriou [LP78] und Stockmayer und Chandra [SC79] ﬁnden. Es gibt mehrere gute Bücher, die sich mit Komplexitätstheorie beschäftigen. Eine exzellente Einführung in die Komplexitätstheorie kann man in den Lehrbüchern von 214 6 Komplexitätstheorie Hopcroft und Ullman [HU79] und Sipser [Sip97] ﬁnden. Eine ausführliche Darstellung der Komplexitätstheorie bieten Arora und Barak [AB09], Bovet und Crescenzi [BC94], Papadimitriou [Pap94] und Balcázar, Díaz, und Gabarró [BDG88, BDG90]. Von der deutschsprachigen Literatur empfehlen wir wärmstens das Lehrbuch von Reischuk [Rei90]. Kontrollaufgaben 1. Wie deﬁniert man die Komplexität eines Algorithmus (einer TM) im schlechtesten Fall? Warum benutzt man am häuﬁgsten diese Art der Komplexitätsmessung? 2. Die Zeitkomplexität einer Turingmaschine kann man immer um einen konstanten Faktor beschleunigen, wenn ihre Zeitkomplexität superlinear ist. Dies ist überraschend und kann unnatürlich erscheinen. Hat dieses Phänomen eine natürliche Erklärung? Warum hat man diese „Schwäche“ des Turingmaschinenmodells in der Komplexitätstheorie akzeptiert? 3. Deﬁnieren Sie die O-, Ω- und Θ-Notation und erklären Sie, warum man sich oft nur mit einer asymptotischen Analyse zufrieden gibt. 4. Sei p(n) ein Polynom vom Grad d mit p(n) > 0 für alle n. Beweisen Sie, dass p(n) ∈ O(n d) gilt. Gilt auch p(n) ∈ Θ(n d)? 5. Wir haben gelernt, die Komplexität von Algorithmen (Turingmaschinen) zu messen. Jetzt möchten wir die Komplexität algorithmischer Probleme bestimmen. Wie gehen wir vor? Warum kann man nicht einfach sagen, dass die Komplexität eines Problems die Komplexität des optimalen Algorithmus für dieses Problem ist? 6. Bei der Messung der Komplexität von Algorithmen (Programmen) benutzt man zwei unterschiedliche Arten der Messung – die Messung mit dem uniformen Kostenmaß und die mit dem logarithmischen Kostenmaß. Diskutieren Sie die Vor- und Nachteile dieser Maße. 7. Welche sind die fundamentalen Komplexitätsklassen und wie werden die deﬁniert? 8. Wozu ist das Konzept der Konstruierbarkeit von Funktionen nützlich? 9. Welche Beziehungen gelten zwischen Speicherplatzkomplexitätsklassen und Zeitkomplexi- tätsklassen? 10. Wie deﬁniert man nichtdeterministische Komplexitätsklassen? Was für eine Rolle spielen dabei die zeit- und platzkonstruierbaren Funktionen? 11. Welche Simulationen von nichtdeterministischer Zeitkomplexität durch deterministische Zeitkomplexität kennen Sie? Formulieren Sie die Resultate als Beziehungen zwischen Komplexitätsklassen und geben Sie eine detaillierte Beschreibung und Analyse der Simula- tionstechniken. 12. Welche Gründe führten dazu, dass man die Klasse P der in polynomieller Zeit lösbaren Entscheidungsprobleme mit der Klasse der „praktisch“ lösbaren Probleme identiﬁziert? Warum hat man nicht beispielsweise die Klasse TIME(n 6) gewählt? 13. Erklären Sie die zwei unterschiedlichen Möglichkeiten, die Klasse NP zu deﬁnieren. Wie hängt die nichtdeterministische Zeitkomplexität mit der deterministischen Komplexität der algorithmischen Beweisveriﬁkation zusammen? 6.7 Zusammenfassung 215 14. Kann man die Klasse NTIME(2 n) durch deterministische Veriﬁzierer beschreiben, die in der Zeit O(2 n) arbeiten? Zur Begründung Ihrer Antwort führen Sie einen detaillierten Beweis. 15. Geben Sie Polynomialzeit-Veriﬁzierer für alle NP-vollständigen Sprachen an, die Sie kennen. 16. Beschreiben Sie das Konzept der polynomiellen Reduktion. Was sind die Ähnlichkeiten und die Unterschiede zur Reduktion in der Theorie der Berechenbarkeit? 17. Geben Sie eine Menge Boole’scher Variablen an, mit der man alle Situationen auf einem Schachbrett beschreiben kann, in der sich nur Türme auf dem Schachbrett beﬁnden. Beschreiben Sie die Konstruktion einer Formel über diesen Variablen, die genau dann erfüllt ist, wenn auf dem Schachbrett genau acht Türme stehen, die sich gegenseitig nicht bedrohen. 18. Sei kSAT das Erfüllungsproblem für Klauseln in kKNF, k ∈ N (eine Formel in KNF ist in kKNF, wenn alle Klauseln höchstens aus k Literalen bestehen). Beweisen Sie folgende polynomielle Reduktionen: (a) SAT ≤p 4SAT, (b) 5SAT ≤p CLIQUE, (c) CLIQUE ≤p 4SAT, (d) 4SAT ≤p 3SAT, (e) 5SAT ≤p VC, (f) VC ≤p SAT. 19. Erklären Sie, wie man das Konzept der NP-Schwere auf Optimierungsprobleme übertragen kann. 20. Beweisen Sie die folgende Behauptung. Sei U ein NP-schweres Optimierungsproblem. Falls U in polynomieller Zeit lösbar ist, dann gilt P = NP. Wenn euch ein Wissenschaftler sagt: „Dies ist das Ende, hier kann man nichts mehr machen“, dann ist er kein Wissenschaftler. L. Gould 7 Algorithmik für schwere Probleme 7.1 Zielsetzung Die Komplexitätstheorie liefert uns die Methoden zur Klassiﬁkation der algorithmischen Probleme bezüglich ihrer Komplexität. Die Algorithmentheorie ist dem Entwurf eﬃzienter Algorithmen zur Lösung konkreter Probleme gewidmet. In diesem Kapitel wollen wir uns mit dem Entwurf von Algorithmen für schwere (zum Beispiel NP-schwere) Probleme beschäftigen. Das mag etwas überraschend klingen, weil unter den in Kapitel 6 vorgestellten üblichen Annahmen der Komplexitätstheorie der Versuch, ein NP-schweres Problem zu lösen, an die Grenze des physikalisch Machbaren stößt. Zum Beispiel würde ein Algorithmus mit der Zeitkomplexität 2 n für eine Eingabe der Länge 100 mehr Zeit brauchen, als das Universum alt ist. Auf der anderen Seite sind viele schwere Probleme von einer enormen Wichtigkeit für die tägliche Praxis, und deshalb suchen Informatiker seit über 40 Jahren nach einer Möglichkeit, schwere Probleme in praktischen Anwendungen doch zu bearbeiten. Die Hauptidee dabei ist, ein schweres Problem durch eine (nach Möglichkeit kleine) Modiﬁkation oder eine Abschwächung der Anforderungen in ein eﬃzient lösbares Problem umzuwandeln. Die wahre Kunst der Algorithmik besteht darin, dass man die Möglichkeiten untersucht, wie man durch minimale (für die Praxis akzeptable) Änderungen der Problem- speziﬁkation oder der Anforderungen an die Problemlösung einen gewaltigen Sprung machen kann, und zwar von einer physikalisch nicht machbaren Berechnungskomplexität zu einer Angelegenheit von wenigen Minuten auf einem Standard-PC. Um solche Eﬀekte, die zur Lösung gegebener Probleme in der Praxis führen können, zu erzielen, kann man folgende Konzepte oder deren Kombinationen benutzen. Schwere Probleminstanzen im Gegensatz zu typischen Probleminstanzen Wir messen die Zeitkomplexität als Komplexität im schlechtesten Fall, was bedeutet, dass Time(n) die Komplexität der Berechnungen auf den schwersten Probleminstanzen der Größe n ist. Es ist aber oft so, dass die schwersten Probleminstanzen so unnatürlich sind, dass sie als Aufgabenstellungen in der Praxis gar nicht vorkommen. Deswegen ist es sinnvoll, die Analyse der Problemkomplexität in dem Sinne zu verfeinern, dass J. Hromkovič, Theoretische Informatik, DOI 10.1007/978-3-658-06433-4_7, © Springer Fachmedien Wiesbaden 2014 218 7 Algorithmik für schwere Probleme Tabelle 7.1 Komplexität n =10 n =50 n = 100 n = 300 2n 1024 16 Ziﬀern 31 Ziﬀern 91 Ziﬀern 2 n 2 32 ≈ 33 · 10 6 16 Ziﬀern 46 Ziﬀern (1.2) n ≈ 6 ≈ 9100 ≈ 83 · 10 6 24 Ziﬀern 10 · 2√ n ≈ 89 ≈ 1345 10240 ≈ 1,64 · 10 6 man die Probleminstanzen nach ihrem Schwierigkeitsgrad klassiﬁziert. Eine erfolgreiche Klassiﬁzierung könnte zur Speziﬁkation einer großen Teilmenge von eﬃzient lösbaren Probleminstanzen führen. Falls die in einer Anwendung typischen Eingaben zu einer solchen Probleminstanzklasse gehören, ist das Problem in dieser Anwendung gelöst. Exponentielle Algorithmen Man versucht nicht mehr, einen polynomiellen Algorithmus zu entwerfen, sondern einen Algorithmus mit einer superpolynomiellen oder sogar exponentiellen Zeitkomplexität. Die Idee dabei ist, dass einige superpolynomielle Funktionen für realistische Eingabegrößen nicht so große Werte annehmen. Tabelle 7.1 zeigt anschaulich, dass (1.2) n Rechneroperatio- nen für n = 100 oder 10 · 2 √ n Operationen für n = 300 in ein paar Sekunden durchführbar sind. Abschwächung der Anforderungen Man kann die Anforderung, mit Sicherheit für jede Eingabe das richtige Resultat zu berechnen, auf unterschiedliche Art und Weise abschwächen. Typische Repräsentanten dieses Ansatzes sind randomisierte Algorithmen und Approximationsalgorithmen. Bei randomisierten Algorithmen tauscht man die deterministische Steuerung durch eine Zufallssteuerung aus. Dadurch können mit einer beschränkten Wahrscheinlichkeit falsche Resultate berechnet werden. Wenn die Fehlerwahrscheinlichkeit nicht größer als 10 −9 ist, dann weicht die Zuverlässigkeit solcher randomisierten Algorithmen von der eines korrekten deterministischen Algorithmus nicht allzu sehr ab. Die Approximationsalgorithmen benutzt man zur Lösung von Optimierungsproblemen. Anstatt zu fordern, eine optimale Lösung zu berechnen, geht man zu der Forderung über, eine Lösung zu berechnen, deren Kosten (Qualität) sich nicht allzu sehr von einer optimalen Lösung unterscheiden. Beide Ansätze können einen Sprung von exponentieller zu polynomieller Komplexität ermöglichen. Die Zielsetzung dieses Kapitels ist es, einige dieser Konzepte kurz vorzustellen. In Abschnitt 7.2 stellen wir die pseudopolynomiellen Algorithmen vor, die einen speziellen Ansatz zur Speziﬁkation einer Teilklasse eﬃzient lösbarer Probleminstanzen darstellen und so als eine Methode zur Realisierung des ersten Ansatzes gelten. Das Konzept der Approximationsalgorithmen wird in Abschnitt 7.3 vorgestellt. In Abschnitt 7.4 präsentieren wir die lokalen Algorithmen, die Möglichkeiten für die Realisierung aller drei oben vorgestellten Konzepte bieten. Die lokale Suche ist die Basis für mehrere Heuristiken. In Abschnitt 7.5 erklären wir die Heuristik des Simulated Annealing, die auf einer Analogie zu einem physikalischen Vorgang basiert. Dem Konzept der Randomisierung wird wegen seiner Wichtigkeit mit Kapitel 8 ein eigenes Kapitel gewidmet. 7.2 Pseudopolynomielle Algorithmen 219 7.2 Pseudopolynomielle Algorithmen In diesem Abschnitt betrachten wir eine spezielle Klasse von Problemen, deren Pro- bleminstanzen man als Folge von Zahlen interpretieren kann. Solche Probleme nennen wir Zahlprobleme und praktisch ist es so, dass wir jedes Problem mit Eingaben aus {0, 1, #} ∗ mit unbeschränkter Anzahl von Symbolen # in den zulässigen Eingaben als ein Zahlproblem verstehen können. Sei x = x1#x2# ... #xn,xi ∈{0, 1}∗ für i =1, 2,...,n ein Wort über {0, 1, #} ∗. Wir interpretieren x als folgenden Vektor Int(x) = (Nummer(x1), Nummer(x2),..., Nummer(xn)) von n natürlichen Zahlen. Jedes graphentheoretische Problem, 1 dessen Probleminstanzen man durch die Adjazenzmatrix repräsentieren kann, ist ein Zahlproblem, weil man jede Adjazenzmatrix als eine Folge von Nullen und Einsen darstellen kann. Das TSP ist ein Zahlproblem, wobei die Zahlenfolge die Kosten der einzelnen Kanten repräsentiert. Wir deﬁnieren für jedes x = x1#x2# ... #xn mit xi ∈{0, 1} ∗ für i =1, 2,...,n, MaxInt(x) = max{Nummer(xi) | i =1, 2,...,n}. Die Hauptidee des Konzeptes der pseudopolynomiellen Algorithmen ist die Suche nach Algorithmen, die eﬃzient auf solchen Eingaben x sind, bei denen MaxInt(x) nicht we- sentlich größer ist als |x|. Weil eine Zahl Nummer(y) exponentiell größer ist als die Länge ihrer binären Darstellung y, handelt es sich um eine echte Einschränkung. Deﬁnition 7.1. Sei U ein Zahlproblem und sei A ein Algorithmus, der U löst. Wir sagen, dass A ein pseudopolynomieller Algorithmus für U ist, falls ein Polynom p mit zwei Variablen existiert, so dass TimeA(x) ∈ O(p(|x|, MaxInt(x))) für alle Probleminstanzen x von U. Wir bemerken sofort, dass für Probleminstanzen x mit MaxInt(x) ≤ h(|x|) für ein Polynom h die Zeitkomplexität TimeA(x) polynomiell in |x| ist. Deﬁnition 7.2. Sei U ein Zahlproblem und sei h eine Funktion von N nach N. Das h-wertbeschränkte Teilproblem von U , Wert(h)- U , ist das Problem, das man aus U durch die Einschränkung der Menge der zulässigen Eingaben auf die Klasse der Probleminstanzen x mit MaxInt(x) ≤ h(|x|) erhält. Der nächste Satz zeigt, dass man auf diese Weise große Klassen von leichten Proble- minstanzen eines schweren Problems speziﬁzieren kann. 1Wie das Problem der Knotenüberdeckung oder das CLIQUE-Problem. 220 7 Algorithmik für schwere Probleme Satz 7.1. Sei U ein Zahlproblem und sei A ein pseudopolynomieller Algorithmus für U. Dann existiert für jedes Polynom h ein polynomieller Algorithmus für das Problem Wert(h)- U. 2 Beweis. Weil A ein pseudopolynomieller Algorithmus für U ist, existiert ein Polynom p mit zwei Variablen, so dass TimeA(x) ∈ O(p(|x|), MaxInt(x)) für jede Probleminstanz x von U.Weil h eine polynomielle Funktion ist, existiert eine Konstante c, so dass MaxInt(x) ∈ O(|x|c) für alle Probleminstanzen x von Wert(h)- U. Damit ist TimeA(x) ∈ O(p(|x|, |x|c)) und somit ist A ein polynomieller Algorithmus für Wert(h)- U. \u0002 Wir zeigen die Anwendung dieses Konzeptes für die Lösung des Rucksack-Problems, welches das folgende NP-schwere Optimierungsproblem ist. Eingabe: 2n+1 positive ganze Zahlen w1,w2,...,wn,c1,...,cn,b für ein n ∈ N−{0}. {Diese Zahlen repräsentieren n Objekte, wobei wi das Gewicht und ci der Nutzen des i-ten Objekts für i =1, 2,...,n ist. Die Zahl b ist eine obere Schranke für das Gesamtgewicht der Objekte, die man in einen gegebenen Rucksack packen kann.} Zulässige Lösungen: Für jedes I =(w1,w2,...,wn,c1,...,cn,b)ist M(I)= { T ⊆{1,...,n} ∣ ∣ ∣ ∣ ∣ ∑ i∈T wi ≤ b } . {Eine zulässige Lösung kann jede Teilmenge der Menge aller n Objekte sein, deren Gesamtgewicht die erlaubte Gewichtskapazität des Rucksacks b nicht überschreitet.} Kosten: Für alle Eingaben I und alle T ∈M(I), ist cost(T, I)= ∑ i∈T ci. {Die Kosten einer zulässigen Lösung T sind der Gesamtnutzen der Objekte, die in den Rucksack gepackt wurden.} Ziel: Maximum. Wir möchten jetzt einen pseudopolynomiellen Algorithmus für das Rucksack-Problem mit der Methode der dynamischen Programmierung entwerfen. Wir wollen die Lösung einer Probleminstanz I =(w1,w2,...,wn,c1,...,cn,b) so berechnen, dass wir mit der Teilinstanz I1 =(w1,c1,b) anfangen und über Ii =(w1,w2,...,wi,c1,...,ci,b) 2Falls U ein Entscheidungsproblem ist, bedeutet das, dass Wert(h)- U∈ P. Falls U ein Optimierungs- problem ist, bedeutet das, dass Wert(h)- U∈ PO. 7.2 Pseudopolynomielle Algorithmen 221 für i =2, 3,...,n bis zu I = In kommen. Genauer wollen wir für jede Instanz Ii und jedes k ∈{0, 1, 2,..., ∑i j=1 cj} ein Tripel (k, Wi,k,Ti,k) ∈ ⎧ ⎨ ⎩ 0, 1, 2,..., i∑ j=1 cj ⎫ ⎬ ⎭ ×{0, 1, 2,...,b}×P({1,...,i}) bestimmen, wobei Wi,k das minimale Gewicht ist, bei dem man den Nutzen k für die Instanz Ii erhalten kann. Die Menge Ti,k ⊆{1,...,i} ist die Menge der Indizes, die eine Lösung mit genau dem Nutzen k bei dem Gesamtgewicht Wi,k deﬁniert, d. h., ∑ j∈Ti,k cj = k und ∑ j∈Ti,k wj = Wi,k. Wir bemerken, dass mehrere Indexmengen den gleichen Nutzen k und das gleiche Ge- wicht Wi,k bestimmen können. In solchen Fällen wählen wir beliebig eine aus, um die Erreichbarkeit von (k, Wi,k) zu dokumentieren. Andererseits kann es vorkommen, dass ein Gesamtnutzen k in Ii nicht erreichbar ist. In diesem Fall wird kein Tripel für k erstellt. Im Folgenden bezeichnet TRIPLEi die Menge aller Tripel für Ii. Wir bemerken, dass |TRIPLEi|≤ i∑ j=1 cj +1. Ein wichtiger Punkt ist, dass man TRIPLEi+1 aus TRIPLEi in der Zeit O(TRIPLEi) berechnen kann. Um TRIPLEi+1 zu berechnen, berechnet man zuerst SETi+1 = TRIPLEi ∪{(k + ci+1,Wi,k + wi+1,Ti,k ∪{i +1}) | (k, Wi,k,Ti,k) ∈ TRIPLEi und Wi,k + wi+1 ≤ b} durch das zusätzliche Einpacken des (i + 1)-ten Objekts zu jedem Tripel in TRIPLEi, falls dabei das Gewicht b nicht überschritten wird. TRIPLEi+1 ist dann eine Teilmenge von SETi+1, bei der man für jeden erreichbaren Nutzen k ein Tripel aus SETi+1 mit minimalem Gewicht für den Nutzen k auswählt. Oﬀensichtlich bestimmt das Tripel mit dem maximalen Nutzen in TRIPLEn eine optimale Lösung der Probleminstanz I = In. Wir illustrieren die Berechnung der Tripel zunächst anhand der Probleminstanz I = (w1,w2,...,w5,c1,...,c5,b), wobei w1 =23,w2 =15,w3 =15,w4 =33,w5 =32, c1 =33,c2 =23,c3 =11,c4 =35,c5 =11 und b = 65. Oﬀensichtlich ist I1 = (23, 33, 65) und die einzig erreichbaren Nutzen sind 0 und 33. Damit gilt TRIPLE1 = {(0, 0, ∅), (33, 23, {1})}. Es gilt I2 = (23, 15, 33, 23, 65) und die einzig erreichbaren Nutzwerte für I2 sind 0, 23, 33 und 56. Somit erhält man TRIPLE2 = {(0, 0, ∅), (23, 15, {2}), (33, 23, {1}), (56, 38, {1, 2})}. 222 7 Algorithmik für schwere Probleme Es ist I3 = (23, 15, 15, 33, 23, 11, 65). Das Einpacken des dritten Objekts ist für jedes Tripel aus TRIPLE2 möglich und bei jedem neuen Tripel erhalten wir dabei einen neuen Nutzwert. Damit hat TRIPLE3 die doppelte Mächtigkeit wie TRIPLE2 (d. h. SET3 = TRIPLE3) und TRIPLE3 = {(0, 0, ∅), (11, 15, {3}), (23, 15, {2}), (33, 23, {1}), (34, 30, {2, 3}), (44, 38, {1, 3}), (56, 38, {1, 2}), (67, 53, {1, 2, 3})}. Für die Tripel (44, 38, {1, 3}), (56, 38, {1, 2}) und (67, 53, {1, 2, 3}) aus TRIPLE3 kann man das vierte Objekt in den Rucksack nicht mehr einpacken und so erhalten wir TRIPLE4 = TRIPLE3 ∪{(35, 33, {4}), (46, 48, {3, 4}), (58, 48, {2, 4}), (68, 56, {1, 4}), (69, 63, {2, 3, 4})}. Am Ende erhalten wir für die Instanz I = I5 TRIPLE5 = {(0, 0, ∅), (11, 15, {3}), (22, 47, {3, 5}), (23, 15, {2}), (33, 23, {1}), (34, 30, {2, 3}), (35, 33, {4}), (44, 38, {1, 3}), (45, 62, {2, 3, 5}), (46, 48, {3, 4}), (56, 38, {1, 2}), (58, 48, {2, 4}), (67, 53, {1, 2, 3}), (68, 56, {1, 4}), (69, 63, {2, 3, 4})}. Damit ist {2, 3, 4} die optimale Lösung für I, weil (69, 63, {2, 3, 4}) das Tripel mit dem maximalem Nutzwert 69 in TRIPLE5 ist. Wir können den entworfenen Algorithmus wie folgt darstellen. Algorithmus DPR Eingabe: I =(w1,w2,...,wn,c1,...,cn,b) ∈ (N −{0})2n+1 für n ∈ N −{0}. Phase 1. TRIPLE(1) = {(0, 0, ∅)}∪{(c1,w1, {1}) | falls w1 ≤ b}. Phase 2. for i =1 to n − 1 do begin SET(i + 1) := TRIPLE(i); for jedes (k, w, T ) ∈ TRIPLE(i) do begin if w + wi+1 ≤ b then SET(i +1) := SET(i +1) ∪{(k + ci+1,w + wi+1,T ∪{i +1})}; end; Bestimme TRIPLE(i + 1) als eine Teilmenge von SET(i + 1), die für jeden erreichbaren Nutzen in SET(i + 1) genau ein Tripel enthält, und zwar ein Tripel mit minimalem Gewicht für den Nutzen k. end; 7.2 Pseudopolynomielle Algorithmen 223 Phase 3. Berechne c := max{k ∈{0, 1, 2,..., n∑ i=1 ci}| (k, w, T ) ∈ TRIPLE(n)} zusammen mit dem entsprechenden Tripel (c, w, T ). Ausgabe: Die Indexmenge T , so dass (c, w, T ) ∈ TRIPLE(n). Aufgabe 7.1. Simulieren Sie die Arbeit des Algorithmus DPR für die Probleminstanz (1, 3, 5, 6, 7, 4, 8, 5, 9) des Rucksack-Problems. Im Folgenden analysieren wir die Zeitkomplexität des Algorithmus DPR. Satz 7.2. Für jede Instanz I des Rucksack-Problems ist TimeDPR(I) ∈ O(|I| 2 · MaxInt(I)), und damit ist DPR ein pseudopolynomieller Algorithmus für das Rucksackproblem. Beweis. Die Zeitkomplexität der ersten Phase ist in O(1). Für die Probleminstanz I =(w1,w2,...,wn,c1,...,cn,b) berechnet DPR die n − 1 Mengen TRIPLE(i + 1). Die Berechnung von TRIPLE(i + 1) aus TRIPLE(i) kann man in der Zeit O(|TRIPLE(i +1)|) durchführen. Weil |TRIPLE(i +1)|≤ n∑ j=1 cj ≤ n · MaxInt(I) für jedes i ∈{1,...,n − 1}, liegt die gesamte Zeitkomplexität der zweiten Phase in O(n2 · MaxInt(I)). Die Zeitkomplexität der dritten Phase liegt in O(n · MaxInt(I)), weil man dort ein Maximum aus höchstens n · MaxInt(I) Werten bestimmt. Weil n ≤|I|, gilt TimeDPR(I) ∈ O(|I| 2 · MaxInt(I)). \u0002 Wir bemerken, dass pseudopolynomielle Algorithmen in der Praxis sehr erfolgreich sein können. Oft sind Gewichte und Nutzen Zahlen aus einem festen Intervall und damit unabhängig von der Anzahl der Parameter der Probleminstanz. Damit können die pseu- dopolynomiellen Algorithmen auf typischen in der Praxis auftretenden Probleminstanzen schnell eine Lösung liefern. Das Interesse aus der Praxis für den Entwurf pseudopolyno- mieller Algorithmen stellt uns eine neue Klassiﬁzierungsfrage. Für welche NP-schweren Probleme existieren pseudopolynomielle Algorithmen und für welche gibt es keine? Wir suchen nach einer Methode, mit der man zeigen kann, dass ein Zahlproblem so schwer ist, dass bei der Voraussetzung P ̸= NP kein polynomieller Algorithmus für Probleminstanzen mit kleinen Zahlen existiert. Das Konzept der NP-Vollständigkeit funktioniert auch für diese Anwendung. Deﬁnition 7.3. Ein Zahlproblem U heißt stark NP-schwer, falls ein Polynom p exis- tiert, so dass Wert(p)- U NP-schwer ist. 224 7 Algorithmik für schwere Probleme Die folgende Behauptung zeigt, dass die starke NP-Schwere das gesuchte Konzept für Beweise der Nichtexistenz pseudopolynomieller Algorithmen ist. Satz 7.3. Sei U ein stark NP-schweres Zahlproblem. Falls P ̸= NP gilt, dann existiert kein pseudopolynomieller Algorithmus für U. Beweis. Weil U stark NP-schwer ist, existiert ein Polynom p, so dass das Problem Wert(p)- U NP-schwer ist. Angenommen, es gibt einen pseudopolynomiellen Algorithmus für U. Nach Satz 7.1 impliziert die Existenz eines pseudopolynomiellen Algorithmus für U die Existenz eines polynomiellen Algorithmus für Wert(h)- U für jedes Polynom h. Das bedeutet aber, dass wir einen polynomiellen Algorithmus für das NP-schwere Problem Wert(p)- U haben und somit P = NP gilt. Dies ist ein Widerspruch zur Annahme P ̸=NP. \u0002 Wir können also wieder die Methode der Reduktion anwenden, um die Nichtexistenz eines pseudopolynomiellen Algorithmus für ein gegebenes Problem zu zeigen. Im Folgenden illustrieren wir eine Möglichkeit für einen Beweis der starken NP-Schwere, indem wir zeigen, dass TSP stark NP-schwer ist. Wir benutzen dabei die bekannte Tatsache, dass das Entscheidungsproblem des Hamiltonschen Kreises (HK) NP-vollständig ist. Das Problem HK ist, zu entscheiden, ob ein gegebener Graph einen Hamiltonschen Kreis besitzt (einen Kreis, der durch jeden Knoten des Graphen genau einmal führt). Lemma 7.1. Das TSP ist stark NP-schwer. Beweis. Weil HK NP-schwer ist, reicht es aus, HK ≤p LangWert(p)-TSP für das Polynom p(n)= n zu zeigen. Sei G =(V, E) eine Eingabe von HK.Sei |V | = n für eine positive ganze Zahl n. Wir konstruieren einen gewichteten vollständigen Graphen (Kn,c) mit Kn =(V, Evoll) wie folgt. Evoll = {{u, v}| u, v ∈ V, u ̸= v} und die Gewichtsfunktion c : Evoll →{1, 2} ist deﬁniert durch c(e)= { 1, falls e ∈ E, 2, falls e/∈ E. Wir bemerken, dass G einen Hamiltonschen Kreis genau dann enthält, wenn die Kos- ten einer optimalen Lösung für (Kn,c) genau n betragen, d. h., wenn ((Kn,c),n) ∈ LangWert(p)-TSP gilt. Damit entscheidet jeder Algorithmus, der LangWert(p)-TSP entschei- det, auch das Problem des Hamiltonschen Kreises. \u0002 Aufgabe 7.2. Betrachten Sie die folgende Verallgemeinerung des Knotenüberdeckungsproblems. Gegeben ist ein Graph G =(V, E) mit Gewichten aus N für jeden Knoten aus V . Die Kosten einer Überdeckung S ⊆ V sind die Summe der Gewichte der Knoten in S. Das gewichtete Überdeckungsproblem ist ein Minimierungsproblem, in dem man eine kostengünstige Überdeckung sucht. Beweisen Sie, dass das gewichtete Überdeckungsproblem stark NP-schwer ist. 7.3 Approximationsalgorithmen 225 7.3 Approximationsalgorithmen In diesem Abschnitt stellen wir das Konzept der Approximationsalgorithmen zur Lösung schwerer Optimierungsprobleme vor. Die Idee ist, den Sprung von exponentieller Zeitkom- plexität zu polynomieller Zeitkomplexität durch die Abschwächung der Anforderungen zu erreichen. Statt der Berechnung einer optimalen Lösung fordern wir nur die Berechnung einer fast optimalen Lösung. Was der Begriﬀ „fast optimal“ bedeutet, legt die folgende Deﬁnition fest. Deﬁnition 7.4. Sei U =(ΣI , ΣO,L, M, cost, goal) ein Optimierungsproblem. Wir sagen, dass A ein zulässiger Algorithmus für U ist, falls für jedes x ∈ L die Ausgabe A(x) der Berechnung von A auf x eine zulässige Lösung für x (das heißt A(x) ∈M(x)) ist. Sei A ein zulässiger Algorithmus für U. Für jedes x ∈ L deﬁnieren wir die Approxi- mationsgüte GüteA(x) von A auf x durch GüteA(x) = max{ cost(A(x)) Opt U (x) , Opt U (x) cost(A(x)) } , wobei Opt U (x) die Kosten einer optimalen Lösung für die Instanz x von U sind. Für jede positive Zahl δ> 1 sagen wir, dass A ein δ-Approximationsalgorithmus für U ist, falls GüteA(x) ≤ δ für jedes x ∈ L. Dass in der Deﬁnition der Approximationsgüte das Maximum der beiden Quotien- ten gebildet wird, stellt sicher, dass diese für jedes gegebene Problem unabhängig vom jeweiligen Optimierungsziel größer ist als 1. Wir illustrieren das Konzept der Approxima- tionsalgorithmen zuerst für das Problem der minimalen Knotenüberdeckung. Die Idee ist, eﬃzient ein maximales Matching 3 in dem gegebenen Graphen zu ﬁnden und dann die zu diesem Matching inzidenten Knoten als eine Knotenüberdeckung auszugeben. Algorithmus VCA Eingabe: Ein Graph G =(V, E). Phase 1. C := ∅; {Während der Berechnung gilt C ⊆ V und am Ende der Berechnung enthält C eine Knotenüberdeckung für G.} A := ∅; {Während der Berechnung gilt A ⊆ E und am Ende der Berechnung ist A ein maximales Matching.} E′ := E; {Während der Berechnung enthält E′ ⊆ E genau die Kanten, die von dem aktuellen C noch nicht überdeckt werden. Am Ende der Berechnung gilt E′ = ∅.} 3Ein Matching in G =(V, E) ist eine Menge M ⊆ E von Kanten, so dass keine zwei Kanten aus M mit dem gleichen Knoten inzident sind. Ein Matching M ist maximal, falls für jedes e ∈ E − M die Menge M ∪{e} kein Matching in G ist. 226 7 Algorithmik für schwere Probleme aa aa bb bb cc cc dd dd ee ee ff ff gg gg hh hh (a) (b) (c) (d) Abbildung 7.1 Phase 2. while E′ ̸= ∅ do begin Nimm eine beliebige Kante {u, v} aus E′; C := C ∪{u, v}; A := A ∪{{u, v}}; E′ := E′ −{alle Kanten inzident zu u oder v}; end; Ausgabe: C. Betrachten wir einen möglichen Lauf des Algorithmus VCA auf dem Graphen aus Abbildung 7.1(a). Sei {b, c} die erste Kante, die VCA gewählt hat. Dann wird C = {b, c}, A = {{b, c}} und E′ = E −{{b, a}, {b, c}, {c, e}, {c, d}} (Abbildung 7.1(b)). Wenn die zweite Wahl einer Kante aus E′ auf {e, f } fällt (Abbildung 7.1(c)), dann wird C = {b, c, e, f }, A = {{b, c}, {e, f }} und E′ = {{d, h}, {d, g}, {h, g}}. Wenn man in der letzten Wahl {d, g} auswählt (Abbildung 7.1(d)), dann erhält man C = {b, c, e, f, d, g}, A = {{b, c}, {e, f }, {d, g}} und E′ = ∅. Damit ist C eine Knotenüberdeckung mit den Kosten 6. Man bemerke, dass {b, e, d, g} und {b, e, d, h} die beiden einzigen optimalen Knotenüberdeckungen sind und dass diese optimalen Überdeckungen bei keiner Wahl von Kanten von VCA erreicht werden können. Aufgabe 7.3. Finden Sie eine Wahl von Kanten in der zweiten Phase von VCA, so dass die resultierende Überdeckung C alle Knoten von G in Abbildung 7.1(a) enthält. Satz 7.4. Der Algorithmus VCA ist ein 2-Approximationsalgorithmus für MIN-VC und TimeVCA(G) ∈ O(|E|) für jede Probleminstanz G =(V, E). Beweis. Die Behauptung TimeVCA(G) ∈ O(|E|) ist oﬀensichtlich, weil jede Kante aus E in VCA genau einmal betrachtet wird. Weil am Ende der Berechnung E′ = ∅ gilt, 7.3 Approximationsalgorithmen 227 berechnet VCA eine Knotenüberdeckung in G (d. h., VCA ist ein zulässiger Algorithmus für MIN-VC). Um GüteVCA(G) ≤ 2 für jeden Graph G zu beweisen, bemerken wir, dass |C| =2 ·|A| und A ein Matching in G ist. Um |A| Kanten des Matchings A zu überdecken, muss man mindestens |A| Knoten wählen. Weil A ⊆ E, ist die Mächtigkeit jeder Knotenüberdeckung in G mindestens |A|, das heißt OptMIN-VC(G) ≥|A|. Daher |C| OptMIN-VC(G) = 2 ·|A| OptMIN-VC(G) ≤ 2. \u0002 Aufgabe 7.4. Konstruieren Sie für jedes n ∈ N −{0} einen zusammenhängenden Graphen Gn, so dass eine optimale Knotenüberdeckung die Mächtigkeit n hat und der Algorithmus VCA eine Überdeckung mit der Mächtigkeit 2n konstruieren kann. Ob eine Approximationsgüte von 2 hinreichend ist, hängt von der konkreten Anwendung ab. Meistens versucht man eine viel kleinere Approximationsgüte zu erreichen, was aber oft viel anspruchsvollere algorithmische Ideen erfordert. Andererseits misst man die Approximationsgüte als die Approximationsgüte im schlechtesten Fall, deshalb kann ein 2-Approximationsalgorithmus auf praktisch relevanten Eingaben viel besser laufen als mit der Approximationsgüte 2. Es gibt Optimierungsprobleme, die für das Konzept der Approximation zu schwer sind in dem Sinne, dass (P ̸= NP vorausgesetzt) keine polynomiellen d-Approximationsalgorithmen (für d> 1) für solche Probleme existieren. In Abschnitt 7.2 haben wir gezeigt, dass TSP zu schwer für das Konzept der pseudopolynomiellen Algorithmen ist. Im Folgenden zeigen wir, dass TSP auch mit dem Konzept der Approximation nicht zu bewältigen ist. Lemma 7.2. Falls P ̸= NP gilt, dann existiert für kein d ≥ 1 ein polynomieller d- Approximationsalgorithmus für TSP. Beweis. Wir führen einen indirekten Beweis, wobei es ausreichend ist, die Aussage für alle d ∈ N −{0} zu zeigen. Angenommen, es gibt also solch eine Konstante d, so dass ein polynomieller d-Approximationsalgorithmus A für TSP existiert. Wir zeigen, dass dann ein polynomieller Algorithmus B für das NP-vollständige Problem des Hamiltonschen Kreises existiert, was der Annahme P ̸= NP widerspricht. Der Algorithmus B für das Problem des Hamiltonschen Kreises arbeitet für jede Eingabe G =(V, E) wie folgt. (i) B konstruiert eine Instanz (K|V |,c) des TSP, wobei K|V | =(V, E′) mit E′ = {{u, v}| u, v ∈ V, u ̸= v} und c(e)= { 1, falls e ∈ E, (d − 1) ·|V | +2, falls e/∈ E. (ii) B simuliert die Arbeit von A auf der Eingabe (K|V |,c). Falls das Resultat von A ein Hamiltonscher Kreis mit Kosten genau |V | ist, akzeptiert B seine Eingabe G. Sonst verwirft B die Eingabe G. 228 7 Algorithmik für schwere Probleme Die Konstruktion der Instanz (K|V |,c) kann B in der Zeit O(|V |2) durchführen. Die zweite Phase von B läuft in polynomieller Zeit, weil A in polynomieller Zeit arbeitet und die Graphen G und K|V | die gleiche Größe haben. Wir müssen noch zeigen, dass B wirklich das Problem des Hamiltonschen Kreises entscheidet. Wir bemerken Folgendes. (i) Wenn G einen Hamiltonschen Kreis enthält, dann enthält K|V | einen Hamiltonschen Kreis mit den Kosten |V |, das heißt OptTSP(K|V |,c)= |V |. (ii) Jeder Hamiltonsche Kreis in K|V |, der mindestens eine Kante aus E′ − E enthält, hat mindestens die Kosten |V |− 1+(d − 1) ·|V | +2 = d ·|V | +1 >d ·|V |. Sei G =(V, E)in HK, das heißt OptTSP(K|V |,c)= |V |. Nach (ii) hat jede zulässige Lösung mit zu |V | unterschiedlichen Kosten mindestens die Kosten d ·|V | +1 >d ·|V | und somit muss der d-Approximationsalgorithmus A eine optimale Lösung mit den Kosten |V | ausgeben. Daraus folgt, dass B den Graphen G akzeptiert. Sei G =(V, E) nicht in HK. Damit hat jede zulässige Lösung für (K|V |,c) höhere Kosten als |V |, also cost(A(K|V |,c)) > |V |. Deswegen verwirft B den Graphen G. \u0002 Um TSP mindestens teilweise zu bewältigen, kombinieren wir das Konzept der Approxi- mation mit der Suche nach der Teilmenge der leichten Probleminstanzen. Wir betrachten jetzt das metrische TSP, Δ-TSP, das nur solche Probleminstanzen des TSP enthält, die die Dreiecksungleichung erfüllen (siehe Beispiel 2.6). Die Dreiecksungleichung ist eine natürliche Einschränkung, die in vielen Anwendungsszenarien eingehalten wird. Wir zeigen jetzt einen polynomiellen 2-Approximationsalgorithmus für Δ-TSP. Algorithmus SB Eingabe: Ein vollständiger Graph G =(V, E) mit einer Kostenfunktion c : E → N+, die die Dreiecksungleichung c({u, v}) ≤ c({u, w})+ c({w, v}) für alle Knoten u, v, w ∈ V erfüllt. Phase 1. Berechne einen minimalen Spannbaum 4 T von G bezüglich c. Phase 2. Wähle einen beliebigen Knoten v aus V . Führe eine Tiefensuche von v in T aus und nummeriere die Knoten in der Reihenfolge, in der sie besucht worden sind. Sei H die Knotenfolge, die dieser Nummerierung entspricht. Ausgabe: Der Hamiltonsche Kreis H = H, v. Wir illustrieren die Arbeit des Algorithmus SB auf der Probleminstanz G aus Ab- bildung 7.2(a). Ein minimaler Spannbaum T =({v1,v2,v3,v4,v5}, {{v1,v3}, {v1,v5}, {v2,v3}, {v3,v4}})in G ist in Abbildung 7.2(b) dargestellt. Abbildung 7.2(c) zeigt ei- ne Tiefensuche von v3 aus in T . Wir bemerken, dass bei der Tiefensuche jede Kante 4Ein Spannbaum eines Graphen G =(V, E) ist ein Baum T =(V, E′) mit E′ ⊆ E. Die Kosten von T sind die Summe der Kosten aller Kanten in E′. 7.3 Approximationsalgorithmen 229 1 1 1 1 1 1 1 2 2 22 2 2 22 2 2 22 33 3 3 3 v1v1 v1 v1 v1 v2v2 v2 v2 v2 v3v3 v3 v3 v3 v4v4 v4 v4v4 v5v5 v5 v5 v5 (a) (b) (c) (d) (e) 1⃝ 2⃝ 3⃝ 4⃝ 5⃝ Abbildung 7.2 von T genau zweimal durchlaufen wird. Diese Tiefensuche bestimmt die Knotenfolge H = v3,v4,v1,v5,v2 und somit ist H = v3,v4,v1,v5,v2,v3 die Ausgabe des Algorithmus SB (Abbildung 7.2(d)). Die Kosten von H sind 2+3+2+3+1 = 11. Eine optimale Lösung ist v3,v1,v5,v4,v2,v3 mit den Kosten 1 + 2 + 2 + 2 + 1 = 8 (Abbildung 7.2(e)). Satz 7.5. Der Algorithmus SB ist ein polynomieller 2-Approximationsalgorithmus für Δ-TSP. Beweis. Wir analysieren zuerst die Zeitkomplexität von SB. Ein minimaler Spannbaum eines Graphen G =(V, E) kann in der Zeit O(|E|) berechnet werden. Die Tiefensuche in einem Baum T =(V, E′) läuft in der Zeit O(|V |). Somit ist TimeSB(G) ∈ O(|E|), d. h., SB arbeitet in linearer Zeit. Jetzt beweisen wir, dass die Approximationsgüte von SB höchstens 2 ist. Sei HOpt ein 230 7 Algorithmik für schwere Probleme optimaler Hamiltonscher Kreis mit cost(HOpt)= OptΔ-TSP(I) für eine Probleminstanz I =((V, E),c). Sei H die Ausgabe SB(I) des Algorithmus SB für die Eingabe I.Sei T =(V, E′) der minimale Spannbaum, den SB in der ersten Phase konstruiert. Zuerst bemerken wir, dass cost(T )= ∑ c∈E′ c(e) < cost(HOpt), (7.1) weil die Entfernung einer Kante aus HOpt in einem Spannbaum resultiert und T ein minimaler Spannbaum ist. Sei W der Weg, der der Tiefensuche in T entspricht. W geht genau zweimal durch jede Kante von T (einmal in jeder Richtung). Wenn cost(W ) die Summe aller Kanten des Weges W ist, dann gilt cost(W )=2 · cost(T ). (7.2) Die Gleichungen (7.1) und (7.2) implizieren cost(W ) < 2 · cost(HOpt). (7.3) Wir bemerken, dass man H aus W erhalten kann, indem man einige Teilwege u, v1, ..., vk, v in W durch die Kante {u, v} (durch den direkten Weg u, v) ersetzt. Dies geschieht genau dann, wenn v1,...,vk schon vor u besucht worden sind, aber v noch nicht besucht wurde. Dieses Ersetzen kann man schrittweise durch die einfache Operation des Ersetzens von Teilwegen von drei Knoten u, w, v durch den Weg u, v realisieren. Diese einfache Operation erhöht aber die Kosten des Weges nicht, weil dank der Dreiecksungleichung c({u, v}) ≤ c({u, w})+ c({w, v}) gilt. Deswegen ist cost(H) ≤ cost(W ). (7.4) Die Ungleichungen (7.3) und (7.4) liefern zusammen cost(H) ≤ cost(W ) < 2 · cost(HOpt) und somit SB(I) OptΔ-TSP(I) = cost(H) cost(HOpt) < 2. \u0002 Aufgabe 7.5. Finden Sie für jedes n ∈ N−{0, 1, 2} eine Kostenfunktion cn für den vollständigen Graphen Kn mit n Knoten, so dass cn den Kanten von Kn mindestens zwei unterschiedliche Werte zuordnet und der Algorithmus SB immer eine optimale Lösung ausrechnet. Aufgabe 7.6.∗ Finden Sie für jede ganze Zahl n ≥ 4 eine Instanz In des Δ-TSP mit der Eigenschaft SB(In) OptΔ-TSP(In) ≥ 2n − 2 n +1 . 7.4 Lokale Suche 231 7.4 Lokale Suche Lokale Suche ist eine Technik für den Algorithmenentwurf für Optimierungsprobleme. Die Idee dieser Technik ist es, für eine gegebene Eingabe x eine zulässige Lösung α aus M(x) auszurechnen und dann schrittweise durch kleine (lokale) Änderungen von α zu einer besseren zulässigen Lösung zu gelangen. Was der Begriﬀ „kleine Änderungen“ bedeutet, wird durch den Begriﬀ der Nachbarschaft deﬁniert. Deﬁnition 7.5. Sei U =(ΣI , ΣO,L, M, cost, goal) ein Optimierungsproblem. Für jedes x ∈ L ist eine Nachbarschaft in M(x) eine Funktion fx : M(x) →P(M(x)) mit folgenden Eigenschaften: (i) α ∈ fx(α) für jedes α ∈M(x), {Eine Lösung α ist immer in der Nachbarschaft von sich selbst.} (ii) falls β ∈ fx(α) für α, β ∈M(x), dann ist α ∈ fx(β), und {Wenn β in der Nachbarschaft von α liegt, dann liegt auch α in der Nachbarschaft von β.} (iii) für alle α, β ∈M(x) existieren eine positive Zahl k und γ1,γ2,...,γk ∈M(x),so dass γ1 ∈ fx(α),γi+1 ∈ fx(γi) für i =1,...,k − 1, und β ∈ fx(γk). {Für alle zulässigen Lösungen α und β ist es möglich, von α zu β über die Nachbar- schaftsrelation zu gelangen.} Falls α ∈ fx(β) sagen wir, dass α und β Nachbarn (bezüglich fx) in M(x) sind. Die Menge fx(α) wird die Nachbarschaft von α in M(x) genannt. Eine zulässige Lösung α ∈M(x) heißt ein lokales Optimum für x bezüglich der Nachbarschaft fx, falls cost(α) = goal{cost(β) | β ∈ fx(α)}. Sei für jedes x ∈ L die Funktion fx eine Nachbarschaft in M(x). Die Funktion f : ⋃ x∈L ({x}×M(x)) → ⋃ x∈L P(M(x)) deﬁniert durch f (x, α)= fx(α) für alle x ∈ L und alle α ∈M(x) ist eine Nachbarschaft für U . In der Anwendung bestimmt man die Nachbarschaft durch sogenannte lokale Trans- formationen. Der Begriﬀ „lokal“ ist dabei wichtig, weil er die Bedeutung hat, dass man nur eine kleine Änderung der Speziﬁkation von α durch eine lokale Transformation erlaubt. Eine lokale Transformation für MAX-SAT kann zum Beispiel die Invertierung eines Bits in der Belegung sein. Dann enthält die Nachbarschaft einer Lösung α die Lösung α selbst 232 7 Algorithmik für schwere Probleme und alle Lösungen, die man durch die ausgewählte lokale Transformation erhalten kann. Für eine Formel Φ von fünf Variablen ist dann {01100, 11100, 00100, 01000, 01110, 01101} die Nachbarschaft von α = 01100 bezüglich der lokalen Transformation der Bitinvertierung. Aufgabe 7.7. Beweisen Sie, dass die Nachbarschaft für MAX-SAT, die durch die lokale Trans- formation der Bitinvertierung bestimmt wird, die Deﬁnition 7.5 erfüllt. Für das TSP kann man folgende Nachbarschaft, 2-Exchange genannt, betrachten (Abbildung 7.3). Wir entfernen zwei beliebige Kanten {a, b} und {c, d} mit |{a, b, c, d}| =4 aus einem Hamiltonschen Kreis, der die Knoten a, b, c, d in dieser Reihenfolge besucht und fügen statt dessen die Kanten {a, c} und {b, d} hinzu. Wir beobachten, dass wir dadurch einen neuen Hamiltonschen Kreis erhalten und dass man die Kanten {a, b} und {c, d} durch keine anderen Kanten als {a, c} und {b, d} ersetzen kann, um einen Hamiltonschen Kreis zu erhalten. Aufgabe 7.8. Erfüllt 2-Exchange die Bedingungen der Deﬁnition 7.5? Begründen Sie Ihre Behauptung. Aufgabe 7.9. Sei H ein Hamiltonscher Kreis in einem Graphen G. Entfernen Sie beliebige drei Kanten {a, b}, {c, d} und {e, f } mit |{a, b, c, d, e, f }| = 6 aus H. Zeichnen Sie alle unterschiedlichen Tripel von Kanten, deren Hinzufügen zu H wieder zu einem Hamiltonschen Kreis in G führt. Wie viele Möglichkeiten gibt es bei der Entfernung von k Kanten, die ein Matching bilden, für k ≥ 3? Die lokale Suche bezüglich der Nachbarschaft ist nichts anderes als eine iterative Bewegung von einer Lösung zu einer besseren, benachbarten Lösung, bis man eine zulässige Lösung β erreicht, in deren Nachbarschaft keine bessere Lösung als β existiert. Das Schema der lokalen Suche bezüglich einer Nachbarschaft f kann man also wie folgt formulieren. Algorithmus LS(f ) Eingabe: Eine Instanz x eines Optimierungsproblems U. Phase 1. Berechne eine zulässige Lösung α ∈M(x) Phase 2. while α ist kein lokales Optimum bezüglich fx do begin Finde ein β ∈ fx(α), so dass cost(β) < cost(α) falls U ein Minimierungsproblem ist und cost(β) > cost(α) falls U ein Maximierungsproblem ist. α := β; end; Ausgabe: α. Wir bemerken, dass LS(f ) immer ein lokales Optimum bezüglich der Nachbarschaft f liefert. Falls alle lokalen Optima auch globale Optima sind, garantiert die lokale Suche die Lösung des Optimierungsproblems. Dies ist der Fall beim Optimierungsproblem 7.4 Lokale Suche 233 aa bb cc dd Abbildung 7.3 des minimalen Spannbaums, wenn die Nachbarschaft durch den Austausch einer Kante bestimmt wird. Wenn sich die Kosten der lokalen Optima nicht zu sehr von den Kosten der optimalen Lösungen unterscheiden, kann die lokale Suche zum Entwurf eines Approximationsal- gorithmus führen. Dies ist der Fall beim Problem des maximalen Schnitts MAX-CUT. Gegeben sei ein Graph G =(V, E). Jedes Paar (V1,V2) mit V1 ∪ V2 = V und V1 ∩ V2 = ∅ ist ein Schnitt von G. Der Preis des Schnitts (V1,V2) ist die Anzahl der Kanten zwischen den Knoten aus V1 und V2, d. h., cost((V1,V2),G)= |E ∩{{u, v}| u ∈ V1,v ∈ V2}|. Das Ziel ist die Maximierung. Wir betrachten lokale Transformationen, die einen Knoten von einer Seite auf die andere Seite schieben. Der auf lokaler Suche basierende Algorithmus kann wie folgt beschrieben werden. Algorithmus LS-CUT Eingabe: Ein Graph G =(V, E). Phase 1. S = ∅; {Während der Berechnung betrachten wir den Schnitt (S, V − S). Am Anfang ist der Schnitt (∅,V ).} Phase 2. while ein Knoten v ∈ V existiert, so dass cost(S ∪{v},V − (S ∪{v})) > cost(S, V − S), oder cost(S −{v}, (V − S) ∪{v}) > cost(S, V − S) gilt do begin Nimm v und bringe ihn auf die andere Seite des Schnitts; end; Ausgabe: (S, V − S). Satz 7.6. LS-CUT ist ein 2-Approximationsalgorithmus für MAX-CUT. Beweis. Es ist oﬀensichtlich, dass der Algorithmus LS-CUT eine zulässige Lösung für MAX-CUT ausgibt. Es bleibt zu zeigen, dass GüteLS-CUT(G) ≤ 2 für jeden Graphen G =(V, E). Sei (Y1,Y2) die Ausgabe von LS-CUT.Weil(Y1,Y2) ein lokales Maximum bezüglich des Austauschs eines Knotens ist, hat jeder Knoten v ∈ Y1 (Y2) mindestens so viele Kanten zu Knoten in Y2 (Y1) wie die Anzahl der Kanten zwischen v und Knoten aus Y1 (Y2) ist. Damit ist mindestens die Hälfte aller Kanten im Schnitt (Y1,Y2). Weil OptMAX-CUT(G) nicht größer als |E| sein kann, ist GüteLS-CUT(G) ≤ 2. \u0002 234 7 Algorithmik für schwere Probleme Aufgabe 7.10. Beweisen Sie, dass LS-CUT ein polynomieller Algorithmus ist. Aufgabe 7.11. Betrachten Sie jetzt das Problem des gewichteten maximalen Schnitts, MAX- GEW-CUT, das eine Verallgemeinerung von MAX-CUT darstellt. Eine Eingabe von MAX- GEW-CUT ist ein Graph G =(V, E) mit einer Kostenfunktion c : E → N −{0}, die jeder Kante e ihre Kosten c(e) zuordnet. Die Kosten eines Schnitts sind die Summe der Kosten der Kanten des Schnitts. Das Ziel ist, eine Lösung mit maximalen Kosten zu ﬁnden. Oﬀensichtlich ist MAX-GEW-CUT ein Zahlproblem. Entwerfen Sie einen Algorithmus mit lokaler Suche, der einen pseudopolynomiellen 2-Approximationsalgorithmus darstellt. Die Algorithmen, die auf lokaler Suche basieren, nennt man lokale Algorithmen. Die lokalen Algorithmen sind mehr oder weniger durch die Wahl der Nachbarschaft bestimmt. Die einzigen noch freien Parameter in dem Schema der lokalen Suche sind die Strategie nach der Suche der besseren Nachbarn und die Entscheidung, ob man die erste gefundene bessere Lösung als neue Lösung nimmt oder ob man unbedingt die beste Lösung in der Nachbarschaft bestimmen möchte. Angenommen P ̸= NP, dann gibt es oﬀensichtlich keine polynomiellen lokalen Algo- rithmen für NP-schwere Optimierungsprobleme. Wir bemerken, dass die Zeitkomplexität eines lokalen Algorithmus als (die Zeit der Suche in der Nachbarschaft) × (die Anzahl der iterativen Verbesserungen) abgeschätzt werden kann. Wir sind jetzt an folgender Frage interessiert. Für welche NP-schweren Optimierungsprobleme existiert eine Nachbarschaft f polynomieller Größe, so dass LS(f ) immer eine optimale Lösung liefert? Dies bedeutet, dass wir bereit sind, eine im schlechtesten Fall mögliche exponentielle Anzahl Verbesserungsiterationen in Kauf zu nehmen, falls jede Iteration in polynomieller Zeit läuft und die Konvergenz zu einer optimalen Lösung gesichert ist. Die Idee dabei ist, dass die Vergrößerung der Nachbarschaften auf der einen Seite die Wahrscheinlichkeit verkleinert, an ein schwaches lokales Optimum zu gelangen, auf der anderen Seite aber die Zeitkomplexität einer Verbesserungsiteration erhöht. Die Frage ist, ob eine Nachbarschaft vernünftiger Größe existiert, so dass jedes lokale Optimum auch ein globales Optimum ist. Diese Fragestellung wird wie folgt formalisiert. Deﬁnition 7.6. Sei U =(ΣI , ΣO,L, M, cost, goal) ein Optimierungsproblem und sei f eine Nachbarschaft für U. Eine Nachbarschaft f heißt echt, falls für jedes x ∈ L jedes lokale Optimum für x bezüglich fx eine optimale Lösung für x ist. Eine Nachbarschaft f heißt polynomiell untersuchbar, falls ein polynomieller Algo- rithmus existiert, der für jedes x ∈ L und jedes α ∈M(x) eine beste Lösung aus fx(α) ﬁndet. Unsere Frage kann man jetzt wie folgt formulieren: Für welche Optimierungsprobleme existieren echte polynomiell untersuchbare Nachbarschaften? 7.4 Lokale Suche 235 Das bekannteste positive Beispiel ist der Simplex-Algorithmus für das Problem der linea- ren Programmierung. Er basiert auf der Existenz einer echten polynomiell untersuchbaren Nachbarschaft, aber er kann auf einigen Probleminstanzen exponentielle Zeitkomplexität beanspruchen, weil eine exponentielle Anzahl Verbesserungsiterationen möglich ist. Aufgabe 7.12. Sei k-Exchange die Nachbarschaft für TSP, in der k Kanten ausgetauscht werden können. Wie mächtig ist die Nachbarschaft k-ExchangeG(H) eines Hamiltonschen Kreises H, wenn Gn Knoten hat? Aufgabe 7.13.∗ Beweisen Sie, dass k-Exchange für keine Konstante k ∈ N −{0} eine echte Nachbarschaft für TSP ist. Unsere Zielsetzung ist jetzt, eine Methode vorzustellen, mit der man die Nichtexistenz einer echten polynomiell untersuchbaren Nachbarschaft für Optimierungsprobleme bewei- sen kann. Wie wir sehen werden, kann das Konzept der starken NP-Schwere auch hier erfolgreich angewendet werden. Deﬁnition 7.7. Sei U =(ΣI , ΣO,L, M, cost, goal) ein Optimierungsproblem, das ein Zahlproblem ist (Zahloptimierungsproblem). Wir sagen, dass U kostenbeschränkt ist, falls für jede Instanz x mit Int(x)=(i1,i2,...,in), ij ∈ N für j =1, 2,...,n, gilt, dass cost(α) ∈ ⎧ ⎨ ⎩ 1, 2,..., n∑ j=1 ij ⎫ ⎬ ⎭ für jede zulässige Lösung α ∈M(x). Wir bemerken, dass fast jedes bekannte Zahloptimierungsproblem kostenbeschränkt ist und somit die Forderung der Kostenbeschränkung keine große Einschränkung der Anwendbarkeit folgender Methode bedeutet. Satz 7.7. Sei U∈ NPO ein kostenbeschränktes Zahloptimierungsproblem. Falls P ̸= NP gilt und U stark NP-schwer ist, dann existiert keine echte polynomiell untersuchbare Nachbarschaft für U. Beweis. Wir führen einen indirekten Beweis. Angenommen, U besitzt eine echte polyno- miell untersuchbare Nachbarschaft f . Dann kann man für jede Eingabe x in polynomieller Zeit p(|x|) einen Iterationsschritt von LS(fx) durchführen. Weil U∈ NPO ist, kann man dann die Startlösung auch in polynomieller Zeit ﬁnden. In jeder Verbesserungsiteration verbessert man die Kosten der aktuellen zulässigen Lösung mindestens um 1, weil die Kosten der Lösungen eines kostenbeschränkten Zahlproblems nur ganze Zahlen sind. Da die Kosten der Lösungen aus dem Intervall von 1 bis ∑ j∈Int(x) j ≤ n · MaxInt(x) sind, gibt es höchstens |x|· MaxInt(x) Verbesserungsiterationen. Damit ist die Gesamtlaufzeit des LS(f )in O(p(|x|) ·|x|· MaxInt(x)). Weil f eine echte Nachbarschaft ist, garantiert LS(f ) eine optimale Lösung für x. Damit ist LS(f ) ein pseudopolynomieller Algorithmus für U.Weil U stark NP-schwer ist, widerspricht dies der Annahme P ̸=NP. \u0002 236 7 Algorithmik für schwere Probleme In Kapitel 6 haben wir bewiesen, dass TSP und MAX-CL stark NP-schwer sind. Wir bemerken, dass beide Probleme kostenbeschränkte Zahloptimierungsprobleme sind. Damit besitzen diese Optimierungsprobleme keine echte polynomiell untersuchbare Nachbarschaft. Für das TSP kann man sogar beweisen, dass es keine echte Nachbarschaft der Größe 2 3√ n besitzt. 7.5 Simulated Annealing In diesem Abschnitt stellen wir Simulated Annealing (simulierte Abkühlung) als eine Heuristik zur Lösung schwerer Probleme vor. Der Begriﬀ Heuristik bezeichnet hier eine Entwurfstechnik für Algorithmen, die keine Lösung von hoher Qualität (guter Approxi- mation) in vernünftiger Zeit für jede Eingabe garantieren. Dies bedeutet, dass wir bei Heuristiken viel mehr von unseren Anforderungen abweichen als in allen bisher vorgestell- ten Methoden. Die Hoﬀnung dabei ist, dass die heuristischen Algorithmen für typische anwendungsrelevante Probleminstanzen vernünftige Resultate in kurzer Zeit liefern. Trotz der Unsicherheit bezüglich der Laufzeit und der Lösungsqualität sind Heuristiken bei den Anwendern sehr beliebt, weil sie gewisse, nicht zu unterschätzende Vorteile haben. Sie sind meistens einfach und schnell zu implementieren und zu testen, so dass die Erstellung eines heuristischen Algorithmus viel kostengünstiger ist als der Entwurf eines spezialisierten, auf das Problem zugeschnittenen Algorithmus. Zweitens sind Heuristiken robust, d. h., sie arbeiten für eine breite Klasse von Problemen erfolgreich, obwohl diese Probleme ziemlich unterschiedliche kombinatorische Strukturen haben. Dies bedeutet, dass eine Änderung der Problemspeziﬁkation im Prozess des Algorithmenentwurfs kein Problem darstellt, weil höchstens ein paar Parameter des heuristischen Algorithmus zu ändern sind. Für den Entwurf eines problemzugeschnittenen Optimierungsalgorithmus bedeutet eine Änderung der Aufgabenspeziﬁkation oft eine solche Änderung der kombinatorischen Struktur, dass man mit dem Entwurf von vorne beginnen muss. Wenn man die lokale Suche auf ein schweres Problem anwendet, bei dem man das Verhalten des lokalen Algorithmus nicht bestimmen kann, dann kann man lokale Suche auch als eine Heuristik betrachten. Sie hat auch die Eigenschaft der Robustheit, weil man sie praktisch auf jedes Optimierungsproblem anwenden kann. Die größte Schwäche der lokalen Suche ist, dass sie in einem lokalen Optimum endet, egal wie gut oder schlecht dieses lokale Optimum ist. Wir wollen jetzt die Methode der lokalen Suche verbessern, indem wir die Fallen der lokalen Optima aufheben. Dabei lassen wir uns durch die physikalische Optimierung von Metallzuständen in der Thermodynamik inspirieren. Der optimale Zustand eines Metalls entspricht der optimalen Kristallstruktur, bei der alle Bindungen zwischen den elementaren Teilchen gleich stark sind. Wenn einige Bindungen durch Belastung wesentlich schwächer und andere stärker werden, besteht Bruchgefahr und das Metall ist in einem schlechten Zustand. Der optimale Zustand entspricht also dem Zustand mit minimaler Energie. Die Optimierungsprozedur besteht aus folgenden zwei Phasen. Phase 1. Dem Metall wird von außen durch ein „heißes Bad“ Energie zugeführt. Dadurch schwächen sich fast alle Bindungen ab und ein chaosähnlicher Zustand entsteht. 7.5 Simulated Annealing 237 Phase 2. Das Metall wird langsam abgekühlt, bis es einen optimalen Zustand mit minimaler Energie erreicht. Diesen Optimierungsprozess kann man mit folgendem Algorithmus auf einem Rechner simulieren. Wir bezeichnen mit E(s) die Energie des Metallzustandes s.Sei cB die Boltzmann-Konstante. Metropolis-Algorithmus Eingabe: Ein Zustand s des Metalls mit der Energie E(s). Phase 1. Bestimme die Anfangstemperatur T des heißen Bades. Phase 2. Generiere einen Zustand q aus s durch eine zufällige kleine Änderung (zum Beispiel eine Positionsänderung eines Elementarteilchens). if E(q) ≤ E(s) then s := q; {akzeptiere q als neuen Zustand} else Akzeptiere q als neuen Zustand mit der Wahrscheinlichkeit Wahr(s → q)=e − E(q)−E(s) cB·T ; {bleibe im Zustand s mit der Wahrscheinlichkeit 1 − Wahr(s → q)} Phase 3. Verkleinere T passend; if T ist nicht sehr nahe bei 0 then goto Phase 2; else output(s); Zuerst beobachten wir die starke Ähnlichkeit zwischen der lokalen Suche und dem Metropolis-Algorithmus. Der Metropolis-Algorithmus besteht aus Iterationsschritten, und in einem Iterationsschritt wird ein neuer Kandidat für einen aktuellen Zustand durch eine lokale Transformation bestimmt. Die wesentlichen Unterschiede sind folgende. (i) Der Metropolis-Algorithmus darf mit gewisser Wahrscheinlichkeit auch in einen schlechteren Zustand mit hoher Energie übergehen und dadurch mögliche lokale Minima überwinden. (ii) Nicht die lokale Optimalität, sondern der Wert von T entscheidet über die Termi- nierung des Metropolis-Algorithmus. Die Wahrscheinlichkeit Wahr(s → q) folgt den Gesetzen der Thermodynamik, die besagen, dass die Wahrscheinlichkeit einer Verschlechterung (eines Energiewachstums) um einen Wert ΔE durch Wahr(ΔE) = e −ΔE cB·T gegeben ist. Diese Wahrscheinlichkeit hat zwei wichtige Eigenschaften. (a) Die Wahrscheinlichkeit Wahr(s → q) verkleinert sich mit wachsendem E(q) − E(s), d. h., starke Verschlechterungen sind weniger wahrscheinlich als schwächere, und 238 7 Algorithmik für schwere Probleme (b) die Wahrscheinlichkeit Wahr(s → q) wächst mit T , d. h., starke Verschlechterungen (Überwindung tiefer lokaler Minima) sind am Anfang bei großem T wahrscheinlicher als bei kleinem T . Ein wichtiger Punkt ist, dass die Möglichkeit, die lokalen Minima durch Verschlechterung zu überwinden, notwendig für das Erreichen des Optimums ist. Um den Metropolis- Algorithmus zur Lösung kombinatorischer Optimierungsprobleme einzusetzen, reicht es aus, die folgende Beziehung zwischen den Begriﬀen der Thermodynamik und den Begriﬀen der kombinatorischen Optimierung festzustellen. Menge der Systemzustände ̂= Menge der zulässigen Lösungen, Energie eines Zustandes ̂= Kosten einer zulässigen Lösung, ein optimaler Zustand ̂= eine optimale Lösung, Temperatur ̂= ein Programmparameter. Sei U =(ΣI , ΣO,L, M, cost, Minimum) ein Optimierungsproblem mit einer Nachbarschaft f . Dann kann man Simulated Annealing bezüglich f als eine Simulation des Metropolis- Algorithmus wie folgt beschreiben. Algorithmus SA(f ) Eingabe: Eine Probleminstanz x ∈ L. Phase 1. Berechne eine zulässige Lösung α ∈M(x). Wähle eine Anfangstemperatur T . Wähle eine Reduktionsfunktion g, abhängig von T und der Anzahl der Iterationen I. Phase 2. I := 0; while T> 0(oder T ist nicht zu nah an 0) do begin Wähle zufällig ein β aus fx(α); if cost(β) ≤ cost(α) then α := β; else begin Generiere zufällig eine Zahl r aus dem Intervall [0, 1]; if r< e− cost(β)−cost(α) T then α := β; end; I := I +1; T := g(T, I); end; Ausgabe: α. Bei einer „vernünftigen“ Nachbarschaft und passender Wahl von T und g kann man beweisen, dass SA(f ) das Optimum erreicht. Das Problem ist aber, dass man die An- zahl der dazu hinreichenden Iterationen nicht einschränken kann. Selbst Versuche, eine 7.6 Zusammenfassung 239 Approximationsgüte nach einer gewissen Anzahl Operationen zu garantieren, führten dazu, dass man eine viel größere Anzahl Iterationen als |M(x)| für eine solche Garantie braucht. Trotzdem gibt es viele Anwendungen, bei denen Simulated Annealing akzeptable Lösungen liefert und deswegen wird es häuﬁg eingesetzt. Ein positiver Aspekt ist auch, dass die Wahl der Parameter T und g beim Benutzer liegt, und so kann er alleine über Prioritäten in Bezug auf den Tradeoﬀ zwischen Laufzeit und Lösungsqualität entscheiden. 7.6 Zusammenfassung Der Algorithmenentwurf ist für die Lösung schwerer Probleme entscheidend, weil die qualitativen Sprünge in der Anforderung an die Rechnerressourcen (von exponentieller zu polynomieller Komplexität) nicht durch die Verbesserung der Rechnertechnologie zu erreichen sind. Um eﬃziente Algorithmen für schwere Probleme zu erhalten, muss man etwas auf der Ebene der Anforderungen bezahlen. Entweder reduzieren wir die Menge der zulässigen Eingaben (d. h., wir lösen das Problem nicht in seiner allgemeinen formalen Darstellung), oder wir verzichten auf die Sicherheit, immer eine richtige oder optimale Lösung zu bekommen. Die Kunst der Algorithmik liegt darin, große Gewinne auf der Seite der Eﬃzienz durch kleine Nachlässe in der Problemformulierung zu bewirken. Pseudopolynomielle Algorithmen laufen in polynomieller Zeit auf Probleminstanzen von Zahlproblemen, bei denen die Zahlen eine polynomielle Größe in der Eingabelänge haben. Für das Rucksackproblem kann man einen pseudopolynomiellen Algorithmus entwerfen. Das Konzept der NP-Vollständigkeit ist auch hier hilfreich, um die Nichtexistenz pseudopolynomieller Algorithmen unter der Annahme P ̸= NP für gewisse Probleme zu beweisen. Approximationsalgorithmen sind Algorithmen für Optimierungsprobleme, die eine zulässige Lösung liefern, deren Kosten sich nicht zu viel von den Kosten einer optimalen Lösung unterscheiden. Für das metrische TSP, MIN-VC und MAX-CUT kann man polynomielle Approximationsalgorithmen entwerfen. Für das allgemeine TSP gibt es keinen polynomiellen Approximationsalgorithmus mit einer konstanten Approximationsgüte, falls P ̸=NP. Die lokalen Algorithmen für ein Optimierungsproblem starten mit einer zulässigen Lösung und versuchen durch kleine Änderungen (lokale Transformationen) iterativ zu einer besseren Lösung zu gelangen. Lokale Algorithmen enden immer in einem lokalen Optimum bezüglich der erlaubten lokalen Transformationen. Die Kosten der lokalen Optima können sich wesentlich von den optimalen Kosten unterscheiden. Simulated Annealing ist eine Heuristik, die auf der lokalen Suche aufbaut und das Verlassen der lokalen Optima ermöglicht. Simulated Annealing ist robust und einfach zu implementieren und wird deswegen oft in der Praxis angewendet. Die Konzepte der pseudopolynomiellen Algorithmen für Zahlprobleme und der starken NP-Schwere sind Garey und Johnson [GJ79] zu verdanken. Der pseudopolynomielle Algorithmus für das Rucksackproblem wurde von Ibarra und Kim [IK74] entworfen. Der erste Approximationsalgorithmus wurde von Graham [Gra66] entworfen. Die ersten lokalen Algorithmen wurden von Bock [Boc58] und Croes [Cro58] entworfen. Das Konzept der echten polynomiell untersuchbaren Nachbarschaft wurde von Papadimitriou und Steiglitz 240 7 Algorithmik für schwere Probleme [PS82] eingeführt. Der Metropolis-Algorithmus für die Simulation der Abkühlung wurde von Metropolis, A. und M. Rosenbluth und Teller [MRR +53] entdeckt. Die Möglichkeit, diesen Algorithmus in der kombinatorischen Optimierung anzuwenden, kam von Černý [Čer85] und Kirkpatrick, Gellat und Vecchi [KGV83]. Eine systematische Übersicht über Methoden zur Lösung schwerer Probleme ist in [Hro04a] gegeben. Zum weiteren Lesen empfehlen wir noch wärmstens Papadimitriou und Steiglitz [PS82], Cormen, Leiserson und Rivest [CLR90] und Schöning [Sch01]. Zum Thema Approximationsalgorithmen sind reichhaltige Quellen Ausiello, Crescenzi, Gambosi, Kann, Marchetti-Spaccamela und Protasi [ACG+99], Hochbaum [Hoc97], Mayr, Prömel und Steger [MPS98] und Vazirani [Vaz01]. Kontrollaufgaben 1. Die meisten bekannten NP-schweren Probleme sind interessant für die Praxis. Welche grundsätzlichen Ansätze zur Lösung von Instanzen schwerer Probleme gibt es? 2. Erklären Sie das Konzept pseudopolynomieller Algorithmen und illustrieren Sie es durch die Anwendung auf das einfache Rucksack-Problem. Beim einfachen Rucksack-Problem sind die Kosten und das Gewicht für jedes Objekt gleich (d. h., das Gewicht entspricht dem Nutzen). Vereinfachen Sie den pseudopolynomiellen Algorithmus für das allgemeine Rucksack-Problem, um einen pseudopolynomiellen Algorithmus für das einfache Rucksack- Problem zu erhalten. Führt diese Vereinfachung auch zu einer geringeren Zeitkomplexität? 3. Gibt es eine Methode, die unter der Voraussetzung P ̸= NP die Nichtexistenz eines pseudopolynomiellen Algorithmus für ein Zahlproblem beweisen kann? 4. Erklären Sie das Konzept von Approximationsalgorithmen. Wie misst man die Güte eines Approximationsalgorithmus? 5. Entwerfen Sie einen Greedy-Algorithmus für das einfache Rucksack-Problem, siehe Kon- trollaufgabe 2. Zeigen Sie, dass der entworfene Algorithmus ein polynomieller 2-Approxi- mationsalgorithmus ist. 6. Erklären Sie das Konzept der lokalen Suche. Schlagen Sie für alle hier betrachteten NP- schweren Optimierungsprobleme ein paar Nachbarschaften vor. Für welche Probleme kann man die lokale Suche erfolgreich anwenden (eﬃzient und mit der Garantie, eine gute Lösung zu ﬁnden)? 7. Wann ist eine Nachbarschaft echt und polynomiell untersuchbar? Hilft uns das Konzept der starken NP-Schwere zu zeigen, dass gewisse Optimierungsprobleme zu schwer für die lokale Suche sind? 8. Was sind die gemeinsame Grundlage und der wesentliche Unterschied zwischen lokaler Suche und Simulated Annealing? Was garantiert Simulated Annealing und was nicht? Mit welcher Wahrscheinlichkeit akzeptiert man eine Verschlechterung, und was sind die wichtigsten Eigenschaften dieses Wahrscheinlichkeitskriteriums? Das Gewebe dieser Welt ist aus Notwendigkeit und Zufall gebildet; Die Vernunft des Menschen stellt sich zwischen beide und weiß sie zu beherrschen; Sie behandelt das Notwendige als den Grund ihres Daseins; Das Zufällige weiß sie zu lenken, zu leiten und zu nutzen, . . . J. W. von Goethe 8 Randomisierung 8.1 Zielsetzung Der Begriﬀ Zufall ist einer der fundamentalsten und meist diskutierten Begriﬀe der Wissenschaft. Die grundlegende Frage ist, ob der Zufall objektiv existiert oder ob wir diesen Begriﬀ nur benutzen, um Ereignisse mit unbekannter Gesetzmäßigkeit zu erklären und zu modellieren. Darüber streiten die Wissenschaftler seit der Antike. Demokrit meinte, dass das Zufällige das Nichterkannte ist, und dass die Natur in ihrer Grundlage determiniert ist. Damit meinte Demokrit, dass in der Welt Ordnung herrscht und dass diese Ordnung durch eindeutige Gesetze bestimmt ist. Epikur widersprach Demokrit mit folgender Meinung: „Der Zufall ist objektiv, er ist die eigentliche Natur der Erscheinung.“ Die Religion und die Physik vor dem 20. Jahrhundert bauten auf der kausal-determinis- tischen Auﬀassung auf. Interessant ist zu bemerken, dass auch Albert Einstein die Benutzung des Begriﬀs Zufall nur als Kennzeichnung des noch nicht vollständigen Wissens zuließ und an die Existenz einfacher und klarer deterministischer Naturgesetze glaubte. Die Entwicklung der Wissenschaft (insbesondere der Physik und der Biologie) im 20. Jahrhundert führte eher zu der Epikur’schen Weltanschauung. Die experimentelle Physik bestätigte die Theorie der Quantenmechanik, die auf Zufallsereignissen aufgebaut ist. In der Evolutionsbiologie zweifelt man heute nicht an der These, dass ohne zufällige Mutationen der DNA die Evolution nicht stattgefunden hätte. Am besten formulierte der ungarische Mathematiker Alfréd Rényi eine moderne, überwiegend akzeptierte Ansicht der Rolle des Zufalls: „Es gibt keinen Widerspruch zwischen Kausalität und dem Zufall. In der Welt herrscht der Zufall, und eben deshalb gibt es in der Welt Ordnung und J. Hromkovič, Theoretische Informatik, DOI 10.1007/978-3-658-06433-4_8, © Springer Fachmedien Wiesbaden 2014 242 8 Randomisierung Gesetz, die sich in den Massen von zufälligen Ereignissen, den Gesetzen der Wahrscheinlichkeit entsprechend, entfalten.“ Für uns Informatiker ist wichtig zu begreifen, dass es sich oft lohnt, statt vollständig deterministischer Systeme und Algorithmen zufallsgesteuerte (randomisierte) Systeme und Algorithmen zu entwerfen und zu implementieren. Dabei geht es um nichts anderes, als von der Natur zu lernen. Es scheint eine Tatsache zu sein, dass die Natur immer den einfachsten und eﬃzientesten Weg geht und dass ein solcher Weg durch die Zufallssteuerung bestimmt wird. Die Praxis bestätigt diese Ansicht. In vielen Anwendungen können einfache zufallsgesteuerte Systeme und Algorithmen das Gewünschte eﬃzient und zuverlässig leisten, obwohl jedes vollständig deterministische System für diesen Zweck so komplex und ineﬃzient wäre, dass jeder Versuch, es zu bauen, praktisch sinnlos wäre. Dies ist auch der Grund dafür, dass man heutzutage die Klasse der praktisch lösbaren Probleme nicht mehr mit der deterministisch polynomiellen Zeit, sondern eher mit zufallsgesteuerten (randomisierten) polynomiellen Algorithmen verknüpft. Die Zielsetzung dieses Kapitels ist nicht, die Grundlage des Entwurfs von randomi- sierten Algorithmen und der Komplexitätstheorie der randomisierten Berechnung zu präsentieren, weil dazu zu viele Vorkenntnisse aus der Wahrscheinlichkeitstheorie, der Komplexitätstheorie und der Zahlentheorie notwendig wären. Wir ziehen es vor, anhand dreier Beispiele das Konzept der Zufallssteuerung zu veranschaulichen und dadurch auch ansatzweise ein etwas tieferes Verständnis zu gewinnen für die überlegene Stärke der Zufallssteuerung gegenüber der deterministischen Steuerung. Dieses Kapitel ist wie folgt aufgebaut. In Abschnitt 8.2 präsentieren wir einige ele- mentare Grundlagen der Wahrscheinlichkeitstheorie. In Abschnitt 8.3 entwerfen wir ein randomisiertes Kommunikationsprotokoll zum Vergleich der Inhalte zweier großer Daten- banken, das unvergleichbar eﬃzienter als jedes deterministische Kommunikationsprotokoll für diese Aufgabe ist. In Abschnitt 8.4 nutzen wir das vorgestellte Kommunikations- protokoll, um die Methode der häuﬁgen Zeugen als ein Paradigma für den Entwurf zufallsgesteuerter Algorithmen zu erklären. Wir wenden diese Methode ein weiteres Mal an, um einen eﬃzienten randomisierten Algorithmus für den Primzahltest zu entwickeln. Dabei zählt der Primzahltest zu den wichtigsten Entscheidungsproblemen in der Praxis, und wir kennen keinen Algorithmus, der den Primzahltest deterministisch für typische Eingabelängen ausreichend schnell entscheiden kann. In Abschnitt 8.5 stellen wir die Methode der Fingerabdrücke als eine spezielle Variante der Methode der häuﬁgen Zeugen vor. Wir wenden diese Methode an, um eﬃzient die Äquivalenz von zwei Polynomen zu entscheiden. Wie üblich beenden wir das Kapitel mit einer kurzen Zusammenfassung. 8.2 Elementare Wahrscheinlichkeitstheorie Wenn ein Ereignis (eine Erscheinung) eine unumgängliche Folge eines anderen Ereignisses ist, dann sprechen wir von Kausalität oder Determinismus. Wie wir schon in der Ein- leitung bemerkt haben, gibt es auch andere als völlig bestimmte, eindeutige Ereignisse. Die Wahrscheinlichkeitstheorie wurde entwickelt, um Situationen und Experimente mit mehrdeutigen Ergebnissen zu modellieren und zu untersuchen. Einfache Beispiele solcher Experimente sind der Münzwurf und das Würfeln. Es gibt hier keine Möglichkeit, das 8.2 Elementare Wahrscheinlichkeitstheorie 243 Ergebnis vorherzusagen, und deswegen sprechen wir von zufälligen Ereignissen.Inder Modellierung eines Wahrscheinlichkeitsexperiments betrachten wir also alle möglichen Er- gebnisse des Experimentes, die wir elementare Ereignisse nennen. Aus philosophischer Sicht ist es wichtig, dass die elementaren Ereignisse als atomare Ergebnisse zu betrachten sind. Atomar bedeutet, dass man ein elementares Ereignis nicht als eine Kollektion von noch einfacheren Ergebnissen betrachten kann, und sich somit zwei elementare Ereignisse gegenseitig ausschließen. Beim Münzwurf sind die elementaren Ereignisse „Kopf“ und „Zahl“ und beim Würfeln sind die elementaren Ereignisse die Zahlen „1“, „2“, „3“, „4“, „5“ und „6“. Ein Ereignis deﬁniert man dann als eine Teilmenge der Menge der elementaren Ereignisse. Zum Beispiel ist {2, 4, 6} das Ereignis, dass beim Würfeln eine gerade Zahl fällt. Weil elementare Ereignisse auch als Ereignisse betrachtet werden, stellt man sie, um konsistent zu bleiben, als einelementige Mengen dar. Im Folgenden betrachten wir nur Experimente mit einer endlichen Menge S elementarer Ereignisse, was die Anschaulichkeit der folgenden Deﬁnition erhöht. Wir möchten jetzt eine sinnvolle Theorie entwickeln, die jeder Erscheinung E ⊆ S eine Wahrscheinlichkeit zuordnet. Dass diese Aufgabe gar nicht so einfach ist, dokumentiert die Tatsache, dass man seit der Begründung der Wahrscheinlichkeitstheorie in den Werken von Pascal, Fermat und Huygens in der Mitte des 17. Jahrhunderts fast 300 Jahre gebraucht hat, bis eine allgemein akzeptierte axiomatische Deﬁnition der Wahrscheinlichkeit von Kolmogorov vorgeschlagen wurde. Unsere Einschränkung auf die Endlichkeit von S hilft uns, die technischen Schwierigkeiten solcher allgemeinen Deﬁnitionen zu vermeiden. Die Idee ist, die Wahrscheinlichkeit eines Ereignisses als das Verhältnis der Summe der Wahrscheinlichkeiten der günstigen (darin enthaltenen) elementaren Ereignisse zu der Summe der Wahrscheinlich- keiten aller möglichen elementaren Ereignisse (8.1) zu sehen. Durch diese Festlegung normiert man die Wahrscheinlichkeitswerte in dem Sinne, dass die Wahrscheinlichkeit 1 der Sicherheit und die Wahrscheinlichkeit 0 einem unmöglichen Ereignis entspricht. Ein anderer zentraler Punkt ist, dass die Wahrschein- lichkeiten der elementaren Ereignisse die Wahrscheinlichkeiten aller Ereignisse eindeutig bestimmen. Bei symmetrischen Experimenten wie dem Würfeln will man allen elementaren Ereignissen die gleiche Wahrscheinlichkeit zuordnen. Sei Wahr(E) die Wahrscheinlichkeit des Ereignisses E. Weil in unserem Modell als Resultat des Experimentes ein elementares Ereignis auftreten muss, setzt man Wahr(S) = 1 für die Menge S aller elementaren Ereignisse. Dann haben wir beim Würfeln Wahr({2, 4, 6})= Wahr({2}) + Wahr({4}) + Wahr({6}) Wahr(S) = Wahr({2}) + Wahr({4}) + Wahr({6}) = 1 6 + 1 6 + 1 6 = 1 2 , d. h., die Wahrscheinlichkeit, eine gerade Zahl zu werfen, ist genau 1 2 . Nach dem Wahr- 244 8 Randomisierung scheinlichkeitskonzept (8.1) erhalten wir für alle disjunkten Ereignisse X und Y Wahr(X ∪ Y )= Wahr(X) + Wahr(Y ) Wahr(S) = Wahr(X) + Wahr(Y ). Diese Überlegungen führen zu der folgenden axiomatischen Deﬁnition der Wahrscheinlich- keit. Deﬁnition 8.1. Sei S die Menge aller elementaren Ereignisse eines Wahrscheinlichkeits- experiments. Eine Wahrscheinlichkeitsverteilung auf S ist jede Funktion Wahr : P(S) → [0, 1], die folgende Bedingungen erfüllt: (i) Wahr({x}) ≥ 0 für jedes elementare Ereignis x, (ii) Wahr(S)=1 und (iii) Wahr(X ∪ Y )= Wahr(X)+ Wahr(Y ) für alle Ereignisse X, Y ⊆ S mit X ∩ Y = ∅. Wahr(X) nennt man die Wahrscheinlichkeit des Ereignisses X. Das Paar (S, Wahr) wird als Wahrscheinlichkeitsraum bezeichnet. Falls Wahr({x})= Wahr({y}) für alle x, y ∈ S, nennt man Wahr die uniforme Wahrscheinlichkeitsverteilung (oder Gleichverteilung) auf S. Aufgabe 8.1. Beweisen Sie, dass folgende Eigenschaften für jeden Wahrscheinlichkeitsraum (S, Wahr) immer gelten: (i) Wahr(∅)=0, (ii) Wahr(S − X)=1 − Wahr(X) für jedes X ⊆ S, (iii) für alle X, Y ⊆ S mit X ⊆ Y gilt Wahr(X) ≤ Wahr(Y ), (iv) Wahr(X ∪ Y ) = Wahr(X) + Wahr(Y ) − Wahr(X ∩ Y ) ≤ Wahr(X) + Wahr(Y ) für alle X, Y ⊆ S, (v) Wahr(X)= ∑ x∈X Wahr(x) für alle X ⊆ S. Wir bemerken, dass alle Eigenschaften aus Aufgabe 8.1 unserer Zielsetzung und damit der informellen Deﬁnition (8.1) entsprechen. Somit entspricht die Addition der Wahrschein- lichkeiten unserer intuitiven Vorstellung, dass die Wahrscheinlichkeit, dass irgendeines von mehreren unvereinbaren Ereignissen eintritt, gleich der Summe der Wahrscheinlichkeiten der betrachteten Ereignisse ist. Was entspricht der Multiplikation zweier Wahrscheinlichkeiten? Betrachten wir zwei Wahrscheinlichkeitsexperimente, die in dem Sinne unabhängig sind, dass kein Resultat eines Experimentes einen Einﬂuss auf das Resultat des anderen Experimentes hat. Ein Beispiel dafür ist, zweimal zu würfeln. Egal, ob wir auf einmal mit zwei Würfeln spielen oder ob wir zweimal denselben Würfel rollen lassen, die Resultate beeinﬂussen sich nicht gegenseitig. Zum Beispiel hat eine 3 beim ersten Wurf keinen Einﬂuss auf das Ergebnis des zweiten Wurfs. Wir wissen, dass Wahr(i)= 1 6 für beide Experimente und für alle 8.2 Elementare Wahrscheinlichkeitstheorie 245 i ∈{1, 2,..., 6} gilt. Betrachten wir jetzt die Zusammensetzung beider Zufallsexperimente (Würfe) als ein Zufallsexperiment. Die Menge der elementaren Ereignisse ist hier S2 = {(i, j) | i, j ∈{1, 2,..., 6}}, wobei für ein elementares Ereignis {(i, j)} der Index i das Ergebnis des ersten Wurfs und j das des zweiten ist. Wie soll jetzt korrekterweise die Wahrscheinlichkeitsverteilung Wahr2 auf S2 aus ({1, 2,..., 6}, Wahr) bestimmt werden? Wir bauen auf der Intuition auf, dass die Wahrscheinlichkeit des Eintretens von zwei vollständig unabhängigen Ereignissen gleich dem Produkt der Wahrscheinlichkeiten dieser Ereignisse ist und damit Wahr2({(i, j)}) = Wahr({i}) · Wahr({j})= 1 6 · 1 6 = 1 36 für alle i, j ∈{1, 2,..., 6} gilt. Überprüfen wir die Korrektheit dieser Überlegung. Die Menge S2 beinhaltet genau 36 elementare Ereignisse, die alle gleich wahrscheinlich sind. Damit ist tatsächlich Wahr2({(i, j)})= 1 36 für alle (i, j) ∈ S2. Aufgabe 8.2. Sei k ∈ N −{0}. Sei (S, Wahr) ein Wahrscheinlichkeitsraum, wobei Wahr eine uniforme Wahrscheinlichkeitsverteilung (Gleichverteilung) über S = {0, 1, 2,..., 2k − 1} ist. Erzeugen Sie (S, Wahr) durch k-fachen Münzwurf. Es bleibt zu klären, wie man die Wahrscheinlichkeitstheorie anwendet, um zufallsge- steuerte (randomisierte) Algorithmen zu entwerfen und zu analysieren. Dazu benutzt man zwei verschiedene Möglichkeiten. Die erste Möglichkeit ist, mit dem Modell der NTM mit endlichen Berechnungen zu starten und jeden nichtdeterministischen Schritt als ein Zufallsexperiment zu betrachten. Dies bedeutet, dass man bei einer Wahl aus k Möglichkeiten jeder Möglichkeit die Wahrscheinlichkeit 1 k zuordnet. Dann bestimmt man die Wahrscheinlichkeit einer Berechnung als das Produkt der Wahrscheinlichkeiten aller zufälligen Entscheidungen dieser Berechnung. Sei SA,x die Menge aller Berechnungen einer NTM (eines nichtdeterministischen Programms) A auf einer Eingabe x. Wenn man jeder Berechnung C aus SA,x die oben beschriebene Wahrscheinlichkeit Wahr(C) zuordnet, dann ist (SA,x, Wahr) ein Wahrscheinlichkeitsraum. Aufgabe 8.3. Beweisen Sie, dass (SA,x, Wahr) ein Wahrscheinlichkeitsraum ist. Die Summe der Wahrscheinlichkeiten der Berechnungen aus SA,x mit einer falschen Ausgabe A(x) für die Eingabe x ist dann die Fehlerwahrscheinlichkeit des Algorithmus A auf der Eingabe x, FehlerA(x). Die Fehlerwahrscheinlichkeit des Algorithmus A deﬁniert man als eine Funktion FehlerA : N → [0, 1] wie folgt. FehlerA(n) = max{FehlerA(x) ||x| = n}. Außer den Fehlerwahrscheinlichkeiten kann man zum Beispiel auch untersuchen, wie groß die Wahrscheinlichkeit ist, dass eine Berechnung aus höchstens t(n) Schritten besteht (d. h., wie groß die Summe der Wahrscheinlichkeiten der Berechnungen ist, die kürzer als t(n) sind). Die andere Möglichkeit, die randomisierten Algorithmen zu deﬁnieren, ist einfach, einen randomisierten Algorithmus als eine Wahrscheinlichkeitsverteilung über einer Menge deter- ministischer Algorithmen zu betrachten. Für 2 k viele deterministische Algorithmen kann 246 8 Randomisierung die Umsetzung wie folgt aussehen. Man gibt einem deterministischen Algorithmus (einer TM) A eine Folge von Zufallsbits (ein zusätzliches Band mit einer binären Zufallsfolge) als zusätzliche Eingabe. Jede Folge von Zufallsbits bestimmt eindeutig eine deterministi- sche Berechnung von A auf der gegebenen Eingabe x. Die Zufallsfolgen als elementare Ereignisse zu betrachten, entspricht also der Betrachtung der Berechnungen aus SA,x als elementare Ereignisse. Gewöhnlicherweise haben alle Zufallsfolgen die gleiche Wahrschein- lichkeit, und somit handelt es sich um die uniforme Wahrscheinlichkeitsverteilung über der Menge aller Berechnungen aus SA,x. Die Beispiele randomisierter Algorithmen in den nächsten zwei Abschnitten bauen auf diesem Modell der randomisierten Algorithmen auf. Die Folgen von zufälligen Bits interpretiert man in diesen Beispielen als eine zufällige Zahl, die dann die Berechnung und somit das Resultat der Berechnung beeinﬂusst. 8.3 Ein randomisiertes Kommunikationsprotokoll Die Zielsetzung dieses Abschnitts ist zu zeigen, dass randomisierte Algorithmen wesentlich eﬃzienter als bestmögliche deterministische Algorithmen sein können. Betrachten wir die folgende Aufgabenstellung. Wir haben zwei Rechner RI und RII. Ursprünglich besaßen diese eine Datenbank mit gleichem Inhalt. Mit der Zeit hat sich der Inhalt dynamisch geändert, aber wir haben versucht, die gleichen Änderungen auf beiden Rechnern zu machen, um idealerweise die gleiche Datenbank auf beiden Rechnern zu erhalten. Nach einer gewissen Zeit wollen wir nun überprüfen, ob RI und RII wirklich noch die gleichen Daten haben. Im Allgemeinen bezeichnen wir durch n die Größe der Datenbank in Bits. Konkret betrachten wir ein großes n =1016, was bei Gendatenbanken eine realistische Größe sein dürfte. Unser Ziel ist es, einen Kommunikationsalgorithmus (ein Protokoll) zu entwerfen, der feststellt, ob die Inhalte der Datenbanken von RI und RII unterschiedlich oder gleich sind. Die Komplexität des Kommunikationsalgorithmus messen wir in der Anzahl der ausgetauschten Bits zwischen RI und RII. Man kann beweisen, dass jedes deterministische Protokoll für diese Aufgabe einen Austausch von n Bits zwischen RI und RII nicht vermeiden kann. Also existiert kein Protokoll, das höchstens n − 1 Kommunikati- onsbits benutzen darf und diese Aufgabe zuverlässig löst. Wenn man bei der Datenmenge mit n =1016 noch sicherstellen soll, dass alle Kommunikationsbits korrekt ankommen, würde man wahrscheinlich auf den Versuch verzichten, die Aufgabe auf diese Weise zu lösen. Die Lösung in dieser Situation bietet folgendes zufallsgesteuertes Protokoll. Es basiert auf dem Primzahlsatz (Satz 2.3). Ein zufallsgesteuertes Kommunikationsprotokoll R =(RI,RII) Ausgangssituation: RI hat n Bits x = x1 ...xn, RII hat n Bits y = y1 ...yn. Phase 1. RI wählt zufällig mit einer uniformen Wahrscheinlichkeitsverteilung p als eine der Prim(n2) ∼ n 2/ ln n2 Primzahlen kleiner gleich n2. Phase 2. RI berechnet die Zahl s = Nummer(x) mod p und schickt die binäre Darstellung von s und p an RII. Phase 3. Nach Empfang von s und p berechnet RII die Zahl q = Nummer(y) mod p. Falls q ̸= s, dann gibt RII die Ausgabe „ungleich“ aus. Falls q = s, dann gibt RII die Ausgabe „gleich“ aus. 8.3 Ein randomisiertes Kommunikationsprotokoll 247 Jetzt analysieren wir die Arbeit von R =(RI,RII). Zuerst bestimmen wir die Komple- xität, gemessen als die Anzahl der Kommunikationsbits, und dann analysieren wir die Zuverlässigkeit (Fehlerwahrscheinlichkeit) von R =(RI,RII). Die einzige Kommunikation besteht darin, dass RI die Zahlen s und p an RII schickt. Weil s ≤ p<n 2 gilt, ist die Länge der binären Nachricht 2 ·⌈log2 n2⌉≤ 4 ·⌈log2 n⌉.Für n =1016 sind dies höchstens 4 · 16 ·⌈log2 10⌉ = 256 Bits. Es ist also eine sehr kurze Nachricht, die man problemlos zuverlässig übertragen kann. Bei der Analyse der Fehlerwahrscheinlichkeit unterscheiden wir zwei Möglichkeiten bezüglich der tatsächlichen Relation zwischen x und y. (i) Sei x = y. Dann gilt Nummer(x)mod p = Nummer(y)mod p für alle Primzahlen p. Also gibt RII die Antwort „gleich“ mit Sicherheit. In diesem Fall ist also die Fehlerwahrscheinlichkeit 0. (ii) Sei x ̸= y. Wir bekommen eine falsche Antwort „gleich“ nur dann, wenn RI eine zufällige Primzahl p gewählt hat, die die Eigenschaft hat, dass z = Nummer(x)mod p = Nummer(y)mod p gilt. In anderer Form geschrieben: Nummer(x)= x′ · p + z und Nummer(y)= y′ · p + z für irgendwelche natürlichen Zahlen x′ und y′. Daraus folgt, dass Nummer(x) − Nummer(y)= x ′ · p − y′ · p =(x ′ − y′) · p, also dass p die Zahl |Nummer(x) − Nummer(y)| teilt. Also gibt unser Protokoll R =(RI,RII) eine falsche Antwort nur, wenn die gewählte Primzahl p die Zahl |Nummer(x) − Nummer(y)| teilt. Wir wissen, dass p aus Prim(n2) Primzahlen aus {2, 3,...,n 2} mit uniformer Wahrscheinlichkeitsverteilung gewählt wurde. Es ist also hilfreich festzustellen, wie viele dieser Prim(n2) ∼ n2/ ln n2 Primzahlen die Zahl |Nummer(x) − Nummer(y)| teilen können. Weil die binäre Länge von x und y gleich n ist, gilt w = |Nummer(x) − Nummer(y)| < 2n. Sei w = pi1 1 pi2 2 ...p ik k ,wobei p1 <p2 < ··· <pk Primzahlen und i1,i2,...,ik positive ganze Zahlen sind. Wir wissen, dass jede Zahl eine solche eindeutige Faktorisierung besitzt. Unser Ziel ist zu beweisen, dass k ≤ n − 1. Wir beweisen es indirekt. Angenommen, k ≥ n. Dann ist für genügend große n w = p i1 1 pi2 2 ...p ik k >p1p2 ...pn > 1 · 2 · 3 ·· · ·· n = n! > 2 n. Das widerspricht aber der bekannten Tatsache, dass w< 2 n. Also kann w höchstens n − 1 unterschiedliche Primfaktoren haben. Weil jede Primzahl aus {2, 3,...,n 2} 248 8 Randomisierung die gleiche Wahrscheinlichkeit hat, gewählt zu werden, ist die Wahrscheinlichkeit, ein p zu wählen, das w teilt, höchstens n − 1 Prim(n2) ≤ n − 1 n2/ ln n2 ≤ ln n2 n für genügend große n. Also ist die Fehlerwahrscheinlichkeit von R für unterschiedliche Inhalte x und y höchstens ln n 2/n, was für n =10 16 höchstens 0, 369 · 10−14 ist. Eine so kleine Fehlerwahrscheinlichkeit ist kein ernsthaftes Risiko, aber nehmen wir an, dass jemand sich eine noch kleinere Fehlerwahrscheinlichkeit wünscht. Dann kann man das Protokoll (RI,RII) 10-mal mit 10 unabhängigen Wahlen einer Primzahl wie folgt laufen lassen. Protokoll R10 Anfangssituation: RI hat n Bits x = x1 ...xn und RII hat n Bits y = y1 ...yn. Phase 1. RI wählt zufällig mit uniformer Wahrscheinlichkeitsverteilung zehn Prim- zahlen p1,p2,...,p10 aus {2, 3,...,n 2}. Phase 2. RI berechnet si = Nummer(x)mod pi für i =1, 2,..., 10 und schickt die binären Darstellungen von p1,p2,...,p10,s1,s2,...,s10 an RII. Phase 3. Nach dem Empfang von p1,p2,...,p10,s1,s2,...,s10 berechnet RII qi = Nummer(y)mod pi für i =1, 2,..., 10. Falls ein i ∈{1, 2,..., 10} existiert, so dass qi ̸= si, dann gibt RII die Ausgabe „ungleich“ aus. Falls qj = sj für alle j ∈{1, 2,..., 10}, dann gibt RII die Ausgabe „gleich“ aus. Wir bemerken, dass die Kommunikationskomplexität von R10 zehnmal größer ist als die Komplexität von R. In unserem Fall n =10 16 sind dies aber höchstens 2560 Bits, was kein technisches Problem darstellt. Wie ändert sich aber die Fehlerwahrscheinlichkeit? Falls x = y, wird R10 wieder keinen Fehler machen und gibt mit Sicherheit die richtige Antwort „gleich“ aus. Falls x ̸= y, wird R10 nur dann eine falsche Antwort liefern, wenn alle 10 zufällig gewählten Primzahlen zu den höchstens n − 1 Primzahlen, die |Nummer(x) − Nummer(y)| teilen, gehören. Weil die 10 Primzahlen in 10 unabhängigen Experimenten gewählt worden sind, ist die Fehlerwahrscheinlichkeit höchstens ( n − 1 Prim(n2) )10 ≤ ( ln n2 n )10 = 2 10 · (ln n)10 n10 für genügend große n. Für n =1016 ist dies höchstens 0, 472 · 10−141. Wenn wir bedenken, dass die Anzahl der Mikrosekunden seit dem Urknall bis zum heutigen Tag eine 24-stellige Zahl ist und die Anzahl Protonen im bekannten Universum eine 79-stellige Zahl ist, kann man eine Fehlerwahrscheinlichkeit unter 10−141 leichten Herzens in Kauf nehmen. Auch wenn ein deterministisches Protokoll mit Kommunikationskomplexität 1016 Bits praktisch 8.4 Die Methode der häuﬁgen Zeugen und der randomisierte Primzahltest 249 realisierbar wäre, ist es klar, dass man aus Kostengründen das zufallsgesteuerte Protokoll implementieren würde. Die Konstruktion von R10 aus R gibt uns eine wichtige Einsicht. Wir können die Fehlerwahrscheinlichkeit zufallsgesteuerter Algorithmen durch mehrfaches Durchlaufen des Algorithmus nach unten drücken. Bei einigen Algorithmen, wie bei unserem Protokoll, reichen wenige Wiederholungen für einen extremen Rückgang der Fehlerwahrscheinlichkeit. Aufgabe 8.4. Betrachten Sie das Protokoll Rk,k ∈ N −{0}, das auf der zufälligen Wahl von k Primzahlen aus {2, 3,...,n2} basiert. Wie entwickelt sich die Fehlerwahrscheinlichkeit von Rk mit wachsendem k? Aufgabe 8.5. Ein anderer Ansatz, um die Fehlerwahrscheinlichkeit nach unten zu drücken, ist, das Protokoll R in ein Protokoll Qr,r ∈ N −{0, 1, 2} umzuändern. Qr arbeitet genau wie R,nur am Anfang wählt Qr zufällig eine Primzahl aus der Menge {2, 3,...,nr} statt aus {2, 3,...,n2}. Bestimmen Sie die Komplexität und die Fehlerwahrscheinlichkeit von Qr für jedes r ≥ 2. Aufgabe 8.6. Sei δ> 1 eine positive ganze Zahl. Entwerfen Sie ein randomisiertes Protokoll, das den Vergleich von zwei Datenbanken der Größe n mit einer Fehlerwahrscheinlichkeit von höchstens 1 δ realisiert. Welcher der beiden Ansätze aus den Aufgaben 8.4 und 8.5 ist eﬃzienter bezüglich der Kommunikationskomplexität? Lohnt es sich mehr, mehrere kleine Primzahlen oder eher eine (potentiell) größere Primzahl zufällig zu wählen? 8.4 Die Methode der häuﬁgen Zeugen und der randomisierte Primzahltest In diesem Abschnitt wollen wir zuerst nach den Gründen suchen, warum unser zufallsge- steuertes Protokoll R unvergleichbar eﬃzienter ist, als jedes deterministische Protokoll für die gestellte Aufgabe sein kann. Das Protokoll R haben wir durch eine einfache Anwendung der sogenannten Methode der häuﬁgen Zeugen gewonnen. Wir stellen diese Methode jetzt vor. Im Allgemeinen betrachtet man ein Entscheidungsproblem, bei dem man entscheiden soll, ob eine gegebene Eingabe eine gewisse Eigenschaft hat oder nicht. Setzen wir noch voraus, dass wir keinen eﬃzienten deterministischen Algorithmus für die Aufgabe gefunden haben (oder sogar, dass kein eﬃzienter Algorithmus für die Aufgabe existiert). Bei einer Anwendung der Methode der häuﬁgen Zeugen fängt man jetzt mit der Suche nach einer passenden Deﬁnition von Zeugen an. Ein Zeuge (vergleiche Deﬁnition 6.9) sollte eine Zusatzinformation zur Eingabe sein, mit deren Hilfe man eﬃzient deterministisch beweisen kann, dass die Eingabe die gewünschte Eigenschaft hat (oder dass die Eingabe die Eigenschaft nicht hat). In unserem Beispiel war eine Primzahl p der Zeuge für den Unterschied zwischen x und y (d. h. Zeuge von x ̸= y), falls Nummer(x)mod p ̸= Nummer(y)mod p. Wenn man also ein solches p geschenkt bekommt, kann man eﬃzient den Fakt „x ist unterschiedlich von y“ beweisen. In der Realität können wir auf ein solches Geschenk nicht hoﬀen und, schlimmer noch, wir können uns den Zeugen nicht alleine eﬃzient determinis- tisch ausrechnen (sonst hätten wir einen eﬃzienten deterministischen Algorithmus für die 250 8 Randomisierung Aufgabe). Um einen eﬃzienten randomisierten Algorithmus zu entwerfen, brauchen wir für jede Eingabe eine Menge von Zeugenkandidaten, von denen ausreichend viele wirklich Zeugen sind. In unserem Beispiel sind Kandidaten für Zeugen alle ungefähr n2/ ln n2 Primzahlen kleiner gleich n2. Von diesen Kandidaten sind mindestens (n2/ ln n2) − (n − 1) Zeugen und somit ist die Wahrscheinlichkeit, einen Zeugen aus der Kandidatenmenge zufällig zu ziehen, mindestens n2 ln n2 − (n − 1) n2 ln n2 ≥ 1 − ln n2 n . Das ist sehr günstig, weil dieser Wert sehr nahe an 1 ist. Aber auch wenn die Wahrschein- lichkeit, einen Zeugen zu ziehen, nur 1 2 wäre, sind die Zeugen noch immer ausreichend häuﬁg. Es reicht in diesem Fall aus, einfach mehrere Zufallsversuche zu machen. Dadurch wächst die Wahrscheinlichkeit schnell, in mindestens einem der Versuche einen Zeugen zu bekommen, was für uns hinreichend ist. Jetzt kann man noch fragen, wie es möglich ist, dass wir einen Zeugen nicht deter- ministisch schnell ﬁnden können, wenn es so viele zwischen den Zeugenkandidaten gibt. Eine Möglichkeit wäre doch, systematisch der Reihe nach alle Kandidaten auf eine so geschickte Weise durchzuprobieren, dass man nach kurzer Zeit einen Zeugen ﬁndet. Das Problem ist aber, dass für jede Eingabe die Zeugen anders unter den Zeugenkandidaten verteilt sein können. Wenn man sich also auf eine Durchsuchungsstrategie festlegt, könnte man immer Eingaben ﬁnden, bei denen die Strategie versagt. Betrachten wir unser Beispiel. Hier kann man sogar beweisen, dass keine Strategie existiert, die eﬃzient einen Zeugen für jede Eingabe (x, y) ﬁndet. Um dies zu veran- schaulichen, nehmen wir die einfache Strategie, die die Primzahlen eine nach der an- deren beginnend mit der kleinsten ausprobiert. Es ist klar, dass spätestens nach n Proben ein Zeuge gefunden werden muss, weil höchstens n − 1 Nicht-Zeugen zwischen den Kandidaten sind. Leider bedeuten n Proben eine Kommunikationskomplexität von n · 4 · log2 n, was wir uns nicht leisten können. Warum haben wir nicht immer die Möglich- keit, nach ein paar Proben einen Zeugen zu ﬁnden? Weil unsere Strategie bei Eingaben (x, y) mit Nummer(x) − Nummer(y)= p1 · p2 ·· · ·· pk,wobei k = n/(2(log n)2) und p1 <p2 < ··· <pk die kleinsten Primzahlen sind, k + 1 Proben braucht, um einen Zeugen zu ﬁnden. Man kann sich leicht vorstellen, dass man bei jeder anderen Aufzählung der Primzahlen speziﬁsche Eingaben ﬁndet, für die viele Proben notwendig werden, um einen Zeugen zu ﬁnden. Die Methode der häuﬁgen Zeugen ist eine erfolgreiche starke Methode zum Entwurf zufallsgesteuerter Algorithmen. Der eﬃziente zufallsgesteuerte Primzahltest basiert auf dieser Methode, und der Primzahltest gehört zu den wichtigsten algorithmischen Aufgaben von großer praktischer Bedeutung. Zu erklären, wie man hier die Zeugen deﬁniert, ist eine zu komplexe Aufgabe für diese Einführung und daher ist unsere Zielsetzung viel bescheidener. Wir zeigen nur, wie man den randomisierten Primzahltest für ungerade Zahlen n mit ungeradem n−1 2 eﬃzient machen kann. Zuerst klären wir, was bei zahlentheoretischen Problemen eﬃzient bedeutet. Für eine Zahl n ist die Eingabegröße ⌈log2(n +1)⌉. Ein polynomieller Algorithmus für den Prim- zahltest für n muss also eine polynomielle Laufzeit bezüglich ⌈log2(n +1)⌉ haben.Inder 8.4 Die Methode der häuﬁgen Zeugen und der randomisierte Primzahltest 251 Praxis testen wir Zahlen von mehreren Hunderten von Ziﬀern (zum Beispiel log2 n ≈ 500) und deswegen kann man sich eine exponentielle Komplexität in log2 n nicht leisten. So- mit ist der naive deterministische Algorithmus, der überprüft, ob eine der Zahlen aus {2, 3,..., ⌊√n⌋} die gegebene Zahl n teilt, von exponentieller Komplexität (mindestens √n =2 log2 n 2 = √2 log2 n). Bei diesem Einsatz ist ein Zeuge der Tatsache „p ist keine Primzahl“ jede natürliche Zahl m> 1,m ̸= p, die p teilt. Solche Zeugen1 sind aber im Allgemeinen nicht häuﬁg. Wenn n = p · q für zwei Primzahlen p und q, dann hat n nur zwei Zeugen p und q in der Kandidatenmenge der Mächtigkeit Ω( √n). Deswegen muss man versuchen, Zeugen auf eine neue Art zu bestimmen. Satz 8.1 (Satz von Fermat). Für jede Primzahl p und jede natürliche Zahl a mit ggT(a, p)=1 gilt a p−1 mod p =1. Beweis. In dem Beweis nutzen wir die Tatsache, dass jede Zahl eine eindeutige Zerlegung in Primzahlfaktoren hat. Weil p eine Primzahl ist, gilt c · d mod p =0 ⇐⇒ c mod p =0 oder d mod p = 0 (8.2) für alle natürlichen Zahlen c und d. Sei a eine beliebige Zahl mit ggT(a, p) = 1. Betrachten wir die Zahlen m1 =1 · a, m2 =2 · a, ..., mp−1 =(p − 1) · a. Wir behaupten, dass mu mod p ̸= mv mod p für alle u, v ∈{1,...,p − 1} mit u ̸= v. Wir beweisen dies indirekt. Angenommen mu mod p = mv mod p für irgendwelche u, v ∈{1,...,p − 1}, u>v. Dann teilt p die Zahl mu − mv =(u − v) · a. Das ist aber nicht möglich, weil u − v< p und nach der Voraussetzung des Satzes ggT(a, p) = 1 ist. Somit gilt |{m1 mod p, m2 mod p, ..., mp−1 mod p}| = p − 1. Jetzt behaupten wir, dass keine der Zahlen mi mod p eine 0 ist. Falls mu mod p =(u · a)mod p =0 für ein u gelten würde, dann wäre nach (8.2) u mod p =0 oder a mod p =0. 1Die auf der klassischen Deﬁnition von Primzahlen basieren. 252 8 Randomisierung Die Primzahl p kann aber keine der Zahlen u und a teilen, weil u<p und ggT(a, p)=1. Somit gilt {m1 mod p, m2 mod p,...,mp−1 mod p} = {1, 2,...,p − 1}. (8.3) Betrachten wir jetzt die Zahl m = m1 · m2 · ··· · mp−1. Der Deﬁnition von mi folgend ist m =1 · a · 2 · a ·· · ·· (p − 1) · a =1 · 2 ·· · ·· (p − 1) · ap−1. (8.4) Aus (8.3) erhalten wir m mod p =1 · 2 ·· · ·· (p − 1) mod p. (8.5) Die Gleichungen (8.4) und (8.5) implizieren 1 · 2 ·· · ·· (p − 1) · ap−1 mod p =1 · 2 ·· · ·· (p − 1) mod p, das heißt, ap−1 mod p =1. \u0002 Eine Verfeinerung des Fermat’schen Satzes ist die folgende Behauptung: „p ist eine Primzahl“ ⇐⇒ (a p−1 2 mod p) ∈{1,p − 1} für alle a ∈{1,...,p − 1}. Diese Behauptung liefert eine neue Deﬁnition von Primzahlen. Nach dieser Deﬁnition kommen als Zeugen für die Tatsache „n ist keine Primzahl“ die Zahlen a in Frage, für die a n−1 2 mod n/∈{1,n − 1} gilt. Der folgende Satz besagt, dass solche Zeugen für gewisse Zahlen n hinreichend häuﬁg sind. Satz 8.2. Für jede ungerade natürliche Zahl n mit ungeradem n−1 2 (das heißt n mod 4= 3) gilt (i) falls n eine Primzahl ist, dann ist a n−1 2 mod n ∈{1,n − 1} für alle a ∈{1,...,n − 1}, und (ii) falls n keine Primzahl ist, dann ist a n−1 2 mod n/∈{1,n − 1} für mindestens die Hälfte der Zahlen a aus {1,...,n − 1}. Aufgabe 8.7.∗ Beweisen Sie Satz 8.2. Somit haben wir für Zahlen m, die keine Primzahlen sind und für die m mod 4 = 3 gilt, die Wahrscheinlichkeit mindestens 1 2 , in einem Zufallsversuch einen Zeugen ihrer Nichtzugehörigkeit zu den Primzahlen zu wählen. Um mit dieser Deﬁnition von Zeugen deﬁnitiv einverstanden zu sein, müssen wir noch feststellen, dass a n−1 2 mod n 8.4 Die Methode der häuﬁgen Zeugen und der randomisierte Primzahltest 253 eﬃzient zu berechnen ist. Wir können uns natürlich nicht leisten, n−1 2 -mal mit a zu multiplizieren, weil dann die Anzahl der Operationen exponentiell in ⌈log2 n⌉ wäre. Wenn wir a b mod n zu berechnen haben und b =2 k gilt, dann kann dies mit der Methode des wiederholten Quadrierens mit k Multiplikationen wie folgt berechnet werden: a 2 mod n = a · a mod n, a 4 mod n =(a2 mod n) · (a2 mod n)mod n, a8 mod n =(a4 mod n) · (a4 mod n)mod n, ... a 2k mod n =(a2k−1 mod n)2 mod n. Im Allgemeinen sei b = ∑k i=1 bi · 2i−1 (das heißt b = Nummer(bkbk−1 ...b1)) für ein k ∈ N −{0} und bi ∈{0, 1} für i =1,...,k. Dann ist oﬀensichtlich ab = a b1·20 · a b2·21 · a b3·22 ·· · · · a bk·2k−1 . Um a b mod n zu berechnen, berechnet man zuerst alle Zahlen ai = a 2i mod n durch wiederholtes Quadrieren. Danach multipliziert man mod n alle Zahlen ai, für die bi =1 gilt. Die Anwendung dieses Ansatzes für die Berechnung von a n−1 2 mod n für ein a ∈ {1,...,n − 1} bedeutet, dass man während der ganzen Berechnung nur mit Zahlen aus {0, 1,...,n − 1} arbeitet, also mit Zahlen der binären Länge ⌈log2 n⌉. Oﬀensichtlich ist die Anzahl der Multiplikationen solcher Zahlen hier kleiner als 2 ·⌈log2 n−1 2 ⌉∈ O(log2 n). Nach dem logarithmischen Kostenmaß ist die gesamte Komplexität der Berechnung von a n−1 2 mod n in O((log2 n)2). Damit erhalten wir folgenden randomisierten Algorithmus für den Primzahltest. Solovay-Strassen-Algorithmus Eingabe: Eine ungerade Zahl n mit ungeradem n−1 2 . Phase 1. Wähle zufällig bezüglich der uniformen Wahrscheinlichkeitsverteilung ein a ∈{1, 2,...,n − 1}. Phase 2. Berechne x := a n−1 2 mod n. Phase 3. if x ∈{1,n − 1} then output „Primzahl“; else output „keine Primzahl“; Analysieren wir die Fehlerwahrscheinlichkeit dieses randomisierten Primzahltests. Falls n eine Primzahl ist, gilt nach (i) von Satz 8.2, dass a n−1 2 mod n ∈{1,n − 1} für alle a ∈{1,...,n − 1} und somit ist die Ausgabe des Algorithmus immer „Primzahl“ (d. h., die Fehlerwahrscheinlichkeit ist gleich 0). 254 8 Randomisierung Falls n keine Primzahl ist, ist die Wahrscheinlichkeit, dass a kein Zeuge dieser Tatsache ist, nach (ii) von Satz 8.2 mindestens 1 2 . Damit ist die Fehlerwahrscheinlichkeit in diesem Fall höchstens 1 2 . Diese Fehlerwahrscheinlichkeit ist oﬀensichtlich zu groß, aber wenn wir statt eines a zwanzig Zahlen a1,...,a20 unabhängig aus {1, 2,...,n − 1} wählen und die Antwort „Primzahl“ nur geben, wenn a n−1 2 i mod n ∈{1,n − 1} für alle i ∈{1,..., 20}, dann ist die Fehlerwahrscheinlichkeit kleiner als 10 −6. Aufgabe 8.8. Sei k ∈ N,k ≥ 2. Wie weit kann man die Fehlerwahrscheinlichkeit des Solovay- Strassen-Algorithmus reduzieren, wenn man statt eines Versuchs einen Zeugen zu ﬁnden, k unabhängige Versuche macht? Begründen Sie Ihre Antwort. 8.5 Die Methode der Fingerabdrücke und die Äquivalenz von zwei Polynomen In Abschnitt 8.3 haben wir die Methode der häuﬁgen Zeugen benutzt, um zwei große Zahlen Nummer(x) und Nummer(y) mittels eines randomisierten Kommunikationsprotokolls zu vergleichen. Die dort vorgestellte spezielle Anwendung der Methode der häuﬁgen Zeugen nennt man auch die Methode der Fingerabdrücke, die man allgemein wie folgt darstellen kann. Schema der Methode der Fingerabdrücke Aufgabe: Entscheide die Äquivalenz (im gegebenen Sinne) von zwei Objekten O1 und O2, deren genaue Darstellung sehr umfangreich ist. Phase 1. Sei M eine „geeignete“ Menge von Abbildungen von vollständigen Darstel- lungen betrachteter Objekte in partielle Darstellungen dieser Objekte. Wähle zufällig eine Abbildung h aus M . Phase 2. Berechne h(O1) und h(O2). h(Oi) nennt man den Fingerabdruck von Oi für i =1, 2. Phase 3. if h(O1)= h(O2) then output „O1 und O2 sind äquivalent“; else output „O1 und O2 sind nicht äquivalent“; In unserem Beispiel in Abschnitt 8.3 waren O1 und O2 zwei große Zahlen von n Bits (n =10 16). Die Menge M war {hp | hp(m)= m mod p für alle m ∈ N,p ist eine Primzahl, p ≤ n2}. Für die zufällig gewählte Primzahl p waren hp(O1)= O1 mod p und hp(O2)= O2 mod p die Fingerabdrücke von O1 und O2. Der Kernpunkt der Methode ist, dass hp(Oi) im Vergleich zu Oi eine wesentlich kürzere Darstellung hat und dadurch der Vergleich von hp(O1) und hp(O2) wesentlich einfacher ist als der Vergleich von O1 und O2. Das kann man aber nur dadurch erreichen, dass hp(Oi) keine vollständige Beschreibung von Oi ist. Also muss man das Risiko einer fehlerhaften Entscheidung in Kauf nehmen. Der Rest der Grundidee basiert auf dem Prinzip der 8.5 Die Methode der Fingerabdrücke und die Äquivalenz von zwei Polynomen 255 Methode der häuﬁgen Zeugen. Die Menge M ist die Menge der Kandidaten für einen Zeugen der Nicht-Äquivalenz von O1 und O2. Wenn für jedes Paar unterschiedlicher Objekte O1 und O2 in M zahlreiche (bezüglich |M |) Zeugen von O1 ̸= O2 vorhanden sind, kann man die Fehlerwahrscheinlichkeit beliebig nach unten drücken. Die Kunst der Anwendung der Methode der Fingerabdrücke besteht in der geeigneten Wahl der Menge M . Einerseits sollen die Fingerabdrücke so kurz wie möglich sein, um einen eﬃzienten Vergleich zu ermöglichen. Andererseits sollen sie so viele Informationen wie möglich über die abgebildeten Objekte enthalten, um die Wahrscheinlichkeit des Verlustes des Unterschiedes zwischen O1 und O2 in den Fingerabdrücken h(O1) und h(O2) gering zu halten.2 Somit muss bei der Wahl von M immer der Tradeoﬀ zwischen dem Grad der „Komprimierung“ von Oi zu h(Oi) und der Fehlerwahrscheinlichkeit im Auge behalten werden. In unserer Anwendung dieser Methode in Abschnitt 8.3 ist es uns gelungen, mit zu 0 strebender Fehlerwahrscheinlichkeit einen exponentiellen Sprung zwischen der Darstellung von Oi und h(Oi) zu schaﬀen, nämlich |h(Oi)|∈ O(log2 |Oi|). Im Folgenden wollen wir ein Äquivalenzproblem betrachten, für das kein (deterministi- scher) polynomieller Algorithmus bekannt ist und das man eﬃzient randomisiert mit der Methode der Fingerabdrücke lösen kann. Das Problem ist das Äquivalenzproblem von zwei Polynomen von mehreren Variablen über einem endlichen Körper Zp. Zwei Polynome P1(x1,...,xn) und P2(x1,...,xn) heißen äquivalent über Zp, wir schreiben auch P1(x1,...,xn) ≡ P2(x1,...,xn)mod p, falls für alle (α1,...,αn) ∈ (Zp) n gilt, dass P1(α1,...,αn)mod p = P2(α1,...,αn)mod p. Für dieses Äquivalenzproblem ist kein polynomieller Algorithmus bekannt. Es könnte jemand widersprechen, dass so ein Vergleich doch einfach ist; es reicht aus, nur die Koeﬃ- zienten bei gleichen Termen zu vergleichen. Zwei Polynome sind genau dann gleich, wenn die Koeﬃzienten bei allen Termen gleich sind. Die Schwierigkeit des Äquivalenztests liegt aber darin, dass für einen solchen einfachen Vergleich beide Polynome in der Normalform vorliegen müssen. Die Normalform eines Polynoms von n Variablen x1,x2,...,xn und Grad 3 d ist d∑ i1=0 d∑ i2=0 ··· d∑ in=0 ci1,i2,...,in · x i1 1 · x i2 2 ·· · · · x in n . Die Polynome für unseren Äquivalenztest dürfen aber in einer beliebigen Form, wie zum Beispiel P (x1,x2,x3,x4,x5,x6)=(x1 + x2) 10 · (x3 − x4) 7 · (x5 + x6) 20 eingegeben werden. Wenn wir uns an die binomische Formel (x1 + x2) n = n∑ k=0 (n k ) · xk 1 · x n−k 2 2Daher kommt auch der Name der Methode, weil bei Menschen Fingerabdrücke als eine fast eindeutige Identiﬁkation gelten. 3Der Grad eines Polynoms von mehreren Variablen ist das Maximum der Grade der einzelnen Variablen. 256 8 Randomisierung erinnern, wird uns klar, dass P (x1,x2,x3,x4,x5,x6) genau 11 · 8 · 21 = 1848 Terme (mit Koeﬃzienten ungleich 0) hat. Also kann eine Normalform eines Polynoms exponentiell länger sein als seine eingegebene Darstellung, und somit kann man die Normalform im All- gemeinen nicht in polynomieller Zeit erzeugen. Wir müssen versuchen, die Polynome ohne Erzeugung der Normalform zu vergleichen. Wir wählen dazu eine sehr einfache Strategie. Für zwei Polynome P1(x1,...,xn) und P2(x1,...,xn), ist ein α =(α1,...,αn) ∈ (Zp)n ein Zeuge von P1(x1,...,xn) ̸≡ P2(x1,...,xn), wenn P1(α1,...,αn)mod p ̸= P2(α1,...,αn)mod p. In der Sprache der Methode der Fingerabdrücke ist hα(P1)= P1(α1,...,αn)mod p der Fingerabdruck von P1. Damit ist der folgende einfache randomisierte Algorithmus bestimmt: Algorithmus AQP Eingabe: Eine Primzahl p und zwei Polynome P1 und P2 über n Variablen x1,. . . , xn,wobei n ∈ N −{0}, und vom Grad höchstens d,wobei d ∈ N. Phase 1. Wähle zufällig4 ein α =(α1,...,αn) ∈ (Zp) n. Phase 2. Berechne die Fingerabdrücke hα(P1)= P1(α1,...,αn)mod p und hα(P2)= P2(α1,...,αn)mod p. Phase 3. if hα(P1)= hα(P2) then output „P1 ≡ P2“; else output „P1 ̸≡ P2“; Untersuchen wir jetzt die Fehlerwahrscheinlichkeit des Algorithmus AQP. Falls P1 und P2 äquivalent über Zp sind, dann gilt P1(α1,...,αn)mod p = P2(α1,...,αn)mod p für alle (α1,α2,...,αn) ∈ (Zp) n. Somit ist die Fehlerwahrscheinlichkeit für die Eingaben P1,P2 mit P1 ≡ P2 gleich 0. Seien P1 und P2 zwei Polynome, die nicht äquivalent sind. Wir zeigen jetzt, dass die Fehlerwahrscheinlichkeit kleiner als 1 2 ist, wenn p> 2nd ist. Die Frage P1(x1,...,xn) ≡ P2(x1,...,xn) ist äquivalent zu der Frage Q(x1,...,xn)= P1(x1,...,xn) − P2(x1,...,xn) ≡ 0. 4bezüglich der Gleichverteilung über (Zp)n 8.5 Die Methode der Fingerabdrücke und die Äquivalenz von zwei Polynomen 257 Das heißt, wenn P1 und P2 nicht äquivalent sind, dann ist das Polynom Q nicht identisch zu 0. Unser Ziel ist jetzt zu zeigen, dass die Anzahl der Nullstellen eines Polynoms Q ̸≡ 0 von n Variablen und Grad d beschränkt ist. Dadurch gibt es genügend viele Zeugen α ∈ (Zp)n mit Q(α) mod p ̸= 0 (d. h. mit P1(α) mod p ̸= P2(α) mod p). Wir fangen mit dem bekannten Satz über die Anzahl Nullstellen für Polynome mit einer Variablen an. Satz 8.3. Sei d ∈ N und sei P (x) ein Polynom einer Variablen x vom Grad d über einem beliebigen Körper. Dann ist entweder P (x) überall gleich 0 oder P hat höchstens d Wurzeln (Nullstellen). Beweis. Wir beweisen den Satz mit Induktion bezüglich d. (i) Induktionsanfang. Sei d = 0. Dann ist P (x)= c für eine Konstante c. Falls c ̸= 0, dann hat P (x) keine Nullstelle. (ii) Induktionsschritt. Sei die Behauptung des Satzes gültig für d − 1,d ≥ 1. Wir beweisen sie für d.Sei P (x) ̸≡ 0 und sei a eine Nullstelle von P . Dann ist P (x)=(x − a) · P ′(x) wobei P ′(x)= P (x) (x−a) ein Polynom vom Grad d − 1 ist. Mit der Induktionsannahme hat P ′(x) höchstens d − 1 Nullstellen. Somit hat P (x) höchstens d Nullstellen. \u0002 Jetzt sind wir bereit den Beweis zu führen, dass es genügend viele Zeugen (Nicht- nullstellen von Q(x1,...,xn)= P1(x1,...,xn) − P2(x1,...,xn)) der Nichtäquivalenz unterschiedlicher P1 und P2 über Zp für eine genügend große Primzahl p gibt. Satz 8.4. Sei p eine Primzahl, und seien n, d ∈ N −{0}. Sei Q(x1,...,xn) ̸≡ 0 ein Polynom über Zp mit n Variablen x1,...,xn, wobei jede Variable in Q höchstens Grad d hat. Dann hat Q höchstens n · d · pn−1 Nullstellen. Beweis. Wir beweisen den Satz per Induktion bezüglich der Anzahl n der Variablen. (i) Induktionsanfang. Sei n = 1. Nach Satz 8.3 hat Q(x1) höchstens d = n · d · pn−1 (für n = 1) Nullstellen. (ii) Induktionsschritt. Sei die Induktionsannahme gültig für n − 1, n ∈ N −{0}. Wir beweisen sie für n. Wir können Q als Q(x1,x2,...,xn)= Q0(x2,...xn)+ x1 · Q1(x2,...,xn)+ ... + x d 1 · Qd(x2,...,xn) = d∑ i=0 x i 1 · Qi(x2,...,xn) für irgendwelche Polynome Q0(x2,...xn),Q1(x2,...,xn),...,Qd(x2,...,xn) 258 8 Randomisierung ausdrücken. Falls Q(α1,α2,...,αn) mod p = 0 für ein α =(α1,...,αn) ∈ (Zp)n, dann gilt entweder (a) Qi(α2,...,αn)mod p = 0 für alle i =0, 1,...,d, oder (b) es existiert ein j ∈{0, 1,...,d} mit Qj(α2,...,αn) mod p ̸= 0 und α1 ist eine Nullstelle des Polynoms Q(x1)= Q0(α2,...αn)+ x1 · Q1(α2,...,αn)+ ··· + x d 1 · Qd(α2,...,αn) in einer Variablen x1. Wir zählen jetzt getrennt die Anzahl der Nullstellen im Fall (a) und (b). (a) Weil Q(x1,...,xn) ̸≡ 0, existiert eine Zahl k ∈{0, 1,...,d}, so dass Qk(x2,..., xn) ̸≡ 0. Nach der Induktionsannahme ist die Anzahl der Nullstellen von Qk höchstens (n − 1) · d · p n−2. Dann gibt es aber höchstens (n − 1) · d · p n−2 Elemente α =(α2,...,αn) ∈ (Zp)n−1, so dass Qi(α) mod p = 0 für alle i ∈{0, 1, 2,...,d}. Weil der Wert α1 von x1 keinen Einﬂuss auf die Bedingung (a) hat und somit frei wählbar ist, gibt es höchstens p · (n − 1) · d · pn−2 = (n − 1) · d · pn−1 Elemente α =(α1,α2,...,αn) ∈ (Zp)n, die die Eigenschaft (a) haben. (b) Weil Q(x1) ̸≡ 0, hat Q nach Satz 8.3 höchstens d Nullstellen (d. h. höchstens d Werte α1 ∈ Zp mit Q(α1) mod p = 0). Deswegen gibt es höchstens d · p n−1 Werte α =(α1,α2,...,αn) ∈ (Zp) n, die die Bedingung (b) erfüllen. Zusammenfassend hat Q(x1,...,xn) höchstens (n − 1) · d · pn−1 + d · p n−1 = n · d · p n−1 Nullstellen. \u0002 Korollar 8.1. Sei p eine Primzahl, und seien n, d ∈ N −{0}. Für jedes Polynom Q(x1,...,xn) ̸≡ 0 über Zp vom Grad höchstens d gibt es mindestens (1 − n · d p ) · pn Zeugen von Q ̸≡ 0. Beweis. Die Anzahl der Elemente in (Zp) n ist genau pn und nach Satz 8.4 sind höchstens n · d · p n−1 davon keine Zeugen. Somit ist die Anzahl der Zeugen mindestens pn − n · d · p n−1 = (1 − n · d p ) · pn. \u0002 Damit ist die Wahrscheinlichkeit des Ziehens eines Zeugen aus den pn Elementen von (Zp) n mindestens (1 − n · d p ) . 8.6 Zusammenfassung 259 Für p> 2nd ist diese Wahrscheinlichkeit größer als 1 2 . Durch wiederholtes zufälliges Ziehen aus (Zp) n kann man die Wahrscheinlichkeit, dass mindestens ein Zeuge für Q ̸≡ 0 (d. h. für P1(x1,...,xn) ̸≡ P2(x1,...,xn)) gefunden wird, beliebig nahe an 1 bringen. Für mehrere Anwendungen des Algorithmus AQP ist es wichtig, dass die Primzahl p frei wählbar ist. Dieser Freiheitsgrad kommt dadurch zustande, dass man das Äquivalenz- problem für einige Objekte auf den Vergleich von zwei Polynomen reduzieren kann, ohne dabei Bedingungen an den Körper zu stellen, über dem die Polynome verglichen werden sollen. 8.6 Zusammenfassung Einen randomisierten (zufallsgesteuerten) Algorithmus kann man als einen nichtdetermi- nistischen Algorithmus mit zugeordneten Wahrscheinlichkeiten für die Verzweigung der Berechnungen oder als eine Wahrscheinlichkeitsverteilung über deterministischen Algorith- men betrachten. Die Zufallssteuerung ist das inhärente Prinzip von Naturprozessen, das sich durch Einfachheit und Eﬃzienz auszeichnet. Nicht anders ist es in der Algorithmik, wo man durch einfache randomisierte Algorithmen Probleme viel eﬃzienter als mit den besten deterministischen Algorithmen lösen kann. Am Beispiel des Entwurfs eines Kommunikationsprotokolls für den Vergleich der Inhalte von zwei Datenbanken haben wir gesehen, dass Randomisierung exponentiell eﬃzienter als Determinismus sein kann. Die Grundidee des Entwurfs eines eﬃzienten randomisierten Protokolls basierte auf der Methode der häuﬁgen Zeugen. Ein Zeuge ist eine Zusatz- information zur Eingabe, mit deren Hilfe man das Resultat eﬃzient berechnen kann, obwohl kein eﬃzienter Lösungsansatz ohne einen Zeugen möglich (oder bekannt) ist. Für die erfolgreiche Anwendung der Methode der häuﬁgen Zeugen ist es wichtig, dass man für jede Eingabe (Probleminstanz) eine Menge von Zeugenkandidaten bestimmen kann und dass ein großer Anteil der Kandidaten Zeugen sind. Einen Zeugen kann man dann einfach durch (wiederholtes) zufälliges Ziehen aus der Menge der Zeugenkandidaten mit großer Wahrscheinlichkeit gewinnen. Der Grund, warum man durch systematisches deter- ministisches Durchsuchen der Menge der Zeugenkandidaten einen Zeugen nicht eﬃzient ﬁnden kann, liegt darin, dass die Zeugen in der Kandidatenmenge regellos verteilt sind. Bei dieser „chaotischen“ Struktur der Kandidatenmenge riskiert jeder deterministische Suchalgorithmus zu viele Fehlversuche bei der Suche nach einem Zeugen. Die Kunst diese Methode anzuwenden liegt in einer geeigneten Deﬁnition von Zeugen. Wir haben gezeigt, wie man Zeugen deﬁnieren kann, um einen eﬃzienten randomisierten Primzahltest für ungerade Zahlen n mit ungeradem (n−1) 2 zu entwickeln. Für den Primzahltest kann man unsere Deﬁnition von Zeugen so weiter entwickeln, dass man eﬃzient den Primzahltest für alle natürlichen Zahlen durchführen kann. Ein Spezialfall der Methode der häuﬁgen Zeugen ist die Methode der Fingerabdrücke für die Lösung von Äquivalenzproblemen. Die Idee ist, komplexen Objekten mit umfangreicher Darstellung durch eine zufällige Abbildung sogenannte Fingerabdrücke (kurze partielle Darstellungen) zuzuordnen und so den Äquivalenztest auf den eﬃzient durchführbaren Vergleich der Fingerabdrücke zu reduzieren. Die zufällig ausgesuchten Abbildungen spielen in diesem Fall die Rolle der Zeugen. Auf diese Weise kann man einen eﬃzienten 260 8 Randomisierung randomisierten Test für die Äquivalenz von zwei Polynomen entwickeln. Für diesen Äquivalenztest ist kein (deterministischer) polynomieller Algorithmus bekannt, und es gibt mehrere weitere Äquivalenztests, die sich eﬃzient auf den Vergleich von zwei Polynomen reduzieren lassen. Eine ausführliche Übersicht über randomisierte Algorithmen kann man in dem Buch [MR95] von Motwani und Raghavan ﬁnden, das aber für Anfänger aufgrund des technischen Schwierigkeitsgrades eher ungeeignet ist. Eine Einführung in das Gebiet des Entwurfs von randomisierten Algorithmen ist in Kapitel 5 von [Hro04a] gegeben. Eine ausführliche Darstellung der Methoden für den Entwurf zufallsgesteuerter Systeme, die durch viele anschauliche Beispiele transparent gemacht werden, ist in [Hro04b] zu ﬁnden. Mehr über die zufallsgesteuerten Kommunikationsprotokolle kann man in [Hro97] erfahren. Eine anschaulich präsentierte Anwendung des randomisierten Äquivalenztests von zwei Polynomen für den semantischen Vergleich von zwei Datenstrukturen zur Repräsentierung Boole’scher Funktionen ﬁndet man in [Sip97]. Eine eindrucksvolle Übersicht über die Konzepte im Bereich des Entwurfs von randomisierten Algorithmen hat Karp in [Kar91] zusammengestellt. Kontrollaufgaben 1. Um Zufallsexperimente zu modellieren, haben wir den Begriﬀ des Wahrscheinlichkeits- raums eingeführt. Ein Wahrscheinlichkeitsraum ist gegeben durch die Menge S aller möglichen Ergebnisse (Endresultate, elementare Ereignisse) des Experiments und durch die Wahrscheinlichkeitsverteilung über S, die jedem Ergebnis (elementaren Ereignis) sei- ne Wahrscheinlichkeit zuordnet. Die Wahrscheinlichkeit eines elementaren Ereignisses ist der erwartete Anteil der Versuche, die mit diesem elementaren Ereignis enden. Wel- che Konsequenzen hat diese Bedeutung der Wahrscheinlichkeit für die Bestimmung der Wahrscheinlichkeiten beliebiger Ereignisse, falls S endlich (oder abzählbar) ist? 2. Modellieren Sie das Experiment des fünﬀachen Münzwurfs. Wie hoch ist die Wahrschein- lichkeit, dass die Anzahl der gefallenen Köpfe und die Anzahl der gefallenen Zahlen sich höchstens um 1 unterscheidet? 3. Wie modelliert man randomisierte Algorithmen? Was entspricht hier einem Wahrschein- lichkeitsraum? 4. Betrachten Sie folgende Aufgabe. Zwei Rechner haben jeweils ein Wort mit einer Länge von 18 Bits gespeichert und sollen feststellen, ob die Wörter gleich sind. Sie benutzen dazu das zufallsgesteuerte Kommunikationsprotokoll R. Aus wie vielen Primzahlen wird zufällig eine gewählt? Können Sie alle auﬂisten? Wie viele Bits werden kommuniziert und wie groß ist die Fehlerwahrscheinlichkeit? Lohnt es sich hier, das Protokoll R10 anzuwenden? 5. Analysieren Sie die Komplexität und die Fehlerwahrscheinlichkeit einer Modiﬁkation des Protokolls R, in dem R statt aus Prim(n 2) aus der Menge Prim(⌊n 3/2⌋ ) eine Primzahl zufällig wählt. Wie ändert sich die Fehlerwahrscheinlichkeit, wenn man die Primzahl aus Prim(n · (ln n)2) oder aus Prim(100 · n · ln n) wählt? 6. Erklären Sie die Grundidee der Methode der häuﬁgen Zeugen. Wie kann man diese Methode zum Entwurf eines randomisierten Algorithmus für den Primzahltest anwenden? 7. Erklären Sie die Grundidee der Methode der Fingerabdrücke. Wie kann man diese Methode zum Vergleich von zwei Polynomen anwenden? 8.6 Zusammenfassung 261 8. Betrachten wir das randomisierte Protokoll zum Vergleich von zwei n-Bit-Strings a = a1 ...an und b = b1 ...bn, ai,bi ∈{0, 1} für i =1,...,n. Sei p eine Primzahl. Wir betrachten die Polynome Pa(x)= n∑ i=1 aix i−1 und Pb(x)= n∑ i=1 bix i−1 über Zp. Oﬀensichtlich sind a und b genau dann gleich, wenn Pa(x) und Pb(x) identisch sind. Nutzen Sie diese Tatsache und den Algorithmus AQP, um ein neues zufallgesteuertes Protokoll für den Vergleich von a und b zu entwerfen. Wie beeinﬂusst die Wahl von p die Fehlerwahrscheinlichkeit und die Kommunikationskomplexität? Gibt es eine Wahl von p,so dass das neue Protokoll „eﬃzienter“ als R eine kleine Fehlerwahrscheinlichkeit garantiert? Ihre Idee ist echt wahnsinnig. Die Grundfrage ist, ob sie wahnsinnig genug ist, um wahrhaftig sein zu können. N. Bohr 9 Kommunikation und Kryptographie 9.1 Zielsetzung Im 20. Jahrhundert hat sich die Theoretische Informatik überwiegend der Untersuchung der sequentiellen Rechnermodelle gewidmet, die der Vorstellung von Neumanns entsprechen. Was sollten die zukünftigen Kernprobleme der Informatik sein? Die Vernetzung der Rechner konfrontiert den Nutzer nicht mehr nur mit einem einzelnen Rechner, sondern mit einer unübersichtlichen vernetzten Welt vieler asynchroner und unvorhersehbarer Aktivitäten. Das Verständnis des Rechnens in der vernetzten Welt ist derzeit nicht sehr tief und seine Entwicklung wird eine der Hauptaufgaben der Informatik sein. Die Vielfalt der Forschungsfragen, die im Zusammenhang mit verteiltem Rechnen, Kooperation und Kommunikation zwischen Rechnern, Prozessen und Menschen gestellt worden sind, kann man in dieser beschränkten Übersicht gar nicht vorstellen. Die Probleme, bezogen auf den Entwurf und die Analyse eﬃzienter Kommunikationsalgorithmen (Protokolle) und auf den Entwurf leistungsfähiger Netze, sind stark von den zugänglichen Technologien abhängig. Diese Technologien entwickelten sich schnell von klassischen Telefonverbindungen bis hin zu optischen Netzen, und jede Technologie ist an anderen Problemen und Fragestellungen interessiert. Weil wir keine Möglichkeit sehen, eine kurze, verständliche Übersicht über dieses Thema zu geben, beschränken wir uns hier auf ein Beispiel eines Netzentwurfes und konzentrieren uns mehr auf das Gebiet der Kryptographie, das sich mit der sicheren Kommunikation in Netzen beschäftigt. Unsere erste Zielsetzung ist, den Leser mit einigen grundlegenden Konzepten und Ideen aus dem Gebiet der Kryptographie bekannt zu machen. Dabei halten wir auch die gerade Linie der kontinuierlichen Entwicklung der informatikbezogenen Konzepte in diesem Buch ein, weil die wesentlichen Fortschritte der Kryptographie auf den in den vorhergehenden Kapiteln vorgestellten Konzepten der Komplexitätstheorie, Algorithmik und Randomisierung aufbauen. Außerdem ist gerade die Kryptographie das Gebiet, das bis zu einem gewissen Grad kontraintuitive Resultate bereitstellt und dadurch auf eindrucksvolle Weise Möglichkeiten eröﬀnet, von denen die meisten Anwender nicht zu träumen wagten. J. Hromkovič, Theoretische Informatik, DOI 10.1007/978-3-658-06433-4_9, © Springer Fachmedien Wiesbaden 2014 264 9 Kommunikation und Kryptographie Sender Empfänger Klartext KlartextKryptotext Kryptoanalytiker SchlüsselSchlüssel Verschlüsselung Entschlüsselung Abbildung 9.1 Die Organisation dieses Kapitels ist wie folgt gestaltet. Abschnitt 9.2 ist einer informel- len Vorstellung der klassischen Kryptosysteme gewidmet. Abschnitt 9.3 stellt das Konzept der Public-Key-Kryptosysteme vor und illustriert es anhand des RSA-Kryptosystems. In Abschnitt 9.4 wenden wir das Konzept der Public-Key-Kryptosysteme an, um zwei Protokolle für digitale Unterschriften zu entwerfen. In Abschnitt 9.5 lernen wir inter- aktive Protokolle und Zero-Knowledge-Beweissysteme kennen, mit denen man eﬃzient Beweise veriﬁzieren kann, ohne sie zu lesen. Abschnitt 9.6 präsentiert den Entwurf eines Kommunikationsnetzes. 9.2 Klassische Kryptosysteme Kryptologie bezeichnet ursprünglich die Lehre der Geheimsprachen. Innerhalb der Kryp- tologie unterscheidet man zwischen der Kryptographie, der Wissenschaft, Kryptosysteme zu entwickeln, und der Kryptoanalyse, der Kunst, diese zu brechen. Hier beschäftigen wir uns nur mit der Kryptographie. Das betrachtete Szenario ist in Abbildung 9.1 dargestellt. Eine Person, genannt Sender, will eine geheime Nachricht einer anderen Person, ge- nannt Empfänger, zuschicken. Die geheime Nachricht ist in Form eines Textes dargestellt und wir nennen diesen Text Klartext. Um zu verhindern, dass ein Unbefugter, der auf irgendeinem Weg in den Besitz der Nachricht gelangt, den geheimen Inhalt lesen kann, schickt man die Nachricht in einer chiﬀrierten (verschlüsselten) Form. Die Art der Chiﬀrie- rung (Verschlüsselung) ist ein gemeinsames Geheimnis des Senders und des Empfängers, und die Verschlüsselung wird mit Hilfe eines sogenannten Schlüssels durchgeführt. Der verschlüsselte Klartext heißt Kryptotext. Nach dem Empfang wird der Kryptotext entschlüsselt, und das Ergebnis der Entschlüsselung ist der ursprüngliche Klartext. Formal ist ein Kryptosystem ein Tripel (K, A, S), wobei K die Menge aller erlaubten Klartexte, A die Menge aller möglichen Kryptotexte und S die Menge der Schlüssel ist. Oft ist K =Σ m für ein m ∈ N und ein Alphabet Σ, d. h., dass man den zu verschlüsselnden Text in Blöcke der Länge m aufteilt und jeden Block einzeln verschlüsselt. In diesem Fall ist auch A =Γk für ein k ∈ N und ein Alphabet Γ. Jeder Schlüssel α ∈S bestimmt eindeutig eine injektive Funktion Eα von K nach A. Die Verschlüsselung entspricht also der Berechnung von Eα(x) für einen Klartext x ∈K und die Entschlüsselung entspricht der Berechnung von E−1 α (c) für einen Kryptotext c ∈A. Die inverse Funktion E−1 α zu der Funktion Eα bezeichnet man gewöhnlich mit Dα. Die Anforderungen an ein Kryptosystem sind: 9.3 Public-Key-Kryptosysteme und RSA 265 (i) Die Funktionen Eα und Dα sollen eﬃzient berechenbar sein, und (ii) ohne α zu kennen, soll es unmöglich oder zumindest im komplexitätstheoretischen Sinne schwer sein, aus einem gegebenen Kryptotext Eα(x) den Klartext x zu berechnen. Wahrscheinlich ist CAESAR das einfachste Kryptosystem. Seien die Klartexte Wörter über dem lateinischen Alphabet mit 26 Buchstaben. Die Menge der Schlüssel S ist {0, 1, 2,..., 25}. Für einen gegebenen Schlüssel k ∈S ersetzt man bei der Verschlüsselung jeden Buchstaben des Klartextes durch einen Buchstaben, der in der alphabetischen Ordnung um k Positionen weiter hinten liegt. Am Ende des Alphabets geht man zyklisch wieder zum Anfang. So erhält man für k = 3 und den Klartext KRYPTOGRAPHIEISTFASZINIEREND den Kryptotext NUBSWRJUDSKLHLVWIDVCLQLHUHQG. Dieses Kryptosystem ist aber leicht zu brechen. Wenn man weiß, dass es sich um CAESAR handelt, reicht es aus, alle 26 möglichen Schlüssel durchzuprobieren. Schwerer kann man es dem Kryptoanalytiker machen, indem man Schlüssel aus {0, 1,..., 25} m für ein m ∈ N −{0} betrachtet. Für einen solchen Schlüssel α = α1,α2,...,αm zerteilt man den Klartext in Blöcke der Länge m und ersetzt den i-ten Buchstaben jedes Blocks durch den Buchstaben, der in der alphabetischen Ordnung um αi Positionen weiter hinten liegt. Falls α =3, 1, 6, dann erhält man für den Klartext KRYPTOGRAPHIE 3163163163163 den Kryptotext NSESUUJSGSIOH. Dieses Kryptosystem kann man auch brechen, wenn man zum Beispiel die Kenntnis der Häuﬁgkeiten des Auftretens der Buchstaben in konkreten natürlichen Sprachen einsetzt. Es existieren aber auch klassische Kryptosysteme, die sehr schnell arbeiten und die man mit dem heutigen Wissen nicht brechen kann. Der Nachteil der klassischen Kryptosysteme liegt darin, dass sie auf einem gemeinsamen Geheimnis von Sender und Empfänger basieren. Die Kenntnis des Verschlüsselungsmechanismus impliziert direkt auch die Kenntnis des Entschlüsselungsmechanismus (häuﬁg sind die Schlüssel für beide Verfahren gleich). Das bedeutet, dass der Sender und der Empfänger sich zuerst auf einen festen Schlüssel einigen müssen, ohne über ein Kryptosystem zur sicheren Kommunikation zu verfügen. Wie man dieses Problem lösen kann, ist das Thema des nächsten Abschnitts. 9.3 Public-Key-Kryptosysteme und RSA Die in Abschnitt 9.2 vorgestellten klassischen Kryptosysteme nennt man auch symmetri- sche Kryptosysteme, weil man mit Kenntnis des Verschlüsselungsverfahrens (Entschlüsse- lungsverfahrens) einfach die Entschlüsselungsprozedur (Verschlüsselungsprozedur) ableiten 266 9 Kommunikation und Kryptographie kann. Damit sind Sender und Empfänger gleichwertig und teilen sich den Schlüssel als ein gemeinsames Geheimnis. Außer dem bereits erwähnten Problem der Realisierung einer sicheren Einigung auf einen gemeinsamen Schlüssel hat das System noch eine andere Schwäche. Wenn man ein Kommunikationssystem hat, in dem sich mehrere Teilnehmer den gleichen Schlüssel teilen, reicht ein Verräter aus, und die Sicherheit der gesamten Kommunikation bricht zusammen. Eine revolutionäre Idee zur Überwindung dieses Pro- blems basiert auf einer komplexitätstheoretischen Überlegung. Für die Verschlüsselung sucht man eine sogenannte Einweg-Funktion f , die folgende Eigenschaften haben soll: (i) f ist eﬃzient berechenbar, (ii) f −1 ist nicht eﬃzient berechenbar und (iii) f −1 ist eﬃzient berechenbar, falls man eine geheime Information kennt (in Analogie zu Zeugen und Zertiﬁkaten in Kapitel 6 und 8). Wenn man eine solche Einweg-Funktion f hätte, könnte der Empfänger f veröﬀentlichen, und die Sender könnten f zur Verschlüsselung der Nachrichten benutzen. Trotz der Veröﬀentlichung von f (zum Beispiel in einem Telefonbuch) sichert uns die Eigenschaft (ii), dass niemand die Kryptotexte entschlüsseln kann. Nur der Empfänger, der als einziger eine geheime zusätzliche Information über f besitzt, kann die Kryptotexte entschlüsseln. Kryptosysteme, die auf diesem Prinzip basieren, nennt man Public-Key- Kryptosysteme (Kryptosysteme mit einem öﬀentlichen Schlüssel). Die Frage ist, ob solche Einweg-Funktionen überhaupt existieren. Man könnte sogar meinen, dass die Antwort nein sein sollte, weil die drei Eigenschaften (i), (ii) und (iii) für eine Funktion f zusammen unnatürlich aussehen. Dass die Idee doch nicht so abwegig ist, zeigt das folgende Beispiel. Betrachten wir die folgende Verschlüsselung. Jeder Buchstabe wird einzeln durch eine Folge von 14 Dezimalziﬀern verschlüsselt. Für jeden Buchstaben wählt man nichtde- terministisch aus irgendeinem Telefonbuch einen Namen, der mit diesem Buchstaben anfängt, und die entsprechende Telefonnummer nimmt man in den Kryptotext auf. Falls die Nummer weniger als 14 Ziﬀern hat, setzt man an den Anfang entsprechend viele Nullen. Tabelle 9.1 zeigt, wie man auf diese Weise das Wort Kryptographie verschlüsseln kann. Vorausgesetzt, dass alle außer dem Empfänger nur klassische Telefonbücher haben, die nach dem Namen sortiert sind, ist es eine sehr aufwendige Aufgabe, die Telefonnummer in dem Telefonbuch zu ﬁnden, um den der Nummer zugehörigen Namen zu erfahren. 1 Nur der Empfänger, der ein nach Telefonnummern sortiertes Welttelefonbuch besitzt, kann eﬃzient den Kryptotext entschlüsseln. Das vorgestellte Beispiel ist nur eine Ideenillustration und wir wollen es nicht ernsthaft als ein Kryptosystem in Betracht ziehen. Bevor wir aber einen ernsthaften Entwurf eines Kryptosystems mit öﬀentlichen Schlüsseln vorstellen, geben wir eine formale Deﬁnition einer Einweg-Funktion an. 1Man könnte auch auf die Idee kommen, die Telefonnummer einfach anzurufen. Abgesehen von den Kosten für die Anrufe kann man sich auch schnell in einem Experiment davon überzeugen, dass dieser Weg keinen einfachen Erfolg garantiert. 9.3 Public-Key-Kryptosysteme und RSA 267 Tabelle 9.1 Name Telefonnummer K Knuth 00128143752946 R Rivest 00173411020745 Y Yao 00127345912233 P Papadimitriou 00372453008122 T Thomas 00492417738429 O Ogden 00012739226541 G Garey 00012228915020 R Rabin 00048327450028 A Adleman 00173555248001 P Papadimitriou 00372453008122 H Hopcroft 00013782442358 I Ibarra 00124327010098 E Edmonds 00183274553211 Deﬁnition 9.1. Seien Σ und Γ zwei Alphabete. Eine Funktion f : Σ ∗ → Γ ∗ heißt Einweg- Funktion, falls sie folgende Eigenschaften hat. (i) Es existieren Konstanten c und d aus N −{0}, so dass 1 c ·|x|≤|f (x)|≤ d ·|x| für alle x ∈ Σ∗ gilt. {Das bedeutet nur, dass |x| und |f (x)| in linearer Relation stehen.} (ii) Die Funktion f kann man in polynomieller Zeit berechnen. (iii) Für jeden randomisierten polynomiellen Algorithmus A und jedes k ∈ N −{0} existiert eine Konstante nA,k, so dass für jedes n ≥ nA,k und ein zufällig ausgesuchtes w ∈ Σn die Wahrscheinlichkeit, 2 dass A(f (w)) = w, kleiner als n −k ist. {Diese Bedingung garantiert, dass polynomiell viele unabhängig wiederholte Läufe eines randomisierten polynomiellen Algorithmus nicht zur Berechnung von f −1 mit konstanter Fehlerwahrscheinlichkeit führen können.} Bisher hat man von keiner Funktion bewiesen, dass sie eine Einweg-Funktion ist. Dies hängt hauptsächlich mit dem Schwierigkeitsgrad von Beweisen unterer Schranken in der Komplexitätstheorie zusammen. Trotzdem kennt man einige plausible Kandidaten. Ein Kandidat ist das Potenzieren modulo einer natürlichen Zahl n, das man, wie in Kapitel 8 gezeigt wurde, eﬃzient berechnen kann. Die inverse Funktion ist der diskrete Logarithmus (das Auﬂösen der Gleichung a x mod n = b nach der Unbekannten x), für deren Berechnung kein eﬃzienter (randomisierter) Algorithmus bekannt ist. Ein anderer Kandidat für eine Einweg-Funktion ist die Multiplikation von zwei Zahlen, die oﬀensichtlich eﬃzient berechenbar ist. Die inverse Funktion ist die Faktorisierung, für die kein eﬃzienter Algorithmus bekannt ist. Auf der Schwierigkeit der Faktorisierung 2Die Wahrscheinlichkeit betrachtet man über zufällige Entscheidungen von A sowie über die zufällige Wahl von w. 268 9 Kommunikation und Kryptographie basiert das folgende Public-Key-Kryptosystem RSA, das nach seinen Erﬁndern Rivest, Shamir und Adleman benannt wurde. Sei ggT(a, b) für zwei Zahlen a, b ∈ N der größte gemeinsame Teiler von a und b. Der Empfänger bestimmt die Verschlüsselung und die Entschlüsselung durch folgende Berechnung. Er generiert zufällig zwei große Primzahlen 3 p und q und berechnet n = p · q und ϕ(n)=(p − 1) · (q − 1), wobei ϕ die sogenannte Euler’sche Funktion ist. Für jede positive natürliche Zahl n ist ϕ(n) die Anzahl der Zahlen a ∈ N −{0}, so dass ggT(a, n)=1 und a<n. Danach wählt er ein großes d> 1, so dass ggT(d, ϕ(n)) = 1 (9.1) gilt und berechnet die Zahl e,1 <e<ϕ(n), so dass e · d mod ϕ(n)=1. (9.2) Die Zahlen n und e bilden den öﬀentlichen Schlüssel und die Zahlen p, q, ϕ(n) und d bleiben das Geheimnis des Empfängers. Bei der Chiﬀrierung betrachten wir den Klartext als eine Dezimalzahl, die kleiner als n ist. Wenn der Klartext länger ist oder anders dargestellt wird, wandelt man ihn zuerst injektiv in eine Folge von Dezimalzahlen der Länge ⌈log10 n⌉− 1 um und dann verschlüsselt man die Dezimalzahlen der Länge ⌈log10 n⌉− 1 einzeln. Für eine Zahl w ∈{0, 1,...,n − 1} ist die Verschlüsselungsfunktion Ee,n(w)= we mod n. Für einen Kryptotext c ist die Entschlüsselungsfunktion Dd,n(c)= c d mod n. Wie wir schon in Kapitel 8 gezeigt haben, sind Ee,n und Dd,n durch die Methode des wiederholten Quadrierens eﬃzient berechenbar. Dank des randomisierten Primzahltests aus Abschnitt 8.4 kann man auch zufällige Primzahlen eﬃzient generieren. Die Zahlen d und e kann man mit Hilfe des Euklidischen Algorithmus ebenfalls eﬃzient berechnen4 und somit ist die gesamte Schlüsselberechnung des Empfängers eﬃzient durchführbar. Man kennt keinen eﬃzienten (randomisierten) Algorithmus, der aus der Kenntnis von e und n eine der Zahlen d, p, q oder ϕ(n) berechnen kann. Die Kenntnis einer dieser vier geheimen Zahlen reicht allerdings aus, um alle restlichen eﬃzient zu berechnen. Genauso kennt man keinen eﬃzienten Algorithmus, der aus Ee,n(x),e und n (ohne d zu kennen) den Klartext x bestimmen kann. Jetzt wollen wir zeigen, dass RSA wirklich funktioniert, d. h., dass Dd,n(Ee,n(w)) = w für alle w< n. Dazu benutzen wir den Euler’schen Satz, der eine Verallgemeinerung des Fermat’schen Satzes ist. 3Groß bedeutet mehrere hundert Bits lang. 4Genauer: d wird als eine große Zahl zufällig gewählt. Dann wird getestet, ob d teilerfremd mit ϕ(n) ist. Falls nicht, wählt man ein neues d. Da es hinreichend viele zu ϕ(n) teilerfremde Zahlen gibt, wird d schnell gefunden. Die Zahl e wird automatisch durch den Euklidischen Algorithmus zum Testen von ggT(d, ϕ(n)) = 1 mitbestimmt. 9.3 Public-Key-Kryptosysteme und RSA 269 Satz 9.1 (Satz von Euler). Seien w und n zwei positive natürliche Zahlen, für die ggT(w, n)=1 gilt, und sei ϕ(n) die Euler’sche Funktion von n. Dann gilt wϕ(n) mod n =1. Der Euler’sche Satz ist eine Folgerung aus Resultaten der Gruppentheorie, die besagen, dass die Ordnung jedes Elementes die Ordnung der Gruppe teilt, und dass die zyklische Gruppe (Z/nZ) ∗ modulo n die Ordnung ϕ(n) hat. Nach der Deﬁnition der Ordnung von Elementen einer Gruppe gilt für jedes w ∈ (Z/nZ)∗ mit Ordnung k wk mod n =1. Weil ϕ(n)= k · b für eine positive natürliche Zahl b, erhalten wir wϕ(n) mod n = wk·b mod n =(wk mod n)b mod n = (1) b mod n =1. Aufgabe 9.1. Führen Sie einen alternativen Beweis des Euler’schen Satzes, indem Sie den Beweis des Fermat’schen Satzes aus Abschnitt 8.4 verallgemeinern. Das heißt, beweisen Sie Folgendes. Seien x1,x2,...,xϕ(n) ∈{1, 2,...,n − 1} alle Zahlen b mit der Eigenschaft ggT(b, n)=1. Dann ist für jedes a ∈{1, 2,...,n − 1} (ax1 mod n, ax2 mod n,...,axϕ(n) mod n) eine Permutation von x1,x2,...,xϕ(n). Jetzt sind wir bereit, die Korrektheit von RSA zu beweisen. Satz 9.2. Seien p, q, n, e und d wie in RSA bestimmt. Dann gilt für alle natürlichen Zahlen w kleiner als n Dd,n(Ee,n(w)) = wed mod n = w. Beweis. Nach der Wahl von d und e bezüglich (9.1) und (9.2) gilt e · d = j · ϕ(n) + 1 (9.3) für ein j ∈ N −{0}. Wir müssen also beweisen, dass wj·ϕ(n)+1 mod n = w (9.4) für alle w< n. Wir unterscheiden drei Möglichkeiten bezüglich der Relation zwischen p, q und w. (i) Keine der Primzahlen p und q teilt w. Wenn p und q die Zahl w nicht teilen und w< p · q, dann ggT(p · q, w)=1 270 9 Kommunikation und Kryptographie und damit sind die Voraussetzungen des Euler’schen Satzes für n = p · q und w erfüllt. Daher ist wϕ(n) mod n =1 und wjϕ(n) mod n =1. (9.5) Wenn man beide Seiten von (9.5) mit w multipliziert, erhält man die angestrebte Gleichung (9.4). (ii) Eine der Primzahlen p und q teilt w, und die andere teilt w nicht. Ohne Beschränkung der Allgemeinheit setzen wir voraus, dass p die Zahl w teilt und dann q die Zahl w nicht teilt. Aus dem Satz von Fermat folgt wq−1 mod q =1, was w(q−1)(p−1) mod q = 1 impliziert, d. h., wϕ(n) mod q =1 und somit wjϕ(n) mod q =1. (9.6) Weil p die Zahl w teilt, gilt (9.6) auch modulo n = p · q, d. h., wjϕ(n) mod n =1. Durch Multiplizieren mit w erreicht man auch hier das gewünschte Ergebnis. (iii) Beide Zahlen p und q teilen w. Diese Möglichkeit kann nicht auftreten, weil p und q Primzahlen sind und p · q> w gilt. \u0002 Die Public-Key-Kryptosysteme haben viele Vorteile gegenüber den symmetrischen Kryptosystemen, weil sie uns die Erstellung sicherer Kommunikationsprotokolle für un- terschiedliche Aufgaben ermöglichen (wie zum Beispiel digitale Unterschriften), die man mit symmetrischen Kryptosystemen nicht lösen könnte. Die klassischen symmetrischen Kryptosysteme haben aber auch einen wesentlichen Vorteil gegenüber den Public-Key- Kryptosystemen. Insbesondere auch aufgrund der Hardwarerealisierung sind sie in der Praxis oft 100-mal schneller als Public-Key-Verfahren. Das führt meistens dazu, dass man ein Public-Key-Kryptosystem nur zum Austausch eines Schlüssels für ein symme- trisches Kryptosystem verwendet und den Rest der Kommunikation mit symmetrischen Kryptosystemen realisiert. 9.4 Digitale Unterschriften Um eine einfache Anwendung der Public-Key-Kryptosysteme zu zeigen, stellen wir zwei einfache Protokolle für digitale (elektronische) Unterschriften vor. Handschriftliche Unter- schriften sind juristisch gesehen eine Art Echtheitsgarantie. In der digitalen Kommunika- tion kann man aber keine handschriftlichen Unterschriften leisten. Außerdem hätte man gerne noch fälschungssicherere Unterschriften, als es die handschriftlichen sind. 9.4 Digitale Unterschriften 271 Betrachten wir folgendes Szenario. Ein Kunde K will der Bank B eine Echtheitsgarantie für eine Überweisung von seinem Konto geben, oder ein anderes Dokument für die Bank unterschreiben. Wir stellen folgende natürliche Forderungen an Kommunikationsprotokolle für solche digitalen Unterschriften. (i) B muss von der Echtheit der Unterschrift von K überzeugt werden. Sowohl B als auch K müssen vor einem Dritten (Fälscher) F geschützt werden, der sich als K gegenüber B ausgeben möchte. (ii) K sollte vor solchen Aktivitäten von B geschützt werden, bei denen B behauptet, ein unterschriebenes Dokument von K zu haben, obwohl K dieses Dokument nicht unterschrieben hat (d. h., B darf es nicht erlernen können, die Unterschrift von K zu fälschen). Aufgabe 9.2. Entwerfen Sie ein Kommunikationsprotokoll für digitale Unterschriften, dass auf einem klassischen Kryptosystem basiert und die Erfüllung der Forderung (i) garantiert. Eigenschaft (ii) ist schwerer zu erfüllen als (i), weil sie auf den ersten Blick kontraintuitiv aussieht. Einerseits soll B von der Echtheit der Unterschrift von K überzeugt werden, und für die Überprüfung muss er etwas über die Generierung (Erzeugung) der Unterschrift wissen. Andererseits darf B nicht zu viel über die Art, wie K unterschreibt, wissen, weil sonst B die Unterschrift von K nachmachen könnte. Die folgende einfache Lösung bietet das Konzept der Public-Key-Kryptosysteme. Der Kunde K hat ein Public-Key-Kryptosystem mit der Verschlüsselungsfunktion EK und der Entschlüsselungsfunktion DK,wobei DK(EK(w)) = EK(DK(w)) für jedes w gilt. Diese Bedingung wird zum Beispiel vom RSA-Kryptosystem erfüllt. Die Bank B kennt die öﬀentliche Verschlüsselungsfunktion EK. Dann kann K wie folgt eine Unterschrift leisten. K nimmt das Dokument w und schickt (w, DK(w)) an B. B überprüft dann mit dem Test „w = EK(DK(w))“ die Echtheit der Unterschrift. Weil kein anderer außer K die Nachricht DK(w) berechnen kann, ist B von der Echtheit der Unterschrift (w, DK(w)) überzeugt. Damit ist die Forderung (i) erfüllt. Weil der Schlüssel EK öﬀentlich ist, hat auch B noch zusätzlich die Möglichkeit, jeden Dritten5 mit dem Paar (w, Dk(w)) überzeugen zu können, dass K das Dokument w unterschrieben hat. Die Forderung (ii) ist auch erfüllt, weil die Kenntnis von (w, Dk(w)) B nicht helfen kann, ein anderes Dokument u mit DK(u) zu unterschreiben. Aufgabe 9.3. Das vorgestellte Protokoll hält das Dokument w nicht geheim. Jeder, der lauscht, kann w erfahren. Stellen wir also die folgende zusätzliche Bedingung. (iii) Kein Dritter, der die Kommunikation zwischen B und K belauscht, darf den Inhalt des unterschriebenen Dokumentes erfahren. Entwerfen Sie ein Kommunikationsprotokoll, das alle drei Bedingungen (i), (ii) und (iii) erfüllt. Betrachten wir jetzt eine kompliziertere Version digitaler Unterschriften, die man das Authentizitätsproblem nennt. Hier braucht man kein Dokument zu unterschreiben, son- dern man muss nur den anderen von seiner eigenen Identität überzeugen. Die Forderungen an ein Kommunikationsprotokoll für die Authentiﬁzierung sind wie folgt: 5der EK kennt und mit Sicherheit weiß, dass es der öﬀentliche Schlüssel von K ist 272 9 Kommunikation und Kryptographie (i ′) genau wie (i), (ii ′) K sollte von solchen Aktivitäten von B geschützt werden, bei denen B sich gegenüber einem Dritten als K ausgeben möchte. 6 Unser Kommunikationsprotokoll ist nicht für das Authentizitätsproblem geeignet. B lernt die Unterschrift (w, DK(w)) in dieser elektronischen Kommunikation und kann sich in der Kommunikation mit einem Dritten mit dieser Unterschrift als K ausgeben. Es gibt Situationen, in denen so etwas sehr unerwünscht ist. Außerdem kann irgendjemand der Kommunikation lauschen und dabei auch das Paar (w, DK(w)) lernen. Er kann dann ebenfalls mit der öﬀentlichen Verschlüsselungsfunktion EK die Echtheit der Unterschrift überprüfen und sich dann als K ausgeben. Die Lösung des Authentizitätsproblems ﬁndet man in der Einbeziehung zweier Public- Key-Kryptosysteme. K besitzt das bereits erwähnte Kryptosystem (DK,EK) und B hat ein anderes Public-Key-Kryptosystem (DB,EB). Die Verschlüsselungsfunktionen EK und EB sind öﬀentlich, und damit auch sowohl K als auch B bekannt. Die Funktion DK ist das Geheimnis von K und die Funktion DB ist das Geheimnis von B. K leistet seine Unterschrift entsprechend dem folgenden Protokoll. • B wählt zufällig eine Zahl w und schickt EK(w)an K. • K berechnet w durch DK(EK(w)). K berechnet c = EB(DK(w)) und schickt es an B. • B überprüft, ob w = EK(DB(c)) = EK(DB(EB(DK(w)))). Es ist oﬀensichtlich, dass B von der Echtheit der Unterschrift von K überzeugt ist. K ist der Einzige, der DK kennt und somit der Einzige, der aus EK(w) die Zahl w bestimmen kann, und so die Nachricht DK(w) und somit auch EB(DK(w)) erzeugen kann. Die Nachricht EK(w) kann niemand außer K entschlüsseln und EB(DK(w)) kann niemand außer B entschlüsseln. Aus diesem Grund kann kein Lauscher F die Unterschrift (w, EB(DK(w))) lernen und deren Echtheit überprüfen. Damit ist Bedingung (i) erfüllt. B lernt in dieser Kommunikation die Unterschrift (w, EB(DK(w))). Durch mehrere Aufträge von K kann B mehrere solche Paare kennenlernen. Das hilft ihm aber nicht dabei, sich als K auszugeben. Wenn man in dem ganzen Netz unter diesem Protokoll arbeitet, wird jeder Dritte C zuerst EK(u)an B, der sich als K ausgibt, schicken. Weil B kein DK besitzt, kann er u nicht bestimmen. B kann höchstens für alle gespeicherten Paare (w, EB(DK(w))) den Wert EK(w) berechnen und mit EK(u) vergleichen. Falls EK(w)= EK(u)ist,hat B Glück gehabt und kann sich als K ausgeben. Weil aber u zufällig als eine Zahl von mehreren hundert Dezimalziﬀern gewählt wurde, ist die Wahrscheinlichkeit des Erfolges von B auch bei einer großen Unterschriftensammlung kleiner als 1 durch die Anzahl der Protonen im bekannten Universum. Diese Wahrscheinlichkeit kann man noch geringer halten, indem man regelmäßig die Schlüssel ändert. 7 6Also ist (ii′) ähnlich zu (ii), weil es eine Forderung gegen potentielle Fälschung der Unterschrift von K durch B ist. 7Es gibt auch bessere Protokolle für digitale Unterschriften, deren Vorstellung aber den Rahmen dieses Lehrbuches sprengen würde. Die weiterführende Literatur ist in Abschnitt 9.7 angegeben. 9.5 Interaktive Beweissysteme und Zero-Knowledge-Beweise 273 Aufgabe 9.4. Betrachten Sie das Authentizitätsproblem, bei dem man eine Unterschrift leistet, um sich als Person K auszugeben, und nicht um ein Dokument zu unterschreiben. In diesem Fall arbeitet das vorgestellte Protokoll mit großer Wahrscheinlichkeit zuverlässig. Bewirken Sie durch eine kleine Änderung dieses Protokolls, dass die Wahrscheinlichkeit, dass sich B als K gegenüber einem Dritten ausgeben kann, gleich 0 ist. Dies muss unabhängig davon gelten, wie groß die Liste der von B gesammelten Unterschriften von K ist. 9.5 Interaktive Beweissysteme und Zero-Knowledge-Beweise In Kapitel 6 haben wir gelernt, dass jede Sprache aus NP einen polynomiellen Veriﬁzierer hat. Dies bedeutet, dass alle Aussagen x ∈ L für ein L ∈ NP einen Beweis polynomieller Länge in |x| haben, und dass dieser Beweis in polynomieller Zeit veriﬁziert werden kann, d. h., dass bezüglich der Veriﬁkation die Sprachen aus NP leicht sind. In Kapitel 8 sind wir zu der Einsicht gelangt, dass man die praktische Lösbarkeit mit randomisierten polynomiellen Berechnungen statt mit deterministischen polynomiellen Berechnungen verknüpfen sollte. So entsteht die Frage, für welche Sprachen (Entscheidungsprobleme) man Beweise „praktisch“ veriﬁzieren kann. Um diese Fragestellung zu untersuchen, betrachten wir folgendes Kommunikationsprotokoll. Wir haben zwei Teilnehmer – den Beweiser und den Veriﬁzierer. Der Beweiser ist ein Algorithmus (eine TM), der keiner Komplexitätseinschränkung unterliegt. Der Veriﬁzierer ist ein randomisierter polynomieller Algorithmus (eine randomisierte polynomielle TM). Für eine Sprache L erhalten beide Teilnehmer die gleiche Eingabe x. Der Veriﬁzierer und der Beweiser dürfen kommunizieren, indem sie Nachrichten (Wörter) polynomieller Länge in |x| austauschen. Der Beweiser mit unbeschränkter Berechnungsstärke will den Veriﬁzierer von der Tatsache „x ∈ L“ überzeugen, und zu diesem Zweck darf er auch lügen (falsche Aussagen produzieren). Die Aufgabe des Veriﬁzierers ist es, Fragen an den Beweiser zu stellen, so dass er mit hoher Wahrscheinlichkeit bestimmen kann, ob der Beweiser einen Beweis von „x ∈ L“ hat oder nicht. Die Anzahl der Kommunikationsrunden ist höchstens polynomiell in |x| und die gesamte Berechnung des Veriﬁzierers läuft in polynomieller Zeit. Der Veriﬁzierer muss die Kommunikation mit der Entscheidung x ∈ L oder x/∈ L beenden. Dieses Kommunikationsprotokoll zwischen Beweiser und Veriﬁzierer nennt man ein interaktives Beweissystem. Deﬁnition 9.2. Sei L ⊆ Σ∗ für ein Alphabet Σ. Wir sagen, dass L ein interaktives Beweissystem besitzt, falls ein Veriﬁzierer (randomisierter polynomieller Algorithmus) V existiert, so dass für alle x ∈ Σ∗ gilt: (i) Falls x ∈ L, dann existiert ein Beweiser B, so dass V nach der Kommunikation mit B das Wort x mit einer Wahrscheinlichkeit größer als 2 3 akzeptiert, und {Wenn x ∈ L, existiert ein Beweiser, der einen Beweis von „x ∈ L“ hat, und diesen Beweis kann der Veriﬁzierer eﬃzient überprüfen.} (ii) falls x/∈ L, dann endet für jeden Beweiser B die Kommunikation zwischen V und B mit dem Verwerfen von x mit einer Wahrscheinlichkeit größer als 2 3 . {Wenn x/∈ L, gibt es keinen Beweis von „x ∈ L“ und so muss jede Strategie, den Veriﬁzierer vom ungültigen „x ∈ L“ zu überzeugen, mit hoher Wahrscheinlichkeit entdeckt werden.} 274 9 Kommunikation und Kryptographie Wir deﬁnieren die Klasse IP durch IP = {L | L besitzt ein interaktives Beweissystem}. Die Beschränkung der Fehlerwahrscheinlichkeit in Deﬁnition 9.2 ist nicht entscheidend. Nach O(|x|) unabhängigen Wiederholungen der Kommunikation eines interaktiven Be- weissystems kann man die Fehlerwahrscheinlichkeit unter 2 −|x| drücken, und somit ändert die Forderung der richtigen Ausgabe mit einer Wahrscheinlichkeit von mindestens 1−2 −|x| nichts an der Deﬁnition der Klasse IP. Das folgende Resultat haben wir schon vorhergesehen. Lemma 9.1. NP ⊆ IP. Beweis. Weil NP = VP gilt, existiert für jede Sprache L ∈ NP und jedes x ∈ L ein Zeuge (Beweis, Zertiﬁkat) c von „x ∈ L“ mit einer in |x| polynomiellen Länge. Ein Beweiser, der c besitzt, schickt einfach c dem Veriﬁzierer, der in polynomieller Zeit deterministisch mit Sicherheit die Tatsache „x ∈ L“ überprüft. Wenn x/∈ L, existiert kein Beweis von „x ∈ L“ und so gibt es keine Möglichkeit, den deterministischen Veriﬁzierer in polynomieller Zeit in |x| von „x ∈ L“ zu überzeugen. \u0002 Die Frage ist jetzt, ob es auch interaktive Beweissysteme für Sprachen gibt, die außerhalb von NP liegen. Wir betrachten folgendes Problem. Gegeben sind zwei Graphen G1 und G2, und man soll entscheiden, ob G1 und G2 isomorph8 sind. Das Problem der Graphenisomorphie ist in NP, weil man den Isomorphismus nichtdeterministisch erraten und dann eﬃzient deterministisch veriﬁzieren kann. Von dem komplementären Problem weiß man nicht, ob es in NP liegt. Die Vermutung ist eher negativ, weil man nicht glaubt, dass das nichtdeterministische Raten für die Überprüfung der Nichtexistenz eines Isomorphismus zwischen zwei gegebenen Graphen hilfreich sein könnte. Sei NICHTISO = {(G1,G2) | G1 und G2 sind nicht isomorph}. Wir beschreiben jetzt ein interaktives Beweissystem für NICHTISO.Sei (G1, G2) eine Eingabe hierfür. Der Veriﬁzierer V überprüft zuerst, ob G1 und G2 die gleiche Anzahl Kanten und Knoten haben. Falls nicht, akzeptiert V die Eingabe (G1,G2) mit Sicherheit. Wenn G1 und G2 die gleiche Anzahl Kanten und Knoten haben, wählt V zufällig ein i ∈{1, 2} und eine Permutation (j1,j2,...,jn) von (1, 2,...,n), wobei n die Anzahl der Knoten ist, beide mit uniformer Wahrscheinlichkeitsverteilung. Dann nimmt V den Graphen Gi und permutiert seine Knoten entsprechend (j1,j2,...,jn). Den resultierenden Graphen Gi(j1,j2,...,jn) schickt V zu dem Beweiser B mit der Auﬀorderung zu deklarieren, ob Gi(j1,j2,...,jn) isomorph zu G1 oder zu G2 ist. Wenn G1 und G2 nicht isomorph sind, kann B erkennen (berechnen), dass Gi isomorph zu Gi(j1,j2,...,jn) ist und schickt die richtige Antwort i zu V . Falls aber G1 und G2 isomorph sind, hat B keine Möglichkeit auszurechnen, welche zufällige Zahl i der Veriﬁzierer gezogen hat. Dem Beweiser bleibt nicht anderes übrig, als ein j ∈{1, 2} zu raten und an V zu 8Zwei Graphen sind isomorph, wenn man die Knoten des einen Graphen so umbenennen kann, dass man den anderen Graphen erhält. 9.5 Interaktive Beweissysteme und Zero-Knowledge-Beweise 275 schicken. Die Wahrscheinlichkeit, dass j = i gilt, ist 1 2 . Falls j ̸= i, verwirft V die Eingabe. Falls j = i, wiederholt V die Kommunikation mit einem neuen zufällig gewählten i und mit einer neuen zufällig gewählten Permutation. Wenn danach der Beweiser wieder die richtige Antwort gibt, dann akzeptiert V die Eingabe (G1,G2). Wenn die zweite Antwort falsch ist, dann verwirft V die Eingabe. Wir zeigen jetzt, dass das beschriebene Protokoll ein interaktives Beweissystem für NICHTISO ist. Falls (G1,G2) ∈ NICHTISO, existiert ein Beweiser, der zwischen G1 und G2 unterscheiden kann und deswegen gibt er immer die richtige Antwort. Daher akzeptiert V die Eingabe (G1,G2) mit Sicherheit. Falls (G1,G2) /∈ NICHTISO, kann kein Beweiser zwischen G1 und G2 bezüglich Isomorphie unterscheiden. Die Wahrscheinlichkeit, dass der Beweiser trotzdem zweimal die richtige Antwort rät, ist 1 2 · 1 2 = 1 4 . Damit ist die Fehlerwahrscheinlichkeit in diesem Fall 1 4 . Wenn der Veriﬁzierer k Anfragen an B stellen würde, würde sich die Fehlerwahrscheinlichkeit auf 2 −k reduzieren. Wie stark das randomisierte Veriﬁzieren wirklich ist, zeigt das folgende Resultat. Satz 9.3∗ (Satz von Shamir). IP = PSPACE. Das Bemerkenswerte an diesem Resultat ist, dass die Aussagen „x ∈ L“ für die Sprachen L ∈ PSPACE exponentiell lange Beweise haben dürfen, die man wegen ihrer Länge nicht vollständig übertragen und lesen kann, und trotzdem kann man deren Existenz in polynomieller Zeit randomisiert überprüfen. 9 Als letztes möchten wir die Zero-Knowledge-Beweissysteme betrachten, die kryptogra- phische Anwendungen haben. Hier verzichten wir auf die formale Deﬁnition. Die Idee besteht in der zusätzlichen Forderung an das Beweissystem, dass der Veriﬁzierer in der Kommunikation mit dem Beweiser nichts (kein einziges Bit) lernt außer dem, was er sich auch alleine ohne Kommunikation mit dem Beweiser ausrechnen kann. 10 Dies bedeutet unter anderem, dass V kein Bit des Beweises, in dessen Besitz sich der Beweiser beﬁndet, erfährt. Zero-Knowledge-Beweissysteme sind von großer praktischer Bedeutung. Eine mögliche Anwendung liegt zum Beispiel in folgender Situation. Wir wollen eine Zugangskontrolle für einen Rechner konstruieren, die nur die Benutzung mit gültigem Passwort zulässt, aber dabei die Anonymität des Nutzers nicht verletzt. Dies bedeutet, dass der Benutzer in die Rolle des Beweisers schlüpft und versucht, die Zugangskontrolle (den Veriﬁzierer) davon zu überzeugen, dass er im Besitz eines Passwortes ist, ohne ein einziges Bit dieses Passwortes zu verraten. Eine andere Anwendung für Zero-Knowledge-Beweissysteme ist die Verwendung privater Informationen einer anderen Person zur Berechnung, ohne die Daten dabei zu lernen. Stellen wir uns vor, dass der Veriﬁzierer einen Funktionswert f (x, y) berechnen will und dabei nur den Wert y kennt. Der Beweiser kennt x und ist bereit, dem 9Die Tatsache L ∈ PSPACE besagt nur, dass die Beweise von „x ∈ L“ eine polynomielle „Breite“ haben, wobei die Breite eines Beweises als die Länge der längsten Aussagen in einer Folge äquivalenter Aussagen betrachtet wird. 10Dieses „nichts lernen“ formalisiert man so, dass man die gesamte Kommunikation zwischen V und B als einen Wahrscheinlichkeitsraum betrachtet, wobei die Wahrscheinlichkeitsverteilung durch die zufälligen Entscheidungen von V bestimmt wird. Ein interaktives Beweissystem ist dann Zero-Knowledge, wenn eine randomisierte polynomielle TM M existiert, die diese Kommunikation mit der gleichen Wahrscheinlichkeitsverteilung generiert. 276 9 Kommunikation und Kryptographie Veriﬁzierer zu helfen, aber nur unter der Bedingung, dass der Veriﬁzierer kein einziges Bit von x erfährt. Für welche Funktionen und Entscheidungsprobleme es Zero-Knowledge- Beweissysteme gibt, ist eine spannende Frage. Es ist für uns manchmal überraschend, wie viele Problemstellungen man mit Zero-Knowledge-Beweissystemen bewältigen kann. Zum Beispiel kann man beweisen, dass jede Sprache aus NP ein Zero-Knowledge-Beweissystem hat. Wegen des Schwierigkeitsgrades und der umfangreichen benötigten Vorkenntnisse verzichten wir auf die Beweise und stellen nur ein Zero-Knowledge-Beweissystem für den Graphenisomorphismus vor. Eingabe: (G1,G2) für B und V .Sei n die Anzahl der Knoten in G1 und G2. B: Der Beweiser wählt zufällig ein i ∈{1, 2} und eine Permutation π =(j1,j2,...,jn) von (1, 2,...,n). Danach wendet er π auf Gi an und schickt den resultierenden Graphen Gi(π) an den Veriﬁzierer. V : Der Veriﬁzierer wählt zufällig ein j ∈{1, 2} und schickt es an den Beweiser. Dies entspricht der Anforderung, einen Beweis für den Isomorphismus von Gi(π) und Gj zu liefern. B: Falls G1 und G2 isomorph sind, bestimmt der Beweiser den Isomorphismus δ,so dass Gj(δ)= Gi(π) und schickt δ an den Veriﬁzierer. Falls G1 und G2 nicht isomorph sind und i = j, dann schickt der Beweiser die Permutation δ = π. Sonst (wenn kein Isomorphismus zwischen Gj und Gi(π) existiert) schickt der Beweiser ebenfalls δ = π. V : Der Veriﬁzierer akzeptiert (G1,G2) genau dann, wenn Gj(δ)= Gi(π). Analysieren wir zuerst, ob dieses Protokoll ein interaktives Beweissystem ist. Falls G1 und G2 isomorph sind, kann der Beweiser immer eine Permutation δ ﬁnden, so dass Gj(δ)= Gi(π) gilt. Damit akzeptiert V die Eingabe (G1,G2) mit Sicherheit (d. h. mit Fehlerwahrscheinlichkeit 0). Falls G1 und G2 nicht isomorph sind, kann B ein δ mit Gj(δ)= Gi(π) nur dann schicken, wenn i = j. Die Wahrscheinlichkeit, dass i = j für zwei zufällige Zahlen i, j ∈{1, 2}, ist genau 1 2 . Das k-malige Wiederholen dieses Protokolls, in dem V nur akzeptiert, wenn er k-mal die richtige Antwort bekommt, drückt die Fehlerwahrscheinlichkeit auf 2 −k. Weil wir die formale Deﬁnition von Zero-Knowledge-Beweissystemen nicht gegeben haben, können wir auch keinen formalen Beweis dieser Eigenschaft eines Beweissystems führen. Trotzdem können wir intuitiv gut verstehen, warum dieses Protokoll ein Zero- Knowledge-Beweissystem ist. Über den Isomorphismus zwischen G1 und G2 (sofern einer existiert) wird in der Kommunikation nichts verraten. Die erste Kommunikationsnachricht Gi(π) kann man als ein Zufallsereignis betrachten, das durch die zufällige Wahl von i und π bestimmt wird. Die zweite Nachricht j ist auch eine Zufallszahl und die Permutation δ ist ebenfalls durch die zufällig ausgesuchte Permutation π bestimmt (δ ist entweder direkt π oder eine Anwendung der zufälligen Permutation π auf den Isomorphismus zwischen G1 und G2). Damit bildet für ein festes (G1,G2) die Menge aller möglichen Kommunikationen (Gi(π),j,δ) zwischen B und V einen Wahrscheinlichkeitsraum. Man kann zeigen, dass der Veriﬁzierer die Tripel (Gi(π),j,δ) auch alleine mit gleicher Wahrscheinlichkeitsverteilung erzeugen kann. Daher kann der Veriﬁzierer die Arbeit des Beweisers simulieren und somit kann er aus der Kommunikation nichts anderes erfahren, als das, was er auch alleine berechnen kann. 9.6 Entwurf eines Kommunikationsnetzes 277 x0 x1 x2 x3 y0 y1 y2 y3 Abbildung 9.2 9.6 Entwurf eines Kommunikationsnetzes Die Kommunikation zwischen unterschiedlichen Objekten wie Menschen, Rechnern, Pro- zessoren eines Parallelrechners usw. kann man durch unterschiedliche Verbindungsarten realisieren. Jede Technologie stellt andere Möglichkeiten und andere Einschränkungen dar und deswegen muss man sich bei dem Entwurf von Telegraphennetzen, festen und drahtlosen Telefonnetzen, optischen Glasfasernetzen, parallelen Rechnern usw. jeweils mit anderen Problemen beschäftigen. Eine Vielfalt von Problemstellungen tritt auch bei dem Entwurf von Kommunikationsstrategien in gegebenen Netzen auf. Diese Vielfalt ist zu groß, um sie systematisch in einem Abschnitt darzustellen. Deswegen ist unsere Zielsetzung hier viel bescheidener. Wir wollen die Art der Problemstellung anhand eines beispielhaften Entwurfs eines festen Verbindungsnetzes illustrieren. Betrachten wir die folgende Aufgabenstellung. Wir haben 2n Teilnehmer x0,x1,...,xn−1,y0,y1,...,yn−1, die wir als Knoten eines Netzes darstellen. Wir sollen zwischen den x-Teilnehmern x0,x1,...,xn−1 und den y-Teilnehmern y0,y1,...,yn−1 ein Netz bauen, so dass jederzeit jeder der x-Teilnehmer eine Verbindung für ein Gespräch mit einem ausgewählten y- Teilnehmer bekommen kann. Das Netz betrachtet man als einen Graphen G =(V, E), x0,x1,...,xn−1,y0,y1,...,yn−1 ∈ V und die Bereitstellung einer Verbindung von xi zu yj bedeutet, einen Weg xi,v1,...,vm,yj zwischen xi und yj in G zu ﬁnden, bei dem keine Kante dieses Weges für Gespräche zwischen anderen Paaren von Knoten benutzt wird. Dies bedeutet, dass jede Kante zu jedem Zeitpunkt nur ausschließlich für ein einziges Gespräch benutzt werden darf. Die einfachste Lösung würde von jedem Knoten xi eine Verbindung zu jedem y- Teilnehmer ziehen. Dadurch bekommt man einen vollständigen bipartiten Graphen, wie für n = 4 in Abbildung 9.2 dargestellt. Diese Lösung ist aber praktisch nicht realisierbar. Hierbei hat man n2 Kanten für 2n Teilnehmer, was für n = 10000 die Anzahl der Verbindungsdrähte auf 100 Millionen wachsen lässt. Außer den hohen Herstellungskosten und der hohen Kabelzahl, die das Netz unübersichtlich erscheinen lassen, kann man so 278 9 Kommunikation und Kryptographie LO(v) LO(v) LU(v) LU(v) RO(v)RO(v) RU(v)RU(v) (a) (b) v in LLRR v in LRRL ZustandZustand Abbildung 9.3 ein Netz auch aus technologischen Gründen nicht realisieren. Die Anzahl der zu einem Knoten inzidenten Kanten (der Knotengrad) muss eine Konstante sein, d. h., sie darf nicht mit der Anzahl der Teilnehmer wachsen. Formulieren wir jetzt genauer die Anforderungen an eine mögliche praktikable Lösung. Jeder Knoten des Netzes darf höchstens den Grad 4 haben, die Teilnehmerknoten haben höchstens den Grad 2. Im Folgenden nennen wir die Knoten, die nicht den Teilnehmern entsprechen, die Schaltungsknoten. Die Schaltungsknoten sind von der Struktur her alle gleich (Abbildung 9.3) und sehr einfach. Jeder Schaltungsknoten v hat genau vier inzidente Kanten LO(v), LU (v), RO(v) und RU (v). Der Knoten v kann in einem von zwei Zuständen LLRR oder LRRL sein. Wenn v in dem LLRR Zustand ist, bedeutet das, dass die Kanten LO(v) und LU (v) miteinander verbunden sind, und auch die Kanten RO(v) und RU (v) (Abbildung 9.3(a)). Wenn v in dem Zustand LRRL ist, liegen LO(v) und RU (v) auf einem gemeinsamen Kommunikationsweg und RO(v) verknüpft mit LU (v) ist ein Teil eines anderen Kommunikationsweges (Abbildung 9.3(b)). Ein x-Teilnehmerknoten u hat nur zwei Kanten L(u) und R(u) und er kann sich alleine entscheiden, welche dieser beiden er für die Kommunikation benutzen will. Damit hat u auch zwei Zustände L und R bezüglich der Kante, die er zur Kommunikation benutzt. Die erlaubten Kommunikationsanforderungen an das Netz sind durch eine Permutation (i0,i1,...,in−1) von (0, 1,...,n − 1) gegeben. Eine Kommunikationsaufgabe (i0,i1,...,in−1) bedeutet, dass der x-Teilnehmer xj mit dem y-Teilnehmer yij sprechen will, für alle j ∈{0, 1,...,n − 1}. Wir fordern, dass man für jede der n! Permutationen (i0,...,in−1) eine solche Zustandszuordnung der Schaltungsknoten ﬁnden kann, dass die n Gespräche (x0,yi0), (x1,yi1),..., (xn−1,yin−1) simultan stattﬁnden können. Ein Netz mit 2n Teilnehmern, das alle Permutationen von (0, 1,...,n − 1) realisieren kann, nennt man n-Permutationsnetzwerk. Im Prinzip bedeutet dies, dass eine Festlegung von n kantendisjunkten Wegen zwischen xj und yij für j =0, 1,...,n − 1 möglich ist. Wir bemerken, dass die Vereinigung von Knoten xi und yi zu einem Knoten für i =0, 1,...,n − 1 zu einem Telefonnetz führt, in dem beliebige Paare von Teilnehmern kommunizieren können. Als die Kosten eines Permutationsnetzwerkes betrachten wir die Anzahl der Schaltungs- knoten11 und diese Kosten wollen wir natürlich minimieren. Der zweite Parameter, den wir auch in dem Entwurf minimieren wollen, ist die Länge des längsten Pfades zwischen 11und damit auch die Anzahl der Kanten (Verbindungsdrähte) 9.6 Entwurf eines Kommunikationsnetzes 279 LLRR x0 x1 x2 x3 y0 y1 y2 y3 (0, 00) (1, 00) (2, 00) (0, 01) (1, 01) (2, 01) (0, 10) (1, 10) (2, 10) (0, 11) (1, 11) (2, 11) Abbildung 9.4 einem x-Teilnehmer und einem y-Teilnehmer. Schön wäre es auch, eine regelmäßige, überschaubare Verbindungsstruktur zu erzeugen, insbesondere wünschenswert wäre die sogenannte Modularität. Modularität bedeutet, dass man die Netze für 2n Teilnehmer als Bausteine für die Herstellung von Netzen mit mehr (zum Beispiel mit 4n) Teilneh- mern benutzen kann. Die Modularität verringert oﬀensichtlich die Kosten zukünftiger Netzwerkerweiterungen und ist damit auch von großer Bedeutung. Das Netz in Abbildung 9.4 stellt eine preiswerte Lösung für die acht Teilnehmer x0,x1,x2,x3,y0,y1,y2,y3 dar. Die Anzahl der Schaltungsknoten ist nur 4 und alle Kom- munikationswege zwischen x-Teilnehmern und y-Teilnehmern haben genau die Länge 2. Die Zustände der Knoten des 4-Permutationsnetzwerkes in Abbildung 9.4 bestimmen die Realisierung der Permutation (3, 0, 2, 1). Aufgabe 9.5. Beweisen Sie, dass das Netz in Abbildung 9.4 eine Lösung unseres Entwurfspro- blems darstellt. Unsere Zielsetzung ist jetzt, ein asymptotisch optimales Netzwerk für diese Kommu- nikationsaufgabe zu entwerfen. Zuerst zeigen wir, dass jedes Netz für 2n Teilnehmer mindestens Ω(n log n) Schaltungsknoten haben muss. Satz 9.4. Sei n ∈ N −{0}. Jedes n-Permutationsnetzwerk hat mindestens Ω(n log n) Schaltungsknoten. Beweis. Sei n ∈ N −{0} und sei Netn ein n-Permutationsnetzwerk. Wenn man in Netn eine Permutation realisieren will, muss man eine passende Zuordnung der Zustände für die Knoten des Netzes ﬁnden. Zwei unterschiedliche Permutationen erfordern oﬀen- sichtlich unterschiedliche Zustandszuordnungen. Damit muss die Anzahl der möglichen Zustandszuordnungen in Netn mindestens n! sein, also die Anzahl der Permutationen von n Elementen. 280 9 Kommunikation und Kryptographie Sei m die Anzahl der Schaltungsknoten in Netn. Die Anzahl unterschiedlicher Zustands- zuordnungen in Netn ist genau 2n · 2m. Damit ist 2m · 2n ≥ n! (das heißt 2 m ≥ n!/2 n), und somit 12 gilt m ≥ log2(n!) − n ≥ n · log n − n · (ln e + 1) ∈ Ω(n log n). \u0002 Aufgabe 9.6. Beweisen Sie, dass jedes n-Permutationsnetzwerk mindestens einen Weg der Länge log2 n zwischen x-Teilnehmern und y-Teilnehmern beinhalten muss. Um ein n-Permutationsnetzwerk einer Größe in O(n log n) zu entwerfen, beginnen wir mit sogenannten r-dimensionalen Schmetterlingen13 Butr für jedes r ∈ N. Butr = (Vr,Er), wobei Vr = {(i, w) | i ∈{0, 1,...,r},w ∈{0, 1}r} und Er = {{(i, w), (i +1,w)}| i ∈{0, 1,...,r − 1}} ∪{{(i, xay), (i +1, xby)}| i ∈{0, 1,...,r − 1},x ∈{0, 1}i, a, b ∈{0, 1},a ̸= b, y ∈{0, 1}r−i−1}. Der 2-dimensionale Schmetterling But2 ist in Abbildung 9.4 dargestellt. Eine anschau- liche Darstellung von Butr legt die (r +1) · 2r Knoten von Butr als eine Matrix von r + 1 Zeilen und 2 r Spalten an. Auf der Position (i, j) der Matrix liegt genau der Knoten (i, w) mit Nummer(w)= j. Eine Kante liegt zwischen (i, x) und (i +1,y) nur dann, wenn entweder x = y gilt (die vertikalen Kanten, die in jeder Spalte von oben nach unten laufen), oder wenn sich x und y nur im i-ten Bit unterscheiden (Abbildung 9.4). Wenn für j =0, 1,..., 2 r − 1 dem Teilnehmer xj der Knoten (0,w) mit Nummer(w)= j und dem Teilnehmer yj der Knoten (r, w′) mit Nummer(w′)= j zugeordnet wird, dann enthält Butr für jedes Paar (xd,yc), d, c ∈{0, 1,..., 2 r − 1}, folgenden Weg zwischen xd und yc. Seien Nummer(a0a1 ...ar−1)= d und Nummer(b0b1 ...br−1)= c für irgendwelche ak,bk ∈{0, 1} für k =0, 1,...,r − 1. Damit entspricht der Kno- ten (0,a0a1 ...ar−1) dem Teilnehmer xd und der Knoten (r, b0b1 ...br−1) entspricht dem Teilnehmer yc.Wenn a0 = b0 gilt, startet der Weg mit der vertikalen Kan- te {(0,a0a1 ...ar−1), (1,a0a1 ...ar−1)}.Wenn a0 ̸= b0, dann nimmt man die Kante {(0,a0a1 ...ar−1), (1,b0a1 ...ar−1)}. Damit erreicht man nach dem ersten Zug in beiden Fällen den Knoten (1,b0a1 ...ar−1). Im Allgemeinen erreicht der Weg von (0,a1 ...ar−1) nach (r, b0b1 ...br−1) nach k Kantenzügen den Knoten (k, b0b1 ...bk−1akak+1ak+2 ...ar) und wird mit der Kante {(k, b0b1 ...bk−1akak+1 ...ar−1), (k +1,b0b1 ...bkak+1 ...ar−1)} 12Entsprechend der Stirling’schen Formel n! ≈ n n en · √2πn. 13„butterﬂy“ in der englischsprachigen Literatur 9.6 Entwurf eines Kommunikationsnetzes 281 fortgesetzt. Damit sorgt die s-te Kante für die Umstellung des s-ten Bits as auf das s-te Zielbit bs. In Butr kann man also jeden x-Teilnehmer mit einem beliebigen y-Teilnehmer verbinden. Leider reicht das Netzwerk nicht aus, um eine beliebige Permutation zu realisieren. Über den Knoten (⌊ r 2 ⌋,b0b1 ...b⌊r/2⌋a⌊r/2⌋+1 ...ar−1) führen in unserer Strategie alle Wege mit einem Zielknoten aus der Menge {(r, b0b1 ...b⌊r/2⌋e⌊r/2⌋+1 ...er−1) | ej ∈{0, 1} für j = ⌊ r 2 ⌋ +1,...,r − 1} und mit einem Startknoten aus der Menge {(0,f0f1 ...f⌊r/2⌋a⌊r/2⌋+1 ...ar−1) | fi ∈{0, 1} für i =0, 1,..., ⌊ r 2 ⌋}. Es gibt bis zu 2 r 2 −1 solcher Wege, aber wir dürfen über einen Knoten höchstens zwei Wege führen. Wir nutzen jetzt die r-dimensionalen Schmetterlinge als Bausteine zum Entwurf eines 2r-Permutationsnetzes, das man Beneš-Netzwerk nennt. Das r-dimensionale Beneš- Netzwerk Benesr erhält man, wenn man zwei r-dimensionale Butr-Netzwerke A und B zusammenfügt, indem man die entsprechenden Knoten der r-ten (letzten) Zeilen von A und B miteinander verschmilzt. Abbildung 9.5 zeigt Benes3. Wir sehen, dass das Benesr-Netz 2r + 1 Zeilen hat, die ersten r + 1 Zeilen bilden ein Butr-Netz und die letzten r + 1 Zeilen bilden auch ein Butr-Netz. Abbildung 9.6 zeigt eine rekursive Deﬁnition des Benesr-Netzes aus zwei Benesr−1-Netzen. Aufgabe 9.7. Geben Sie für jedes r ∈ N −{0} eine formale Beschreibung von Benesr als Graph an. Satz 9.5. Für jedes r ∈ N −{0} ist Benesr ein 2 r-Permutationsnetzwerk. Beweis. Wir zeigen, dass man für jede Permutation (i0,...,i2r−1)2 r Wege von xj nach yij für j =0, 1,..., 2r − 1 so wählen kann, dass keine Kante auf mehr als einem Weg liegt und jeder Knoten auf genau einem Weg liegt.14 Damit ist oﬀensichtlich, dass eine Zustandszuordnung für die Knoten existiert, die die Kommunikation realisiert, die durch die Permutation (i0,...,i2r−1) bestimmt ist. Wir beweisen diese Aussage mit Induktion bezüglich r. (i) Induktionsanfang. Sei r = 1. Dann ist Benes1 oﬀensichtlich ein 2-Permutationsnetzwerk (siehe Abbil- dung 9.7). 15 (ii) Induktionsschritt. Sei r ≥ 2. Wir setzen voraus, dass für jede Permutation von (0, 1,... , 2 r−1 − 1) im Benesr−1-Netz 2r−1 knotendisjunkte Wege zwischen den 2r−1 x-Teilnehmern und 14Solche Gruppen von Wegen nennt man paarweise knotendisjunkte Wege. 15Man bemerke, dass für r = 1 sogar Butr hinreichend ist. 282 9 Kommunikation und Kryptographie x0 x1 x2 x3 x4 x5 x6 x7 y0 y1 y2 y3 y4 y5 y6 y7 Abbildung 9.5 den 2r−1 y-Teilnehmern existieren. Wir können jetzt Benesr als ein Netz ansehen (Abbildung 9.6), das aus zwei Benesr−1-Netzen A und B und aus zwei „ersten Zeilen“ von Butr besteht. Dank der Induktionsvoraussetzung ist es hinreichend, eine solche Kommunikations- strategie zu ﬁnden, dass genau die Hälfte (also 2r−1) der Wege über den Netzteil Benesr−1 A geht, und dass die Kommunikationsaufgaben für A und B genau zwei Permutationen von (0, 1,..., 2 r−1 − 1) entsprechen. Um dieses zu garantieren, reicht es aus, die Wege in den ersten und letzten Zwischenzeilen von Benesr so zu wählen, dass kein Knoten v in den ersten Zeilen von A und B (also in der zweiten Zeile von Benesr) auf zwei Wegen liegt (nur eine Kante zwischen v und der 0-ten Zeile von Benesr darf benutzt werden), und dass kein Knoten u aus der letzten Zeile von A und B auf zwei Wegen liegt (nur eine der Kanten von u zu den y-Teilnehmern darf benutzt werden). Formal kann man diese Forderung wie folgt ausdrücken: 9.6 Entwurf eines Kommunikationsnetzes 283 x0 x1 x2r−1−1 x2r−1 x2r−1x2r−1+1 .. . .. . .. ... . y0 y1 y2r−1−1 y2r−1 y2r−1y2r−1+1 Benesr−1Benesr−1 A B Abbildung 9.6 x0 x1 y0 y1 (0, 0) (0, 0) (1, 0) (0, 1) (1, 1) (2, 1) Abbildung 9.7 (1) Für alle i ∈{0, 1,..., 2r−1} müssen die zwei Wege von xi und xi+2r−1 unter- schiedliche Benesr−1-Netze benutzen (ein Weg muss über A und ein Weg muss über B geführt werden (Abbildung 9.8(a))). (2) Für alle j ∈{0, 1,..., 2r−1} müssen die zwei Wege nach yj und yj+2r−1 über unterschiedliche Benesr−1-Netze geführt werden (Abbildung 9.8(a)). Im Folgenden zeigen wir, dass man die Wege einfach einen nach dem anderen bestimmen kann. Sei (i0,i1,...,i2r−1) eine beliebige Permutation von (0, 1,..., 2 r − 1). Wir nehmen jetzt die Paare (x0,yi0) und (x2r−1,yi2r−1 ) und legen den Weg von x0 nach yi0 über A und den Weg von x2r−1 nach yi2r−1 über B (Abbildung 9.8(b)). Falls |i2r−1 − i0| =2 r−1, entstehen dabei keine nachfolgenden Anforderungen. Falls |i2r−1 − i0| ̸=2r−1 müssen wir wegen (2) den Weg nach yi0+2r−1 mod 2r über B und den Weg nach yi2r−1 +2r−1 mod 2r 284 9 Kommunikation und Kryptographie x0 x2r−1 xi xi+2r−1 yj yj+2r−1 yi2r−1 yi0+2r−1 mod 2r yi2r−1 +2r−1 yi0 A A B B (a) (b) Abbildung 9.8. Eine transponierte Darstellung des Benesr-Netzes. über A leiten (Abbildung 9.8(b)). Dadurch können wieder Anforderungen an die Führung von zwei Wegen aus den Knoten xq+2r−1 mod 2r und xs+2r−1 mod 2r für iq =(i2r−1 +2r−1)mod 2r und is =(i0 +2r−1)mod 2r entstehen. 16 Wir sehen, dass die Fortsetzung dieser Strategie immer in höchstens zwei erfüllbaren Anforderungen an die Führung von zwei Wegen resultiert. Ein Weg muss über A und ein Weg muss über B geführt werden. Damit kann die vorgestellte Strategie erfolgreich die Wege zur Realisierung der gegebenen Permutation bestimmen. \u0002 Aufgabe 9.8. Bestimmen Sie die Wege in Benes3, die die Permutationen (7 2 1 3 0 5 6 4), (0 1 2 3 7 6 5 4) und (0 7 1 6 2 5 3 4) realisieren. 16Falls |((q +2r−1)mod2r) − ((s +2r−1)mod2r)| ̸=2r−1. 9.7 Zusammenfassung 285 Mit Satz 9.5 haben wir unsere Zielsetzung erfüllt. Benesr ist ein 2 r-Permutationsnetz- werk mit insgesamt (2r +1) · 2 r Knoten und r · 2 r+2 Kanten. 17 Zusätzlich haben die Beneš- Netzwerke eine regelmäßige Struktur mit hohem Grad an Modularität (Abbildung 9.6). Die Entfernung zwischen beliebigen x-Teilnehmern und einem y-Teilnehmer beträgt genau 2r und ist daher logarithmisch in der Anzahl der Teilnehmer. Aufgabe 9.9.∗ Sei G =(V, E) ein Graph. Ein balancierter Schnitt von G ist ein Paar (V1,V2), so dass (i) V = V1 ∪ V2, V1 ∩ V2 = ∅ und (ii) −1 ≤|V1|−|V2|≤ 1. Die Kosten eines Schnittes (V1,V2) sind cost(V1,V2)= |E ∩{{v, u}| v ∈ V1,u ∈ V2}|, d. h. die Anzahl der Kanten zwischen V1 und V2. Die Bisektionsbreite von G misst die Kosten des minimalen balancierten Schnittes. Im Netzwerkdesign wird der Entwurf von Netzen mit möglichst großer Bisektion angestrebt. Eine große Bisektion von G garantiert, dass bei jeder Zerlegung von G in ungefähr zwei gleich große Teile hinreichend viele Kommunikationskanäle zwischen diesen Teilen vorhanden sind. Beweisen Sie, dass Butr und Benesr eine Bisektionsbreite in Ω(2 r) haben. 18 9.7 Zusammenfassung Die Kryptographie beschäftigt sich mit dem Entwurf von Kryptosystemen, die einen sicheren Austausch geheimer Informationen ermöglichen. Bei den klassischen (symme- trischen) Kryptosystemen bestimmt der Schlüssel das Verschlüsselungsverfahren sowie das Entschlüsselungsverfahren, und somit ist der Schlüssel das gemeinsame Geheimnis von Sender und Empfänger. Public-Key-Kryptosysteme arbeiten mit einem öﬀentlichen Verschlüsselungs-Schlüssel, weil dessen Kenntnis keine Hilfe zur Entschlüsselung dar- stellt. Die Public-Key-Kryptosysteme basieren auf der Idee der Einweg-Funktionen. Eine Einweg-Funktion ist eﬃzient berechenbar, aber die zugehörige inverse Funktion ist oh- ne zusätzliches Wissen (ein Geheimnis, das nur der Empfänger besitzt) nicht eﬃzient berechenbar. Kandidaten für Einweg-Funktionen sind die Multiplikation, deren inverse Funktion die Faktorisierung ist, und die Potenzierung modulo einer natürlichen Zahl n, deren inverse Funktion der diskrete Logarithmus ist. Interaktive Beweissysteme ermöglichen eﬃzient auf randomisierte Weise die Überprü- fung der Existenz von Beweisen. Alle Sprachen in PSPACE haben interaktive Beweissys- teme. Zero-Knowledge-Beweissysteme ermöglichen es, zu überprüfen, ob andere Personen im Besitz eines Beweises (Geheimnisses) sind, ohne ein einziges Bit des Beweises zu lernen. Alle Sprachen in NP haben Zero-Knowledge-Beweissysteme. Die Qualität von Verbindungsnetzen wird auch an der Menge der zu übertragenden Daten oder der Menge von realisierten Kommunikationsanforderungen in einem kurzen 17Damit ist die Anzahl der Schaltungsknoten in O(n · log2 n), wenn n der Anzahl der Teilnehmer entspricht. 18Die Bisektionsbreite ist ungefähr so hoch wie die Anzahl der Teilnehmer, oder anders ausgedrückt, die Bisektionsbreite ist in Ω( m log m ), wobei m die Anzahl der Knoten ist. 286 9 Kommunikation und Kryptographie Zeitraum gemessen. Der Entwurf eines Netzes, das optimal bezüglich gegebener Quali- tätsparameter sein sollte, führt meistens auf nichttriviale Optimierungsprobleme, die man mit den Methoden der diskreten Mathematik zu lösen versucht. Das Konzept der Public-Key-Kryptosysteme wurde erstmals von Diﬃe und Hellman [DH76] vorgeschlagen. Das bekannte RSA-Kryptosystem ist nach seinen Erﬁndern Ri- vest, Shamir und Adleman [RSA78] benannt. Die interaktiven und Zero-Knowledge- Beweissysteme gehen auf Goldwasser, Micali und Rackoﬀ [GMR85] zurück. Shamir [Sha92] war der erste, der IP = PSPACE bewiesen hat. Das Beneš-Netzwerk hat Beneš in seinen Arbeiten über Telefonnetze [Ben64, Ben65] entworfen. Die Tatsache, dass Beneš-Netzwerke Permutationsnetze sind, wurde von Waksman [Wak68] bewiesen. Für eine anschauliche didaktisch gute Einleitung in die Kryptographie empfehlen wir wärmstens Delfs und Knebl [DK02] und Salomaa [Sal96]. Ausführlichere Auskunft über interaktive Systeme kann man in den Lehrbüchern von Bovet und Crescenzi [BC94] und Sipser [Sip97] ﬁnden. Ausführliche Informationen über Kommunikationsprobleme in Net- zen kann man in dem hervorragenden und umfangreichen Lehrbuch von Leighton [Lei92] und in dem Übersichtsartikel von Hromkovič, Klasing, Monien und Peine [HKMP96] ﬁnden. Kontrollaufgaben 1. Was ist ein Kryptosystem? Wie kann man den Grad seiner Sicherheit messen? 2. Erklären Sie das Konzept der Einweg-Funktionen. Welche Kandidaten für Einweg-Funktio- nen kennen Sie? 3. Erklären Sie den Aufbau des RSA-Kryptosystems und beweisen Sie die Eindeutigkeit der RSA-Kodierung. 4. Wie können Public-Key-Kryptosysteme angewendet werden, um Protokolle für digitale Unterschriften zu entwerfen? Warum ist das Authentizitätsproblem schwerer als das Problem der digitalen Unterschrift? 5. Erklären Sie das Konzept interaktiver Beweissysteme. Wie kann man ein Zero-Knowledge- Beweissystem deﬁnieren? Wo können solche Systeme Anwendung ﬁnden? 6. Warum unterscheiden sich die Beweissysteme für das Graphenisomorphieproblem und das Graphennichtisomorphieproblem so stark? Bei welchem dieser zwei Beweissysteme ist ein berechnungs-stärkerer Beweiser gefordert? 7. Was ist ein n-Permutationsnetzwerk? Entwerfen Sie ein 3-Permutationsnetzwerk und ein 7-Permutationsnetzwerk. 8. Zeichnen Sie einen 4-dimensionalen Schmetterling. Erklären Sie anschaulich, wie man aus einem k-dimensionalen Schmetterling einen (k + 1)-dimensionalen Schmetterling bauen kann. 9. Zeichnen Sie Benes2. 10. Beweisen Sie, dass Benes3 ein 8-Permutationsnetzwerk ist. 11. Bestimmen Sie die Wege in Benes2, die die Permutationen (4321),(4123)und(3142) realisieren. Ich habe viel von meinem Lehrer gelernt, noch mehr von meinen Freunden, aber am meisten von meinen Schülern. Talmud 10 Grammatiken und Chomsky-Hierarchie 10.1 Zielsetzung Das Thema Grammatiken gehört in die Theorie der formalen Sprachen, die eines der ältesten Gebiete der Informatik ist. Der Begriﬀ „Grammatik“ liegt im Schnittpunkt zwischen Linguistik und Informatik und wurde sogar auch in der Evolutionsbiologie zur Wachstumsmodellierung eingesetzt. Das Konzept der Grammatiken ist einfach. Unsere bisherigen Berechnungsmodelle wie endliche Automaten und Turingmaschinen haben wir als Akzeptoren von Sprachen betrachtet. Diese Modelle sind eine Möglichkeit, unendliche Objekte wie Sprachen (oder Mengen) auf endliche Weise zu beschreiben. Eine Maschine A, die eine Sprache L = L(A) akzeptiert, ist eine eindeutige endliche Repräsentation der Sprache L. Grammatiken bieten einen anderen Weg, unendliche Sprachen auf endliche Weise eindeutig zu beschreiben. Grammatiken sind Mechanismen zur Generierung (Erzeugung) von Wörtern. Für die Menge L aller Wörter, die ein solcher Mechanismus erzeugt, ist der Mechanismus eine Beschreibung von L. Für die Linguisten ist diese Art Mechanismus deswegen von großer Bedeutung, weil sie auf diese Weise versuchen, die Mengen syntaktisch (grammatisch) korrekter Texte in konkreten natürlichen Sprachen zu beschreiben und auf diese Weise ein formales Mittel für die Untersuchung und die automatische Übersetzung natürlicher Sprachen zu gewinnen. Die Bedeutung in der Informatik beschränkt sich nicht nur auf die Möglichkeit, überschaubar unendliche Objekte zu beschreiben. Sogenannte kontextfreie Grammatiken, die eine einfache spezielle Art von Grammatiken darstellen, nutzt man, um Programmiersprachen darzustellen. So entsprechen die durch solche Grammatiken generierten Wörter syntaktisch korrekten Programmen in der modellierten Program- miersprache. Damit ist das Konzept der kontextfreien Grammatiken zentral für den Compilerbau. Aus der Sicht der Berechenbarkeit ist es wichtig zu bemerken, dass die Menge der Sprachen, die durch allgemeine Grammatiken erzeugbar sind, genau der Menge der rekursiv aufzählbaren Sprachen entspricht. Damit ist das Konzept der Grammatiken genau so stark wie das Konzept der Turingmaschinen, was eine weitere Bestätigung der Church’schen These ist. J. Hromkovič, Theoretische Informatik, DOI 10.1007/978-3-658-06433-4_10, © Springer Fachmedien Wiesbaden 2014 288 10 Grammatiken und Chomsky-Hierarchie Das Kapitel ist wie folgt gegliedert. In Abschnitt 10.2 stellen wir das Konzept der Grammatiken vor. Die Zielsetzung ist es zu erklären, wie der Generierungsmechanismus von Grammatiken funktioniert und in welche Klassen man die Menge aller Grammatiken einteilt und warum Grammatiken klassiﬁziert werden. Abschnitt 10.3 behandelt die einfachste Klasse von Grammatiken, die sogenannten regulären Grammatiken. Wir zeigen, dass diese Grammatiken genau die Klasse der regulären Sprachen generieren und somit in der Beschreibungsstärke äquivalent zu den endlichen Automaten sind. Zusammen mit dem Erlernen des Entwurfs von regulären Grammatiken ist dies die Hauptzielsetzung dieses Abschnitts. Abschnitt 10.4 ist von zentraler Bedeutung für dieses Kapitel. Die Zielsetzung ist zu zeigen, dass die kontextfreien Grammatiken eine natürliche Art einer nichtdeterminis- tischen Rekursion sind. Auf der Suche nach einem Berechnungsmodell, das genau die durch kontextfreie Grammatiken generierten Sprachen akzeptieren kann, landen wir bei nichtdeterministischen Kellerautomaten. Der Keller ist gerade die Datenstruktur, die wir in der Informatik benutzen, um Rekursionen zu modellieren und implementieren. Außerdem lernen wir hier kontextfreie Grammatiken zu entwerfen, spezielle Normalformen zu konstruieren und das Pumping-Lemma für kontextfreie Sprachen anzuwenden, das ein Instrument zum Beweisen der Nichtkontextfreiheit einer Sprache darstellt. In Abschnitt 10.5 beschäftigen wir uns mit den allgemeinen (unbeschränkten) Gram- matiken und mit den sogenannten kontextsensitiven Grammatiken. Das Hauptziel ist, die Äquivalenz zwischen den Konzepten der Grammatiken und Turingmaschinen zu zeigen und damit den Glauben an die Church’sche These noch zu verstärken. Weiter bemerken wir, dass das Konzept der kontextsensitiven Grammatiken genau den nichtdeterministischen Turingmaschinen mit linearem Speicherplatz entspricht. Dieses Kapitel wurde auf den Wunsch einiger Leser für die dritte Auﬂage des Buches hinzugefügt. Die Idee war dabei, die Abdeckung des klassischen Stoﬀs der Theoretischen Informatik für einen Anfängerkurs zu garantieren. Damit sollte die für einige Kollegen zu starke Fokussierung auf die neueren Informatikkonzepte kompensiert werden und das Buch auch für klassisch gestaltete Grundvorlesungen über Theoretische Informatik als fast vollständiges Material zur Verfügung stehen. In der Vorlesung kann dieses Kapitel auch schon direkt nach Kapitel 4 über Turingmaschinen eingesetzt werden. Die Teile über reguläre und kontextfreie Grammatiken sind auch dazu geeignet, direkt nach Kapitel 3 über endliche Automaten behandelt zu werden. Bei einer stark eingeschränkten 1 Nutzung dieses Kapitels empfehlen wir, nur das Konzept der allgemeinen und der kontextfreien Grammatiken in die Vorlesung aufzunehmen, weil gerade die Äquivalenz zu den Turing- maschinen und das Konzept der Rekursion mit der Anwendung im Compilerbau die aus Sicht der Informatik wichtigsten Beiträge dieses Kapitels sind. 1Das Material in diesem Buch übersteigt die Kapazität einer Einführungsveranstaltung über Theoretische Informatik. Wenn man sich auch für die Vorstellung neuerer Konzepte entscheidet, muss man auf die Vorstellung einiger Teilbereiche der klassischen Themen verzichten. 10.2 Das Konzept der Grammatiken 289 10.2 Das Konzept der Grammatiken Die Zielsetzung dieses Abschnitts ist, zunächst die Grammatiken als ein weiteres Konzept zur formalen Beschreibung von Sprachen vorzustellen. Bisher haben wir die Sprachen entweder durch die Beschreibung der Eigenschaften der in der Sprache enthaltenen Wörter oder durch Automaten oder Maschinen, die diese Sprachen erkennen, speziﬁziert. In diesem Kapitel wollen wir eine neue formale Beschreibungsmethode vorstellen. Diese Methode basiert auf einem Erzeugungsverfahren. Mechanismen zur Erzeugung gewisser Objekte werden in der Mathematik oft zum axiomatischen Deﬁnieren von Objektklassen verwendet. Nehmen wir als Beispiel die Deﬁnition einer Boole’schen Formel über einer Variablenmenge {x1,x2,...,xn}: (i) Die Konstanten 0 und 1 und die Variablen x1,x2,...,xn sind Boole’sche Formeln. (ii) Wenn A eine Boole’sche Formel ist, dann ist auch ¬A (die Negation von A) eine Boole’sche Formel. (iii) Wenn A und B Boole’sche Formeln sind, dann sind auch (A ∨ B) und (A ∧ B) Boole’sche Formeln. (iv) Kein Objekt, das sich nicht durch die Anwendung der Regeln (i), (ii) und (iii) erzeugen lässt, ist eine Boole’sche Formel über {x1,x2,...,xn}. Auf die oben beschriebene Weise haben wir eine axiomatische Deﬁnition Boole’scher Formeln über der Variablenmenge {x1,...,xn} und den logischen Verknüpfungen ¬, ∨ und ∧ erhalten. Natürlich können wir solche Boole’schen Formeln als Wörter über dem Alphabet {0, 1,x1,x2,...,xn, ¬, ∨, ∧, (, )} betrachten und somit bestimmt der angegebene Erzeugungsmechanismus die Sprache aller Boole’schen Formeln über {x1,x2,...,xn} mit den logischen Operationen der Negation, der Disjunktion und der Konjunktion. Das folgende Beispiel präsentiert ein Erzeugungsverfahren für eine Sprache über {a, b}, die zu den typischen hier behandelten Sprachen gehört. Beispiel 10.1. Wir betrachten das folgende Erzeugungsverfahren für eine Sprache L über dem Alphabet {a, b}: (i) λ ∈ L. (ii) Falls x ∈ L, dann ist auch axb ∈ L. (iii) Keine anderen als die durch (i) und (ii) erzeugten Wörter gehören zu L. Wir bemerken, dass wir, beginnend mit x = λ nach (i), durch dreimaliges Anwenden der Regel (ii) zu dem Wort a 3b3 kommen. Die Hypothese liegt nahe, dass L = {a ibi | i ∈ N}. Wir beweisen dies durch separate Beweise für beide Inklusionen. 290 10 Grammatiken und Chomsky-Hierarchie 1. Wir zeigen, dass {a ibi | i ∈ N}⊆ L. Wir beweisen a ibi ∈ L für alle i ∈ N mittels Induktion bezüglich i. • Induktionsanfang. Für i = 0 haben wir aibi = a 0b0 = λ. Nach der Regel (i) ist λ in L. • Induktionsschritt. Nach der Induktionsannahme ist a i−1bi−1 in L. Wir sollen zeigen, dass auch a ibi ∈ L.Wennman x gleich a i−1bi−1 setzt, folgt aus Regel (ii), dass auch axb = aa i−1bi−1b = a ibi in L sein muss. 2. Wir zeigen, dass L ⊆{a ibi | i ∈ N}. Wir beobachten, dass L nach den Regeln (i) und (ii) nur Wörter gerader Länge enthalten kann. 2 Wir zeigen mittels Induktion bezüglich i, dass für alle i ∈ N der String a ibi das einzige Wort der Länge 2i in L ist. • Induktionsanfang. Sei i = 0. Das Wort a 0b0 = λ ist das einzige Wort der Länge 0 in L. • Induktionsschritt. Unter der Induktionsannahme, dass für alle j ∈{0, 1,...,i − 1} der String a jbj das einzige Wort der Länge 2j in L ist, folgern wir nach (ii) und (iii), dass a ibi das einzige Wort der Länge 2i in L ist. ♦ Aufgabe 10.1. Sei L wie folgt deﬁniert: (i) λ ∈ L. (ii) Falls x ∈ L, dann sind auch axb und bxa in L. (iii) Falls x, y ∈ L, dann ist auch xy in L. (iv) Keine anderen Wörter gehören zu L als diejenigen, die man durch die Anwendung der Regeln (i), (ii) und (iii) erzeugen kann. Welche Sprache L wird auf diese Weise generiert? Beweisen Sie Ihre Behauptung. Aufgabe 10.2. Geben Sie Erzeugungsmechanismen zur Generierung folgender Sprachen an: (a) {xabby | x, y ∈{a, b} ∗}, (b) {x1y ||x| = |y| und x, y ∈{0, 1, 2}∗}, (c) {x1110 | x ∈{0, 1}∗}, (d) {x ∈{0, 1} ∗ ||x|0 ≥ 3 }. Das Konzept der Grammatiken basiert auf der vorgestellten Idee der axiomatischen De- ﬁnition, ist aber etwas anders umgesetzt. Man betrachtet zuerst zwei disjunkte Alphabete ΣT und ΣN. Die Symbole aus ΣT werden Terminalsymbole genannt und entsprechen der bisherigen Nutzung zur Gestaltung von Wörtern aus der betrachteten Sprache. Die Symbole aus ΣN werden Nichtterminalsymbole (oder kurz Nichtterminale) genannt. 2Ein formaler Induktionsbeweis dieser Tatsache ist oﬀensichtlich. 10.2 Das Konzept der Grammatiken 291 Die Nichtterminale spielen die Rolle von Variablen und dürfen in den Wörtern der gene- rierten Sprache nicht auftreten. Nichtterminale oder Wörter, die Nichtterminalsymbole enthalten, dürfen durch andere Wörter ersetzt werden. Wie und was ersetzt wird, wird durch die sogenannten Regeln (auch Produktionen oder Ableitungsregeln genannt) bestimmt, die den Regeln eines axiomatischen Systems entsprechen. Grob betrachtet ist eine Grammatik eine Menge von Regeln. Eine Regel ist zum Beispiel X → aXb. Diese Regel besagt, dass man jedes Auftreten des Nichtterminals X durch aXb ersetzen darf. Wenn man zu dieser Regel noch die Regel X → λ hinzunimmt, die das Nichtterminal X durch das leere Wort λ ersetzt (also X löscht), dann kann man durch diese zwei Regeln, ausgehend von X, ein beliebiges Wort aus L = {a ibi | i ∈ N} erzeugen. Das Konzept der Grammatiken fordert, dass man bei der Generierung von Wörtern immer mit einem fest gegebenen ausgezeichneten Nichtterminal anfangen muss und die gegebenen Regeln so lange anwendet, bis das generierte Wort kein Nichtterminalsymbol mehr enthält. Wenn man ein Wort über ΣT erreicht, ist der Erzeugungsprozess beendet und das generierte Wort gehört in die Menge der durch die Grammatik erzeugten Wörter. Für den Rest dieses Kapitels legen wir die Nutzung der vorkommenden Symbole fest. (i) Für die Terminalsymbole nutzen wir die Kleinbuchstaben a, b, c, d, e und die Ziﬀern. (ii) Die Großbuchstaben, vorzugsweise A, B, C, D, X, Y, Z, nutzen wir, um Nichttermi- nale zu bezeichnen. (iii) Mit den Kleinbuchstaben u, v, w, x, y und z werden Wörter über ΣT bezeichnet. (iv) Die griechischen Buchstaben wie α, β und γ werden für beliebige Wörter über ΣT ∪ ΣN verwendet. Solche Wörter bezeichnen wir auch als Satzformen. Jetzt führen wir eine formale Deﬁnition einer Grammatik ein. Ähnlich wie bei Maschinen- modellen deﬁnieren wir zuerst eine Struktur (in diesem Fall ein 4-Tupel) und dann ihre Semantik durch die Beschreibung ihrer Dynamik. Deﬁnition 10.1. (i) Eine Grammatik G ist ein 4-Tupel G =(ΣN, ΣT,P,S), wobei: (a) ΣN ist ein Alphabet, genannt Nichtterminalalphabet oder die Menge der Nichtterminale von G. Die Symbole aus ΣN nennt man Nichtterminale. (b) ΣT ist ein Alphabet, genannt Terminalalphabet oder die Menge der Ter- minalsymbole von G. Die Symbole aus ΣT nennt man Terminalsymbole. Es gilt ΣN ∩ ΣT = ∅. (c) S ∈ ΣN heißt Startsymbol oder Startnichtterminal. {Jede Generierung eines Wortes muss mit dem Wort S starten.} 292 10 Grammatiken und Chomsky-Hierarchie (d) P ist eine endliche Teilmenge von Σ ∗ΣNΣ ∗ × Σ ∗ für Σ=ΣN ∪ ΣT, genannt die Menge der Ableitungsregeln von G. Die Elemente von P heißen Regeln oder auch Produktionen. Statt (α, β) ∈ P schreiben wir auch α →G β, in Worten: α kann in G durch β ersetzt werden. {Es ist wichtig zu beobachten, dass für alle (α, β) ∈ P das Wort α mindestens ein Nichtterminalsymbol aus ΣN enthalten muss, was durch α ∈ Σ∗ΣNΣ∗ gefordert wird. Die Bedeutung ist, dass in einem Wort γαω mit γ, ω ∈ Σ ∗ die Satzform α durch β ersetzt werden kann, wodurch das Wort γβω entsteht.} (ii) Seien γ, δ ∈ (ΣN ∪ ΣT) ∗. Wir sagen, dass δ aus γ in einem Ableitungsschritt in G ableitbar ist (oder dass γ in G in δ übergeht), γ ⇒G δ, genau dann, wenn ω1 und ω2 aus (ΣN ∪ ΣT) ∗ und eine Regel (α, β) ∈ P existieren, so dass γ = ω1αω2 und δ = ω1βω2. Wir sagen, dass δ aus γ in G ableitbar ist, γ ⇒∗ G δ, genau dann, wenn (a) entweder γ = δ, (b) oder ein n ∈ N −{0} und n +1 Wörter ω0,ω1,...,ωn ∈ (ΣN ∪ ΣT) ∗ existieren, so dass γ = ω0,δ = ωn und ωi ⇒G ωi+1 für i =0, 1, 2,...,n − 1. {Somit ist ⇒ ∗ G die reﬂexive und transitive Hülle von ⇒G,wennman ⇒G als eine Relation auf Wörtern betrachtet.} Eine Folge von Ableitungsschritten ω0 ⇒G ω1 ⇒G ω2 ⇒G ... ⇒G ωn heißt eine Ableitung in G. (iii) Falls S ⇒ ∗ G w für ein Wort w ∈ Σ ∗ T, dann sagen wir, dass w von G erzeugt wird. Die von G erzeugte Sprache ist L(G)= {w ∈ Σ∗ T | S ⇒∗ G w}. Im Folgenden nutzen wir die Bezeichnung α ⇒i G β für eine Ableitung α ⇒ ∗ G β, die aus genau i Ableitungsschritten besteht. Beispiel 10.2. Sei G =(ΣN, ΣT,P,S), wobei (i) ΣN = {S}, (ii) ΣT = {a, b}, (iii) P = {S → λ, S → SS, S → aSb, S → bSa}. 10.2 Das Konzept der Grammatiken 293 Ein Beispiel einer Ableitung des Wortes baabaabb in G ist S ⇒G SS ⇒G SaSb ⇒G SaSSb ⇒G bSaaSSb ⇒G baaSSb ⇒G baabSaSb ⇒G baabaSb ⇒G baabaaSbb ⇒G baabaabb. Der erste Ableitungsschritt entspricht der Anwendung der Regel S → SS.Imzweiten Ableitungsschritt wurde gemäß S → aSb das zweite S von links ersetzt. Die Regel S → SS wurde zur Ersetzung des zweiten S im dritten Schritt angewendet. Im vierten Schritt haben wir das erste S von links nach der Regel S → bSa durch bSa ersetzt, und so weiter. Wir vermuten, dass L(G)= Lge = {w ∈{a, b} ∗ ||w|a = |w|b}. Die Tatsache, dass L(G) nur Wörter mit einer gleichen Anzahl a’s und b’s generieren kann, ist oﬀensichtlich, weil jede Regel die gleiche Anzahl a’s und b’s erzeugt. Es bleibt also nur zu überprüfen, ob in G alle Wörter mit gleicher Anzahl a’s und b’s ableitbar sind. Wir zeigen dies mit Induktion bezüglich der Wortlänge, die für alle Wörter in Lge gerade ist. (i) Induktionsanfang. Sei n = 0. Das Wort λ ∈ Lge kann man durch die Regel S → λ in einem Ableitungs- schritt S ⇒G λ erzeugen. Somit gilt S ⇒ ∗ G λ und λ ∈ L(G). (ii) Induktionsschritt. Sei Lge ∩ ⋃2n−2 i=0 (ΣT)i ⊆ L(G) für ein n ∈ N −{0}. Wir sollen zeigen, dass alle Wörter der Länge 2n aus Lge auch in L(G) sind. Formal bedeutet dies, dass wir für jedes Wort x aus Lge ∩{a, b}2n die Existenz einer Ableitung von x in G begründen müssen. Sei x ein beliebiges Wort aus Lge ∩{a, b}2n. Wir unterscheiden 4 Fälle bezüglich des ersten und des letzten Symbols von x. (ii.1) Sei x = ayb.Weil |y| =2n−2 und |y|a = |y|b, existiert nach Induktionsannahme eine Ableitung S ⇒ ∗ G y in G. Somit gibt es folgende Ableitung S ⇒G aSb ⇒ ∗ G ayb = x von x in G. (ii.2) Der Fall x = bya ist analog zu (ii.1). Statt der Regel S → aSb wendet man im ersten Ableitungsschritt die Regel S → bSa an. (ii.3) Sei x = aya.Weil |x|a = |x|b = n, gilt n = |y|b = |y|a +2. Damit muss es eine Zerlegung von y der Form y = uv 294 10 Grammatiken und Chomsky-Hierarchie geben, so dass |u|b = |u|a + 1 und |v|b = |v|a +1. Man kann zum Beispiel u als das kürzeste Präﬁx von y deﬁnieren, das mehr b’s als a’s beinhaltet. Damit gilt x = auva, au ∈ Lge und va ∈ Lge. Nach der Induktionsannahme existieren Ableitungen S ⇒ ∗ G au und S ⇒ ∗ G va von au und va in G. Damit erhalten wir die Ableitung S ⇒G SS ⇒ ∗ G auS ⇒∗ G auva = x von x in G. (ii.4) Der Fall x = byb für ein y ∈ (ΣT) 2n−2 ist analog zu Fall (ii.3). ♦ Aufgabe 10.3. Betrachten Sie die folgende Grammatik G =(ΣN, ΣT,P,S), wobei ΣN = {S, A, B},ΣT = {a, b} und P = {S → λ, S → ASB, S → BSA, S → ABS, S → SAB, S → BAS, S → SBA, S → SS, A → a, B → b}. Welche Sprache wird durch G erzeugt? Begründen Sie ihre Behauptung. Wir beobachten, dass Grammatiken nichtdeterministische Erzeugungsmechanismen sind. Für den Nichtdeterminismus bestehen zwei Gründe. Zuerst dürfen mehrere Regeln existieren, die die gleiche linke Seite haben. Wir haben die freie Wahl, eine dieser Regeln anzuwenden, falls die linke Seite der Regel irgendwo als Teilwort in dem bisher abgeleiteten Wort vorhanden ist. Zweitens können mehrere linke Seiten von Regeln als Teilwörter auftreten, und es ist nicht festgelegt, welches der Teilwörter als erstes ersetzt wird. Zum Beispiel zerstört die Anwendung der Regel aX → by im Wort aaaXbb die Möglichkeit, im nächsten Ableitungsschritt die Regel Xb → XZ anzuwenden, obwohl diese Regel für das Wort aaaXbb anwendbar ist. Beispiel 10.3. Hier möchten wir eine Grammatik für die Sprache L3a = {w ∈{a, b} ∗ ||w|a =3} konstruieren. Wir betrachten die Grammatik G =({S, X}, {a, b},P,S) mit 3 Regeln. Mit der Regel S → XaXaXaX garantieren wir, dass genau 3 Symbole a erzeugt werden. Mit den Regeln X → λ und X → bX ermöglichen wir es, aus jedem Nichtterminal X ein beliebiges Wort aus {b}∗ zu generieren. Im Folgenden führen wir einen formalen Beweis von L(G)= L3a. 10.2 Das Konzept der Grammatiken 295 (i) Zuerst zeigen wir L3a ⊆ L(G). Sei x ∈ L3a. Dann ist x = biab jab kab m für irgendwelche i, j, k, m ∈ N. Um zu zeigen, dass x ∈ L(G), konstruieren wir eine Ableitung von x in G. S ⇒G XaXaXaX ⇒ i G biXaXaXaX ⇒G biaXaXaX ⇒j G biab jXaXaX ⇒G biab jaXaX ⇒k G biab jab kXaX ⇒G biab jab kaX ⇒ m G biab jab kab mX ⇒G biab jab kab m. (ii) Wir zeigen L(G) ⊆ L3a. Sei w ein beliebiges Wort aus L(G). Dies bedeutet, dass eine Ableitung S ⇒G ω1 ⇒G ω2 ⇒G ... ⇒G ωn = w von w ∈{a, b}∗ in G existiert. Um w ∈ L3a zu zeigen, beweisen wir zuerst mittels Induktion, dass ωi ∈ ({b}∗ ·{X, λ}· a) 3 ·{b}∗ ·{X, λ} gilt für alle i ∈{1, 2,...,n}. • Induktionsanfang. Weil wir aus S starten und es nur die eine Regel S → XaXaXaX zur Ersetzung von S gibt, ist ω1 = XaXaXaX und somit hat ω1 die gewünschte Form. • Induktionsschritt. Sei ωi−1 in ({b}∗ ·{X, λ}· a)3 ·{b} ∗ ·{X, λ} für ein i ≤ n. Wir zeigen, dass auch ωi die gewünschte Form hat. Weil X das einzige Nichtterminal in ωi−1 ist, wird bei ωi−1 ⇒G ωi die Regel X → λ oder X → bX angewendet. Somit gilt auch ωi ∈ ({b}∗ ·{X, λ}· a) 3 ·{b}∗ ·{X, λ}. Weil ({b} ∗ ·{X, λ}· a) 3 ·{b}∗ ·{X, λ}∩{a, b}∗ = L3a, erhalten wir w ∈ L3a. ♦ Aufgabe 10.4. Entwerfen Sie Grammatiken für die folgenden Sprachen: (a) {ω ∈{0, 1} ∗ ||ω|1 ≥ 2}, (b) {ω ∈{a, b} ∗ | ω = xabbay, x, y ∈{a, b} +}, (c) {ω ∈{0, 1} ∗ ||ω|0 ist gerade}. Bisher haben wir nur Grammatiken für einfache formale Sprachen betrachtet. Die Lin- guisten versuchen, die Grammatiken zur syntaktischen Beschreibung natürlicher Sprachen zu nutzen. Für diesen Zweck betrachtet man die sogenannten syntaktischen Kategorien wie ⟨Satz⟩, ⟨Text⟩, ⟨Nomen⟩ und ⟨Adjektiv⟩ als Nichtterminalsymbole. Beachten Sie, dass 296 10 Grammatiken und Chomsky-Hierarchie ⟨Nomen⟩ ein Symbol darstellt, aus dem man ein beliebiges Nomen ableiten kann. Die Terminalsymbole sind alle Wörter aus dem Wörterbuch. Somit nutzen wir zum Beispiel [Brot] als ein Terminalsymbol, das dem deutschen Wort „Brot“ entspricht. Mit Regeln wie ⟨Text⟩→⟨Satz⟩⟨Text⟩, ⟨Satz⟩→⟨Subjekt⟩⟨Verb⟩⟨Objekt⟩, ⟨Subjekt⟩→⟨Adjektiv⟩⟨Nomen⟩, ⟨Nomen⟩→ [Vater], ⟨Adjektiv⟩→ [lieber] kann man Texte in der Sprache erzeugen. In der Informatik benutzt man Grammatiken im Compilerbau, um syntaktisch korrekte Programme einer Programmiersprache zu beschreiben. Die Deﬁnition von Programmier- sprachen durch relativ einfache Grammatiken hat die Konstruktion eﬃzienter Compiler wesentlich vereinfacht. Zum Beispiel beschreiben die folgenden Regeln ⟨Ausdruck⟩→⟨Ausdruck⟩ + ⟨Ausdruck⟩, ⟨Ausdruck⟩→⟨Ausdruck⟩−⟨Ausdruck⟩, ⟨Ausdruck⟩→⟨Ausdruck⟩∗⟨Ausdruck⟩, ⟨Ausdruck⟩→⟨Ausdruck⟩ / ⟨Ausdruck⟩, ⟨Ausdruck⟩→ (⟨Ausdruck⟩), ⟨Ausdruck⟩→ [id] das System zur Erzeugung arithmetischer Ausdrücke. Das einzige Nichtterminal ist ⟨Ausdruck⟩ und die Terminalsymbole sind die arithmetischen Operationen +, −, ∗,/,die Klammern ( und ) und das Zeichen [id] für den Operanden. Aufgabe 10.5. Schreiben Sie eine Ableitung des arithmetischen Ausdrucks (id + id) ∗ ((id)/(id − id)) durch die Anwendung der oben angegebenen Regeln. Alle bisher vorgestellten Beispiele von Grammatiken haben einfache Regeln verwendet, bei denen auf der linken Seite immer nur ein Nichtterminal stand. Es geht aber nicht immer so einfach. Regeln wie aXbcSX → Saa oder abXba → aXZ sind manchmal nicht zu vermeiden, wenn man zum Beispiel eine Berechnung einer Turingmaschine nachahmen will. 3 In der Literatur unterscheidet man viele spezielle Grammatikklassen. Hier betrachten wir nur vier Basisklassen, die die sogenannte Chomsky-Hierarchie bilden. 3Wie dies geht, zeigen wir in Abschnitt 10.5. 10.2 Das Konzept der Grammatiken 297 Deﬁnition 10.2. Sei G =(ΣN, ΣT,P,S) eine Grammatik. (i) G heißt Typ-0-Grammatik. {Typ-0-Grammatiken stellen die Klasse der allgemeinen, uneingeschränkten Gram- matiken dar.} (ii) G heißt kontextsensitiv oder Typ-1-Grammatik, falls für alle Paare (α, β) ∈ P |α|≤|β| gilt. {Also kann man kein Teilwort α durch ein kürzeres Teilwort β ersetzen.} (iii) G heißt kontextfrei oder Typ-2-Grammatik, falls für alle Regeln (α, β) ∈ P α ∈ ΣN und β ∈ (ΣN ∪ ΣT) ∗ gilt. {Also haben alle Regeln die Form X → β für ein Nichtterminal X.} (iv) G heißt regulär oder Typ-3-Grammatik, falls für alle Regeln (α, β) ∈ P α ∈ ΣN und β ∈ Σ∗ T · ΣN ∪ Σ∗ T gilt. {Die Regeln einer regulären Grammatik haben die Form X → u oder X → uY für ein u ∈ Σ∗ T,X, Y ∈ ΣN.} Für i =0, 1, 2, 3 ist eine Sprache L vom Typ i genau dann, wenn sie von einer Grammatik G vom Typ i erzeugt wird. Die Familie aller Sprachen vom Typ i wird mit Li bezeichnet. Zum Beispiel ist die Grammatik G3 =({S, X}, {a, b}, {S → abX, X → bX, X → a 2},S) eine reguläre Grammatik. Die Grammatik G2 =({S, A, B}, {a, b}, {S → abbAB, A → a, B → bA},S) ist kontextfrei, aber nicht regulär, weil S → abbAB keine reguläre Regel ist. Die restlichen Regeln A → a und B → bA sind regulär. Die Grammatik G1 =({S, A}, {a, b}, {S → aAb, aA → Sb, A → a},S) ist kontextsensitiv, aber nicht kontextfrei, weil aA → Sb keine kontextfreie Regel ist. Die Grammatik G0 =({S, X, Y }, {0, 1}, {S → XSY, SY → 11,X → λ, 00X → S11, 0Y 1 → 00},S) ist eine Typ-0-Grammatik, die nicht kontextsensitiv ist, weil sie die sogenannten verkürzenden Regeln X → λ und 0Y 1 → 00 enthält. 298 10 Grammatiken und Chomsky-Hierarchie Das Hauptinteresse liegt in den Begriﬀen kontextsensitiv und kontextfrei. Die Deﬁni- tion 10.2 spiegelt die Klassiﬁkation der Grammatiken nach dem Linguisten Chomsky wieder. Die Kontextfreiheit bedeutet, dass man ein Nichtterminal X durch ein Wort α unabhängig davon ersetzen kann, wo sich das X in dem bisher abgeleiteten Wort beﬁndet und welche Symbole seine Nachbarn sind. Im Kontrast dazu stehen die kontextsensitiven Regeln, bei denen das Ersetzen eines Nichtterminals von seiner Umgebung abhängt. Zum Beispiel besagen die Regeln aaX → aabY, bX → b, Xbb → Zabb Folgendes: • X kann durch bY ersetzt werden, falls links von X das Wort aa steht, • X kann durch λ ersetzt werden (gelöscht werden), wenn b links von X steht (man bemerke, dass diese Regel auch nicht kontextsensitiv ist), und • X kann durch Za ersetzt werden, wenn das Wort bb rechts von X steht. Wenn die drei oben geschriebenen Regeln die einzigen Regeln mit X auf der linken Seite sind, dann bedeutet dies, dass man bei allen anderen „Umgebungen“ von X das Nichtterminal X nicht ersetzen darf. Bei einer Regel wie aXZb → SY bb ist es natürlich schwer, über die Ersetzung eines Nichtterminals in einer Umgebung zu sprechen. Das ist die bedauerliche Folge des präsentierten Formalismus, den man heute gewöhnlicherweise benutzt. Ursprünglich war die kontextsensitive Grammatik durch Regeln der Form (α, X, β) → γ deﬁniert, wobei die Bedeutung war, dass X durch γ ersetzt werden darf, wenn α links und β rechts von X stehen. In unserem Formalismus würde die Regel als αXβ → αγβ geschrieben. Es kann gezeigt werden, dass diese zwei unterschiedlichen Deﬁnitionen der kontextsensitiven Ableitungsregel zu der gleichen Klasse der kontextsensitiven Sprachen führen. Der Hauptunterschied zwischen den kontextfreien und regulären Grammatiken ist, dass die rechte Seite einer regulären Regel immer höchstens ein Nichtterminal enthält. Dadurch kann es in einer Ableitung nie dazu kommen, dass ein abgeleitetes Wort aus (ΣN ∪ ΣT) ∗ zwei oder mehr Nichtterminale enthält. Zweitens darf sich das Nichtterminal auf der rechten Seite einer regulären Regel nur als das letzte Symbol rechts beﬁnden. Damit sind alle ableitbaren Wörter einer regulären Grammatik von der Form uX oder u für ein u ∈ Σ∗ T und X ∈ ΣN. 10.3 Reguläre Grammatiken und endliche Automaten 299 Der wesentlichste Unterschied zwischen kontextsensitiven Grammatiken und Typ-0- Grammatiken ist, dass die kontextsensitiven Grammatiken keine verkürzenden Regeln α → β mit |β| < |α| enthalten. Wie wir sehen werden, spielt dieser Unterschied eine wichtige Rolle. Die allgemeinen Grammatiken können beliebige Speicherinhalte oder Konﬁgurationen eines Rechners generieren und damit durch Ableitungen beliebige Berech- nungen simulieren. Für die Simulation können sie beliebig viel Speicherplatz verwenden und nachdem die Berechnung beendet wurde, könnten die Typ-0-Grammatiken die letzte Konﬁguration der Berechnungssimulation zu der ausgerechneten Ausgabe schrumpfen lassen. Im Gegensatz dazu können die kontextsensitiven Grammatiken nie Wörter aus (ΣN ∪ ΣT) ∗ in der Ableitung verwenden, die länger als das erzeugte Terminalwort sind. In den folgenden Abschnitten werden wir die Nutzung und die Beschreibungsstärke der eingeführten vier Grammatiktypen untersuchen. 10.3 Reguläre Grammatiken und endliche Automaten Die Zielsetzung dieses Abschnitts ist es, die Abgeschlossenheit der Sprachklasse L3 zu untersuchen und danach zu zeigen, dass L3 genau der Menge LEA der regulären Sprachen entspricht. Wir beginnen mit einer einfachen Beobachtung. Lemma 10.1. L3 enthält alle endlichen Sprachen. Beweis. Sei L = {w1,w2,...,wk} eine endliche Sprache über einem Alphabet Σ. Wir konstruieren eine reguläre Grammatik G wie folgt: G =({S}, Σ, {S → w1,S → w2,...,S → wk},S). Oﬀensichtlich gilt L(G)= L. \u0002 Aufgabe 10.6. Beweisen Sie im Formalismus der endlichen Automaten, dass LEA alle endlichen Sprachen enthält. Gelingt Ihnen auch ein so einfacher Beweis wie in Lemma 10.1? Wir sagen, dass eine Sprachklasse L abgeschlossen bezüglich einer binären Ope- ration ⃝ über Sprachen ist, falls für alle L1,L2 ∈L L1 ⃝ L2 ∈L gilt. Wir sagen, dass eine Sprachklasse L abgeschlossen bezüglich einer unären Ope- ration △ ist, falls für jede Sprache L ∈L △(L) ∈L gilt. In der Theorie der formalen Sprachen betrachtet man die binären Operationen Vereini- gung (∪), Schnitt (∩) und Konkatenation (·) und die unären Operationen Kleene’scher Stern (∗), Kleene’sches +, Homomorphismus und Komplement. Der Begriﬀ der Abgeschlos- senheit kommt aus der Algebra, wo gefordert wird, dass alle algebraischen Strukturen 300 10 Grammatiken und Chomsky-Hierarchie (Algebren) wie Halbgruppen oder Körper abgeschlossen bezüglich der betrachteten Ope- rationen sind. Die Klasse L3 ist eine „nette“ Klasse, weil sie bezüglich der meisten grundlegenden Operationen abgeschlossen ist. Im Folgenden dokumentieren wir die Abge- schlossenheit bezüglich einiger Operationen, für die diese Eigenschaft insbesondere leicht über das Konzept der Grammatiken zu beweisen ist. Wir erinnern daran, dass wir in Abschnitt 3.3 die Abgeschlossenheit von LEA bezüglich ∪, ∩ und Komplementbildung bewiesen haben. Lemma 10.2. L3 ist abgeschlossen bezüglich Vereinigung. Beweis. Seien L und L ′ zwei beliebige Sprachen aus L3. Dann gibt es zwei reguläre Grammatiken G =(ΣN, ΣT,P,S) und G′ =(Σ ′ N, Σ ′ T,P ′,S′), so dass L(G)= L und L(G′)= L ′. Weil die Nichtterminale nach Bedarf umbenannt werden dürfen, nehmen wir ohne Beschränkung der Allgemeinheit an, dass ΣN ∩ Σ ′ N = ∅. Wir konstruieren eine Grammatik G′′ =(Σ ′′ N, Σ ′′ T,P ′′,S′′) mit L(G′′)= L(G) ∪ L(G′). Die Idee ist sehr einfach. Wir nehmen ein neues Startsymbol S′′ /∈ ΣN ∪ Σ ′ N und erlauben, es durch die Startsymbole der Grammatiken G und G′ zu ersetzen. Wenn wir dann P und P ′ zur neuen Menge der Regeln vereinigen, erhalten wir die gewünschte Grammatik G′′. Formal sieht dies folgendermaßen aus: Σ ′′ N = {S′′}∪ ΣN ∪ Σ′ N, Σ′′ T =ΣT ∪ Σ′ T und P ′′ = P ∪ P ′ ∪{S′′ → S, S′′ → S′}. Es bleibt zu beweisen, dass L(G′′)= L(G) ∪ L(G′) gilt. Wir zeigen die entsprechenden Inklusionen einzeln. (i) Wir zeigen L(G) ∪ L(G′) ⊆ L(G ′′). Sei x ein beliebiges Wort aus L(G) ∪ L(G′). Ohne Beschränkung der Allgemeinheit nehmen wir an, dass x ∈ L(G). Dann existiert eine Ableitung S ⇒∗ G x von x in G. Die folgende Ableitung S′′ ⇒G′′ S ⇒ ∗ G′′ x ist eine Ableitung von x in G ′′,weil G′′ alle Regeln von G besitzt. 10.3 Reguläre Grammatiken und endliche Automaten 301 (ii) Wir zeigen L(G′′) ⊆ L(G) ∪ L(G ′). Sei x ein beliebiges Wort in L(G′′). Sei S′′ ⇒G′′ α1 ⇒G′′ α2 ⇒G′′ ... ⇒G′′ αn = x eine Ableitung von x in G′′. Wir haben nur die Regeln S′′ → S′ und S′′ → S mit der linken Seite S′′. Deswegen gibt es nur zwei Möglichkeiten für α1, entweder α1 = S oder α1 = S′. Falls α1 = S, dann ist S ⇒G′′ α2 ⇒G′′ ... ⇒G′′ αn = x auch eine Ableitung in G,weilΣN ∩ Σ′ N = ∅. Somit ist x in L(G). Falls α1 = S′, ist diese Ableitung mit S′ statt S auch eine Ableitung von x in G′ und somit x ∈ L(G′). \u0002 Lemma 10.3. L3 ist abgeschlossen bezüglich Konkatenation. Beweis. Seien L und L ′ zwei beliebige Sprachen aus L3. Seien G =(ΣN, ΣT,P,S) und G′ =(Σ ′ N, Σ ′ T,P ′,S′) zwei reguläre Grammatiken mit L(G)= L, L(G′)= L ′ und ΣN ∩Σ ′ N = ∅. Wir konstruieren eine Grammatik G′′ =(Σ ′′ N, Σ ′′ T,P ′′,S′′) mit L(G′′)= L(G) · L(G′). Die Idee der Konstruktion ist wie folgt: Wir setzen S′′ = S und lassen G′′ die Ableitungen von G bis zum letzten Ableitungsschritt nachmachen. Eine Ableitung endet immer durch die Anwendung einer Regel A → w,wobei w ein Terminalwort (w ∈ Σ ∗ T) ist. Wir ersetzen jede solche Regel A → w in G durch eine neue Regel A → wS′ in G′′. Dadurch kann in G′′ die Ableitung eines Wortes aus L(G′) angehängt werden. 4 Eine formale Konstruktion von G′′ kann wie folgt beschrieben werden: Σ′′ N =ΣN ∪ Σ′ N, Σ ′′ T =ΣT ∪ Σ′ T, S′′ = S und P ′′ = P ′ ∪ (P ∩ (ΣN × Σ∗ T · ΣN)) ∪{A → wS′| A → w ∈ P, w ∈ Σ∗ T}. Den Beweis von L(G′′)= L(G) · L(G′) überlassen wir dem Leser. \u0002 Aufgabe 10.7. Vervollständigen Sie den Beweis von Lemma 10.3. 4Und nicht nur das. Es kann in G′′ kein Wort abgeleitet werden, das nicht ein Suﬃx aus G′ enthält. 302 10 Grammatiken und Chomsky-Hierarchie Aufgabe 10.8. Verwenden Sie die Beweisidee aus Lemma 10.3, um zu zeigen, dass L3 abge- schlossen bezüglich des Kleene’schen Sterns ist. Aufgabe 10.9. Konstruieren Sie reguläre Grammatiken für folgende Sprachen: (a) {abbxaaaybba | x, y ∈{a, b, c}∗}, (b) {aabbx | x ∈{a, b} ∗ und |x|a =3}, (c) {xaabby | x, y ∈{a, b} ∗, |xy|b ist gerade}. Führen Sie auch die Beweise, um zu zeigen, dass die von Ihnen konstruierten Grammatiken tatsächlich die Sprachen erzeugen. Wir wollen jetzt zeigen, dass L3 = LEA, indem wir zu jedem endlichen Automaten eine äquivalente reguläre Grammatik konstruieren und umgekehrt zu jeder regulären Grammatik einen äquivalenten endlichen Automaten entwerfen. Wir beginnen mit der einfacheren Richtung. Satz 10.1. Zu jedem endlichen Automaten A existiert eine reguläre Grammatik G mit L(A)= L(G). Beweis. Sei A =(Q, Σ,δ,q0,F ) ein endlicher Automat. Wir konstruieren eine reguläre Grammatik G =(ΣN, ΣT,P,S) mit L(G)= L(A). Die Idee ist, jede Berechnung von A auf einem Wort durch eine Ableitung dieses Wortes in G zu simulieren. Der endliche Automat liest in jedem Berechnungsschritt einen Buchstaben a aus der Eingabe und ändert dabei den Zustand. Die Grammatik G kann diesen Schritt durch einen Ableitungsschritt simulieren, in dem a erzeugt wird. Die Zustände von A werden in dieser Situation zu den Nichtterminalen von G.Wenn A in einem akzeptierenden Zustand endet, dann akzeptiert A das gelesene Wort. Also erlauben wir G nur dann ihre Ableitung zu beenden, wenn das aktuelle Nichtterminal einem akzeptierenden Zustand entspricht. Die formale Konstruktion sieht wie folgt aus: ΣN = Q, ΣT =Σ, S = q0, P = {p → aq | für alle a ∈ Σ,p,q ∈ Q, so dass δ(p, a)= q} ∪{s → λ | s ∈ F }. Wir beweisen jetzt L(G)= L(A). (i) Wir zeigen L(A) ⊆ L(G). Sei x = a1a2 ...an für ai ∈ Σ ein beliebiges Wort in L(A). Sei (q0,a1a2 ...an) A (q1,a2 ...an) A (q2,a3 ...an) A ... A (qn−1,an) A (qn,λ) eine akzeptierende Berechnung (qn ∈ F )von A auf x. Weil δ(qi,ai+1)= qi+1 die Existenz der Regel qi → ai+1qi+1 in G impliziert und 10.3 Reguläre Grammatiken und endliche Automaten 303 qn ∈ F die Regel qn → λ in P erfordert, erhalten wir die folgende Ableitung von x in G: q0 ⇒G a1q1 ⇒G a1a2q2 ⇒G a1a2a3q3 ⇒G ... ⇒G a1 ...an−1qn−1 ⇒G a1 ...an−1anqn ⇒G a1 ...an−1an. (ii) L(G) ⊆ L(A) zu zeigen, überlassen wir dem Leser. \u0002 Aufgabe 10.10. Vervollständigen Sie den Beweis von Satz 10.1. Aus dem Beweis von Satz 10.1 sehen wir, dass die Nichtterminale beim Entwurf regulärer Grammatiken die gleiche Rolle spielen können wie die Zustände beim Entwurf endlicher Automaten. Im Zustand haben wir immer die notwendige Information über das bisher gelesene Präﬁx der Eingabe gespeichert. Analog kann das aktuelle Nichtterminal der Träger der gleichen Information über das bisher erzeugte Präﬁx sein. Wir illustrieren dies durch folgendes Beispiel. Beispiel 10.4. In Beispiel 10.3 haben wir für die Sprache L3a = {w ∈{a, b} ∗||w|a =3} eine Grammatik konstruiert. Diese Grammatik ist aber nicht regulär, weil die Regel S → XaXaXaX dieser Grammatik nicht regulär ist. Wir konstruieren eine reguläre Grammatik H =({X0,X1,X2,X3}, {a, b},P,X0), wobei die Bedeutung des Nichtterminals Xi ist, dass bisher i Symbole a erzeugt worden sind. Um ein beliebiges Wort aus L3a zu generieren, verwenden wir die Regel Xi → bXi für i =0, 1, 2, 3, um jederzeit eine beliebige Anzahl b’s generieren zu können. Die Regeln X0 → aX1,X1 → aX2,X2 → aX3 garantieren uns, dass die Anzahl der erzeugten Symbole a durch das Nichtterminal Xi gespeichert wird. Mit der Regel X3 → λ garantieren wir, dass eine Ableitung in H nur dann mit einem Wort aus {a, b}∗ terminieren kann, wenn genau drei Symbole a generiert worden sind. Ein Beispiel einer Ableitung für das Wort bbabbbaabbb ist X0 ⇒H bX0 ⇒H bbX0 ⇒H bbaX1 ⇒ 3 H bbabbbX1 ⇒H bbabbbaX2 ⇒H bbabbbaaX3 ⇒ 3 H bbabbbaabbbX3 ⇒H bbabbbaabbb. Zu zeigen, dass L3a = L(H) gilt, wird dem Leser überlassen. ♦ 304 10 Grammatiken und Chomsky-Hierarchie Aufgabe 10.11. Beweisen Sie L3a = L(H) für die Grammatik H aus Beispiel 10.4. Aufgabe 10.12. Entwerfen Sie reguläre Grammatiken für folgende Sprachen: (a) {x ∈{0, 1} ∗||x|0 ist gerade und |x|1 ist ungerade}, (b) {x ∈{0, 1} ∗||x|0 mod3=1}, (c) {x ∈{a, b}∗||x|a ≥ 2 und |x|b mod3=0}, (d) {x = y0101z| y, z ∈{0, 1} ∗, |y|0 ist gerade, |z|1 ist ungerade}. Die Idee der Simulation eines endlichen Automaten durch eine reguläre Grammatik war einfach: die Nichtterminale haben als Informationsträger die Rolle der Zustände übernommen. Diese Idee möchten wir auch gerne für den Weg von regulären Grammatiken zu endlichen Automaten benutzen. Wenn eine reguläre Grammatik nur Regeln der Form X → a und X → aY (10.1) für ein Terminalsymbol a hätte, wäre es möglich, direkt einen äquivalenten nichtdeter- ministischen endlichen Automaten zu bauen. Für die Regel X → aY würde man zum Beispiel Y ∈ δ(X, a) nehmen, und für die Regel X → a deﬁniert man qF ∈ δ(X, a) für einen akzeptierenden Zustand qF mit δ(qF ,b)= ∅ für alle Terminalsymbole b. Leider schaﬀen wir es nicht, die Regeln der Form X → Y, X → λ, X → uY für |u|≥ 2 direkt in die Konstruktion eines NEA umzusetzen. Die Beweisidee, die wir hier verfolgen, besteht darin, zuerst zu zeigen, dass jede reguläre Grammatik in eine äquivalente reguläre Grammatik transformiert werden kann, die nur Regeln der Form (10.1) hat. Deﬁnition 10.3. Eine reguläre Grammatik G =(ΣN, ΣT,P,S) heißt normiert, wenn alle Regeln der Grammatik nur eine der folgenden drei Formen haben: (i) S → λ, wobei S das Startsymbol ist, (ii) A → a für A ∈ ΣN und a ∈ ΣT, und (iii) B → bC für B, C ∈ ΣN und b ∈ ΣT. Um zu zeigen, dass zu jeder regulären Grammatik eine äquivalente normierte Grammatik existiert, ersetzen wir die unerwünschten Regelformen eine nach der anderen. Zuerst ersetzen wir die Regeln X → Y , die wir Kettenregeln nennen. Lemma 10.4. Für jede reguläre Grammatik G existiert eine äquivalente reguläre Gram- matik G′, so dass G′ keine Regel der Form X → Y für zwei Nichtterminale X und Y enthält. Beweis. Sei G =(ΣN, ΣT,P,S) eine beliebige reguläre Grammatik. Für jedes Nichttermi- nal A ∈ ΣN bestimmen wir die Menge Kett(A) der Nichtterminale, die von A ausgehend durch die ausschließliche Anwendung von Kettenregeln erreicht werden können. Formal geschrieben, Kett(A)= {D ∈ ΣN | A ⇒ ∗ G D}. 10.3 Reguläre Grammatiken und endliche Automaten 305 Die Menge Kett(A) kann algorithmisch auf folgende Weise berechnet werden: Setze Kett0(A)= {A} und Ketti+1(A)=Ketti(A) ∪{B ∈ ΣN |∃ C ∈ Ketti(A) so dass C → B ∈ P } für i =0, 1,..., |ΣN|− 2. Dann gilt oﬀensichtlich Kett(A)=Kett|ΣN|−1(A)= ∞⋃ i=1 Ketti(A). Die Idee der Konstruktion von G ′ ist, dass man die Ableitungsteile A ⇒G A1 ⇒G A2 ⇒G ... ⇒G An ⇒G α in G durch einen Ableitungsschritt A ⇒G′ α in G′ ersetzt. Das kann man erreichen durch das Hinzufügen der Regel A → α für alle rechten Seiten α ∈ Σ∗ T ∪ Σ+ T · ΣN der Regeln, deren linke Seiten in Kett(A) liegen. Die formale Konstruktion sieht wie folgt aus: G′ =(ΣN, ΣT,P ′,S), wobei P ′ = ( P ∪ ⋃ A∈ΣN {A → α ∣ ∣ ∣ ∣ α ∈ Σ∗ T ∪ Σ+ TΣN und ∃C ∈ Kett(A) mit C → α }) − ΣN × ΣN. Wir verzichten auf den formalen Beweis der Äquivalenz L(G)= L(G′). \u0002 Aufgabe 10.13. Betrachten wir die folgende Grammatik G =({X0,X1,X2,X3}, {a, b},P,X0}) mit P = {X0 → X1,X0 → X3,X1 → aX1,X1 → X0,X1 → λ, X3 → bbX3,X3 → X0,X3 → X2,X2 → λ}. Verwenden Sie die Konstruktion aus Lemma 10.4, um alle Kettenregeln von G zu beseitigen. Als Nächstes beseitigen wir alle Regeln A → λ für alle Nichtterminale A, die nicht gleich dem Startsymbol S sind. Wir bemerken, dass S → λ nicht entfernt werden kann, weil dann keine Möglichkeit mehr besteht, das leere Wort aus der Sprache L(G) zu generieren. Lemma 10.5. Für jede reguläre Grammatik G existiert eine äquivalente Grammatik G′, die keine Regel A → λ für ein Nichtterminal A, welches nicht gleich dem Startsymbol ist, enthält. 306 10 Grammatiken und Chomsky-Hierarchie Beweis. Wir nehmen im Folgenden an, dass die erzeugte Sprache das Wort λ nicht beinhaltet und somit die Regel S → λ nicht auftritt. Die Aussage könnte auch ohne diese Annahme bewiesen werden, aber der Beweis würde dadurch sehr viel technischer werden. Sei G =(ΣN, ΣT,P,S) eine beliebige reguläre Grammatik, die keine Kettenregeln besitzt, und seien A1 → λ, A2 → λ, ..., Ak → λ alle Regeln in P aus (ΣN −{S}) ×{λ}. Wir konstruieren eine reguläre Grammatik G′ =(ΣN, ΣT,P ′,S) mit P ′ =(P −{A1 → λ, A2 → λ,...,Ak → λ}) ∪ k⋃ i=1{B → w | B → wAi ∈ P }. Die Idee der Konstruktion ist, dass die Regeln B → wAi und Ai → λ eine Ableitung B ⇒G wAi ⇒G w in G ermöglichen. Mit dem Einführen der Regel B → w kann die Grammatik G′ ohne die Regel Ai → λ diese Ableitung durch einen Ableitungsschritt nachahmen. Wir beweisen jetzt, dass L(G)= L(G′). Wie üblich beweisen wir die entsprechenden Inklusionen separat. (i) Wir zeigen L(G) ⊆ L(G′). Sei x ein beliebiges Wort aus L(G) und sei S ⇒G w1B1 ⇒G w1w2B2 ⇒G ... ⇒G w1w2 ...wn−2Bn−2 ⇒G w1w2 ...wn−2wn−1Bn−1 ⇒G w1w2 ...wn−2wn−1wn = x mit wi ∈ Σ∗ T und Bi ∈ ΣN für i =1, 2,...,n − 1 eine Ableitung von x in G. Falls wn ̸= λ, sind alle Regeln, die in S ⇒∗ G x benutzt werden,5 in P −{A1 → λ,...,Ak → λ} und somit ist diese Ableitung auch eine Ableitung in G′. Falls wn = λ, war die letzte angewandte Regel in der Ableitung Bn−1 → λ. Dann sieht die Ableitung von x in G ′ folgendermaßen aus: S ⇒G′ w1B1 ⇒G′ ... ⇒G′ w1w2 ...wn−2Bn−2 ⇒G′ w1 ...wn−2wn−1 = x. Die ersten n − 2 Ableitungsschritte sind gleich wie die ersten n − 2 Ableitungsschritte in G und der letzte Ableitungsschritt wurde durch die Regel Bn−2 → wn−1 ∈ P ′ realisiert. Die Regel Bn−2 → wn−1 ist in P ′, weil die Regeln Bn−2 → wn−1Bn−1 und Bn−1 → λ in P sind. 5Bemerken Sie, dass in einer Ableitung einer regulären Grammatik höchstens einmal eine Regel der Form A → λ angewendet werden kann. 10.3 Reguläre Grammatiken und endliche Automaten 307 (ii) Wir zeigen L(G′) ⊆ L(G). Sei x ein beliebiges Wort aus L(G ′) und sei S ⇒ ∗ G′ w1D ⇒G′ w1w2 = x für ein D ∈ ΣN und w1,w2 ∈ Σ ∗ T eine Ableitung von x in G′. Der Teil der Ableitung S ⇒ ∗ G′ w1D benutzt nur Regeln aus P ,weil D ∈ ΣN. Somit gilt S ⇒ ∗ G w1D. Falls D → w2 ∈ P , dann gilt auch w1D ⇒G w1w2 = x. Falls D → w2 /∈ P , dann existiert ein Ai ∈ ΣN so dass D → w2Ai,Ai → λ ∈ P. Dann ist S ⇒∗ G w1D ⇒G w1w2Ai ⇒G w1w2 = x eine Ableitung von x in G. \u0002 Aufgabe 10.14. In Beispiel 10.4 haben wir eine reguläre Grammatik für die Sprache L3a konstruiert. Diese Grammatik enthält die Regel X3 → λ. Benützen Sie Lemma 10.5, um diese Regel zu eliminieren. Aufgabe 10.15. In Aufgabe 10.13 haben wir eine Grammatik ohne Kettenregeln konstru- iert. Diese Grammatik enthält aber mehrere Regeln der Form X → λ. Konstruieren Sie nach Lemma 10.5 eine äquivalente reguläre Grammatik, die keine solche Regel außer S → λ enthält. Jetzt sind wir soweit, dass wir unsere Hauptbehauptung über die Formen der regulären Grammatiken formulieren können. Satz 10.2. Zu jeder regulären Grammatik existiert eine äquivalente normierte reguläre Grammatik. Beweis. Sei G eine beliebige reguläre Grammatik. Wenn wir die Konstruktionen aus Lemmata 10.4 und 10.5 anwenden, erhalten wir eine reguläre Grammatik G0 =(ΣN, ΣT,P0,S) mit folgenden Eigenschaften: (i) L(G0)= L(G) und (ii) P0 enthält nur Regeln folgender Formen: (ii.1) S → λ, (ii.2) A → w für ein A ∈ ΣN und w ∈ Σ+ T, (ii.3) B → uC für B, C ∈ ΣN und u ∈ Σ+ T. 308 10 Grammatiken und Chomsky-Hierarchie Um eine Grammatik in normierter Form zu erhalten, müssen wir die Regeln A → w mit |w|≥ 2 und B → uC mit |u|≥ 2 durch Regeln der Form D → a und D → aX für a ∈ ΣT ersetzen. Um unnötige schwer durchschaubare formale Konstruktionsbeschreibungen zu vermeiden, zeigen wir nur, wie man solche Regeln eine nach der anderen ersetzen kann. Sei A → a1a2 ...akB eine Regel aus P mit k ≥ 2,ai ∈ ΣT für i =1,...,k. Wir nehmen neue6 Nichtterminal- symbole A1,A2,...,Ak−1 und ersetzen diese Regel durch die Regeln A → a1A1,A1 → a2A2,. . . ,Ak−2 → ak−1Ak−1,Ak−1 → akB. Analog wird eine Regel C → b1b2 ...bm mit m ≥ 2,bi ∈ ΣT für i =1,...,m, durch die Regeln C → b1C1,C1 → b2C2,. . . ,Cm−2 → bm−1Cm−1,Cm−1 → bm ersetzt. Bei der Ersetzung jeder unerwünschten Regel muss man darauf achten, dass immer neue Nichtterminalsymbole genommen werden. (Also existiert nur eine Regel mit Ai auf der linken Seite, und zwar Ai → ai+1Ai+1 und Ai wird nirgendwo anders verwendet.) Wenn man dies einhält, ist es oﬀensichtlich, dass man dadurch eine zu G0 äquivalente Grammatik erzeugt. \u0002 Aufgabe 10.16. Betrachten wir die Grammatik G aus Aufgabe 10.13. Konstruieren Sie eine zu G äquivalente normierte reguläre Grammatik. Aufgabe 10.17. Betrachten wir die Grammatik G =({S, X1,X2,X3}, {0, 1},P,S) mit P = {S → 0S, S → 1S, S → 01S, S → 1110X1,X1 → 0X1,X1 → 1X1,X1 → 0001X2, X2 → 1X2,X2 → 0011X3,X3 → 0X3,X3 → 1X3,X3 → 0,X3 → 1}. Konstruieren Sie eine zu G äquivalente normierte reguläre Grammatik. Welche Sprache erzeugt G? Jetzt präsentieren wir das Hauptresultat: Die Konzepte der regulären Grammatiken und der endlichen Automaten deﬁnieren die gleiche Klasse regulärer Sprachen. Satz 10.3. L3 = LEA. 6Symbole, die nicht in ΣN ∪ ΣT vorhanden sind 10.3 Reguläre Grammatiken und endliche Automaten 309 uuaabb vv X Y Abbildung 10.1 Beweis. Satz 10.1 besagt, dass man zu jedem EA eine äquivalente reguläre Grammatik konstruieren kann. Somit gilt LEA ⊆L3. Uns bleibt zu zeigen, dass L3 ⊆LEA.Sei L eine beliebige Sprache aus L3. Aus Satz 10.2 wissen wir, dass L = L(G) für eine normierte reguläre Grammatik G.Weilzujedem nichtdeterministischen endlichen Automaten ein äquivalenter EA existiert,7 reicht es, einen zu G äquivalenten nichtdeterministischen endlichen Automaten M zu konstruieren. Die Idee der Konstruktion basiert auf der Umkehrung der Konstruktion aus Satz 10.2. Abbildung 10.1 zeigt den Schritt des NEA M , der der Anwendung der Regel X → aY in der Ableitung von G entspricht. Sei G =(ΣN, ΣT,P,S) die normierte reguläre Grammatik mit L(G)= L. Wir konstruieren M =(ΣN ∪{qF }, ΣT,δ,S,F ), wobei F = { {qF }, falls S → λ/∈ P, {qF ,S}, falls S → λ ∈ P und δ(qF ,a)= ∅ für alle a ∈ ΣT, Y ∈ δ(X, a), falls X → aY ∈ P, wobei X, Y ∈ ΣN und a ∈ ΣT, qF ∈ δ(X, a), falls X → a ∈ P, wobei X ∈ ΣN und a ∈ ΣT. Wir beweisen jetzt L(G)= L(M ). (i) Wir zeigen L(G) ⊆ L(M ). Wir nehmen im Folgenden an, dass das Startsymbol S nicht auf einer rechten Regelseite auftaucht. Dies kann ohne Einschränkung der Allgemeinheit erreicht werden, indem man ein neues Startsymbol S0 und die Regel S0 → S einführt. Sei x = a1a2 ...an für ai ∈ ΣT ein beliebiges nichtleeres Wort aus L(G). Sei S ⇒G a1A1 ⇒G a1a2A2 ⇒G ... ⇒G a1a2 ...an−1An−1 ⇒G a1a2 ...an 7Potenzmengenkonstruktion aus Satz 3.2 310 10 Grammatiken und Chomsky-Hierarchie eine Ableitung von x in G. In dieser Ableitung wurden nacheinander die Regeln S → a1A1,A1 → a2A2,...,An−2 → an−1An−1,An−1 → an von G angewendet. Der Konstruktion von M folgend, gilt A1 ∈ δ(S, a1),A2 ∈ δ(A1,a2),. . . ,An−1 ∈ δ(An−2,an−1), qF ∈ δ(An−1,an). Dadurch ergibt sich folgende akzeptierende Berechnung von M auf x: (S, a1a2 ...an−1an) M (A1,a2 ...an−1an) M (A2,a3 ...an−1an) M ... M (An−2,an−1an) M (An−1,an) M (qF ,λ). Falls λ ∈ L(G), dann kann es nur mit der Regel S → λ ∈ P erzeugt werden. Dann ist aber S ∈ F und somit λ ∈ L(M ). (ii) L(M ) ⊆ L(G) zu zeigen geht analog, und wir überlassen es dem Leser. \u0002 Aufgabe 10.18. Vervollständigen Sie den Beweis von Satz 10.3, indem Sie die Tatsache L(M ) ⊆ L(G) formal beweisen. Aufgabe 10.19. Betrachten Sie die Grammatik H aus Beispiel 10.4. Konstruieren Sie einen zu H äquivalenten endlichen Automaten. Aufgabe 10.20. In Aufgabe 10.9 haben Sie reguläre Grammatiken für drei reguläre Sprachen entworfen. Wandeln Sie zuerst die von Ihnen entworfenen Grammatiken in äquivalente normierte reguläre Grammatiken um und konstruieren Sie dann mit Hilfe von Satz 10.3 äquivalente nichtdeterministische endliche Automaten. Aufgabe 10.21. In Abschnitt 3.4 haben wir das Pumping-Lemma für reguläre Sprachen als eine Methode zum Beweisen der Nichtregularität von Sprachen entwickelt. Der Beweis basierte auf der Tatsache, dass in jeder genügend langen Berechnung sich zwei Zustände wiederholen müssen. Somit entsteht die Möglichkeit eine Schleife zu bilden, die beliebig viele Male wiederholt werden darf, ohne dass es der Automat merken kann. Lange Ableitungen regulärer Grammatiken können auch nicht die Wiederholung von Nichtterminalen vermeiden. Benutzen Sie diese Eigenschaft, um das Pumping-Lemma im Formalismus der Grammatiken zu beweisen. 10.4 Kontextfreie Grammatiken und Kellerautomaten Wie wir schon erwähnt haben, sind die kontextfreien Grammatiken die Grammatiken, die uns als Informatiker am meisten interessieren. Die Klasse L2 (in der Literatur auch als LCF bezeichnet) der durch kontextfreie Grammatiken erzeugbaren Sprachen nennt man die Klasse der kontextfreien Sprachen. Die Zielsetzungen dieses Abschnitts sind: 10.4 Kontextfreie Grammatiken und Kellerautomaten 311 • Die Beschreibungsstärke kontextfreier Grammatiken zu untersuchen und dabei ähnlich wie bei regulären Sprachen eine Pumping-Methode zu entwickeln, mit der man die Nichtexistenz kontextfreier Grammatiken für gewisse konkrete Sprachen zeigen kann (also die Tatsache L/∈L2 beweisen kann). • Die Kellerautomaten als ein Maschinenmodell einzuführen, das genau die Beschrei- bungsstärke kontextfreier Grammatiken hat. Zuerst beobachten wir, dass L3 ⊊ L2, d. h., es gibt kontextfreie Sprachen, die keine regulären Sprachen sind. 8 Betrachten wir die Grammatik G =({S}, {0, 1},P,S), wobei P = {S → λ, S → 0S1}. Wir sehen sofort, dass L(G)= {0n1 n | n ∈ N}, was keine reguläre Sprache ist. Eine andere häuﬁg benutzte kontextfreie, aber nicht reguläre Sprache ist die Sprache LRev = {wwR | w ∈{0, 1}∗}. Diese Sprache kann anschaulich durch die folgende kontextfreie Grammatik GRev =({S}, {0, 1}, {S → λ, S → 0S0,S → 1S1},S) erzeugt werden. Aufgabe 10.22. Beweisen Sie LRev = L(GRev). Aufgabe 10.23. Betrachten Sie die Sprache der Palindrome LPal = {w ∈{0, 1} ∗ | w = wR}. Konstruieren Sie eine kontextfreie Grammatik G mit L(G)= LPal. Beweisen Sie, dass LPal keine reguläre Sprache ist. Die in diesem Abschnitt bisher betrachteten kontextfreien Grammatiken waren sehr einfach, weil alle rechten Seiten der Regeln jeweils nur ein Nichtterminal enthalten haben. Damit ist in jeder Ableitung solcher Grammatiken immer nur ein Nichtterminal in den erzeugten nichtterminalen Wörtern. Solch einfache Regeln reichen aber nicht aus, um jede Sprache aus L2 zu erzeugen. In Beispiel 10.2 benötigten wir für die Sprache Lge = {w ∈{a, b}∗ ||w|a = |w|b} auch die Regel S → SS. Das folgende Beispiel zeigt eine andere Strategie, um Lge zu generieren. 8Die Inklusion L3 ⊆L2 ist oﬀensichtlich, weil reguläre Grammatiken ein Spezialfall kontextfreier Grammatiken sind. 312 10 Grammatiken und Chomsky-Hierarchie Beispiel 10.5. Wir betrachten die folgende Grammatik: G =({S, A, B}, {a, b},P,S). Die Bedeutung des Auftretens eines Nichtterminals A ist, dass wir die Generierung eines Symbols a schulden, um die Anzahl a’s und b’s auszugleichen. Analog bedeutet das Vorhandensein eines B in einem nichtterminalen Wort, dass wir für den Ausgleich ein b schulden. Wir nehmen zuerst die Regeln S → λ, S → aB und S → bA. Damit kann man Wörter generieren, die mit a oder mit b anfangen, und wenn a [b] generiert wurde, deutet B [A] das Schulden für die Generierung eines b [a] an. Die Regeln A → a, A → aS, B → b, B → bS ermöglichen dann, den Ausgleich zu erreichen. Wenn aber in dem generierten Wort das nächste Symbol unterschiedlich von dem Ausgleichssymbol ist, nutzen wir die Regeln A → bAA und B → aBB, die das Erfassen höherer Verschuldung in der Generierung von a’s oder b’s garantieren.♦ Aufgabe 10.24. Beweisen Sie, dass die kontextfreie Grammatik aus Beispiel 10.5 die Sprache Lge = {w ∈{a, b} ∗ ||w|a = |w|b} erzeugt. Aus dem Grund, dass in den Ableitungen kontextfreier Grammatiken mehrere Nichtter- minale in einem Wort auftreten dürfen, suchen wir eine überschaubare Darstellung von Ableitungen, die unabhängig von der Wahl des als nächsten zu ersetzenden Nichtterminals ist. Deﬁnition 10.4. Sei G =(ΣN, ΣT,P,S) eine kontextfreie Grammatik. Ein Syntax- baum (Ableitungsbaum) T für G ist ein markierter, geordneter 9 Baum mit folgenden Eigenschaften: (i) T hat eine Wurzel, die mit S markiert ist. (ii) Jeder Knoten von T ist mit einem Symbol aus ΣN ∪ ΣT ∪{λ} markiert. (iii) Innere Knoten sind mit Symbolen aus ΣN markiert. (iv) Alle Blätter sind mit Symbolen aus ΣT ∪{λ} markiert. Hierbei kann ein Blatt nur dann mit λ markiert sein, wenn es das einzige Kind seines Vorgängerknotens ist. (v) Wenn ein innerer Knoten, der mit A ∈ ΣN markiert ist, genau k Kinder besitzt, die von links nach rechts mit α1,α2,...,αk markiert sind, dann existiert eine Regel A → α1α2 ...αk, mit αi ∈ ΣN ∪ ΣT ∪{λ} in P . 9In einem geordneten Baum sind die Kinder eines Knotens von links nach rechts geordnet. 10.4 Kontextfreie Grammatiken und Kellerautomaten 313 Die Markierungen der Blätter von T von links nach rechts gelesen ergeben ein Wort x = xT ∈ L(G). Wir sagen auch, dass T ein Syntaxbaum zur Generierung von x in G ist. Wir veranschaulichen diese Deﬁnition mit einem Beispiel. Sei G = {{S}, {(, ), +, −, ∗,/, [id]},P,S} mit P = {S → S + S, S → S − S, S → S ∗ S, S → S/S, S → (S),S → [id]} die Grammatik zur Erzeugung arithmetischer Ausdrücke. Ein Ableitungsbaum (Syntaxbaum) T zur Generierung von ([id] ∗ [id] ∗ [id]) + ([id]/([id] − [id])) ist in Abbildung 10.2 dargestellt und beinhaltet die Ableitung S ⇒G S + S ⇒G (S)+ S ⇒G (S ∗ S)+ S ⇒G (S ∗ S ∗ S)+ S ⇒G ([id] ∗ S ∗ S)+ S ⇒G ([id] ∗ [id] ∗ S)+ S ⇒G ([id] ∗ [id] ∗ [id]) + S ⇒G ([id] ∗ [id] ∗ [id]) + (S) ⇒G ([id] ∗ [id] ∗ [id]) + (S/S) ⇒G ([id] ∗ [id] ∗ [id]) + ([id]/S) ⇒G ([id] ∗ [id] ∗ [id]) + ([id]/(S)) ⇒G ([id] ∗ [id] ∗ [id]) + ([id]/(S − S)) ⇒2 G ([id] ∗ [id] ∗ [id]) + ([id]/([id] − [id])). Wir beobachten, dass jede Ableitung eindeutig einem Syntaxbaum zugeordnet werden kann. Andererseits können mehrere unterschiedliche Ableitungen zum gleichen Syntax- baum führen. Zum Beispiel ist die hier vorgestellte Ableitung eine sogenannte Links- ableitung, weil immer das am weitesten links stehende Nichtterminal in dem aktuellen Wort ersetzt wird. Das wird aber nicht gefordert. Durch die Ersetzung des am weitesten rechts stehenden Nichtterminals erhält man eine andere Ableitung, die durch denselben Syntaxbaum dargestellt wird. Aufgabe 10.25. Schreiben Sie eine Rechtsableitung, in der also immer das am weitesten rechts stehende Nichtterminal zuerst ersetzt wird, für das Wort ([id]∗ [id]∗ [id])+([id]/([id]− [id])), die ebenfalls dem Syntaxbaum in Abbildung 10.2 zugeordnet werden kann. Aufgabe 10.26. Sei G eine kontextfreie Grammatik. Für ein Wort x ∈ L(G) können unter- schiedliche Syntaxbäume zur Generierung von x in G existieren. Konstruieren Sie eine kontextfreie Grammatik G für die Sprache L = {0n1n | n ∈ N}, so dass für jedes x ∈ L −{λ} mindestens zwei unterschiedliche Syntaxbäume zur Generierung von x in G existieren. Aufgabe 10.27. Sei G eine beliebige kontextfreie Grammatik. Konstruieren Sie eine kontextfreie Grammatik G ′, so dass L(G)= L(G ′) und G ′ für jedes nichtleere Wort x ∈ L(G) genau doppelt so viele unterschiedliche Syntaxbäume zur Generierung von x hat wie G. 314 10 Grammatiken und Chomsky-Hierarchie SS SS S SSSS S S S S S ( (( ) )) ∗ ∗ − / + [id][id] [id][id] [id][id] Abbildung 10.2 Mit ähnlichem Vorgehen wie bei regulären Grammatiken kann man jede kontextfreie Grammatik so modiﬁzieren, dass sie keine Regeln der Form X → λ (außer S → λ) und keine Kettenregeln enthält. Aufgabe 10.28. Sei G eine kontextfreie Grammatik. Konstruieren Sie eine zu G äquivalente kontextfreie Grammatik, die keine Regeln der Form X → λ für X unterschiedlich vom Startsymbol enthält. Aufgabe 10.29. Sei G eine kontextfreie Grammatik ohne Regeln der Form X → λ für X unterschiedlich vom Startsymbol. Konstruieren Sie eine zu G äquivalente kontextfreie Grammatik, die keine Kettenregel enthält. Aufgabe 10.30. Sei G =(ΣN, ΣT,P,S) eine kontextfreie Grammatik. Ein Nichtterminal X ∈ ΣN nennt man nutzlos oder ein nutzloses Symbol, wenn X in keiner aus S startenden Ableitung auftritt (nicht erreichbar ist) oder sich aus X kein Terminalwort ableiten lässt. Schreiben Sie einen Algorithmus, der für eine gegebene Grammatik G eﬃzient alle nutzlosen Symbole ﬁndet. Gemäß der Deﬁnition nutzloser Symbole in Aufgabe 10.30 ist es oﬀensichtlich, dass das Entfernen aller nutzlosen Symbole aus ΣN und aller Regeln, die nutzlose Symbole beinhalten, keinen Einﬂuss auf die durch G generierte Sprache hat. Im Folgenden sagen wir, dass eine kontextfreie Grammatik G normiert ist, falls G 10.4 Kontextfreie Grammatiken und Kellerautomaten 315 (i) keine nutzlosen Symbole, (ii) keine Regeln der Form X → λ und (iii) keine Kettenregeln enthält. Man beachte, dass man für jede kontextfreie Grammatik G eine normierte kontextfreie Grammatik G′ konstruieren kann, so dass L(G′)= L(G) −{λ}. Ausgehend von den normierten kontextfreien Grammatiken kann man noch „regulärere“ Formen erreichen. Deﬁnition 10.5. Sei G =(ΣN, ΣT,P,S) eine kontextfreie Grammatik mit λ/∈ L(G). Wir sagen, dass G in Chomsky-Normalform ist, falls alle Regeln von der Form A → BC für A, B, C ∈ ΣN, oder A → a für A ∈ ΣN und a ∈ ΣT sind. Wir sagen, dass G in Greibach-Normalform ist, wenn alle Produktionen von der Form A → aα für A ∈ ΣN,a ∈ ΣT und α ∈ Σ∗ N sind. Satz 10.4. Für jede kontextfreie Grammatik G mit λ/∈ L(G) existieren zu G äquivalente Grammatiken in Chomsky-Normalform und in Greibach-Normalform. Der formale Beweis von Satz 10.4 ist zu technisch10 und so verzichten wir auf ihn in dieser Einführung in die Theorie der formalen Sprachen. Wir deuten nur an, wie man eine Chomsky-Normalform erzeugen kann. Zuerst werden nutzlose Symbole, dann (X → λ)- Produktionen11 und Kettenregeln entfernt. Die normierte kontextfreie Grammatik hat dann Regeln A → a und A → α für |α|≥ 2. Die Regel A → a für A ∈ ΣN und a ∈ ΣT ist schon in Chomsky-Normalform. Wie man die Regeln A → α für α ∈ (ΣN ∪ ΣT)∗, |α|≥ 2, ersetzt, erklären wir anschaulich an einigen Regelbeispielen. Zuerst nehmen wir für jedes a ∈ ΣT ein neues Nichtterminal Xa und erweitern P um die Produktion Xa → a. Danach ersetzen wir alle Terminalsymbole a in den rechten Seiten der Produktionen durch die entsprechenden Nichtterminale Xa. Auf diese Weise wird, zum Beispiel, die Regel A → bBCaaAc 10insbesondere die Konstruktion einer Grammatik in Greibach-Normalform. 11Weil λ/∈ L(G), brauchen wir S → λ auch nicht. 316 10 Grammatiken und Chomsky-Hierarchie durch die Regel A → XbBCXaXaAXc ersetzt. Jetzt haben alle Regeln die Form A → a oder A → α für ein α ∈ (ΣN) ∗, |α|≥ 2. Es bleibt noch übrig, jede Produktion A → Y1Y2 ...Yk mit k> 2 und Yi ∈ ΣN für i = 1,...,k zu ersetzen. Wir nehmen für diese Regel k−2 neue Nichtterminale Z2,Z3,...,Zk−1, wobei die Bedeutung von Zi ist, dass man aus Zi nichts anderes als YiYi+1 ...Yk ableiten kann. Dann ersetzen wir A → Y1Y2 ...Yk durch die Regeln A → Y1Z2,Z2 → Y2Z3,. . . ,Zi → YiZi+1,. . . ,Zk−1 → Yk−1Yk. Wenn man diese Prozedur für jede Regel A → α mit |α| > 2 mit immer neuen Nichtter- minalen Zi realisiert, erhält man eine kontextfreie Grammatik in Chomsky-Normalform. Aufgabe 10.31. Wir haben in Beispiel 10.5 eine kontextfreie Grammatik für die Sprache Lge gesehen. Wenn wir in dieser Grammatik auf die Regel S → λ verzichten, akzeptiert diese Grammatik die Sprache Lge −{λ}. Konstruieren Sie aus dieser Grammatik eine äquivalente Grammatik in Chomsky-Normalform. Unsere nächste Aufgabe ist es, eine Methode zu entwickeln, mit der man für konkrete Sprachen L die Tatsache L/∈L2 (L ist nicht kontextfrei) beweisen kann. Die Strategie ist ähnlich wie bei regulären Sprachen. Eine Wiederholung eines Nichtterminals X in der Ableitung aus diesem Nichtterminal führt zur Möglichkeit, das entsprechende Stück der Ableitung X ⇒ ∗ αXβ beliebig oft zu wiederholen und dadurch Wörter αiXβi für ein beliebiges i abzuleiten. Der Unterschied zu den regulären Sprachen ist der, dass man, anstatt an einer Stelle zu „pumpen“, an zwei Stellen gleichzeitig „pumpt“. Lemma 10.6 (Pumping-Lemma für kontextfreie Sprachen). Sei L kontextfrei. Dann existiert eine nur von L abhängige Konstante nL, so dass für alle Wörter z ∈ L mit |z|≥ nL eine Zerlegung z = uvwxy von z existiert, so dass (i) |vx|≥ 1, (ii) |vwx|≤ nL und (iii) {uviwx iy | i ∈ N}⊆ L. 10.4 Kontextfreie Grammatiken und Kellerautomaten 317 Beweis. Sei L eine kontextfreie Sprache und sei G =(ΣN, ΣT,P,S) eine normierte kontextfreie Grammatik mit L(G)= L −{λ}.Sei m die maximale Länge der rechten Seite einer Produktion und damit die maximale Mächtigkeit 12 möglicher Verzweigungen in den Ableitungsbäumen von G. Wir wählen nL = m |ΣN|+1. Für jedes z ∈ L mit |z|≥ nL muss jeder Ableitungsbaum für z genau |z| Blätter haben und somit ist die Tiefe (die maximale Pfadlänge zwischen der Wurzel und einem Blatt) mindestens |ΣN| +1. Betrachten wir einen Pfad mit maximaler Länge von der Wurzel zu einem Blatt, daher von der Länge mindestens |ΣN|+1. Auf diesem Weg muss mindestens ein Nichtterminal als Markierung zweimal vorkommen. Wir suchen die erste Wiederholung eines Nichtterminals ausgehend von dem Blatt in Richtung der Wurzel. Damit ﬁnden wir zwei Knoten v1 und v2 (Abbildung 10.3), so dass Folgendes gilt: (i) v1 und v2 haben die gleiche Markierung A ∈ ΣN. (ii) v1 und v2 liegen auf einem Pfad zwischen einem Blatt und der Wurzel und v1 ist näher an der Wurzel als v2. (iii) Der Teil des Pfades von v1 zu dem Blatt hat eine Länge von höchstens13 |ΣN| +1. Gemäß Abbildung 10.3 erhalten wir die Zerlegung von z. Ausgehend von S existiert eine Ableitung S ⇒ ∗ G uAy mit u, y ∈ Σ∗ T. (10.2) Wenn man den Teilbaum mit der Wurzel v2 betrachtet, gibt es eine Ableitung A ⇒∗ G w mit w ∈ Σ∗ T. (10.3) Der Teilbaum mit der Wurzel v1 garantiert die Existenz der Ableitung A ⇒∗ G vAx ⇒ ∗ G vwx (10.4) für v, x ∈ Σ∗ T. Zusammenfassend deﬁniert die Ableitung S ⇒ ∗ G uAy ⇒ ∗ G uvAxy ⇒ ∗ G uvwxy (10.5) die Zerlegung z = uvwxy 12die Anzahl der aus einem Knoten ausgehenden Kanten 13Wir wählen v2 und v1 als die ersten zwei identischen Nichtterminale in der Richtung vom Blatt zur Wurzel. Dies bedeutet, dass auf dem Pfad von v1 zum Blatt bis auf die Nichtterminale in v1 und v2 alle Nichtterminale paarweise unterschiedlich sind. 318 10 Grammatiken und Chomsky-Hierarchie v1 v2 A A u v w x y Abbildung 10.3 von z.Weil G eine normierte kontextfreie Grammatik ist, ist die Länge der rechten Seite der in v1 angewendeten Regel mindestens 2. Weil keine Regeln X → λ vorkommen, können v und x nicht beide leer sein. Somit gilt (i), das heißt |vx|≥ 1. Weil der Pfad von v1 über v2 zu dem Blatt die Länge höchstens |ΣN| + 1 hat, 14 ist die Anzahl der Blätter des Teilbaumes mit der Wurzel v1 und Verzweigungsgrad höchstens m durch nL = m |ΣN|+1 von oben beschränkt. Also gilt |vwx|≤ nL und somit ist (ii) erfüllt. Wenn man die Ableitungen (10.2) und (10.3) kombiniert, erhält man die Ableitung S ⇒ ∗ G uAy ⇒ ∗ G uwy = uv0wx 0y von uwy und somit ist uv0wx 0y in L. Wenn wir mit der Ableitung (10.2) beginnen, dann i-mal den ersten Teil von (10.4) anwenden und am Ende die Ableitung (10.3) einsetzen, erhalten wir die Ableitung S ⇒ ∗ G uAy ⇒∗ G uvAxy ⇒ ∗ G ... ⇒∗ G uviAx iy ⇒ ∗ G uviwx iy 14Man bemerke, dass dank unserer Voraussetzung dieser Pfad der längste in dem Teilbaum mit der Wurzel v1 ist. 10.4 Kontextfreie Grammatiken und Kellerautomaten 319 von uviwx iy in G für jedes i ∈ N −{0}. Somit gilt auch (iii) und das Lemma wurde bewiesen. \u0002 Bei der Anwendung des Pumping-Lemmas für kontextfreie Sprachen geht man genauso vor wie bei der Anwendung des Pumping-Lemmas für reguläre Sprachen. Für eine gegebene Sprache L wollen wir zeigen, dass das Pumping-Lemma für L nicht gilt. Dafür haben wir zuerst die Wahl eines genügend langen Wortes z aus L, weil das Pumping-Lemma für alle ausreichend langen Wörter gelten muss. Dann müssen wir beweisen, dass für jede Zerlegung von z = uvwxy, die (i) und (ii) erfüllt, die Bedingung (iii) nicht gilt. Wir präsentieren eine Anwendung dieser Strategie in folgendem Beispiel. Beispiel 10.6. Wir zeigen, dass die Sprache L = {a nbnc n | n ∈ N} keine kontextfreie Sprache ist. Wir nutzen das Pumping-Lemma für kontextfreie Sprachen, um einen indirekten Beweis zu führen. Nehmen wir also an, L sei kontextfrei, weswegen das Pumping-Lemma für L gilt. Wählen wir das Wort z = a nLbnLc nL, das oﬀensichtlich länger als die durch das Lemma gegebene Konstante nL ist. Oﬀensichtlich beinhaltet jede Zerlegung von z = uvwxy mit der Eigenschaft (ii) |vwx|≤ nL in dem Teilwort vwx und somit in v und x höchstens zwei unterschiedliche Buchstaben. Weil nach (i) |vx|≥ 1 gilt, enthält das Wort uv2wx 2y keinesfalls die gleiche Anzahl a’s, b’s und c’s. Somit ist uv2wx 2y nicht in L und die Bedin- gung (iii) des Pumping-Lemmas ist nicht erfüllt, was einen Widerspruch zur Voraussetzung darstellt. ♦ Aufgabe 10.32. Beweisen Sie, dass die folgenden Sprachen keine kontextfreien Sprachen sind. (a) {0 n1 n0 n 2 | n ∈ N}, (b) {anbnc m | m ≤ n, m, n ∈ N}, (c) {a ibjc k | i, j, k ∈ N,k =max{i, j}}, (d) {ww | w ∈{0, 1} ∗}, (e) {0 i1 i 2 | i ∈ N}. Aufgabe 10.33. Die folgenden Sprachen sind kontextfrei. Deswegen kann man ihre Kontextfrei- heit mit dem Pumping-Lemma nicht widerlegen. Wenn man trotzdem versuchen würde, mit dem Pumping-Lemma die Nichtkontextfreiheit dieser Sprachen zu beweisen, wird man oﬀensichtlich scheitern. Erklären Sie im Detail, wo der Beweisversuch scheitert. (a) {0, 001, 00001}, (b) {0n1 n | n ∈ N}, (c) {w ∈{a, b} ∗ ||w|a = |w|b}. 320 10 Grammatiken und Chomsky-Hierarchie Aufgabe 10.34. In dem Beweis des Pumping-Lemmas für kontextfreie Sprachen haben wir normierte kontextfreie Grammatiken benutzt. Schreiben Sie den Beweis um, indem Sie die Grammatik für L in Chomsky-Normalform annehmen. Wo und wie vereinfacht dies den Beweis? Es gibt Situationen, in denen es sehr schwer oder umständlich ist, das Pumping- Lemma für den Beweis von L/∈L2 anzuwenden. Dieses Instrument für das Beweisen der Nichtexistenz einer kontextfreien Grammatik für eine gegebene Sprache L kann man verbessern, indem man sich in dem ausgesuchten Wort einige (aber mindestens nL viele) Symbole markiert und dann nur die Zerlegungen betrachtet, die (i) und (ii) für die Anzahl der markierten Symbole erfüllen. Diese Überlegung führt zum folgenden Lemma. Lemma 10.7 (Lemma von Ogden). Für jede kontextfreie Sprache L gibt es eine nur von L abhängige Konstante nL, so dass für alle Wörter z ∈ L mit |z|≥ nL und alle Markierungen von mindestens nL Buchstaben in z eine Zerlegung z = uvwxy von z existiert, so dass (i) vx mindestens einen markierten Buchstaben enthält, (ii) vwx höchstens nL markierte Buchstaben enthält und (iii) {uviwx iy | i ∈ N}⊆ L. Aufgabe 10.35. Beweisen Sie das Lemma von Ogden. Hinweis: Nehmen Sie eine Grammatik in Chomsky-Normalform als eine Darstellung von L. Wählen Sie einen Pfad im Syntaxbaum für z, ausgehend von der Wurzel und der Kante folgend, die zu einem Teilbaum mit mehreren markierten Symbolen führt. Der Vorteil des Lemmas von Ogden liegt nicht nur darin, dass man die Beweise von L/∈L2 vereinfachen kann. Der Hauptvorteil des Lemmas von Ogden ist, dass seine Anwendung für einige Sprachen zum Beweis von L/∈L2 führt, obwohl diese Tatsache mit dem Pumping-Lemma für kontextfreie Sprachen nicht zu beweisen ist. Beispiel 10.7. Betrachten wir die Sprache L = {a nblc jdk | n, l, j, k ∈ N −{0},l = j = k}∪{b, c, d} ∗. Zuerst zeigen wir, dass das Pumping-Lemma für kontextfreie Sprachen nicht helfen kann, um L/∈L2 nachzuweisen. Sei z ein beliebiges 15 Wort aus L. Wir unterscheiden zwei Fälle. (i) Das Wort z enthält kein Symbol a. Dann ist uviwx iy in L für alle i ∈ N für jede Zerlegung z = uvwxy von z,weil {b, c, d}∗ ⊆ L. 15Wir fordern nicht einmal, dass |z|≥ nL. 10.4 Kontextfreie Grammatiken und Kellerautomaten 321 (ii) Das Wort z enthält mindestens ein Symbol a, das heißt z = a nblc jdk für n, l, j, k ∈ N,n ≥ 1,l = j = k. In diesem Fall gibt es folgende Zerlegung von z: z = uvwxy mit u = λ, v = λ, w = λ, x = a und y = a n−1blc jdk, die alle drei Bedingungen (i), (ii) und (iii) des Pumping-Lemmas erfüllt, weil insbesondere uviwx iy = a n+i−1blc jdk ∈ L gilt für alle i ∈ N. Also gibt es für jedes Wort z ∈ L eine Zerlegung des Wortes z, so dass die Bedingungen (i), (ii) und (iii) des Pumping-Lemmas erfüllt sind. Trotzdem ist L keine kontextfreie Sprache, und wir zeigen dies mit Hilfe des Lemmas von Ogden. Sei z = ab lc ldl ∈ L mit l ≥ nL. Wir markieren alle Symbole des Teilwortes bc ld. Aus der Bedingung (ii) des Lemmas von Ogden erhalten wir, dass jede Zerlegung z = uvwxy von z die Eigenschaft hat, dass vwx entweder kein Symbol b oder kein Symbol d enthält. Weil vx mindestens ein markiertes Symbol enthält, wird in dem Wort uv2wx 2y mindestens die Anzahl eines Buchstabens e ∈{b, c, d} erhöht, ohne eine andere Anzahl eines Buchstabens h ∈{b, c, d}−{e} zu erhöhen. Damit gilt uv2wx 2y/∈ L. ♦ Aufgabe 10.36. Finden Sie eine andere Sprache L/∈L2, für die keine Anwendung des Pumping- Lemmas zu dem Beweis von L/∈L2 führen kann, für die aber die Nichtkontextfreiheit von L durch das Lemma von Ogden bewiesen werden kann. Aufgabe 10.37. Beweisen Sie mit Hilfe des Lemmas von Ogden, dass folgende Sprachen nicht kontextfrei sind. (a) {albjc k | l, j, k ∈ N,l ̸= j, j ̸= k und l ̸= k}, (b) {0 n1 m0 k | n, m, k ∈ N,m =max{n, k}}, (c) {0 n1 n2 k | n, k ∈ N,k ̸= n}. Zur Beschreibung regulärer Sprachen haben wir Berechnungsmodelle wie endliche Automaten und nichtdeterministische endliche Automaten sowie den Generierungsmecha- nismus regulärer Grammatiken kennengelernt. Die kontextfreien Grammatiken stellen eine natürliche und einfache Form von Generierungsverfahren dar und unsere nächste Frage ist, ob man für die entsprechende Klasse der kontextfreien Sprachen auch ein Berechnungsmodell ﬁnden kann, das genau die Sprachen dieser Klasse akzeptiert. An- ders gesagt, wir hätten gerne eine Klasse von Maschinen, so dass für jede kontextfreie Grammatik G eine zu G äquivalente Maschine aus dieser Klasse existiert und umgekehrt. Im Folgenden zeigen wir, dass ein solches Berechnungsmodell existiert. Dabei ist das Wichtigste, dass dieses zu kontextfreien Grammatiken äquivalente Maschinenmodell nicht 322 10 Grammatiken und Chomsky-Hierarchie ... Eingabeband Z0 X a q endliche Kontrolle Keller Ein-Weg-Lesekopf Abbildung 10.4 künstlich erzeugt wird. Das Umgekehrte gilt. Das Modell ist im Gegenteil unter den Maschinenmodellen genauso natürlich wie die kontextfreien Grammatiken unter den Grammatiken. Dieses Berechnungsmodell heißt nichtdeterministischer Kellerauto- mat. Es nutzt die Datenstruktur des Kellers (Last-in-ﬁrst-out-Speicherstrategie), die die natürlichste Datenstruktur zur Implementierung rekursiver Programme ist. Wir beschreiben zuerst das Modell nichtdeterministischer Kellerautomaten auf eine anschauliche, nicht vollständig formale Weise. Ein Kellerautomat hat ein Eingabeband, das am Anfang das Eingabewort enthält. Dieses Eingabeband kann genau wie bei endlichen Automaten nur eingeschränkt benutzt werden. Der Lesekopf kann das Eingabeband nur lesen und darf sich nur von links nach rechts bewegen. Dadurch dient das Eingabeband nur zum Einlesen der Eingabe und kann nicht (wie bei einer Turingmaschine) als Speicher benutzt werden. Es gibt aber einen Unterschied zu endlichen Automaten in der Arbeit mit dem Eingabeband. Ein Kellerautomat ist nicht gezwungen, sich nach dem Lesen eines Buchstabens aus dem Eingabeband mit dem Lesekopf nach rechts zu bewegen. Er kann mit dem Kopf eine gewisse Zeit auf dem gleichen Feld des Bandes stehen bleiben und in dieser Zeit einige Rechnungen auf den Daten im Keller durchführen. Die endliche Kontrolle ist wie bei allen bisherigen Rechnermodellen durch eine endliche Menge von Zuständen gesteuert. Den Keller kann man als potenziell unendliches Band mit eingeschränkter Nutzung betrachten. Der Kellerautomat kann nur auf das Top-Symbol ganz oben im Keller zugreifen und somit nur dieses Symbol lesen (Abbildung 10.4). Wenn man den Keller nicht vertikal wie in Abbildung 10.4, sondern horizontal wie in Abbildung 10.5 zeichnet, dann bedeutet dies, dass man nur den Inhalt der am weitesten rechts stehenden nichtleeren Zelle des Bandes lesen kann. Wenn man sich irgendwelche Daten anschauen will, die tiefer im Keller vorliegen, dann geht dies nur so, dass man zuerst darüber liegende Daten löschen und dadurch auch unwiderruﬂich vergessen muss, um die gewünschten Daten ganz oben zugreifbar im Keller zu haben. Genauer gesagt hat die Transitionsfunktion drei Argumente: 10.4 Kontextfreie Grammatiken und Kellerautomaten 323 .. ... .Z0 X Keller unendlich viele leere Speicherzellen Abbildung 10.5 • das gelesene Symbol auf dem Eingabeband, • den Zustand der endlichen Kontrolle, • das Top-Symbol des Kellers. In einer Aktion kann der Kellerautomat den Zustand ändern, den Lesekopf maximal ein Feld nach rechts bewegen und das Top-Symbol X des Kellers durch ein Wort α ersetzen. Wenn das Wort α das leere Wort λ ist, dann betrachten wir dies als das Löschen des Top-Symbols X, was die Schrumpfung des Kellerinhaltes bedeutet. Wenn das Top-Symbol X durch ein nichtleeres Wort α = Y1Y2 ...Yk ersetzt wird, dann steht danach Y1 in dem Feld, wo X vorher stand, und die nächsten k − 1 Symbole Y2,...,Yk liegen jeweils in den nachfolgenden, vorher leeren k − 1 Feldern des Kellers. Wenn die ganze Eingabe gelesen wurde,16 betrachtet man zwei Möglichkeiten, das Akzeptieren der Eingabe zu deﬁnieren. Eine Möglichkeit ist, die Akzeptierung über eine ausgezeichnete Menge von akzeptierenden Zuständen zu deﬁnieren wie bei endlichen Automaten. Die zweite Möglichkeit, die wir auch in der folgenden formalen Deﬁnition vorziehen, ist zu akzeptieren, wenn der Keller leer ist (geleert wurde). Am Anfang jeder Berechnung steht im Keller ein einziges Sonderzeichen Z0 und so weiß der Kellerautomat, wann er ganz unten im Keller ist und entscheidet selbst, ob er dieses letzte Zeichen in dem Keller löschen will oder nicht. Deﬁnition 10.6. Ein nichtdeterministischer Kellerautomat 17 (NPdA) ist ein 6- Tupel M =(Q, Σ, Γ,δ,q0,Z0), wobei die Elemente des Tupels folgende Bedeutung haben: (i) Q ist eine endliche Menge, die Zustandsmenge genannt wird, (ii) Σ ist das Eingabealphabet, {Wie üblich werden genau alle Wörter aus Σ ∗ als mögliche Eingaben betrachtet.} (iii) Γ ist das Kelleralphabet, {Γ beinhaltet alle Symbole, die im Laufe von Berechnungen im Keller auftreten dürfen, analog zum Arbeitsalphabet einer Turingmaschine.} (iv) q0 ∈ Q ist der Anfangszustand, {Mit der gleichen Bedeutung wie bei endlichen Automaten.} (v) Z0 ∈ Γ ist das Initialisierungssymbol des Kellers, {Jede Berechnung von M startet in q0 mit Z0 als einzigem Inhalt des Kellers.} 16wie bei endlichen Automaten 17„nondeterministic pushdown automaton“ im Englischen 324 10 Grammatiken und Chomsky-Hierarchie (vi) δ ist eine Abbildung von Q × (Σ ∪{λ}) × Γ in endliche Teilmengen von Q × Γ∗. {Wenn man λ als zweites Argument hat, spiegelt dies die Situation wieder, in der M das Eingabesymbol nicht lesen will und damit in seiner Aktion den Lesekopf nicht bewegen wird. Wenn das zweite Argument ein Symbol a ∈ Σ ist, wird der Lesekopf immer automatisch ein Feld nach rechts voranschreiten.} Eine Konﬁguration von M ist ein Tripel (q, w, α) ∈ Q × Σ∗ × Γ ∗, wobei • q ∈ Q der aktuelle Zustand ist, • w ∈ Σ∗ der bisher nicht gelesene Teil der Eingabe ist, {Der Lesekopf zeigt auf das Feld mit dem ersten Symbol von w, falls w ̸= λ.} • α ∈ Γ∗ der aktuelle Kellerinhalt ist. Ein Schritt M von M ist eine Relation auf Konﬁgurationen, deﬁniert durch (i) (q, aw, αX) M (p, w, αβ), falls (p, β) ∈ δ(q, a, X) für p, q ∈ Q, a ∈ Σ,X ∈ Γ und α, β ∈ Γ∗, und {M liest im Zustand q das Eingabesymbol a und das Kellersymbol X. Danach geht M in den Zustand p, bewegt den Lesekopf ein Feld nach rechts und ersetzt das Top-Symbol X des Kellers durch β.} (ii) (q, w, αX) M (p, w, αβ), falls (p, β) ∈ δ(q, λ, X). {M entscheidet sich im Zustand q, kein Eingabesymbol zu lesen, und deswegen ändert M nur den Zustand und ersetzt im Keller X durch β, ohne den Lesekopf auf dem Eingabeband zu bewegen.} Eine endliche Berechnung von M ist eine Folge von Konﬁgurationen C1, C2, ..., Cm, so dass Ci M Ci+1 für i =1, 2,...,m − 1 gilt. Für jedes Wort x ∈ Σ∗ ist (q0,x,Z0) die Anfangskonﬁguration von M auf x. Eine Berechnung C0,C1,...,Cm von M heißt eine akzeptierende Berechnung von M auf x, falls C0 =(q0,x,Z0) und Cm =(p, λ, λ) für irgendeinen Zustand p ∈ Q. Wie üblich18 bezeichnet M ∗ die reﬂexive und transitive Hülle der Relation M . Die vom Kellerautomaten M akzeptierte Sprache L(M ) ist deﬁniert durch L(M ) = {w ∈ Σ∗ | (q0,w,Z0) M ∗ (p, λ, λ) für ein p ∈ Q}. 18wie bei endlichen Automaten und Turingmaschinen 10.4 Kontextfreie Grammatiken und Kellerautomaten 325 Wir beobachten, dass ein NPdA mit leerem Keller nicht arbeiten kann. Wenn daher in einer Berechnung der Keller geleert wurde und das Eingabewort noch nicht zu Ende gelesen wurde, wird in dieser Berechnung die Eingabe nicht akzeptiert. Andererseits liegt unten im Keller am Anfang das Sonderzeichen Z0, und wenn der NPdA dieses Initialisierungssymbol nirgendwo anders verwendet oder es nie durch ein anderes Symbol ersetzt, dann weiß er immer, wann er auf dem untersten Boden des Kellers ist.19 Eine andere wichtige Bemerkung ist, dass ein NPdA auf einer Eingabe auch unendliche Berechnungen haben kann. Dies kommt dadurch zustande, dass er beliebig viele Schritte machen kann, ohne ein Eingabesymbol zu lesen (sogenannte λ-Schritte). Zusätzlich sollen wir nicht vergessen, dass die Größe des Kellers nicht beschränkt ist. Ein NPdA benutzt zwei Möglichkeiten zur nichtdeterministischen Verzweigung von Berechnungen. Zuerst kann er sich entscheiden, ob er das Eingabesymbol liest oder einen λ-Schritt realisieren will. In diesen beiden Fällen kann er sich dann für die gegebenen Argumente aus einer endlichen Menge von Aktionen eine aussuchen. Im Folgenden geben wir zwei Beispiele von Kellerautomaten an. Beispiel 10.8. Wir bauen einen NPdA für die nicht reguläre Sprache L = {0n1n | n ∈ N}. Sei M =({q0,q1,q2}, {0, 1}, {0,Z0},δ,q0,Z0). Um λ akzeptieren zu können, nehmen wir δ(q0,λ,Z0)= {(q0,λ), (q1,Z0)}. Im Zustand q1 realisieren wir die Speicherung der Anzahl der Nullen des längsten Präﬁxes der Eingabe, das nur aus Nullen besteht: δ(q1, 0,Z0)= {(q1,Z00)}, δ(q1, 0, 0) = {(q1, 00)}. Beim ersten Auftreten des Symbols 1 wechselt M in den Zustand q2 und löscht in diesem Zustand jeweils eine 0 aus dem Keller für jede gelesene 1 auf dem Eingabeband. δ(q1, 1, 0) = {(q2,λ)}, δ(q2, 1, 0) = {(q2,λ)}. Durch δ(q2, 0,X)= ∅ für X ∈{0,Z0} garantieren wir, dass kein Wort mit mehr als einem Wechsel zwischen Nullen und Einsen bearbeitet und so akzeptiert werden kann. Mit der Aktion δ(q2,λ,Z0)= {(q2,λ)} 19Also triﬀt er gezielt die Entscheidung, den Keller zu leeren. 326 10 Grammatiken und Chomsky-Hierarchie akzeptieren wir alle Wörter der Form 0n1n für n ≥ 1. Wenn 0n1n nur ein echtes Prä- ﬁx der Eingabe ist, wird die Eingabe nicht akzeptiert, weil M mit dem leeren Keller nicht weiterarbeiten kann. Im Detail bedeutet dies, dass wir für alle nicht angegebenen Argumente die Menge der möglichen Aktionen auf ∅ setzen. Als Beispiel einer akzeptierenden Berechnung geben wir die Berechnung (q0, 0 31 3,Z0) M (q1, 031 3,Z0) M (q1, 0 213,Z00) M (q1, 013,Z000) M (q1, 13,Z0000) M (q2, 1 2,Z000) M (q2, 1,Z00) M (q2,λ,Z0) M (q2,λ,λ) von M auf 0 31 3. ♦ Aufgabe 10.38. Entwerfen Sie einen NPdA für die Sprache {0n1n | n ∈ N −{0}}, der nur einen Zustand hat. Aufgabe 10.39. Beweisen Sie, dass der Automat aus Beispiel 10.8 wirklich die Sprache L = {0n1n | n ∈ N} akzeptiert. Im Einzelnen bedeutet dies zu zeigen, dass für jedes Wort 0n1n eine akzeptierende Berechnung von M existiert und dass eine akzeptierende Berechnung nur auf einem Wort aus L vorkommen kann. Aufgabe 10.40. Entwerfen Sie nichtdeterministische Kellerautomaten für folgende Sprachen: (a) {0 n1 2n+3 | n ∈ N}, (b) {0 i1 j2 k | i, j, k ∈ N,k ≥ i + j}, (c) {0 i1 j2 k | i, j, k ∈ N,i = j oder k =2i}. Beispiel 10.9. Betrachten wir die Sprache LR = {w#wR | w ∈{0, 1}∗} über dem Alphabet {0, 1, #}. Wir beschreiben einen NPdA M mit L(M )= LR, ohne die detaillierte Darstellung von δ anzugeben. M arbeitet auf einer Eingabe x#y mit x ∈{0, 1} ∗ wie folgt. Jeder gelesene Buchstabe auf dem Eingabeband wird zuerst im Keller gespeichert. Somit erreicht M einmal die Konﬁguration (q0, #y, Z0x). Nach dem ersten Lesen von # ändert M den Zustand und beginnt, das oberste Kellersym- bol mit dem gelesenen Eingabesymbol zu vergleichen. Wenn die Symbole übereinstimmen, wird das oberste Kellersymbol gelöscht. Auf diese Weise kann man überprüfen, ob y = x R, weil das Wort x so in dem Keller gespeichert wird, dass es vom Ende zum Anfang aus dem Keller gelesen wird. Wenn M den Kellerboden mit dem Symbol Z0 erreicht, erhält er die Möglichkeit, in einem λ-Schritt den Keller zu leeren. ♦ Aufgabe 10.41. Geben Sie eine formale Beschreibung des NPdA aus Beispiel 10.9 an. Aufgabe 10.42. Entwerfen Sie nichtdeterministische Kellerautomaten für folgende Sprachen: (a) {wwR | w ∈{a, b}∗}, 10.4 Kontextfreie Grammatiken und Kellerautomaten 327 (b) {wuwR | w ∈{a, b}∗,u ∈{0, 1} ∗}, (c) {w ∈{0, 1} ∗ ||w|0 = |w|1}, (d) {w ∈{0, 1} ∗ ||w|0 ≤ 3|w|1 +4}. Aufgabe 10.43.∗ Deﬁnieren Sie einen nichtdeterministischen Kellerautomaten, der ähnlich wie ein NEA mit akzeptierenden Zuständen akzeptiert, d. h., er akzeptiert, wenn das ganze Eingabewort gelesen wurde und er sich in einem akzeptierenden Zustand beﬁndet. Zeigen Sie, dass dieses Modell von NPdA äquivalent zu unserem Modell ist, d. h., zu jedem NPdA, der mit akzeptierenden Zuständen akzeptiert, existiert ein äquivalenter NPdA, der mit dem leeren Keller akzeptiert, und umgekehrt. Jetzt zeigen wir, dass NPdAs genau die Klasse der kontextfreien Sprachen akzeptieren. Wir beginnen mit der einfacheren Richtung, zu jeder kontextfreien Grammatik einen äquivalenten NPdA zu konstruieren. Lemma 10.8. Sei L eine kontextfreie Sprache, die λ nicht enthält. Dann existiert ein NPdA M , so dass L = L(M ) gilt. Beweis. Sei G =(ΣN, ΣT,P,S) eine kontextfreie Grammatik in Greibach-Normalform mit L = L(G). Die Idee ist, einen NPdA M mit nur einem Zustand zu konstruieren, so dass für jede Linksableitung 20 S ⇒ ∗ G wα mit w ∈ (ΣT) ∗ und α ∈ (ΣN) ∗ eine Berechnung von M existiert, in der bisher das Wort w gelesen wurde und der Keller das Wort αR enthält. Dieses soll deswegen möglich sein, weil G in jedem Ableitungsschritt ein Terminalsymbol generiert, was man durch das Lesen dieses Symbols auf dem Eingabeband von M nachahmen kann. Formal setzen wir M =({q0}, ΣT, ΣN,δ,q0,S), wobei δ(q0,a,A)= {(q0,αR) | A → aα ∈ P } für alle a ∈ ΣT und A ∈ ΣN. Wir beweisen jetzt L(G)= L(M ), indem wir für jedes i ∈ N zeigen: G kann genau dann in i Linksableitungsschritten die Satzform a1 ...aiα für aj ∈ ΣT und α ∈ (ΣN)∗ erreichen (das heißt S ⇒i G a1 ...aiα), wenn M in i Schritten a1 ...ai lesen und dabei die Konﬁguration (q0,x,αR) für ein beliebiges x erreichen kann. Wir beweisen diese Aussage mit Induktion bezüglich i. • Induktionsanfang. Für i = 0 ist die Behauptung trivial. 20Wir bemerken, dass alle Wörter einer Linksableitung einer Grammatik in Greibach-Normalform aus (ΣT)∗ · (ΣN)∗ sein müssen. 328 10 Grammatiken und Chomsky-Hierarchie a1a1 a2a2 aiai q0q0 xx A A → aiγ βRβR γR ...... Abbildung 10.6 • Induktionsschritt. Sei nun i ≥ 1 und die Behauptung gelte für alle j< i.Sei S ⇒ i−1 G a1a2 ...ai−1Aβ ⇒G a1a2 ...ai−1aiγβ die Linksableitung von a1 ...aiα in G mit α = γβ. Im letzten Ableitungsschritt wurde die Regel A → aiγ angewendet. Der Berechnungsschritt des NPdA M , der der Anwendung der Regel A → aiγ in der Ableitung von G entspricht, ist in Abbildung 10.6 dargestellt. Aus der Konstruktion von M folgt dann (q0,γR) ∈ δ(q0,ai,A). (10.6) Durch die Induktionsannahme und die Anwendung von (10.6) erhalten wir für ein beliebiges x ∈ (ΣT) ∗ (q0,a1 ...ai−1aix, S) M i−1 (q0,aix, βRA) M (q0,x,βRγR). Weil βRγR =(γβ)R = αR, ist damit der Beweis abgeschlossen. Die Beweisrichtung von einer Berechnung von M zu einer Linksableitung von G ist analog und wir überlassen diese dem Leser. \u0002 Man bemerke, dass der in Lemma 10.8 konstruierte NPdA M nur einen Zustand besitzt und in Echtzeit arbeitet. Dies bedeutet, dass jede Berechnung von M aus höchstens so vielen Schritten besteht, wie die Eingabe lang ist. Aufgabe 10.44. Beweisen Sie Lemma 10.8, indem Sie im Beweis anstatt Grammatiken in Greibach-Normalform Grammatiken in Chomsky-Normalform verwenden. Können Sie in diesem Fall die Länge der Berechnungen bezüglich der Länge der Eingabe (des generierten Wortes) abschätzen? Aufgabe 10.45. Erweitern Sie die Aussage von Lemma 10.8 auch für kontextfreie Sprachen, die λ beinhalten. 10.4 Kontextfreie Grammatiken und Kellerautomaten 329 Jetzt wollen wir zu jedem NPdA M eine zu M äquivalente kontextfreie Grammatik konstruieren. Zuerst bemerken wir, dass diese Aufgabe einfach ist, wenn M nur einen Zustand hat, also wenn M unabhängig von seinen Zuständen arbeitet. Lemma 10.9. Sei M ein NPdA mit nur einem Zustand. Dann ist L(M ) kontextfrei. Aufgabe 10.46. Beweisen Sie Lemma 10.9. Hinweis: Die Konstruktion von G aus M ist nur eine Umkehrung der Konstruktion von M aus G in Lemma 10.8. Was können wir aber bei allgemeinen nichtdeterministischen Kellerautomaten tun? Überraschenderweise können wir zu jedem NPdA einen äquivalenten NPdA konstruieren, der nur einen Zustand besitzt. Die Idee dabei ist, die durch die Zustände gespeicherte Information im Keller zu speichern. Zum Beispiel würde man statt eines obersten Symbols X das Symbol (q, X) in den Keller schreiben, wobei q der aktuelle Zustand ist. Das geht so weit gut in Berechnungsschritten, in denen man etwas in den Keller schreibt. Wenn man aber nur das oberste Kellersymbol löscht, besteht die Möglichkeit nicht, den aktuellen Zustand in das oberste Symbol hineinzuschreiben (einzukodieren). Das folgende Lemma zeigt uns, wie man dieses Problem mit Hilfe des Nichtdeterminismus lösen kann. Lemma 10.10. Für jeden nichtdeterministischen Kellerautomaten M existiert ein äqui- valenter nichtdeterministischer Kellerautomat M1, der nur einen Zustand hat. Beweis. Sei M =(Q, Σ, Γ,δ,q0,Z0) mit |Q| > 1 ein Kellerautomat. Unsere Aufgabe ist jetzt, einen Kellerautomaten M1 =(Q1, Σ, Γ1,δ1,q1 0,Z 1 0 ) mit Q1 = {q1 0} und L(M )= L(M1) zu konstruieren. Die einfachste Idee wäre, die Arbeitssymbole aus Γ1 als Paare (p, A) ∈ Q × Γ zu betrachten und auf diese Weise den aktuellen Zustand p ∈ Q immer in dem obersten Kellersymbol zur Verfügung zu haben. Diese Idee funktioniert aber nicht, weil bei der Simulation von Berechnungsschritten, in denen das oberste Kellersymbol ohne Ersatz gelöscht wird, keine Möglichkeit besteht, den neuen aktuellen Zustand zu speichern (die Information über diesen Zustand zu bewahren). Deswegen wählen wir Γ1 = Q × Γ × Q, wobei die Bedeutung von A ′ =(q, A, p) ∈ Γ1 ist, dass der Kellerautomat M das Symbol A nur bei einem Übergang vom Zustand q in den Zustand p löschen darf. Wenn A′ das oberste Symbol des Kellers von M1 ist, ist q der aktuelle Zustand von M . Wie soll uns diese Idee mit einem zweiten gespeicherten Zustand helfen? 330 10 Grammatiken und Chomsky-Hierarchie qA1 p1 p1 A2 p2 p2 A3 p3 p3 A4 p4 ... pi Ai+1 pi+1 pi+1 Ai+2 pi+2 ... Abbildung 10.7 Wir wollen immer darauf achten, dass der Kellerinhalt wie in Abbildung 10.7 aussieht. Formal bedeutet dies, wenn ein Tripel (q, A, p) direkt über einem Tripel (r, B, s)im Keller liegt, so muss p = r sein. Wenn dies so ist und wir das Löschen von A1 in M (von (q, A, p)in M1) nur beim Übergang von q nach p erlauben, dann enthält das neue oberste Kellersymbol (r, B, s)=(p, B, s) den aktuellen neuen Zustand p. Wie erreicht man, dass der Keller immer einen Inhalt wie in Abbildung 10.7 enthält? Dazu nutzt man den Nichtdeterminismus von M1.Wenn M aus dem Zustand r nach s übergeht und dabei das oberste Kellersymbol A durch CDE ersetzt (das heißt (s, CDE) ∈ δ(r, a, A) für ein a ∈ Σ), dann ersetzt M1 das oberste Symbol (r, A, h) durch die drei Symbole (s, C, \u0002), (\u0002,D, △), (△,E,h) für eine beliebige Wahl von \u0002 und △ aus Q und rät damit, bei welchen Übergängen diese Symbole gelöscht werden dürfen. Wenn (r, A, h) nicht das oberste Symbol ist, dann scheitert M1 in dieser Berechnung. Wichtig ist zu bemerken, dass der aktuelle Zustand s als erstes Element des obersten Tripels steht und dass die ehemalige Position des überschriebenen (r, A, h) nur beim Übergang in denselben Zustand h gelöscht werden kann. Weil unter (r, A, h) irgendein (h, ·, ·) liegt, liegt jetzt (h, ·, ·)unter (△,E,h) und damit ist die Konsistenz gewährleistet. 10.4 Kontextfreie Grammatiken und Kellerautomaten 331 .. . ... ... aa bb r s A D D B1 B2 Bm ... ... ...... Abbildung 10.8 Jetzt setzen wir dieses Konzept in die formale Konstruktion von M1 um. Q1 = {q1 0}, Γ1 = {Z 1 0 }∪ Q × Γ × Q, δ1(q1 0,λ,Z 1 0 )= {(q1 0, (q0,Z0,q)) | q ∈ Q}, δ1(q1 0,a, (r, A, h)) = {(q1 0, ((s, B1,s2)(s2,B2,s3) ... (sm,Bm,h)) R) | für alle (s, (B1B2 ...Bm) R) ∈ δ(r, a, A) und alle s2,s3,...,sm ∈ Q} ∪{(q1 0,λ) | (h, λ) ∈ δ(r, a, A)} für alle a ∈ Σ ∪{λ}, A ∈ Γ, r, s, h ∈ Q (siehe Abbildungen 10.8 und 10.9). \u0002 Lemmata 10.8 bis 10.10 ergeben zusammen das folgende Resultat. Satz 10.5. Die nichtdeterministischen Kellerautomaten erkennen genau die Klasse der kontextfreien Sprachen (d. h., die Formalismen der nichtdeterministischen Kellerautomaten und kontextfreien Grammatiken sind bezüglich ihrer Beschreibungsstärke äquivalent). Aufgabe 10.47. Betrachten Sie den NPdA M mit L(M )= {0n1n | n ∈ N} aus Beispiel 10.8. Wenden Sie die Konstruktion aus Lemma 10.10 an, um einen äquivalenten NPdA M mit nur einem Zustand zu konstruieren. 332 10 Grammatiken und Chomsky-Hierarchie ... ...... aa bb q1 0q1 0 r h h h h u u A D D B1 B2 Bm−1 Bm s s2 s2 s3 sm−1 sm sm ............ Abbildung 10.9 10.5 Allgemeine Grammatiken und Turingmaschinen Grammatiken ohne Einschränkungen sind ein starker Generierungsmechanismus. Die Zielsetzung dieses Abschnitts ist zu zeigen, dass allgemeine Grammatiken der stärkste For- malismus zur Sprachgenerierung sind. Wir zeigen dies durch den Beweis ihrer Äquivalenz zu Turingmaschinen, was gleichzeitig die Church’sche These untermauert. Um eine Intuition für die Stärke allgemeiner Grammatiken zu gewinnen, starten wir ähnlich wie bei Turingmaschinen und konstruieren eine Grammatik für die Sprache L = {0 2n | n ∈ N}. Sei G =(ΣN, ΣT,P,S) mit ΣT = {0} und ΣN = {A, B, C, D, S}. Wir beginnen mit den Startregeln S → CA0D und S → 0, die uns ermöglichen, das Wort 0 abzuleiten und die Anfangssatzform CA0D mit einer 0 zu erzeugen. Die nächste Idee ist, die Anzahl Nullen zu verdoppeln, indem A in der bisher abgeleiteten Satzform von links nach rechts „wandert“ und jede 0 gegen zwei Nullen austauscht. A0 → 00A und AD → BD. 10.5 Allgemeine Grammatiken und Turingmaschinen 333 Ähnlich kann B von rechts nach links wandern und die Anzahl der Nullen wieder verdop- peln. 0B → B00 und CB → CA. Am Rand der bisher abgeleiteten Satzform kann man sich entscheiden, die Ableitung zu beenden, indem man alle Nichtterminalsymbole löscht. Am linken Rand erreicht man dies durch die Regeln CB → λ und D → λ. Wenn man sich am rechten Rand entscheidet, die Ableitung zu beenden, dann kann man folgende Regeln benutzen: AD → λ und C → λ. Aufgabe 10.48. Die Regeln D → λ und C → λ in der oben beschriebenen Grammatik kann man unabhängig davon anwenden, wo sich A oder B in der bisher abgeleiteten Satzform beﬁnden. Warum gehen wir dabei kein Risiko ein, dass eine unerwünschte Anzahl von Symbolen 0 generiert wird? Aufgabe 10.49. Konstruieren Sie eine Grammatik für jede der folgenden Sprachen: (a) {anbnc n | n ∈ N}, (b) {0 n1 n 2 | n ∈ N −{0}}, (c) {0 3n | n ∈ N}, (d) {aibjc k | k = i · j, k, i, j ∈ N −{0}}, (e) {0 p | p ist eine Primzahl}. Die Konstruktion der Grammatik G deutet auf die Ähnlichkeit zu Turingmaschinen hin. Ähnlich wie der Zustand in einer Konﬁguration über das Band wandert, bewegen sich die Nichtterminale A und B über die bisher abgeleitete Satzform. Also besteht die Möglichkeit, durch Nichtterminale die Zustände zu repräsentieren und die Arbeit einer Turingmaschine nachzuahmen. Satz 10.6. LRE = L0, d. h., die Menge der rekursiv aufzählbaren Sprachen ist genau die gleiche Menge wie die Menge aller Sprachen, die durch Grammatiken generiert werden können. Beweis. Wir führen den Beweis, indem wir nacheinander L0 ⊆LRE und LRE ⊆L0 zeigen. (i) L0 ⊆LRE. Sei G eine beliebige Grammatik. Wir sollen eine TM M konstruieren, so dass L(G)= L(M ) gilt. Weil die Idee sehr anschaulich ist, verzichten wir auf die formale Konstruktion von M . Ohne Beschränkung der Allgemeinheit betrachten wir M als eine nichtdeterministi- sche 1-Band-TM. Die 1-Band-TM M soll für jede gegebene Eingabe w ∈ L auf dem 334 10 Grammatiken und Chomsky-Hierarchie Eingabeband akzeptieren.21 Dies schaﬀt sie, indem sie eine Ableitung aus S in G auf dem Arbeitsband nachahmt und das Resultat mit der Eingabe vergleicht. Genauer kann dies wie folgt ablaufen. M schreibt S auf das Arbeitsband. Dann geht M wiederholt wie folgt vor. 1. M wählt nichtdeterministisch eine nichtleere Position i auf dem Arbeitsband und eine Regel α → β aus G. 22 2. M überprüft, ob α auf dem Band rechts von der i-ten Position liegt. Wenn nicht, endet M in qreject. Wenn ja, ersetzt 23 M das Teilwort α durch β. 3. M überprüft, ob der Inhalt des Arbeitsbandes ein Terminalwort ist. Falls nein, setzt M mit Schritt 1 fort. Falls ja, vergleicht M das Eingabewort mit dem abgeleiteten Wort auf dem Arbeitsband. Falls die Wörter gleich sind, akzeptiert M die Eingabe. Sonst verwirft M die Eingabe. (ii) LRE ⊆L0. Sei M =(Q, Σ, Γ,δ,q0,qaccept,qreject) eine TM. Eine Grammatik G kann M so simulieren, dass G zuerst „doppelspurig“ ein beliebiges Wort erzeugt. Die obere Spur beinhaltet das Wort, die untere Spur die Startkonﬁguration von M auf diesem Wort. Dieses kann durch folgende Regeln für alle a ∈ Σ bewirkt werden: S → ( a q0a ),S → ( a q0a )A, A → (a a )A, A → (a a ),A → (B B )C, C → (B B )C, C → (B B ). Somit erhalten wir die Ableitung S ⇒ ∗ G ( a1 q0a1 )(a2 a2 )(a3 a3 ) ... (an−1 an−1 )(an an )(B B )j für ein beliebiges Wort a1a2 ...an (ai ∈ Σ für i =1,...,n) und ein beliebiges j ∈ N. Dabei ist j eine nichtdeterministisch geratene Zahl, so dass die TM M bei der Arbeit auf der Eingabe a1a2 ...an genau n + j Zellen des Bandes benutzt. Der Großbuchstabe B symbolisiert eine leere Zelle des Bandes. Danach simuliert G auf der unteren Spur die Berechnung der TM M auf a1a2 ...an. Die Simulation kann man durch folgende Regeln realisieren: Für alle a, b ∈ Σ ∪{B},p, q ∈ Q, X, Y, Z ∈ Γ ∪{B}, so dass (p, Y, R) ∈ δ(q, X), nehmen wir die Regel ( a qX )( b Z ) → ( a Y )( b pZ ). 21Für eine Eingabe w/∈ L kann M verwerfen oder unendlich lange arbeiten. 22M kann die Regel α → β in ihrem Zustand speichern. 23Wenn |α| ̸= |β|, erfordert dies natürlich die Verschiebung des Suﬃxes des Arbeitsbandes rechts von α. 10.6 Zusammenfassung 335 Für alle (p, Y, L) ∈ δ(q, X) nehmen wir ( a Z )( b qX ) → ( a pZ )( b Y ). Für alle (p, Y, N) ∈ δ(q, X) nehmen wir ( a qX ) → ( a pY ). Falls M den Zustand qaccept erreicht, ist a1 ...an in L(M ) und somit muss dieses Terminalwort in G ableitbar sein. Dies erreichen wir durch folgende Regeln, die die untere Spur auﬂösen: ( a qacceptα ) → a, ( b α )a → ba, a ( b α ) → ab, B → λ für alle a, b ∈ Σ ∪{B},α ∈ Γ ∪{B}. Auf den formalen Beweis der Korrektheit dieser Konstruktion verzichten wir hier. \u0002 Aufgabe 10.50. Beweisen Sie, dass L1 = NSPACE(n). 10.6 Zusammenfassung Grammatiken sind Mechanismen zur Generierung von Sprachen und damit für eine endliche Darstellung von Sprachen eine Alternative zu Maschinenmodellen. Die Ausdrucksstärke allgemeiner Grammatiken entspricht der Ausdrucksstärke der Turingmaschinen, was eine weitere Unterstützung der Glaubwürdigkeit der Church’schen These ist. Die Grammatiken werden nach Chomsky klassiﬁziert. Die einfachste Form sind die regulären Grammatiken, die genau die Beschreibungsstärke endlicher Automaten haben. Die wichtigste Klasse von Grammatiken sind die kontextfreien Grammatiken. Aus linguistischer Sicht ist diese Klasse wichtig, weil die kontextfreien Regeln der Ersetzung von Nichtterminalsymbolen unabhängig von deren Umgebung (Kontext) entsprechen, und der Grad der Kontextsensitivität zentral für die Untersuchung der Generierung von natürlichen Sprachen ist. In der Informatik kann man Programmiersprachen mit Hilfe kontextfreier Grammatiken beschreiben, und damit sind die kontextfreien Grammatiken ein wichtiges Werkzeug zum Compilerbau. Die nichtdeterministischen Kellerautomaten stellen ein natürliches Maschinenmodell zur Implementierung der Rekursion dar und sind in ihrer Beschreibungsstärke äquivalent zu den kontextfreien Grammatiken. Die Klasse der regulären Sprachen ist eine echte Teilmenge der Menge der kontextfreien Sprachen, was relativ einfach durch die Anwendung der Pumpingtechnik bewiesen werden kann. Diese pumpingbasierte Beweismethode kann man weiterentwickeln, um Instrumente zu bauen, mit denen man die Nichtkontextfreiheit von Sprachen beweisen kann. Dank der Anwendung der Pumpingtechniken wissen wir, dass die Klasse der kontextfreien Sprachen eine echte Teilmenge der Klasse der kontextsensitiven Sprachen ist. 336 10 Grammatiken und Chomsky-Hierarchie Kontrollaufgaben 1. Erklären Sie das Konzept der Grammatiken. Was ist eine Ableitung? Generiert jede Ableitung in einer Grammatik G ein Terminalwort aus L(G)? 2. Die Grammatiken sind nichtdeterministische Generierungsmechanismen. Wo überall tritt der Nichtdeterminismus auf? 3. Welche Grammatiktypen kennen wir und nach welchen Kriterien werden sie klassiﬁziert? 4. Wir können die Nichtterminale regulärer Grammatiken als Informationsträger deuten, die die wichtigsten Merkmale des bisher abgeleiteten Präﬁxes repräsentieren. Dies bietet uns eine ähnliche Entwurfsmethode von regulären Grammatiken wie wir sie durch die Zuordnung der Bedeutung zu einzelnen Zuständen für den Entwurf endlicher Automaten gewonnen haben. Nutzen Sie diese Strategie, um reguläre Grammatiken für die regulären Sprachen aus Aufgabe 3.7 zu entwerfen. Beweisen Sie die Korrektheit für mindestens eine der entworfenen Grammatiken. 5. Welche Abgeschlossenheitseigenschaften der Klasse L3 = LEA kann man einfach mittels regulärer Grammatiken beweisen? Für welche Operationen über Sprachen würde man eher die endlichen Automaten benutzen? 6. Beschreiben Sie Algorithmen, die Folgendes tun, sodass die erzeugte Sprache sich nicht ändert: (i) Entfernung aller nutzlosen Symbole aus einer Grammatik. (ii) Entfernung aller Kettenregeln aus einer regulären Grammatik. (iii) Entfernung aller Regeln X → λ (für X ̸= S) aus einer regulären Grammatik. 7. Obwohl endliche Automaten ein Maschinenmodell sind und reguläre Grammatiken einen Generierungsmechanismus darstellen, haben diese Modelle eine starke Ähnlichkeit in ihrer Arbeitsweise. Wie würden Sie diese Ähnlichkeit speziﬁzieren und wie nutzt man sie zur gegenseitigen Simulation aus? 8. Entwerfen Sie kontextfreie Grammatiken für folgende Sprachen: (a) {a ibjc kd l | i, j, k, l ∈ N,i ̸= j und k =2l}, (b) {a ibjc kd l | i, j, k, l ∈ N,j ̸= k oder i ̸= l}, (c) {a ibjc kd l | i, j, k, l ∈ N,j ̸= k und i ̸= l}, (d) {0 n1 2n2 i | n, i ∈ N}, (e) {0 i1 j2 k | i, j, k ∈ N,i + k = j}, (f) {w000111wR | w ∈{0, 1} ∗}. Wandeln Sie die entworfenen kontextfreien Grammatiken in die Chomsky-Normalform um. 9. Entwerfen Sie eine kontextfreie Grammatik für die Sprache L = {a nbic j | n, i, j ∈ N,i + j = n}, die für jedes Wort aus L mindestens zwei unterschiedliche Ableitungsbäume hat. 10. Welche der folgenden Sprachen ist kontextfrei? Beweisen Sie ihre Behauptung: 10.6 Zusammenfassung 337 (a) {w0 n1 nwR | w ∈{0, 1} ∗,n ∈ N}, (b) {wwRw | w ∈{0, 1} ∗}, (c) {0 n1 n! | n ∈ N}, (d) {wwRuuR | u, w ∈{0, 1}∗}, (e) {aibjc kd l | i, j, k, l ∈ N,i + l = j + k}, (f) {a nbmc 2nd 2m | n, m ∈ N}. 11. Führen Sie den Beweis des Pumping-Lemmas für kontextfreie Sprachen, indem Sie an- nehmen, dass Sie eine Grammatik in Chomsky-Normalform haben. Wie wird durch diese Annahme der Beweis vereinfacht? Wäre es auch eine Erleichterung, eine Grammatik in Greibach-Normalform zu nehmen? 12. Entwerfen Sie nichtdeterministische Kellerautomaten, die die Sprachen aus Kontroll- aufgabe 8 akzeptieren. Wandeln Sie mindestens einen dieser Kellerautomaten in einen äquivalenten NPdA um, der nur einen Zustand hat. Konstruieren Sie dann zu diesem NPdA mit einem Zustand eine äquivalente kontextfreie Grammatik. 13. Konstruieren Sie kontextsensitive Grammatiken für die Sprachen {02n | n ∈ N} und {anbnc n | n ∈ N}. Literaturverzeichnis [AB09] S. Arora and B. Barak. Computational Complexity. Cambridge University Press, 2009. [ACG +99] G. Ausiello, P. Crescenzi, G. Gambosi, V. Kann, A. Marchetti-Spaccamela, and M. Protasi. Complexity and Approximation. Combinatorial Optimization Problems and their Approximability Properties. Springer-Verlag, 1999. [BC94] D.P. Bovet and P. Crescenzi. Introduction to the Theory of Complexity. Prentice-Hall, 1994. [BDG88] J.L. Balcázar, J. Díaz, and J. Gabarró. Structural Complexity I. Springer- Verlag, 1988. [BDG90] J.L. Balcázar, J. Díaz, and J. Gabarró. Structural Complexity II. Springer- Verlag, 1990. [Ben64] V. Beneš. Permutation groups, complexes and rearrangable multistage connec- ting networks. Bell System Technical Journal, 43:1619–1640, 1964. [Ben65] V. Beneš. Mathematical Theory of Connecting Networks and Telephone Traﬃc. Academic Press, 1965. [Boc58] F. Bock. An algorithm for solving “traveling-salesman” and related network optimization problems: Abstract. Bulletin of the 14th National Meeting of the Operations Research Society of America, page 897, 1958. [Čer85] V. Černý. A thermodynamical approach to the traveling salesman problem: An eﬃcient simulation algorithm. Journal of Optimization Theory and Appli- cations, 45:41–55, 1985. [Cha66] G.J. Chaitin. On the length of programs for computing ﬁnite binary sequences. Journal of the ACM, 13:407–412, 1966. [Cha69] G.J. Chaitin. On the simplicity and speed of programs for computing deﬁnite sets of natural numbers. Journal of the ACM, 16:407–412, 1969. [Cha74] G.J. Chaitin. Information-theoretic limitations of formal systems. Journal of the ACM, 13:403–424, 1974. [Chu36] A. Church. An undecidable problem in elementary number theory. American Journal of Mathematics, 58:345–363, 1936. [CLR90] T. Cormen, C. Leiserson, and R.L. Rivest. Introduction to Algorithms. MIT Press and McGraw-Hill, 1990. J. Hromkovič, Theoretische Informatik, DOI 10.1007/978-3-658-06433-4, © Springer Fachmedien Wiesbaden 2014 340 Literaturverzeichnis [Coo71] S. Cook. The complexity of theorem-proving procedures. In Proceedings of 3rd ACM Symposium on Theory of Computing, pages 151–157, 1971. [Cro58] G.A. Croes. A method for solving traveling-salesman problems. Operations Research, 6:791–812, 1958. [DH76] W. Diﬃe and M. Hellman. New directions in cryptography. IEEE Transactions on Information Theory, 22(6):644–654, 1976. [DK02] H. Delfs and H. Knebl. Introduction to Cryptography. Springer-Verlag, 2002. [EP00] K. Erk and L. Priese. Theoretische Informatik (Eine umfassende Einführung). Springer-Verlag, 2000. [GJ79] M. Garey and D. Johnson. Computers and Intractability. Freeman, 1979. [GKP94] R. Graham, D. Knuth, and O. Patashnik. Concrete Mathematics. Addison- Wesley, 2nd edition, 1994. [GMR85] S. Goldwasser, S. Micali, and C. Rackoﬀ. Knowledge complexity of interactive proofs. In Proceedings of the 17th ACM Symposium on Theory of Computation, pages 291–304. ACM, 1985. [Göd31] K. Gödel. Über formal unentscheidbare Sätze der Principia Mathematica und verwandter Systeme. Monatshefte für Mathematik und Physik, 38:173–198, 1931. [Gra66] R. Graham. Bounds for certain multiprocessor anomalies. Bell System Technical Journal, 45:1563–1581, 1966. [Har93] D. Harel. Algorithmics. The Spirit of Computing. Addison-Wesley, 1993. [HKMP96] J. Hromkovič, R. Klasing, B. Monien, and R. Peine. Dissemination of infor- mation in interconnection networks. In Combinational Network Theory, pages 125–212, 1996. [HKP +05] J. Hromkovič, R. Klasing, A. Pelc, P. Ružička, and W. Unger. Dissemination of Information in Communication Networks. Broadcasting, Gossiping, Leader Election and Fault-Tolerance. Springer-Verlag, 2005. [HMU06] J.E. Hopcroft, R. Motwani, and J.D. Ullman. Introduction to Automata Theory, Languages, and Computation. Addison-Wesley, 3rd edition, 2006. [Hoc97] D.S. Hochbaum. Approximation Algorithms for NP-hard Problems. PWS Publishing Company, 1997. [Hro97] J. Hromkovič. Communication Complexity and Parallel Computing. Springer- Verlag, 1997. Literaturverzeichnis 341 [Hro04a] J. Hromkovič. Algorithmics for Hard Problems. Introduction to Combinatorial Optimization, Randomization, Approximation and Heuristics. Springer-Verlag, 2nd edition, 2004. [Hro04b] J. Hromkovič. Randomisierte Algorithmen. Methoden zum Entwurf von zu- fallsgesteuerten Systemen für Einsteiger. B.G. Teubner, 2004. [HS65] J. Hartmanis and R.E. Stearns. On the computational complexity of algo- rithms. Transactions of AMS, 117:285–306, 1965. [HSL65] J. Hartmanis, R. Stearns, and P. Lewis. Hierarchies of memory limited computations. In Proceedings of 6th IEEE Symposium on Switching Circuit Theory and Logical Design, pages 179–190, 1965. [HU79] J.E. Hopcroft and J.D. Ullman. Introduction to Automata Theory, Languages, and Computation. Addison-Wesley, 1979. [IK74] O.H. Ibarra and C.E. Kim. Fast approximation algorithms for the knapsack and sum of subsets problem. Journal of the ACM, 21:294–303, 1974. [Kar72] R.M. Karp. Reducibility among combinatorial problems. In R. Miller, editor, Complexity of Computer Computation, pages 85–104. Plenum Press, 1972. [Kar91] R.M. Karp. An introduction to randomized algorithms. Discrete Applied Mathematics, 34:165–201, 1991. [KGV83] S. Kirkpatrick, P.D. Gellat, and M.P. Vecchi. Optimization by simulated annealing. Science, 220:671–680, 1983. [Kle36] S.C. Kleene. General recursive functions of natural numbers. Mathematische Annalen, 112:727–742, 1936. [Kol65] A.N. Kolmogorov. Three approaches for deﬁning the concept of information quantity. Probl. Information Transmission, 1:1–7, 1965. [Kol68] A.N. Kolmogorov. Logical basis for information theory and probability theory. IEEE Transactions on Information Theory, 14:662–664, 1968. [Lei92] F.T. Leighton. Introduction to Parallel Algorithms and Architectures: Arrays, Trees, Hypercubes. Morgan Kaufmann Publ. Inc., 1992. [Lev73] L.A. Levin. Universal sorting problems. Problems of Information Transmission, 9:265–266, 1973. [LP78] H.R. Lewis and Ch.H. Papadimitriou. The eﬃciency of algorithms. Scientiﬁc American, 238(1), 1978. [LV93] M. Li and P.M.B. Vitányi. An Introduction to Kolmogorov Complexity and Its Applications. Springer-Verlag, 1993. 342 Literaturverzeichnis [MPS98] E.W. Mayr, H.J. Prömel, and A. Steger, editors. Lectures on Proof Veriﬁcation and Approximation Algorithms. Number 1967 in Lecture Notes in Computer Science. Springer-Verlag, 1998. [MR95] R. Motwani and P. Raghavan. Randomized Algorithms. Cambridge University Press, 1995. [MRR +53] N. Metropolis, A.W. Rosenbluth, M.N. Rosenbluth, A.H. Teller, and E. Teller. Equation of state calculation by fast computing machines. Journal of Chemical Physics, 21:1087–1091, 1953. [Pap94] Ch.H. Papadimitriou. Computational Complexity. Addison-Wesley, 1994. [Pos36] E. Post. Finite combinatory process-formulation. Journal of Symbolic Logic, 1:103–105, 1936. [Pos46] E. Post. A variant of a recursively unsolvable problem. Transactions of AMS, 52:264–268, 1946. [PS82] Ch.H. Papadimitriou and K. Steiglitz. Combinatorial Optimization: Algorithms and Complexity. Prentice-Hall, 1982. [Rei90] R. Reischuk. Einführung in die Komplexitätstheorie. B.G. Teubner, 1990. [Ric53] H.G. Rice. Classes of recursively enumerable sets and their decision problems. Transactions of AMS, 89:25–59, 1953. [Rog67] H. Rogers. Theory of Recursive Functions and Eﬀective Computability. McGraw-Hill, 1967. [RSA78] R.L. Rivest, A. Shamir, and L. Adleman. A method for obtaining digital signatures and public-key cryptosystems. Communications of the ACM, 21:120–12, 1978. [Sal73] A. Salomaa. Formal Languages. Academic Press, 1973. [Sal96] A. Salomaa. Public-Key Cryptography. Springer-Verlag, 1996. [SC79] L.J. Stockmeyer and A.K. Chandra. Intrinsically diﬃcult problems. Scientiﬁc American, 240(5), 1979. [Sch95] U. Schöning. Perlen der Theoretischen Informatik. BI-Wissenschaftsverlag, 1995. [Sch01] U. Schöning. Algorithmik. Spektrum Akademischer Verlag, 2001. [Sha92] A. Shamir. IP = PSPACE. Journal of the ACM, 39:869–877, 1992. [Sip97] M. Sipser. Introduction to the Theory of Computation. PWS Publishing Company, 1997. Literaturverzeichnis 343 [Tra63] B.A. Trakhtenbrot. Algorithms and Automatic Computing Machines. D.C. Heath & Co., 1963. [Tur36] A.M. Turing. On computable numbers with an application to the Entschei- dungsproblem. In Proceedings of the London Mathematical Society, volume 42 of 2, pages 230–265, 1936. [Vaz01] V. Vazirani. Approximation Algorithms. Springer-Verlag, 2001. [Wak68] A. Waksman. A permutation network. Journal of the ACM, 15:159–163, 1968. [Weg05] I. Wegener. Theoretische Informatik – eine algorithmenorientierte Einführung. B.G. Teubner, 2005. Index ableitbar 292 Ableitung 292 Ableitungsbaum 312 Ableitungsregeln 291 Ableitungsschritt 292 Abzählbarkeit 128 Adjazenzmatrix 16, 187, 219 Adleman 268, 286, 342 Äquivalenzproblem 26, 255 algorithmisch erkennbar 98 Algorithmus 24, 91 Approximations- 225 AQP 256 Aufzählungs- 32 DPR 222 Greedy- 240 LS(f ) 232 LS-CUT 233 Metropolis- 237 pseudopolynomieller 219 randomisierter 246 SA(f ) 238 SB 228 Solovay-Strassen- 253 VCA 225 zulässiger 225 Alphabet 14 Approximationsalgorithmus 225 Approximationsgüte 225 ASCII 35 Authentizitätsproblem 271 Beneš-Netzwerk 281 Berechnungsbaum 77, 116 Beweiser 273 Beweissystem 285 interaktives 273f, 276 Zero-Knowledge- 276, 285 Beweisveriﬁkation 191 Bin(·)15 Bisektion 285 Bisektionsbreite 285 Blanksymbol 95 Boole’sche Formel 14, 197, 289 Buchstabe 14 CAESAR 265 Chomsky 12, 335 Chomsky-Normalform 315 Church 112, 121, 339 Church’sche These 112, 128, 332 Clique 30, 206 Compilerbau 287, 335 COMPOSITE 193 Cook 199, 213, 340 Demokrit 241 Determinismus 242 Diagonalisierungsmethode 132 Diagonalsprache 133 Diﬃe 286 digitale Unterschrift 264, 270 Divide and Conquer 188 DLOG 174 Dreiecksungleichung 29 3SAT 206, 209 dynamische Programmierung 220 EA 51, 75 Echtzeit 328 Einstein 241 Einweg-Funktion 265f, 268, 285 endlicher Automat 51, 75 Darstellung 49 J. Hromkovič, Theoretische Informatik, DOI 10.1007/978-3-658-06433-4, © Springer Fachmedien Wiesbaden 2014 346 Index nichtdeterministischer 76 entscheidbar 97 Entscheidungsproblem 24 semantisch nichttriviales 145 Entschlüsselung 264 Epikur 241 Ereignis 242 atomares 242 elementares 242 ERF 26 Erweiterung 209 erzeugte Sprache 292 Euler’sche Funktion 268 EXPTIME 174 Faktorisierung 268, 285 Fehlerwahrscheinlichkeit 245, 248, 254ﬀ Fermat 243, 251 Fingerabdruck 254, 256 Gleichverteilung 244 Gödel 163 Goldwasser 286 Grammatik 287, 291, 332, 335 kontextfreie 297, 310 kontextsensitive 297 normierte 304 normierte kontextfreie 317 reguläre 297, 299 Typ-0- 297 Typ-1- 297 Typ-2- 297 Typ-3- 297 Graphenisomorphie 274 Greedy-Algorithmus 240 Greibach-Normalform 315 Halteproblem 140 Hamiltonscher Kreis 25, 28, 193, 224 Hellman 286 Hilbert 128, 163 Hilbert’sches Hotel 128, 131 HK 25, 224 Homomorphismus 23, 119, 151, 299 ILP 31 Implementierung der Rekursion 335 In(·) 178 Informatik 1 InKonf(·) 178, 184f innere Konﬁguration 178 IP 274 isomorph 274 Isomorphismus 274 kanonische Ordnung 19, 128 Kardinalität 19 Keller 288, 322 Kellerautomat 322, 335 nichtdeterministischer 323 Kett(·) 304 Kettenregel 304 Klartext 264 Klausel 31, 206 Kleene 20, 121 Kleene’scher Stern 20, 299 KNF 31 Knotenüberdeckung 30, 206 KodL 148 KodTM 119 Kolmogorov 34, 243 Kolmogorov-Komplexität 34, 73, 157 einer natürlichen Zahl 37 Kommunikation 263, 277 Kommunikationsprotokoll 271 Kommunikationsweg 278 Komplement 20, 136, 299 Komplexitätsklassen 174 Komplexitätsmaße 168 nichtdeterministische 182 Komprimierung 33 Konﬁguration einer MTM 105 einer NTM 113 einer TM 96 eines EA 53 eines NEA 76 eines NPdA 324 innere 178 Konkatenation 18, 299 Index 347 kontextfrei 297 kontextsensitiv 297, 335 Kostenmaß logarithmisches 174, 253 uniformes 174 Kryptoanalyse 264 Kryptographie 263f, 286 Kryptologie 264 Kryptosystem 264 klassisches 264f Public-Key- 266, 271 symmetrisches 265 Kryptotext 264 L2diag 134 L3a 294 Last-in-ﬁrst-out 322 LCF 310 Ldiag 133, 205 (Ldiag) ∁ 137 LEA 53, 99, 299 leere Sprache 20 leeres Wort 14 Lemma von Ogden 320 Lempty 142, 145 (Lempty) ∁ 142f LEQ 144 Lge 293, 311, 316 Lgleich 106, 172 LH 140 (LH) ∁ 144 LH,λ 145 Linksableitung 313 Lk,diag 134 LMitte 99, 172 LNEA 80 Logarithmus diskreter 267, 285 lokale Suche 232 lokales Optimum 231 LPal 311 LP 102, 172 Lquad 116 LR 326 LR 98 LRE 97, 125, 333 LRev 311 LU 138 (LU) ∁ 144 Lungleich 114 Maschinencode 34, 111 Matching 225 MAX-CL 30 MAX-CUT 211, 233 MAX-SAT 31 MaxInt 219, 235 Methode der Diagonalisierung 132 der dynamischen Programmierung 220 der Fingerabdrücke 254, 259 der häuﬁgen Zeugen 250, 259 der Kolmogorov-Komplexität 157 der Reduktion 134 Metropolis-Algorithmus 237 Micali 286 MIN-VC 30, 226 Modularität 278, 285 MPKP 151 MTM 104, 174 Nachbarschaft 231 echte 236 polynomiell untersuchbare 235 NEA 76 Netz 277 Netzwerk 279 Beneš- 281 Netzwerkdesign 285 Nichtdeterminismus 75, 113 NICHTISO 274 Nichtterminalalphabet 291 Nichtterminale 290 Nichtterminalsymbole 290 NLOG 183 Normalform Chomsky- 315 Greibach- 315 348 Index konjunktive 31 NP 183, 190, 194, 273 NP-schwer 195, 212, 223, 236 stark 223, 236 NP-Schwere 211f starke 239 NP-vollständig 195 NP-Vollständigkeit 205 NPdA 323 NPO 210 NPSPACE 183 NTM 113 Nummer(·)15 nutzloses Symbol 314 O-Notation 171 Ogden 320 Optimierungsproblem 28 kostenbeschränktes 235 P 174, 181, 190, 194 Palindrom 311 Permutation 278f Permutationsnetzwerk 279, 281 PKP 149 platzkonstruierbar 176 PO 211 Polynome 255 polynomialzeit-reduzierbar 182 Post 121, 149, 342 Post’sches Korrespondenzproblem 149 modiﬁziertes 151 Potenzmenge 19 Potenzmengenkonstruktion 80 Präﬁx 19 Prim(·) 40, 246 Primzahlsatz 41, 246 Primzahltest 25, 250, 253, 259 Produktionen 291 Programmveriﬁkation 160 Protokoll kryptographisches 3, 264 zufallsgesteuertes 246 PSPACE 174 Pumping-Lemma für kontextfreie Sprachen 316 für reguläre Sprachen 70 Pumpingtechnik 70, 335 Quantenrechner 7, 112 Rackoﬀ 286 REACHABLE 189 Rechtsableitung 313 Reduktion 134 EE- 135 polynomielle 195 rekursive 134 Regeln 291 regulär 53, 297, 308 rekursiv 25, 97, 126 rekursiv aufzählbar 97 Relationsproblem 26 Rényi 241 Reversal 18 Riemann’sche Hypothese 41 Rivest 268, 286, 339, 342 RSA 268f, 271 Rucksack-Problem 220 SAT 205 Satz des Pythagoras 92 Primzahl- 41, 246 von Cantor und Bernstein 126 von Cook 199 von Euler 268 von Fermat 251 von Rice 146 von Savitch 188 von Shamir 275 Satzform 291 Schaltungsknoten 278 Schlüssel 264 Schmetterling 280 r-dimensionaler 281 Schwellenwert-Sprache 212 Shamir 268, 275, 286, 342 Simulated Annealing 236 Simulation 63 Index 349 Spannbaum 30, 228 Speicherplatzkomplexität 168 Sprache 20 entscheidbare 97 kontextfreie 311 leere 20 reguläre 53, 308 rekursive 97 universelle 138 Startnichtterminal 291 Startsymbol 291 Stirling’sche Formel 280 Suﬃx 19 Syntaxbaum 312 Teile und Herrsche 188 Teilwort 19 Telefonnetze 286 Terminalalphabet 291 Terminalsymbole 290 TM 94, 332 Top-Symbol 322 total berechenbar 98 TSP 28, 224, 227 metrisches 29, 229 Turing 93, 122, 164, 343 Turingmaschine 94, 332 Mehrband- 104, 174 nichtdeterministische 113 universelle 138 Umkehrung 18 Veriﬁzierer 192, 273 Verkettung 18 Verschlüsselung 264 von-Neumann-Maschine 168 VP 192, 274 Wahrscheinlichkeit 243 Wahrscheinlichkeitsraum 244 Wahrscheinlichkeitstheorie 242 Wahrscheinlichkeitsverteilung 244 Wort 14 Länge 14 leeres 14 Zahlproblem 219 Zeitkomplexität 168 zeitkonstruierbar 176 Zertiﬁkat 191 Zeuge 191, 249, 256, 259 zufällig 39 Zufall 13, 39, 241 Zufallssteuerung 242","libVersion":"0.3.2","langs":""}