{"path":"sem4/DMDB/VRL/extra/slides/DMDB-s20-cloud-databases.pdf","text":"Data Modeling and Databases Spring Semester 2025 Cloud Databases Cloud Databases Gustavo Alonso Institute of Computing Platforms Department of Computer Science ETH ZÃ¼rich 1 Key Value Store Cloud Databases 2 Key Value Store â€¢ Blocks, files, and objects are pure storage based on a somewhat similar notion, but each one of them adds more information and more functionality at the price of latency â€¢ Internet scale created a demand for something slightly different: â€¢ Access a chunk of data that does not need to be parsed (unlike objects) but can be of various sizes (unlike blocks) and needs to be system supported names that simplifies finding the data (unlike file systems) â€¢ Data can be dynamic with many entries being created and deleted (not good for blocks, files, or objects) â€¢ The solution is Key Value Stores (KVS) Cloud Databases 3 A database engine â€¢ Data stored following a schema â€¢ Tables, constraints â€¢ Smaller addressable unit: tuple â€¢ Data accessed and manipulated through SQL â€¢ Relational algebra, well defined execution model â€¢ Well defined operators, optimizer â€¢ ACID through transactions â€¢ Maintain data consistency â€¢ Ensures recoverability and persistence Cloud Databases 4 Schemas â€¢ Organize the data â€¢ Through normal forms, avoid redundancy and structure the data in clear units (tables) â€¢ Through constraints guarantee data integrity â€¢ Must be done before data can be used in a database â€¢ Dominant decomposition a problem (views help but only so much) Cloud Databases 5 SQL â€¢ Well defined interface â€¢ Efficient implementations â€¢ Declarative language â€¢ Only limited set of operators â€¢ Impedance mismatch between SQL and regular programming languages Cloud Databases 6 ACID â€¢ Strong guarantees â€¢ Work done by the database engine, not the application â€¢ Well defined properties and understandable concurrency â€¢ Expensive, both in throughput and response time â€¢ Does not scale that well Cloud Databases 7 Get rid of everything to get a Key Value Store â€¢ No schema â€¢ Just a key and blob attached to it, whose structure is unknown â€¢ No SQL â€¢ Get and Set as the only operations â€¢ No ACID â€¢ Eventual consistency â€¢ Potentially no transactions â€¢ Keep keys to identify the data and an index to find it = Key Value Store Cloud Databases 8 KVS architecture â€¢ Data stored as a Key-Value pair â€¢ Key: identifier used to locate and retrieve the data â€¢ Value: a binary object of arbitrary size â€¢ Data organized as a distributed hash table â€¢ Hash the key => get the location for the value â€¢ KV pairs stored in several (3, 5, 7, â€¦ copies) places for availability â€¢ Access is quorum based for consistency â€¢ Typically, eventual consistency â€¢ Actual storage uses already reserved buckets of different sizes and objects are located in the corresponding buckets â€¢ Can be run in main memory for very fast access or with values in storage for large data items Cloud Databases 9 Key Value Stores â€¢ Schema: â€¢ Key + BLOB (or pointer to blob) â€¢ Index on key (hashing) â€¢ Deployment â€¢ Horizontal partitioning â€¢ Replication of partitions â€¢ Replication strategy â€¢ Quorums (simple majority) â€¢ Asynchronous replication across all copies (eventual consistency) Cloud Databases 10 KVS: Advantages â€¢ Very fast lookups (hashing) â€¢ Easy to scale to many machines (simply add more copies/machines) â€¢ Excellent match for the cloudâ€™s elasticity â€¢ Can be implemented in main memory of machines (faster access) â€¢ Useful in many applications: â€¢ Lookup user profiles â€¢ Retrieve users web pages and info â€¢ Scalable and easy to get going (no schema) Cloud Databases 11 KVS: Disadvantages â€¢ No easy support for queries beyond point queries as data is treated as a BLOB â€¢ Ranges, joins â€¢ Queries on attributes (there are no attributes) â€¢ Depending on implementation, data can appear as being inconsistent â€¢ Application dependent correctness â€¢ Used for large scale deployments (worldwide) Cloud Databases 12 KVS: to keep in mind â€¢ As initially proposed (No-SQL movement), KVS and similar system had a major hidden problem: â€¢ Pushes complexity and responsibility to the application â€¢ That complexity will affect the development of applications â€¢ Some operations are very costly to implement â€¢ Range queries require to read all copies â€¢ Joins out of the question â€¢ Works well as cache or specialized system â€¢ Not a replacement for a full blown database Cloud Databases 13 KVS as caches â€¢ KVS are often used as main memory caches to avoid having to access cloud storage: â€¢ Higher throughput â€¢ Lower latency â€¢ Databases: store results of a query in a value, key is generated by hashing the query. When running queries, check first the KVS and answer form there if the query is stored â€¢ Web pages, pictures, adds, etc. â€¢ A very common use case for memcached Cloud Databases 14 Application KVS KVS KVS KVS Cloud Storage Data distribution in KVS â–ª Basic design of a KVS âž¢ How should we maps hash(key) to machines? âž¢ How should we manage different replicas? âž¢ How should we answer queries given that we have replicas? â–ª NaÃ¯ve strategy: âž¢ 3 machines: (Hash mod 3) is the machine ID âž¢ What if we remove one machine (M2)? we need to move all data from M2 to M0 and M1 âž¢ What hash function should we use? âž¢ (Hash mod 2)? key Hash Hash mod 3 \"john\" 163342856 2 2 \"bill\" 759463473 9 0 \"jane\" 500079912 4 1 \"steve\" 978717334 3 0 \"kate\" 342165799 5 2 key Hash Hash mod 3 \"john\" 163342856 2 2 \"kate\" 342165799 5 2 key Hash Hash mod 3 \"bill\" 759463473 9 0 \"steve\" 978717334 3 0 key Hash Hash mod 3 \"jane\" 50007 99124 1 Machine 0 Machine 1 Machine 2 15Cloud Databases Simple design and its problems â–ª Fundamental Questions: âž¢ How should we maps hash(key) to machines? âž¢ How should we manage different replicas? âž¢ How should we answer queries given that we have replicas? â–ª NaÃ¯ve strategy: âž¢ 3 machines: (Hash mod 3) is the machine ID â–ª Problem? One machine fail, we need to move all data from M2 to M0 and M1 â–ª What hash function should we use? â–ª (Hash mod 2)? â–ª Rehashing Problem: A single machine failure can cause all data to be remapped to a different machine (in the worst case) key Hash Hash mod 3 Hash mod 2 \"john\" 163342856 2 2 0 \"bill\" 759463473 9 0 1 \"jane\" 500079912 4 1 0 \"steve\" 978717334 3 0 1 \"kate\" 342165799 5 2 1 All keys will change machine if we use Hash mod 3 then Hash mod 2 16Cloud Databases Consistent Hashing â–ª Idea: Horizontal partition the data with hashing â–ª Each machine deals with the data points in the clockwise direction, before the next machine. key Hash \"john\" 1633428562 \"bill\" 7594634739 \"jane\" 5000799124 \"steve\" 9787173343 \"kate\" 3421657995 John Bill Steve Kate Jane A B C 17Cloud Databases Removing nodes â–ª When machine A leaves the cluster âž¢ B takes change of everything between A->C (Aâ€™s old responsibility) âž¢ Câ€™s responsibility does not change â–ª If there are K data points and n machines, on average, a single machine failure triggers K/n data redistribution. key Hash \"john\" 1633428562 \"bill\" 7594634739 \"jane\" 5000799124 \"steve\" 9787173343 \"kate\" 3421657995 John Bill Steve Kate Jane A B C 18Cloud Databases Adding nodes â–ª When machine D joins the cluster âž¢ D takes charge of D->C âž¢ A takes charge of A->D âž¢ B and Câ€™s responsibility does not change â–ª Potential problem? âž¢ Load balancing is not good (A an D, on average, has 2x less data than C in this example) key Hash \"john\" 1633428562 \"bill\" 7594634739 \"jane\" 5000799124 \"steve\" 9787173343 \"kate\" 3421657995 John Bill Steve Kate Jane A B C D 19Cloud Databases Load balancing through finer partitioning â–ª Node A (physical node) becomes 10 â€œvirtual nodesâ€: A0â€¦ A9 â–ª B0â€¦B9, C0â€¦C9 â–ª Then do the same exercise as if there were 30 â€œmachinesâ€ John Bill Steve Kate Jane A B C D 20Cloud Databases Dealing with failures â–ª What if a node fails? (instead of join/leave which allows the time for data redistribution) â–ª We need to replicate data (store multiple copies) on ð›¾ machines. âž¢ The example on the right, ð›¾ = 3 âž¢ The next r - 1 nodes in clockwise direction store the same data. â–ª How can we guarantee consistency among replicas? âž¢ Not trivial. âž¢ Thought experiment: â€¢ T1: write master copy of object X on D â€¢ System: replicate to other nodes every 5 min â€¢ D crashes before T1's change on D being replicated â€¢ T2: read object X âž¢ Problem? â€¢ T2 has access to a staled object. 21Cloud Databases Dealing with replication â–ª Using quorums â–ª Let: âž¢ N: The number of replicas for the data. âž¢ R: The number of machines contacted in read operations âž¢ W: The number of machines that have to be blocked in write operations â–ª Make sure that R + W > N, Example: âž¢ W = N, R = 1 âž¢ W = 1, R = N â–ª Guarantee: Always being able to read a latest version. 22Cloud Databases Many variations â€¢ Cloud computing has given raise to many variations on these themes â€¢ Most serious system converging back towards a full transactional engine with some form of schema and SQL support â€¢ Specialized systems in use as accelerators (memcached) or concrete application stages (profile look up) â€¢ Lesson to learn = the concepts change relatively little, the use cases and the deployments can change substantially affecting the way given solutions are used and perceived. Cloud Databases 23 Cloud Native Databases Cloud Databases 24 The problem with traditional database engines in the cloud â€¢ In the cloud, the separation of storage and compute causes overhead if one is not careful â€¢ A traditional database generates too much traffic in terms of I/O, especially if it is mirrored for availability Cloud Databases 25 Amazon Aurora: Design Considerations for High Throughput Cloud-Native Relational Databases, SIGMOD 2017 Amazon Aurora â€¢ A modification of MySQL to make it work in the cloud: â€¢ Separate storage and cache management from transaction processing â€¢ Primary copy, asynchronous replication â€¢ 6 copies of the data in 3 different availability zones, continuous back- up on S3 â€¢ Updates with quorum (4 out of 6) â€¢ Primary copy and up to 15 read- only replicas (20 ms lag) Cloud Databases 26 https://aws.amazon.com/rds/auroraSnowflake â€¢ A data warehouse specialized for analytical queries developed entirely on the cloud (cloud native) â€¢ Separates compute (nodes running VMs with a local disk) from storage (Amazonâ€™s S3) Cloud Databases 27 Documentation: https://docs.snowflake.com/en/user-guide-intro.html Paper: https://dl.acm.org/doi/10.1145/2882903.2903741 Amazonâ€™s S3 â€¢ Simple Storage Service (S3) is an object storage service in the cloud that acts as the persistent storage that is available to applications â€¢ Unlike conventional local disks or distributed file system! â€¢ Object store (key-value) [object = file] â€¢ HTTP(S) PUT/GET/DELETE interface â€¢ No update in place (objects must be written in full) â€¢ Can read parts (ranges) of an object instead of the whole object â€¢ High CPU overhead (because of HTTP) â€¢ I/O is extra expensive (network bandwidth, latency, interface) Cloud Databases 28 Something familiar â€¢ A virtual warehouse is a collection of worker nodes (EC2 instances in Amazon) â€¢ Each worker node has a cache in its local disk where it stores the objects (table files or parts thereof) accessed before â€¢ The cache uses a simple LRU replacement policy Cloud Databases 29 Micro-partitions â€¢ Micro-partitions are Snowflakeâ€™s name for extents â€¢ What is interesting is how they are organized to facilitate query processing â€¢ Size ranges between 50 and 500 MB (before compression, the data is always compressed when in S3) â€¢ Each micro partition has metadata describing what is inside â€¢ The metadata can be read without reading the whole micro-partition â€¢ The metadata is used to read just the part of the micro-partition that is relevant â€¢ Data in the micro-partition is stored in columnar form (by columns not by rows) Cloud Databases 30Cloud Databases 31https://docs.snowflake.com/en/user-guide/tables-clustering-micropartitions.html A not so uncommon design â€¢ Snowflake is combining many tricks used before in different contexts: â€¢ Horizontal partitioning of the tables (by row): allows to read the table in parallel, to put different parts of the table in different processing nodes, and â€“ if organized accordingly- allows to read only the needed tuples instead of all the table â€¢ Columnar format (storage by column): the preferred storage format for analytics, improves cache locality, enables vectorized processing, facilitates projection operations (SQL), allows to process only the part of the table that is relevant â€¢ Storage level processing to read only the part of the file that is needed (helped by the micro-partitions and the columnar format) Cloud Databases 32 Pruning based on metadata â€¢ The header for a micro-partition contains information about the data. SELECT * FROM T WHERE age > 45 â€¢ If header contains the min and max age in the data, we can decide we do not need that micro- partition simply by looking at the header Cloud Databases 33 Pruning â€¢ Snowflake does not use indexes â€¢ Indexes require a lot of space â€¢ Indexes induce random accesses (very bad for slow storage like S3) â€¢ Indexes need to be maintained and selected correctly â€¢ Instead, it uses the metadata to store information that allows to filter micro-partitions (min/max, #distinct values,#nulls, bloom filters, etc.) â€¢ The metadata is much smaller than an index and easier to load than a whole index â€¢ By splitting table in potentially many micro-partitions, it can significantly optimize the data movement to and from storage Cloud Databases 34 Writing to disk â€¢ S3 does not support update in place, a file is replaced in its entirety (immutable) â€¢ Snowflake uses this feature to implement snapshots of the data (like shadow paging): â€¢ When a micro-partition is modified, a new file is written â€¢ The old micro-partition can be kept or discarded â€¢ Allows time travel (read the data in the past up to 90 days) and provides fault-tolerance (the old data can be recovered from the old micro-partitions) Cloud Databases 35 An interesting future Cloud Databases 36 The IT industry â€¢ The IT industry is, above all, an industry: â€¢ Trends and developments driven by demand and market â€¢ For hardware, there is a high investment cost to new developments â€¢ New processors pay off only after they reach a high volume of sales â€¢ Hardware evolves, software evolves much slower: â€¢ Radical solutions are difficult because the entire stack needs to be created â€¢ Window of opportunity when new applications show up (the killer app) â€¢ Example: the parallel computing craze of the late 80â€™s â€¢ Currently undergoing profound changes for economic, scientific, and political reasons Cloud Databases 37 A theme since a couple of decades â€¢ 2011 Report â€¢ Exponential growth for several decades â€¢ Exponential growth no longer possible â€¢ Switch to multicore and parallelism â€¢ Energy consumption becomes an issue â€¢ Multicore introduces parallelism that we do not know how to exploit well â€¢ Situation will not change in near future â€¢ Alternative is specialization â€¢ Either somebody comes up with a new great invention or there is a problem Cloud Databases 38 General purpose computing Slow improvements lead to specialization Cloud Databases 39 Economic factors â€¢ It is not only Mooreâ€™s Law â€¢ The increase in unit cost for production was offset with the increase in demand â€¢ Smaller sizes are more complex and difficult to produce => more expensive => not longer economical â€¢ But the demand is still there, so what to do? Cloud Databases 40 Why is a CPU general purpose? CPUs designed for a wide range of tasks and use cases. Very different architecture than GPUs â€¢ CPU is low latency (very fast) but low throughput (parallelism) â€¢ GPUs are high latency (slow) but high throughput (parallelism) â€¢ CPUs designed for running several processes (a few) in a time-shared manner = context switching, isolation, state keeping, memory virtualization, etc. Cloud Databases 41 What to expect for databases â€¢ Higher degree of specialization at all levels of the architecture â€¢ CPU less relevant with heavy processing moving to accelerators (TPUs, DPUs, GOUs, etc.) â€¢ Heterogeneous computing â€¢ Widening of use cases (e.g., vector search = ANNS) â€¢ We have it covered in later courses: â€¢ Big Data â€¢ Cloud Computing Architecture â€¢ Data Management Systems Cloud Databases 42","libVersion":"0.5.0","langs":""}