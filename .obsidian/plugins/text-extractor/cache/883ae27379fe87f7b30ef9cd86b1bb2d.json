{"path":"sem5/NumCS/VRL/extra/NumCS-script.pdf","text":"Numerische Methoden Vasile Gradinaru Dokument zur Vorlesung 401-0663-00 Numerische Methoden D-INFK an der ETH Z¨urich 2025-. ©Seminar f¨ur Angewandte Mathematik, ETH Z¨urich1 Generiert am September 18, 2025 1Alle Rechte vorbehalten; keine Vervielf¨altigung jeglicher Art oder Umfang ist ohne die Zus- timmung des Autors erlaubt. Denken vor Rechnen! Ziele dieser VorlesungComputer−Theorie Wissenschaft TechnikExperiment Numerische Methodensimulation Die numerischen Methoden sind allgegenw¨artig in der modernen Wissenschaft und Technik, vor allem als Komponenten der Computersimulationen, aber auch als Hilf- swerkzeuge bei Experimenten oder bei der Entwicklung neuer Theorien. Der Exper- imentator sollte wissen, wie die Ergebnisse, die seine Ger¨ate liefern, zustandekom- men, wie zuverl¨assig sie sind, warum manche Artefakte entstehen und wie diese zu vermeiden sind. Beim Aufstellen neuer Theorien braucht man oft Berechnungen, die nicht analytisch durchf¨uhrbar sind. Oder der Rechner ist einfach schneller in manchen F¨allen. . . Man baut oft ein neues mathematisches Modell f¨ur ein physikalis- ches Problem und man testet es: ob die Ergebnisse etwas mit der Realit¨at zu tun haben? Damit dies m¨oglich ist, muss man numerische Methoden w¨ahlen, die geeignet zur L¨osung des Problems sind. Wir werden sehen, dass dies nicht so ein- fach ist, sobald das mathematische Modell nicht trivial ist. Wir k¨onnen numerischen Approximationen zur L¨osung mathematischer Probleme nur trauen, wenn diese nu- merische L¨osungen sehr sorgf¨altig berechnet worden sind. Das genaue Wissen, wo die Grenzen eines numerischen Verfahrens liegen, und was typischerweise passiert, wenn diese Grenzen nicht eingehalten werden, ist sogar wichtiger als das Verfahren selber, das man einfach in einem Buch oder im Internet ﬁnden kann: wir wollen sicher die Eigenschaften eines Models nicht mit diejenigen eines Verfahren verwech- seln! Das Eineignen dieses Wissens geht nur ¨uber Probieren und Experimentieren, 2 wenn die Zeit f¨ur die fundierte mathematische Analyse verschiedener Verfahren fehlt. Deswegen ist der Inhalt dieser Vorlesung Implementierung- und Experimentierung- sorientiert: die Studenten m¨ussen selber die Methoden in verschiedene Umgebungen ausprobieren, um Erfahrungen zu sammeln. Lernziele dieser Vorlesung sind: • ¨Ubersicht ¨uber die wichtigsten Algorithmen zur L¨osung der grundlegenden numerischen Probleme in der Physik und ihren Anwendungen; • ¨Ubersicht ¨uber Softwarequellen (engl. software repositorys) zur Probleml¨osung; • Fertigkeit konkrete Probleme mit diesen Werkzeugen numerisch zu l¨osen; • F¨ahigkeit numerische Resultate zu interpretieren. Der Schwerpunkt liegt auf dem Erwerb von Fertigkeiten in der Anwendung von numerischen Verfahren: Algorithmen (Prinzipien, Ziele und Grenzen) und Imple- mentationen (Eﬃzienz und Stabilit¨at) werden mittels relevanten numerischen Ex- perimenten (Design und Interpretation) vorgestellt. Wir werden Theorien und Be- weise nur dann vorbringen, wenn es essenziell f¨ur das Verst¨andnis der Algorithmen erscheinen wird. Achtung: die hier eingebundene Codes wurden ¨uber die Jahre von verschiedene Personen entwickelt; in der Zeit ¨andern sich die Standards, so dass es sein kann, dass die aktuelle Syntax und verwendete Konventionen nicht immer ¨ubereinstimmen oder nicht ganz konsistent sind. Auf verschiedene Ger¨ate k¨onnen auch verscheidene Versionen von Software installiert worden, das ist eine andere Quelle von m¨ogliche Abweichungen. Materialien Viele Ideen und Materialien, die in diesem Skript Platz gefunden haben, sind aus den Diskussionen und Erfahrungen am Seminar f¨ur Angewandte Mathematik an der ETH Z¨urich entstanden. Meinen Kollegen bin ich daf¨ur sowie f¨ur deren Vertrauen, mich diese neue und aufregende Vorlesung entwickeln zu lassen, sehr dankbar. Der Autor ist von den vorhandenen Materialen der Slides Numerical Methods for CSE und Numerische Mathematik, die vor allem vom Prof. Ralf Hiptmair entwickelt wor- den sind, ausgegangen. Ich danke auch meinen Assistierenden und den zahlreichen Studenten der Studieng¨ange Physik und Rechnergest¨utze Wissenschaften (RW/CSE), die mir Kommentare und Verbesserungsvorschl¨age zur Vorlesung und zum Skript gegeben haben. Folgende B¨ucher wurden f¨ur die Vorbereitung dieses Skriptes verwendet und k¨onnen teilweise als Begleitlekt¨ure zur Vorlesung in Betracht gezogen werden. ∗ M. Hanke-Bourgeois, Grundlagen der Numerischen Mathematik und des Wissenschaftlichen Rechnens, Mathematische Leitf¨aden, B.G. Teubner, Stuttgart, 2009, NEBIS 009860519 3 ∗ P. Deuflhard and A. Hohmann, Numerische Mathematik. Eine algorith- misch orientierte Einf¨uhrung, DeGruyter, Berlin, 2008, NEBIS 009855563 ∗ J. St¨or and R. Bulirsch, Einfuhrung in die Numerische Mathematik, Springer Verlag, 2005, NEBIS 009840490 ∗ W. Dahmen and A. Reusken, Numerik f¨ur Ingenieure und Naturwis- senschaftler, Springer 2008, NEBIS 00985199 ∗ Quarteroni, Sacco and Saleri, Numerische Mathematik 1 + 2, Springer 2007, NEBIS 009842244 ∗ L.N. Trefethen and D. Bau, Numerical Linear Algebra, SIAM 1997, NEBIS 009819168 ∗ G. Strang, Wissenschaftliches Rechnen, Springer 2010, NEBIS 009860694 ∗ P. DeVries and J. Hasbun, A First Course in Computational Physics, Jones and Bartlett Publishers; 2 edition, 2010, NEBIS 006032942 ∗ W.L. Dunn, J.K. Shultis, Exploring Monte Carlo Methods, Amsterdam, Elsevier, 2012, NEBIS 009882256 ∗ T. M¨uller-Gronbach, E. Novak, K. Ritter, Monte Carlo-Algorithmen, Berlin, Springer, 2012, NEBIS 009889710 ∗ H.P. Langtangen, A primer on scientiﬁc programming with Python, Springer, 2012, NEBIS 009893026 ∗ H.P. Langtangen, Python scripting for computational science, Springer, 2009, NEBIS 009859422 ∗ Python Scientiﬁc Lecture Notes Erwartet werden solide Kenntnisse in Analysis (Approximation und Vektoranalysis) und linearer Algebra (Gauss-Elimination, Matrixzerlegungen, sowie Algorithmen, Vektor- und Matrizenrechnung: Matrixmultiplikation, Determinante, LU-Zerlegung nicht-singul¨arer Matrizen). Die meist verwendete Python Packete in diesem Dokument sind numpy, scipy, matplotlib, sympy. Das Packet pandas kann auch sehr n¨utzlich sein, wurde hier aber nicht verwendet. Es wird ausdr¨ucklich empfohlen, die Dokumentation zur verwendeten Funktionen in jeder Sektion genau anzuschauen. Die Pr¨ufung ﬁndet zur Zeit am Computer mit CodeExpert, was f¨ur die ¨Ubungen auch in Verwendung ist; dort sind alle n¨otige Packete einstalliert. Einige arbeiten lieber mit jupyther-Notebooks oder IDEs wie code, PyCharm, Spyder, Atom, die aber bei der Pr¨ufung nicht zur Verf¨ugung stehen. Eine minimale Methode f¨ur sich zu arbeten ist: nur ein einfaches Texteditor und ipython in Shell verwen- den. Wir gehen einfach davon aus, dass Studierende selber einen Einstieg in (ein- fachem) Python mit Wissenschaftliche Packete gemacht haben. Zur Not empﬁehlt sich Python Scientiﬁc Lecture Notes anzuschauen. Contents 1 Vor dem Start 5 1.1 Rundungsfehler und Fehlerpropagation . . . . . . . . . . . . . . . . . 5 1.2 Rechenaufwand . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19 1.3 Rechnen mit Matrizen . . . . . . . . . . . . . . . . . . . . . . . . . . 24 2 Polynominterpolation 39 2.1 Interpolation und Polynome . . . . . . . . . . . . . . . . . . . . . . . 39 2.2 Newton Basis und Dividierte Diﬀerenzen . . . . . . . . . . . . . . . . 42 2.3 Lagrange- und baryzentrische Interpolationsformeln . . . . . . . . . . 47 2.4 Chebyshev-Interpolation . . . . . . . . . . . . . . . . . . . . . . . . . 51 2.4.1 Fehlerbetrachtung . . . . . . . . . . . . . . . . . . . . . . . . . 54 2.4.2 Interpolation und Auswertung . . . . . . . . . . . . . . . . . . 56 2.5 DFT und Chebyshev-Interpolation . . . . . . . . . . . . . . . . . . . 58 4 Chapter 1 Vor dem Start 1.1 Rundungsfehler und Fehlerpropagation In der Mathematik haben Sie bisher meistens exakte Formeln (z.B. f¨ur die L¨osung einer quadratischen Gleichung) oder Deﬁnitionen und Ergebnisse in Form von Gren- zwerte (z.B. Deﬁnition der Ableitung einer Funktion, des Riemannschen Integral, der Taylor-Reihe, der Exponentialfunktion) oder in Form von Existenzergebisse (z.B. Existenz von n Eigenwerte einer n × n Matrix). Oft kann man jedoch diese Quan- tit¨aten nicht exakt (analytisch) berechnen, so muss man mit numerischen Approxi- mationen arbeiten: z.B. mit einer abgebrochenen Taylor-Reihe oder einem Term der konvergierenden Folge. Approximationen, die die Rechenzeit, n¨otigem Spe- icher und Fehler in so allgemein wie m¨oglich F¨alle gleichzeitig optimieren, sind das Ziel der Numerischen Mathematik. Wichtig dabei ist, dass jede dieser Quantit¨aten (Rechenzeit, Speicher, Fehler) pr¨azise quantiﬁzierbar in Termen von Diskretisierungsparameter (zur Verf¨ugung des Nutzers ) ist; der Fehler sollte systematisch durch enstprechende Wahl der Diskretisierungsparameter verkleinert werden k¨onnen (auf Kosten der Rechenzeit und/oder des Speicheraufwands). Damit befassen wir uns in den n¨achsten Kapiteln. Die Rundungsfehleranalyse, die wir hier kurz ansprechen, ist nicht wesentlich! Es ist zwar wichtig, die oﬀensichtlich vermeidbare Fehler zu vermeiden und auch immer einen kritischen Auge auf die Algorithmen und numerische Ergebnisse zu behalten; die Ideen, die zu Approximationen und eﬃziente Algorithmen f¨uhren, sind uns wichtiger. Dort, wo die Rundungsfehler und die Fehlerpropa- gation auf die Wahl eines Algorithmen sich auswirken, werden wir das auch beto- nen und vielleicht auch gelegentlich die Rundungsfehleranalyse machen, sonst nicht. Trotzdem, ist es gut zu wissen, was es mit der Rundung schief gehen kann. Deﬁnition 1.1.1. (relativer, absoluter Fehler) Sei ˜x ∈ R n eine Approximation an x ∈ R n. Der absolute Fehler ist ∥˜x − x∥, mit ∥ · ∥ eine Norm in Rn. Wenn ˜x ̸= 0, dann betrachten wir auch den relativen Fehler ∥˜x − x∥/∥x∥. Beispiel 1.1.2. Rundungsfehler sind bereits bei der einfachsten Operationen dabei. Code 1.1.3: Rundungsfehler 5 CHAPTER 1. VOR DEM START 6 1 a = 4/3; b = a - 1; c = 3*b; e = 1-c print(e) # exact should be 0, get 2.220446049250313e-16 3 a = 1012/113; b = a-9; c = 113*b; e = 5+c 5 print(e) # exact should be 0, get 6.750155989720952e-14 7 a = 83810206/6789 b = a - 12345 9 c = 6789*b e = c-1 11 print(e) # exact should be 0, get -1.6079866327345371e-09 Meistens liefern selbst analytische Formeln die exakte L¨osung nicht. Der Grund ist, Computer k¨onnen nur endlich viele Zahlen exakt darstellen, alle anderen Zahlen werden durch darstellbare Zahlen (Maschinenzahlen) approximiert. Die Menge der Maschinenzahlen ist eine diskrete Untermenege von R, die nicht geschlossen f¨ur die arithmetischen Operationen +, −, ·, / ist, so dass Rundung und Rundungsfehler unvermeidbar sind. Die Maschinenzahlen werden als Gleitpunktzahlen dargestellt. Gegeben eine Basis B ∈ N, B ≥ 2, ein Bereich von Exponenten {emin, . . . , emax} ⊂ Z und die Anzahl Ziﬀern m ∈ N (f¨ur die Mantisse d), die Menge der Maschinenzahlen ist M = {d · BE, mit d = i · B−m, i = Bm−1, . . . , Bm − 1, E ∈ {emin, . . . , emax} } . Diese Zahlen haben die Form x = ±0.a1a2 . . . am · Be normalisiert mit der Bedingung a1 ̸= 0 falls x ̸= 0; z.B. −1.473 wird als −0.1473·101 und 14.7 · 10−4 wird als 0.147 · 10−2 dargestellt. Die wichtigsten Informationen ¨uber die zur Verf¨ugung stehenden Maschinenzahlen (wie z.B. Extrema und Maschinen- genauigkeit) erh¨alt man mit numpy.finfo. Probieren Sie numpy.finfo(np.float64).resolution, numpy.finfo(np.float64).tiny, numpy.finfo(np.float64).max. Die Abst¨ande zwischen den Maschienenzahlen sind gr¨osser f¨ur grosse Zahlen: Bemin−1 Abstand Bemin −m Abstand Bemin−m+1 Abstand Bemin−m+2 0 In diesem Bereich beﬁnden sich nicht-normalisierte Zahlen Manchmal triﬀt man auch die eps-Zahlen; probieren Sie numpy.finfo(float64).eps f¨ur die Diﬀerenz zwischen 1.0 und die n¨achst gr¨osste Maschiennzahl (2.220446049250313·10−16 auf meinem Rechner) und numpy.finfo(float64).epsneg f¨ur die Diﬀerenz zwischen 1.0 und die n¨achst kleinste Maschiennzahl (1.1102230246251565 · 10−16 auf meinem Rechner). CHAPTER 1. VOR DEM START 7 Bei der Approximation reelen Zahlen mit Maschinenzahlen enstehen Rundungs- fehler. So verr¨at der relative Fehler die Anzhal korrekten Ziﬀern der Approximation: wenn ∥˜x − x∥/∥x∥ < 10−ℓ, dann ˜x hat ℓ ∈ N korrekte Ziﬀern. Beispiel 1.1.4. (Unterlauf und ¨Uberlauf) Code 1.1.5: Unterlauf und ¨Uberlauf 1 import numpy as np np.seterr(under=\"warn\") 3 numbers = np.array([0.5,0.1,0.03,1e-16]) 5 print(np.sum(numbers**20)) 7 numbers = np.array([42.1,-42,-0.1]) should_be_zero = np.sum(numbers) 9 print(should_be_zero) # 1.4155343563970746e-15 assert should_be_zero**30 == 0 # don’t compare floats via == 11 # correct way to compare floats: tol = 1E-16 # np.finfo(float64).eps is better 13 s = 1. # compared to which |x| should be small x = should_be_zero 15 if( abs(x) < tol*s): print(x, \" is 0 for us\") else: print(x,\" is not 0 for us\") 17 x = should_be_zero/100 if( abs(x) < tol*s): print(x, \" is 0 for us\") 19 else: print(x,\" is not 0 for us\") 21 print( np.prod(10**np.arange(30.)) ) # np.float64(inf) print( np.uint8(23) - np.uint8(42) ) # np.uint8(237) 23 print(np.array([1, 2]) / 0.0) # inf print(np.array([0, 0]) / 0.0) # NaN Wenn das Ergebnis einer arithmetischer Operation ¨uber die gr¨osste Maschinen- zahl liegt reden wir ¨uber ¨Uberlauf (overﬂow); wenn das Ergebnis zwischen 0 un die kleinste positive Maschinenzahl liegt, reden wir ¨uber Unterlauf (underﬂow). Un- terlauf wird oft ignoriert (die Warnungen dazu sind a priori ausgeschaltet), da es meistens harmlos ist, jedoch es kann zu einem Verlust der Genauigkeit f¨uhren. Uber- laufe sind problematischer, darum gibt es meisten auch Warnungen dazu. Spezielle F¨alle sind inf und NaN. Die Auswirkungen von Rundungsfehler in einem komplizierten Algorithmus k¨onnen unter Umst¨anden sehr schwierig bis unm¨oglich vorauszusehen. Jedoch gibt es F¨alle, wenn das klar ist, also F¨alle, die vermeidbar sind. Beispiel 1.1.6. Die Berechnung der Nullstellen eines quadratischen Polynoms mit der Standardformel (die Sie aus der Schule kennen) ist instabil. Wir beschrenken uns auf dem einfachen Fall p(x) = x2 + ax + b. Der folgende Code implementiert direkt das Formel mit dem Diskriminante, die jeder Sch¨uler auswendig lernt. Wir wenden dann diese Formel f¨ur p(x) = (x − c)(x − 1/c) = x2 − (c + 1/c)x + 1, so ist a = c + 1/c und b = 1. Wir testen mit 101 ¨aquidistante Werte f¨ur c zwichen CHAPTER 1. VOR DEM START 8 2 und 1000. Anschliessend ploten wir die relativen Fehler in den zwei gefundenen Nullstellen von p. Code 1.1.7: Instabile Berechnung der L¨osungen einer quadratischen Gleichung import numpy as np 2 def zerosquadpol(a,b): 4 \"\"\" compute the real roots of p(x) = x**2 + a*x + b 6 with the standard discriminant formula caution: round-off! 8 \"\"\" 10 z = np.array([]) D = a**2 - 4*b 12 if D.any()<0: print('no real zeros') else: 14 wD = np.sqrt(D) z = np.array([ (-a-wD)/2 , (-a+wD)/2 ]) 16 return(z) 18 \"\"\" 20 Test this function for p(x) = (x-c)*(x-1/c) a and b are computed from c, which intorduces a small relative roundoff error 22 \"\"\" 24 n = 100 c = np.linspace(2,1000,num=101, endpoint=True ) 26 a = -(c+1/c) b = 1. + 0*c 28 z = zerosquadpol(a,b) z0 = z[0] 30 z1 = z[1] x0 = 1/c 32 x1 = c 34 import matplotlib.pyplot as plt params = {'axes.labelsize': 20, 36 'legend.fontsize': 20, 'figure.figsize' : (18,12), 38 'xtick.labelsize': 16, 'lines.markersize': 12, 40 'ytick.labelsize': 16 } 42 plt.rcParams.update(params) plt.semilogy(c, abs(z0-x0)/x0, 'r+', label='small root') 44 plt.semilogy(c, abs(z1-x1)/x1, 'b.', label='large root') plt.ylabel('relative error') 46 plt.xlabel('c') plt.grid() 48 plt.legend() plt.savefig('qprootsunstable.eps') CHAPTER 1. VOR DEM START 9 50 plt.savefig('qprootsunstable.pdf') plt.savefig('qprootsunstable.png') 52 plt.show() 0 200 400 600 800 1000 c 10 −16 10 −15 10 −14 10 −13 10 −12 10 −11relative error small root large root Abb. 1.1.8. Rundungsfehler in den mit der Standardformel berechenten zwei Null- stellen von p(x) = (x − c)(x − 1/c). Die Berechnung von a hat kleine Rundungsfehler von der Gr¨ossenordnung von numpy.finfo(float64).eps ≈ 10−16 produziert, was sich in den relativen Fehler ≈ 10−11 in der Approximation der kleinsten Nullstelle und ≈ 10−16 in der Approx- imation der gr¨osten Nullstelle, vor allem f¨ur gr¨ossere Werte von c. Der wesentliche Unterschied in der Berechnung der zwei Nullstellen ist, dass wir bei der kleinsten Nullstelle eine Substraktion statt einer Addition haben. Bemerkung 1.1.9. Wenn wir zwei fast gleich grosse Zahlen, die jede kleine rela- tive Fehler mittragen, substrahieren, erhalten wir ein kleines Wert, doch die Fehler k¨onnen sich addieren, so dass relativ zum kleinen Wert des Ergebnisses haben wir einen riesigen Fehler. Diese extreme Vergr¨osserung des relativen Fehlers bei der Substraktion zweier fast gleicher Zahlen heisst Ausl¨oschung. (absolute) Fehler Ausl¨oschung ˆ= Substraction zweier ungef¨ahr gleich grossen Zahlen (⇒ extreme Verst¨arkung der relativen Fehlern) Der Rundungsfehler der Substraktion selber ist ver- nachl¨assigbar! CHAPTER 1. VOR DEM START 10 Beispiel 1.1.10. (Ausl¨oschung in Dezimalsystem) Wir betrachten zwei positive Zahlen x, y, die ungef¨ahr gleich gross sind und die mit relativer Fehler der Gr¨ossenordnung 10−7 behaftet sind. Das bedutet, ab der 7te Stelle nach der Komma sind die Ziﬀern verf¨alscht, was wir hier mit ∗ bezeichnen. Bei der Substraktion x − y verschieben sich die verf¨alschten Ziﬀern nach links, so dass der relative Fehler des Ergebnisses ≈ 10−3 ist: x = 0.123467∗ ← 7te Ziﬀer verf¨alscht y = 0.123456∗ ← 7te Ziﬀer verf¨alscht x − y = 0.000011∗ = 0.11∗000 · 10−4 ← 3te Ziﬀer verf¨alscht unwesentliche Nulle Beispiel 1.1.11. (Ausl¨oschung bei der Auswertung von Diﬀerenzenquotienten) Aus Analysis wissen wir die Deﬁnition der Ableitung einer diﬀerentierbarer Funktion f : I ⊂ R → R in einem Punkt x ∈ I als Grenzwert des Diﬀerenzenquotienten f ′(x) = lim h→0 f (x + h) − f (x) h . Daraus kommt man auf die Idee, die Ableitung von f in x via den Diﬀerenzenquo- tienten f¨ur kleines h zu approximieren: f ′(x) ≈ f (x + h) − f (x) h mit |h| ≪ 1 . Der Satz von Taylor um x sagt uns, dass der Fehler ungef¨ahr wie h gegen 0 gehen sollte, wenn die Funktion mindestens 2 mal stetig diﬀerentierbar ist: f (x+h) = f (x)+f ′(x)h+ 1 2 f ′′(c)h2 mit c = c(x, h) ∈ [min{x.x+h}, max{x, x+h}] , d.h. f (x + h) − f (x) h − f ′(x) = 1 2 f ′′(c)h . Das l¨asst sich leicht implementieren. Wir probieren diese Idee f¨ur f = exp und x = 0. Die vorige Analysis sagt voraus, dass der relative Fehler gegen 0 zumindest wie h sinken sollte. CHAPTER 1. VOR DEM START 11 Code 1.1.12: Ableitung via Diﬀerenzenquotient from numpy import exp, sin, cos 2 f = exp; df = exp; x = 0 4 print(f.__name__, x) h = 0.1 6 for k in range(1,21): ndf = (f(x+h) - f(x))/h 8 print( 'log(h) =', -k, 'rel. err. =', 10 abs( (ndf-df(x))/df(x) ) ) 12 h *= 0.1 log10(h) relativer Fehler -1 0.05170918075648 -2 0.00501670841679 -3 0.00050016670838 -4 0.00005000166714 -5 0.00000500000696 -6 0.00000049996218 -7 0.00000004943368 -8 0.00000000607747 -9 0.00000008274037 -10 0.00000008274037 -11 0.00000008274037 -12 0.00008890058234 -13 0.00079927783736 -14 0.00079927783736 -15 0.11022302462516 -16 1.00000000000000 Wir beobachten zuerst einen Abfall des relativen Fehlers, dann einen raschen Zuwachs des relativen Fehlers, wenn h < 10−8. Der Eﬀekt der Ausl¨oschung ist deut- lich st¨arker als die Konvergenzgeschwindigkeit der Approximation mit dem Quotien- tendiﬀerenz. Man kann die genaue Rundungsfehleranalyse in diesem Fall machen und erhalten ∣ ∣ ∣ ∣ex − df ex ∣ ∣ ∣ ∣ ≈ h + 2eps h → min f¨ur h = √ 2 eps . np.sqrt(2*np.finfo(np.float64).eps)= 2.1073424255447017 · 10−8. Ist Ausl¨oschung vermeidbar? Mit ein bisschen mehr Mathematik, ja! Beispiel 1.1.13. (Stabile Berechnung der Nullstellen eines quadratischen Polynoms) Wir kehren zur¨uck zur Aufgabe der Nussltellen von p(x) = x2 + ax + b. Um die Substraktion zu vermeiden, kann man sich nur die Nussltelle ausrechnen, die keine Substraktion braucht und die andere auf Grund von der Vieta Formel x2 = b/x1 berechnen. Eine stabile Implementierung ist: CHAPTER 1. VOR DEM START 12 Code 1.1.14: Stabile Implementierung der L¨osung quadratischer Gleichung def stabzerosquadpol(a,b): 2 \"\"\" compute the real roots of p(x) = x**2 + a*x + b 4 with the standard discriminant formula avoiding cancellation 6 \"\"\" 8 z = np.array([]) D = a**2 - 4*b 10 if D.any()<0: print('no real zeros') else: 12 wD = np.sqrt(D) z2 = np.where(a<0, (wD-a)/2, (-wD-a)/2) 14 z1 = b/z2 z = np.array([z1, z2]) 16 return(z) 0 200 400 600 800 1000 c 2 × 10 −16 3 × 10 −16relative error small root large root 0 200 400 600 800 1000 c 10 −16 10 −15 10 −14 10 −13 10 −12 10 −11relative error stable small root unstable small root Abb. 1.1.15. Links: stabile Berechnung der Nullstellen eines quadratischen Polynom mit Vieta Rechts: Ausl¨oschung bei der Berechnung des gr¨osseren Nullstele via Standardformel Beispiel 1.1.16. (Summe der Potenzreihen) Die Potenzreihe f¨ur ex konvergiert ¨uberall, so k¨onnte man ex beliebig gut ¨uberall approximieren, indem man die Summe der ersten Terme der Potenzreihe abbricht. CHAPTER 1. VOR DEM START 13 Code 1.1.17: Summe der ersten Terme der Potenzreihe von ex import numpy as np 2 def expeval(x, tol=1e-8): y = 1.; term = 1. # first term 4 k = 1 while abs(term) > tol*y: 6 term *= x/k # next summand y += term # summation 8 k += 1 return y 10 xs = np.arange(-20,21,2) 12 ys = np.array([expeval(x) for x in xs]) zs = np.exp(xs) 14 A = np.array([xs, ys, zs, abs(ys-zs)/zs]) print(A.T) 16 def println(line): 18 opt = \"\"; opt += str( int(line[0]) ); opt += \" & \" opt += '{:.12e}'.format(line[1]); opt += \" & \" 20 opt += '{:.12e}'.format(line[2]); opt += \" & \" opt += '{:.16f}'.format(line[3]) 22 return(opt) 24 print(\" \\\\\\\\\\n\".join([println(line) for line in A.T]) ) CHAPTER 1. VOR DEM START 14 x Approximation ˜exp(x) exp(x) | exp(x)−˜exp(x)| exp(x) -20 6.147561824192e-09 2.061153622439e-09 1.9825830337278934 -18 1.598372035943e-08 1.522997974471e-08 0.0494905855000887 -16 1.124750330014e-07 1.125351747193e-07 0.0005344259515302 -14 8.315441787438e-07 8.315287191036e-07 0.0000185918296272 -12 6.144210514234e-06 6.144212353328e-06 0.0000002993214527 -10 4.539992960354e-05 4.539992976248e-05 0.0000000035010444 -8 3.354626281246e-04 3.354626279025e-04 0.0000000006620042 -6 2.478752175842e-03 2.478752176666e-03 0.0000000003325188 -4 1.831563887901e-02 1.831563888873e-02 0.0000000005307235 -2 1.353352831996e-01 1.353352832366e-01 0.0000000002736031 0 1.000000000000e+00 1.000000000000e+00 0.0000000000000000 2 7.389056095384e+00 7.389056098931e+00 0.0000000004799685 4 5.459814992815e+01 5.459815003314e+01 0.0000000019230582 6 4.034287929504e+02 4.034287934927e+02 0.0000000013442484 8 2.980957980774e+03 2.980957987042e+03 0.0000000021025843 10 2.202646574759e+04 2.202646579481e+04 0.0000000021437994 12 1.627547911384e+05 1.627547914190e+05 0.0000000017238449 14 1.202604279794e+06 1.202604284165e+06 0.0000000036341346 16 8.886110500976e+06 8.886110520508e+06 0.0000000021979897 18 6.565996891074e+07 6.565996913733e+07 0.0000000034509718 20 4.851651930671e+08 4.851651954098e+08 0.0000000048287371 Wir beobachten: f¨ur grosse negative x ist die Approximation mit der Partialsumme der Pontezreihe katastrophal. In diesem Fall werden grosse Zahlen mit alternieren- den Vorzeichen summiert, so dass die Ausl¨oschung das Ergebnis bedeutungslos macht. Wir k¨onnen das vermeiden, indem wir ex = 1/e−x f¨ur x < 0 verwenden. Beispiel 1.1.18. (Ableitung via rein imagin¨aren Schritt) F¨ur das Vermeiden der Ausl¨oschung in der Approximation der Ableitung f ′(x0) via Diﬀerentenquotien bi- etet sich einen einfachen Trick an: ein rein imagin¨aren Schritt ih ∈ C mit i2 = −1. Wir nehmen hier an, die Funktion f ist sehr glatt um x0, und zwar als Funktion einer complexen Variable x ∈ C. In der Sprache der Vorlesung Mathematische Methoden nehmen wir an, dass f analytisch in einer Umgebung von x0. Konkret bedeutet dies, dass f sich schreiben l¨asst als die Summe einer Potenzreihe, die in einem complexen Disk um x0 konvergent ist: f (x) = ∞∑ j=0 aj(x − x0)j f¨ur alle x : |x − x0| < ρ mit festen ρ > 0, aj ∈ R . Das bedeutet auch, f ist unedlich mal diﬀerentierbar in einer Umgebung von x0 und die Ableitungen in x0 sind f (n)(x0) = n!an ∈ R. Unter diesen Voraussetzungen k¨onnen wir den Satz von Taylor mit einem rein imagin¨aren Schritt schreiben: f (x0 + ih) = f (x0) + f ′(x0)ih − 1 2 f ′′(x0)h2 − iC · h3 f¨ur R ∋ h → 0 . CHAPTER 1. VOR DEM START 15 Die Terme f (x0) und f ′′(x0)h2 sind reel, so verschwinden sie, wenn wir nur das Imagin¨arteil der oberen Gleichung betrachten: Imf (x0 + ih) = hf ′(x0) − ReC · h3 f¨ur R ∋ h → 0 . Wir erhaten also eine Approximation an der Ableitung, die so schnell wie h2 kon- vergiert: f ′(x0) = Imf (x0 + ih) h + ReC · h2 f¨ur R ∋ h → 0 , die auch keine Substraktion beinhaltet: f ′(x0) ≈ Imf (x0 + ih) h . Mehr Glattheit (und mehr Mathematik) erlaubt eine genaure (Fehler verh¨allt sich wie h2 statt h) und ausl¨oschungsfreie Formel, Abbildung 1.1.19. h 10 −15 10 −13 10 −11 10 −9 10 −7 10 −5 10 −3 10 −1relative error arctan diffd1 h diffih h 2 hrelative error sqrt diffd1 h diffih h 2 10 −16 10 −13 10 −10 10 −7 10 −4 10 −1 h 10 −15 10 −13 10 −11 10 −9 10 −7 10 −5 10 −3 10 −1relative error exp diffd1 h diffih h 2 10 −16 10 −13 10 −10 10 −7 10 −4 10 −1 hrelative error sin diffd1 h diffih h 2 Abb. 1.1.19. Relativer Fehler bei der Berechnung der Ableitung in x = 1.1 einiger glatten Funktionen mit dem Diﬀerenzenquotient und mit dem rein ima- gin¨aren Schritt. Die vorige Methode funktioniert perfekt wenn die Funktion sehr glatt ist und die Auswertung von Imf (x + ih) exakt ist. H¨atten wir in der Auswertung von f erhe- bliche Fehler (z.B. wenn f nicht analytisch vorhanden ist, sondern nur Auswertun- gen davon via Experimente m¨oglich sind), w¨aren diese bei der Division mit einem (m¨oglicherweise von der Genauigkeitsanforderungen an f ′(x) verlagter) kleinen h CHAPTER 1. VOR DEM START 16 massiv vergr¨ossert. Darum ist wichtig Alternativen zu haben. Der n¨achste Beispiel zeigt eine wichtige Technik zur Konvergenzbeschleunigung, die, bei einem rechtzei- itgen Abbruch, die Ausl¨oschung vermeiden (jedoch nicht verhindern) kann. Beispiel 1.1.20. (Konvergenzbeschleunigung nach Richardson / Bogolyubov-Krylov) Eine M¨oglichkeit, die Ausl¨oschung zu vermeiden ist, eine Methode zu verwenden, die so schnell konvergiert, dass wir gar nicht in den Bereich kommen, wo die Ausl¨oschung passiert. Als Beispiel betrachten wir wieder die Aufgabe der numerischen Approx- imation der Ableitung in einem Punkt x einer glatten gegebenen Funktion f . Die ersten Terme im Satz von Taylor um x sind: f (x + h) = f (x) + f ′(x)h + 1 2 f ′′(x)h2 + 1 6 f ′′′(x)h3 + 1 24 f (4)(x)h4 + . . . . (1.1.1) Schreiben wir dasselbe auch f¨ur x − h: f (x − h) = f (x) − f ′(x)h + 1 2 f ′′(x)h2 − 1 6 f ′′′(x)h3 + 1 24 f (4)(x)h4 + . . . . (1.1.2) Substarhieren wir die beiden letzten Gleichungen, k¨urzen sich die geraden Potenzen in h: f (x + h) − f (x − h) = 2f ′(x)h + 1 3 f ′′′(x)h3 + 1 60 f (5)(x)h5 + . . . . Daraus folgt f ′(x) = f (x + h) − f (x − h) 2h + 1 6 f ′′′(x)h2 + 1 120 f (5)(x)h4 + . . . . Das weist auf einer wie h2 konvergierede Approximation von f ′(x) via den sym- metrischen Diﬀerenzenquotien d(h) = f (x+h)−f (x−h) 2h hin. Leider leidet auch diese (bessere!) Methode von der Ausl¨oschung. Die wichtige Bemerkung nun ist, dass der Ausdruck des Fehlers f ′(x) − f (x + h) − f (x − h) 2h = n∑ k=1 1 (2k + 1)! f (2k+1)h2k + Ch2n+2 eine Summe von nur geraden Potenzen von h ist: f ′(x) − d(h) = c1h2 + c2h4 + . . . + cnh2n + Ch2n+2 , (1.1.3) wobei wir mit d(h) den symmetrischen Diﬀerenzenquotien und alle andere Faktoren der Potenzen in h mit c1, . . . , cn notiert haben. Die entscheidende Idee ist nun, dasselbe f¨ur einen anderen h zu schreiben. Zum Beispiel k¨onnen wir h halbieren: f ′(x) − d(h/2) = c1 1 22 h2 + c2 1 24 h4 + . . . + cn 1 22n h2n + ˜Ch2n+2 , (1.1.4) Wir bilden nun die lineare Kombination der letzten zwei Gleichungen, die den Term h2 eliminiert: f ′(x) − 22d(h/2) − d(h) 22 − 1 = ˜c2h4 + ˜c3h6 + . . . + ˜cnh2n + ˜Ch2n+2 , (1.1.5) CHAPTER 1. VOR DEM START 17 wobei ˜c2, . . . einfach neuere Koeﬃzienten der Potenzen von h notieren. Diese neue Formel d1(h) = 22d(h/2) − d(h) 22 − 1 liefert also eine Approximation an f ′(x), die wie h4 konvergiert. Wir k¨onnen den Trick nun wiederholen! Schreiben wir die Gleichung 1.1.5 nun f¨ur h und f¨ur h/2: f ′(x) − d1(h) = ˜c2h4 + ˜c3h6 + . . . + ˜cnh2n + ˜Ch2n+2 f ′(x) − d1(h/2) = ˜c2 1 24 h4 + ˜c3 1 26 h6 + . . . + ˜cn 1 22n h2n + ˜C ′h2n+2 . Die lineare Kombination lezten zwei Gleichungen, die den Term in h4 eliminiert, gibt f ′(x) − 24d1(h/2) − d1(h) 24 − 1 = ˜c′ 3 1 26 h6 + . . . + ˜c′ n 1 22n h2n + ˜C ′h2n+2 , (1.1.6) wobei ˜c′ 3, . . . einfach neuere Koeﬃzienten der Potenzen von h notieren. Der Fehler f¨ur die neue Formel d2(h) = 24d1(h/2) − d1(h) 24 − 1 sinkt also wie h6. Diese Prozedur kann man beliebig weiterverfolgen, solange die Glattheit von f es erlaubt. Notieren wir den symmetrischen Diﬀerenzenquotient dℓ,0 = d(h/2ℓ) , f¨ur die Levels ℓ = 0, 1, 2, . . . , , Die Approximationsformel nach einer linearen Kombination von diesen ist: Rℓ,1 = 22d( 1 2h) − d(h) 22 − 1 = 41Rℓ,0 − Rℓ−1,0 41 − 1 , die Approximationsformel nach einer linearen Kombination der letzten ist: Rℓ,2 = 24d1( 1 2h) − d1(h) 24 − 1 = 42Rℓ,1 − Rℓ−1,1 42 − 1 , und so weiter. Allgemein haben wir also die Deﬁnition f¨ur k = 1, 2, . . . , ℓ: Rℓ,k = 4kRℓ,k−1 − Rℓ−1,k−1 4k − 1 , (1.1.7) was der Schema von Richardson ℓ k=0 k=1 k=2 k=3 0 d(h) = R0,0 ց 1 d(h/2) = R1,0 → ց R1,1 ց 2 d(h/22) = R2,0 → ց R2,1 → ց R2,2 ց 3 d(h/23) = R3,0 → ց R3,1 → ց R3,2 → ց R3,3 ... ... ... ... ... (1.1.8) CHAPTER 1. VOR DEM START 18 entspricht. Die erste Spalte beinhaltet symmetrische Diﬀerenzenquotienten f¨ur verscheidene h. Die andere Elemente ergeben sich durch die liniare Kombinationen, die die Konver- genzordnung erh¨ohen. Auf der Diagonale enstehen also die Ergebnisse der Richard- son Schema mit den verschiedenen verwendeten h, also immer bessere Approxima- tionen. Der Fehler nimmt mit k und mit ℓ sehr schnell ab: f ′(x) − Rℓ,k = C · (h/2ℓ)2k+2 . W¨urde das Ergebnis ganz rechts unten nicht gen¨ugend sein, kann man das kleinste h noch ein mal verkleinern und nur noch eine Zeile in der oberen Tabelle unten berechenen und hinzuf¨ugen. Mit einem Fehlersch¨atzer k¨onnen wir die Richardson Schema abbrechen, wenn der Fehler als klein genug gesch¨atzt worden ist. Code 1.1.21: Beschleunigung des symmetrischen Diﬀerenzenquotient nach Richardson 1 import numpy as np 3 # Richardson extrapolation; fixed level for vectorisation def diffRichardsonV(f,x, h0, rtol=1e-12, atol=1e-12): 5 nit = 30 # max depth of iterations # first column at once 7 h = h0/2**np.arange(nit) fp, fm = f(x+h), f(x-h) 9 y = (fp-fm)/2/h # go column by column in Richardson 11 # we prefer to store it in the last part of the vector for j in range(1,nit): 13 fact = 4**j y[j:]=(fact*y[j:] - y[j-1:-1])/(fact-1) 15 errest = abs(y[j]-y[j-1]) print(j, h[j], errest) 17 if errest < rtol*abs(y[j]) or errest < atol: break 19 #return y[:j], h[:j] return y[:j+1], h[:j+1] # return the last computed io to show cancellation Diese Methode zur Konvergenzbeschleunigung funktioniert immer, wenn der Fehler sich als gerade Potenzen von h wie hier sich schreiben l¨asst. Bei Quadratur heisst das Romberg Schema; das verwendete Algorithmus heisst Aitken-Neville und die Idee ist bekannt als Richardson extrapolation, obwohl Richardson selber Bo- golyubov und N.M. Krylov zitiert. Selbstverst¨andlich sind auch andere Sequen- zen als die hier verwendete h, h/2, h/22, h/23, . . . m¨oglich und unter Umst¨anden (Bulirsch-Sequenz) n¨utzlich. Es ist jedoch wichtig zu betonen, die Konvergenzbeschleunigung kuriert nicht das Problem der Ausl¨oschung (im Unterschied zur Technik mit complexen Zahlen). Wenn wir das Algorithmus nicht vorzeitig via Fehelersch¨atzung abbrechen, sehen CHAPTER 1. VOR DEM START 19 wir bald auch die Ausl¨oschung. In der Abbildung 1.1.22 sehen wir in 3 F¨alle, wie die Ausl¨oschung im letzten Level der Richardson Schema passiert. h 10 −16 10 −14 10 −12 10 −10 10 −8 10 −6 10 −4 10 −2 10 0relative error arctan diffd2 diffRichardsonV h 2 h 8 hrelative error sqrt diffd2 diffRichardsonV h 2 h 8 10 −16 10 −13 10 −10 10 −7 10 −4 10 −1 h 10 −16 10 −14 10 −12 10 −10 10 −8 10 −6 10 −4 10 −2 10 0relative error exp diffd2 diffRichardsonV h 2 h 8 10 −16 10 −13 10 −10 10 −7 10 −4 10 −1 hrelative error sin diffd2 diffRichardsonV h 2 h 8 Abb. 1.1.22. Relativer Fehler bei der Berechnung der Ableitung in x = 1.1 einiger glatten Funktionen mit dem symmetrsichen Diﬀerenzenquotient und mit der Richardson Schema 1.2 Rechenaufwand Deﬁnition 1.2.1. Der Rechenaufwand (computational eﬀort / computational cost) eines numerischen Algorithmus ist die Anzahl elementarer Operationen (Additionen, Substraktionen, Multiplikationen, Divisionen). Manchmal werden auch Aufrufe el- ementarer Funktionen wie sin, cos, √·, exp, ln, etc. dazu mitgez¨ahlt. 70 Jahre zuvor war die Anzahl elementarer Operationen einen guten Indikator f¨ur die zu erwartende Laufzeit eines Programs, heute aber nicht mehr, ausser auf einer Art ungenauer assymptoticher Weise. Heute ist die Bandbreite vom Arbeitsspe- icher und somit die Art und Weise, wie die Daten in die Berechnungen gelesen und geschrieben werden, entscheidend f¨ur die Laufzeit eines Programs. Moderne Rechenarchitekturen verwenden verschiedene Levels von Cache: Daten, die sich im niedrigen Levels beﬁnden, werden deutlich schneller und wom¨oglich gleichzeitig bear- beiet, als Daten die sich in oberen Cache-Levels beﬁnden. Bestimmte Operationen, wie z.B. die implementierte grundlegende lineare Algebra Operationen (BLAS = Basic Linear Algebra Subprograms) wurden bereits so weit optimiert, dass alle zur Verf¨ugung stehende Cores eines Prozessors in Parallel die Daten in Cache bearbeiten. CHAPTER 1. VOR DEM START 20 Deﬁnition 1.2.2. Die asymptotische (Rechen-)Komplexit¨at eines Algorithmus charak- terisiert die ung¨unstigste (worst case) Abh¨angigkeit seines Rechenaufwands von einem oder mehreren Problemgr¨oßenparametern, wenn diese gegen ∞ wachsen. “Ung¨unstigst” bedeutet, dass der maximale Aufwand f¨ur einen Satz zul¨assiger Daten ber¨ucksichtigt wird. Ein Beispiel von Problemgr¨oßenparametern ist die L¨ange n → ∞ der Input/Output-Vektoren in ein Problem aus der linearen Algebra. Deﬁnition 1.2.3. Wir schreiben F (n) = O(G(n)) f¨ur zwei Funktionen F, G : N → R, falls es gibt eine Konstante C > 0 und ein n∗ ∈ N so dass F (n) ≤ C G(n) f¨ur alle n ≥ n∗ . Anschaulich: F (n) = O(G(n)) bedeutet F w¨achst h¨ochstens genauso schnell wie G. Im Kontext der asymptotische Komplexit¨at versteht sich die Landau Notation auch “scharf” im Sinne, es gibt keine Funktion, die langsamer w¨achst als g, die in O(·) verwendet werden kann. D.h: F (n) = O(G(n)) bedeutet in unserem Kontext immer etwas pr¨asizer: F w¨achst genauso schnell wie G. Zum Beispiel: die Gauss- Elimination braucht f¨ur allgemeine n × n Matrizen O(n3) elementare Operationen; die Gauss-Elimination braucht f¨ur obere Dreiecksmatrizen O(n2) Operationen und f¨ur Diagonalmatrizen O(n) Operationen. Die angegeben Komplexit¨aten beziehen sich jeweils auf dem worst case Szenario in der Klasse der betrachteten Matrizen und k¨onnen jeweils nicht durch lansgamer wachsende Funktionen in n ersetzt werden. Die praktische Bedeutung der asymptotischer Complexit¨at ist eine (unpr¨azise) Voraussage f¨ur die Laufzeit einer Implementierung des Algorithmus. Zum Beispiel, ein Algorithmus mit Complexit¨at O(n2) k¨onnte 4× l¨anger Zeit brauchen, wenn die Problemgr¨osse sich verdoppelt, f¨ur n so gross, dass die Cache-Levels und sonstiger Einﬂuss der Hardware vernachl¨assigbar sind. Die O(h)-Notation wird manchmal auch f¨ur asymptorisches Verhalten h → 0 “misbraucht”. In den vorigen Beispiele haben wir gesehen, das Fehler verhielt sich in einem Fall theoretisch (bis Ausl¨oschung dominiert) wie C · h und in dem anderen Fall wie C ·h2. Korrekterweise m¨usste man o(h) und o(h2) schreiben, um zu betonen, dass h → 0 und nicht h → ∞ betrachtet wird. Damit wir die actuelle Hardware optimal verwenen k¨onnen, m¨ussen wir die the- oretisch optimalen Algorithmen mit Bedacht implementieren. BLAS ist eine Samm- lung von Speziﬁkationen, die aus der 1970-Jahren stammt, so ist sie (immer noch) in FORTRAN77 implementiert. Wrappers f¨ur andere Programmiersprachen existieren, sowie professionele Implementierungen: OpenBLAS, ATLAS (Automatically Tuned Linear Algebra Software – standard in Linux), Intel MKL (Math Kernel Linbrary). In dieser Vorlesung verwenden wir Python, die f¨ur rasche, ﬂexible und sichere Implementierungen konzipiert worden ist. Es lohnt sich einiges im Interent ¨uber die Entstehung und Prinzipien von Python zu lesen, einige dazugeh¨orige Spruche sind legend¨ar (z.B.: “batteries included”, “we are all adults here”). Um diese Ziele zu erreichen musste man gewisse Compromisse eingehen, die sich negativ auf die Laufzeit Python-Programme auswirken (z.B kostenspielige Kopien, Ausschluss von parallelen Threads). Jedoch langsame Teile von schnell entwickelte und sicher CHAPTER 1. VOR DEM START 21 getestete Python-Programme k¨onnen zu einer schnell ausf¨uhrbare Sprache migriert und im Python-Programm leicht eingebunden werden. Die Verwendung von spezial- isierten Bibliotheken wie numpy nimmt dem Programmierer sogar diesen Aufwand ab. Die Python-Bibliothek numpy stellt die Verbindung zu optimierten BLAS Im- plementierungen und zu optimierten FORTRAN- oder C- oder C++-Implementierungen wichtiger numerischen Algorithmen, so dass die Computer-Hardware aus Python aus optimal verwendet wird. Wenn wir also numpy geschickt verwenden, k¨onnen wir die bestm¨ogliche Rechenzeit erreichen, ohne dass wir uns M¨uhe mit komplizierten FORTRAN- oder C- oder C++-Aufrufe und Verlinkungen besch¨aftigen. Die Python- Bibliothek scipy baut auf numpy auf, gibt zum Teil andere, ﬂexiblere Funktionssig- naturen oder gar andere Implementierungen f¨ur mache Algorithmen (manche nur in Python mit numpy implementiert) und beinhaltet viele Sammlungen von Bibli- totheken, die auf bestimte Bereiche spezialisiert sind, z.B. integrate, interpolate, optimisation, linalg, special, stats, fft, sparse, signal, io. Manche Funk- tionen bauen auf Aufrufe von Funktionen, die in FORTRAN- oder C- oder C++ im- plementiert, kompiliert und verlinkt sind, manche sind numpy-basierte Python Im- plementierungen. Wir verwenden auch die Bibliothek sympy, die (noch bescheidene aber gute) symbolische Berechnungen erm¨oglicht und die sehr weit verwendete und extrem ﬂexible Bibliothek matplotlib um leicht die n¨otigen (meist 2D-) Plots zu erzeugen. Das wichtigste Datentyp von numpy ist ndarray. Bitte lesen Sie die Tutorials und Dokumentationen dazu. Wichtig, um die optimale Laufzeit zu er- reichen ist: die Daten soweit m¨oglich in Array auf einmal (ohne Schleifen, d.h. vektorisiert) verwenden, wo m¨oglich Kopien vermeiden, z.B. indem man slices und views verwendet und gen¨ugend Speicher rechtzeitig belegen. Wichtig um schwer auﬃndbare Bugs zu vermeiden ist: Speicher eines ndarray immer mit dem richtigen Datentyp initialisieren. Das scheint zum Teil gegen welcher Grund- prinzipien von Python und gegen welcher Wunsche nach Flexibilit¨at zu verstossen, ist jedoch n¨otig: mit der Verwednung von numpy haben wir Laufgeschwindigkeit gegen ein bisschen Sicherheit getauscht. Wenn wir eine complexe Zahl in einem Ele- ment eines arrays, der (automatisch) mit floats initialisiert worden ist, schreiben, werden wir das vielleicht f¨ur das Algorithmus wichtige Imagin¨arteil stillschwiegend verlieren: viel Spass bei der zeitraubenden Bug suche! Unvohersehebare und oft nicht reproduzierbare Abst¨urze k¨onnen beim Schreiben auf deklarieter jedoch nicht initialisierten Speicher passieren - die C und C++-Progammierer kennen das. Im Umgang mit ndarrays ist auch wichtig, eine echte Kopie zu erzwingen, wenn man eine braucht, denn auf Grund von Eﬃzienz machen ndarrays oft gar keine (echte) Kopie. Diese Mehrarbeit kann auch die Suche nach manche ¨uble Bugs ersparren, wenn ein ndarray, der nicht zu ver¨andern gilt, pl¨otzlich wegen einem slice oder shallow copy neue Werte enth¨allt. Probieren Sie: CHAPTER 1. VOR DEM START 22 Code 1.2.4: View ist schnell, aber Achtung bei ¨Anderungen! 1 import numpy as np n = 4 3 A = np.diag(np.mgrid[1:n+1]) A[:, -1] = A[-1,:] = np.mgrid[1:n+1] 5 print('A=\\n', A) B = A[::-1,:] # B is not a copy, but another view on A 7 print('B=\\n', B) # note: rows of A in reversed order A[2,2] = -2 9 print('A=\\n', A) # changed an element on the diagonal of A print('B=\\n', B) # the corrspondig elment \"of B\" changed! 11 B[2,2] = -10 print('B=\\n', B) # changed an element on the diagonal of B 13 print('A=\\n', A) # the corrspondig elment \"of A\" changed! print('B.base=\\n', B.base) Noch eine grosse Warnung: verwenden Sie niemals numpy.vectorize! Iro- nischerweise vektorisiert diese Funktion nicht so, wie man sich w¨unscht, sondern schreibt eine sehr langsam in Ausf¨uhrung Schleife um den Code. Das steht zwar als erstens in der Note der Dokumentation, wird aber sehr leicht und sehr gerne ig- noriert, denn die Aufgabe einen Code vektorisiert zu schreiben ist zwar sehr wichtig aber manchmal auch sehr schwierig... Ein Beispiel von immer wichtiger echter Vektorisierung und gewissen Kompro- missen f¨ur Sicherheit sehen wir in den vorigen Code 1.1.21: wir setzten eine harte obere Schranke f¨ur die Iteration (nit=30) und verwenden im folgenden ndarrays dieser Dimension, auch wenn de facto die Iteration viel fr¨uher (circa 7) abbricht. Ein ndarray namens h wird auf einmal gebaut und die Funktionswerte f¨ur alle diese einzelnen h werden auf einmal (vektorisiert) in den Aufrufen fp = f(x+h) und fm = f(x-h) erzeugt. y wird auch vektorisiert erzeugt (y = (fp-fm)/2/h) und via Slices (also keine Kopien!) y[j:] in jedem Schritt der unvermeidbarer j-Schleife ver¨andert. Der Preis f¨ur diese sehr schnell laufende Implementierung ist: ein grosser Teil vom bereitgestellten Speicher wird in unseren Beispielen gar nicht verwendet; wenn in einem Fall mehr als nit=30 n¨otig w¨aren, w¨are das direkt nicht m¨oglich, son- dern man m¨usste nit von Hand erh¨ohen und das Program nochmals laufen lassen. Das Erweitern einem ndarray ist sehr teuer und sollte man wenn m¨oglich vermeiden, geanu so wie explizite Schleifen (z.B. for) durch die Elemente einem ndarray. Als ¨Ubung, schreiben Sie den Code 1.1.12 in eine vektorisierter Version (ohne Schleifen) um. Warum k¨onnen wir nicht ohne weiteres dasselbe mit dem Code 1.1.17 tun? Schlagen Sie trotzdem eine Umschreibung vom Code 1.1.17 in vektorisierter Form vor und besprechen Sie die Vor- und Nachteile Ihrer Implemenierrung. Hier sind einige ¨Ubungen und Beispiele f¨ur die Indizierung, slices und views: CHAPTER 1. VOR DEM START 23 Code 1.2.5: ¨Ubungen mit Indizierungen 1 \"\"\"https://numpy.org/doc/stable/user/basics.indexing.html#advanced-indexing\"\"\" x = np.arange(10) # 0,1,2,...,9 3 print(x[0], x[-1], x[-2]) # 0, 9, 8 print(x.reshape(2,5)) # makes a copy 5 print(x) # no change x.shape = (2,5) # modifiy the atribute shape of this x 7 print(x) # see: now x is 2-dimensional print(x[0]) # 0 refers now to the first row in x 9 # x[0] is a view, not a copy: x[0][2] = -2; print(x) 11 x[0,2] = 2; print(x) # more efficient than x[0][2] # be caution with the data type! 13 x[0,2] = 2.5; print(x) # 2.5 was cast to the data type of x (int) # basic slicing returns a view 15 y = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) print(y[1:7:2]) # ([1, 3, 5] 17 print(y[-2:10], y[-2:]) # [8, 9] print(y[-3:3:-1]) # [7 6 5 4] 19 print(y[2], y[2:3]) # watch the difference! print(x) 21 print(x[1:10:5, ::-1]) # is the same as the more general: obj = (slice(1, 10, 5), slice(None, None, -1)); print(x[obj]) 23 x = np.array([[[1],[2],[3]], [[4],[5],[6]]]) print(x.shape); print(x[1:2]) 25 print(x[..., 0]) # is the same as print(x[:, :, 0]) 27 print(x[:, np.newaxis, :, :].shape) # is synomim to: print(x[:, None, :, :].shape) 29 # advanced indexing returns a copy x = np.arange(10, 1, -1); print(x) 31 print(x[np.array([3, 3, 1, 8])]) # [7, 7, 9, 2] y = np.arange(35).reshape(5, 7); print(y) 33 print(y[np.array([0, 2, 4]), np.array([0, 1, 2])]) print(y[np.array([0, 2, 4]), 1] ) # elements [0,1], [2,1], [4,1] 35 print(y[np.array([0, 2, 4])]) # copies of the rows 0,2,4 of y # select corners of a 4x3 array: 37 x = np.arange(12).reshape(4,3); print(x) rows = np.array([[0,0], [3,3]]) 39 cols = np.array([[0,2], [0,2]]) print( x[rows, cols] ) 41 # shorter via broadcasting: rows = np.array([0, 3]) 43 cols = np.array([0, 2]) print( rows[:, np.newaxis] ) 45 print( x[rows[:, np.newaxis], cols] ) # even shorter: 47 print( x[np.ix_(rows, cols)] ) print( np.ix_(rows, cols) ) 49 # note the difference: print( x[rows, cols] ) CHAPTER 1. VOR DEM START 24 Code 1.2.6: Weitere ¨Ubungen mit Indizierungen # boolean array inedixng 2 x = np.array([[1., 2.], [np.nan, 3.], [np.nan, np.nan]]) print( x[~np.isnan(x)] ) # [1.,2.,3.] 4 x = np.array([1., -1., -2., 3]) x[x < 0] += 20 6 print(x) # [ 1., 19., 18., 3.] x = np.arange(35).reshape(5, 7) 8 b = x > 20 print( b[:, 5] ) # [False, False, False, True, True] 10 print( x[b[:, 5]] ) x = np.array([[0, 1], [1, 1], [2, 2]]) 12 rowsum = x.sum(-1) print( x[rowsum <= 2, :] ) 14 # combining advanced and baisc indexing creates concatenations y = np.arange(35).reshape(5,7) 16 print( y[np.array([0, 2, 4]), 1:3] ) # asigning 18 x = np.arange(10) x[2:5] = 1; print(x) 20 x[2:5] = np.arange(3); print(x) # No surprise, because adv. indexing makes a copy, but confusing! 22 x = np.arange(0, 50, 10); print(x) x[np.array([1, 1, 3, 1])] += 1; print(x) 24 # variabe number of indicies z = np.arange(81).reshape(3, 3, 3, 3) 26 indices = (1, 1, 1, 1); print( z[indices] ) # 40 indices = (1, 1, 1, slice(0, 2)) # same as [1, 1, 1, 0:2] 28 print( z[indices] ) # [39, 40] indices = (1, Ellipsis, 1) # same as [1, ..., 1] 30 print( z[indices] ) In dieser Vorlesung versuchen wir ein gutes Beispiel zu geben, wie man vektorisiert, was es geht, ohne das die Lesbarkeit und vor allem die Korrektheit zu verspielen. “Versuchen” ist das richtige Wort: die Codes wurden in laufen vielen Jahren und von verscheidenen Personen geschrieben, so dass manche Inkonsistenzen und Veralterung sind unumg¨anglich, so bitte ich schon jetzt um Verzeihung. 1.3 Rechnen mit Matrizen Aus mathematischen Gesichtspunkt betrachten wir Vektoren immer als Spalten, also x ∈ C m ist immer eine m × 1-Matrix; wenn wir den Vektor als Zeile haben wollen, schreiben wir xT ; wenn wir ihn als Zeile und complex konjugiert brauchen, dann notieren wir xH. Wir kennen aus der linearen Algebra den Skalarpodukt zweier Vektoren b, a aus C m: ⟨b, a⟩ = b · a = b Ha = n∑ k=1 bkak ∈ C CHAPTER 1. VOR DEM START 25 und den Tensorprodukt (dyadisches Produkt, manchmal irref¨uhrend ¨ausseres Pro- dukt genannt) der Vektoren b ∈ C m und a ∈ C n: baH = [biaj] ∈ C m,n als spezielle F¨alle von Produkt der Matrizen A ∈ C m,n und B ∈ C n,p: AB = [ n∑ j=1 aijbjk ] ∈ C m,p . Man kann diese Multiplikation auch so visualisieren: m n p p m n = und auch die zwei spezielle F¨alle: der erste Fall entspricht dem (Euklidischen) Skalarprodukt, wenn das Ergebnis ein Skalar ist, der zweite Fall entspricht dem Tensorprodukt (dyadischen Produkt) zweier Vektoren, wenn das Ergebnis eine Ma- trix vom Rang 1 ist: m m = m n = n m Da in numpy die 1-dimensionale arrays immer Zeilen sind, muss man in den Imple- mentierungen aufpassen: CHAPTER 1. VOR DEM START 26 Code 1.3.1: Verschiedene Arten den Skalarprodukt zu berechnen a = np.array([1., 2., 3.]) # 1. ensures data type is float 2 b = np.array([4.j, 5.j, 6.j]) # 1j = complex imaginary unit print( a*b ) # use broadcasting: array as result, not a scalar product 4 print( (a*b).sum(), np.sum(a*b) ) # object functions should be faster print( b @ a ) # no conjugation 6 print( np.dot(b,a) ) # no conjugation, better style print( np.vdot(b,a) ) # conjugationn of first argument 8 print( np.dot( b.conj(), a) ) # manual conjugation; b.conj() makes no copy print( (b.conj()*a).sum(), np.sum(np.conjugate(b)*a) ) # last makes a copy 10 print( np.tensordot(b, a, axes = 1) ) # same as dot print( np.tensordot(b.conj(), a, axes = 1) ) # same as vdot 12 # look up np.einsum print( np.einsum('i,i',b,a) ) # np.dot(b,a) 14 print( np.einsum('i,i',b.conj(),a) ) # np.vdot(b,a) 16 # make them 2D ndarrays; caution on type and shape of output A = a.reshape(1,3); print(A) # view, no copy 18 B = b.reshape(1,3); print(B) print( A @ b) 20 # print( A @ B) \"\"\"mismatch\"\"\" print( B @ A.T) # real scalar product of b and a 22 print( B.conj() @ A.T) # complex scalar product of b and a print(A @ b, A @b.conj()) Weitere wichtige Funktionen sind tensordot, outer, kron und vor allem das sehr elegante und universelle einsum (allgemein bekannt als Einstein Notation, erfunden eigentlich von Ricci). einsum kann viel schneller und RAM sparsamer als @, sum, transpose aber langsamer als dot, inner, tensordot (diese verwenden BLAS) sein, braucht aber mehr Zeit zum lernen; es gibt Einf¨uhrungen, die besser als die numpy-Dokumenation sind, suchen Sie nach “einstein summation in numpy”: CHAPTER 1. VOR DEM START 27 Code 1.3.2: Tensor-, ¨aussere, Kronecker Produkt und Einstein Summation a = np.array([1., 2., 3.]) # 1. ensures data type is float 2 b = np.array([4.j, 5.j, 6.j]) # 1j = complex imaginary unit print( a*b ) # use broadcasting: array as result 4 print( np.tensordot(b, a, axes = 1) ) # same as dot 6 print( np.outer(b,a) ) print( np.outer(b.conj(),a) ) 8 print( np.tensordot(b, a, axes = -1) ) # tensor product of b and a print( np.tensordot(b.conj(), a, axes = -1) ) # tensor product 10 # look up np.einsum print( np.einsum('i,i',b,a) ) # np.dot(b,a) 12 print( np.einsum('i,j',b,a) ) # outer product print( np.einsum('i,i->i',b,a) ) # b*a 14 # make them 2D ndarrays A = a.reshape(1,3); print(A) 16 B = b.reshape(1,3); print(B) print( A @ b) 18 # print( A @ B) \"\"\"mismatch\"\"\" print( B @ A.T) # real scalar product of b and a 20 print( B.conj() @ A.T) # complex scalar product of b and a print( B.T @ A) # tensor product of b and a 22 print( B.T.conj() @ A) # tensor product of b and a and conjugation print( np.asmatrix(b).H @ A ) 24 print( np.asmatrix(b).H * a ) # broadcasting print( b[:, np.newaxis] * a )# broadcoasting 26 print( b[:, np.newaxis].conj() * a )# broadcoasting 28 A, B = np.array([[1,2],[3,4]]), np.array([[10,11],[12,13]]) print(A, B) 30 print( np.outer(A,B) ) print( np.kron(A,B) ) # Kroneker product 32 print( np.einsum('ij,kl->ikjl',A,B).reshape(4,4) ) # Kroneker product print( np.einsum('ik,kj->ij',A,B) ) # A @ B 34 print( np.einsum('ik,kj->ji',A,B) ) # (A @ B).T print( np.einsum('ii->i',A) ) # diag(A) 36 print( np.einsum('ii->',A) ) # trace(A) x = np.array([-1., -2.]) 38 print( np.einsum('ij,j->i',A,x) ) # A @ x print( np.einsum('i,ij->j',x,A) ) # x.dot(A) 40 print( np.einsum('s,st,t->',x,A,x) ) # x.dot(A @ x) quadratic form Hier haben wir mit winzigen klaren Arrays gearbeitet, um die Eﬀekte jedes Befehls sehen zu k¨onnen. Aufgabe: bauen Sie sehr grosse (soweit der Arbeitspeicher reicht) volle zuf¨allige (np.random.rand) Arrays und probieren Sie jedes Befehl in ipython oder jupyther mit dem Preﬁx timeit, z.B.: timeit np.sum(np.conjugate(bb)*aa) um die zu erwartenden Unterschiede in der Rechenzeit abzusch¨atzten. Wenn Sie auch Lesbarkeit des Codes ber¨ucksichtigen, welche der vorigen Rechenarten sind zu bevorzugen? Nutzen Sie konsistent Ihre bevorzugte Variante! CHAPTER 1. VOR DEM START 28 Bemerkung 1.3.3. Das Ergebnis der Multiplikation einer m × n Matrix A mit einem Vektor x ist eine lineare Kombination der Spalten von A: A x = n∑ i=j xi A:,j . Bemerkung 1.3.4. Der Rang des Ergebnisses der Matrixmultiplikation ist der kle- inste Rang der Faktoren: Rank(AX) = min(RankA, RankX) . In der Tat, jede Spalte im Ergebnis B der Matrixmultiplikation AX = B ist eine lineare Kombination der Spalten von A: A [X:,1 X:,2 · · · X:,p] = [B:1 B:,2 · · · B:,p] (1.3.9) AX:,1 = B:,1 AX:,2 = B:,2 ... AX:,p = B:,p AX = B . In der Tat, die Koeﬃzienten in dieser linearen Kombination sind die Eintr¨age in den entsprechenden Spalten von X, z.B. B:,1 = ∑n j=1 xj,1A:,j. Somit ist RankB ≤ RankA. Jede Zeile im Ergebnis B der Matrixmultiplikation AX = B ist eine lineare Kom- bination der Zeilen von X:    A1,: ... Am,:    X =    B1,: ... Bm,:    . (1.3.10) In der Tat, die Koeﬃzienten in dieser linearen Kombination sind die Eintr¨age in den entsprechenden Spalten von A, z.B. B1,: = A1,:X bedeutet B1,: = ∑n k=1 A1,kXk,:. Somit ist RankB ≤ RankX. Damit ist der Rang des Ergebnisses der kleinste Rang von A und X. Bemerkung 1.3.5. Die Formel 1.3.9 zeigt das Ergebnis der Matrixmultiplikation als eine Matrix, die aus Spalten B:,j = AX:,j gebaut ist, w¨ahrend die Formel 1.3.10 zeigt sie als eine Matrix, die aus Zeilen Bi,: = Ai,:X gebaut ist. Je nach Struktur der Matrizen A und X kann die eine oder die andere Sichtweise vorteilhaft sein, z.B: CHAPTER 1. VOR DEM START 29 n m 0 p n = 0 p m und 0 n m p n = 0 p m Die Teile, die ein Block aus Nulle multipliziert, ist f¨ur das Ergebniss nicht relevant, Speicher und/oder Arbeit kann man sich daf¨ur sparen: n m 0 p n = p m Die Besetzungsmuster und die Rechenregel sind sehr wichtig bei Berechnungen mit Matrizen, die vorwiegend 0 Eintr¨age haben: d¨unnbestezte (sparse) Matrizen. Z.B., die Ergebnisse der Multiplikation von Pfeil-Matrizen: = CHAPTER 1. VOR DEM START 30 = = Bemerkung 1.3.6. Die Menge der Diagonalmatrizen, bzw. der unterer Dreick- esmatrizen, bzw. der oberer Dreiecksmatrizen sind jeweils geschlossen f¨ur Matrix- multiplikation: = = CHAPTER 1. VOR DEM START 31 = Vorw¨arts- bzw. R¨uckw¨artssubstitution zeigt, dass die Inverse von nicht-singul¨aren unterer bzw. oberer Dreickesmatrizen auch untere bzw. obere Dreiceksmatrizen sind; dasselbe gilt nat¨urlich auch f¨ur invertierbare Diagonalmatrizen. Bemerkung 1.3.7. Eis ist auch wichtig zu merken, was der Unterschied im Ergebnis der Multiplikation mit einer Diagonalmatrix von links oder von rechts auf einer anderen Matrix ist. Die Multiplikation mit einer Diagonalmatrix von links skaliert die Zeilen:      d1 0 0 0 d2 0 . . . 0 0 dn           a11 a12 . . . a1m a21 a22 a2m ... ... an1 an2 . . . anm      =      d1a11 d1a12 . . . d1a1m d2a21 d2a22 . . . d2a2m ... ... dnan1 dnan2 . . . dnanm      =    d1A1,: ... dnAn,:    . Multiplikation mit einer Diagonalmatrix von rechts skaliert die Spalten:      a11 a12 . . . a1m a21 a22 a2m ... ... an1 an2 . . . anm           d1 0 0 0 d2 0 . . . 0 0 dm      =      d1a11 d2a12 . . . dma1m d1a21 d2a22 . . . dma2m ... ... d1an1 d2an2 . . . dmanm      = [ d1A:,1 . . . dmA:,m ] . Bemerkung 1.3.8. Die blinde Verwendung von Prozeduren aus numpy garantiert nicht die bestm¨ogliche Rechenzeit. Denken vor Rechnen! Beispiel 1.3.9. Sei A eine zuf¨allige volle n × n Matrix und D eine zuf¨allige n × n Diagonalmatrix. Die direkte Multiplikation DA via dem oﬀensichtlichen D @ A kostet O(n3) Operationen. Wenn wir die vorige Bemerkung zur Multiplikation mit Diagonalmatrizen verwenden, d.h. die Struktur der d¨unnbesetzten Matrix D ver- wenden, indem wir nur die Zeilen von A mit den jeweiligen Eintr¨age aus der Diago- nale von D skalieren, kostet das nur O(n2) Operationen. Dabei m¨ussen wir nat¨urlich darauf aufpassen, dass wir keine unn¨otige Kopien erstellen und dass wir Schleifen vermeiden. Eine solche Implementierung ist: D.diagonal()[:,np.newaxis] * A. CHAPTER 1. VOR DEM START 32 Hier nahmen wir an, die grosse Diagonalmatrix sei gegeben, so brauchen wir eine View (keine Kopie) der Diagonale mit D.diagonal() und nutzen np.newaxis um Broadcasting zu erwirken. Die Rechenzeiten auf meinem Laptop in August 2025 sind in Abbildung 1.3.10. Denken vor Rechnen zahlt sich aus. 10 1 10 2 10 3 10 4 size n 10 −6 10 −5 10 −4 10 −3 10 −2 10 −1 10 0 10 1 10 2 ime (s) D @ A D.diagonal()[:,np.newaxis] * A O( n 3 ) O( n 2 ) Abb. 1.3.10. Rechenzeiten f¨ur die zwei Arten, die Multiplikation mit einer Diag- onalmatrix zu berechnen. Bemerkung 1.3.11. Wenn die Verwendung von timeit in ipython aus welche Grunde nicht m¨oglich ist, kann man die langen Laufzeiter eines Teils des Codes so messen: Code 1.3.12: M¨ogleiche Laufzeitmessung f¨ur func 1 import time 3 nrEXP = 10 #number of experiments ts = [] # measured times 5 for k in range(nrEXP): start = time.process_time() 7 result = func() # call the function/part we measure end = time.process_time() 9 ts.append(end - start) print(f'Min. Time taken: {min(ts):.6f} seconds') CHAPTER 1. VOR DEM START 33 Die Aufrufe messen “wall time”, das stark von allen weiteren gleichzeitigen Prozesse auf dem Rechner beeinﬂusst sind, so m¨ussen wir viele Messungen machen; die klein- ste gemessene Zeit ist das besste, was wir mit diesem Experiment aussagen k¨onnen. F¨ur sehr lange Rechenzeiten kann es sein, dass man sich wenigere Experimente sich leistet, man sollte jedoch so viele wie m¨oglich, mindestens 3, machen, sonst sind die Angaben irrelevant und zu pessimistisch. Man kann diese Funktion auch in einem decorator schreiben: Code 1.3.13: M¨oglicher decorator f¨ur Laufzeitmessung import time 2 def mytimeit(rep=1): # decorator with argument 4 def mytimeit_real_decorator(func): # actual decorator def wrapper(*args, **kwargs): 6 ts = [] # measured times for k in range(rep): 8 start = time.process_time() result = func(*args, **kwargs) 10 end = time.process_time() ts.append(end - start) 12 print(f'Min. Time taken: {min(ts):.6f} seconds in ',func.__name__) wrapper.ats.append(min(ts)) # record the minimal measured time 14 return result wrapper.ats = [] # declare atribute to use later 16 return wrapper return mytimeit_real_decorator Vor der Deﬁnition einer Funktion, die wir messen wollen schreiben wir @mytimeit(rep=nrEXP), probieren Sie z.B. nrEXP = 10 @mytimeit(rep=nrEXP) def slowmultiply(D,A): X = D @ A return(X) slowmultiply(D,A) print(slowmultiply.ats) slowmultiply(D,A) print(slowmultiply.ats) Bemerkung 1.3.14. Aus der Linearen Algebra wissen wir, es ist manchmal wichtig, Operationen mit Zeilen oder Spalten einer Matrix via Matrixmultiplikationen math- ematisch darzustellen. Sei i > j. Die Operation mit Zeilen Bi,: = (−2)Aj,: + Ai,: CHAPTER 1. VOR DEM START 34 realisiert man via Matrixmultiplikation von Links mit der Elementarmatrix E(ij), dessen Elemente, die der Diagonalmatrix sind, ausser das (i, j)-Element (unterhalb der Hauptdiagonale, da i > j), der −2 ist, z.B.: E(2,1)A =   1 0 0 −2 1 0 0 0 1    a11 a12 a13 a21 a22 a23 a31 a32 a33   =   a11 a12 a13 −2a11 + a21 −2a12 + a22 −2a13 + a23 a31 a32 a33   . Multiplikation von rechts mit einer solchen Elementarenmatrix agiert mit den Spal- ten von A: AE(2,1) =  a11 a12 a13 a21 a22 a23 a31 a32 a33     1 0 0 −2 1 0 0 0 1   =  a11 − 2a12 a12 a13 a21 − 2a22 a22 a23 a31 − 2a32 a32 a33   , d.h. B:,j = A:,i + (−2) ∗ A:,j, da i > j. F¨ur i < j haben wir B:,j = (−2) ∗ A:,i + A:,j: AE(1,2) =  a11 a12 a13 a21 a22 a23 a31 a32 a33    1 −2 0 0 1 0 0 0 1   =  a11 −2a11 + a12 a13 a21 −2a21 + a22 a23 a31 −2a31 + a32 a33   . Bemerkung 1.3.15. Das Produkt zweier Matrizen kann auch blockweise berech- net werden. Dabei muss man nur darauf achten, dass die Multiplikation von Ma- trizen/Bl¨ocke nicht kommutativ ist. Seien die Dimensionen M, N, K ∈ N und die Blockl¨angen 1 ≤ n < N (n′ := N − n), 1 ≤ m < M (m′ := M − m), 1 ≤ k < K (k′ := K − k); nehmen wir die Matrizen A11 ∈ R m,n A12 ∈ R m,n′ A21 ∈ R m′,n A22 ∈ R m′,n′ , B11 ∈ R n,k B12 ∈ R n,k′ B21 ∈ R n′,k B22 ∈ R n′,k′ , die wir als Bl¨ocke verwenden, um gr¨ossere Matrizen zu formen: A = [A11 A12 A21 A22 ] ∈ R M,N , B = [B11 B12 B21 B22 ] ∈ R N,K . Das Produkt AB kann man mit der selber Formel wie das Product einfacher 2 × 2- Matrizen ausrechnen: [A11 A12 A21 A22 ] [B11 B12 B21 B22 ] = [A11B11 + A12B21 A11B12 + A12B22 A21B11 + A22B21 A21B12 + A22B22 ] . (1.3.11) Name Operation #{*,/} #{+,-} Komplexit¨at Skalarprodukt x ∈ R n, y ∈ R n → xHy n n-1 O(n) Tensorprodukt x ∈ Rm, y ∈ R n → xyH nm 0 O(mn) Matrix × Vektor x ∈ R n, A ∈ R m,n → Ax mn (n-1)m O(mn) Matrixprodukt A ∈ R m,n, B ∈ R n,p → AB mnp (n-1)mp O(mnp) Bemerkung 1.3.16. F¨ur m = n = p kann man den Matrixpordukt via den divide- and-conquer Strassen Algorithmus dank Block-Partiniorierung und Rekursivit¨at zur asymtotischen Komplexit¨at von O(nlog2 7) und sogar O(n2.36), ist jedoch bedeutungs- los f¨ur Anwendungen. CHAPTER 1. VOR DEM START 35 Bei der Multiplikation mit Diagonalmatrizen haben wir den Eﬀekt von “Denken vor Rechnen” gesehen: statt O(n3) kann man einfach O(n2) erreichen, wenn man elementare lineare Algebra verwendet. Hier kommen weitere Beispiele dazu. Beispiel 1.3.17. Matrizen von Rang 1 kann man immer als Tensorprodukt zweier Vektoren schreiben, was f¨ur eine geschickte weitere Rechnung n¨utzlich sein kann: y = (ab T ) x kostet O(mn) Operationen, denn die volle m × n-Matrix ( ab T ) multipliziert x. Im Gegenteil: y = a (b T x) kostet O(m + n) Operationen, denn b T x ist das Skalarpodukt. 10 1 10 2 10 3 10 4 size n 10 −6 10 −4 10 −2 10 0 10 2time (s) ( ab T ) x a ( b T x ) O( n 2 ) O(n) Abb. 1.3.18. Rechenzeiten f¨ur die zwei Arten, die Multiplikation mit einer Matrix von Rang 1 zu berechnen. Beispiel 1.3.19. Seien A, B ∈ R n,p mit p ≪ n, also mit kleinem Rang, und sei R das obere/rechte Teil des Products ABT . Die direkte Multiplikation Ux, einfach implementiert als np.triu(A @ B.T) @ x, kostet O(pn2) Operationen. Schauen CHAPTER 1. VOR DEM START 36 wir, was diese Operation f¨ur p = 1 eigentlich ist: y = triu(ab T )x =           a1b1 a1b2 . . . . . . a1bn 0 a2b2 a2b3 . . . . . . a2bn ... . . . . . . . . . ... . . . . . . . . . ... ... . . . . . . ... 0 . . . . . . 0 anbn                    x1 ... ... xn          =          a1 . . . . . . an          (           1 1 . . . . . . 1 0 1 1 . . . . . . 1 ... . . . . . . . . . ... . . . . . . . . . ... ... . . . . . . ... 0 . . . . . . 0 1           ︸ ︷︷ ︸ T (          b1 . . . . . . bn                   x1 ... ... xn          )) . Die Klammern zeigen die Reihenfolge der Operationen. Wir sehen, das teuerste Teil ist die Multiplikation mit der spezielen Matrix T: obere Dreiecksmatrix nur mit 1er. Das Ergebniss der Mutliplikation dieser Matrix T mit einem Vektor v ist erstaunlich einfach:           1 1 . . . . . . 1 0 1 1 . . . . . . 1 ... . . . . . . . . . ... . . . . . . . . . ... ... . . . . . . ... 0 . . . . . . 0 1                    v1 ... ... vn          =          sn sn−1 ... ... s1          , sj := n∑ k=n−j+1 vj , d.h, das Ergebnis enth¨allt die Teilsummen von Elementen von (umgekehrten) v. Teilsummen k¨onnen mit v.cumsum() oder np.cumsum(v) eﬃzient berechnet werden. In diesem Fall kann man also die Berechnung so organisieren, dass das Ganze nur O(n) statt O(n2) Operationen braucht. F¨ur p Spalten macht das entsprechend O(n × p) Operationen via ABT = p∑ ℓ=1 A:,ℓ(B:,ℓ)⊤ , was f¨ur p ≪ n deutlich g¨unstiger als O(pn2) der direkten Implementierung ist. CHAPTER 1. VOR DEM START 37 10 1 10 2 10 3 10 4 size n 10 −6 10 −4 10 −2 10 0 10 2time (s) direct way partial sums O( n 2 ) O(n) 10 2 10 3 10 4 size n 10 −6 10 −5 10 −4 10 −3 10 −2 10 −1 10 0 10 1time (s) direct way partial sums O( n 2 ) O(n) Abb. 1.3.20. Rechenzeiten f¨ur die zwei Arten, die Multiplikation mit dem oberen Teil einer Matrix von Rang p = 1 (links) und p = 5 (rechts) zu berechnen. Deﬁnition 1.3.21. Der Kronecker-Produkt A ⊗ B zweier beliebiger Matrizen A ∈ R m,n und B ∈ R l,k. mit m, n, l, k ∈ N ist die (ml) × (nk)-Matrix A ⊗ B :=         (A)1,1B (A)1,2B . . . . . . (A)1,nB (A)2,1B (A)2,2B ... ... ... ... ... ... ... (A)m,1B (A)m,2B . . . . . . (A)m,nB         ∈ R ml,nk . Beispiel 1.3.22 (Multiplikation des Kronecker-Produkts mit einem Vektor). W¨urden wir zuerst die Matrix A ⊗ B bauen und dann mit dem nk-lange Vektor x multi- plizieren, h¨atten wir die Komplexit¨at O(m · n · l · k). Wenn wir den Vektor x in n gleich grosse Bl¨ocke aufteilen x =      x1 x2 ... xn      mit xj ∈ R k , k¨onnen wir das Ergebnis direkt berechnen: (A ⊗ B)x =        A1,1Bx1 + A1,2Bx2 + · · · + A1,nBxn A2,1Bx1 + A2,2Bx2 + · · · + A2,nBxn ... ... Am,1Bx1 + Am,2Bx2 + · · · + Am,nBxn        . Die Idee ist, die kleinere Produkte Bxj f¨ur j = 1, . . . , n auf einmal zu berechenen und dann siese mit den Keoﬃzienten aus den Spalten von A linear zu kombinieren: CHAPTER 1. VOR DEM START 38 10 2 size n 10 −5 10 −4 10 −3 10 −2 10 −1 10 0 10 1 10 2time (s) direct way L A way O( n 4 ) O( n 3 ) Abb. 1.3.23. Rechenzeiten f¨ur die zwei Arten (A ⊗ B)x zu berechnen. Chapter 2 Polynominterpolation 2.1 Interpolation und Polynome In diesem und im n¨achsten Kapitel wird es um die Interpolation von Funktionen gehen. Wir nehmen an, dass wir eine Funktion haben, die z.B. einen physikalischen Vorgang beschreibt, aber deren analytische Form sehr verschachtelt und unhandlich oder gar nicht bekannt ist. Meistens ist es aber n¨utzlich, Extremstellen, Ableitun- gen oder Integrale davon zu bestimmen. Hier bietet es sich an, die komplizierte Funktion durch eine wesentlich einfachere Funktion zu approximieren. Je nach In- terpolationsverfahren erhalten wir m¨oglicherweise sogar andere Einsichten in die Funktion; z.B. k¨onnen wir Informationen ¨uber die Hauptschwingung erhalten. Wir wollen also in diesem Kapitel einige Methoden zur Interpolation erarbeiten. Deﬁnitionen Das Ziel der Interpolation ist es, eine Funktion ˜f zu gegebenen Datenpunkten (bzw. Funktionswerten) zu ﬁnden: [x0 x1 · · · xn y0 y1 · · · yn ] , xi ∈ R, yi ∈ R . Dabei heissen xi St¨utzstellen oder Knoten und yi = f (xi) sind die dazugeh¨origen Datenpunkte (bzw. Funktionswerte). Die Interpolationsbedingung ˜f (xi) = yi f¨ur alle i = 1, 2, . . . , n (2.1.1) sollte exakt (und nicht im Sinne der Ausgleichsrechnung) erf¨ullt werden. Die interessante und vorgegebene Funktion f liegt normalerweise in einem un- endlich-dimensionalen Vektorraum V von Funktionen gewisser Glattheit. Wir suchen also zu f ∈ V eine Approximation ˜f in einem linearen Unterraum Vn (von V) der endlichen Dimension dim Vn = n. Dabei werden wir die Elemente einer Basis des Raumes Vn verwenden, sodass wir schreiben k¨onnen: f (x) ≈ fn(x) = n∑ j=1 αjbj(x) . 39 CHAPTER 2. POLYNOMINTERPOLATION 40 Hier bilden die bj(x) eine Basis in Vn: span {b1, . . . , bn} = Vn. Bemerkung 2.1.1. Hier handelt es sich nicht um eine Ausgleichsrechnung. In der Tat bauen wir eine Interpolationsfunktion, die durch alle Datenpunkte yi = ˜f (ti) laufen soll. Bemerkung 2.1.2. F¨ur einen linearen Raum V kann man Unterr¨aume Vn von ver- schiedenen Arten von Funktionen betrachten, oder sogar f¨ur eine Art von Unterraum Vn verschiedene Basen verwenden. Zun¨achst wollen wir uns aber ausschliesslich mit Polynomen besch¨aftigen. Wir werden im folgenden h¨auﬁg p(x) f¨ur ˜f (x) schreiben, um diesen Sachverhalt zu verdeutlichen. Beispiel 2.1.3. Beispiele von m¨oglichen bj(x) sind: (1) bj(x) = xj−1 polynominale Interpolation (2) bj(x) = cos ((j − 1) arccos (x)) Chebyshev1 Interpolation (3) bj(x) = ei2πjx = cos (2πjx) + i sin (2πjx) trigonometrische Interpolation . Beispiel 2.1.4. (Taylor-Formel) Aus der Analysis ist die Taylorformel (um den Punkt a) bereits bekannt: f (x) = pn(x) + Rn(x) mit Restglied Rn(x) = (x − a)n+1 (n + 1) f (n+1)(ξ) (2.1.2) mit ξ zwischen a und x. Die Taylorformel wird h¨auﬁg in der Mathematik und Physik verwendet um Funktionen lokal zu approximieren. F¨ur numerische Berechnungen ist sie aber nicht gut genug, da sie nur lokal approximiert und auch Ableitungen ben¨otigt werden, die wir eventuell nur schwer oder gar nicht ausrechnen k¨onnen. Existenz Es stellt sich nat¨urlich die Frage, ob unsere Interpolationsfunktionen ¨uberhaupt gute Approximationen sind oder ob es ¨uberhaupt m¨oglich ist, jede Funktion zu approximieren. Dazu kennen wir aber den aus der Analysis bekannten Satz: Theorem 2.1.5. (Satz von Peano) Angenommen f ist stetig, dann gilt: Es existiert ein Polynom p, welches die Funktion f in der ∥·∥∞-Norm beliebig gut approximiert. Bemerkung 2.1.6. Je glatter f ist, desto bessere Konvergenz erhalten wir im Allgemeinen mit den einzelnen Verfahren. 1Richtige Deutsche Schreibweise ist Tschebyschjow oder nach ¨alteren Konventionen Tschebyschow oder Tschebyscheﬀ, jedoch ich verwende die gebr¨auchliche Englische Schreibweise, die in der Literatur verwendet ist, obwohl er selber meist auf Fraz¨osisch unter den Namen Tcheby- chef publiziert hat. CHAPTER 2. POLYNOMINTERPOLATION 41 Polynome und Monombasis Deﬁnition 2.1.7. (Raum das Polynome) Der Raum das Polynome von Grad ≤ k, k ∈ N ist gegeben durch: Pk := {x ↦→ αkxk + αk−1xk−1 + · · · + α1x + α0, αj ∈ K} . (2.1.3) Deﬁnition 2.1.8. (Monom) Die Funktionen der Form x ↦→ xk, k ∈ (N ∪ {0}) heissen Monome. x ↦→ αkxk + αk−1xk−1 + · · · + α0 ist die Monom-Darstellung eines Polynoms. Theorem 2.1.9. (Eigenschaften des Raumes das Polynome) Pk ist ein Vektorraum der Dimension k + 1. Zus¨atzlich gilt Pk ⊂ C ∞ (R). Proof. Die Monome vom Grad ≤ k bilden eine Basis des Vektorraums das Polynome, also gilt dim Pk = k + 1. Ausserdem sind alle Polynome ∞-Mal stetig diﬀerentierbar und liegen somit in C ∞ (R) (man schreibt Pk ⊂ C ∞ (R)). Berechnung mit der Monombasis Theorem 2.1.10. (Eindeutigkeit) Ein Polynom p(x) k-ten Grades ist eindeutig durch k+1 Punkte yi = p(xi) bestimmt. Damit haben wir bereits eine Methode zur Polynomdarstellung kennengelernt. Wir m¨ussen lediglich die Funktion, die wir approximieren wollen, an einer bestimmten Anzahl an St¨utzstellen auswerten. Danach lassen sich die Koeﬃzienten in der Monombasis durch L¨osen eines Gleichungssystems leicht bestimmen. Beispiel 2.1.11. (Berechnung der Koeﬃzienten in der Monombasis) Wir m¨ochten das Polynom durch die Punkte y0, . . . , yn berechnen: Das gesuchte Polynom pn ∈ Pn hat in der Monombasis die Form: pn(x) = αnxn + · · · + α1x1 + α0 . Wir ﬁnden also das lineare Gleichungssystem:      1 x0 · · · xn 0 1 x1 · · · xn 1 ... ... . . . ... 1 xn · · · xn n           α0 α1 ... αn      =      y0 y1 ... yn      . Bemerkung 2.1.12. Dieses Gleichungssystem weist die Vandermonde matrix auf; deswegen ist es sehr schlecht konditioniert (die Vandermonde Matrix sehen wir auch in einem Beispiel bei der QR-Zerlegung, cf. Abbildung ??). W¨urden die Koeﬃzien- ten α0, . . . , αn daraus berechnet, w¨urden diese sehr ungenau ausfallen. Die Funktion polyfit kann f¨ur die bequeme Berechnung von α0, . . . , αn verwendet werden, sie gibt aber auch die Warnung RankWarning: Polyfit may be poorly conditioned. CHAPTER 2. POLYNOMINTERPOLATION 42 Auswertung und Bemerkungen Allgemein lassen sich Polynome, wie im Beispiel oben, ¨uber die Matrixdarstel- lung auswerten: Mit bekannten αi lassen sich aus den Auswertungstellen ti die dazugeh¨origen yi bestimmen. Eine eﬃzientere M¨oglichkeit der Auswertung ist aber das sogenannte Horner-Schema: Deﬁnition 2.1.13. (Horner-Schema) p(x) = (x . . . x (x (αnx + αn−1) + · · · + α1) + α0) . (2.1.4) Bemerkung 2.1.14. Das Polynom pn(x) = αnxn + · · · + α1x1 + α0 wird in python durch ein array p = [αn, αn−1, . . . , α1, α0] der L¨ange n + 1 repr¨asentiert. Das Modul numpy stellt die Funktion polyval(p,x) zur Verf¨ugung, um ein solches Polynom via Horner-Schema schnell auszuwerten. Mit unserer Notation haben wir p[0] = αn, p[1] = αn−1, . . . , p[n] = α0 und eine m¨ogliche Implementierung ist: Code 2.1.15: Horner-Schema 1 def horner(p, x): y = p[0] 3 for i in range(1,len(p)): y = x * y + p[i] 5 return y p ist hier ein array mit den Koeﬃzienten αi des Interpolationspolynoms. Bemerkung 2.1.16. Die Monombasis ist zwar sehr anschaulich, hat aber keine n¨utzlichen numerischen Eigenschaften, sondern eher Nachteile. Daher wird diese auch nur sehr selten verwendet. Das Horner-Schema wirkt aber stabilisierend: ob- wohl polyfit grosse Fehler in die Berechnung der Koeﬃzienten α0, . . . , αn pro- duziert, liefert die Auswertung ¨uber polyval wieder relativ gute Ergebnisse. 2.2 Newton Basis und Dividierte Diﬀerenzen Grundlagen In der Monombasis k¨onnen neu gewonnene Datenpunkte nur durch Neuberechnung des Interpolationspolynoms eingebunden werden. Die Newton-Basis erlaubt das Hinzuf¨ugen von Datenpunkte, ohne die bereits vorhandene Informationen zu verw- erfen. Betrachten wir hierzu zun¨achst den einfachen Fall, wo wir nur eine St¨utzstelle haben und eine weitere hinzuf¨ugen m¨ochten: Beispiel 2.2.1. (Motivation Newtonbasis) Angenommen, wir haben zun¨achst nur eine St¨utzstelle (x0, y0) zur Verf¨ugung. Da- raus ergibt sich trivialerweise als Interpolationspolynom z.B. p0(x) = y0. Nun CHAPTER 2. POLYNOMINTERPOLATION 43 m¨ochten wir eine zweite St¨utzstelle (x1, y1) hinzuf¨ugen, ohne das bisher berech- nete Polynom zu verwerfen. Mithilfe der Interpolationsbedingungen p0(x0) = y0 und p1(x1) = y1 ﬁnden wir folgendes Polynom: p0(x0) = y0 (x1,y1) −→ p1(x) = p0(x) + (x − x0) (y1 − y0) (x1 − x0) , Nun m¨ochten wir noch eine weitere St¨utzstelle (x2, y2) hinzuf¨ugen. Wir wenden also das selbe Schema an: p2(x) = p1(x) + ( y2−y1 x2−x1 ) − ( y1−y0 x1−x0 ) x2 − x0 (x − x0) (x − x1) . Dieses Schema l¨asst sich beliebig fortf¨uhren. Bemerkung 2.2.2. Nach dem Theorem 2.1.10 sind die im obigen Beispiel gefun- denen Polynome eindeutig und entsprechen das Polynomen, die wir mit der ”naiven Herangehensweise” erhalten w¨urden. Newton Basis und Dividierten Diﬀerenzen Theorem 2.2.3. (Newtonbasis) Die Newtonbasis: N0(x) := 1 , N1(x) := x − x0 , N2(x) := (x − x0)(x − x1) , . . . , Nn(x) := n−1∏ i=0(x − xi) (2.2.5) ist eine Basis des Raumes Pn. Wie erhalten wir im Allgemeinen die Koeﬃzienten βj zur Darstellung eines Polynoms in der Newtonbasis? pn(x) = β0N0(x) + β1N1(x) + · · · + βnNn(x) , mit βj ∈ R . Wir k¨onnen diese Koeﬃzienten βj durch das L¨osen eines linearen Gleichungssystems erhalten: β0N0(xj) + β1N1(xj) + · · · + βnNn(xj) = yj, j = 0, . . . , n ⇔        1 0 · · · 0 1 (x1 − x0) · · · 0 ... ... . . . ... 1 (xn − x0) · · · n−1∏ i=0(xn − xi)             β0 β1 ... βn      =      y0 y1 ... yn      . Dabei haben wir verwendet: Nn(x) = n−1∏ i=0(x − xi) = 0 f¨ur x = x0, . . . , xn−1 . CHAPTER 2. POLYNOMINTERPOLATION 44 Bemerkung 2.2.4. Beim L¨osen dieses Gleichungssystems f¨allt auf, dass immer wieder die selben Ausdr¨ucke berechnet werden. Die Dividierten Diﬀerenzen nutzen diesen Sachverhalt und stellen damit eine eﬃzientere Methode zur Berechnung der Koeﬃzienten dar. Deﬁnition 2.2.5. (Dividierte Diﬀerenzen) Die dividierten Diﬀerenzen sind deﬁniert durch: y[xi] = yi und y[xi, . . . , xi+k] = y[xi+1, . . . , xi+k] − y[xi, . . . , xi+k−1] xi+k − xi . (2.2.6) Zur Berechnung verwendet man folgendes Schema: x0 y[x0] > y[x0, x1] x1 y[x1] > y[x0, x1, x2] > y[x1, x2] > y[x0, x1, x2, x3] x2 y[x2] > y[x1, x2, x3] > y[x2, x3] x3 y[x3] , (2.2.7) wobei jedes ”‘>”’ f¨ur eine in (2.2.6) deﬁnierte Rekusion steht. Bemerkung 2.2.6. ( ¨Aquidistante St¨utzstellen) Falls die xj ¨aquidistant mit Abstand h sind, d.h. falls xj = x0 + j · h, so k¨onnen wir schreiben: y[x0, x1] = y1−y0 h := 1 h ∆y0 y[x0, x1, x2] = 1 h ∆y1− 1 h ∆y1 2h := 1 2! h2 ∆2y0 ... ... y[x0, . . . , xn] = 1 n! hn ∆ny0 . (2.2.8) Beachte: Das n in ∆n ist als Index zu verstehen, nicht als Potenz! Code 2.2.7: Dividierte Diﬀerenzen in einer Matrix 1 def divdiff(x, y): n = y.size 3 T = np.zeros((n, n)) T[:,0] = y 5 for level in range(1, n): for i in range(n-level): 7 T[i, level] = (T[i+1,level-1] - T[i,level-1]) / (x[i+level]-x[i]) 9 return T[0,:] CHAPTER 2. POLYNOMINTERPOLATION 45 Theorem 2.2.8. (Newton, 1676) Es gibt ein Polynom n-ten Grades, das durch (x0, y0), . . . , (xn, yn) geht. Dieses l¨asst sich wie folgt berechnen: p(x) = y[x0] + y[x0, x1](x − x0) + · · · + y[x0, x1, . . . , xn](x − x0) · · · (x − xn−1). Die Koeﬃzienten βj in der Newtonbasis sind also gegeben durch βj = y [x0, x1, . . . , xj]. Auswertung, Kosten und Fehler F¨ur die Auswertung des Interpolationspolynoms bietet sich wieder ein abgewandeltes Horner-Schema an: Wir verwenden p im folgenden Algorithmus als ”‘Speichervariable”’: p := βn p := (x − tn−1) p + βn−1 p := (x − tn−2) p + βn−2 . Bemerkung 2.2.9. Der Aufwand (gegliedert nach den Teilschritten) ist gegeben durch: (1) O(n2) f¨ur die Berechnung der dividierten Diﬀerenzen; (2) O(n) jeweils f¨ur jede Auswertung. Bemerkung 2.2.10. Das obere naive Algorithmus zur Berechnung der dividierten Diﬀerenzen mit n Intrpolationsknoten verwendet ein Teil der Speicher einer n × n- Matrix T . F¨ur das Newton-Polynom brauchen wir aber nicht alle diese Werte, sondern nur die Elemente von der Diagonale. Wenn wir die Deﬁnition der dividierten Diﬀerenzen anschauen, sehen wir dass jedes Element auf der Diagonale von T auch in einer anderen Reihenfolge sich berchenen l¨asst. Das zeigt uns einerseits, wie man das neue Newton-Polynom aus dem alten Newton-Polynom entsteht, wenn wir einem Interpolationsknoten dazu nehmen und andereseits ist das ein Hinweis, wie man das Polynom nur mit dem Speicher eines Vektors der L¨ange n aufbaut. Wir nehmen an, wir haben bereits aus n Interpolationsbedingungen (x0, y0), (x1, y1), . . . (xn1, yn−1) die Koeﬃzienten des Newton Polynom vom Grad n − 1 erhalten, die wir ¨uber den Speicher der Messungen geschrieben haben y[k] := βk = y[x0, x1 . . . , xk] f¨ur k = 0, 1, , . . . , n − 1 , so dass die Messungen y1, . . . , yn−1 nicht mehr zur Verf¨ugung stehen. Wenn wir die neue Information (xn, yn) erhalten, k¨onnen wir dir dividierten diﬀerenzen bilden y[x0, xn], y[x0, x1, xn], y[x0, x1, x2, xn], . . . , y[x0, x1, . . . , xn−1, xn], als ob wir (xn, yn) nicht unten sondern oben in der Tabelle (2.2.7) hinzuf¨ugen w¨urden. Dabei verwenden wir nur den Speicherort der neuer Messung yn. CHAPTER 2. POLYNOMINTERPOLATION 46 Aufgabe: implementieren Sie ein sequenziellen Algorithmus, der die Koeﬃzien- ten des Newton Interpolationspolynom nur mit der Verwendung von O(n) Spe- icherpl¨atze und O(n2) Operationen auskommt. Aufgabe: schrieben Sie Implementierung in einer vektorisierter Form um. Aufgabe: implementieren Sie ein rekursiver Algorithmus mit denselben Eigen- schaften. Entdecken Sie experimentell die Schranke, die Python auf Rekursivit¨at stellt. Ist ein rekursiver Algorithmus hier ¨uberhaupt sinnvoll? Warum? , Bisher haben wir uns noch nicht mit dem Fehler ∥f (x) − p(x)∥ des Inter- polationspolynoms besch¨aftigt. Insbesondere zur Entwicklung besserer Interpola- tionsverfahren m¨ochten wir den Fehler nun genauer absch¨atzen. Theorem 2.2.11. Angenommen, f ist n-mal stetig diﬀerenzierbar und yi = f (xi) f¨ur i = 0, . . . , n, dann existiert ein ξ ∈ ] min i xi, max i xi [ , so dass y [x0, x1, . . . , xn] = f (n)(ξ) n! . (2.2.9) Theorem 2.2.12. (Fehler) Sei f : [a, b] → R (n+1)-mal stetig diﬀerenzierbar und p das Interpolationspolynom zu f in x0,x1, . . . , xn ∈ [a, b]. Dann gilt: f¨ur jedes x ∈ [a, b] gibt es ein ξ ∈ ] a, b [ mit f (x) − p(x) = (x − x0) · · · (x − xn) f (n+1)(ξ) (n + 1)! . (2.2.10) Beispiel 2.2.13. Als Beispiel einer Anwendung dieses Satzes wollen wir uns die Funktion f (x) = log10 x = ln x ln 10 auf dem Intervall [55, 58] anschauen. Als St¨utzstellen verwenden wir {55, 56, 57, 58}. Als Fehler an der Stelle x = 56.5 ergibt sich: |log10(56.5) − p(56.5)| ≤ 1.5 · 0.5 · 0.5 · 1.5 · 6 (55)4 ln(10) · 1 4! ≈ 6.7 · 10−9 . Der geringe Fehler, den wir erhalten haben, l¨asst sich vor allem auf das kleine Intervall und die geringe Oszillation der Funktion zur¨uckf¨uhren. Aus der Formel zur Absch¨atzung des Fehlers geht hervor, dass die Wahl der St¨utzstellen massgeblichen Einﬂuss auf den Fehler hat. Also stellt sich nat¨urlich die Frage, welche Wahl x1, . . . , xn der St¨utzstellen ist am besten? D.h. wann ist max x∈[a,b] |(x − x0) · · · (x − xn)| minimal? Eine andere Frage ist, wie wirken sich Fehler in den Funktionswerten (z.B. Mess- fehler) auf das Interpolationspolynom aus? (xi, yi) f¨ur i = 0, 1, . . . , n → p(x) (xi, ˜yi) f¨ur i = 0, 1, . . . , n → ˜p(x) } p(x) − ˜p(x) =? Um diese Fragen zu beantworten, wechseln wir in eine andere Basis des Raumes das Polynome P, die sogenannten Lagrange-Polynome. Die Lagrange-Basis hat einige n¨utzliche Eigenschaften, die uns helfen werden, diese Fragen zu beantworten. CHAPTER 2. POLYNOMINTERPOLATION 47 2.3 Lagrange- und baryzentrische Interpolations- formeln Im letzten Abschnitt haben wir uns mit dem Fehler im Interpolationspolynom besch¨aftigt. Wir m¨ochten hier eine Basis einf¨uhren, die uns die Untersuchung des Fehlers deutlich vereinfacht. Diese Basis ist zur praktischen Berechnung von Inter- polationspolynomen ungeeignet. Stattdessen wird die baryzentrische Formel ver- wendet. Deﬁnition 2.3.1. (Lagrange Polynome) F¨ur die Knoten x0, x1, ..., xn ∈ R deﬁnieren wir die (Familie von) Polynome(n) ℓi(x) = n∏ j=0 j̸=i x − xj xi − xj . (2.3.11) Die ℓi werden auch die Lagrange Polynome zu den St¨utzstellen x0, x1, ..., xn genannt. Beispiel 2.3.2. Gegeben seien die St¨utzstellen x0, x1 und x2. Wir erhalten die Lagrange Polynome: ℓ0(x) = x − x1 x0 − x1 · x − x2 x0 − x2 ℓ1(x) = x − x0 x1 − x0 · x − x2 x1 − x2 ℓ2(x) = x − x0 x2 − x0 · x − x1 x2 − x1 . Die Abbildung 2.3.3 zeigt die 5 Lagrange Polynome zu den 5 Knoten 0, 1 4 , 1 2 , 3 4, 1. 0.0 0.2 0.4 0.6 0.8 1.0 −0.5 0.0 0.5 1.0 Abb. 2.3.3. Die 5 Lagrange Polynome zu Knoten 0, 1 4, 1 2, 3 4, 1. CHAPTER 2. POLYNOMINTERPOLATION 48 Theorem 2.3.4. (Lagrange Interpolationsformel) Die Lagrange-Polynome ℓi(x) zu den St¨utztstellen (x0, y0), . . . , (xn, yn) bilden eine Basis des Raumes das Polynome Pn+1, und wir k¨onnen schreiben: p(x) = n∑ i=0 yiℓi(x) mit ℓi(x) = ∏ j̸=i x − xj xi − xj . (2.3.12) Bemerkung 2.3.5. (Eigenschaften der Lagrange Polynome) 1. ℓi(xj) = 0 f¨ur alle j ̸= i, 2. ℓi(xi) = 1 f¨ur alle i, 3. grad ℓi = n f¨ur alle i. 4. n∑ k=0 ℓk(x) = 1 und n∑ k=0 ℓ(m) k (x) = 0 f¨ur m > 0. Wenn wir L(x) = (x − x0)(x − x1) · · · · · (x − xn) und λk = ∏ j̸=k 1 xk − xj = 1 (xk − x0) · · · · · (xk − xk−1)(xk − xk+1) · · · · · (xk − xn) dann l¨asst sich p umschreiben in p(x) = L(x) n∑ k=0 λk x − xk yk . F¨ur das konstante Polynom 1 ergibt das 1 = L(x) n∑ k=0 λk x − xk was zur baryzentrischen Formel f¨ur das Interpolationspolynom f¨uhrt: p(x) = n∑ k=0 λk x−xk yk n∑ k=0 λk x−xk . (2.3.13) Diese Formel ist skalierungsinvariant bez¨uglich des Interpolationsintervalls [a, b] mit a = x0 < . . . , xn = b; die Auswertung der baryzentrischen Formel ist extrem stabil f¨ur x ∈ [a, b] und l¨asst sich mit O(n) Operationen durchf¨uhren, wenn man die λk im Voraus ausrechnet (in O(n2) Operationen). Auch einige zus¨atzliche St¨utzstellen lassen sich leicht hinzuf¨ugen; die daruf basierte Neuauswertung des enstandenen Interpolationspolynoms kostet nur O(n) Operationen. F¨ur eine spezielle Wahl der St¨utzstellen (n + 1 Chebyshev-Abszissen xk = cos(kπ/n)) sind λk = (−1)k. Wenn wir andere λk zulassen, dann ergibt die baryzentrische Formel kein Polynom mehr, CHAPTER 2. POLYNOMINTERPOLATION 49 so dass sich der Weg zu einer Verallgemeinerung zur Iterpolation mittels rationaler Funktionen ¨oﬀnet. Die baryzentrische Formel kann auch als Startpunkt zur spektralen Methode f¨ur die Diﬀerentialgleichungen genommen werden. Wenn wir die unbekannte Funk- tion u durch dessen Interpolationspolynom ersetzten und die Ableitungen ausrech- nen, dann brauchen wir ℓ′ j(xi) und ℓ′′ j (xi) um die Diﬀerentialgleichung der Form F (u′′, u′, x) = 0 zu einem linearen Gleichungssystem in den Unbekannten yi = u(xi) umzuschreiben. Eine geschickte Rechnung ergibt f¨ur i ̸= j: ℓ′ j(xi) = λj/λi xi − xj (2.3.14) ℓ′′ j (xi) = −2 λj/λi xi − xj (∑ k̸=j λk/λi xk − xj − 1 xi − xj . ) (2.3.15) w¨ahrend f¨ur i = j gilt ℓ′ j(xj) = − ∑ k̸=j λk/λj xk − xj ℓ′′ j (xj) = − ∑ k̸=j ℓ′′ k(xj) . Fehlerbetrachtung Wenden wir uns zun¨achst dem Fall zu, dass, z.B. durch ungenaue Messungen, die Werte ˜yi an den St¨utzstellen xi mit Fehlern behaftet sind. Wir m¨ochten nun den Fehler zwischen dem korrekten Interpolationspolynom p(x) und dem mit Fehlern behafteten Polynom ˜p(x) berechnen. Dazu schreiben wir p und ˜p in der Lagrange- Basis: p(x) = n∑ i=0 yiℓi(x) und ˜p = n∑ i=0 ˜yiℓi(x) . Dann ergibt sich f¨ur den Fehler: |p(x) − ˜p(x)| = ∣ ∣ ∣ ∣ ∣ n∑ i=0 (yi − ˜yi)ℓi(x) ∣ ∣ ∣ ∣ ∣ ≤ max i=0,...,n |yi − ˜yi| · n∑ i=0 |ℓi(x)| . Deﬁnition 2.3.6. (Lebesgue-Konstante) Die Lebesgue-Konstante Λ zu den St¨utzstellen x0, . . . , xn im Intervall [a, b] ist deﬁniert durch Λn := max x∈[a,b] n∑ i=0 |ℓi(x)| . (2.3.16) Bemerkung 2.3.7. Die Lebesgue-Konstante h¨angt oﬀensichtlich nur von der Wahl der St¨utzstellen ab. CHAPTER 2. POLYNOMINTERPOLATION 50 Theorem 2.3.8. (Auswirkung von Messfehlern) Es gilt: max x∈[a,b] |p(x) − ˜p(x)| ≤ Λn max i=0,...,n |yi − ˜yi| , (2.3.17) wobei die Lebesgue-Konstante Λn die bestm¨ogliche Konstante in dieser Ungleichung ist. Theorem 2.3.9. (Fehler) Sei f : [a, b] → R und p das Interpolationspolynom zu f , Seien x0, . . . , xn die S¨utzstellen, dann gilt: ∥f (x) − p(x)∥∞ = max x∈[a,b] |f (x) − p(x)| ≤ (1+Λn) max x∈[a,b] |f (x) − q(x)| f¨ur alle q ∈ Pn . Proof. Sei q ∈ Pn, dann gilt: |f (x) − p(x)| = |[f (x) − q(x)] + [q(x) − p(x)]| ≤ |f (x) − q(x)| + |q(x) − p(x)| . Seien yi = f (xi) und ˜yi = q(xi). Wir wissen, dass p eine Approximation von f ist und dass q sein eigenes Interpolationspolynom zu sich selbst ist. Dann: max x∈[a,b] |p(x) − q(x)| ≤ Λn max i=0,1,...,n ∥f (xi) − q(xi)∥ ≤ Λn max x∈[a,b] |f (x) − q(x)| . Die letzte zwei Ungleichungen ergeben dann: max x∈[a,b] |f (x) − p(x)| ≤ max x∈[a,b] |f (x) − q(x)| + Λn max x∈[a,b] |f (x) − q(x)| . Beispiel 2.3.10. (Lebesgue-Konstante) Auf dem Intervall [−1, 1] ist die Lebesgue-Konstante f¨ur eine ¨aquidistante Verteilung der St¨utzstellen xj = −1 + 2 n j: Λ10 ≈ 40 , Λ20 ≈ 30000 , Λ40 ≈ 1010 . Wir sehen also, dass schon f¨ur etwa 40 St¨utzstellen der Fehler sehr gross werden kann! Bemerkung 2.3.11. F¨ur ¨aquidistant verteilte St¨utzstellen gilt: Λn ≈ 2n+1 n log n . Beispiel 2.3.12. (Runge-Funktion) Als Beispiel betrachten wir die sogenannte Runge-Funktion auf dem Intervall [−5, 5]: f (x) = 1 1 + x2 . Zur Berechnung des Interpolationspolynoms w¨ahlen wir ¨aquidistant verteilte St¨utzstellen. Das Interpolationspolynom ist in der Abbildung 2.3.13 dargestellt. Wir sehen, dass in den Randbereichen starke Oszillationen auftreten. Die Abbildung 2.3.14 zeigt, dass wir durch h¨oheren Grad des Interpolationspolynoms sogar noch h¨ohere Fehler erhalten. CHAPTER 2. POLYNOMINTERPOLATION 51 −5 −4 −3 −2 −1 0 1 2 3 4 5 −0.5 0 0.5 1 1.5 2 1/(1+x 2) Interpolating polynomial Abb. 2.3.13. Interpolationspolynom 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0 Degree d 100 101Interpolation error (maximum norm) Abb. 2.3.14. ∥f − p∥∞ auf [−5, 5] Die Wahl der St¨utzstellen hat erheblichen Einﬂuss auf den Fehler. Aus dem obi- gen Beispiel l¨asst sich zudem eine einfache Grundregel f¨ur die Interpolation ableiten: Verwende f¨ur eine Interpolation hohen Grades niemals ¨aquidistant verteilte St¨utzstellen! Wie k¨onnen wir genauere Interpolationen durch h¨oheren Grad des Interpolation- spolynoms erhalten? M¨ogliche Ansatzpunkte k¨onnten sein: 1. das Intervall in kleinere Intervalle zerteilen und f¨ur jeden Teil ein eigenes In- terpolationspolynom berechnen. 2. die ideale Verteilung der St¨utzstellen w¨ahlen, die diesen Nachteil nicht haben. Die erste Idee bringt uns zu Splines und Finite Elemente.Im n¨achsten Kapitel wer- den wir uns mit der zweiten Idee n¨aher besch¨aftigen: Chebyschev-Knoten und - Abszyssen, die mit Chebyshev Polynome kommen. 2.4 Chebyshev-Interpolation Hier m¨ochten wir durch geschickte Wahl der St¨utzstellen den Interpolationsfehler verkleinern. Bisher haben wir immer ¨aquidistant verteilte St¨utzstellen verwendet, in diesem Kapitel werden wir die Chebyshev-Knoten als die ideale Verteilung ken- nenlernen. Deﬁnition 2.4.1. (Chebyshev-Polynome erster Art) Die Chebyshev-Polynome (erster Art) sind: Tn(x) = cos (n arccos(x)) , x ∈ [−1, 1] . (2.4.18) Deﬁnition 2.4.2. (Chebyshev-Polynome zweiter Art) Die Chebyshev-Polynome zweiter Art sind: Un(x) = sin ((n + 1) arccos(x)) sin (arccos(x)) , x ∈ [−1, 1] . (2.4.19) CHAPTER 2. POLYNOMINTERPOLATION 52 Auf dem ersten Blick ist es nicht sofort einzusehen, dass die Tn(x) ¨uberhaupt Poly- nome sind! Im folgenden Satz wird die Aussage aber klar. Theorem 2.4.3. (Eigenschaften) Das n-te Chebyshev-Polynom ist ein Polynom von Grad n und es gilt f¨ur x ∈ [−1, 1]: 1. T0(x) = 1 , T1(x) = x , Tn+1(x) = 2xTn(x) − Tn−1(x) ; 2. |Tn(x)| ≤ 1 ; 3. Tn(cos( kπ n )) = (−1)n f¨ur k = 0, 1, . . . , n ; 4. Tn(cos( (2k+1)π 2n )) = 0 f¨ur k = 0, 1, . . . , n − 1 . Proof. Es gilt: cos((n + 1)t) = 2 cos(nt) cos(t) − cos((n − 1)t) . F¨ur cos(t) = x folgt die Rekursion. Daraus folgt auch direkt, dass Tn(x) ein Polynom ist, da Tn(x) nach der Rekursion nur aus Polynomen besteht. Die anderen Punkte ergeben sich durch einfaches Nachrechnen. −1 −0.8 −0.6 −0.4 −0.2 0 0.2 0.4 0.6 0.8 1 −1 −0.8 −0.6 −0.4 −0.2 0 0.2 0.4 0.6 0.8 1 tTn(t) n=0 n=1 n=2 n=3 n=4 Abb. 2.4.4. T0, . . . , T4 −1 −0.8 −0.6 −0.4 −0.2 0 0.2 0.4 0.6 0.8 1 −1 −0.8 −0.6 −0.4 −0.2 0 0.2 0.4 0.6 0.8 1 tTn(t) n=5 n=6 n=7 n=8 n=9 Abb. 2.4.5. T5, . . . , T9 Deﬁnition 2.4.6. (Chebyshev-Knoten) Die n + 1 Chebyshev-Knoten x0, . . . , xn im Intervall [−1, 1] sind die Nullstellen von Tn+1(x). Bemerkung 2.4.7. F¨ur ein beliebiges Intervall [a, b] sind die Chebyshev-Knoten gegeben durch: xk = a + 1 2 (b − a) (cos ( 2k + 1 2 (n + 1) π) + 1) k = 0, . . . , n . (2.4.20) CHAPTER 2. POLYNOMINTERPOLATION 53 Deﬁnition 2.4.8. (Chebyshev-Abszissen) Die n − 1 Chebyshev-Abszissen x0, . . . , xn−2 im Intervall [−1, 1] sind die Extrema des Chebyshev-Polynoms Tn(x). Gleichzeitig sind sie jedoch auch die Nullstellen von Un−1(x). Man nimmt je nach Kontext noch die Punkte −1 und 1 hinzu und erh¨alt dann n + 1 Chebyshev-Abszissen. Bemerkung 2.4.9. F¨ur ein beliebiges Intervall [a, b] sind die Chebyshev-Abszissen gegeben durch: xk = a + 1 2 (b − a) (cos ( k n π) + 1) k = 0, . . . , n . (2.4.21) Schliesst man die Endpunkte a und b aus, so ist k = 1, . . . , n − 1. −1.0 −0.5 0.0 0.5 1.0 x 5 10 15 20Polynomial degree n Chebychev nodes in [−1,1] Abb. 2.4.10. Chebyshev-Knoten: links f¨ur n = 11, rechts f¨ur verschiedene n Bemerkung 2.4.11. In der Theorem 2.4.10 sehen wir, dass die Chebyshev-Knoten zu den R¨andern des Intervalls dichter werden. Zus¨atzlich zu diesen Eigenschaften haben die Chebyshev-Polynome eine weitere wichtige Eigenschaft im Raum das Polynome: Theorem 2.4.12. (Orthogonalit¨at I) Die Chebyshev-Polynome sind orthogonal bez¨uglich des Skalarprodukts ⟨f, g⟩ = 1∫ −1 f (x)g(x) 1 √ 1 − x2 dx . (2.4.22) CHAPTER 2. POLYNOMINTERPOLATION 54 Proof. ⟨Tk, Tj⟩ = 1∫ −1 1√1−x2 Tk(x)Tj(x)dx Substitution x = cos(φ) ⇒ dx = − sin(φ)dφ = −√1 − x2dφ = − 0∫ π cos(kφ) cos(jφ)dφ = π∫ 0 1 2 [cos ((j − k) φ) + cos ((j + k) φ)] dφ =    0 f¨ur j ̸= k π 2 f¨ur j = k ̸= 0 π f¨ur j = k = 0 . Theorem 2.4.13. (Orthogonalit¨at II) Die Chebyshev-Polynome T0, T1, . . . , Tn sind orthogonal bez¨uglich des diskreten Skalarpro- dukts im Raum das Polynome von Grad ≤ n: (f, g) = n∑ l=0 f (xl)g(xl), (2.4.23) wobei (x0, x1, . . . , xn) die Nullstellen von Tn+1 sind. Proof. cos (k 2l+1 n+1 π 2 ) = cos ( k (l + 1 2 ) h) mit h = π n+1 (Tk, Tj) = n∑ l=0 cos ( k (l + 1 2 ) h) cos ( j ( l + 1 2) h) = 1 2 n∑ l=0 [cos ( (k − j) (l + 1 2 ) h) + cos ((k + j) (l + 1 2)) h] = 1 2Re[ n∑ l=0 ei(k−j)(l+ 1 2 )h + ei(k+j)(l+ 1 2 )h] =    0 f¨ur k ̸= j 1 2(n + 1) f¨ur k = j ̸= 0 n + 1 f¨ur k = j = 0 . 2.4.1 Fehlerbetrachtung Wir wollen nun untersuchen, welche Vorteile wir durch die Verteilung der St¨utzstellen erhalten haben. Dazu betrachten wir nochmals die Fehlerabsch¨atzung, die wir im letzten Kapitel kennengelernt haben. Der Fehler war gegeben durch: f (x) − p(x) = (x − x0) · · · (x − xn) f (n+1)(ξ) (n + 1)! . CHAPTER 2. POLYNOMINTERPOLATION 55 Theorem 2.4.14. (Fehlerabsch¨atzung) Unter allen (x0, . . . , xn) mit xi ∈ R wird max x∈[−1,1] |(x − x0) · · · (x − xn)| minimal f¨ur xk = cos ( 2k + 1 n + 1 π 2 ) , xk sind Nullstellen von Tn+1. Bemerkung 2.4.15. Die Nullstellen der Chebyshev-Polynome Tn (d.h. Chebyshev- Knoten) sind also die bestm¨ogliche Wahl f¨ur die St¨utzstellen. Die Extrema des Chebyshev-Polynoms T2n (d.h. Chebyshev-Abszissen) beinhalten die Nullstellen von Tn. Zwischen jede zwei nebeneinander liegenden Chebyshev-Abszissen gibt es eine Nullstelle von T2n. Aufgrund dieser Eigenschaften und der einfacheren Verbindung in der FFT werden eher die Chebyshev-Abszissen statt der Chebyshev-Knoten f¨ur praktische Berechnungen verwendet. Theorem 2.4.16. (Lebesgue-Konstante) Die Lebesgue-Konstante Λn f¨ur die Chebyshev-Interpolation ist gegeben durch Λn ≈ 2 π log n f¨ur n → ∞ . Beispiel 2.4.17. (Runge-Funktion) Zur Veranschaulichung wollen wir die Runge-Funktion im Intervall [−5, 5] nochmals betrachten. Zur Erinnerung, die Runge-Funktion war gegeben durch: f (x) = 1 x2 + 1 . Diesmal w¨ahlen wir die Chebyshev-Knoten zur Interpolation. Zum Vergleich machen wir dieselbe Interpolation mit einer ¨aquidistanten Verteilung der St¨utzstellen. Die beiden Interpolationspolynome sind in Theorem 2.4.18, bzw. Theorem 2.4.19 dargestellt. −5 −4 −3 −2 −1 0 1 2 3 4 5 −0.5 0 0.5 1 1.5 2 1/(1+x 2) Interpolating polynomial Abb. 2.4.18. ¨Aquidistante St¨utzstellen −5 −4 −3 −2 −1 0 1 2 3 4 5 −0.2 0 0.2 0.4 0.6 0.8 1 1.2 t Function f Chebychev interpolation polynomial Abb. 2.4.19. Chebyshev-Knoten CHAPTER 2. POLYNOMINTERPOLATION 56 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0 Degree d 100 101Interpolation error (maximum norm) Abb. 2.4.20. ¨Aquidistante St¨utzstellen 2 4 6 8 10 12 14 16 18 20 10 −2 10 −1 10 0 10 1 Polynomial degree nError norm ||f−p n || ∞ ||f−p n || 2 Abb. 2.4.21. Chebyshev-Knoten Der Interpolationsfehler der Chebyshev-Interpolation ist eindeutig geringer. Aber wie verh¨alt sich der Fehler als Funktion des Grades der Interpolationsfunktion im Falle der Chebyshev-Interpolation? Aus Theorem 2.4.21 wird deutlich, dass wir einen geringeren Fehler bei der Chebyshev- Interpolation erhalten, wenn wir den Grad des Interpolationspolynoms erh¨ohen. Die ¨aquidistante Verteilung der St¨utzstellen hingegen verursacht immer st¨arker werden- den Oszillationen an den R¨andern des Intervalls und einen gr¨osser werdenden Fehler (Theorem 2.4.20). 2.4.2 Interpolation und Auswertung Neben den schon kennengelernten n¨utzlichen Eigenschaften wollen wir uns nun mit der praktischen Anwendung der Chebyshev-Interpolation besch¨aftigen. Theorem 2.4.22. (Interpolationspolynom) Das Interpolationspolynom p zu f mit den Chebyshev-Knoten x0, x1, . . . , xn, also den Nullstellen von Tn+1, ist gegeben durch: p(x) = 1 2 c0 + c1T1(x) + · · · + cnTn(x) , (2.4.24) wobei f¨ur die ci gilt: ck = 2 n + 1 n∑ l=0 f     cos ( 2l + 1 n + 1 π 2 ) ︸ ︷︷ ︸ =xi(Knoten)      cos (k 2l + 1 n + 1 π 2 ) . (2.4.25) Bemerkung 2.4.23. F¨ur grosse n (n ≥ 15) berechnet man die ck mithilfe der Fast Fourier Transformation (FFT). Diese wird im n¨achsten Kapitel behandelt. Damit ergibt sich f¨ur den Aufwand zur Berechnung des Interpolationspolynoms: CHAPTER 2. POLYNOMINTERPOLATION 57 Direkte Berechnung der ck (n + 1)2 Operationen Dividierte Diﬀerenzen n(n+1) 2 Operationen (zum Vergleich) ck mittel FFT n log n Operationen. Auch die Auswertung des Interpolationspolynomes an beliebigen Punkten gestaltet sich durch den sogenannten Clenshaw-Algorithmus sehr einfach: Theorem 2.4.24. (Clenshaw Algorithmus) Das Interpolationspolynom sei, wie oben, gegeben durch p(x) = 1 2 c0 + c1T1(x) + · · · + cnTn(x) . Seien dn+2 = dn+1 = 0. Setze nun: dk = ck + (2x)dk+1 − dk+2 f¨ur k = n, n − 1, . . . , 0 . (2.4.26) Dann gilt: p(x) = 1 2(d0 − d2) (p(x) ist also ¨uber eine R¨uckw¨artsrekursion gegeben. Proof. Wie wissen bereits, dass gilt Tk+1(x) = 2xTk(x)−Tk−1, also folgt mit mehrma- ligem Einsetzen: p(x) = 1 2c0 + c1T1(x) + · · · + cn︸︷︷︸ = dn Tn(x) = 1 2c0 + c1T1(x) + · · · + (cn−2 − dn)Tn−2(x) + (cn−1 + 2xdn) ︸ ︷︷ ︸ = dn−1 Tn−1(x) = 1 2c0 + c1T1(x) + · · · + (cn−3 − dn−1)Tn−3(x) + (cn−2 + 2xdn−1 − dn) ︸ ︷︷ ︸ = dn−2 Tn−2(x) = · · · = 1 2 c0 + xd1 − d2 = 1 2(c0 + 2xd1 − d2︸ ︷︷ ︸ = d0 −d2) = 1 2(d0 − d2) . Code 2.4.25: Clenshaw Algorithmus 1 def clenshaw(a, x): # Grad n des Polynoms 3 n = a.shape[0] - 1 if not type(x) == ndarray: x = array([x]) 5 d = tile( reshape(a,n+1,1), (1, x.shape[0]) ) for j in range(n, 1, -1): 7 d[j-1,:] = d[j-1,:] + 2.0*x*d[j,:] d[j-2,:] = d[j-2,:] - d[j,:] 9 y = d[0,:] + x*d[1,:] return y CHAPTER 2. POLYNOMINTERPOLATION 58 Bemerkung 2.4.26. Der Clenshaw-Algorithmus ist sehr stabil. Das dies nicht immer der Fall sein muss zeigt die folgende instabile Rekursion: xn+1 = 10xn − 9 . Wenn x0 = 1, dann ist xn = 1 f¨ur alle n. Bei einem Mess- oder Rundungsfehler am Anfang der Rekursion x0 = 1 + ε erhalten wir nach n Schritte xn = 1 + 10nε . Mit andern Worten, der relative Fehler wird in jedem Schritt mit 10 multipliziert, sodass er nach n Schritten 10n gr¨osser ist. 2.5 DFT und Chebyshev-Interpolation Wir sahen, man kann glatte Funktion numerisch gut durch Chebyshev–Polynome interpolieren. Mit der DFT bekommt man einen sehr eﬃzienten Weg diese Interpo- lation durchzuf¨uhren. Wir haben nun ein Chebyshev Interpolationspolynom p ∈ Pn auf [−1, 1] zu einer Funktion f : [−1, 1] → C. Nach der dortigen Konstruktion gilt dann f¨ur die Chebyshev - Knoten tk die Gleichheit f (tk) = p(tk), wobei die die Chebyshev - Knoten deﬁniert sind als: tk := cos ( 2k + 1 2(n + 1) π) , k = 0, . . . , n. Wir deﬁnieren uns die beiden Hilfsfunktion g und q: g : [−1, 1] → C : g(s) := f (cos 2πs) q : [−1, 1] → C : q(s) := p(cos 2πs) . Dann ¨ubersetzt unsere Interpolationsbedingung in den Chebyshev - Knoten wie folgt: p(tk) = f (tk) ⇔ q ( 2k + 1 4(n + 1) ) = g ( 2k + 1 4(n + 1) ) 10 x = 1/2 CHAPTER 2. POLYNOMINTERPOLATION 59 Wir wenden nun die Translation s∗ = s + 1 4(n+1) an und deﬁnieren wieder neue Hilfsfunktionen: g∗(s) := g(s∗) = g (s + 1 4(n + 1) ) q∗(s) := g(s∗) = q (s + 1 4(n + 1) ) Wir zeigen nun, dass q(s) ein trigonometrische Polynom ist, genauer zeigen wir, dass q(s) ∈ T2n+1 ist: q(s) = p(cos 2πs) = n∑ j=0 ajTj(cos(2πs)) = n∑ j=0 aj cos(2πjs) = n∑ j=0 1 2 aj (e2πijs + e−2πijs) = n+1∑ j=−n bje−2πijs , wobei bj :=    0 , for j = n + 1 , 1 2aj , for j = 1, . . . , n , a0 , for j = 0 , 1 2an−j , for j = −n, . . . , −1 , Die Symmetrie q(s) = q(1 − s) und q ( 2k+1 4(n+1) ) = f (tk) f¨ur k = 0, . . . , n ergiben zusammen die Interpolationsbedingungen q ( k 2(n + 1) + 1 4(n + 1) ) = zk := {yk , for k = 0, . . . , n , y2n+1−k , for k = n + 1, . . . , 2n + 1 , (2.5.27) und somit ist q∗ das trigonometrische Interpolationspolynom von g∗. Man kann also die Chebyshev-Interpolation durch eine DFT durchf¨uhren. Weiterhin ¨ubertragen sich die Fehlerabsch¨atzungen von der trigonometrischen Interpolation direkt auf die Tschebyscheﬀ-Interpolation hier. F¨ur die konkrete Implementierung ist es n¨utzlich, die Rechnungen anders zu gestallten. Die Nullstellen von Tn+1, d.h. die Chebyshev-Knoten tk, lassen sich auch schreiben als tk = cos(2πξk) mit ξk = 2k + 1 4(n + 1) . CHAPTER 2. POLYNOMINTERPOLATION 60 Wir sahen bereits q(s) = p(cos 2πs) = n+1∑ j=−n bj exp(−2πijs) . Nun ist yk = p(tk) = p(cos 2πtk) = q(tk) = q ( 2k + 1 4(n + 1) ) = n+1∑ j=−n bj exp (−2πij 2k + 1 4(n + 1) ) . Jetzt unterscheidet sich die Rechnung von der vorigen, denn wir isolieren exp ( −2πij 4(n + 1) ): yk = n+1∑ j=−n bj exp ( −2πij 4(n + 1) ) exp ( −2πijk 2n + 2 ) f¨ur k = 0, 1, . . . , n + 1 und mit der Notation ζj = bj exp ( −2πij 4(n + 1) ) f¨ur j = −n, . . . , −1, 0, 1, . . . , n + 1 erhalten wir yk = n+1∑ j=−n ζj exp (−2πijk 2n + 2 ) f¨ur k = 0, 1, . . . , n . Der Nenner 2n + 2 in dieser Formel verleitet zu einer Fourier-Transformation dop- pelter L¨ange. Wie in (2.5.27) deﬁnieren wir zj = yj f¨ur j = 0, 1, . . . , n und per Symmetrie zj = z2n+1−j f¨ur j = n + 1, . . . , 2n + 1, und somit haben wir zℓ = n+1∑ j=−n ζj exp (−2πij ˜ξℓ) mit ˜ξℓ = ℓ 2n + 2 f¨ur ℓ = 0, 1, . . . , 2n + 1 . Jetzt kommt ω2n+2 = exp( −2πi 2n+2 ) ins Spiel: zℓ = n+1∑ j=−n ζjωjℓ 2n+2 = 2n+1∑ k=0 ζk−nω(k−n)ℓ 2n+2 = ω−nℓ 2n+2 2n∑ j=0 ζj−nωjℓ 2n+2 oder ωnℓ 2n+2zℓ = 2n∑ j=0 ζj−nωjℓ 2n+2 . Somit F2n+2ζ = [exp ( −πinℓ n + 1 ) zℓ] . CHAPTER 2. POLYNOMINTERPOLATION 61 Code 2.5.1: Eﬃziente Interpolation in den Chebyshev-Knoten from numpy import exp, pi, real, hstack, arange 2 from scipy.fft import ifft 4 def chebexp(y): \"\"\"Efficiently compute coefficients $\\alpha_j$ in the Chebychev expansion 6 $p = \\sum\\limits_{j=0}^{n}\\alpha_j T_j$ of $p\\in\\Cp_n$ based on values $y_k$, $k=0,\\ldots,n$, in Chebychev nodes $t_k$, $k=0,\\ldots,n$. These values are 8 passed in the row vector $y$. \"\"\" 10 # degree of polynomial 12 n = y.shape[0] -1 14 # create vector z by wrapping and componentwise scaling 16 # r.h.s. vector t = arange(0, 2*n+2) 18 z = exp(-pi*1.0j*n/(n+1.0)*t) * hstack([y,y[::-1]]) 20 # Solve linear system for ζ with effort O(n log n) c = ifft(z) 22 # recover γj 24 t = arange(-n, n+2) b = real(exp(0.5j*pi/(n+1.0)*t) * c) 26 # recover coefficients c of Chebyshev expansion 28 a = hstack([ b[n], 2*b[n+1:2*n+1] ]) 30 return a Bemerkung 2.5.2. Die Chebyshev–Abszissen sind als die Extrema des Chebyshev– Polynoms T2n deﬁniert: ζk = cos (kπ 2n ) f¨ur k = 0, 1, . . . , 2n . Die Chebyshev–Abszissen mit geradem k sind genau die Nullstellen von Tn, d.h. die n Chebyshev–Knoten. Wir k¨onnen auch an die Chebyshev–Abszissen sehr eﬃzient und stabil interpolieren. Code 2.5.3: Eﬃziente Interpolation in den Chebyshev-Abszissen from numpy import exp, pi, real, hstack, arange, zeros, flipud, cos, linspace 2 from scipy.fft import ifft, fft 4 def chebexp(y): \"\"\"Efficiently compute coefficients $\\alpha_j$ in the Chebyshev expansion 6 $p = \\sum\\limits_{j=0}^{n}\\alpha_j T_j$ of $p\\in\\Cp_n$ based on values $y_k$, $k=0,\\ldots,n$, in Chebyshev abscissas $t_k$, $k=0,\\ldots,n$. These values are CHAPTER 2. POLYNOMINTERPOLATION 62 8 passed in the row vector y. \"\"\" 10 # degree of polynomial 12 n = y.shape[0]-1 # create vector z by wrapping 14 z = zeros(2*n, dtype=complex) z[:n+1] = 1.*y 16 z[n+1:] = flipud(y)[1:-1] c = fft(z)/(2.*n) 18 a = 1.*c[:n+1] 20 a[0] *= 0.5 a[-1] *= 0.5 22 return a 24 def evalchebexp(a,N): 26 n = a.shape[0]-1 c = zeros(2*N, dtype=complex) 28 c[:n+1] = 1.*a c[0] *= 2 30 c[n] *= 2 c[2*N-n+1:] = c[n-1:0:-1] 32 z = ifft(c)*2*N y = 1.*z[:N+1] 34 x = cos(arange(N+1)*pi/N) return x,y 36 def rungef(x): 38 \"\"\"Compute the value of the function of Runge's counterexample. \"\"\" 40 return 1.0 / (1.0 + x**2) 42 def step(x): 44 return where(abs(x-0.5)<0.25, 1., 0.) def ramp(x): 46 return where(x<1., x, 0.) def hat(x): 48 return where(abs(x)<=1.,1-abs(x), 0.) def morf(x): 50 return where(abs(x)<=1,0.5*(1+cos(pi*x)),0.) def smooth(x): 52 return 1./sqrt(1.+0.5*sin(2*pi*x)) 54 if __name__ == \"__main__\": 56 f = rungef n = 10 58 x = 5*cos(arange(n+1)*pi/n) # Chebyshev points y = f(x) 60 c = chebexp(y) CHAPTER 2. POLYNOMINTERPOLATION 63 62 t = linspace(-5.0, 5.0, 201) N = 200 64 xN, yN = evalchebexp(c,N) 66 from pylab import plot, legend, xlabel, show plot(t, f(t), \"r\") 68 plot(5*xN,yN,\"b--\") plot(x, y, \"k*\") 70 legend([\"Function\",\"Chebychev\"]) xlabel(\"t\") 72 show()","libVersion":"0.5.0","langs":""}