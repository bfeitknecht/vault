{"path":"HS23/LinAlg/UE/s/LinAlg-u02-s.pdf","text":"D-INFK Linear Algebra HS 2023 Afonso Bandeira Bernd G¨artner Solution for Assignment 2 1. a) We start by computing the powers of A. We get A2 =   0 1 0 0 3 1 1 0 3   and A3 =   1 0 3 3 1 9 0 3 1  . Plugging this into the equation, we get B := A3 + xA2 + yA + zI =  z + 1 x 3 + y 3 + y 1 + 3x + z 9 + x + 3y x 3 + y 1 + 3x + z   ! = 0. In particular, we want to choose x, y, z such that all entries will become zero. From b12 = x we can deduce x = 0, from b13 = 3 + y we can deduce y = −3, and from b11 = z + 1 we get z = −1. In fact, with this choice of x, y, z all entries of B become 0 and hence we found the unique solution. b) Note that one could prove this in a very formal way with induction. We opted for a slightly less but still sufficiently formal proof. Let k ∈ N be arbitrary. If k = 0, then we have (AB)k = I = I 2 = AkBk. Thus, assume now k > 0 and consider the expression (AB) k = AB × AB × · · · × AB ︸ ︷︷ ︸ k times = ABAB · · · AB︸ ︷︷ ︸ k repetitions of AB . Consider now the following algorithm: While there is an appearance of BA in the above expression, replace it by AB. Note that this operation does not change the product because matrix multipli- cation is associative and we are additionally given AB = BA in this exercise. Moreover, the algorithm must eventually terminate as the number of ways to arrange k A’s and k B’s in a string is finite and the algorithm will never consider the same arrangement twice (to see this, note that the sum of indices of the A’s decreases in every step of the algorithm). Once the algorithm terminates, all A’s must be to the left of the B’s (as otherwise we could find an appearance of BA). Hence, we have (AB)k = AkBk. c) According to the previous question, we have (AB)k = AkBk. We compute (AB)k = AkBk = Ak0 = 0. Therefore, AB is nilpotent with degree at most k. Remark: The nilpotent degree is not necessarily equal to k. If A is nilpotent of degree 5 and we choose B = A2, then B is nilpotent of degree 3. But AB = A3 is nilpotent of degree 2. d) Using distributivity of matrix multiplication, we compute: (I − A)(I + A + . . . + Ak−1) = (I + A + . . . + Ak−1) − A(I + A + . . . + Ak−1) = I + A + . . . + A k−1 − A − A2 − . . . − Ak = I − Ak = I (A is nilpotent of degree k) e) As is often the case, there are many ways to prove this. Here, we will argue by induction over k that the first k rows in T k must be zero for all k ∈ N. • Property: The k first columns of T k are all zero. • Base case: For k = 1, the property is true because by our assumptions on T , the first column of T must be zero. • Induction step: Fix a natural number 1 ≤ k < n and assume that the property is true for this k (induction hypothesis). We prove that the property is true for k + 1. In other words, we prove that the first (k + 1) columns of T k+1 are all zero. For this, we start by splitting T k+1 into T k+1 = T kT . By our induction hypothesis, the first k columns of T k are zero. In particular, we have T k =   | | | | | | 0 0 · · · 0 v1 v2 · · · vn−k | | | | | |   for some vectors v1, . . . , vn−k ∈ Rn. Now recall the column picture AB =   | | | Ab1 Ab2 · · · Abn | | |   for the product of two n × n matrices A and B where b1, b2, . . . , bn are the columns of B. Consider splitting T k+1 as T k+1 = T kT =   | | | T kt1 T kt2 · · · T ktn | | |   where t1, t2, · · · , tn are the columns of T . We want to argue that the first (k + 1) columns T kt1, T kt2, . . . , T ktk+1 of this matrix are zero. Let i ∈ {1, . . . k + 1}. By definition of T , only the first i − 1 entries of column ti can be non-zero. Moreover, we know that T kti is a linear combination of the columns of T k where the coefficients are the entries of vector ti. We just observed that at most the first i − 1 coefficients in this linear combination can be non-zero. At the same time, we know that the first k columns of T k are all zero. Hence, we conclude T kti = 0. 2. Consider the three constraints p(−1) = 0, p(0) = 2 and p(1) = 2 that we have on p. Each of these con- straints gives us an equation involving the unknowns a, b and c. In particular, we get the three equations a − b + c = 0 from p(−1) = 0 c = 2 from p(0) = 2 a + b + c = 2 from p(1) = 2 that we can also write down in matrix form   1 −1 1 0 0 1 1 1 1     a b c   =   0 2 2   . In order to solve this system, let us use the elimination method from the lecture. For this, let us define A =   1 −1 1 0 0 1 1 1 1   and b =   0 2 2   . We already highlighted the first pivot in A. After one step of elimination, we get E21A =   1 −1 1 0 0 1 1 1 1   and E21b =   0 2 2   with E21 = I as we already had a21 = 0. In the second step, we obtain E31E21A =   1 −1 1 0 0 1 0 2 0   and E31E21b =   0 2 2   with E31 =   1 0 0 0 1 0 −1 0 1  . Next, we need to permute rows 2 and 3 with the matrix P23 =   1 0 0 0 0 1 0 1 0   in order to get our next pivot. We obtain P23E31E21A =   1 −1 1 0 2 0 0 0 1   and P23E31E21b =   0 2 2   . In the last elimination step we again find that we do not need to do anything. In other words, we have E32 = I and get E32P23E31E21A =   1 −1 1 0 2 0 0 0 1   and E32P23E31E21b =   0 2 2   . We arrived at the desired upper triangular shape. It remains to use back substitution to get c = 2, b = 1 and a = −1. 3. a) Note that we almost provided a decomposition A = CR with C =   v1 v2 v3   and R = [ w1 w2 w3]. The only case where this is not a valid decomposition is if either v1 = v2 = v3 = 0 or w1 = w2 = w3 = 0. But still, the given decomposition already implies that the rank of A is at most 1. If we are guaranteed v1 ̸= 0 and w1 ̸= 0, then we indeed have a valid CR decomposition and conclude rank(A) = 1. b) We can use our decomposition of A. In particular, we want Ax =   v1 v2 v3   [ w1 w2 w3] x ! = 0. From this, we observe that it suffices to find a vector x with [ w1 w2 w3] x = w1x1 + w2x2 + w3x3 = 0. There are now many possibilities for x. One of them is x1 = w2, x2 = −w1, and x3 = 0. This is a non-zero vector because we have w1 ̸= 0. c) Consider the vector w =   w1 w2 w3  . We claim that L is exactly the set of vectors that are orthogonal to w. In other words, we claim L = {x ∈ R3 : x · w = 0}. Note that {x ∈ R3 : x · w = 0} is a hyperplane. In order to prove our claim, let us first consider an arbitrary vector x ∈ {x ∈ R3 : x · w = 0}. Because we have x · w = 0, we also get Ax =   v1 v2 v3   [w1 w2 w3] x =   v1 v2 v3   0 = 0. Hence, x is a solution, i.e. x ∈ L. For the reverse direction, we proceed with an indirect proof: Consider an arbitrary vector x ∈ R3 that is not orthogonal to w, i.e. x · w = c ̸= 0 for some c ∈ R. Then we have Ax =   v1 v2 v3   [ w1 w2 w3] x =   v1 v2 v3   c =   cv1 cv2 cv3   ̸= 0 since both v1 ̸= 0 and c ̸= 0. In particular, x is not in L. This concludes the proof. 4. a) We have to find some angle ϕ ∈ R such that A = Q(ϕ). Since cos(ϕ) should be zero, we have two candidates ϕ = 1 2 π and ϕ = 3 2 π if we restrict ourselves to ϕ ∈ [0, 2π). But we also need sin(ϕ) = 1 which is only true for ϕ = 1 2 π. Indeed, for ϕ = 1 2 π we have Q(ϕ) = [ 0 −1 1 0 ] = A. b) We use the definition of the matrix product and the following addition theorems of the trigonometric functions sin (α + β) = sin α cos β + cos α sin β cos (α + β) = cos α cos β − sin α sin β to get Q(ϕ1) · Q(ϕ2) = [ cos ϕ1 − sin ϕ1 sin ϕ1 cos ϕ1 ] [ cos ϕ2 − sin ϕ2 sin ϕ2 cos ϕ2 ] = [ cos ϕ1 cos ϕ2 − sin ϕ1 sin ϕ2 − cos ϕ1 sin ϕ2 − sin ϕ1 cos ϕ2 cos ϕ1 sin ϕ2 + sin ϕ1 cos ϕ2 cos ϕ1 cos ϕ2 − sin ϕ1 sin ϕ2 ] = [cos(ϕ1 + ϕ2) − sin(ϕ1 + ϕ2) sin(ϕ1 + ϕ2) cos(ϕ1 + ϕ2) ] . So the matrix product Q(ϕ1)Q(ϕ2) is a rotation matrix Q(ϕ1 + ϕ2) with the rotation angle ϕ3 = ϕ1 + ϕ2. c) Since A is a rotation matrix, there exists ϕ ∈ R with A = Q(ϕ). Choose B as the rotation matrix B = Q(−ϕ). By part b), we have AB = Q(ϕ)Q(−ϕ) = Q(ϕ − ϕ) = Q(0) = I = Q(0) = Q(−ϕ + ϕ) = Q(−ϕ)Q(ϕ) = BA. Multiple choice Let A be an m1 × n1 matrix and let B be an m2 × n2 matrix for natural num- bers m1, n1, m2, n2. For each statement, determine whether it is true or not (regardless of what values m1, n1, m2, n2 take). 1. If A2 is defined, then A must be square. √ (a) Yes (b) No Explanations: For matrix multiplication to work, the number of columns of the first matrix has to equal the number of rows of the second matrix. In this case, both the first and second matrix are A which implies that A must be square. 2. If A2 = I, then A = I. (a) Yes √ (b) No Explanations: A possible counterexample is A = [0 1 1 0 ]. 3. If A3 = 0, then A = 0. (a) Yes √ (b) No Explanations: A possible counterexample is A = [0 1 0 0 ] . 4. If A = [ 1 a 0 1 ], then An = [ 1 na 0 1 ] for all n ∈ N. √ (a) Yes (b) No Explanations: A full proof could be obtained via induction. We will provide an intuitive explanation. Consider a 2 × 2 matrix B and consider what happens when we multiply A with B. The first row of AB will be the first row of B, plus a times the second row of B. The second row of AB will be copied from the second row of B. Now if the second row of B is [ 0 1] , this means that left-multiplying with A corresponds to adding a to the entry in the first row and second column. 5. If AB = B for some choice of B, then A = I. (a) Yes √ (b) No Explanations: If B = 0 and A ̸= I, we still have AB = B. 6. If both products AB and BA are defined, then A and B must be square. (a) Yes √ (b) No Explanations: A could be a 3 × 2 matrix while B is a 2 × 3 matrix. 7. If both products AB and BA are defined, then AB and BA must be square. √ (a) Yes (b) No Explanations: If both products are defined, we must have n1 = m2 and n2 = m1. Observe that AB is an m1 × n2 matrix while BA is an m2 × n1 matrix. Hence, both must be square. 8. If two columns of A are equal and AB is defined, the corresponding columns of AB must also be equal. (a) Yes √ (b) No Explanations: A counterexample would be A = [1 1 0 0 ] and B = [ 1 0 1 0 ]. 9. If two columns of B are equal and AB is defined, the corresponding columns of AB must also be equal. √ (a) Yes (b) No Explanations: This can be seen from the column picture of matrix multiplication (page 12 in blackboard notes). 10. If two rows of A are equal and AB is defined, the corresponding rows of AB must also be equal. √ (a) Yes (b) No Explanations: This can be seen from the row picture of matrix multiplication (page 12 in blackboard notes). 11. If two rows of B are equal and AB is defined, the corresponding rows of AB must also be equal. (a) Yes √ (b) No Explanations: A counterexample would be A = [1 1 0 0 ] and B = [ 1 0 1 0 ]. 12. If A and B are symmetric matrices and AB is defined, AB is also symmetric. (a) Yes √ (b) No Explanations: A counterexample would be A = [1 0 0 0 ] and B = [ 0 1 1 0 ].","libVersion":"0.3.2","langs":""}