{"path":"sem3/LinAlg/VRL/extra/slides/LinAlg-slides-w08.pdf","text":"Robert Weismantel Week 8: Orthogonal vectors, orthogonal complements of subspaces and projections Orthogonality of vectors and subspaces The target Orthogonality is a key concept that allows us to decompose a space into two subspaces, understand systems of linear equations, and allows us to define a pseudoinverse. Definition Vectors v , w ∈ Rn are orthogonal/ perpendicular (see Def. 1.15) if v T w = n ∑ i=1 vi wi = 0. Subspaces V and W are orthogonal if for all v ∈ V and w ∈ W , the vectors v and w are orthogonal. Lemma Let v1, . . . , vk and w1, . . . , wl be bases of subspace V and W . V and W are orthogonal if and only if vi and wj are orthogonal for all i and j. Robert Weismantel October 24, 2024 2 / 16 Preliminaries Proof of the first lemma Suppose V and W are orthogonal. Since vi ∈ V for all i and wj ∈ W for all j, we have v T i wj = 0 for all i, j. Conversely, assume that v T i wj = 0 for all i and j. Let v = ∑ k i=1 λi vi ∈ V and w = ∑ l j=1 µj wj ∈ W . v T w = k ∑ i=1 λi v T i w = k ∑ i=1 λi v T i l ∑ j=1 µj wj = k ∑ i=1 l ∑ j=1 µj λi v T i wj = 0. Lemma Let V and W be two orthogonal subspaces of Rn. Let v1, . . . , vk be a basis of subspace V . Let w1, . . . , wl be a basis of subspace W . The set of vectors {v1, . . . , vk , w1, . . . , wl } are linearly independent. Robert Weismantel October 24, 2024 3 / 16 Further preliminaries Proof of the second lemma Consider the linear combination (∗) k ∑ i=1 λi vi + l ∑ j=1 µj wj = 0. We want to show λi = 0 for all i and µj = 0 for all j. Let v = ∑ k i=1 λi vi . (∗) is equivalent to v = − ∑ l j=1 µj wj . We obtain v T v = − l ∑ j=1 µj v T wj = 0. Hence, v = 0. This implies λi = 0 for all i (v1, . . . , vk is a basis of V ). Accordingly, one shows that µj = 0 for all j by considering w = ∑l j=1 µj wj and noticing that w T w = 0. Robert Weismantel October 24, 2024 4 / 16 The orthogonal complement of a subspace I Corollary Let V and W be orthogonal subspaces. Then V ∩ W = {0}. Moreover, dim(V + W ) = dim({v + w | v ∈ V , w ∈ W }) = dim(V ) + dim(W ) ≤ n. Definition Let V be a subspace of Rn. We define the orthogonal complement of V as V ⊥ = {w ∈ Rn | w T v = 0 for all v ∈ V }. V ⊥ is a subspace of Rn! Theorem Let A ∈ Rm×n be a matrix. Then N(A) = C(AT )⊥ = R(A)⊥. Robert Weismantel October 24, 2024 5 / 16 Proof of the Theorem Proof that N(A) ⊆ C(AT )⊥. Let x ∈ N(A). Take any b ∈ C(AT ) = R(A), i.e., b = AT y for some y ∈ Rm. Then bT x = y T Ax = y T 0 = 0. Hence, x ∈ C(AT )⊥. Proof that C(AT )⊥ ⊆ N(A). Let x ∈ C(AT )⊥. By definition, bT x = 0 for all b ∈ C(AT ). Define y as the following specific vector: y := Ax ∈ Rm. Then b := AT y ∈ C(AT ) and hence, x T b = 0. We obtain 0 = x T b = x T A T y = x T A T Ax = ∥Ax∥ 2 ⇐⇒ x ∈ N(A). Recall from Part 1: If r = dim(R(A)) = dim(C(AT )), then n − r = dim(N(A). Robert Weismantel October 24, 2024 6 / 16 The orthogonal complement of a subspace II Theorem Let V , W be orthogonal subspaces of Rn. The statements are equivalent. (i) W = V ⊥. (ii) dim(V ) + dim(W ) = n. (iii) Every u ∈ Rn can be written as u = v + w with unique v ∈ V , w ∈ W . Recall for the proof Let v1, . . . , vk be a basis of V and w1, . . . , wl a basis of W . V and W are orthogonal if and only if v T i wj = 0 for all i ∈ {1, . . . , k }, j ∈ {1, . . . , l}. (i) implies (ii): Define A ∈ Rk×n to be the matrix with row vectors v1, . . . , vk . Then V = R(A) = C(AT ). Moreover, W = V ⊥ = N(A) from the previous theorem. From the remark one slide before: dim(V ) = k and hence, dim(W ) = n − k . Robert Weismantel October 24, 2024 7 / 16 Proof continued (ii) implies (iii): The vectors in the set {v1, . . . , vk , w1, . . . , wl } are linearly independent. Since by assumption l = n − k , this set is a basis of Rn. Hence, for all u ∈ Rn, u = k ∑ i=1 λi vi + l ∑ j=1 µj wj , where λ1, λk , µ1, . . . , µl ∈ R. Define the unique vectors v := ∑ k i=1 λi vi , w := ∑ l j=1 µj wj . (iii) implies (i): We need to show that W = V ⊥. W ⊆ V ⊥ since W is orthogonal to V . For the reverse inclusion, let u ∈ V ⊥ ⊆ Rn. From (iii) u = v + w where v ∈ V and w ∈ W . Then 0 = uT v = v T v + v T w = v T v = ∥v ∥2 ⇒ v = 0 ⇒ u = w ∈ W . Robert Weismantel October 24, 2024 8 / 16 Decomposition of Rn Lemma Let V be a subspace of Rn. Then V = (V ⊥)⊥. Proof. Let v1, . . . , vk be a basis of V and w1, . . . , wl a basis of V ⊥. l = n − k. Moreover, v T i wj = 0 for all i and j and hence, (V ⊥) ⊥ = {x ∈ Rn | x T wj = 0 for all j = 1, . . . , n − k}. Since v T i wj = 0 for all j = 1, . . . , n − k we obtain that V ⊆ (V ⊥)⊥. From the Theorem before, dim((V ⊥)⊥) = n − (n − k) = k . Since {v1, . . . , vk } ⊆ V ⊆ (V ⊥)⊥ are linearly independent, they are a basis of (V ⊥)⊥. Hence V = (V ⊥)⊥. Corollary For a subspace V of Rn, Rn = V + V ⊥ = {v + w | v ∈ V , w ∈ V ⊥}. Robert Weismantel October 24, 2024 9 / 16 The set of all solutions to a system of linear equations Corollary For A ∈ Rm×n, N(A) = C(AT )⊥ and C(AT ) = N(A)⊥. To refine our understanding, Let A ∈ Rm×n. There are two important subspaces associated with A: N(A) = {x ∈ Rn | Ax = 0} R(A) = C(AT ) = {AT y | y ∈ Rm} = {x ∈ Rn | ∃y ∈ Rm such that x = AT y }. N(A) is the orthogonal complement of R(A) and R(A) the orthogonal complement of N(A). Hence ∀x ∈ Rn there exist x0 ∈ N(A) and x1 ∈ R(A) such that x = x0 +x1 and x T 1 x0 = 0. Theorem {x ∈ Rn | Ax = b} = x1 + N(A) where x1 ∈ R(A) such that Ax1 = b. Robert Weismantel October 24, 2024 10 / 16 A link between the nullspaces of A and AT A Lemma Let A ∈ Rm×n. Then N(A) = N(AT A) and C(AT ) = C(AT A). Proof. If x ∈ N(A) then Ax = 0 and so A⊤Ax = 0, thus x ∈ N(A⊤A). If x ∈ N(A⊤A) then A⊤Ax = 0. This implies that x ⊤A ⊤Ax = x ⊤0 = 0. This gives 0 = x ⊤A ⊤Ax = (Ax)⊤(Ax) = ∥Ax∥ 2, so Ax = 0 and so x ∈ N(A). For the second statement we notice C(A T ) = N(A) ⊥ = N(A T A)⊥ = C((A T A)T ) = C(A T A). Robert Weismantel October 24, 2024 11 / 16 Projections Definition (Projection of a vector onto a subspace) The projection of a vector b ∈ Rm on a subspace S (of Rm) is the point in S that is closest to b. In other words projS(b) = argmin p∈S ∥b − p∥. (1) Sanity check This is only a proper definition if the minimum exists and is unique. The one-dimensional case Let S be the subspace corresponding to the line that goes through the vector a ∈ Rm \\ {0}, i.e. S = {λ a | λ ∈ R} = C(a). By drawing a two dimensional example one can see that the projection p is the vector in the subspace S such that the “error vector” e = b − p is perpendicular to a (i.e. b − p ⊥ a). Robert Weismantel October 24, 2024 12 / 16 The one dimensional case Lemma Let a ∈ Rm \\ {0}. The projection of b ∈ Rm on S = {λ a | λ ∈ R} = C(a) is projS(b) = aaT aT a b. Proof. Let p ∈ S, p = λ a for λ ∈ R. ∥b −p∥2 = (b −p) T (b −p) = bT b −2bT p +pT p = ∥b∥ 2 −2λ bT a+λ 2∥a∥ 2 = g(λ ). g is a convex, quadratic function in one variable λ . The minimizer is obtained at the point λ ∗ where the derivative vanishes. g′(λ ) = −2bT a + 2λ ∥a∥ 2 = 0 ⇐⇒ λ ∗ = bT a aT a . Hence, projS(b) = λ ∗a = a bT a aT a = a aT b aT a = aaT aT a b. Robert Weismantel October 24, 2024 13 / 16 About our initial intuition Our guess was that the projection p should be the vector in the subspace S such that the “error vector” e = b − p is perpendicular to a, i.e., (b − projS(b)) ⊥ a. By substituting what we just computed we get aT (b − projS(b)) = aT (b − aaT aT a b) = aT b − aT ( aaT aT a b) = aT b − 1 aT a aT aaT b = aT b − aT b = 0. A final check The projection of a vector that is already a multiple of a should be the vector itself. This is indeed true! Robert Weismantel October 24, 2024 14 / 16 The general case The idea is similiar to the one-dimensional case Let S be a subspace in Rm with dimension n. Let a1, . . . , an be a basis of S, i.e., S = span(a1, . . . , an) = C(A) = {Ax | x ∈ Rn} where A =   | | | a1 a2 · · · an | | |   . Lemma The projection of a vector b ∈ Rm to the subspace S = C(A) can be written as projS(b) = Aˆx, where ˆx satisfies the normal equations A T Aˆx = A T b. Recall for m = 1 projS(b) = λ ∗a = aaT aT a b ⇐⇒ aT aλ ∗a = aT ba ⇐⇒ aT aλ ∗ = aT b. Robert Weismantel October 24, 2024 15 / 16 Proof. b ∈ Rm. Hence b = p + e where p ∈ S and e ∈ S⊥, i.e., pT e = 0. Consider another point p′ ∈ S. Then p − p′ ∈ S and hence, eT (p − p′) = 0. This gives ∥p′ − b∥2 = ∥p′ − p + p − b∥2 = ∥p′ − p − e∥2 = ∥p′ − p∥2 + ∥e∥2 ≥ ∥e∥2 = ∥p − b∥2. We have shown that projS(b) = p = Aˆx ∈ S where b = p + e with e ∈ S⊥. Since S = C(A), (b − projS(b)) ⊥ ai for all i = 1, . . . , n ⇐⇒ aT i (b − projS(b)) = 0 for all i. This is equivalent to saying that A T (b − projS(b)) = 0 ⇐⇒ A T (b − Aˆx) = 0 ⇐⇒ A T Aˆx = A T b. Robert Weismantel October 24, 2024 16 / 16","libVersion":"0.3.2","langs":""}