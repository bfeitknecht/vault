{"path":"sem2/DDCA/VRL/slides/DDCA-L26-virtual-memory.pdf","text":"Digital Design & Computer Arch. Lecture 26: Virtual Memory Konstantinos Kanellopoulos Prof. Onur Mutlu ETH Zürich Spring 2024 31 May 2024 A Blueprint for Fundamentally Better Architectures n Onur Mutlu, \"Intelligent Architectures for Intelligent Computing Systems\" Invited Paper in Proceedings of the Design, Automation, and Test in Europe Conference (DATE), Virtual, February 2021. [Slides (pptx) (pdf)] [IEDM Tutorial Slides (pptx) (pdf)] [Short DATE Talk Video (11 minutes)] [Longer IEDM Tutorial Video (1 hr 51 minutes)] 2 Fundamentally Better Architectures Data-centric Data-driven Data-aware 3 Readings n Virtual Memory n Recommended q H&H Chapter 8.4 q Kim & Mutlu, “Memory Systems,” Computing Handbook, 2014. n https://people.inf.ethz.ch/omutlu/pub/memory-systems-introduction_computing- handbook14.pdf q Jacob & Mudge, “Virtual Memory: Issues of Implementation,” IEEE Computer, 1998. q Kanellopoulos et al. \"Utopia: Fast and Efficient Address Translation via Hybrid Restrictive & Flexible Virtual-to-Physical Address Mappings” MICRO 2023 q Hajinazar et al. “The Virtual Block Interface: A Flexible Alternative to the Conventional Virtual Memory Framework” ISCA 2020 4 Virtual Memory Memory (Programmer’s View) 6 Ideal Memory n Zero access time (latency) n Infinite capacity n Zero cost n Infinite bandwidth (to support multiple accesses in parallel) 7 Abstraction: Virtual vs. Physical Memory n Programmer sees virtual memory q Can assume the memory is “infinite” n Reality: Physical memory size is much smaller than what the programmer assumes n The system (system software + hardware, cooperatively) maps virtual memory addresses to physical memory q The system automatically manages the physical memory space transparently to the programmer + Programmer does not need to know the physical size of memory nor manage it à A small physical memory can appear as a huge one to the programmer à Life is easier for the programmer -- More complex system software and architecture A classic example of the programmer/(micro)architect tradeoff Requires indirection and mapping between virtual and physical address spaces Benefits of Automatic Management of Memory n Programmer does not deal with physical addresses n Each process has its own q Virtual address space (very large) q Independent mapping of virtualàphysical addresses n Enables q Code and data to be located anywhere in physical memory (relocation and flexible location of data) q Isolation/separation of code and data of different processes in physical memory (protection and isolation) q Code and data sharing between multiple processes (sharing) 910 A System with Physical Memory Only n Examples: q most early supercomputers q early personal computers (PCs) q many older embedded systems CPU’s load or store instructions generate physical memory addresses CPU 0: 1: N-1: Memory Physical Addresses The Problem n Physical memory is of limited size (cost) q What if you need more? q Should the programmer be concerned about the size of code/data blocks fitting physical memory? q Should the programmer manage data movement from disk to physical memory? q Multiple programs may need the physical memory q Should the programmer make sure all processes (different programs) can fit in physical memory? q Should the programmer ensure two processes do not unintentionally or incorrectly use the same physical memory portion? n ISA can have an address space greater than the physical memory size q E.g., a 64-bit address space with byte addressability à 16 ExaBytes q What if you do not have enough physical memory? 11 Difficulties of Direct Physical Addressing n Programmer needs to manage physical memory space q Inconvenient & difficult q More difficult when you have multiple processes n Difficult to support code and data relocation q Addresses are directly specified in the program n Difficult to support multiple processes (esp. concurrently) q Protection and isolation between multiple processes q Sharing of physical memory space without problems n Difficult to support data/code sharing across processes q Different processes need to reference the same physical address 12 Virtual Memory n Idea: Give each program the illusion of a large address space while having a small physical memory q So that the programmer does not worry about managing physical memory (within a process or across processes) n Programmer can assume they have “infinite” amount of physical memory n Hardware and software cooperatively and automatically manage the physical memory space to provide the illusion q Illusion is maintained for each independent process 13 Basic Mechanism n Indirection and mapping (of addresses) n Address generated by each instruction in a program is a “virtual address” q i.e., it is not the physical address used to address main memory q called “linear address” in x86 n An “address translation” mechanism maps this address to a “physical address” q called “real address” in x86 q Address translation mechanism can be implemented in hardware and software together 14 Virtual Memory: Conceptual View n Illusion of large, separate address space per process 15 Kim & Mutlu, “Memory Systems,” Computing Handbook, 2014 https://people.inf.ethz.ch/omutlu/pub/memory-systems-introduction_computing-handbook14.pdf Process 1 Process 2 Requires indirection and mapping between virtual and physical address spaces 16 A System with Virtual Memory (Page-based) n Address Translation: The hardware converts virtual addresses into physical addresses via an OS-managed lookup table (page table) CPU 0: 1: N-1: Memory 0: 1: P-1: Page Table Disk Virtual Addresses Physical Addressesvirtualvirtualphysical Process 1 Process 24GB4GB16MB Virtual Page Virtual Page Physical Page Mapping Page-based Virtual-to-Physical MappingFour Issues in Indirection and Mapping n When to map a virtual address to a physical address? q When the virtual address is first referenced by the program n What is the mapping granularity? q Byte? Kilo-byte? Mega-byte? Giga-byte? … q Multiple granularities? n Where and how to store the virtualàphysical mappings? q Operating system data structures? Hardware? Cooperative? n What to do when physical address space is full? q Evict an unlikely-to-be-needed virtual address from physical memory 18 Virtual Pages, Physical Frames n Virtual address space divided into pages n Physical address space divided into frames n A virtual page is mapped to q A physical frame, if the page is in physical memory q A location in disk, otherwise n If an accessed virtual page is not in memory, but on disk q Virtual memory system brings the page into a physical frame and adjusts the mapping à this is called demand paging n Page table is the table that stores the mapping of virtual pages to physical frames 19 Physical Memory as a Cache n In other words… n Physical memory is a cache for pages stored on disk q In fact, it is a fully-associative cache in modern systems (a virtual page can potentially be mapped to any physical frame) n Similar caching issues exist as we have covered earlier: q Placement: where and how to place/find a page in cache? q Replacement: what page to remove to make room in cache? q Granularity of management: large, small, uniform pages? q Write policy: what do we do about writes? Write back? 20 Cache/Virtual Memory Analogues Cache Virtual Memory Block Page Block Size Page Size Block Offset Page Offset Miss Page Fault Index Virtual Page Number Metadata (Tag) Store Page Table Data Store Physical Memory 21 Virtual Memory Definitions n Page size: the mapping granularity of virtualàphysical address spaces q dictates the amount of data transferred from hard disk to DRAM at once n Page table: table that stores virtualàphysical page mappings q lookup table used to translate virtual page addresses to physical frame addresses (and find where the associated data is) n Address translation: the process of determining the physical address from the virtual address 22 Virtual to Physical Mapping n Most accesses hit in physical memory n Programs see the large capacity of virtual memory 23H&H, Chapter 8.4 Address Translation 24H&H, Chapter 8.4 Virtual Memory Example n System: q Virtual memory size: 2 GB = 231 bytes q Physical memory size: 128 MB = 227 bytes q Page size: 4 KB = 212 bytes 25 Virtual Memory Example (Continued) n System: q Virtual memory size: 2 GB = 231 bytes q Physical memory size: 128 MB = 227 bytes q Page size: 4 KB = 212 bytes n Organization: q Virtual address: 31 bits q Physical address: 27 bits q Page offset: 12 bits q # Virtual pages = 231/212 = 219 (VPN = 19 bits) q # Physical pages = 227/212 = 215 (PPN = 15 bits) 26 Virtual Memory Example (Continued) 27H&H, Chapter 8.4 How Do We Translate Addresses? n Page table q Has entry for each virtual page n Each page table entry has: q Valid bit: whether the virtual page is located in physical memory (if not, it must be fetched from the hard disk) q Physical page number: where the virtual page is located in physical memory q (Replacement policy, dirty/modified, permission/access bits) 28 Page Table for Our Example (Continued) 29H&H, Chapter 8.4 Page Table Address Translation Example 0 0 1 0x0000 1 0x7FFE 0 0 0 0 1 0x0001 0 0 1 0x7FFF 0 0 V Virtual Address 0x00002 47C Hit Physical Page Number 1219 15 12 Virtual Page NumberPage Table Page Offset Physical Address 0x7FFF 47C 30 Page Table is Indexed with the VPN Page Table Provides The PPN Page Table is located at physical memory address specified by the PTBR (Page Table Base Register) Page offset bits do not change during translation Page Table Address Translation Example 1 n What is the physical address of virtual address 0x5F20? n We first need to find the page table entry containing the translation for the corresponding VPN n Look up the PTE at the address q PTBR + VPN*PTE-size 0 0 1 0x0000 1 0x7FFE 0 0 0 0 1 0x0001 0 0 1 0x7FFF 0 0 V Hit Physical Page Number 15Page Table 31 Page Table Address Translation Example 1 n What is the physical address of virtual address 0x5F20? q VPN = 5 q Entry 5 in page table indicates VPN 5 is in physical page 1 q Physical address is 0x1F20 0 0 1 0x0000 1 0x7FFE 0 0 0 0 1 0x0001 0 0 1 0x7FFF 0 0 V Virtual Address 0x00005 F20 Hit Physical Page Number 1219 15 12 Virtual Page NumberPage Table Page Offset Physical Address 0x0001 F20 32 Page Table Address Translation Example 2 n What is the physical address of virtual address 0x73E0? 0 0 1 0x0000 1 0x7FFE 0 0 0 0 1 0x0001 0 0 1 0x7FFF 0 0 V Hit Physical Page Number 15Page Table 33 Page Table Address Translation Example 2 n What is the physical address of virtual address 0x73E0? q VPN = 7 q Entry 7 in page table is invalid, so the page is not in physical memory q The virtual page must be swapped into physical memory from disk 0 0 1 0x0000 1 0x7FFE 0 0 0 0 1 0x0001 0 0 1 0x7FFF 0 0 V Virtual Address 0x00007 3E0 Hit Physical Page Number 19 15 Virtual Page NumberPage Table Page Offset 34 Issue: Page Table Size n Suppose 64-bit VA and 40-bit PA, how large is the page table? n 252 entries x ~4 bytes » 254 bytes and that is for just one process! and the process may not be using the entire VM space! 35 VPN Page Offset page table concat PA 64-bit 12-bit52-bit 28-bit 40-bit Page Table Challenges (I) n Challenge 1: Page table is large q at least part of it needs to be located in physical memory q solution: multi-level (hierarchical) page tables 36 Multi-Level Page Tables n Idea: Organize page table in a hierarchical manner such that only a small first-level page table has to be in physical memory n Multi-level (hierarchical) page tables 37 Multi-Level Page Table Example n First-level page table has to be in physical memory n Only the needed second-level page tables can be kept in physical memory 38 Multi-Level Page Table: Address Translation n For N-level page table, we need N page table accesses to find the PTE 39 Multi-Level Page Tables from x86 Manual 40 Example from the x86 architecture CR3: Control Register 3 (or Page Directory Base Register) x86 Page Tables (I): Small Pages 41 x86 Page Tables (II): Large Pages 42 Four-level Paging in x86-64 43 Page Table in x86-64 9 bits Virtual Address 9 bits 9 bits 9 bits CR3 44 Physical Frame Number PL4 PL3 PL2 PL1 Page Table Walk in x86-64 9 bits Virtual Address 9 bits 9 bits 9 bits 45 Physical Frame Number 12 bits 12 bits 1 2 3 4 Four sequential memory accesses during a page table walk in x86-64 Page Table Challenges (II) n Challenge 1: Page table is large q at least part of it needs to be located in physical memory q solution: multi-level (hierarchical) page tables n Challenge 2: Each instruction fetch or load/store requires at least two memory accesses: 1. one for address translation (page table read) 2. one to access data with the physical address (after translation) n Two memory accesses to service an instruction fetch or load/store greatly degrades execution time q Num. of memory accesses increases with multi-level page tables q Unless we are clever… à speed up the translation… 46 Translation Lookaside Buffer (TLB) n Idea: Cache the Page Table Entries (PTEs) in a hardware structure in the processor to speed up address translation n Translation lookaside buffer (TLB) q Small cache of most recently used Page Table Entries, i.e., recently used Virtual-to-Physical translations q Reduces the number of memory accesses required for most instruction fetches and loads/stores to only one TLB access 47 Translation Lookaside Buffer (TLB) n Page table accesses have temporal and spatial locality q Memory accesses have temporal and spatial locality q Large page sizes better exploit spatial locality (KBs, MBs, GBs) q Consecutive instructions and loads/stores are likely to access same page n TLB: cache of page table entries (i.e., translations) q Small: accessed in ~1 cycle q Typically 16 - 512 entries at level 1 q Usually high associativity q > 90-99 % hit rates typical (depends on workload) q Reduces the number of memory accesses for most instruction fetches and loads/stores to only one TLB access 48 Example Two-Entry TLB Hit1 V =01 15 15 15 = Hit1Hit0 Hit 19 19 19 Virtual Page Number Physical Page Number Entry 1 1 0x7FFFD 0x0000 1 0x00002 0x7FFF Virtual Address 0x00002 47C 1219 Virtual Page Number Page Offset V Virtual Page Number Physical Page Number Entry 0 12 Physical Address 0x7FFF 47C TLB 49 TLB is a Translation (PTE) Cache n All issues we discussed in caching and prefetching lectures apply to TLBs n Example issues: q Instruction vs. Data TLBs q Multi-level TLBs q Associativity and size choices and tradeoffs q Insertion, promotion, replacement policies q What to keep in which TLB and how to decide that q Prefetching into the TLBs q TLB coherence q Shared vs. private TLBs across cores/threads q … 50 Virtual Memory Support and Examples Supporting Virtual Memory n Virtual memory requires both HW+SW support q Page Table is in memory q Can be cached in special hardware structures called Translation Lookaside Buffers (TLBs) n The hardware component is called the MMU (memory management unit) q Includes Page Table Base Register(s), TLBs, page walkers n It is the job of the software (e.g., the Operating System) to q Populate page tables, decide what to replace in physical memory q Change the Page Table Base Register on context switch (to use the running thread’s page table) q Handle page faults and ensure correct mapping 52 Address Translation n How to obtain the physical address from a virtual address? n Page size specified by the ISA q Today: 4KB, 8KB, 2GB, … (small and large pages mixed together) q Trade-offs? (remember cache lectures) n Page Table contains an entry for each virtual page q Called Page Table Entry (PTE) q What is in a PTE? 53 What Is in a Page Table Entry (PTE)? 54 n Page table is the “tag store” for the physical memory data store q A mapping table between virtual memory and physical memory n PTE is the “tag store entry” for a virtual page in memory q Need a valid bit à to indicate validity/presence in physical memory q Need tag bits (PFN) à to support translation q Need bits to support replacement q Need a dirty bit to support “write back caching” q Need protection bits to enable access control and protection 55 Recall: Address Translation (I) n Parameters q P = 2p = page size (bytes) q N = 2n = Virtual-address limit q M = 2m = Physical-address limit virtual page number page offset virtual address physical frame number page offset physical address 0p–1 address translation pm–1 n–1 0p–1p Page offset bits do not change as a result of translation 56 Recall: Address Translation (II) virtual page number (VPN) page offset virtual address physical frame number (PFN) page offset physical address 0p–1pm–1 n–1 0 p–1p page table base register (per process) if valid=0 then page not in memory (page fault) valid physical frame number (PFN) VPN acts as table index n Separate (set of) page table(s) per process n VPN forms index into page table (points to a page table entry) n Page Table Entry (PTE) provides information about page access 57 Address Translation: Page Hit 58 Address Translation: Page FaultPage Fault (“A Miss in Physical Memory”) n If a page is not in physical memory but disk q Page table entry indicates virtual page not in memory q Access to such a page triggers a page fault exception q OS exception handler invoked to move data from disk into memory n Other processes can continue executing n OS has full control over page placement CPU Memory Page Table Disk Virtual Addresses Physical Addresses CPU Memory Page Table Disk Virtual Addresses Physical Addresses Before fault After fault Disk 60 Servicing a Page Fault 1. Processor signals I/O controller q Read block of length P starting at disk address X and store starting at memory address Y 2. Disk-to-mem read occurs q Direct Memory Access (DMA) q Under control of I/O controller 3. Controller signals completion q Interrupts processor q OS resumes suspended process Disk Memory-I/O bus Processor Cache Memory I/O controller Reg (2) DMA Transfer (1) Initiate Block Read (3) Read Done Page Replacement Algorithms n If physical memory is full (i.e., list of free physical pages is empty), which physical frame to replace on a page fault? n Is True LRU feasible? q 4GB memory, 4KB pages, how many possibilities of ordering? n Modern systems use approximations of LRU q E.g., the CLOCK algorithm n And, more sophisticated algorithms to take into account “frequency” of use q E.g., the ARC algorithm q Megiddo and Modha, “ARC: A Self-Tuning, Low Overhead Replacement Cache,” FAST 2003. 61 CLOCK Page Replacement Algorithm n Keep a circular list of physical frames in memory (OS does) n Keep a pointer (hand) to the last-examined frame in the list n When a page is accessed, set the R bit in the PTE n When a frame needs to be replaced, replace the first frame that has the reference (R) bit not set, traversing the circular list starting from the pointer (hand) clockwise q During traversal, clear the R bits of examined frames q Set the hand pointer to the next frame in the list 62 Cache versus Page Replacement n Physical memory (DRAM) is a cache for disk q Managed by system software via the virtual memory subsystem n Page replacement is similar to cache replacement n Page table is the “tag store” for physical memory data store n What is the difference? q Required speed of access to cache vs. physical memory q Number of blocks in a cache vs. physical memory q “Tolerable” amount of time to find a replacement candidate (disk versus memory access latency) q Role of hardware versus software 63 Memory Protection Memory Protection n Multiple programs (i.e., processes) run concurrently q Each process has its own page table q Each process can use its entire virtual address space without worrying about where other programs are n A process can only access physical pages mapped in its page table – cannot overwrite memory of another process q Provides protection and isolation between processes q Enables access control mechanisms per page 65 Page Table is Per Process n Each process has its own virtual address space q Full address space for each program q Simplifies memory allocation, sharing, linking and loading 66 Virtual Address Space for Process 1: Physical Address Space (DRAM)VP 1 VP 2 PP 2Address Translation 0 0 N-1 0 N-1 M-1 VP 1 VP 2 PP 7 PP 10 (e.g., read-only library code) ... ... Virtual Address Space for Process 2: Access Protection/Control via Virtual Memory Page-Level Access Control (Protection) n Not every process is allowed to access every page q E.g., need supervisor (i.e., kernel) level privilege to access system pages q E.g., may not be able to execute “instructions” in some pages n Idea: Store access control information on a page basis in the process’s page table n Enforce access control at the same time as translation à Virtual memory system serves two functions today Address translation (for illusion of large physical memory) Access control (protection) 68 Two Functions of Virtual Memory 69 VM as a Tool for Memory Access Protection 70 Page Tables Process i: Physical AddrRead? Write? PP 6Yes No PP 4Yes Yes XXXXXXXNo No VP 0: VP 1: VP 2: • • • • • • • • • Process j: PP 0 Memory Physical AddrRead? Write? PP 6Yes Yes PP 9Yes No XXXXXXXNo No • • • • • • • • • VP 0: VP 1: VP 2: PP 2 PP 4 PP 6 PP 8 PP 10 PP 12 • • • n Extend Page Table Entries (PTEs) with permission bits n Check bits on each access and during a page fault q If violated, generate exception (Access Protection exception) Privilege Levels in x86 71 Privilege Levels in x86 n Four privilege levels in x86 (referred to as rings) q Ring 0: Highest privilege (operating system) q Ring 1: Not widely used q Ring 2: Not widely used q Ring 3: Lowest privilege (user applications) n Supervisor = Kernel (in modern terminology) “Supervisor” “User” x86: A Closer Look at the PDE/PTE n PDE: Page Directory Entry (32 bits) n PTE: Page Table Entry (32 bits) PPNPTE Flags &PTPDE Flags Protection: PDE’s Flags n Protects all 1024 pages in a page table Protection: PTE’s Flags n Protects one page at a time Page Level Protection in x86 76 Protection: PDE + PTE = ???Food for Thought: What If? n Your hardware is unreliable and someone can flip the access protection bits q such that a user-level program can gain supervisor-level access (i.e., access to all data on the system) q by flipping the access control bit from user to supervisor! n Can this happen? 78 Remember RowHammer? One can predictably induce errors in most DRAM memory chips 79 Remember RowHammer? n One can predictably induce bit flips in commodity DRAM chips q >80% of the tested DRAM chips are vulnerable n First example of how a simple hardware failure mechanism can create a widespread system security vulnerability 80 Row of Cells Row Row Row Row Wordline VLOWVHIGH Victim Row Victim Row Hammered Row Repeatedly reading a row enough times (before memory gets refreshed) induces disturbance errors in adjacent rows in most real DRAM chips you can buy today OpenedClosed 81 Modern DRAM is Prone to Disturbance Errors Flipping Bits in Memory Without Accessing Them: An Experimental Study of DRAM Disturbance Errors, (Kim et al., ISCA 2014) CPU loop: mov (X), %eax mov (Y), %ebx clflush (X) clflush (Y) mfence jmp loop Download from: https://github.com/CMU-SAFARI/rowhammer DRAM Module A Simple Program Can Induce Many Errors Y X CPU Download from: https://github.com/CMU-SAFARI/rowhammer DRAM Module A Simple Program Can Induce Many Errors Y X1. Avoid cache hits – Flush X from cache 2. Avoid row hits to X – Read Y in another row CPU loop: mov (X), %eax mov (Y), %ebx clflush (X) clflush (Y) mfence jmp loop Download from: https://github.com/CMU-SAFARI/rowhammer DRAM Module A Simple Program Can Induce Many Errors Y X CPU loop: mov (X), %eax mov (Y), %ebx clflush (X) clflush (Y) mfence jmp loop Download from: https://github.com/CMU-SAFARI/rowhammer DRAM Module A Simple Program Can Induce Many Errors Y X CPU loop: mov (X), %eax mov (Y), %ebx clflush (X) clflush (Y) mfence jmp loop Y X Download from: https://github.com/CMU-SAFARI/rowhammer DRAM Module A Simple Program Can Induce Many Errors A real reliability & security issue CPU Architecture Errors Access-Rate Intel Haswell (2013) 22.9K 12.3M/sec Intel Ivy Bridge (2012) 20.7K 11.7M/sec Intel Sandy Bridge (2011) 16.1K 11.6M/sec AMD Piledriver (2012) 59 6.1M/sec 87Kim+, “Flipping Bits in Memory Without Accessing Them: An Experimental Study of DRAM Disturbance Errors,” ISCA 2014. Observed Errors in Real SystemsOne Can Take Over an Otherwise-Secure System 88 Exploiting the DRAM rowhammer bug to gain kernel privileges (Seaborn, 2015) Flipping Bits in Memory Without Accessing Them: An Experimental Study of DRAM Disturbance Errors (Kim et al., ISCA 2014) Google’s Original RowHammer Attack The following slides are from Mark Seaborn and Thomas Dullien’s BlackHat 2015 talk https://www.blackhat.com/docs/us-15/materials/us-15-Seaborn-Exploiting-The-DRAM-Rowhammer-Bug-To-Gain-Kernel-Privileges.pdf 90 This slide is from Mark Seaborn and Thomas Dullien’s BlackHat 2015 talk https://www.blackhat.com/docs/us-15/materials/us-15-Seaborn-Exploiting-The-DRAM-Rowhammer-Bug-To-Gain-Kernel-Privileges.pdf 91 This slide is from Mark Seaborn and Thomas Dullien’s BlackHat 2015 talk https://www.blackhat.com/docs/us-15/materials/us-15-Seaborn-Exploiting-The-DRAM-Rowhammer-Bug-To-Gain-Kernel-Privileges.pdf 92 This slide is from Mark Seaborn and Thomas Dullien’s BlackHat 2015 talk https://www.blackhat.com/docs/us-15/materials/us-15-Seaborn-Exploiting-The-DRAM-Rowhammer-Bug-To-Gain-Kernel-Privileges.pdf 93 This slide is from Mark Seaborn and Thomas Dullien’s BlackHat 2015 talk https://www.blackhat.com/docs/us-15/materials/us-15-Seaborn-Exploiting-The-DRAM-Rowhammer-Bug-To-Gain-Kernel-Privileges.pdf 94 This slide is from Mark Seaborn and Thomas Dullien’s BlackHat 2015 talk https://www.blackhat.com/docs/us-15/materials/us-15-Seaborn-Exploiting-The-DRAM-Rowhammer-Bug-To-Gain-Kernel-Privileges.pdf 95 This slide is from Mark Seaborn and Thomas Dullien’s BlackHat 2015 talk https://www.blackhat.com/docs/us-15/materials/us-15-Seaborn-Exploiting-The-DRAM-Rowhammer-Bug-To-Gain-Kernel-Privileges.pdf 96 This slide is from Mark Seaborn and Thomas Dullien’s BlackHat 2015 talk https://www.blackhat.com/docs/us-15/materials/us-15-Seaborn-Exploiting-The-DRAM-Rowhammer-Bug-To-Gain-Kernel-Privileges.pdf 97 This slide is from Mark Seaborn and Thomas Dullien’s BlackHat 2015 talk https://www.blackhat.com/docs/us-15/materials/us-15-Seaborn-Exploiting-The-DRAM-Rowhammer-Bug-To-Gain-Kernel-Privileges.pdf 98 This slide is from Mark Seaborn and Thomas Dullien’s BlackHat 2015 talk https://www.blackhat.com/docs/us-15/materials/us-15-Seaborn-Exploiting-The-DRAM-Rowhammer-Bug-To-Gain-Kernel-Privileges.pdf 99 This slide is from Mark Seaborn and Thomas Dullien’s BlackHat 2015 talk https://www.blackhat.com/docs/us-15/materials/us-15-Seaborn-Exploiting-The-DRAM-Rowhammer-Bug-To-Gain-Kernel-Privileges.pdf 100 This slide is from Mark Seaborn and Thomas Dullien’s BlackHat 2015 talk https://www.blackhat.com/docs/us-15/materials/us-15-Seaborn-Exploiting-The-DRAM-Rowhammer-Bug-To-Gain-Kernel-Privileges.pdf 101 This slide is from Mark Seaborn and Thomas Dullien’s BlackHat 2015 talk https://www.blackhat.com/docs/us-15/materials/us-15-Seaborn-Exploiting-The-DRAM-Rowhammer-Bug-To-Gain-Kernel-Privileges.pdf 102 This slide is from Mark Seaborn and Thomas Dullien’s BlackHat 2015 talk https://www.blackhat.com/docs/us-15/materials/us-15-Seaborn-Exploiting-The-DRAM-Rowhammer-Bug-To-Gain-Kernel-Privileges.pdf 103 This slide is from Mark Seaborn and Thomas Dullien’s BlackHat 2015 talk https://www.blackhat.com/docs/us-15/materials/us-15-Seaborn-Exploiting-The-DRAM-Rowhammer-Bug-To-Gain-Kernel-Privileges.pdf 104 This slide is from Mark Seaborn and Thomas Dullien’s BlackHat 2015 talk https://www.blackhat.com/docs/us-15/materials/us-15-Seaborn-Exploiting-The-DRAM-Rowhammer-Bug-To-Gain-Kernel-Privileges.pdf Security Implications 105 Security Implications 106 More Security Implications (I) 107 Source: https://lab.dsst.io/32c3-slides/7197.html Rowhammer.js: A Remote Software-Induced Fault Attack in JavaScript (DIMVA’16) “We can gain unrestricted access to systems of website visitors.” Takeaway and Food for Thought n If hardware is unreliable, higher-level security and protection mechanisms (as in virtual memory) may be compromised n The root of security and trust is at the very low levels… q in the hardware itself q RowHammer, Spectre, Meltdown are recent key examples… n What should we assume the hardware provides? n How do we keep hardware reliable? n How do we design secure hardware? n How do we design secure hardware with high performance, high energy efficiency, low cost, convenient programming? 108 Plenty of exciting and highly-relevant research questions Some Issues in Virtual Memory Three Major Issues in Virtual Memory 1. How large is the page table and how do we store and access it? 2. How can we speed up translation & access control check? 3. When do we do the translation in relation to cache access? n There are many other issues we will not cover in detail q What happens on a context switch? q How can you handle multiple page sizes? q … 110 Virtual Memory Issue I n How large is the page table? n Where do we store it? q In hardware? q In physical memory? (Where is the PTBR?) q In virtual memory? (Where is the PTBR?) n How can we store it efficiently without requiring physical memory that can store all page tables? q Idea: multi-level page tables q Only the first-level page table has to be in physical memory q Remaining levels are in virtual memory (but get cached in physical memory when accessed) 111 Recall: Solution: Multi-Level Page Tables 112 Example from the x86 architecture Page Table Access n How do we access the Page Table? n Page Table Base Register (CR3 in x86) n Page Table Limit Register n If VPN is out of the bounds (exceeds PTLR) then the process did not allocate the virtual page à access control exception n Page Table Base Register is part of a process’s context q Just like PC, status registers, general purpose registers q Needs to be loaded when the process is context-switched in 113 More on x86 Page Tables (I): Small Pages 114 More on x86 Page Tables (II): Large Pages 115 x86 Page Table Entries 116 x86 PTE (4KB page) 117 x86 Page Directory Entry (PDE) 118 X86-64 Page Table Entry Structure 119Intel® 64 and IA-32 Architectures Software Developer’s Manual Volume 3A: System Programming Guide, Part 1 X86-64 Page Table: Accessing 4KB pages 120Intel® 64 and IA-32 Architectures Software Developer’s Manual Volume 3A: System Programming Guide, Part 1 X86-64 Page Table: Accessing 2MB pages 121Intel® 64 and IA-32 Architectures Software Developer’s Manual Volume 3A: System Programming Guide, Part 1 X86-64 Page Table: Accessing 1GB pages 122Intel® 64 and IA-32 Architectures Software Developer’s Manual Volume 3A: System Programming Guide, Part 1 Three Major Issues in Virtual Memory 1. How large is the page table and how do we store and access it? 2. How can we speed up translation & access control check? 3. When do we do the translation in relation to cache access? n There are many other issues we will not cover in detail q What happens on a context switch? q How can you handle multiple page sizes? q … 123 Recall: Translation Lookaside Buffer (TLB) n Idea: Cache the Page Table Entries (PTEs) in a hardware structure in the processor to speed up address translation n Translation lookaside buffer (TLB) q Small cache of most recently used Page Table Entries, i.e., recently used Virtual-to-Physical translations q Reduces the number of memory accesses required for most instruction fetches and loads/stores to only one TLB access 124 Virtual Memory Issue II n How fast is the address translation? q How can we make it fast? n Idea: Use a hardware structure that caches PTEs à Translation Lookaside Buffer (TLB) n What should be done on a TLB miss? q What TLB entry to replace? q Who handles the TLB miss? HW vs. SW? n What should be done on a page fault? q What virtual page to replace from physical memory? q Who handles the page fault? HW vs. SW? 125126 Speeding up Translation with a TLB n A cache of address translations q Avoids accessing the page table on every memory access n Index = lower bits of VPN (virtual page #) n Tag = unused bits of VPN + process ID n Data = a page-table entry n Status = valid, dirty The usual cache design choices (placement, replacement policy, multi-level, etc.) apply here too. Handling TLB Misses n The TLB is small; it cannot hold all PTEs q Some translation requests will inevitably miss in the TLB q Must access memory to find the required PTE n Called walking the page table n Large performance penalty n Better TLB management & prefetching can reduce TLB misses n Who handles TLB misses? q Hardware or software? Handling TLB Misses (II) n Approach #1. Hardware-Managed (e.g., x86) q The hardware does the page walk q The hardware fetches the PTE and inserts it into the TLB n If the TLB is full, the entry replaces another entry q Done transparently to system software q Can employ specialized structures and caches n E.g., page walkers and page walk caches n Approach #2. Software-Managed (e.g., MIPS) q The hardware raises an exception q The operating system does the page walk q The operating system fetches the PTE q The operating system inserts/evicts entries in the TLB Handling TLB Misses (III) n Hardware-Managed TLB + No exception on TLB miss. Instruction just stalls + Independent instructions may execute and help tolerate latency + No extra instructions/data brought into caches -- Page directory/table organization is fetched into the system: OS has little flexibility in deciding these n Software-Managed TLB + The OS can define the page table oganization + More sophisticated TLB replacement policies are possible -- Need to generate an exception à performance overhead due to pipeline flush, exception handler execution, extra instructions brought to caches Three Major Issues in Virtual Memory 1. How large is the page table and how do we store and access it? 2. How can we speed up translation & access control check? 3. When do we do the translation in relation to cache access? n There are many other issues we will not cover in detail q What happens on a context switch? q How can you handle multiple page sizes? q … 130 Teaser: Virtual Memory Issue III n When do we do the address translation? q Before or after accessing the L1 cache? 131 Address Translation and Caching n When do we do the address translation? q Before or after accessing the L1 cache? n In other words, is the cache virtually addressed or physically addressed? q Virtual versus physical cache n What are the issues with a virtually addressed cache? n Synonym problem: q Two different virtual addresses can map to the same physical address à same physical address can be present in multiple locations in the cache à can lead to inconsistency in data 132 Homonyms and Synonyms n Homonym: Same VA can map to two different PAs q Why? n VA is in different processes n Synonym: Different VAs can map to the same PA q Why? n Different pages can share the same physical frame within or across processes n Reasons: shared libraries, shared data, copy-on-write pages within the same process, … n Do homonyms and synonyms create problems when we have a cache? q Is the cache virtually or physically addressed? 133 Cache-VM Interaction CPU TLB cache lower hier. physical cache CPU cache tlb lower hier. virtual (L1) cache VA PA CPU cache tlb lower hier. virtual-physical cache VA PA VA PA See backup slides for more A Modern Example Virtual Memory System Evolution of Address Translation 136 Simple Address Translation Modern Address Translation L1 Data TLB L1 Instruction TLB L1 Data Cache L1 Data TLB L1 ITLB PTW Cache L2 TLB PTW Walker L1 Data CacheSoftware Page Table Walker Memory Management Unit n The Memory Management Unit (MMU) is responsible for resolving address translation requests q One MMU per core (usually) n MMU typically has three key components: q Translation Lookaside Buffers that cache recently-used virtual-to-physical translations (PTEs) q Page Table Walk Caches that offer fast access to the intermediate levels of a multi-level page table q Hardware Page Table Walker that sequentially accesses the different levels of the Page Table to fetch the required PTE 137 Intel Skylake: MMU 138 L1 Instruction TLB L1 Data TLB L2 Unified TLB Hardware Page Table Walker Page Walk Caches https://www.7-cpu.com/cpu/Skylake.html Intel Skylake: L1 Data TLB 139 L1 Data TLB Intel Skylake: L1 Data TLB n Separate L1 Data TLB structures for 4KB, 2MB, and 1GB pages n L1 DTLB q 4KB: 64-entry, 4-way, 1 cycle access, 9 cycle miss q 2MB: 32-entry, 4-way, 1 cycle access, 9 cycle miss q 1GB: 4 entry, fully-associative 140 n Virtual-to-physical mappings are inserted in the corresponding TLB after a TLB miss n During a translation request, all three L1 TLBs are looked up in parallel https://www.7-cpu.com/cpu/Skylake.html L1 Data TLB: Parallel Lookup Example 141 L1 4KB TLB L1 2MB TLB L1 1GB TLB Set 0 Set 1 Set 2 Set 3 Set 0 Set 1 Set 0 Set 1 001010100100101000000000011100000001 Virtual Address 31th bit to index 1GB 22th bit to index 2MB 13-14th bit to index 4KB Intel Skylake: L2 Unified I/D TLB 142 L2 Unified TLB Intel Skylake: L2 Unified TLB 143 n L2 Unified TLB caches translations for both instr. and data q private per individual core n 2 separate L2 TLB structures for 4KB/2MB and 1GB pages n L2 TLB q 4KB/2MB: 1536-entry, 12-way, 14 cycle access, 9 cycle miss q 1GB: 16-entry, 4-way, 1 cycle access, 9 cycle miss penalty n Challenge: How can the L2 TLB support both 4KB and 2MB pages using a single structure? (Not enough publicly available information for Intel Skylake) https://www.7-cpu.com/cpu/Skylake.html L2 Unified TLB: Accessing the TLB 144 n The 4KB/2MB structure of the L2 TLB is probed in 2 steps n Step 1: Assume the page size is 4KB, calculate the index bits and access the L2 TLB q If the tag matches, it is a hit. If the tag does not match, go to Step 2. n Step 2: Assume the page size is 2MB, re-calculate the index and access the L2 TLB. q If the tag matches, it is a hit. If the tag does not match, it is an L2 TLB miss. n General algorithm: Re-calculate index and probe TLB for all remaining page sizes Similar to “associativity in time” (also called pseudo-associativity) Step 1: Calculate Index for 4KB 145 L2 TLB Set 0 Set 1 Set 2 Set 3 001010100100101000000000011100000001 Virtual Address 13-14th bit to index 4KB Step 2: Re-calculate Index for 2MB 146 L2 TLB Set 0 Set 1 Set 2 Set 3 001010100100101000000000011100000001 Virtual Address 22th-23th bit to index 2MB L2 TLB: N-Step Index Re-Calculation n Pros: + Simple and practical implementation 147 n Cons: - Varying L2 TLB hit latency (faster for 4KB, slower for 2MB) - Slower identification of L2 TLB Miss as all page sizes need to be tested n Potential Optimizations: 1. Parallel Lookup: Look up for 4KB and 2MB pages in parallel 2. Page Size Prediction: Predict the probing order Tradeoffs are similar to “associativity in time” (also called pseudo-associativity) Hardware Page Table Walker 148 Hardware Page Table Walker Hardware Page Table Walker (I) n A per-core hardware component that walks the multi-level page table to avoid expensive context switches & SW handling n HW PTW consists of 2 components: q A state machine that is designed to be aware of the architecture’s page table structure q Registers that keep track of outstanding TLB misses 149 Hardware Page Table Walker STATE MACHINE TLB Miss Registers Hardware Page Table Walker (II) n Pros: + Avoids the need for context switch on TLB miss + Overlaps TLB misses with useful computation + Supports concurrent TLB misses 150 n Cons: - Hardware area and power overheads - Limited flexibility compared to software page table walk Hardware Page Table Walker (III) 151 n PTW accesses the CR3 register that maintains information about the physical address of the root of the page table (PML4) n PTW concatenates the content of CR3 with the first 9 bits of the virtual address Intel® 64 and IA-32 Architectures Software Developer’s Manual Volume 3A: System Programming Guide, Part 1 Hardware Page Table Walker (IV) n Hardware PTWs allow overlapping TLB misses with useful computation 152 Software PTW Hardware PTW Saved Cycles LOAD A TLB Miss Context Switch – TLB Miss Handler LOAD B TLB Hit LOAD A TLB Miss LOAD B TLB Hit Page Table Walk VPN = 1 VPN = 5 VPN = 1 VPN = 5 Page Walk Caches 153 Page Walk Caches Page Walk Caches 154 n Page Walk Caches cache translations from non-leaf levels of a multi-level page table to accelerate page table walks n Page Walk Caches are low-latency caches that provide faster access to the page table levels n compared to accessing the regular cache/memory hierarchy for every page table walk Intel Skylake: MMU 155 L1 Instruction TLB L1 Data TLB L2 Unified TLB Hardware Page Table Walker Page Walk Caches Modern Virtual Memory Designs A14 “Firestorm” (iPhone 12 Pro) Intel/AMD/ARM Decode width 8 4, 5 (Samsung M3), 5 (Cortex-X1) ROB size 630 352 (Intel Willow Cove) Load/store queue size ~148 outstanding loads ~106 outstanding stores Intel Sunny Cove (128-LQ, 72-SQ) AMD Zen3 (64-LQ, 44-SQ) L1-TLB 256 entries 64 entries L2-TLB 3072 entries 1536 entries Page size 16KB 4KB L1-I cache 192KB 48KB (Intel Ice Lake) L1-D cache 128KB, 3-cycles 32KB (Intel/AMD), 4-cycles L2 cache 8MB shared across two big-cores, ~16-cycles 1MB (Intel Cascade Lake) L3 cache 16MB shared across all CPU cores and integrated GPU 1.375 MB/core 156https://www.anandtech.com/show/16226/apple-silicon-m1-a14-deep-dive/2 https://news.ycombinator.com/item?id=25257932 Virtual Memory Summary Virtual Memory Summary n Virtual memory gives the illusion of “infinite” capacity n A subset of virtual pages are located in physical memory n A page table maps virtual pages to physical pages – this is called address translation n A TLB speeds up address translation n Multi-level page tables keep the page table size in check n Using different page tables for different programs provides memory protection 158 There is More… We Will Not Cover… n How to handle virtualized systems? q Virtual machines running programs q Hypervisors n Alternative page table structures q Hashed page tables q Inverted page tables q … n … 159 Virtual Memory in Virtualized Environments n Virtualized environments (e.g., Virtual Machines) need to have an additional level of address translation 160 Guest - OS Host - OS CPU Guest Virtual Guest-Physical / Host-Virtual Host Physical Virtual Memory: Parting Thoughts n VM is one of the most successful examples of q architectural support for programmers q how to partition work between hardware and software q hardware/software cooperation q programmer/architect tradeoff n Going forward: How does virtual memory scale into the future? Four key trends: q Increasing, huge physical memory sizes (local & remote) q Hybrid physical memory systems (DRAM + NVM + SSD) q Many accelerators in the system addressing physical memory q Virtualized systems (hypervisors, software virtualization, local and remote memories) 161 Rethinking Virtual Memory n Nastaran Hajinazar, Pratyush Patel, Minesh Patel, Konstantinos Kanellopoulos, Saugata Ghose, Rachata Ausavarungnirun, Geraldo Francisco de Oliveira Jr., Jonathan Appavoo, Vivek Seshadri, and Onur Mutlu, \"The Virtual Block Interface: A Flexible Alternative to the Conventional Virtual Memory Framework\" Proceedings of the 47th International Symposium on Computer Architecture (ISCA), Virtual, June 2020. [Slides (pptx) (pdf)] [Lightning Talk Slides (pptx) (pdf)] [ARM Research Summit Poster (pptx) (pdf)] [Talk Video (26 minutes)] [Lightning Talk Video (3 minutes)] [Lecture Video (43 minutes)] 162 Rethinking Virtual Memory (I) 163 Konstantinos Kanellopoulos, Hong Chul Nam, F. Nisa Bostanci, Rahul Bera, Mohammad Sadrosadati, Rakesh Kumar, Davide Basilio Bartolini, and Onur Mutlu, \"Victima: Drastically Increasing Address Translation Reach by Leveraging Underutilized Cache Resources\" Proceedings of the 56th International Symposium on Microarchitecture (MICRO), Toronto, ON, Canada, November 2023. [Slides (pptx) (pdf)] [arXiv version] [Victima Source Code (Officially Artifact Evaluated with All Badges)] Officially artifact evaluated as available, functional, reusable and reproducible. Distinguished artifact award at MICRO 2023 Rethinking Virtual Memory (II) 164 Konstantinos Kanellopoulos, Rahul Bera, Kosta Stojiljkovic, Nisa Bostanci, Can Firtina, Rachata Ausavarungnirun, Rakesh Kumar, Nastaran Hajinazar, Mohammad Sadrosadati, Nandita Vijaykumar, and Onur Mutlu, \"Utopia: Fast and Efficient Address Translation via Hybrid Restrictive & Flexible Virtual-to-Physical Address Mappings\" Proceedings of the 56th International Symposium on Microarchitecture (MICRO), Toronto, ON, Canada, November 2023. [Slides (pptx) (pdf)] [arXiv version] [Utopia Source Code] Enabling VM Research 165 Konstantinos Kanellopoulos, Konstantinos Sgouras, Onur Mutlu ”Virtuoso: An Open-Source, Comprehensive and Modular Simulation Framework for Virtual Memory Research” [Code] Presented as poster at ACM SRC @ MICRO 2023 – 3rd place Lectures on Virtual Memory 166 https://www.youtube.com/watch?v=2RhGMpY18zw&list=PL5PHm2jkkXmi5CxxI7b3JCL1TWybTDtKq&index=22 Lectures on Virtual Memory 167https://www.youtube.com/watch?v=teb8jc3U1JY&list=PL5Q2soXY2Zi9UFWwfRtSjpKX_IRF0lPSe&index=23 Lectures on Virtual Memory 168https://www.youtube.com/watch?v=PPR7YrBi7IQ&list=PL5Q2soXY2Zi9xidyIgBxUz7xRPS-wisBN&index=24 Lectures on Virtual Memory n Computer Architecture, Spring 2015, Lecture 20 q Virtual Memory (CMU, Spring 2015) q https://www.youtube.com/watch?v=2RhGMpY18zw&list=PL5PHm2jkkXmi5CxxI7b3 JCL1TWybTDtKq&index=22 n Computer Architecture, Fall 2020, Lecture 12c q The Virtual Block Interface (ETH, Fall 2020) q https://www.youtube.com/watch?v=PPR7YrBi7IQ&list=PL5Q2soXY2Zi9xidyIgBxUz7 xRPS-wisBN&index=24 n Computer Architecture, Fall 2023, Lecture 23 q Utopia, Victima, Virtuoso (ETH, Fall 2023) q https://www.youtube.com/watch?v=teb8jc3U1JY&list=PL5Q2soXY2Zi9UFWwfRtSjp KX_IRF0lPSe&index=23 169https://www.youtube.com/onurmutlulectures Research Opportunities Research Opportunities n If you are interested in doing research in Computer Architecture, Security, Systems & Bioinformatics: q Email Mohammad and Prof. Mutlu with your interest q Take the seminar course and the “Computer Architecture” course q Do readings and assignments on your own & talk with us n There are many exciting projects and research positions, e.g.: q Novel memory/storage/computation/communication systems q New execution paradigms (e.g., in-memory computing) q Hardware security, safety, reliability, predictability q GPUs, TPUs, FPGAs, PIM, heterogeneous systems, … q Security-architecture-reliability-energy-performance interactions q Architectures for genomics/proteomics/medical/health/AI/ML q A limited list is here: https://safari.ethz.ch/theses/ 171 https://people.inf.ethz.ch/omutlu/projects.htm Bachelor’s Seminar in Computer Architecture n Fall 2024 (offered every Fall and Spring Semester) n 2 credit units n Rigorous seminar on fundamental and cutting-edge topics in computer architecture n Critical paper presentation, review, and discussion of seminal and cutting-edge works in computer architecture q We will cover many ideas & issues, analyze their tradeoffs, perform critical thinking and brainstorming n Participation, presentation, synthesis report, lots of discussion n You can register for the course online n https://safari.ethz.ch/architecture_seminar 172 Research Opportunities n If you are interested in doing research in Computer Architecture, Security, Systems & Bioinformatics: q Email me and Prof. Mutlu with your interest q Take the seminar course and the “Computer Architecture” course q Do readings and assignments on your own & talk with us n There are many exciting projects and research positions, e.g.: q Novel memory/storage/computation/communication systems q New execution paradigms (e.g., in-memory computing) q Hardware security, safety, reliability, predictability q GPUs, TPUs, FPGAs, PIM, heterogeneous systems, … q Security-architecture-reliability-energy-performance interactions q Architectures for genomics/proteomics/medical/health/AI/ML q A limited list is here: https://safari.ethz.ch/theses/ 173 https://people.inf.ethz.ch/omutlu/projects.htm https://www.youtube.com/watch?v=mV2OuB2djEs SAFARI Introduction & Research Computer architecture, HW/SW, systems, bioinformatics, security, memory Digital Design & Computer Arch. Lecture 26: Virtual Memory Konstantinos Kanellopoulos Prof. Onur Mutlu ETH Zürich Spring 2024 31 May 2024 Backup Slides More on Issues in Virtual Memory Virtual Memory and Cache Interaction Address Translation and Caching n When do we do the address translation? q Before or after accessing the L1 cache? n In other words, is the cache virtually addressed or physically addressed? q Virtual versus physical cache n What are the issues with a virtually addressed cache? n Synonym problem: q Two different virtual addresses can map to the same physical address à same physical address can be present in multiple locations in the cache à can lead to inconsistency in data 179 Homonyms and Synonyms n Homonym: Same VA can map to two different PAs q Why? n VA is in different processes n Synonym: Different VAs can map to the same PA q Why? n Different pages can share the same physical frame within or across processes n Reasons: shared libraries, shared data, copy-on-write pages within the same process, … n Do homonyms and synonyms create problems when we have a cache? q Is the cache virtually or physically addressed? 180 Cache-VM Interaction 181 CPU TLB cache lower hier. physical cache CPU cache tlb lower hier. virtual (L1) cache VA PA CPU cache tlb lower hier. virtual-physical cache VA PA VA PA Physical Cache 182 Virtual Cache 183 Virtual-Physical Cache 184 Virtually-Indexed Physically-Tagged n If (index-bits + byte-in-block-bits < page-offset-bits), the cache index bits come only from page offset (same in VA and PA) q Also implies Cache Size ≤ (page size ´ associativity) n If both cache and TLB are on chip q index both arrays concurrently using VA bits q check cache tag (physical) against TLB output at the end 185 VPN Page Offset TLB PPN Index BiB physical cache tag data= cache hit?TLB hit? Virtually-Indexed Physically-Tagged n If (index-bits + byte-in-block-bits < page-offset-bits), the cache index bits include VPN Þ Synonyms can cause problems q The same physical address can exist in two locations n Solutions? 186 VPN Page Offset TLB PPN Index BiB physical cache tag data= cache hit?TLB hit? a Some Solutions to the Synonym Problem n Limit cache size to (page size times associativity) q get index from page offset n On a write to a block, search all possible indices that can contain the same physical block, and update/invalidate q Used in Alpha 21264, MIPS R10K n Restrict page placement in OS q make sure index(VA) = index(PA) q Called page coloring q Used in many SPARC processors 187 n 32 KB, 64B cacheline size, 8-way associative, 64 sets n Virtually-indexed physically-tagged (VIPT) n #set-index bits (6) + #byte-in-block-bits (6) = log2(Page Size) q No synonym problem n “SEESAW: Using Superpages to Improve VIPT Caches, Parasar+, ISCA’18 n https://en.wikichip.org/wiki/intel/microarchitectures/skylake_(server) n https://uops.info/cache.html n https://www.7-cpu.com/cpu/Skylake.html L1-D Cache in Intel Skylake 188 An Exercise (I) 189190 An Exercise (II) 191 An Exercise (Concluded) 192 A Potpourri of Issues Trade-Offs in Page Size n Large page size (e.g., 1GB) q Pro: Fewer PTEs required è Saves memory space q Pro: Fewer TLB misses è Improves performance q Con: Cannot have fine-grained permissions q Con: Large transfers to/from disk n Even when only 1KB is needed, 1GB must be transferred n Waste of bandwidth/energy n Reduces performance q Con: Internal fragmentation n Even when only 1KB is needed, 1GB must be allocated n Waste of space n Q: What is external fragmentation? Some System Software Tasks for VM n Keeping track of which physical frames are free n Allocating free physical frames to virtual pages n Page replacement policy q When no physical frame is free, what should be removed? n Sharing pages between processes n Copy-on-write optimization n Page-flip optimization 195 Virtual Memory in Virtualized Environments n Virtualized environments (e.g. Virtual Machines) need to have an additional level of address translation 196 Guest - OS Host - OS CPU Guest Virtual Guest-Physical / Host-Virtual Host Physical Shadow Paging n System maintains a new shadow page table which maps guest-virtual page directly to host-physical page n Guest-virtual to Guest-physical page table is read-only for the Guest OS n Pros: + Fast TLB Miss / Page Table Walk n Cons: - To maintain a consistent shadow page table, the system handles every update to Guest and Host page tables 197 Shadow Paging 198 Guest Page Table Host Page Table Shadow Page Table Guest Virtual Address Host Physical Address Guest Virtual Address sCR3 Host Physical Address 4 Memory Accesses Nested Paging n Nested paging is the widely used hardware technique to virtualize memory in modern systems n Two-dimensional hardware page-table walk: q For every level of Guest Page table n Perform a 4-level Host Page table walk 199 n Pros: + Easy for the system to maintain/update two page tables n Cons: - TLB Misses are more costly (up to 24 memory accesses) Nested Paging 200 Guest Page Table Host Page Table Guest Physical Address Guest Virtual Address Host Physical Address Guest Virtual Address gCR3 Host Physical Address gPA gPA gPA gPA gPA 5 + 5 + 5 + 5 + 4 = 24 Memory Accesses","libVersion":"0.3.2","langs":""}