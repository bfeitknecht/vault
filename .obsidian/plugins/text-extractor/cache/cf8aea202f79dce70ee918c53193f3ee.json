{"path":"sem4/DMDB/VRL/extra/slides/DMDB-s10-analytics-1-frequent-itemsets.pdf","text":"Bowen Wu Data Modelling & Databases Lecture 10: Analytics I (Frequent items set and Data Cubes) bowen.wu@inf.ethz.ch PersNr LectureID Grade 26120 5001 5.0 27550 5001 5.5 26120 5041 5.75 One Instance: Tests PersNr Name Level Room 2125 John AP 226 2126 David FP 232 2127 Anna FP 310 Professor PersNr Name Semester 24002 Peter 8 25403 Mary 6 26120 Frey 2 Student LectureID Title CP ProfNr 5001 Databases 4 2125 5041 Networks 4 2126 5043 Theory 4 2127 Lecture Prerequisite Follow-up 5001 5041 5001 5043 Requires PersNr LectureID 26120 5001 27550 5001 26120 5041 Attends Query Q Query is a function that takes as input a DB instance, output a new relation. A Query Language consists of a set of functions that you can express in that language. A Statistical View of DB ▪ So far, we have been taking a logic-based view on DB – A DB instance forms an interpretation of some logic system. ▪ In this lecture, we are going to take a more statistical view – A DB instance contains a set of samples of the history, what can we learn about the future? ➢ Data Mining ➢ In-DB Machine Learning ▪ It is a huge area on its own, in this lecture we will scratch the surface and walk through how to use some off-the-shelf tools. ▪ The goal of this lecture is NOT to teach you machine learning – the goal is to introduce basic concepts and focus on showing you that you can do all these in a DB. PersNr Name Level Room 2125 John AP 226 2126 David FP 232 2127 Anna FP 310 Professor Overview ▪ Associated Rule Mining ▪ Data Cubes Frequent Itemsets TID Items 1 Bread, Coke, Milk 2 Beer, Bread 3 Beer, Coke, Diaper, Milk 4 Beer, Bread, Diaper, Milk 5 Coke, Diaper, Milk ▪ Example (Market basket analysis): Find the set of items that customers frequently buy together in a supermarket. A baskset An item Itemset The frequency is defined as “support”. Frequent Itemsets ▪ Problem: Find sets of items that appear together “frequently” in baskets ▪ Support for itemset I: Number of baskets containing all items in I ▪ Given a support threshold s, then sets of items that appear in at least s baskets are called frequent itemsets TID Items 1 Bread, Coke, Milk 2 Beer, Bread 3 Beer, Coke, Diaper, Milk 4 Beer, Bread, Diaper, Milk 5 Coke, Diaper, Milk Support of {Beer, Bread} = 2 Frequent Itemsets ▪ Items = {milk, coke, pepsi, beer, juice} ▪ Support threshold = 3 baskets B1 = {m, c, b} B2 = {m, p, j} B3 = {m, b} B4 = {c, j} B5 = {m, p, b} B6 = {m, c, b, j} B7 = {c, b, j} B8 = {b, c} ▪ Frequent itemsets: {m}, {c}, {b}, {j}, {m,b}, {b, c}, {c, j} ▪ Support: ➢ {m} = 5 ➢ {c} = 5 ➢ {p} = 2 ➢ {b} = 6 ➢ {j} = 4 ➢ {m, b} = 4 ➢ {b, c} = 4 ➢ {c, j} = 3 Finding Frequent Itemsets ▪ Structure of the Problem null AB AC AD AE BC BD BE CD CE DE A B C D E ABC ABD ABE ACD ACE ADE BCD BCE BDE CDE ABCD ABCE ABDE ACDE BCDE ABCDE Given d items, there are 2d possible itemsets Finding Frequent Itemsets ▪ Brute-force approach, each itemset is a candidate : ➢ Consider each itemset in the lattice, and count the support of each candidate by scanning the data ➢ Time Complexity ~ O(NMw) ➢ Expensive since M = 2d TID Items 1 Bread, Milk 2 Bread, Diaper, Beer, Eggs 3 Milk, Diaper, Beer, Coke 4 Bread, Milk, Diaper, Beer 5 Bread, Milk, Diaper, Coke N Transactions List of Candidates M w A-priori • Apriori principle (Main observation): – If an itemset is frequent, then all of its subsets must also be frequent ⇔If an itemset is not frequent, then all of its supersets cannot be frequent – The support of an itemset never exceeds the support of its subsets (anti-monotonicity) )()()(:, YsXsYXYX  A-priori Found to be frequent Frequent subsets A-priori Found to be Infrequent null AB AC AD AE BC BD BE CD CE DE A B C D E ABC ABD ABE ACD ACE ADE BCD BCE BDE CDE ABCD ABCE ABDE ACDE BCDE ABCDE null AB AC AD AE BC BD BE CD CE DE A B C D E ABC ABD ABE ACD ACE ADE BCD BCE BDE CDE ABCD ABCE ABDE ACDE BCDE ABCDEPruned Infrequent supersets A-priori Algorithm • Ck = A set of candidate itemsets of size k • C1 = {All possible singleton itemsets} • Lk = A set of frequent itemsets of size k • While Ck not empty (initially k=1) • Scan the database to find which itemsets in Ck are frequent and put them into Lk • Use Lk to generate a collection of candidate itemsets Ck+1 of size k+1 • k = k+1 A-priori Algorithm Item Count Bread 4 Coke 2 Milk 4 Beer 3 Diaper 4 Eggs 1 Itemset Count {Bread,Milk} 3 {Bread,Beer} 2 {Bread,Diaper} 3 {Milk,Beer} 2 {Milk,Diaper} 3 {Beer,Diaper} 3 Itemset Count {Bread,Milk,Diaper} 2 Items (1-itemsets) Pairs (2-itemsets) (No need to generate candidates involving Coke or Eggs) Triplets (3-itemsets) (No need to generate candidates involving {Bread,Beer}, {Milk,Beer}) min support = 3 If every subset is considered, 6 1 + 6 2 + 6 3 = 6 + 15 + 20 = 41 With support-based pruning, 6 1 + 4 2 + 1 = 6 + 6 + 1 = 13 TID Items 1 Bread, Milk 2 Bread, Diaper, Beer, Eggs 3 Milk, Diaper, Beer, Coke 4 Bread, Milk, Diaper, Beer 5 Bread, Milk, Diaper, Coke A-priori Algorithm • While Ck not empty • Scan the database to find which itemsets in Ck are frequent and put them into Lk • Use Lk to generate a collection of candidate itemsets Ck+1 of size k+1 • A-Priori Principle: An itemset of size k+1 is candidate to be frequent only if all of its subsets of size k are known to be frequent • Construct a candidate of size k+1 by combining frequent itemsets of size k • If k = 1, take the all pairs of frequent items • If k > 1, join pairs of itemsets that differ by just one item • For each generated candidate itemset ensure that all subsets of size k are frequent. • k = k+1 Ck = Set of candidate itemsets of size k Lk = Set of frequent itemsets of size k Generate Candidates Ck+1 • Assumption: The items in an itemset are ordered • The order ensures that if item y > x appears before x, then x is not in the itemset Create a candidate itemset of size k+1, by joining two itemsets of size k, that share the first k-1 items 1st Item 2nd Item 3rd Item 1 2 3 1 2 5 1 4 5 An example of L3 Each row corresponds to an itemset. Generate Candidates Ck+1 Create a candidate itemset of size k+1, by joining two itemsets of size k, that share the first k-1 items Item 1 Item 2 Item 3 1 2 3 1 2 5 1 4 5 1 2 3 5 1st Item 2nd Item 3rd Item 1 2 3 1 2 5 1 4 5 1 2 4 5 Are we missing something? What about this candidate? 1st Item 2nd Item 3rd Item 1 2 3 1 2 5 1 4 5 Generating Candidates Ck+1 in SQL • self-join Lk insert into Ck+1 select p.item1, p.item2, ..., p.itemk, q.itemk from Lk p, Lk q where p.item1=q.item1, ..., p.itemk-1=q.itemk-1, p.itemk < q.itemk • L3={abc, abd, acd, ace, bcd} • Self-join: 𝐿3 ⋈ 𝐿3 – abcd from abc and abd – acde from acd and ace item1 item2 item3 a b c a b d a c d a c e b c d item1 item2 item3 item4 a b c d a c d e Ck = Set/Table of candidate itemsets of size k Lk = Set/Table of frequent itemsets of size k Association Rules ▪ Association Rules: If-then rules about the contents of baskets ▪ {i1, i2,…,ik} → j means: “if a basket contains all of i1,…,ik then it is likely to contain j” ▪ In practice there are many rules, want to find significant/interesting ones! ▪ Confidence of this association rule is the probability of j given I = {i1,…,ik} )support( )support( )conf( I jI jI  =→ Association Rules ▪ Not all high-confidence rules are interesting ▪ The rule X → milk may have high confidence for many itemsets X, because milk is just purchased very often (independent of X) and the confidence will be high ▪ Interest of an association rule I → j: difference between its confidence and the fraction of baskets that contain j ▪ Interesting rules are those with high positive or negative interest values (usually above 0.5) ]Pr[)conf()Interest( jjIjI −→=→ Association Rules B1 = {m, c, b} B2 = {m, p, j} B3 = {m, b} B4= {c, j} B5 = {m, p, b} B6 = {m, c, b, j} B7 = {c, b, j} B8 = {b, c} • Association rule: {m, b} →c • Confidence = 2/4 = 0.5 • Interest = |0.5 – 5/8| = 1/8 • Item c appears in 5/8 of the baskets • Rule is not very interesting! Problem ▪ Problem: Find all association rules with support ≥s and confidence ≥c ➢ Note: Support of an association rule is the support of the set of items on the left side ▪ Hard part: Finding the frequent itemsets! ➢ If {i1, i2,…, ik} → j has high support and confidence, then both {i1, i2,…, ik} and {i1, i2,…,ik, j} will be “frequent” Mining Association Rules ▪ Step 1: Find all frequent itemsets I ➢ (we have solved this already!) ▪ Step 2: Rule generation ➢ For every subset A of I, generate a rule A → I \\ A • Since I is frequent, A is also frequent (Apriori property) • Variant 1: Single pass to compute the rule confidence • confidence(A,B→C,D) = support(A,B,C,D) / support(A,B) • Variant 2: • Observation: If A,B,C→D is below confidence, so is A,B→C,D ⇔ If A,B→C,D is above confidence, then so is A,B,C→D • Can generate “bigger” rules from smaller ones! ➢ Output the rules above the confidence threshold Mining Association Rules B1 = {m, c, b} B2 = {m, p, j} B3 = {m, c, b, n} B4= {c, j} B5 = {m, p, b} B6 = {m, c, b, j} B7 = {c, b, j} B8 = {b, c} ▪ Support threshold s = 3, confidence c = 0.75 ▪ 1) Frequent itemsets: ➢ {b,m} {b,c} {c,m} {c,j} {m,c,b} ▪ 2) Generate rules: ➢ b→m: c=4/6 b→c: c=5/6 b,c→m: c=3/5 ➢ m→b: c=4/5 … b,m→c: c=3/4 ➢ b→c,m: c=3/6 Example in Real DB http://madlib.apache.org/docs/latest/group__grp__assoc__rules. html Overview ▪ Associated Rule Mining ▪ Data Cubes Data Cubes and OLAP ▪ The previous algorithms deal with a variety of ways to extract information from data ▪ They try to find information provided by groups of data (itemsets, clusters, nearest neighbors, etc.) ▪ An intermediate step between those more ML algorithms and conventional query processing are data cubes ▪ Data cubes are part of analytical databases, are manipulated through extensions of SQL (and other languages), and are widely used in a wide range of applications as the source of data for reports or for further analysis An example of a data cube Dimensions: Region, product, day Fact: Sales Region Day Product Sales R1 D2 P3 105000 R2 D1 P2 27000 R2 D3 P4 43500 CUBE SELECT Country, Region, SUM(Sales) AS TotalSales FROM Sales GROUP BY CUBE (Country, Region); https://learn.microsoft.com/en-us/sql/t-sql/queries/select-group-by- transact-sql?view=sql-server-ver16 The CUBE operator ▪ The CUBE operator performs several aggregations: SELECT A, B, AGGREGATE(C) FROM table_name GROUP BY CUBE (A , B); ▪ For all combinations (A,B) ▪ For all values of A ▪ For all values of B ▪ The total ▪ The set of aggregations performed is 2n with n the number of columns involved ▪ For 3 attributes: (A,B,C), (A,B), (A,C), (B,C), (A), (B), (C), total ROLLUP SELECT Country, Region, SUM(Sales) AS TotalSales FROM Sales GROUP BY ROLLUP (Country, Region); https://learn.microsoft.com/en-us/sql/t-sql/queries/select-group-by-transact-sql?view=sql-server-ver16 The ROLLUP operator ▪ The ROLLUP operator performs several aggregations along a hierarchy defined in the operator SELECT A, B, AGGREGATE(C) FROM table_name GROUP BY ROLLUP (A , B); ▪ For all combinations (A,B) ▪ For all values of A ▪ The total ▪ Aggregates by (A,B), then by (A), and then the total ▪ With 3 attributes: (A,B,C), (A,B), (A), total ROLLUP and GROUP BY SELECT Time, Region, Department, sum(Profit) AS Profit FROM sales GROUP BY ROLLUP(Time, Region, Dept) SELECT Time, Region, Department, SUM(Profit) FROM Sales GROUP BY Time, Region, Department UNION ALL SELECT Time, Region, '' , SUM(Profit) FROM Sales GROUP BY Time, Region UNION ALL SELECT Time, '', '', SUM(Profits) FROM Sales GROUP BY Time UNION ALL SELECT '', '', '', SUM(Profits) FROM Sales; https://docs.oracle.com/cd/F49540_01/DOC/server.815/a68003/rollup_c.htm CORRELATION SELECT weight_class, CORR(list_price, min_price) \"Correlation\" FROM product_information GROUP BY weight_class ORDER BY weight_class, \"Correlation\"; WEIGHT_CLASS Correlation ------------ ----------- 1 .999149795 2 .999022941 3 .998484472 4 .999359909 5 .999536087 Pearsons correlation (-1,1), if 1 or -1, there is a linear equation capturing the relation between the values of X and Y https://docs.oracle.com/en/database/oracle/oracle-database/19/sqlrf/CORR.html#GUID-E73AF5E2- 38A4-436A-955C-5122C079F49C COVARIANCE SELECT job_id, COVAR_POP(SYSDATE-hire_date, salary) AS covar_pop, COVAR_SAMP(SYSDATE-hire_date, salary) AS covar_samp FROM employees WHERE department_id in (50, 80) GROUP BY job_id ORDER BY job_id, covar_pop, covar_samp; JOB_ID COVAR_POP COVAR_SAMP ---------- ----------- ----------- SA_MAN 660700 825875 SA_REP 579988.466 600702.34 SH_CLERK 212432.5 223613.158 ST_CLERK 176577.25 185870.789 ST_MAN 436092 545115 https://docs.oracle.com/en/database/oracle/oracle-database/19/sqlrf/COVAR_POP.html#GUID- D728D05F-D2E3-405C-986F-088B8353553A RANK SELECT RANK(15500, .05) WITHIN GROUP (ORDER BY salary, commission_pct) \"Rank\" FROM employees; Rank ---------- 105 SELECT RANK(15500) WITHIN GROUP (ORDER BY salary DESC) \"Rank of 15500\" FROM employees; Rank of 15500 -------------- 4 Returns the position of a tuple in an ordered set https://docs.oracle.com/en/database/oracle/oracle- database/19/sqlrf/RANK.html#GUID-0950BD34-C994-41DA- A8F9-34B3FE53BBBA CUMULATIVE DISTRIBUTION SELECT CUME_DIST(15500, .05) WITHIN GROUP (ORDER BY salary, commission_pct) \"Cume-Dist of 15500\" FROM employees; Cume-Dist of 15500 ------------------ .972222222 SELECT job_id, last_name, salary, CUME_DIST() OVER (PARTITION BY job_id ORDER BY salary) AS cume_dist FROM employees WHERE job_id LIKE 'PU%' ORDER BY job_id, last_name, salary, cume_dist; JOB_ID LAST_NAME SALARY CUME_DIST ---------- ------------------------- ---------- ---------- PU_CLERK Baida 2900 .8 PU_CLERK Colmenares 2500 .2 PU_CLERK Himuro 2600 .4 PU_CLERK Khoo 3100 1 PU_CLERK Tobias 2800 .6 PU_MAN Raphaely 11000 1 Computes the percentage of values below a given value","libVersion":"0.5.0","langs":""}