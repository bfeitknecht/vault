{"path":"sem5/A2/VRL/extra/A2-script.pdf","text":"Analysis II for INFK E. Kowalski ETH Z¨urich – Fall 2019 Version of February 19, 2023 kowalski@math.ethz.ch Contents Chapter 1. Preliminaries 1 Chapter 2. Ordinary diﬀerential equations 2 2.1. Introduction 2 2.2. Linear diﬀerential equations 4 2.3. Linear diﬀerential equations of order 1 6 2.4. Linear diﬀerential equations with constant coeﬃcients 8 2.5. An example: the harmonic oscillator 14 2.6. Other methods 15 Chapter 3. Diﬀerential calculus in Rn 17 3.1. Introduction 17 3.2. Continuity in Rn 19 3.3. Partial derivatives 24 3.4. The diﬀerential 27 3.5. Higher derivatives 37 3.6. Change of variable 40 3.7. Taylor polynomials 43 3.8. Critical points 46 3.9. Lagrange multipliers 51 3.10. The inverse and implicit functions theorems 55 Chapter 4. Integration in R n 59 4.1. Line integrals 59 4.2. The Riemann integral in Rn 69 4.3. Improper integrals 77 4.4. The change of variable formula 78 4.5. Geometric applications of integrals 83 4.6. The Green formula 85 4.7. The Gauss–Ostrogradski formula 89 Bibliography 94 ii CHAPTER 1 Preliminaries We assume knowledge of the contents of Analysis I (for instance, properties of real numbers, sequences and series, continuous functions on intervals, diﬀerentiable functions on intervals, Riemann integral, improper integrals). These can be found in the script [1] of Prof. M. Burger. We denote by N “ t0, 1, 2, . . .u the set of all natural numbers, by Q the rational numbers, by R the real numbers and by C the complex numbers. In addition to continuous and diﬀerentiable functions deﬁned on intervals with values in R, as in [1, Kap. III, IV], we will also consider functions f : I Ñ Rd, where d ě 2 and I is an interval. This means that f pxq “ pf1pxq, . . . , fdpxqq for some functions fi : I Ñ R. We then say: ‚ That f is continuous (on I, or at a point x0 P I) if each coordinate function fi is continuous (on I or at x0); ‚ That f is diﬀerentiable (on I, or at a point x0 P I) if each coordinate function fi is diﬀerentiable (on I or at x0), in which case we write f 1px0q “ pf 1 1px0q, . . . , f 1 dpx0qq. A primitive F of a continuous function f : I Ñ Rd is a diﬀerentiable function F : I Ñ Rd such that F 1 “ f . A primitive always exists, for instance F pxq “ ´ż x x0 f1ptqdt, . . . , ż x x0 fdptqdt ¯ when writing f “ pf1, . . . , fdq as before. Acknowledgments. Thanks to T. Rieder for sending a number of corrections. 1 CHAPTER 2 Ordinary diﬀerential equations 2.1. Introduction A diﬀerential equation is an equation where the unknown (or unknowns) is a function f , and the equation relates values of f at a point x with values of derivatives of the function at the same point x. If the function has one variable only (as is the case in this chapter), one speaks of ordinary diﬀerential equations.1 Example 2.1.1. (1) The exponential function f pxq “ ex, deﬁned for x P R, satisﬁes f 1pxq “ f pxq for all x P R. One says that this function is a solution (on R) of the diﬀerential equation y1 “ y. This is not the only solution: in fact, for any constant a P R, the function fapxq “ ae x also satisﬁes f 1 apxq “ fapxq for all x P R. Later, we will see that there are no other solutions. (2) In the mechanics of Newton, the movement of a particle P with mass m ą 0, given by its position f ptq “ pxptq, yptq, zptqq P R3 for all times t is determined by the equation mf 2ptq “ sum of forces acting on P at time t, and by the “initial condition”, which means the speciﬁcation of the position f p0q and speed f 1p0q at some starting time t0. Note that the forces acting on the particle at time t are expressions involving f ptq (position) and f 1ptq (speed), at the same time t. Also, the solution is unique because of the initial conditions (otherwise, as in Example (1), there would be inﬁnitely many solutions). Since f is a function with one variable t but with values in R3, we recall again that the derivatives of f are simply taken for each coordinate separately f 1ptq “ px1ptq, y1ptq, z1ptqq, f 2ptq “ px2ptq, y2ptq, z2ptqq. For example, a particle subject to no external force satisﬁes the equation mf 2ptq “ 0, so that f 2ptq “ 0 for all t, which means that the motion is a straight line (each of the coordinates is of the form xptq “ a0t ` b0, yptq “ a1t ` b1, zptq “ a2t ` b2). Classical newtonian mechanics (and its solutions) forms a basic tool to simulate phys- ical behavior of objects in applications (such as computer games, computer generated videos, etc). (3) For any given continuous function a, the diﬀerential equation f 1 ´ a “ 0 has a solution, namely any primitive of the function a. The existence of the solutions follows from the Fundamental Theorem of Calculus ([1, §5.4]): we may deﬁne f pxq “ ż x x0 aptqdt. In general, diﬀerential equations are closely related with integration theory. (4) An equation like f 1px ` 1q ´ f pxq “ 0 is not an ordinary diﬀerential equation, because it relates the value of f at the point x with the derivative at another point. 1When there is more than one variable, one speaks of partial diﬀerential equations, referring to partial derivatives in multi-variable calculus (see Chapter 3). 2 Remark 2.1.2. In computer science, besides simulations of physical systems, diﬀer- ential equations arise frequently in the analysis of algorithms, for instance the running time of certain algorithms might be a solution of a diﬀerential equation, or might be approximated by such a solution. It is customary to write down a diﬀerential equation without writing the evaluation f ptq or f pxq but only the function’s name (or its derivatives), and to use the letter x or t for the variable when it appears elsewhere in the equation. In physics, the time variable t plays an important role, and derivation with respect to time is often denoted by a dot: 9y instead of y1, and :y instead of y2. When one needs to specify initial conditions (values of the unknown function at some ﬁxed value that specify the solution uniquely), one writes for instance yp0q “ a, y1p0q “ b to say that the function f should satisfy f p0q “ a and f 1p0q “ b. Example 2.1.3. The important function f pxq “ e´x2, for x P R, satisﬁes the diﬀer- ential equation y1 “ ´2xy, i.e., for every x P R, we have f 1pxq “ ´2xe ´x2 “ ´2xf pxq. In a physics context, where the variable is understood as time, this might be written 9y “ ´2ty. Ordinary diﬀerential equations are classiﬁed according to their order, which is the highest derivative that appears in the equation. So Newton’s equations are of order 2 (because forces are expressed in terms of y and y1, and acceleration involves y2). There is a trick to reduce any ordinary diﬀerential equation of order k ě 2 to an equation of order 1, but for a function that takes values in a higher-dimensional space (keeping however a single variable). We illustrate it with an example. Example 2.1.4. The diﬀerential equation (2.1) y2 “ xpx ` 1qy1 ´ 3y, with unknown a diﬀerentiable function f : R Ñ R, can be transformed into the equation (2.2) Y 1 “ ˆ 0 1 ´3 xpx ` 1q ˙ Y with unknown a diﬀerentiable function F : R Ñ R2, where the right-hand side is a matrix product. Indeed, if F is a solution of this equation and we write F pxq “ ˆf0pxq f1pxq ˙, then the equation for F means that ˆf 1 0pxq f 1 1pxq ˙ “ ˆ 0 1 ´3 xpx ` 1q ˙ ˆ f0pxq f1pxq ˙ “ ˆ f1pxq ´3f0pxq ` xpx ` 1qf1pxq ˙ for all x. So we have f1 “ f 1 0, and therefore F pxq “ ˆf0pxq f 1 0pxq ˙, where the function f0 : R Ñ R satisﬁes (second row of the equation) the ordinary diﬀerential equation f 2 0 pxq “ f 1 1pxq “ ´3f0pxq ` xpx ` 1qf 1 0pxq for all x. Conversely, given a solution f of (2.1), putting F pxq “ ˆ f pxq f 1pxq ˙ gives a solution of (2.2). This trick explains why many general results are stated for equations of order 1. On the other hand, the solution of speciﬁc equations of order 2 or higher might be easier without using this trick. 3 Although it is “physically” clear that Newton’s equation have solutions, it is not at all obvious that ordinary diﬀerential equations should have solutions in general. In fact, it is possible that a solution only exists “locally” around an initial point. Example 2.1.5. Consider the equation 2yy1 “ 1 on R with the initial condition yp0q “ 1. Writing the left-hand side as py2q 1, we see that y2 satisﬁes y2 “ x ` a for some constant a P R, and we have a “ 1 because of the initial condition. Hence the solution is f pxq “ ? x ` 1. But although the equation can be asked for all x P R, here the solution only makes sense for x ą ´1. At least one can prove that this local existence always holds, for nice enough equations of the form y1 “ F px, yq (and many others that can be brought to this form). Theorem 2.1.6. Suppose F : R2 Ñ R is a diﬀerentiable function of two variables (see Chapter 3). Let x0 P R and y0 P R2. Then the ordinary diﬀerential equation y1 “ F px, yq has a unique solution f deﬁned on a “largest” open interval I containing x0 such that f px0q “ y0. In other words, there exists I and a function f : I Ñ R such that for all x P I, we have f 1pxq “ F px, f pxqq, and one cannot ﬁnd a larger interval containing I with such a solution. As is the case for polynomial equations, it is in general impossible to write down “explicitly” the solution to such an equation. Example 2.1.7. The function F can be arbitrary, for instance F pt, uq “ u3 exppcosptu2 ´ 1qq ` 3 sinptq for the complicated diﬀerential equation y1 “ y3 exppcospxy2 ´ 1qq ` 3 sinpxq whose solutions f : I Ñ R (for some interval I) satisfy f 1pxq “ f pxq 3 exppcospxf pxq2 ´ 1qq ` 3 sinpxq “ 0, for all x P I. 2.2. Linear diﬀerential equations The simplest diﬀerential equations are the linear diﬀerential equations. Definition 2.2.1. Let I Ă R be an open interval and k ě 1 an integer. An ho- mogeneous linear ordinary diﬀerential equation of order k on I is an equation of the form ypkq ` ak´1ypk´1q ` ¨ ¨ ¨ ` a1y1 ` a0y “ 0 where the coeﬃcients a0, . . . , ak´1 are complex-valued functions on I, and the unknown is a complex-valued function from I to C that is k-times diﬀerentiable on I. An equation of the form (2.3) ypkq ` ak´1ypk´1q ` ¨ ¨ ¨ ` a1y1 ` a0y “ b, where b : I Ñ C is another function, is called an inhomogeneous linear ordinary diﬀeren- tial equation, with associated homogeneous equation the one with b “ 0. Note that if the coeﬃcients are real-valued, it is often of interest to ﬁnd only the real-valued solutions. 4 Example 2.2.2. The equations y1 “ y, y2 “ ´y, y1 ` 2xy “ 0 (which admit as particular solutions the functions exppxq, cospxq and expp´x2q, respec- tively) are linear and homogeneous. The equation y1 ´ y “ cospxq is linear and inhomogeneous. The equations 2yy1 “ 1, y1 “ y2, cospy2q “ exppx ` yq, py ` y1q 3 “ 1, y1 ´ y “ xe y are not linear. The main property of linear diﬀerential equations (explaining the adjective) is that if we write Dpf q for the left-hand side of the equation, so that Dpf q “ f pkq ` ak´1f pk´1q ` ¨ ¨ ¨ ` a1f 1 ` a0f, then the operation D is linear: for any numbers z1 and z2 and (k-times diﬀerentiable) functions f1 and f2, we have Dpz1f1 ` z2f2q “ z1Dpf1q ` z2Dpf2q. Indeed, let f “ z1f1 ` z2f2, then Dpf q “ f pkq ` ¨ ¨ ¨ ` a1f 1 ` a0f “ z1pf pkq 1 ` ak´1f pk´1q ` ¨ ¨ ¨ ` a0f1q ` z2pf pkq 2 ` ak´1f pk´1q ¨ ¨ ¨ ` a0f2q “ z1Dpf1q ` z2Dpf2q. The main theoretical results concerning linear diﬀerential equations are summarized in the following result: Theorem 2.2.3. Let I Ă R be an open interval and k ě 1 an integer, and let ypkq ` ak´1ypk´1q ` ¨ ¨ ¨ ` a1y1 ` a0y “ 0 be a linear diﬀerential equation over I with continuous coeﬃcients. (1) The set S of k-times diﬀerentiable solutions f : I Ñ C of the equation is a complex vector space which is a subspace of the space of complex-valued functions on I. (1bis) If the functions ai are real-valued, the set S of real-valued solutions is a real vector space which is a subspace of the space of real-valued functions on I. (2) The dimension of S is k, and for any choice of x0 P I and any py0, . . . , yk´1q P Ck, there exists a unique f P S such that f px0q “ y0, f 1px0q “ y1, . . . , f pk´1qpx0q “ yk´1. (2bis) If the functions ai are real-valued, the dimension of the space of real-valued solutions, as a real vector space, is k, and for any choice of x0 P I and any py0, . . . , yk´1q P Rk, there exists a unique real-valued solution f such that f px0q “ y0, f 1px0q “ y1, . . . , f pk´1qpx0q “ yk´1. If b and the coeﬃcients ai are real-valued, there exists a real-valued solution. (3) Let b be a continuous function on I. There exists a solution f0 to the inhomoge- neous equation ypkq ` ak´1ypk´1q ` ¨ ¨ ¨ ` a1y1 ` a0y “ b, and the set Sb is the set of functions f ` f0 where f P S. (4) For any x0 P I and any py0, . . . , yk´1q P C k, there exists a unique f P Sb such that f px0q “ y0, f 1px0q “ y1, . . . , f pk´1qpx0q “ yk´1. 5 Remark 2.2.4. (1) If b ­“ 0, the set Sb of solutions is not a vector space. (2) Statement (1) of this theorem is elementary: the set S is just the kernel of the linear map that sends a function f to Dpf q. In other words, if f1 and f2 are elements of S and z1, z2 are complex numbers and f “ z1f1 ` z2f2, then Dpf q “ z1Dpf1q ` z2Dpf2q “ 0. Also, if we can ﬁnd any element f0 of the set Sb, then it is elementary that all other elements are of the form f ` f0 where Sb, since for f1 P Sb, we get pf1 ´ f0qpkq ` ak´1pf1 ´ f0qpk´1q ` ¨ ¨ ¨ ` a0pf1 ´ f0q “ Dpf1 ´ f0q “ Dpf1q ´ Dpf0q “ b ´ b “ 0, so that f1 “ f ` f0 where f “ f1 ´ f0 P S. We will illustrate this result in the next sections by explaining how to solve, in practice, two important types of linear diﬀerential equations. Remark 2.2.5. The linearity of the equation has also consequences when trying to solve the inhomogeneous equation. Indeed, for instance, if we know a function f1 solving (2.3) with the right-hand side b1, and one function f2 solving (2.3) with the right- hand side b2, then f1 ` f2 solves (2.3) with right-hand side b1 ` b2, since Dpf1 ` f2q “ Dpf1q ` Dpf2q “ b1 ` b2. 2.3. Linear diﬀerential equations of order 1 Let I Ă R be an open interval. We consider here the linear diﬀerential equation y1 ` ay “ b, when a and b are general continuous functions deﬁned on I. The solution has two steps: ﬁrst solving the homogeneous equation y1 ` ay “ 0 (say that S is the space of solutions, which is a one-dimensional vector space according to Theorem 2.2.3), and then ﬁnding a solution f0 of the inhomogeneous equation, so that the set Sb contains exactly the functions f0 ` f where f P S. If f1 is a basis of S (which only means that f1 is in S and is not the zero function), this means that the solutions are given by f0 ` zf1, where z P C is arbitrary. If a is real-valued, then there exists a real-valued non-zero element f1 of S, and a real-valued solution f0, so that the real-valued solutions of the equation are the functions of the form f0 ` xf1, where x P R is arbitrary. If one wishes the solve the problem with initial value f px0q “ y0, then it suﬃces to solve the equation f0px0q ` zf1px0q “ y0 to determine the value of z. (It might be thought that there is a problem if f1px0q “ 0, but we will see that this never happens for a non-zero function f1 P S). Step 1 (solving the homogeneous equation). Formally, the idea is to transform y1 ` ay “ 0 into y1{y “ ´a, so that plog |y|q1 “ ´a, which implies by integration that y “ z expp´Aq, where z P C and A is a primitive of the function a. There is a potential problem with this argument, since we divided by y, which is a function so that y might vanish at some 6 point in the interval. However, it is easy to check that the conclusion is correct. First, if we deﬁne a function f pxq “ z expp´Apxqq, then we get by the chain rule the relation f 1pxq “ ´zA 1pxq expp´Apxqq “ ´apxqf pxq, so that f is a solution of the equation y1 ` ay “ 0. Conversely, suppose that f is a solution of the equation y1 ` ay “ 0, and deﬁne gpxq “ f pxq exppApxqq. Then we obtain, by the Leibniz rule and the chain rule, the formula g1pxq “ f 1pxq exppApxqq ` A 1pxqf pxq exppApxqq “ ´apxqf pxq exppApxqq ` apxqf pxq exppApxqq “ 0, which means that g is a constant, say z, in which case f pxq “ z expp´Apxqq, as we guessed. We conclude: Proposition 2.3.1. Any solution of y1 ` ay “ 0 is of the form f pxq “ z expp´Apxqq where A is a primitive of a. The unique solution with f px0q “ y0 is f pxq “ y0 exppApx0q ´ Apxqq. Step 2 (solving the inhomogeneous equation). Now consider the equation y1 ` ay “ b. We know that it suﬃces to ﬁnd a single solution f0 to obtain all of them by adding one of the solutions of y1 ` ay “ b found in the previous step. Example 2.3.2. Sometimes, we can make a clever guess that ﬁnds a suitable function f0. Consider for instance the equation y1 “ y ` x2. We might guess that a polynomial can be a solution; it should be of degree 2, so we can try f pxq “ ax2 ` bx ` c, for some constants a, b, c. In that case we get f 1pxq ´ f pxq “ 2ax ` b ´ pax2 ` bx ` cq “ ´ax2 ` p2a ´ bqx ` b ´ c so this function is a solution provided $ ’& ’% a “ ´1 2a ´ b “ 0, hence b “ ´2 b ´ c “ 0, hence c “ ´2. So we can take f0pxq “ ´x2 ´ 2x ´ 2. If there is no obvious guess of the form of a special solution f0, there is a general method that works (but might lead to complicated formulas). It is called “variation of the constant”, because it starts with the formula for a solution of the homogeneous equation, namely f pxq “ z expp´Apxqq, z P C, and looks for a solution f0 of this form, but where now z is considered to be itself a function of x. If we assume this, and compute the derivative f 1, then we see that f0pxq “ zpxq expp´Apxqq is a solution of y1 ` ay “ b if and only if z1pxq expp´Apxqq ´ A 1pxqzpxq expp´Apxqq ` apxqzpxq expp´Apxqq “ bpxq, which (since A1pxq “ apxq) translates into z1pxq “ bpxq exppApxqq. 7 In other words, we can take z to be a primitive Cpxq of the continuous function bpxq exppApxqq, and the special solution is f0pxq “ Cpxq expp´Apxqq. If we use the fundamental theorem of calculus to write primitives (taking the value 0 at x0) of a and b exppAq, this becomes the rather complicated expression f0pxq “ exp ´ ´ ż x x0 aptqdt ¯ ż x x0 bptq exp ´ż t x0 apuqdu ¯ dt, which is a special solution such that f0px0q “ 0. Remark 2.3.3. When solving concrete equations, do not forget the last step of mul- tiplying the “constant” zpxq by expp´Apxqq at the end! 2.4. Linear diﬀerential equations with constant coeﬃcients Let k ě 1 be an integer, and let a0, . . . , ak´1 be complex constant coeﬃcients. We consider the linear diﬀerential equation ypkq ` ak´1ypk´1q ` ¨ ¨ ¨ ` a1y1 ` a0y “ b. Note that the coeﬃcients ai are ﬁxed numbers, but the right-hand side b is still assumed to be a general continuous function. Example 2.4.1. The equation y2 ´ xy “ 0 does not belong to this class, but the equations y1 ´ y “ 0 and y2 ` y “ 0 (satisﬁed by the exponential and by trigonometric functions) have constant coeﬃcients. The solution of the homogeneous equation is very simple in principle. One looks for solutions of the special form f pxq “ eαx for some complex number α P C. Then we have f pjqpxq “ αjeαx for all j ě 0 and for all x, which means that f pkqpxq ` ak´1f pk´1qpxq ` ¨ ¨ ¨ ` a1f 1pxq ` a0f pxq “ e αxpαk ` ak´1αk´1 ` ¨ ¨ ¨ ` a1α ` a0q. We conclude that f is a solution of the homogeneous equation if and only if P pαq “ 0, where P is the polynomial with coeﬃcients a0, . . . , ak´1: P pXq “ X k ` ak´1X k´1 ` ¨ ¨ ¨ ` a1X ` a0. According to the Fundamental Theorem of Algebra, this polynomial of degree k has k complex roots, counted with multiplicity: there exist complex numbers α1, . . . , αk such that P pXq “ pX ´ α1q ¨ ¨ ¨ pX ´ αkq. This polynomial is called the companion or characteristic polynomial of the homogeneous diﬀerential equation. Remark 2.4.2. We repeat that this is only deﬁned when the coeﬃcients of the equa- tion are constant. Remark 2.4.3. Although it is natural to look for complex-valued solutions, one is often interested in situations where the coeﬃcients ai are real and we know that the solution should take real values, or we want such solutions. Suppose that a root α “ β ` iγ is not real, so the imaginary part γ is non-zero. Then the solution f pxq “ e αx does not take real values. However, in that case, the conjugate 8 β ´ iγ “ ¯α ­“ α is also a root of the companion polynomial (which has real coeﬃcients, so that P p¯zq “ P pzq for any z P C) and one can replace the two solutions f1pxq “ eαx, f2pxq “ e ¯αx by the real-valued functions rf1pxq “ e βx cospγxq, rf2pxq “ eβx sinpγxq (note for instance that f1 “ rf1 ` i rf2 and that rf1 “ f1 ` f2 since eiθ “ cospθq ` i sinpθq). The possible existence of multiple roots requires some care in the next step, so we begin by discussing the simple case where this does not happen. Case 1: no multiple roots. Assume that αi ­“ αj for i ­“ j. Then we have found k distinct solutions fjpxq “ eαj x of the homogeneous equation. It is not very diﬃcult to check that these functions are linearly independent, so that the space of linear combinations of these functions has dimension k. According to Theorem 2.2.3, (2), this must be the full vector space S of solutions of the homogeneous linear diﬀerential equation. In other words, any solution of ypkq ` ak´1ypk´1q ` ¨ ¨ ¨ ` a1y1 ` a0y “ 0 is of the form f pxq “ z1e α1x ` ¨ ¨ ¨ ` zke αkx, for some complex numbers pz1, . . . , zkq that can be chosen arbitrarily. If one wishes to ﬁnd the unique solution with f px0q “ y0, . . . , f pk´1qpx0q “ yk´1 for given py0, . . . , yk´1q, one may simply view z1, . . . , zk as unknowns. Substituting x “ x0 in the formula for f pxq and solving for these initial conditions becomes a linear system with unknowns z1, . . . , zk. It is a fact that the system has a unique solution (the determinant is always non-zero), which provides the required function. Remark 2.4.4. If the constants ai are real, the space of real-valued solutions of the equation is obtained as follows: order the roots αj so that α1, . . . , αm are the real solutions of the polynomial P , and αm`1, . . . , αk are the solutions which are not real. Write αj “ aj ` ibj for j ě m ` 1. (Note that we may have m “ 0, if there is no real solution, or m “ k, if all solutions are real). Then the space of real-valued solutions of the homogeneous diﬀerential equation is the space of functions of the form f pxq “ x1eα1x ` ¨ ¨ ¨ ` xme αmx` xm`1e am`1x cospbm`1xq ` ym`1eam`1x sinpbm`1xq` ¨ ¨ ¨ ` xke akx cospbkxq ` ykeakx sinpbkxq. Because such expressions are more complicated to handle, it is often better to work with complex-valued solutions as long as possible. Example 2.4.5. (1) Consider the equation y1 ` ay “ 0, with a constant. The com- panion polynomial is X ` a, so the only solution is α1 “ ´a, and we get the solutions f pxq “ ze´ax. This coincides with the solution in Section 2.3, since a primitive of a is Apxq “ ax. (2) Consider the equation y2 ´ ay “ 0. The companion polynonial is P “ X 2 ´ a. There are then three cases. 9 ‚ (Case 1). If a ą 0, then P “ pX ´ ? aqpX ` ? aq has two real roots, and the solutions take the form f pxq “ z1e? a x ` z2e´? a x. ‚ (Case 2). If a ă 0, then P “ pX ´ ia |a|qpX ` i a |a|q, and the solutions take the form f pxq “ z1ei? |a| x ` z2e´i? |a| x “ pz1 ` z2q cosp a|a| xq ` ipz1 ´ z2q sinp a|a| xq. ‚ (Case 3). If a “ 0, then we have only found one solution (namely f pxq “ 1). However, the equation is easily solved in that case: f is a solution of y2 “ 0 means that f pxq “ z1x`z2 for some complex numbers z1 and z2. So the function f1pxq “ x is a second solution linearly independent of the ﬁrst. (3) What is the solution f to y2 ` y1 ` y “ 0 such that f p0q “ 1 and f 1p0q “ 0? The companion polynomial is P pXq “ X 2 ` X ` 1 “ pX ´ αqpX ´ ¯αq with α “ p´1 ` i ? 3q{2. Since we are interested in real solutions, it is easier to work with the two basic solutions f1pxq “ e ´x{2 cos ´ ? 3 2 x¯ , f2pxq “ e´x{2 sin ´ ? 3 2 x¯ . We know that there exist numbers z1 and z2 such that f pxq “ z1f1pxq ` z2f2pxq, and the initial conditions transform into the linear equations # z1 “ 1 ´ 1 2z1 ` ? 3 2 z2 “ 0 for z1 and z2 (since, for instance, we have f 1 1pxq “ ´1 2e´x{2 cos ´? 3 2 x¯ ´ ? 3 2 e´x{2 sin ´ ? 3 2 x¯ , so f 1 1p0q “ ´1{2, and similarly for f 1 2p0q “ ? 3{2). It follows that z1 “ 1 and z2 “ 1{ ? 3. Case 2 (multiple roots). Suppose that α is a multiple root of order j of the polynomial P , with 2 ď j ď k. Then the j functions fα,0pxq “ e αx, fα,1pxq “ xe αx, ¨ ¨ ¨ , fα,j´1pxq “ xj´1e αx are linearly independent, and are solutions of the homogeneous linear diﬀerential equa- tion. Taking the union of the functions fα,j for all roots of P , each with its multiplicity, gives a basis of the space of solutions. Remark 2.4.6. To say that α is a root of P with multiplicity j ě 1 means either of the following two equivalent conditions: (1) We have P pαq “ ¨ ¨ ¨ “ P pj´1qpαq “ 0. (2) We have a factorization P pXq “ pX ´ αq jQpXq, where Q is a polynomial and Qpαq ­“ 0. We now check the assertion about fα,j being a solution in the case of a double root (j “ 2). Note that f 1 1pxq “ αxeαx ` e αx, f 2 1 pxq “ α2xeαx ` 2αeαx, . . . 10 so that we ﬁnd the formula f pkq 1 pxq ` ak´1f pk´1q 1 pxq ` ¨ ¨ ¨ ` a1f 1 1pxq ` a0f1pxq “ xe αxP pαq ` eαxP 1pαq. Since P pαq “ P 1pαq “ 0 for a double root, the function f1 is a solution of the homogeneous diﬀerential equation. The general case is similar. Once a basis of S is found (using this kind of functions for each root), one can ﬁnd the unique solution with given initial conditions by again substituting x0 in a linear combination, and solving a system of linear equations. Example 2.4.7. Suppose that the companion polynomial factors as P pXq “ XpX ´ 4q 3pX ´ p1 ` iqqpX ´ p1 ´ iqq. Then a basis of the solution space S are the functions f0pxq “ 1 (for the solution 0 of P ) f1pxq “ e4x, f2pxq “ xe4x f3pxq “ x2e 4x (for the solution 4, which is a triple root) f4pxq “ ep1`iqx “ pcospxq ` i sinpxqqe x, f5pxq “ ep1´iqx “ pcospxq ´ i sinpxqqe x. If one is interested in real-valued solutions, it might be easier to use the alternate basis where f4 and f5 are replaced by rf4pxq “ e x cospxq, rf5pxq “ ex sinpxq. We now go back to the general case. If we need to solve an inhomogeneous equation, there remains to ﬁnd a special solution for (2.4) ypkq ` ak´1ypk´1q ` ¨ ¨ ¨ ` a1y1 ` a0y “ b. There are some useful tricks that can be used to avoid the analogue of the method of variation of constants, which is often rather complicated to implement (as we will see below). The ﬁrst is Remark 2.2.5 (following from linearity). The second is that there are special cases of right-hand sides b where one can search explicitly for solutions of a special form. The most important are the following: (1) If bpxq “ xde βx for some integer d ě 0 and some number β which is not a root of the companion polynomial P , then one looks for a solution of the form f pxq “ Qpxqe βx, where Q is a polynomial of degree d. (2) If bpxq “ xd cospβxq or bpxq “ xd sinpβxq for some integer d ě 0 and some number β which is not a root of the companion polynomial P , then one can either transform it to a combination of complex exponentials (and apply linearity and (1)), or one may look for a solution of the form f pxq “ Q1pxq cospβxq ` Q2pxq sinpβxq, where Q1 and Q2 are polynomials of degree d. (3) If b is of the form of the previous two examples but where β is a root of mul- tiplicity j of the companion polynomial, then one looks for f pxq “ Qpxqeβx (or the analogue with cosine and sine), but where Q has degree d ` j. (4) The special case β “ 0 of (1), (2), (3) corresponds to the situation when b is a polynomial of degree d ě 0. So one should search for a polynomial solution f of the same degree d, unless 0 is a root of the companion polynomial, in which case one should look for a polynomial of degree d ` j, where j is the multiplicity of 0 as a root of P . 11 Example 2.4.8. (1) We can illustrate Example (3) (and in fact remember the way it works) by considering the case where P pXq “ X j, so that α “ 0 is a root of order j. The equation is ypjq “ b, and if bpxq “ xde αx “ xd, then a solution is f pxq “ 1 pd ` 1q ¨ ¨ ¨ pd ` jq xd`j, which is indeed a polynomial of degree d ` j. (2) Consider the equation y2 ` 3y1 ` y “ 3x2 ` cospxq. Here we use linearity to ﬁnd a special solution: a solution is f “ 3f1 ` f2, where f1 is a solution of y2 ` 3y1 ` y “ x2 and f2 is a solution of y2 ` 3y1 ` y “ cospxq. The companion polynomial is P “ X 2 ` 3X ` 1 with roots α1 “ p´3 ` ? 5q{2 and α2 “ p´3 ´ ? 5q{2. To ﬁnd f1, we note that β “ 0 is not a root of P , so we look for f1pxq “ ax2 ` bx ` c. Then f 2 1 ` 3f 1 1 ` f1 “ ax2 ` pb ` 6aqx ` pc ` 3b ` 2aq, and the linear system to solve is $ ’& ’% a “ 1 b ` 6a “ 0 hence b “ ´6 c ` 3b ` 2a “ 0 hence c “ 18 ´ 2 “ 16. This means that f1pxq “ x2 ´ 6x ` 16. To ﬁnd f2, since β “ 1 is not a root of P , we consider f2pxq “ a cospxq ` b sinpxq. Then f 2 2 ` 3f 1 2 ` f2 “ pa ` 3b ´ aq cospxq ` pb ` 3a ´ bq sinpxq “ 3b cospxq ` 3a sinpxq, and that means that we can take b “ 1{3, a “ 0, and f2pxq “ 1 3 sinpxq. We conclude that a special solution of the inhomogeneous equation is f pxq “ 3f1pxq ` f2pxq “ 3x2 ´ 18x ` 48 ` 1 3 sinpxq. Finally we discuss the method of variation of constants for linear diﬀerential equations of order ě 2; it does not, in fact, necessarily require that the coeﬃcients are constant, although the computations are often very diﬃcult in general situations. We consider the inhomogeneous equation (2.5) ypkq ` ak´1ypk´1q ` ¨ ¨ ¨ ` a1y1 ` a0y “ b, and we assume that a basis pf1, . . . , fkq of the space S of solutions of the homogeneous equation ypkq ` ak´1ypk´1q ` ¨ ¨ ¨ ` a1y1 ` a0y “ 0 has been found (it may be any basis). We then search for a solution to (2.5) of the form f pxq “ z1pxqf1pxq ` ¨ ¨ ¨ ` zkpxqfkpxq, where z1, . . . , zk are functions such that, moreover, we have $ ’’’& ’’’% z1 1pxqf1pxq ` ¨ ¨ ¨ ` z1 kpxqfkpxq “ 0 z1 1pxqf 1 1pxq ` ¨ ¨ ¨ ` z1 kpxqf 1 kpxq “ 0 ¨ ¨ ¨ z1 1pxqf pk´2q 1 pxq ` ¨ ¨ ¨ ` z1 kpxqf pk´2q k pxq “ 0 12 for all x. The justiﬁcation for requiring these k ´ 1 extra constraints is that we need to ﬁnd k diﬀerent functions, and we may hope to succeed if they satisfy k diﬀerent equations; one of these will be the original one (2.5), in combination with the k ´ 1 extra conditions. Indeed, one can prove that this method works. The most important example is k “ 2. Write again f “ z1f1 `z2f2, and the constraint z1 1f1 ` z1 2f2 “ 0. The reason this condition is useful is that we get by diﬀerentiation the formulas f 1 “ z1 1f1 ` z1 2f2 ` z1f 1 1 ` z2f 1 2 “ z1f 1 1 ` z2f 1 2 f 2 “ z1 1f 1 1 ` z1 2f 1 2 ` z1f 2 1 ` z2f 2 2 , and therefore y2 ` a1y1 ` a0y “ z1pf 2 1 ` a1f 1 1 ` a0f1q ` z2pf 2 2 ` a1f 1 2 ` a0f2q ` z1 1f 1 1 ` z1 2f 1 2. But f1 and f2 solve the homogeneous equation, and hence y2 ` a1y1 ` a0y “ z1 1f 1 1 ` z1 2f 1 2. We conclude that z1, z2 lead to a solution of the inhomogeneous equation provided they satisfy the equations # z1 1f1 ` z1 2f2 “ 0 z1 1f 1 1 ` z1 2f 1 2 “ b. For any given value of x, this is a linear system of equations with unknowns pz1 1pxq, z1 2pxqq. Once it is solved, we can obtain (in principle) the required functions z1 and z2 by com- puting primitives of pz1 1, z1 2q. It is a fact that the determinant f1f 1 2 ´ f 1 1f2 of the system will not vanish when solving this linear system of equations, corresponding to the fact that pf1, f2q is a basis of the space S of solutions of the homogeneous equation. Example 2.4.9. We wish to solve the inhomogeneous equation y2 ` y1 ´ 6y “ 1 1 ` x2 . The roots of the companion polynomial X 2 ` X ´ 6 are α1 “ 2 and α2 “ ´3, so we search for a solution of the type f pxq “ z1pxqe2x ` z2pxqe ´3x satisfying z1 1pxqe2x ` z1 2pxqe ´3x “ 0 for all x. Substituting into the equation, we obtain the system # z1 1pxqe2x ` z1 2pxqe ´3x “ 0 2z1 1pxqe2x ´ 3z1 2pxqe ´3x “ 1 1`x2 . The determinant is ´5e´x so is indeed never zero, and we ﬁnd the solutions for z1 1 and z1 2 given by # z1 1pxq “ e´2x 5p1`x2q z1 2pxq “ ´ e3x 5p1`x2q . This means that a solution is f pxq “ 1 5e2x ż x 0 e´2t 1 ` t2 dt ´ 1 5e´3x ż x 0 e 3t 1 ` t2 dt. 13 2.5. An example: the harmonic oscillator One of the most basic example of linear diﬀerential equation with constant coeﬃcients is given by the harmonic oscillator. Case 1 (harmonic oscillator without friction). Here we have a particle with mass m ą 0 attached at the end of a vertical spring, moving without the eﬀect of gravity or of any friction. We measure its position along the axis of movement by a single function yptq, where t is time, and where the origin of the y-axis refers to the equilibrium position. Then the only force acting on the particle is the restoring force from the spring, which is of the form F “ ´ky for some coeﬃcient k ą 0 that depends on the “strength” of the spring. The Newtonian equations of motion takes the form of the diﬀerential equation m:y “ ´ky, or in other words x is solution of the homogeneous linear diﬀerential equation of order 2 given by :y ` k my “ 0. Since k{m ą 0, the real-valued solutions are of the forme yptq “ a cospωtq ` b sinpωtq where ω “ a k{m, for some real numbers a and b. It is customary to rephrase this in the form yptq “ A cospωt ` ϕq where A “ pa2 ` b2q 1{2 and ϕ is some real number. The advantage of this formula is that it clearly shows not only that the movement of the particle is periodic, with period 2π{ω, but also that its maximal amplitude (around the equilibrium position corresponding to y “ 0) is A. To see why this formula holds, note that ´ a A ¯2 ` ´ b A ¯2 “ 1, so that there exists a real number ϕ such that cospϕq “ a{A and sinpϕq “ ´b{A; we get a cospωtq ` b sinpωtq “ Apcospϕq cospωtq ´ sinpϕq sinpωtqq “ A cospωt ` ϕq. Case 2 (damped harmonic oscillator). Suppose now that the particle also encoun- ters resistance, and that this other force is proportional to velocity (this is an assumption true in many cases, at least approximately). Then the Newton equation for yptq becomes m:y “ ´b 9y ´ ky, where b ą 0 is another parameter measuring the strength of the friction force. We write this as :y ` b m 9y ` k my “ 0, which has companion polynomial X 2 ` b m X ` k m . There are correspondingly three cases, depending on the sign of ∆ “ b2 ´ 4km m2 . 14 If ∆ ą 0, a basis of the space S of solutions is y1ptq “ exp ´´ ´ b 2m ` 1 2 ? ∆ ¯t ¯ , y2ptq “ exp ´´ ´ b 2m ´ 1 2 ? ∆ ¯ t ¯ . Observe that the sum of the two solutions of the quadratic equation is ´b{m ă 0 and the product is k{m ą 0, so that both solutions of the quadratic equations are negative. This means that, as t Ñ `8, we have y1ptq Ñ 0 and y2ptq Ñ 0. Since the condition ∆ ą 0 corresponds to b “large”, the physical behavior is that the friction force is strong enough to essentially bring the motion to a stop, without oscillations. If ∆ “ 0, there is a double root, and a basis of the space S of solutions is y1ptq “ exp ´ ´ bt 2m ¯ , y2ptq “ t exp ´ ´ bt 2m ¯ . We have then also an exponentially fast “return to equilibrium” without oscillations. If ∆ ă 0, we get oscillatory functions as basis for the real solutions of the equation, namely y1ptq “ exp ´ ´ bt 2m ¯ cosp 1 2a |∆| tq y2ptq “ exp ´ ´ bt 2m ¯ sinp 1 2a |∆| tq. The solution can now, as above, be expressed in the form yptq “ Ae´bt{p2mq cosp 1 2a |∆| t ` ϕq, (with A ą 0 and ϕ P R). Since b ą 0, the physical behavior is again return to equilibrium due to friction, but in an oscillatory manner around the equilibrium position. Note that the period 2π{p 1 2a|∆|q is larger than the period 2π{pk{mq of the oscillator with the same parameters but without friction. 2.6. Other methods Besides the techniques described in the previous sections, it is useful to know two other commong methods that can be helpful to solve certain diﬀerential equations that are not of the type previously considered. Change of variable. If a function f pxq is replaced by hpyq “ f pgpyqq, where g is a “new variable”, then any equation satisﬁed by f corresponds to an equation satisﬁed by h, and this equation may be simpler to solve, leading to a solution of the original one. Example 2.6.1. If we make the change of variable hptq “ f petq, then we have relations h 1ptq “ e tf 1petq, h 2ptq “ e tf 1petq ` e 2tf 2petq. If, for instance, we try to solve x2y2 ` xy1 “ y, for x ą 0, then we see that his is equivalent to h 2ptq “ hptq for hptq “ f petq. So the solutions are given by hptq “ aet ` be ´t which means that f pxq “ ax ` b x . 15 Separation of variable. Suppose that a diﬀerential equation of order 1 can be written in the form pgpyqq 1 “ b for some functions g and b (in other words, g1pyqy1 “ b). Then this can be solved by writing gpf pxqq “ Bpxq, where B is a primitive of b, and then “inverting” g. Example 2.6.2. Consider the equation e 2yy1 “ x with x ą 0. To say that f is a solution means that the derivative of 1 2e 2f pxq is x, hence e 2f pxq “ x2 ` a for some constant a, or in other words f pxq “ 1 2 logpx2 ` aq. 16 CHAPTER 3 Diﬀerential calculus in R n In this chapter, n and m are always integers ě 1. 3.1. Introduction We are interested in functions deﬁned on subsets of R n which take values in R, or C, or even in another space Rm, where m ě 1 is an integer. Here are some basic examples of such functions that should be kept in mind. (1) Linear maps f : Rn Ñ Rm, or in other words, functions deﬁned by f pxq “ Ax, where A is a matrix with n columns and m rows, and x is interpreted as a column vector. For instance, for n “ 2 and m “ 1, one can consider f px, yq “ x ` y for px, yq P R 2. Slightly more generally, if in addition we ﬁx y0 P Rm, we can deﬁne the aﬃne-linear map f pxq “ y0 ` Ax. (2) Quadratic forms Q : Rn Ñ R, or in other words, functions of the type Qpxq “ nÿ i“1 nÿ j“1 ai,jxixj for all x “ px1, . . . , xnq, where pai,jq are real numbers. For instance, for n “ 2, one can consider Qpx, yq “ xy; for arbitrary n, one has the quadratic form Qpx1, . . . , xnq “ x2 1 ` ¨ ¨ ¨ ` x2 n. (3) Polynomials in n variables: these generalize the previous two examples. Given an integer d ě 0, a polynomial in n variables of degree ď d is a ﬁnite sum of monomials of degree e ď d, namely a ﬁnite sum of functions Rn Ñ R of the type (3.1) f px1, . . . , xnq “ αxd1 1 ¨ ¨ ¨ xdn n where the degree of the monomial, that is the integer e “ d1 ` ¨ ¨ ¨ ` dn, satisﬁes e ď d. For instance, the function f px, y, zq “ x3 ´ 12xy5z ` xyz is a polynomial of degree 7. Example (1) (aﬃne-linear maps) corresponds to polynomials of degree ď 1, and Example (2) to certain polynomials of degree 2. (4) “Cartesian product” functions: two functions f1 : R n Ñ Rm1 and f2 : Rn Ñ Rm2 combine to produce a function f “ pf1, f2q : Rn Ñ Rm1`m2, deﬁned by f pxq “ pf1pxq, f2pxqq. An important point is that any function f : Rn Ñ R m is a cartesian product f “ pf1, . . . , fmq of functions fi : Rn Ñ R, where fipxq is just the i-th coordinate of f pxq as a vector in Rm. This means that many deﬁnitions and results for 17 Figure 3.1. Graphs of f px, yq “ x2 ` y2 and f px, yq “ sinp 1 2x2 ` xyq functions Rn Ñ Rm may be reduced easily to the case m “ 1 by considering each coordinate separately. (5) Functions with separated variables: if f1, . . . , fn are functions on R (or on a subset of R, the same for each of them), we can deﬁne a function f : Rn Ñ R by f px1, . . . , xnq “ f1px1q ¨ ¨ ¨ fnpxnq, where the variables are “separated”. For instance, any monomial (3.1) is a function with separated variables. (6) Composition of functions: given any function f : R n Ñ R, and a function g : R Ñ R, we can consider the composition g ˝ f . For instance, composing the quadratic form Qpx1, . . . , xnq “ x2 1 ` ¨ ¨ ¨ ` x2 n with the square root, one obtains b x2 1 ` ¨ ¨ ¨ ` x2 n, which is the euclidean norm (length from the origin to the point x P Rn). Com- posing with expp´yq, one gets expp´px2 1 ` ¨ ¨ ¨ x2 nqq. Note that this last function is a function with separated variables (but the eu- clidean norm is not). For functions f : R2 Ñ R, one can visualize the graph of f , which is tpx, y, zq P R 3 : z “ f px, yqu as a surface in R3 (see Figure 3.1 for two examples; using an interactive software is better to understand such pictures, as one can manipulate the graph easily). This visualization is not possible anymore when there are 3 variables or more. This is one reason why multi- variable calculus is often more diﬃcult to understand intuitively than the one-variable case. Remark 3.1.1. Another interesting visualization possibility concerns the case of f : R2 Ñ R2, where one can show f pxq as a vector based at x, at least for a subset of values of x. Figure 3.2 illustrates this for the function f px, yq “ p´x2 ` y ´ 1, x ´ y2 ` 1q. 18 -3 -2 -1 0 1 2 3 -3 -2 -1 0 1 2 3 Figure 3.2. Vector plot 3.2. Continuity in R n The ﬁrst notion that we want to generalize is that of a continuous function. To follow the example of functions of one variable, we need ﬁrst to recall the deﬁnition of convergence and limit of a sequence (or of a function) in Rn. We deﬁne }x} “ b x2 1 ` ¨ ¨ ¨ ` x2 n for x P Rn (the norm of x in the euclidean space Rn; see Section 1.2 in [1])). This function satisﬁes the following properties: }x} ě 0, and }x} “ 0 if and only if x “ 0 }tx} “ |t|}x} for all t P R }x ` y} ď }x} ` }y} (triangle inequality). The deﬁnition of convergence on Rn is given in [1, Def. 2.6.1]. Definition 3.2.1. Let pxkqkPN where xk P Rn. Write xk “ pxk,1, . . . , xk,nq. Let y “ py1, . . . , ynq P Rn. We say that the sequence pxkq converges to y as k Ñ `8 if for all ε ą 0, there exists N ě 1 such that for all n ě N , we have }xk ´ y} ă ε. Lemma 3.2.2. The sequence pxkq converges to y as k Ñ `8 if and only if one of the following equivalent conditions holds: (1) For each i, 1 ď i ď n, the sequence pxk,iq of real numbers converges to yi. (2) The sequence of real numbers }xk ´ y} converges to 0 as k Ñ `8. Proof. The equivalence of the two conditions is elementary: ﬁrst, since |xk,i ´ yi|2 ď nÿ j“1 |xk,j ´ yj|2 “ }xk ´ y} 2, the second condition implies that xk,i Ñ yi for each i; conversely, if the ﬁrst condition holds, then }xk ´ y} 2 “ nÿ j“1 |xk,j ´ yj|2 19 is the sum of n sequences, each converging to 0, hence converges to 0. The fact that these are equivalent to the convergence of pxkq to y is proved in [1, Satz 2.6.3]. □ We next extend the deﬁnition of continuity given in [1, Def. 3.2.1, 3.2.2]. Definition 3.2.3. Let X Ă Rn and f : X Ñ R m. (1) Let x0 P X. We say that f is continuous at x0 if for all ε ą 0, there exists δ ą 0 such that, if x P X satisﬁes ∥x ´ x0∥ ă δ, then }f pxq ´ f px0q} ă ε. (2) We say that f is continuous on X if it is continuous at x0 for all x0 P X. Similarly to [1, Satz 3.2.4], we can test if a function is continuous using sequences. Proposition 3.2.4. Let X Ă Rn and f : X Ñ Rm. Let x0 P X. The function f is continuous at x0 if and only if, for every sequence pxkqkě1 in X such that xk Ñ x0 as k Ñ `8, the sequence pf pxkqqkě1 in Rm converges to f pxq. From this proposition, we can immediately see that most functions that we encounter are continuous. In a similar way, we can deﬁne the limit of a function at a point (see [1, §3.10]). Definition 3.2.5. Let X Ă Rn and f : X Ñ Rm. Let x0 P X and y P Rm. We say that f has the limit y as x Ñ x0 with x ­“ x0 if for every ε ą 0, there exists δ ą 0, such that for all x P X, x ­“ x0, such that ∥x ´ x0∥ ă δ, we have }f pxq ´ y} ă ε. We then write lim xÑx0 x­“x0 f pxq “ y. Remark 3.2.6. In this deﬁnition, we could also remove the assumption that x0 P X, because if x0 R X, we could always extend f to X Ytx0u by, for instance, deﬁning f px0q “ 0. The “sequence” test for this condition is: Proposition 3.2.7. Let X Ă R n and f : X Ñ Rm. Let x0 P X and y P Rm. We have lim xÑx0 x­“x0 f pxq “ y. if and only if, for every sequence pxkq in X such that xk Ñ x as k Ñ `8, and xk ­“ x0, the sequence pf pxkqq in Rm converges to y. Example 3.2.8. Let X Ă Rn and f : X Ñ Rm. Let x0 P X. Then f is continuous at x0 if and only if lim xÑx0 x­“x0 f pxq “ f px0q. The easiest way to prove continuity is in general to use composition: Proposition 3.2.9. Let X Ă Rn, Y Ă Rm and p ě 1 an integer. Let f : X Ñ Y and g : Y Ñ Rp be continuous functions. Then the composite g ˝ f is continuous. Proof. We apply Proposition 3.2.4. If pxkq is a sequence in X converging to x P X in Rn, then by continuity of f , the sequence pf pxkqq is a sequence in Y converging to y “ f pxq. Then by continuity of g, the sequence pgpf pxkqq converges to gpf pxqq. By deﬁnition of g ˝ f , this implies that g ˝ f is continuous. □ 20 Example 3.2.10. (1) Cartesian products of continuous functions are continuous: if f1 : Rn Ñ Rm1 and f2 : R n Ñ R m2 are continuous, then f “ pf1, f2q : Rn Ñ Rm1`m2 is also continuous. In particular, a function f : Rn Ñ Rm is continuous if and only if its coordinates f1, . . . , fm are continuous. This follows from the deﬁnition and is left as an exercise. (2) Any linear map f : R n Ñ Rm is continuous. In particular, the identity map is continuous. To see this, note that according to (1) it is enough to assume that m “ 1. Then there exist numbers a1, . . . , an such that f px1, . . . , xnq “ a1x1 ` ¨ ¨ ¨ anxn. Let y “ py1, . . . , ynq P R n and let pxkq be a sequence converging to y. Then, writing xk “ pxk,1, . . . , xk,nq, we have xk,i Ñ yi for all i, and therefore it follows that aixk,i Ñ aiyi, and then that f pxkq “ a1xk,1 ` ¨ ¨ ¨ ` anxk,n Ñ a1y1 ` ¨ ¨ ¨ ` anyn “ f pyq (see [1, Satz 2.1.8]: for convergent sequences, limpak ` bkq “ lim ak ` lim bk). A similar argument shows that if f1 : X Ñ Rm and f2 : X Ñ Rm are continuous on X, then f1 ` f2 is also continous. Alternatively, one can write f1 ` f2 “ a ˝ pf1, f2q, where a : Rm ˆ Rm Ñ Rm is the addition map. Since a is linear and pf1, f2q is continuous, the sum f1 ` f2 is continuous by composition (Proposition 3.2.9). (3) Functions with separated variables are continuous if the factors are continuous: if f1, . . . , fn are continuous functions on R (or on a subset of R, the same for each of them), then f deﬁned by f px1, . . . , xnq “ f1px1q ¨ ¨ ¨ fnpxnq is continuous on Rn. This follows easily from the rule lim kÑ`8 akbk “ ab for convergent sequences ([1, Satz 2.1.8]). (4) Combining addition and functions with separated variables, one deduces that polynomials in x1, . . . , xn are continuous. (5) Similarly, using the rules lim kÑ`8 akbk “ ab, lim kÑ`8 ak{bk “ a{b when real numbers ak Ñ a and bk Ñ b, with b ­“ 0 in the second case (again [1, Satz 2.1.8]), one checks that if f1 and f2 are continuous functions from X Ă Rn to R, then f1f2 is continuous, and if moreover f2pxq ­“ 0 for all x P X, then f1{f2 is continuous. (6) Analogues of the previous results exist for limits of functions, for instance lim xÑx0pf pxq ` gpxqq “ lim xÑx0 f pxq ` lim xÑx0 gpxq, lim xÑx0 f pxqgpxq “ lim xÑx0 f pxq lim xÑx0 gpxq if both f and g have limits as x Ñ x0. (7) Suppose that f : R2 Ñ R is continuous. Then, if we ﬁx a value y0 P R, then the function g deﬁned on R by gpxq “ f px, y0q is continuous (for instance, it is the composition of f and of the function x ÞÑ px, y0q, which is continuous). However the converse is not true. For instance, deﬁne f px, yq “ # x if y ě 0 ´x if y ă 0. 21 Then each function gpxq “ f px, y0q is continuous, but f itself is not continuous. For instance, we have f p1, 0q “ 1. However, the sequence pxk, ykq “ p1, ´1{kq converges to p1, 0q but we have f p1, ´1{kq “ ´1 for any k ě 1, which does not converge to 1. One of the ﬁrst diﬃculties in extending the deﬁnitions of Analysis I to functions of n ě 2 variables is that the sets on which they are deﬁned can be much more complicated than those used with one variable, which are often just intervals. For n “ 2, one can draw many diﬀerent possible “two-dimensional” shapes, each of which is a possible deﬁnition set for a function. We need analogues of closed and compact intervals (Deﬁnitions 2.5.1 and 3.4.2 in [1]). Definition 3.2.11. (1) A subset X Ă Rn is bounded if the set of }x} for x P X is bounded in R. (2) A subset X Ă Rn is closed if for every sequence pxkq in X that converges in Rn to some vector y P Rn, we have y P X. (3) A subset X Ă Rn is compact if it is bounded and closed. Example 3.2.12. (1) The empty set and Rn are both closed. (2) Let r ą 0 and x0 P Rn. The open disc D “ tx P Rn : }x ´ x0} ă ru is bounded (since, by the triangle inequality, we have }x} ď }x ´ x0} ` }x0} ď r ` }x0} for all x P D). It is not closed, since for instance the sequence xk “ x0 ` pr ´ 1{k, 0, . . . , 0q Ñ x0 ` r where x0 ` r R D. (3) The closed disc ∆ “ tx P Rn : }x ´ x0} ď ru is closed and bounded. Indeed, let xk P ∆ be a sequence that converges to y P Rn. We have b pxk,1 ´ x0,1q2 ` ¨ ¨ ¨ ` pxk,n ´ x0,nq2 ď r for all k and xk,i Ñ yi. Taking k Ñ `8, and using the property pak ď a for all kq ñ lim k ak ď a for converging sequences of real numbers, we deduce that b py1 ´ x0,1q2 ` ¨ ¨ ¨ ` pyn ´ x0,nq2 ď r so that y P ∆. In particular, for n “ 1, this means that a closed interval is a closed set. An interval is compact if, furthermore, it is bounded. (4) If X1 Ă R n and X2 Ă Rm are bounded (resp. closed, resp. compact), then so is X1 ˆ X2 Ă Rn`m. In particular, the set B “ I1 ˆ ¨ ¨ ¨ ˆ In “ tpx1, . . . , xnq P Rn : xi P Iiu is closed (resp. compact) if each interval Ii is closed (resp. compact). Using basic examples of closed sets as above, one can construct many more using the following fundamental property: Proposition 3.2.13. Let f : Rn Ñ Rm be a continuous map. For any closed set Y Ă Rm, the set f ´1pY q “ tx P Rn : f pxq P Y u Ă Rn is closed. 22 Proof. Indeed, let X “ f ´1pY q. If pxkq is a sequence in X that converges to y P Rn, then by continuity we get f pxkq Ñ f pyq. But then f pyq P Y because it is the limit of f pxkq P Y and Y is closed. This means that y P f ´1pY q. □ Example 3.2.14. Let f : Rn Ñ R be a continuous function. The zero set Z “ tx P Rn : f pxq “ 0u is closed in Rn because t0u Ă R is closed. For instance for r ě 0, a circle or a sphere of radius r, deﬁned by tx P R2 : }x ´ x0} “ ru, tx P R3 : }x ´ x0} “ ru, is closed. Similarly, for any r ě 0, the set tx P Rn : |f pxq| ď ru is f ´1pr´r, rsq, hence is closed since the interval r´r, rs is closed. In practice, the closed sets that we will use will very often be of one of these forms. The following Theorem generalizes Theorem 3.4.5 of [1] to more than one variable. Theorem 3.2.15. Let X Ă Rn be a non-empty compact set and f : X Ñ R a contin- uous function. Then f is bounded and achieves its maximum and minimum, or in other words, there exist x` and x´ in X such that f px`q “ sup xPX f pxq, f px´q “ inf xPX f pxq. Another diﬃculty in working with functions of n ě 2 variables is that for n ě 2, the notion of continuity (or of limit) is much stronger than in dimension 1. One intuitive reason is that there are “many more ways” for a sequence to converge to x P Rn than in R. For instance, all of the following sequences converge to p0, 0q in R2, but the way they do it is quite diﬀerent: (1) (Limit along a ray) Take pcospθq{k, sinpθq{kq, where θ P R is ﬁxed. All these points are on the line with angle θ from the x-axis. (2) (Spiraling limit) Take pcospkq{k, sinpkq{kq; here the angle from the x-axis is k, and there is no special direction of convergence. A priori, the limit of f pxk, ykq could exist but be diﬀerent for each of these sequences, or there could be limits in some directions but not others, the “spiraling” limit may or may not exist even if the “ray” limits exist, etc. Example 3.2.16. Deﬁne f p0, 0q “ 0 and f px, yq “ xy x2 ` y2 . Note that f is continuous when deﬁned on R2ztp0, 0qu, since the denominator is contin- uous and is never zero there. Then f pcospθq{k, sinpθq{kq “ sinpθq cospθq{k2 cos2pθq{k2 ` sin 2pθq{k2 “ cospθq sinpθq so the limit exists for every θ, but its value depends on θ. This implies in particular that the function f is not continuous at p0, 0q, hence is not continuous on Rn. On the other hand, for the spiral, we get f pcospkq{k, sinpkq{kq “ cospkq sinpkq 23 which has no limit as k Ñ `8. 3.3. Partial derivatives We now consider the generalization of derivability in R n. In one variable, we restricted to open intervals to deﬁne the derivative. The analogue in R n is the following: Definition 3.3.1. A subset X Ă Rn is open if, for any x “ px1, . . . , xnq P X, there exists δ ą 0 such that the set ty “ py1, . . . , ynq P Rn : |xi ´ yi| ă δ for all iu is contained in X. In other words: any point of Rn obtained by changing any coordinate of x by at most δ is still in X. The basic example to keep in mind is just X “ Rn (and one may assume at ﬁrst that this is the case for the deﬁnitions of partial derivatives and of the diﬀerential below). The following proposition often leads to an easy way to show that a set is open: Proposition 3.3.2. A set X Ă Rn is open if and only if the complement Y “ tx P Rn : x R Xu is closed. Corollary 3.3.3. If f : Rn Ñ R m is continuous and Y Ă R m is open, then f ´1pY q is open in Rn. Proof. This is because the complement of f ´1pY q is the set of points x P X such that f pxq belongs to the complement of Y , which is closed according to the proposition, so this follows from Proposition 3.2.13. □ The following examples are the most important open sets for us. Example 3.3.4. (1) The empty set and Rn are open. In fact, they are the only two sets in Rn that are both open and closed (this is intuitively reasonable, although a rigorous proof requires some care). (2) The open ball of center x0 and radius r D “ tx P Rn : }x ´ x0} ă ru is open in Rn. We can check this both using the deﬁnition and the corollary. For the deﬁnition: let x P D and deﬁne s “ }x ´ x0} ă r. Put δ0 “ 1 2pr ´ sq ą 0. Then any z P Rn such that }z} ă δ0 satisﬁes }x ` z ´ x0} ď }x ´ x0} ` }z} ď s ` δ0 ă r. Deﬁne δ “ δ0{ ? n. If |xi ´ yi| ă δ for all i, then putting z “ y ´ x, we get }y ´ x} “ apy1 ´ x1q2 ` ¨ ¨ ¨ ` pyn ´ xnq2 ă δ? n “ δ0 so }y ´ x0} “ }x ` z ´ x0} ă δ. Using the corollary, let f pxq “ }x ´ x0}, which is a continuous function; then D “ f ´1ps ´ r, rrq, so it is open. On the other hand, the closed ball ∆ is not open: for instance, if we take x “ x0 ` pr, 0, . . . , 0q, then for any δ ą 0, the point x0 ` pr ` δ, 0, . . . , 0q is not in ∆. (3) Let I1, . . . , In be open intervals in R. Then I1 ˆ ¨ ¨ ¨ ˆ In is open in Rn. 24 (4) Arguing as in Example (2), we see more generally that X Ă Rn is open if and only if, for any x P X, there exists δ ą 0 such that the open ball of center x and radius δ is contained in X. Now we can deﬁne partial derivatives. Definition 3.3.5. Let X Ă Rn be an open set. Let f : X Ñ Rm be a function. Let 1 ď i ď n. We say that f has a partial derivative on X with respect to the i-th variable, or coordinate, if for all x0 “ px0,1, . . . , x0,nq P X, the function deﬁned by gptq “ f px0,1, . . . , x0,i´1, t, x0,i`1, . . . , x0,nq on the set I “ tt P R : px0,1, . . . , x0,i´1, t, x0,i`1, . . . , x0,nq P Xu is diﬀerentiable at t “ x0,i. Its derivative g1px0,iq at x0,i is denoted Bf Bxi px0q, Bxif px0q, Bif px0q. Intuitively, this deﬁnition means that we “freeze” all variables except the i-th one, and consider the derivative of the corresponding function of one variable. We recall once more that if m ě 2, so that gptq “ pg1ptq, . . . , gmptqq for some real-valued functions gj : I Ñ R, then g is diﬀerentiable if and only if all gj are diﬀerentiable, and that g1ptq “ pg1 1ptq, . . . , g1 mptqq. Remark 3.3.6. (1) Note that by deﬁnition of an open set, the set I always contains an open interval containing x0,i, so that it makes sense to ask that g be diﬀerentiable at x0,i. (2) The notation Bxif can sometimes be confusing. It is important to remember that here xi refers to a variable, and not to a speciﬁc real value. This is especially a problem when one writes a value of the partial derivative at a point: in Bx1f px1, . . . , xnq, we think of x1 in the partial derivative as a variable (indicating for which variable we compute the derivative), but we think of px1, . . . , xnq as a point in Rn where we evaluate the partial derivative. Writing Bif pxq is sometimes clearer for this reason. It follows immediately from the deﬁnition that partial derivatives have all the prop- erties of the usual derivative of a function of one variable. Proposition 3.3.7. Consider X Ă Rn open and f , g functions from X to Rm. Let 1 ď i ď n. (1) If f and g have partial derivatives with respect to the i-th coordinate on X, then f ` g also does, and Bxipf ` gq “ Bxipf q ` Bxipgq. (2) If m “ 1, and if f and g have partial derivatives with respect to the i-th coordinate on X, then f g also does and Bxipf gq “ Bxipf q g ` f Bxipgq. Furthermore, if gpxq ­“ 0 for all x P X, then f {g has a partial derivative with respect to the i-th coordinate on X, with Bxipf {gq “ pBxipf q g ´ f Bxipgqq{g2. Moreover, computing partial derivatives is as easy as computing ordinary derivatives. 25 Example 3.3.8. (1) Let f be linear from Rn to R. Then if we write f px1, . . . , xnq “ a1x1 ` ¨ ¨ ¨ ` anxn, then we see that Bif pxq “ ai for all x P Rn and 1 ď i ď n. (2) Let f be a function with separated variables, say f pxq “ f1px1q ¨ ¨ ¨ fnpxnq. If each fi is diﬀerentiable on R, then f has partial derivatives, which are Bif pxq “ f1px1q ¨ ¨ ¨ fi´1pxi´1qf 1 i pxiqfi`1pxi`1q ¨ ¨ ¨ fnpxnq (so all partial derivatives also have separated variables). (3) Let f px, y, zq “ cospxy2z3q ´ 12x2. Then we have Bxf “ ´y2z3 sinpxy2z3q ´ 24x Byf “ ´2xyz3 sinpxy2z3q Bzf “ ´3xy2z2 sinpxy2z3q. (4) Let f px, yq be the function of Example 3.2.16. Since f p0, yq “ f px, 0q “ 0, we obtain the partial derivatives Bxf p0, 0q “ Byf p0, 0q “ 0. Definition 3.3.9. Let X Ă Rn open and f : X Ñ Rm a function with partial derivatives on X. Write f pxq “ pf1pxq, . . . , fmpxqq. For any x P X, the matrix Jf pxq “ pBxj fipxqq1ďiďm 1ďjďn with m rows and n columns is called the Jacobi matrix of f at x. Example 3.3.10. Let f : R2 Ñ R3 be deﬁned by f px, yq “ ¨ ˝cospx2 ` yq esinpπxyq ´ 1 y ` 1 x2`1 ˛ ‚ (the variables px, yq should be thought of as a column vector). Then the function has partial derivatives, and for any px, yq P R 2, the Jacobi matrix is Jf px, yq “ ¨ ˝ ´2x sinpx2 ` yq ´ sinpx2 ` yq πy cospπxyqesinpπxyq πx cospπxyqesinpπxyq ´2x p1`x2q2 1 ˛ ‚ (the ﬁrst column has the partial derivatives with respect to x, and the second with respect to y). If we want to evaluate this at some point, say p1, 0q, we obtain Jf p1, 0q “ ¨ ˝ ´2 sinp1q ´ sinp1q 0 ´π ´1 2 1 ˛ ‚ 26 As is clear from examples, often the partial derivatives Bxif of a function themselves admit partial derivatives Bxj pBxif q, and so on. Some of thee notation that are used for multiple partial derivatives are: BxipBxif q “ Bx2 i f “ B2f Bx2 i , BxipBxj f q “ Bxi,xj f “ B2f BxiBxj . Definition 3.3.11 (Gradient, Divergence). Let X Ă Rn be open. (1) Let f : X Ñ R be a function. If all partial derivatives of f exist at x0 P X, then the column vector ¨ ˝Bx1f px0q ¨ ¨ ¨ Bxnf px0q ˛ ‚ is called the gradient of f at x0, and is denoted ∇f px0q. (2) Let f “ pf1, . . . , fnq : X Ñ Rn be a function with values in Rn such that all partial derivatives of all coordinates fi of f exist at x0 P X. Then the real number TrpJf px0qq “ nÿ i“1 Bxifipx0q, the trace of the Jacobi matrix, is called the divergence of f at x0, and is denoted divpf qpx0q. 3.4. The diﬀerential Although partial derivatives are very easy to deﬁne and compute, their existence is not the correct analogue of diﬀerentiability. To be more precise, we want this analogue to provide a way to approximate a function by a linear map, just as the fact that a function f : R Ñ R is diﬀerentiable with derivative a at 0 means that f pxq “ f p0q ` ax ` Epxq where the “error” Epxq has the property that limxÑ0 Epxq{x “ 0, so that the aﬃne-linear map gpxq “ f p0q ` ax “ f p0q ` f 1p0qx is a good approximation to f pxq when x is close to 0. If we consider a function f : R n Ñ R with n ě 2, the problem is that Bx1f p0q, for instance, only gives some information on how f behaves when the ﬁrst variable tends to 0, the others being ﬁxed. It is quite believable that, for certain functions, we will not be able to deduce an approximation for f pxq when x is close to 0 from the approximations along the coordinate axes! Example 3.4.1. (1) Let f px, yq be the function of Examples 3.2.16 and 3.3.8. We have seen that Bxf p0, 0q “ Byf p0, 0q “ 0, but from Example 3.2.16, the function f is not continuous at p0, 0q! So the partial derivatives can not be combined in any reasonable manner to give a good approximation of f for px, yq close to p0, 0q. (2) Let g : R2 Ñ R be deﬁned by gp0, 0q “ 0 and gpx, yq “ xy a x2 ` y2 (see Figure 3.3 for its graph). This function is now continuous at p0, 0q because, for px, yq ­“ p0, 0q, we have |gpx, yq| ď 1 2px2 ` y2q a x2 ` y2 “ 1 2 a x2 ` y2 Ñ 0 27 Figure 3.3. Graph of gpx, yq “ xy{ a x2 ` y2 as px, yq Ñ p0, 0q. Since gpx, 0q “ gp0, yq “ 0, the partial derivatives exist and are both 0 again. But if we compute gpr cospθq, r sinpθqq as r Ñ 0, corresponding to approximating g along a line with angle θ with respect to the x axis, then we get for r ą 0 the formula gpr cospθq, r sinpθqq “ r2 cospθq sinpθq r “ r cospθq sinpθq, which is a linear approximation, in terms of r, but one that cannot be constructed reasonably from the values of the partial derivatives. It turns out that the correct deﬁnition of the generalization of diﬀerentiability is to take the approximation property as the deﬁning condition. Definition 3.4.2. Let X Ă Rn be open and f : X Ñ Rm be a function. Let u be a linear map Rn Ñ R m and x0 P X. We say that f is diﬀerentiable at x0 with diﬀerential u if lim xÑx0 x­“x0 1 }x ´ x0}pf pxq ´ f px0q ´ upx ´ x0qq “ 0 where the limit is in Rm. We then denote df px0q “ u. If f is diﬀerentiable at every x0 P X, then we say that f is diﬀerentiable on X. This deﬁnition means that, close to x0, we can approximate f pxq by the aﬃne-linear function g : R n Ñ Rm deﬁned by gpxq “ f px0q ` upx ´ x0q, with an error that becomes much smaller than }x ´ x0} as x gets close to x0. Remark 3.4.3. (1) If we write f pxq “ pf1pxq, . . . , fmpxqq and similarly write upxq “ pu1pxq, . . . , umpxqq where f1, . . . , fm are functions X Ñ R and u1, . . . , um are linear maps Rn Ñ R, then the deﬁnition of limit shows that f is diﬀerentiable with diﬀerential u if and only if, for each i, the function fi is diﬀerentiable with diﬀerential ui. Furthermore, a linear map u : Rn Ñ R (or linear form) has the simple form upx1, . . . , xnq “ a1x1 ` ¨ ¨ ¨ ` anxn 28 for some coeﬃcients a1, . . . , an in R. So, in the case m “ 1, the approximation for f pxq is f px0q ` a1x1 ` ¨ ¨ ¨ ` anxn, and depends only on the n numbers pa1, . . . , anq. These are the analogues of the single derivative f 1px0q when n “ 1, and we will see that these coeﬃcients are just the values of the partial derivatives of f at x0. (2) Suppose n “ 1 and m “ 1. Then the deﬁnition is equivalent to 0 “ lim xÑx0 x­“0 f pxq ´ f px0q ´ apx ´ xq x ´ x0 “ lim xÑx0 x­“0 f pxq ´ f px0q x ´ x0 ´ a where a is the (unique) coeﬃcient representing the linear map u : R Ñ R (because }x ´ x0} “ |x ´ x0| and a function tends to 0 if and only if its absolute value does). In other words, f is diﬀerentiable according to the deﬁnition above if and only if f is diﬀerentiable at x0 in the sense of Analysis I, with derivative f 1px0q “ a. The following proposition shows that diﬀerentiable functions have some good proper- ties: they are continuous, and have partial derivatives, which can be computed easily in terms of the diﬀerential. Proposition 3.4.4. Let X Ă R n be open and f : X Ñ Rm be a function that is diﬀerentiable on X. (1) The function f is continuous on X. (2) The function f admits partial derivatives on X with respect to each variable. (3) Assume that m “ 1. Let x0 P X, and let upx1, . . . , xnq “ a1x1 ` ¨ ¨ ¨ ` anxn be the diﬀerential of f at x0. We then have Bxif px0q “ ai for 1 ď i ď n. Proof. (1) Let x0 P X. For x ­“ x0, write f pxq “ f px0q ` upx ´ x0q ` Epxq for some Epxq P R. According to the deﬁnition, we have lim xÑx0 Epxq }x ´ x0} “ 0, which implies that Epxq Ñ 0 as x Ñ x0. Since u is continuous and up0q “ 0, we deduce that lim xÑx0 f pxq “ f px0q, which means that f is continuous on X. (2) and (3): we consider only the case n “ 2, m “ 1 and i “ 1 for simplicity, using px, yq for the coordinates. Let px0, y0q P X. We deﬁne Epx, yq by f px, yq “ f px0, y0q ` a1px ´ x0q ` a2py ´ y0q ` Epx, yq. It follows that if we put y “ y0 and vary x only, we have f px, y0q ´ f px0, y0q x ´ x0 “ a1 ` 0 ` Epx, y0q x ´ x0 . 29 Since |x ´ x0| “ }px, yq ´ px0, y0q}, the deﬁnition implies that lim xÑx0 Epx, y0q x ´ x0 “ 0, and therefore lim xÑx0 f px, y0q ´ f px0, y0q x ´ x0 “ a1, which means that the partial derivative Bxf exists at px0, y0q and is equal to a1. □ Example 3.4.5. (1) The simplest example of a diﬀerentiable function is an aﬃne linear function f pxq “ y0 ` upxq where y0 P R m and u : Rn Ñ Rm is linear. Indeed, since f px0q “ y0 ` upx0q, we get f pxq “ y0 ` upxq “ f px0q ` upx ´ x0q which means that f is diﬀerentiable at all x0, with diﬀerential df px0q “ u, independent of x0. (2) Consider the function g : R2 Ñ R of Example 3.4.1 (2). This is not diﬀerentiable at p0, 0q. Indeed, if it were, then since the two partial derivatives at p0, 0q are equal to 0 (as we saw earlier), the proposition shows that the diﬀerential u “ df p0, 0q would be the zero linear map. But then we ﬁnd that 1 }px, yq}pgpx, yq ´ gp0, 0q ´ upx, yqq “ gpx, yq }px, yq} “ xy x2 ` y2 , and from Example 3.2.16, this quantity does not have a limit as px, yq Ñ p0, 0q. (3) Consider the case m “ 1 in general. If a function f : X Ñ R is diﬀerentiable, then according to Proposition 3.4.4 (3), its diﬀerential at x0 is the linear map u : Rm Ñ R such that upt1, . . . , tnq “ nÿ i“1 Bf Bxi px0qti for all t “ ptiq P Rn. A convenient way to represent this is to write uptq “ ∇f px0q ¨ t, where ∇f px0q is the gradient of f at x0, and x ¨ y denotes the scalar product of two vectors: x ¨ y “ x1y1 ` ¨ ¨ ¨ ` xnyn. The aﬃne linear map that approximates f is then gpxq “ f px0q ` ∇f px0q ¨ px ´ x0q. The next issue is to know when a function is diﬀerentiable and to construct more diﬀerentiable functions (they would not be useful if they didn’t exist). For this purpose, there are two basic results: (1) showing that various operations preserve diﬀerentiability; (2) giving a supply of functions for which it is easy to know that they are diﬀerentiable. Proposition 3.4.6. Let X Ă R n be open, f : X Ñ R m and g : X Ñ Rm diﬀerentiable functions on X. (1) The function f ` g is diﬀerentiable with diﬀerential dpf ` gq “ df ` dg, and if m “ 1, then f g is diﬀerentiable. (2) If m “ 1 and if gpxq ­“ 0 for all x P X, then f {g is diﬀerentiable. The next proposition immediately implies that most elementary functions are diﬀer- entiable: 30 Proposition 3.4.7. Let X Ă Rn be open, f : X Ñ Rm a function on X. If f has all partial derivatives on X, and if the partial derivatives of f are continuous on X, then f is diﬀerentiable on X, with diﬀerential determined by its partial derivatives, in the sense that the matrix of the diﬀerential df px0q, with respect to the canonical basis of Rn and Rm, is the Jacobi matrix of f at x0. Example 3.4.8. (1) Let n “ 2, m “ 1 and consider f px, yq “ cospx ` y2q ´ xe y. Pick px0, y0q “ pπ{4, 0q. The function f is diﬀerentiable on R 2 because its has partial derivatives and Jacobi matrix Jf px, yq “ p´ sinpx ` y2q ´ e y, ´2y sinpx ` y2q ´ xe yq where the components are continuous functions on R 2. At px0, y0q, the Jacobi matrix becomes Jf pπ{4, 0q “ p´ sinpπ{4q ´ 1, 0 ´ π{4q “ ´p1 ` 1{? 2, π{4q. so that the diﬀerential u “ df px0, y0q is the linear form upx, yq “ ´p1 ` 1{ ? 2qx ` πy{4, and the aﬃne-linear approximation gpx, yq to f px, yq close to px0, y0q is given by gpx, yq “ f pπ{4, 0q ` upx ´ x0, y ´ y0q “ ? 2 2 ´ π 4 ´ ´ 1 ` 1 ? 2 ¯´ x ´ π 4 ¯ ` πy 4 . (2) Any polynomial in n variables is diﬀerentiable on Rn. Its partial derivatives are also polynomials in n variables. (3) If f1, . . . , fn are functions of class C 1 on R (so that their derivatives are deﬁned and continuous), then the function f pxq “ f1px1q ¨ ¨ ¨ fnpxnq is diﬀerentiable on Rn. The other important rule about diﬀerentiable functions is the chain rule. Proposition 3.4.9 (Chain rule). Let X Ă Rn be open, Y Ă Rm be open, and let f : X Ñ Y and g : Y Ñ R p be diﬀerentiable functions. Then g ˝ f : X Ñ Rp is diﬀerentiable on X, and for any x P X, its diﬀerential is given by the composition dpg ˝ f qpx0q “ dgpf px0qq ˝ df px0q. In particular, the Jacobi matrix satisﬁes Jg˝f px0q “ Jgpf px0qqJf px0q where the right-hand side is a matrix product. Example 3.4.10. (1) To see this formula concretely, assume n “ m “ p “ 2, and write f px, yq “ ˆf1px, yq f2px, yq ˙ , gpu, vq “ ˆ g1pu, vq g2pu, vq. ˙ Then the Jacobi matrices are Jf px, yq “ ˆBxf1 Byf1 Bxf2 Byf2 ˙ , Jgpu, vq “ ˆBug1 Bvg1 Bug2 Bvg2 ˙ . The matrix product JgJf gives us the Jacobi matrix of g ˝ f , namely Jg˝f px, yq “ ˆBug1Bxf1 ` Bvg1Bxf2 Bug1Byf1 ` Bvg1Byf2 Bug2Bxf1 ` Bvg2Bxf2 Bug2Byf1 ` Bvg2Byf2 ˙ . 31 When evaluating such a Jacobi matrix at a given point x0, it must be remembered that all partial derivatives of f are evaluated at x0, and all partial derivatives of g are evaluated at y0 “ f px0q. (2) Suppose p “ 1, so that g ˝ f is real-valued. For the partial derivative of g ˝ f with respect to x1, for instance, we get Bpg ˝ f q Bx1 px0q “ Bg By1 py0q Bf1 Bx1 px0q ` Bg By2 py0q Bf2 Bx1 px0q ` ¨ ¨ ¨ ` Bg Bym py0q Bfm Bx1 px0q, or in other words Bpg ˝ f q Bx1 px0q “ mÿ j“1 Bg Byj py0q Bfj Bx1 px0q, where y0 “ f px0q and the variables in Rm are py1, . . . , ymq. (A way to remember the formula is to think that the j-th coordinate variable yj in the “denominator” of the partial derivative for g corresponds to the “numerator” fj, which is the j-th coordinate of f ). (3) Let f , g : Rn Ñ R be two functions. Deﬁne hpx, yq “ pf px, yq, gpx, yqq and mpu, vq “ uv, so that m ˝ hpx, yq “ f px, yqgpx, yq. The Jacobi matrices of h and m are Jhpx, yq “ ˆBxf Byf Bxg Byg ˙ , Jmpu, vq “ pv, uq (the Jacobi matrix for m is just a row vector). It follows therefore that Bpf gq Bx “ vBxf ` uBxg, evaluated at px, yq, which (since we must replace u and v by the coordinates of hpx, yq) means that Bpf gq Bx “ gpx, yqBxf px, yq ` f px, yqBxgpx, yq, a formula that we can recognize as the Leibniz rule. (4) Let I Ă R be an open interval. Consider f : I Ñ Rm and g : R m Ñ R, so that the composite is a function g ˝ f : I Ñ R. If f is diﬀerentiable on I (which means that each component is a diﬀerentiable function of one variable) and if g is diﬀerentiable on Rn, then g ˝ f is diﬀerentiable on I, and its derivative, which is just the partial derivative with respect to the only variable is determined by pg ˝ f q 1ptq “ dgpf ptqq f 1ptq, i.e., the linear map dgpf ptqq : Rm Ñ R (whose coeﬃcients are the partial derivatives of g), applied to the vector f 1ptq P R m. If we write f ptq “ pf1ptq, . . . , fmptqq, this is just pg ˝ f q 1ptq “ Bg By1 pf ptqqf 1 1ptq ` ¨ ¨ ¨ ` Bg Bym pf ptqqf 1 mptq. Another convenient expression as a scalar product is just pg ˝ f q 1ptq “ ∇gpf ptqq ¨ f 1ptq. 32 Figure 3.4. Graph of f px, yq “ a 1 ´ x2 ´ y2 and tangent space at p1{2, 1{3q Figure 3.5. Graph of gpx, yq “ xy{ a 1 ´ x2 ´ y2 and horizontal plane Definition 3.4.11. Let X Ă Rn be open and f : X Ñ Rm a function that is diﬀer- entiable. Let x0 P X and u “ df px0q be the diﬀerential of f at x0. The graph of the aﬃne linear approximation gpxq “ f px0q ` upx ´ x0q from R n to Rm, or in other words the set tpx, yq P Rn ˆ Rm : y “ f px0q ` upx ´ x0q is called the tangent space at x0 to the graph of f . The tangent space at a point generalizes the tangent line for the graph of a function of one variable. It is the aﬃne subspace in Rm that is “the best” ﬁt to the graph of the function f around x0. It is an aﬃne space of dimension n, since it can be parameterized by x P Rn, which determines uniquely the corresponding point y “ f px0q ` upx ´ x0q such that px, yq belongs to the tangent space. We can also write the points of the tangent space in the form px, yq “ px0, f px0qq ` px ´ x0, upx ´ x0qq which shows that it is the set of points px0, y0q ` w, where w belongs to the graph of u, which is a linear subspace of dimension n in Rn`m. We say that this linear subspace is the linear subspace parallel to the tangent space at x0. Example 3.4.12. (1) Figure 3.4 illustrates (from two diﬀerent angles) the graph of the function f px, yq “ a 1 ´ x2 ´ y2 (which is demi-sphere of radius 1 centered at p0, 0q) and the tangent space at the point px, yq “ p1{2, 1{3q. (2) Consider again the function gpx, yq “ xy{ a x2 ` y2 of Example 3.4.5. Figure 3.5 shows the graph of g and the horizontal plane z “ 0 in R3 that “would be” the tangent plane if the function was diﬀerentiable at p0, 0q. 33 (3) Deﬁne f px, yq “ ax2 ` y2. Let px0, y0q “ p3, 4q. The tangent plane to the graph of f at the point px0, y0q is the set of all px, y, zq in R3 such that z “ f p3, 4q ` ∇f p3, 4q ¨ px ´ 3, y ´ 4q. We have f p3, 4q “ ? 9 ` 16 “ 5, and the gradient at an arbitrary point is given by ∇f px, yq “ ˜ x? x2`y2 y? x2`y2 ¸ so that ∇f px0, y0q “ p3{5, 4{5q. The equation of the tangent plane becomes z “ 5 ` 3px ´ 3q{5 ` 4py ´ 4q{5. If a function is diﬀerentiable at a point x0 P Rn, one meaning of the linear map u “ df px0q is that the value upvq, for a vector v P Rn, gives the “directional derivative” in the direction v, in the sense of the following deﬁnition: Definition 3.4.13. Let X Ă Rn be an open set and let f : X Ñ Rm be a function. Let v P Rn be a non-zero vector and x0 P X. We say that f has directional derivative w P Rm in the direction v, if the function g deﬁned on the set I “ tt P R : x0 ` tv P Xu by gptq “ f px0 ` tvq has a derivative at t “ 0, and this is equal to w. In other words, this means that the limit lim tÑ0 t­“0 f px0 ` tvq ´ f px0q t exists and is equal to w. Remark 3.4.14. It is easy to see that because X is open, the set I contains an open interval s ´ δ, δr for some δ ą 0, so that the derivability of g at t “ 0 makes sense. Proposition 3.4.15. Let X Ă Rn be an open set and let f : X Ñ Rm be a diﬀeren- tiable function. Then for any x P X and non-zero v P Rn, the function f has a directional derivative at x0 in the direction v, equal to df px0qpvq. Remark 3.4.16. (1) What is important to notice in this proposition, is that the values of the directional derivatives are linear with respect to the vector v. So if we know the directional derivatives w1 and w2 in directions v1 and v2, then it follows that the directional derivative in direction v1 ` v2 is w1 ` w2. (2) If we take v to be the vector ei of the canonical basis of Rn, then the directional derivative in direction ei is simply the partial derivative with respect to the i-th variable. Example 3.4.17. (1) Consider the function gpx, yq “ xy{ a x2 ` y2 of Example 3.4.5, (2). Although it is not diﬀerentiable at p0, 0q, it has directional derivatives in all directions pu, vq ­“ p0, 0q, since gp0, 0q “ 0 and gptu, tvq ´ gp0, 0q t “ uv ? u2 ` v2 . 34 Figure 3.6. Directional derivative (In fact, this is just gpu, vq). But this expression is not linear with respect to pu, vq. (2) Suppose that m “ 1. Then the directional derivative in direction u is a real number which has the following geometric meaning: intersect the graph of f in Rn`1 with the plane perpendicular to the hyperplane Rn “ Rn ˆ t0u, which passes through px0, 0q and px0 ` v, 0q. This gives a set Γ which is the graph of the function gptq “ f px0 ` tvq. Now, if v has length 1, then the slope of the tangent line to Γ at px0, f px0qq is equal to the directional derivative at that point. For instance, deﬁne f px, yq “ cospxyq and consider the point p0, ´1q and the direction p1, 1q. Figure 3.6 displays the graph and the corresponding perpendicular plane. We now suppose m “ 1. Let f : X Ñ R be diﬀerentiable, and let x0 P X. The tangent space at x0 to the graph of f is the set of px, yq P Rn ˆ R such that y “ f px0q ` ∇f px0q ¨ px ´ x0q. This is an aﬃne space of dimension n, and the corresponding linear subspace in Rn is the graph of the linear map x ÞÑ ∇f px0q ¨ x, in other words the set of all px, yq P Rn ˆ R such that y “ ∇f px0q ¨ x. A good way to visualize or interpret this linear space is to observe that it is the set of vectors orthogonal to the vector n0 “ p´∇f px0q, 1q P Rn ˆ R. Indeed, we have y ´ ∇f px0q ¨ x “ px, yq ¨ n0 where the right-hand side is now a scalar product in Rn`1. The gradient has another important interpretation, which generalizes the fact that for a function of one variable, the sign of the derivative indicates whether the function is (locally) increasing or decreasing. Precisely, suppose that the gradient vector ∇f px0q is non-zero. Then the vector w0 “ ∇f px0q points in the “direction of greatest increase” of the function f . In other words, it points in the direction where the directional derivative is the largest. This follows from the fact that f pxq ´ f px0q “ ∇f px0q ¨ px ´ x0q ` (small error) and that we know (Cauchy-Schwarz inequality) that |∇f px0q ¨ px ´ x0q| ď }∇f px0q} }x ´ x0} “ }w0} }x ´ x0} 35 Figure 3.7. Some level curves of f px, yq “ x3 ´ xy ` 2y with equality if x ´ x0 is proportional to w0, which corresponds to varying x in the direction of w0. Another way to see this is in terms of directional derivatives. Let v P Rn be a vector of length one. If we remember that the scalar product of two vectors in Rn is the product of their lengths with the cosine of the angle, the directional derivative of f in the direction v at x0 is ∇f px0q ¨ v “ }∇f px0q} cospθq where θ P r0, πs is the angle between the gradient and the direction v. This is maximal when θ “ 0, which means that v is proportional to ∇f px0q. Example 3.4.18. Think of the graph of f : R2 Ñ R as giving the height of a mountain above the point with coordinates px, yq of the map of a region of the earth. Then the gradient ∇f px0q is a vector in R2, and it points in the direction in which the height grows faster: if one wants to climb the slope as quickly as possible, one should walk always in the direction of the gradient. Yet another related geometric property of the gradient is that it is perpendicular to the “level sets” determined by an equation of the form f pxq “ c, where c P R is a ﬁxed real number. To be more precise, ﬁx c, and denote by Lc the set of all x P X where f pxq “ c. Let x0 P Lc be any point in this set. Then, for any diﬀerentiable function of one variable γ : s ´ 1, 1rÑ Rn such that f pγptqq “ c for all t P I and γp0q “ x0, the gradient ∇f px0q is orthogonal in Rn to the vector γ1p0q, which is “tangent” to the level set. This is simply because, by the Chain Rule, we have the relation 0 “ pf ˝ γq 1p0q “ ∇f px0q ¨ γ1p0q. Example 3.4.19. The simplest example is f px, yq “ x2 ` y2. Then the level sets Lc are empty if c ă 0, a single point if c “ 0, and a circle of radius ? c if c ą 0. In this last case, the gradient vector at any point of Lc is p2x, 2yq, and therefore points in the direction orthogonal to the circle. 36 3.5. Higher derivatives We can often straightforwardly compute partial derivatives of a function f : Rn Ñ Rm, and check that not only they exist, and are continuous, but also themselves admit further continuous partial derivatives, etc. This leads naturally to the notion of function of class C k. Definition 3.5.1. Let X Ă Rn be open and f : X Ñ Rm. We say that f is of class C 1 if f is diﬀerentiable on X and all its partial derivatives are continuous. The set of functions of class C 1 from X to Rm is denoted C 1pX; Rmq. Let k ě 2. We say, by induction, that f is of class C k if it is diﬀerentiable and each partial derivative Bxif : X Ñ Rm is of class C k´1. The set of functions of class C k from X to Rm is denoted C kpX; Rmq. If f P C kpX; Rmq for all k ě 1, then we say that f is of class C 8. The set of such functions is denoted C 8pX; Rmq. In practical terms, this means that one has to check all possible combinations of k derivatives, with respect to any combination of k variables, and always obtain continuous functions. Example 3.5.2. (1) If f pxq “ pf1pxq, . . . , fmpxqq, then f is of class C k if and only if each fi is of class C k. (2) If f , g are of class C k, then so is f ` g; if m “ 1, then so is f g, and if gpxq ­“ 0 for all x P X, then so is f {g of class C k. (3) If f pxq “ f1px1q ¨ ¨ ¨ fnpxnq has separated variables, and if fi is of class C k, then f is of class C k. (4) Any polynomial in n variables is of class C 8. (5) Any partial derivative is a linear operation on the functions. (6) Suppose that f is of class C k, and that f pXq Ă Y , where Y Ă Rm is open, and that g : Y Ñ R p is also of class C k. Then the composite g ˝ f is also of class C k. This follows, by induction on k, from the chain rule that expresses partial derivatives of g ˝ f in terms of partial derivatives of f and g. Suppose that k “ 2. Then, in order to show that a function f is of class C 2, we ﬁrst check that f is diﬀerentiable with continuous partial derivatives. There are n such checks to make since f has n partial derivatives. Next there are apparently n 2 second order derivatives, namely Bx1pBx1f q, Bx1pBx2f q, ¨ ¨ ¨ Bx1pBxnf q, until BxnpBx1f q, BxnpBx2f q, ¨ ¨ ¨ BxnpBxnf q. However, if we do it in practice, we see that these derivatives are not independent at all. Example 3.5.3. Let f px, yq “ e´x2? y for x P R, y ą 0. Then ∇f px, yq “ p´2x? y expp´x2? yq, ´ x2 2 ? y expp´x2? yqq. 37 Now we compute the four partial derivatives of order 2: Bx2f “ ´2 ? y expp´x2? yq ` 4x2? y expp´x2? yq Bxyf “ ´ x ? y expp´x2? yq ` x3 ? y expp´x2? yq Byxf “ ´ x ? y expp´x2? yq ` x3 ? y expp´x2? yq By2f “ x2 4y3{2 expp´x2? yq ` x4 4y expp´x2? yq. We see here that Bxyf “ Byxf . This is a general fact. Proposition 3.5.4 (Mixed derivatives commute). Let k ě 2. Let X Ă Rn be open and let f : X Ñ R m be a function of class C k. Then the partial derivatives of order k are independent of the order in which the partial derivatives are taken: for any variables x and y, we have Bx,yf “ By,xf, and for any variables x, y, z, we have Bx,y,zf “ Bx,z,yf “ By,z,xf “ Bz,x,yf “ ¨ ¨ ¨ etc... Example 3.5.5. (1) To convince oneself that this should be true, it is best to look at a monomial ﬁrst. Say f px, y, zq “ xaybzc. Then Bx,yf “ abxa´1yb´1zc “ By,xf and Bx,y,zf “ abcxa´1yb´1zc´1 is the same however we order x, y and z when taking the derivatives. (2) Let k “ 2. In order to ensure that Bx,yf “ By,xf , it is essential to know that f is of class C 2 (so that all partial derivatives of order 2 are continuous), and there are counterexamples otherwise. For instance, one can easily check that f px, yq “ xypx2 ´ y2q x2 ` y2 , px, yq ­“ 0, f p0, 0q “ 0 deﬁnes a function R2 Ñ R which is diﬀerentiable (with ∇f p0, 0q “ 0) and admits partial derivatives of order ď 2, but at p0, 0q, we have Bx,yf p0, 0q “ 1, By,xf p0, 0q “ ´1. In polar coordinates, we have f px, yq “ r sinp4θq{4. Because of the symmetry, we introduce a more compact notation for mixed derivatives of “large order”. If we want to take a derivative of order k, we select the ﬁrst variable (say xi1), compute the partial derivative, then select a second (say xi2), compute the second derivative Bxi2 ,xi1 , etc, up to the ik-th variable. But, provided Proposition 3.5.4 applies (i.e., f is of class C k), the resulting partial derivative Bxik Bxik´1 ¨ ¨ ¨ Bxi1 f 38 Figure 3.8. Function with non-symmetric mixed second derivatives only depends on how many times we took the derivative with respect to each variable. In other words, let m1 be the number of indices j such that ij “ 1, . . . , mn be the number of j such that ij “ n. Then Bxik Bxik´1 ¨ ¨ ¨ Bxi1 f “ Bxm1 1 ,xm2 2 ,...,xmn n f. Let m “ pm1, . . . , mnq. This is a vector of non-negative integers, with m1 ` ¨ ¨ ¨ ` mn “ k (since, in total, we have taken k derivatives). We may use any of the following notation for these expressions: Bxm1 1 ,...,xmn n f “ Bkf Bxm “ Bm x f “ Dmf “ Bmf. Remark 3.5.6. The linearity of the partial derivatives means that Bm x paf1 ` bf2q “ aBm x f1 ` bBm x f2 whenever both partial derivatives on the right-hand side exist. Example 3.5.7. Suppose n “ 3 and k “ 4. There are then 15 possible derivatives of order 4, corresponding to the tuples m “ p4, 0, 0q, m “ p3, 1, 0q, m “ p3, 0, 1q, m “ p2, 2, 0q, m “ p2, 1, 1q m “ p2, 0, 2q, m “ p1, 3, 0q, m “ p1, 2, 1q, m “ p1, 1, 2q, m “ p1, 0, 3q m “ p0, 4, 0q, m “ p0, 3, 1q, m “ p0, 2, 2q, m “ p0, 1, 3q, m “ p0, 0, 4q. For instance, m “ p1, 1, 2q corresponds to the derivative B4f BxByB2z . Example 3.5.8. (Laplace operator) Let X be open in Rn, and let f P C 2pXq. The gradient of f belongs to C 1pX; Rnq, so we can compute its divergence (Deﬁnition 3.3.11). We obtain divp∇pf qq “ nÿ i“1 B Bxi ´ Bf Bxi ¯ “ nÿ i“1 B2f Bx2 i . This diﬀerential expression is called the Laplacian of f , and is denoted ∆f . 39 For the case k “ 2, m “ 1, we organize in a matrix the partial derivatives of order 2 of a function X Ñ R, namely the derivatives B2f BxiBxj , where 1 ď i, j ď n. For a function f of class C 2, this matrix will be symmetric. Definition 3.5.9 (Hessian). Let X Ă Rn be open and f : X Ñ R a C 2 function. For x P X, the Hessian matrix of f at x is the symmetric square matrix Hessf pxq “ `Bxi,xj f ˘ 1ďi,jďn . We also sometimes write simply Hf pxq. Example 3.5.10. Let n “ 3 and f px, y, zq “ x2y ´ cospxz3q. Then we compute Bxf “ 2xy ` z3 sinpxz3q, Byf “ x2, Bzf “ 3xz2 sinpxz3q and then we obtain the Hessian by further diﬀerentiation Hessf px, y, zq “ ¨ ˝ 2y ` z6 cospxz3q 2x 3z2 sinpxz3q ` xz6 cospxz3q 2x 0 0 3z2 sinpxz3q ` xz6 cospxz3q 0 6xz sinpxz3q ` 9x2z6 cospxz3q ˛ ‚. 3.6. Change of variable An important application of the chain rule concerns the computation of partial deriva- tives after a change of variable. Here we have an open set U Ă Rn (with variables that we write py1, . . . , ynq, the “new” variables) and a change of variable g : U Ñ X is a map that expresses the variables px1, . . . , xnq in terms of py1, . . . , ynq, i.e., we consider x1 “ g1py1, . . . , ynq, xn “ gnpy1, . . . , ynq. We should think of g as something “ﬁxed” and very standard (such as going to polar coordinates, or to spherical coordinates, etc). Whenever a function f : X Ñ R is given, the composite h “ f ˝ g : U Ñ R is the function f expressed in terms of the “new” variables y. The chain rule then provides a way to express all partial derivatives of h in terms of those of f , and of the Jacobian matrix of the change of variable g. For instance By1h “ Bf Bx1 Bg1 By1 ` ¨ ¨ ¨ ` Bf Bxn Bgn By1 . Here, since we think that g is ﬁxed, the corresponding partial derivatives are known quantities. There are very common abuses of notation that may be very confusing at ﬁrst, but that are extremely convenient: (1) one thinks of f and h as being the same function, simply expressed in diﬀerent coordinate systems, and one writes simply By1f “ Bf Bx1 Bg1 By1 ` ¨ ¨ ¨ ` Bf Bxn Bgn By1 . (2) one thinks of gi as being the variable xi, expressed in terms of the new variables py1, . . . , ynq, and replaces gi by xi, so the expression becomes By1f “ Bf Bx1 Bx1 By1 ` ¨ ¨ ¨ ` Bf Bxn Bxn By1 . 40 Remark 3.6.1. The ﬁrst of these two simpliﬁcation is very natural if we think of a function like “the distance to the origin”, which we can describe without referring to any particular choice of coordinate system. The point of a change of variable is often to go back and forth, and one can solve for y in terms of x, and write down the corresponding relations Bx1f “ Bf By1 By1 Bx1 ` ¨ ¨ ¨ ` Bf Byn Byn Bx1 . In practice, this can be done by solving the linear system of equations represented by the chain rule. Example 3.6.2. One of the most important example is the change of variable to polar coordinates in R2. The polar coordinates are pr, θq P U “s0, `8rˆR (or sometimes U “s0, `8rˆr0, 2πr) and they parameterize the plane minus the origin p0, 0q by # x “ r cos θ y “ r sin θ. In other words, we consider the map g : U Ñ R2 such that gpr, θq “ pr cos θ, r sin θq, and to express a function f : R2 Ñ R in polar coor- dinates means replacing f by h “ f ˝ g : U Ñ R, so that hpr, θq “ f pr cos θ, r sin θq. The Jacobian matrix of the change of variable is given by Jgpr, θq “ ˆcos θ ´r sin θ sin θ r cos θ ˙ (with determinant r). The chain rule leads to the formulas Brh “ cospθqBxf ` sinpθqByf Bθh “ ´r sinpθqBxf ` r cospθqByf (where all partial derivatives of f are evaluated implicitly at pr cos θ, r sin θq.) This is also often expressed as rBrh “ xBxf ` yByf Bθh “ ´yBxf ` xByf. With the short-hand notation discussed earlier, this becomes rBrf “ xBxf ` yByf Bθf “ ´yBxf ` xByf. Solving for Bxf and Byf , we obtain the relations Bxf “ cospθqBrh ´ 1 r sinpθqBθh Byf “ sinpθqBrh ` 1 r cospθqBθh 41 (where all partial derivatives of h are evaluated implicitly at px, yq such that x “ r cos θ, y “ r sin θ), or Bxf “ cospθqBrf ´ 1 r sinpθqBθf(3.2) Byf “ sinpθqBrf ` 1 r cospθqBθf(3.3) in abbreviated form. One can iterate applying these partial derivatives to obtain expressions for higher derivatives. For instance, let us compute the Laplace operator ∆f “ Bx2f ` By2f in polar coordinates (see Example 3.5.8). Using the formula (3.2) twice, we have Bx2f “ cospθqBrpBxf q ´ 1 r sinpθqBθpBxf q “ cospθqBr´ cospθqBrf ´ 1 r sinpθqBθf ¯ ´ 1 r sinpθqBθ´ cospθqBrf ´ 1 r sinpθqBθf ¯ . Computing further these expressions, this gives Bx2f “ cospθq ! cospθqBr2f ` 1 r2 sinpθqBθf ´ 1 r sinpθqBrθf ) ´ 1 r sinpθq! ´ sinpθqBrf ` cospθqBrθf ´ 1 r cospθqBθf ´ 1 r sinpθqBθ2f ) “ cos 2pθqBr2f ` 2 r2 cospθq sinpθqBθf ´ 2 r cospθq sinpθqBrθf ` 1 r sin 2pθqBrf ` 1 r2 sin 2pθqBθ2f. A similar computation using instead (3.3) twice gives the formula By2f “ sin 2pθqBr2f ´ 2 r2 cospθq sinpθqBθf `2 r cospθq sinpθqBrθf `1 r cos 2pθqBrf ` 1 r2 cos 2pθqBθ2f. We conclude that (for a C 2 function f ), we have Bx2f ` By2f “ Br2f ` 1 r Brf ` 1 r2 Bsθ2f. We look at a concrete example. Let f px, yq “ exppx2 ` y2q. The corresponding expression in polar coordinates is hpr, θq “ exppr2q. We can compute the gradient of f and ∆f using the polar coordinates by writing ∇f “ ˆBxf Byf ˙ “ ˆcospθqBrh ´ r´1 sinpθqBθh sinpθqBrh ` r´1 cospθqBθh ˙ “ ˆ2r cos θ exppr2q 2r sin θ exppr2q ˙ “ ˆ2x exppx2 ` y2q 2y exppx2 ` y2q ˙ , and ∆f “ Br2f ` 1 r Brf ` 1 r2 Bθ2f “ Br2pe r2q ` 2er2 “ p2 ` 4r2qer2 ` 2er2 “ 4p1 ` x2 ` y2qe x2`y2. 42 3.7. Taylor polynomials We consider in this section the case m “ 1, and a function f : X Ñ R. The aﬃne- linear approximation for f pxq when x is close to a point x0 P X involves only the ﬁrst derivatives of f , and is given by T1f px ´ x0; x0q, where T1f py; x0q is the function on Rn such that T1f py; x0q “ f px0q ` ∇f px0q ¨ y “ f px0q ` nÿ i“1 Bf Bxi px0qyi. As a function of y, this is a polynomial of degree ď 1 (it is of degree exactly 1 unless ∇f px0q is zero). In the case n “ 1, we know that we obtain better approximations to a function when considering higher derivatives, and building the Taylor polynomials of the function, deﬁned by Tkf py; x0q “ f px0q ` f 1px0qy ` f 2px0q 2 y2 ` ¨ ¨ ¨ ` f pkqpx0q k! yk, in the sense that f pxq “ Tkf px ´ x0; x0q ` (remainder) with, roughly speaking, a remainder that is much smaller than |x ´ x0|k when x Ñ x0. The same is true in general, but the Taylor polynomials have now n variables. Definition 3.7.1 (Taylor polynomials). Let k ě 1 be an integer. Let f : X Ñ R be a function of class C k on X, and ﬁx x0 P X. The k-th Taylor polynomial of f at the point x0 is the polynomial in n variables of degree ď k given by Tkf py; x0q “ f px0q ` nÿ i“1 Bf Bxi px0qyi ` ¨ ¨ ¨ ` ÿ m1`¨¨¨`mn“k 1 m1! ¨ ¨ ¨ mn! Bkf Bxm1 1 ¨ ¨ ¨ Bxmn n px0qym1 1 ¨ ¨ ¨ ymn n where the last sum ranges over the tuples of n non-negative integers such that the sum is k. This seems a complicated formula, but comparing with the previous section, this means that the polynomial is a sum of monomials 1 m1! ¨ ¨ ¨ mn! Bjf Bxm1 1 ¨ ¨ ¨ Bxmn n px0qym1 1 ¨ ¨ ¨ ymn n where j runs over all integers with 0 ď j ď k, and for a given j, we consider all possible partial derivatives of order j (so that m1 ` ¨ ¨ ¨ ` mn “ j) with the factorial coeﬃcient. Moreover, with clever notation, we can simplify this a lot. First, for any n-tuple m “ pm1, . . . , mnq of non-negative integers, we deﬁne |m| “ m1 ` ¨ ¨ ¨ ` mn and we denote m! “ m1! ¨ ¨ ¨ mn! and moreover, for variables y1, . . . , yn, we denote by ym the monomial ym “ ym1 1 ¨ ¨ ¨ ymn n . Then using the abbreviated notation for partial derivatives from the previous section, we can write Tkf py; x0q “ ÿ |m|ďk 1 m! Bm x f px0qym 43 (by convention, the 0-th partial derivative is just the function f itself, and p0, . . . , 0q! “ 0! “ 1). Example 3.7.2. For k “ 1, we recover the aﬃne-linear map T1f py; x0q “ f px0q ` nÿ i“1 Bxif px0qyi. For k “ 2, we obtain a polynomial of degree ď 2 which is T1f py; x0q “ f px0q ` nÿ i“1 Bxif px0qyi ` 1 2 nÿ i“1 B2 x2 i f px0qy2 i ` ÿ 1ďiăjďn B2 xixj f px0qyiyj. The term of order 2 corresponds to the partial derivatives of order 2, in other words to the tuples pm1, . . . , mnq with m1 ` ¨ ¨ ¨ ` mn “ 2. Indeed, two cases can arise: (1) either all except one mi are zero, and mi “ 2, in which case we obtain the second derivative with respect to xi taken twice, with coeﬃcient 1{m! “ 1{2! “ 1{2. (2) or two of the mi’s are non-zero, equal to 1, and all others are zero; assume that mi “ 1 and mj “ 1 with i ă j, then we get the partial derivative Bxixj with coeﬃcient 1{m! “ 1{p1!1!q “ 1. Another way to express this second term (and to remember it) is to notice that 1 2 nÿ i“1 B2 x2 i f px0qy2 i ` ÿ 1ďiăjďn B2 xixj f px0qyiyj “ 1 2yt Hessf px0qy where yt is the transpose of the column vector y. Hence we can express the second Taylor polynomial concisely in the form f px0q ` ∇f px0q ¨ y ` 1 2 yt Hessf px0qy for y P Rn. For instance, take n “ 2, and suppose that Hessf px0q “ ˆa b b d ˙ . Then 1 2yt Hessf px0qy “ 1 2 py1 y2q ˆa b b d ˙ ˆ y1 y2 ˙ “ 1 2 ay2 1 ` by1y2 ` 1 2dy2 2. The following statement indicates one way that Taylor polynomials give a better and better approximation to a function of class C k (there are more precise versions, but we will not need them). Proposition 3.7.3 (Taylor approximation). Let k ě 1 be an integer. Let X Ă Rn be open and f : X Ñ R be a function of class C k. For x0 in X, if we deﬁne Ekf px; x0q by f pxq “ Tkf px ´ x0; x0q ` Ekf px; x0q then we have lim xÑx0 x­“x0 Ekf px; x0q }x ´ x0}k “ 0. For k “ 2, this means that for a function of class C 2, we have lim xÑx0 1 }x ´ x0}2 ´ f pxq ´ ´ f px0q ` ∇f px0q ¨ px ´ x0q ` 1 2px ´ x0q t Hessf px0qpx ´ x0q¯¯ “ 0. 44 Figure 3.9. f px, yq and its approximations of order 1 and 2 Example 3.7.4. Take n “ 2 and f px, yq “ e3x´sinpxyq, around the point p0, 0q where f p0, 0q “ e0 “ 1. The gradient is ∇f px, yq “ ˆp3 ´ y cospxyqq expp3x ´ sinpxyqq ´x cospxyq expp3x ´ sinpxyqq ˙ so that ∇f p0, 0q “ ˆ3 0 ˙. The Hessian matrix is Hessf px, yq “ e 3x´sinpxyq ˆa b b d ˙ with a “ y2 sinpxyq ` p3 ´ y cospxyqq 2 b “ ´ cospxyq ` xy sinpxyq ´ x cospxyqp3 ´ y cospxyqq d “ x2 sinpxyq ` x2 cospxyq 2, so that Hessf p0, 0q “ ˆ 9 ´1 ´1 0 ˙ Hence the ﬁrst order approximation at px, yq close to p0, 0q is apx, yq “ 1 ` 3x and the second-order approximation at px, yq is gpx, yq “ T2f px, y; p0, 0qq “ 1 ` 3x ` 9x2 2 ´ xy. As a numerical illustration, we ﬁnd that f p´0.0015, 0.003q « 0.99551458963514434461139384694367021911 ap´0.0015, 0.003q “ 0.9955 gp´0.0015, 0.003q “ 0.995514625 so the precision has increased considerably (the diﬀerence goes from « 1.46 ¨ 10 ´5 to « 3.5 ¨ 10 ´8). Figure 3.9 displays the graph of f , as well as that of a and g over r´2, 2s ˆ r´2, 2s. 45 3.8. Critical points Recall that for a function of 1 variable, an important application of the derivative is its use for ﬁnding extrema of a function, using the necessary criterion that if a diﬀerentiable function f has a local maximum or minimum at a point x that is not a boundary of an interval, we have f 1pxq “ 0. Proposition 3.8.1. Let X Ă Rn be open and f : X Ñ R a diﬀerentiable function. If x0 P X is such that f pyq ď f px0q for all y close enough to x0 (local maximum at x0) or f pyq ě f px0q for all y close enough to x0 (local minimum at x0). Then we have df px0q “ 0, or in other words ∇f px0q “ 0, or equivalently Bf Bxi px0q “ 0 for 1 ď i ď n. Proof. Let 1 ď i ď n. Deﬁne gptq “ f px0 ` teiq for t such that x0 ` tei P X (this contains an open interval around 0 since X is open). Then g has a local extremum at t “ 0 by construction, and is diﬀerentiable, so g1ptq “ Bxif px0q “ 0. □ This proposition justiﬁes the following deﬁnition: Definition 3.8.2 (Critical point). Let X Ă Rn be open and f : X Ñ R a diﬀeren- tiable function. A point x0 P X such that ∇f px0q “ 0 is called a critical point of the function f . Proposition 3.8.1 is enough to determine the maximum and minimum of a function of more than one variable in many cases. One issue requires some care however: the existence of a point where a continuous function f : X Ñ R is maximal or minimal is not automatic if X Ă Rn is open. Such points do exist, however, if f is deﬁned on a set ¯X that is compact (Deﬁni- tion 3.2.11), namely if the set ¯X is bounded and closed. But the necessary condition of Proposition 3.8.1 does not apply in this case. The most common strategy is be in such a situation (a continuous function deﬁned on a compact set ¯X), so that a maximum and a minimum are known to exist, and to have a decomposition ¯X “ X Y B, where X is open and B is a “boundary” part. Suppose then that the restriction of f to the open set X is diﬀerentiable. Then, if the maximum or minimum of f is reached in a point of X, this must be a critical point of the restriction of f to X. One can attempt to compute all these points, and evaluate f at these points to determine where the extremal points are. One must in any case also evaluate f on the boundary B in order to compare the values there, which might be larger (or smaller) than the values at the critical points in X. Remark 3.8.3. This problem already occurs with one variable, where one must check the values f paq and f pbq to ﬁnd the maximum of a continuous function f : ra, bs Ñ R, and not only the points x Psa, br where f 1pxq “ 0. 46 Example 3.8.4. Let ¯X be the square r0, 1s ˆ r0, 1s in R 2 and f px, yq “ x2 ´ 2y2. The set ¯X is compact, and ¯X “ X Y B, where X is the open set s0, 1rˆs0, 1r, and B the boundary of the square, which is itself the union of four line segments. The function f is diﬀerentiable on X, and its gradient is ∇f px, yq “ ˆ 2x ´4y ˙ so that the only critical point is p0, 0q, where f p0, 0q “ 0. It is already clear that this is not the maximum, or the minimum, of f on ¯X. On the boundary B, we compute f px, 0q “ x2, f px, 1q “ x2 ´ 2, f p0, yq “ ´2y2, f p1, yq “ 1 ´ 2y2. The maximal values of f on these four segments are respectively 1, ´1, ´2, 1 and the minimal values are 0, ´2, ´2, ´1. We conclude that the maximum of f on ¯X is equal to 1 “ f p1, 0q, and that the minimum is ´2 “ f p0, 1q. In the case n “ 1, the most convenient suﬃcient criterion for the existence of a local extremum at a point x where f 1pxq “ 0 is that the second derivative f 2pxq at this point should exist and be non-zero. Its sign then indicates whether x is a local maximum (if f 2pxq ă 0) or minimum (f 2pxq ą 0). The analogue question for n ě 2 is more delicate. It is natural to think that the second partial derivatives (hence the Hessian matrix) should play the role of the second derivative, but the non-vanishing of Hessf px0q is not enough to have a local extremum if n ě 2, as the following important example shows. Example 3.8.5. Let n “ 2 and f px, yq “ xy. Then ∇f pxq “ py, xq, so the only critical point is p0, 0q, where f p0, 0q “ 0. We have Hessf p0, 0q “ ˆ0 1 1 0 ˙ which is non-zero, but nevertheless, the critical point p0, 0q is not a local maximum (since f px, xq “ x2 ą f p0, 0q for x arbitrarily small) and is not a local minimum (since f px, ´xq “ ´x2 ă f p0, 0q for x arbitrarily small). This phenomenon reﬂects the fact that there is one line (namely, y “ x) in which the restriction of the function has graph a downward parabola, and another (namely y “ ´x) in which it is an upward parabola. Such situations are called “saddle points”. Definition 3.8.6 (Non-degenerate critical point). Let X Ă Rn be open and f : X Ñ R a function of class C 2. A critical point x0 P X of f is called non-degenerate if the Hessian matrix has non-zero determinant. For a non-degenerate critical point x0 of f : X Ñ R, we can classify the behavior of the function f around x0 in terms of the signs of the eigenvalues of the Hessian matrix. Recall, from linear algebra, that if a symmetric matrix H of size n is non-degenerate (has detpHq ­“ 0), then it is diagonalizable, with non-zero real eigenvalues, in an orthonormal basis of Rn. Let p (resp. q) be the number of positive (resp. negative) eigenvalues of H. There exists an orthogonal basis pv1, . . . , vnq of Rn such that, for y “ t1v1 ` ¨ ¨ ¨ ` tnvn P Rn, 47 Figure 3.10. f px, yq “ xy we have ytHy “ t 2 1 ` ¨ ¨ ¨ ` t 2 p ´ t2 p`1 ´ ¨ ¨ ¨ ´ t 2 p`q, where it is perfectly possible that p “ n (in which case, there are no terms with minus sign) or q “ n (in which case, there are no terms with plus sign). The coeﬃcients t1, . . . , tn are given by linear functions ti “ ℓipy1, . . . , ynq if y “ py1, . . . , ynq P Rn. Since ∇f px0q “ 0 (it is a critical point), the second Taylor polynomial of f at x0 is then given by f px0q ` 1 2yt Hessf px0qy “ f px0q ` 1 2 ´ ℓ1pyq 2 ` ¨ ¨ ¨ ` ℓppyq2 ´ ℓp`1pyq 2 ´ ¨ ¨ ¨ ´ ℓp`qpyq 2¯ . When x is very close to x0, the function f pxq is approximated very closely by f px0q ` 1 2 ´ ℓ1px ´ x0q 2 ` ¨ ¨ ¨ ` ℓppx ´ x0q2 ´ ℓp`1px ´ x0q 2 ´ ¨ ¨ ¨ ´ ℓp`qpx ´ x0q 2¯ , and in particular the sign of f pxq ´ f px0q, which tells us whether x0 is a local maximum, or minimum, or neither, is the same as the sign of ℓ1pyq 2 ` ¨ ¨ ¨ ` ℓppyq 2 ´ ℓp`1pyq 2 ´ ¨ ¨ ¨ ´ ℓp`qpyq2 This is very easy to determine, because when x ´ x0 is in the direction of vi, which means when only ℓipx ´ x0q is non-zero, and all other ℓjpx ´ x0q are zero, we get the approximation ℓipx ´ x0q 2, if 1 ď i ď p, ´ℓipx ´ x0q 2, if p ` 1 ď i ď n. If both of these cases occur for suitable choices of i, then there will be negative as well as positive values of f pxq ´ f px0q. So a local extremum is only possible if p “ n or q “ n. Corollary 3.8.7. Let X Ă Rn be open and f : X Ñ R a function of class C 2. Let x0 be a non-degenerate critical point of f . Let p and q be the number of positive and negative eigenvalues of Hessf px0q. (1) If p “ n, equivalently if q “ 0, the function f has a local minimum at x0. (2) If q “ n, equivalently if p “ 0, the function f has a local maximum at x0. (3) Otherwise, equivalently if pq ­“ 0, the function f does not have a local extremum at x0. One then says that f has a saddle point at x0. Remark 3.8.8. (1) The condition p “ n means that the Hessian matrix H at x0 is a positive deﬁnite symmetric matrix (and q “ n means that it is a negative deﬁnite matrix). This also means that ytHy ą 0 for any non-zero vector y P Rn. When pq ­“ 0, the Hessian is also said to be indeﬁnite. 48 (2) From linear algebra, we know an often convenient criterion for a symmetric matrix A “ pai,jq1ďi,jďn to be positive deﬁnite: this is so if and only if the n submatrices Ak “ pai,jq1ďi,jďk, for 1 ď k ď n, have positive determinant. (For negative deﬁnite matrices, apply this to the opposite matrix; be careful that detp´Akq ­“ ´ detpAkq unless the size of the matrix is odd!) For instance, when n “ 2, a matrix ˆa b b d ˙ is positive deﬁnite if and only if a ą 0, ad ´ b2 ą 0. It is negative deﬁnite if and only if a ă 0, ad ´ b2 ą 0, and indeﬁnite if and only if ad ´ b2 ă 0 (note that if a “ 0, then the determinant is ´b2 ă 0 since a “ b “ 0 is not possible for an invertible matrix). For the Hessian matrix at a critical point x0 of a C 2 function f : R2 Ñ R, these conditions become B2f Bx2 px0q ą 0, B2f Bx2 px0q B2f By2 px0q ´ ´ B2f BxBy px0q ¯2 ą 0 for a local minimum at x0, or B2f Bx2 px0q ă 0, B2f Bx2 px0q B2f By2 px0q ´ ´ B2f BxBy px0q ¯2 ą 0 for a local minimum at x0, or B2f Bx2 px0qB2f By2 px0q ´ ´ B2f BxBy px0q ¯2 ă 0 for a saddle point. If n “ 3, the matrix A “ ¨ ˝ a b c b e f c f i ˛ ‚ is positive deﬁnite if and only if a ą 0, ae ´ b2 ą 0, detpAq ą 0. (3) If pq is non-zero, the description with the Taylor polynomial is much more precise: it tells us that f behaves like a downward parabola in the directions corresponding to v1, . . . , vp, and like an upward parabola in the directions vp`1,. . . , vn. Example 3.8.9. (1) Consider again the function f px, yq “ xy on R2 at the critical point p0, 0q, as in Example 3.8.5. Since it is a polynomial of degree 2, it is in fact equal to its second Taylor polynomial; the critical point is non-degenerate since detpHessf p0, 0qq “ ´1. An orthogonal basis of eigenvectors is pv1, v2q with v1 “ p1, 1q (where Hpv1q “ v1) and v2 “ p1, ´1q (where Hpv2q “ ´v2). The expression f pxq “ xy “ 1 2 ´ 1 2 px ` yq2 ´ 1 2 px ´ yq 2¯ 49 Figure 3.11. The graph of e cospx´yq ` x2 and its behavior close to p0, πq corresponds to our previous discussion, and we recover the directions y “ x and y “ ´x where f has diﬀerent behavior. (2) Take n “ 2 and f px, yq “ e cospx´yq ` x2. for px, yq P X “s ´ 4, 4r2. The gradient is ∇f px, yq “ ˆ´ sinpx ´ yq exppcospx ´ yqq ` 2x sinpx ´ yq exppcospx ´ yqq ˙ . The critical points are determined by ∇f px0, y0q “ 0. The second equation becomes sinpx0 ´ y0q “ 0, from which the ﬁrst transforms to x0 “ 0, and hence sinpy0q “ 0. We conclude that the critical points in the indicated region are x1 “ p0, 0q, x2 “ p0, πq and x3 “ p0, ´πq. The Hessian is Hessf px, yq “ ˆ2 0 0 0 ˙ ` e cospx´yq ˆ´ cospx ´ yq ` sin 2px ´ yq cospx ´ yq ´ sin 2px ´ yq cospx ´ yq ´ sin2px ´ yq ´ cospx ´ yq ` sin 2px ´ yq ˙ . The values H1, H2, H3 of the Hessian of f at these three critical points are given respectively by H1 “ ˆ2 ´ e e e ´e ˙ , H2 “ H3 “ ˆ2 ` e´1 ´e ´1 ´e´1 e ´1 ˙ . The matrix H1 is indeﬁnite (the determinant being ´2e ă 0), but H2 and H3 are positive deﬁnite (since 2 ` e´1 ą 0 and the determinant is 2{e ą 0). So p0, πq and p0, ´πq are local minimum of f , while p0, 0q is a saddle point. It is interesting to note from the graphs that this is not so obvious! Remark 3.8.10. If x0 is a degenerate critical point, the Hessian does not allow us to conclude anything concerning local extrema at x0: there could be one (either a local maximum or local minimum) or not. For instance, take f1px, yq “ x4 ` y4, f2px, yq “ x4 ´ y4 and f3px, yq “ ´x4 ´ y4. The gradient of any of these functions vanishes if and only if px, yq “ p0, 0q, and f1p0, 0q “ f2p0, 0q “ f3p0, 0q “ 0. In all three cases, we also have Hessfip0, 0q “ 0, so the information provided by the Hessian is the same. However, it is immediate that p0, 0q is a local 50 Figure 3.12. The behavior close to p0, 0q minimum of f1 (even a global one), a local maximum of f3, and that f2 has a saddle point at p0, 0q. 3.9. Lagrange multipliers A common type of optimization problem does not simply asks for the maximum (or minimum) of a function, but adds constraints to the values of the variable. For instance, we might want to solve a problem like “what is the largest value of f pxq if x is constrained to satisfy an equation gpxq “ 0”. Example 3.9.1. Let pa, b, cq P R 3 be non-zero, and let pα, β, γq P R 3, also non-zero. We want to ﬁnd the maximum of the quadratic form Qpx, y, zq “ ax2 ` by2 ` cz2 for px, y, zq such that }px, y, zq} ď 1 and αx ` βy ` γz “ 0. Geometrically, we intersect the sphere of radius 1 in R3 with a plane and we try to maximize Qpx, y, zq on the intersection. One idea to solve such a problem (which is often suﬃcient) is to parameterize the set of solutions of the constraint gpxq “ 0 in terms of new variables (say u, so that x “ hpuq describes the set of solutions of gpxq “ 0), and to maximize the function f phpuqq for u in the set of parameters. This method is often complicated because there is no simple parameterization of the solutions of gpxq “ 0, or because the parameterization will destroy some natural symmetry of the problem, with the eﬀect that the calculations become more complicated than they should. The method of Lagrange multipliers can be used to solved this constrained maximiza- tion problems without involving a parameterization of the solution set. 51 Proposition 3.9.2. Let X Ă Rn be open and let f : X Ñ R and g : X Ñ R be functions of class C 1. If x0 P X is a local extremum of the function f restricted to the set Y “ tx P X : gpxq “ 0u then either ∇gpx0q “ 0, or there exists λ0 P R such that # ∇f px0q “ λ∇gpx0q gpx0q “ 0, or in other words, there exists λ such that px0, λq is a critical point of the diﬀerentiable function h : X ˆ R Ñ R deﬁned by hpx, λq “ f pxq ´ λgpxq. Such a value λ is called a Lagrange multiplier at x0. Intuitive explanation. Suppose there is a local extremum satisfying the con- straint at x0 and that ∇gpx0q ­“ 0. If we “move” x around x0, staying in the solution set of the equation gpxq “ 0, which means moving perpendicularly to the gradient ∇gpx0q, the function f varies approximately by px ´ x0q ¨ ∇f px0q. This will take values both positive and negative, unless all variations x ´ x0 are orthogonal to ∇f px0q. But all these possible variations represent the vectors orthogonal to ∇gpx0q, so the conclusion is that, for a local extremum, the gradients of f and g at x0 are linearly dependent. And since ∇gpx0q ­“ 0 by assumption, this means that there exists λ P R such that ∇f px0q “ λ∇gpx0q. □ Compared to the problem of ﬁnding critical points of f (which has n equations Bxif px0q “ 0 and n unknowns), we have here n ` 1 equations and n ` 1 unknowns. Note that the values of the Lagrange multipliers λ is usually irrelevant to the ﬁnal prob- lem: they are just auxiliary quantities that are useful to ﬁnd the local extrema. As in the case of Proposition 3.8.1, it is important to remember that the solutions of the equations for Lagrange multipliers are only candidates for local extrema. As in the previous situation, we still need to check whether they are indeed extrema or not, and we may often need to handle a “boundary” component when f is deﬁned on a set ¯X that is compact, and is expressed as X Y B with X open, and B the boundary. Remark 3.9.3. (1) Suppose that f is deﬁned and continuous on a compact set ¯X “ X Y B. If the function g deﬁning the constraint gpxq “ 0 is also continuous, then the intersection ¯Y “ ¯X X tx P ¯X : gpxq “ 0u is still a compact subset of Rn (indeed, it is bounded, as it is contained in ¯X, and it is the intersection of two closed sets – the second because g is continuous –, and it is elementary from Deﬁnition 3.2.11 that the intersection of two closed sets is closed). By restriction, f deﬁnes a continuous function f | ¯Y : ¯Y Ñ R, and in particular Theorem 3.2.15 applies to f | ¯Y , which shows that f has a maximum and a minimum on ¯Y . Suppose now that f is deﬁned on Rn, which is not compact. Then there is another important case in which the existence of a maximum and minimum for the constrained problem is ensured: this is so if the set Y deﬁned by gpxq “ 0 is itself compact, since then we are maximizing or minimizing the continuous function f on this compact set. And 52 since g is continuous, the set Y is always closed, and therefore the question is whether it is bounded or not, which can often be determined very easily. (2) Before deciding to use Lagrange multipliers, it is useful to check if some other method could apply, since the diﬃculty of the computations may vary a lot depending on the approach. (3) The critical points of f on X are obvious candidates for local extrema of f re- stricted to Y , if they happen to be elements of Y . They occur in Proposition 3.9.2 precisely when the Lagrange multiplier λ is zero, since in that case the equation becomes ∇f px0q “ 0 (in addition to gpx0q “ 0). Example 3.9.4. (1) Consider the problem of maximizing f px, yq “ 2x2 ` 3xy ´ y2 on the circle of radius 1 in R2. The circle is compact, so we know that there exists a maximum. The circle is represented by the constraint gpx, yq “ 0 with gpx, yq “ x2`y2´1. Since ∇gpx, yq “ 0 only if px, yq “ 0, for which gpx, yq ­“ 1, only the case of a Lagrange multiplier can occur in Proposition 3.9.2. So we write down the equations $ ’& ’% x2 ` y2 “ 1 4x ` 3y ´ 2xλ “ 0 3x ´ 2y ´ 2yλ “ 0. The last two equations are linear with respect to x and y and have only the zero solution, which is incompatible with the ﬁrst equation, unless the determinant is zero. This is ´p4 ´ 2λqp2 ` 2λq ´ 9 “ 4λ2 ´ 4λ ´ 17. The discriminant of this equation is 288 “ 2 5 ¨ 3 2, so the solutions are λ1 “ 4 ` 12 ? 2 8 “ 1 ` 3 ? 2 2 , λ2 “ 4 ´ 12 ? 2 8 “ 1 ´ 3 ? 2 2 . Writing x “ ´3{p4 ´ 2λq, we obtain the possible values for y, namely y “ ˘ 4 ´ 2λ a p4 ´ 2λq2 ` 9, which gives, for the two values of λ, two values of y each, namely ˘y1 “ ˘0.382683432365089771 ¨ ¨ ¨ , ˘y2 “ ˘0.923879532511286756 ¨ ¨ ¨ . One can check that y2 1 ` y2 2 “ 1, so the corresponding values of x for a given y are ˘ the “other” value of y. Taking all possibilities of the sign into account, this shows that the maximum and minimum are taken at one of the values py2, y1q, p´y2, y1q, py2, ´y1q, p´y2, ´y1q py1, y2q, py1, ´y2q, p´y1, y2q, p´y1, ´y2q. In fact, since f p´x, ´yq “ f px, yq, we only need to check the ﬁrst two values of each row, and for these we obtain f py2, y1q “ 1 2, f p´y2, y1q “ 2.621320343559642 ¨ ¨ ¨ , f py1, y2q “ ´1.621320343559642 ¨ ¨ ¨ , f py1, ´y2q “ 1 2 (In fact, in this case, we have f p´y2, y1q “ λ1 and f py1, y2q “ λ2, but this is a coincidence.) In that case, it is however much simpler to represent the circle by the parameterization pcos θ, sin θq, since this reduces the problem to maximizing or minimizing the function f pcos θ, sin θq “ 2 cos 2 θ ` 3 cos θ sin θ ´ sin 2 θ “ 2 ` 3 sin θpcos θ ´ sin θq. 53 Simply by diﬀerentiating, we ﬁnd that the extreme values are achieved for θ “ π{8 (maximum) or θ “ 5π{8 (minimum). (2) Consider the maximum and minimum of the function f px, y, zq “ x2 ´ y2 with the constraint gpx, y, zq “ 0, where gpx, y, zq “ x2 ` 2y2 ` 3z2 ´ 1. Here the set of solutions of gpx, y, zq “ 0 is closed, since g is continuous, and it is bounded since x2 ď x2 `2y2 `3z2 “ 1, and similarly 2y2 ď 1 and 3z2 ď 1 for any solution. Since f is also continuous, we know that there exist a maximum and a minimum. The gradient of g is ∇gpx, y, zq “ p2x, 4y, 6zq and doesn’t vanish when gpx, y, zq “ 0. So we look for the Lagrange multipliers. The equations ∇f px, y, zq “ λ∇gpx, y, zq and gpx, y, zq “ 0 are $ ’’’& ’’’% 2x “ 2λx ´2y “ 4λy 0 “ 6λz x2 ` 2y2 ` 3z2 ´ 1 “ 0. The third equation shows that either λ “ 0 or z “ 0. In the ﬁrst case, this implies that x “ y “ 0, and therefore z “ ˘1{? 3, giving two possibilities p1 “ p0, 0, 1{ ? 3q and p2 “ p0, 0, ´1{ ? 3q. We have f pp1q “ f pp2q “ 0. If z “ 0, then the equations for x and y become $ ’& ’% 2p1 ´ λqx “ 0 ´2p1 ´ 2λqy “ 0 x2 ` 2y2 “ 1. The ﬁrst equation shows that either x “ 0 or λ “ 1. If x “ 0, then we have the solutions with λ “ 1{2, y “ ˘1{ ? 2, in other words p3 “ p0, 1{ ? 2, 0q, p4 “ p0, ´1{? 2, 0q. Then f pp3q “ f pp4q “ ´1 2 . Finally if λ “ 1, then the second equation shows that y “ 0, and the third gives the solutions p5 “ p1, 0, 0q and p6 “ p´1, 0, 0q. Since f pp5q “ f pp6q “ 1, we conclude that the maximum of f with the constraint g “ 0 is 1, and the minimum is ´1{2. (3) Here is an example with n arbitrarily large. Fix py1, . . . , ynq P Rn non-zero. We want to maximize and minimize the function f px1, . . . , xnq “ x1y1 ` ¨ ¨ ¨ ` xnyn (which is in fact a linear function of x), subject to the constraint x2 1 ` ¨ ¨ ¨ ` x2 n “ 1. This constraint deﬁnes a compact set, so we know that the maximum exists. Since ∇gpxq “ p2x1, . . . , 2xnq is non-zero for all x satisfying gpxq “ 0, we solve the Lagrange multiplier equations. These are # yi “ 2λxi for 1 ď i ď n x2 1 ` ¨ ¨ ¨ ` x2 n “ 1. 54 Since y ­“ 0, we have λ ­“ 0 from any of the ﬁrst n equations. Then these equations state that xi “ yi{p2λq for 1 ď i ď n. It follows that f pxq “ 1 2λpy2 1 ` ¨ ¨ ¨ ` y2 nq. On the other hand, the last equation shows that 1 4λ2 py2 1 ` ¨ ¨ ¨ ` y2 nq “ 1, and hence there are two solutions for λ, namely λ “ ˘ 1 2}y}. We ﬁnd the values x “ ˘y{p2}y}q, and f pxq “ ˘ 1 a y2 1 ` ¨ ¨ ¨ ` y2 n py2 1 ` ¨ ¨ ¨ ` y2 nq “ ˘ b y2 1 ` ¨ ¨ ¨ ` y2 n “ ˘}y}. Hence the constrained maximum of f is }y} and the constrained minimum is ´}y}. If we now consider an arbitrary vector x ­“ 0, and replace it with rx “ x{}x}, which satisﬁes the constraint gprxq “ 0, the result implies by homogeneity that ´}x}}y} ď x1y1 ` ¨ ¨ ¨ ` xnyn ď }x}}y}. This is the Cauchy-Schwarz inequality that we have recovered as a case of constrained optimization! 3.10. The inverse and implicit functions theorems We ﬁnish this chapter by stating without proofs two important theoretical results that are often used in the study of functions of more than 1 variable, and of their level sets. The ﬁrst result is the analogue of the fact that a diﬀerentiable function f : I Ñ R deﬁned on an interval is bijective from I to its image if its derivative is always ą 0 (or always ă 0). In other words, we want conditions that ensure that a function f : X Ñ Rn can be used as a change of variable, i.e., that we can recover x uniquely from the value f pxq. Definition 3.10.1 (Change of variable). Let X Ă Rn be open and f : X Ñ Rn be diﬀerentiable. Let x0 P X. We say that f is a change of variable around x0 if there is a radius r ą 0 such that the restriction of f to the ball B “ tx P Rn : }x ´ x0} ă ru of radius r around x0 has the property that the image Y “ f pBq is open in Rn, and if there is a diﬀerentiable map g : Y Ñ B such that f ˝ g “ IdY and g ˝ f “ IdB. Note that this deﬁnition is local: we do not require the existence of an inverse g for f deﬁned everywhere. For a given y P Y , there could well exist an element x P X, not in B, such that f pxq “ y. 55 Theorem 3.10.2 (Inverse function theorem). Let X Ă Rn be open and f : X Ñ Rn be diﬀerentiable. If x0 P X is such that detpJf px0qq ­“ 0, i.e., such that the Jacobian matrix of f at x0 is invertible, then f is a change of variable around x0. Moreover, the Jacobian of g at x0 is determined by (3.4) Jgpf px0qq “ Jf px0q ´1. In addition, if f is of class C k, then g is of class C k. In contrast to the case n “ 1, there is no easy condition to ensure that f is a “global” change of variable: this must be investigated case by case. It is easy to see that the requirement that det Jf px0q ­“ 0 is necessary for such a statement, and also to see that the formula (3.4) must be true if f is a change of variable. Indeed, if we assume that there exists g diﬀerentiable such that g ˝ f “ Id, then by the chain rule, it follows that Jgpf px0qq ¨ Jf px0q “ JIdpx0q “ 1n, the identity matrix of size n (because the identity function is linear, so is its own diﬀer- ential). This formula implies that Jf px0q is invertible with inverse Jgpf px0qq. Example 3.10.3. (1) Consider the function f px, yq “ psinpxyq, e x ` yq. Then Jf px, yq “ ˆy cospxyq x cospxyq ex 1 ˙ with determinant det Jf px, yq “ y cospxyq ´ xe x cospxyq “ cospxyqpy ´ xexq. This means that f is a change of variable around px, yq, unless either xy “ π{2 ` kπ for some k P Z, or y “ xe x. (2) Consider the function f pr, θ, ϕq “ ¨ ˝ r cospθq sinpϕq r sinpθq sinpϕq r cospϕq ˛ ‚ for r ě 0, 0 ď θ ď 2π and 0 ď ϕ ď π (“spherical coordinates”). The image of f is R3 and the function is diﬀerentiable and injective if the domain is the open set X “s0, `8rˆs0, 2πrˆs0, πr. In fact, r is the distance to the origin of px, y, zq, θ is the angle in the horizontal plane z “ 0 from the x axis to the point px, yq, and ϕ is the angle between the vertical axis x “ y “ 0 and the line px, y, zq (so it is between 0 and π). The Jacobian of f is (3.5) Jf pr, θ, ϕq “ ¨ ˝ cospθq sinpϕq ´r sinpθq sinpϕq r cospθq cospϕq sinpθq sinpϕq r cospθq sinpϕq r sinpθq cospϕq cospϕq 0 ´r sinpϕq ˛ ‚ with determinant (3.6) det Jf pr, θ, ϕq “ ´r2 cos 2pθq sin 3pϕq ´ r2 sin2pθq sin 2pϕq cospϕq ´ r2 cos 2pθq cos 2pϕq sinpϕq ´ r2 sin 2pθq sin 3pϕq “ ´r2 sinpϕq. 56 This is non zero for all pr, θ, ϕq in X, which conﬁrms that the spherical coordinates give a change of variable around any point in X. The last theorem of this chapter concerns the problem of transforming an equation gpx, yq “ 0 into a functional relation y “ f pxq – in other words, of “parameterizing” the solutions of an equation. We consider the case where y is a single value, whereas x runs over Rn. As in the case of the Inverse Function Theorem, there is a general result that shows that such parameterizations exist, but a priori only for x close to a given x0. Theorem 3.10.4 (Implicit Function Theorem). Let X Ă R n`1 be open and let g : X Ñ R be of class C k with k ě 1. Let px0, y0q P R n ˆ R be such that gpx0, y0q “ 0. Assume that Bygpx0, y0q ­“ 0. Then there exists an open set U Ă R n containing x0, an open interval I Ă R containing y0, and a function f : U Ñ R of class C k such that the system of equations # gpx, yq “ 0 x P U, y P I is equivalent with y “ f pxq. In particular, f px0q “ y0. Moreover, the gradient of f at x0 is given by (3.7) ∇f px0q “ ´ 1 pBygqpx0, y0q∇xgpx0, y0q, where ∇xg “ pBx1g, . . . , Bxngq. Idea of the proof. We will explain how to deduce at least the existence of the function from the Inverse Function Theorem. Consider the function ϕ : X Ñ Rn`1 deﬁned by ϕpx, yq “ px, gpx, yqq. It is of class C k. The Jacobian matrix is Jϕpx, yq “ ˆ 1n 0 ∇xg Byg. ˙ Its determinant at px0, y0q is detpJϕpx0, y0qq “ pBygqpx0, y0q, which is non-zero by assumption. This means that ϕ is a change of variable around px0, y0q, by the Inverse Function Theorem. Therefore, by Theorem 3.10.2, there exists an open set V Ă R n`1 containing ϕpx0, y0q “ px0, 0q and a function ψ : V Ñ U of class C k such that ϕ ˝ ψ “ Id. We use pu, vq P Rn ˆ R for the variables in V and write ψpu, vq “ pψ1pu, vq, ψ2pu, vqq where ψ1pu, vq P R n and ψ2pu, vq P R. Then the relation ϕ ˝ ψ “ Id means that pu, vq “ ϕpψ1pu, vq, ψ2pu, vqq “ pψ1pu, vq, gpψ1pu, vq, ψ2pu, vqqq. In particular, this means that ψ1pu, vq “ u, and taking v “ 0, we get 0 “ gpu, ψ2pu, 0qq which shows that we can take f pxq “ ψ2px, 0q to solve the equation, namely that gpx, f pxqq “ 0 for all x. 57 We can also quickly explain the formula (3.7): we start from the relation gpx, f pxqq “ 0, which we write as g ˝ rf “ 0, where rf pxq “ px, f pxqq. Since rf px0q “ px0, y0q, it follows by the chain rule that 0 “ Jgpx0, y0q ¨ J rf px0q. But we have Jgpx, yq “ pp∇xgq t, Bygq, J rf pxq “ ˆ 1n p∇f q t ˙ (a matrix with n`1 rows and n columns), and writing down the coeﬃcients of the matrix product, we obtain the relations 0 “ Bxigpx0, y0q ` pBygqpx0, y0qBxif px0q for 1 ď i ď n. □ Example 3.10.5. (1) Let gpx, yq “ x2 ` y2 ´ 1 and px0, y0q such that gpx0, y0q “ 0. Then pBygqpx0, y0q “ 2y0. Therefore we can solve for y as a function of x, provided y0 ­“ 0. In fact the solution is simply (3.8) f pxq “ #? 1 ´ x2 if y0 ą 0 ´ ? 1 ´ x2 if y0 ă 0. Suppose that y0 ą 0 for instance. Then note that, for a given x, the point px, ? 1 ´ x2q is not the unique solution to gpx, yq “ 0, since px, ´? 1 ´ x2q is also a solution. This explains the restriction to y belonging to some interval containing y0 in the theorem, which is needed if we want to have an exact characterization of the solutions, and not just a suﬃcient condition that gpx, f pxqq “ 0. The formula (3.7) gives the derivative of f at x0, namely f 1px0q “ ´ 1 2y0 Bxgpx0q “ ´x0 y0 . If y0 ą 0, then this is equal to f 1px0q “ ´ x0a1 ´ x2 0 , which is of course the same that one obtains from the formula (3.8). If we consider y0 “ 0, then the picture of the circle shows indeed that the solution set is not the graph of a function of x when x is close to x0 “ ˘1. What can be done when y0 “ 0 (and this is a common occurence) is to use y as a variable to parameterize the solution, instead of x. Indeed, since pBxgqp˘1, 0q “ ˘2 ­“ 0, it follows from the Implicit Function Theorem applied to rgpx, yq “ gpy, xq that there is a parameterization as a function of y. In fact, it is simply x “ a1 ´ y2 or x “ ´ a 1 ´ y2, depending on whether x0 ą 0 or x0 ă 0. 58 CHAPTER 4 Integration in R n This chapter is devoted to integration in Rn. There are in fact at least two diﬀerent importants aspects: (1) integrating functions f : X Ñ R, where X Ă Rn; (2) relating integrals over diﬀerent sets, of diﬀerent dimensions. In (1), besides deﬁning integrals, one is led to analogues of the fundamental compu- tational tools of the Riemann integral, such as the change of variable formula. 4.1. Line integrals We begin with the simplest type of integrals in Rn, namely integration of functions I Ñ Rn, where I is an interval, and other integrals that involve a single variable, which is the integral of a function “along a curve”. We use again the scalar product in Rn, which we denote x ¨ y “ nÿ i“1 xiyi. Definition 4.1.1. (1) Let I “ ra, bs be a closed and bounded interval in R. Let f ptq “ pf1ptq, . . . , fnptqq be a continuous function from I to Rn, i.e., fi is continuous for 1 ď i ď n. Then we deﬁne ż b a f ptqdt “ ´ż b a f1ptq, . . . , ż b a fnptqdt ¯ P Rn. (2) A parameterized curve in Rn is a continuous map γ : ra, bs Ñ Rn that is piecewise C 1, i.e., there exists k ě 1 and a partition a “ t0 ă t1 ă ¨ ¨ ¨ ă tk´1 ă tk “ b such that the restriction of f to stj´1, tjr is C 1 for 1 ď j ď k. We say that γ is a parameterized curve, or a pathx, between γpaq and γpbq. (3) Let γ : ra, bs Ñ Rn be a parameterized curve. Let X Ă Rn be a subset containing the image of γ, and let f : X Ñ Rn be a continuous function. The integral ż b a f pγptqq ¨ γ1ptqdt P R is called the line integral of f along γ. It is denoted ż γ f psq ¨ ds, or ż γ f psq ¨ d⃗s. The integral of continuous functions I Ñ Rn satisfy much of the same rules as the Riemann integral of a function I Ñ R, for instance ż b a pf ptq ` gptqqdt “ ż b a f ptqdt ` ż b a gptqdt. 59 Also, as in the one-variable case, we deﬁne ż a b f ptqdt “ ´ ż b a f ptqdt, if a ă b. In the line integral, γ1ptq and f pγptqq are both vectors in R n for all t (since γ takes values in Rn), so that the ﬁnal integral is a real number. It is customary, when working with line integrals, to say that the function f : X Ñ Rn is a vector ﬁeld : a function that sends each point x in X Ă Rn to a vector in Rn, which we display as based at x. Example 4.1.2. (1) Let x “ px1, . . . , xnq and y “ py1, . . . , ynq be elements of Rn. The function γ : r0, 1s Ñ Rn deﬁned by γptq “ p1 ´ tqx ` ty is a parameterized curve joining γp0q “ x to γp1q “ y. Its image in Rn is exactly the line segment joining x to y. Note that γ1ptq “ y ´ x for all t P r0, 1s (intuitively, this means that γ goes from x to y with constant speed). Let f be a continuous function on X, expressed as f pxq “ pf1pxq, . . . , fnpxqq. Then we have ż γ f psq ¨ d⃗s “ nÿ i“1pyi ´ xiq ż 1 0 fipp1 ´ tqx ` tyqdt. In particular, suppose that yi “ xi for all i except a single value j (which means that the segment γ joins two points along one of the coordinate axes). Then we get ż γ f psq ¨ d⃗s “ pyj ´ xjq ż 1 0 fjpp1 ´ tqx ` tyqdt. (2) Deﬁne γ : r0, 2πs Ñ R2 by γptq “ pcosptq, sinptqq. This is a parameterized curve, whose image is the circle centered at p0, 0q with radius 1. For f px, yq “ pf1px, yq, f2px, yqq, we have ż γ f psq ¨ d⃗s “ ż 2π 0 ´ f1pcosptq, sinptqqp´ sinptqq ` f2pcosptq, sinptqq cosptq ¯ dt. Take for instance f px, yq “ ˆ´y x ˙ . Then we obtain ż γ f psq ¨ d⃗s “ ż 2π 0 psin 2ptq ` cos 2ptqqdt “ 2π. Take now γ1ptq “ pcosptq, sinptqq, but deﬁned for 0 ď t ď 4π. Then the parameterized curve γ1 corresponds to “going twice over the circle”, so the image of γ1 is the same as the image of γ. However, for the same vector ﬁeld f as before, we have ż γ1 f psq ¨ d⃗s “ ż 4π 0 dt “ 4π. (3) A parameterized curve γ : ra, bs Ñ Rn is not required to map diﬀerent times t to diﬀerent points: the trajectory described by γ may have points of self-intersection. 60 -1.0 -0.5 0.5 1.0 -0.3 -0.2 -0.1 0.1 0.2 0.3 Figure 4.1. Lemniscate An example is the circle taken twice over of the previous example, another one is the lemniscate of Bernoulli (4.1) λptq “ ´ cosptq 1 ` sin 2ptq , cosptq sinptq 1 ` sin2ptq ¯ for 0 ď t ď 2π. This is a closed curve, and we have also λpπ{2q “ λp3π{2q “ p0, 0q. Note however that λ1ptq “ 1 p1 ` sin 2ptqq2 ´ ´ sinptq ´ sin 3ptq ´ 2 sinptq cos 2ptq, cos 2ptq ´ 2 sin 2ptq ¯ so that p´1{2, ´1{2q “ λ 1pπ{2q ­“ λ 1p3π{2q “ p1{2, 1{2q. Remark 4.1.3. (1) Let f pxq “ pf1pxq, . . . , fnpxqq. Another notation that is sometimes used, in relation with the notion of diﬀerential form, is ż γ f psq ¨ d⃗s “ ż γ ω where one writes ω “ f1pxqdx1 ` ¨ ¨ ¨ ` fnpxqdxn, using linearly independent “formal symbols” dx1, . . . , dxn to separate the components fi of f . (2) The line integral has a physical interpretation. Suppose we have a particle that moves from x1 to x2 along the path γ, where γptq is the position and γ1ptq is the speed of the particle at time t. Suppose further that a force f , represented by a vector giving its direction and intensity, is applied to the particle during the motion. Then the line integral ż γ f psq ¨ d⃗s is the “work” that is done by the force f along this trajectory. If there are no other forces, then the work is (in Newtonian mechanics) the diﬀerence in the kinetic energy of the particle between the starting and end points of the trajectory. The most important property of the line integral is that it (essentially) only depends on the image curve γpra, bsq Ă Rn, and not on the chosen parameterization. More precisely: Definition 4.1.4. Let γ : ra, bs Ñ Rn be a parameterized curve. An oriented repa- rameterization of γ is a parameterized curve σ : rc, ds Ñ Rn such that σ “ γ ˝ ϕ, where ϕ : rc, ds Ñ ra, bs is a continuous map, diﬀerentiable on sa, br, that is strictly increasing and satisﬁes ϕpaq “ c and ϕpbq “ d. 61 Note that the image σprc, dsq Ă R n of an oriented reparameterization σ of γ is the same as the image γpra, bsq. Also, γ is conversely an oriented reparameterization of σ, since γ “ σ ˝ ϕ´1. Proposition 4.1.5. Let γ be a parameterized curve in Rn and σ an oriented repa- rameterization of γ. Let X be a set containing the image of γ, or equivalently the image of σ, and f : X Ñ Rn a continuous function. Then we have ż γ f psq ¨ d⃗s “ ż σ f psq ¨ d⃗s. Proof. This is a consequence of the change of variable formula for Riemann integrals (see [1, Satz 5.4.6]): since σ “ γ ˝ ϕ, we have σ1puq “ ϕ1puqγ1pϕpuqq for c ď u ď d, and hence using the deﬁnition of line integrals, we get ż σ f psq ¨ d⃗s “ ż d c f pσpuqq ¨ σ1puqdu “ ż d c f pγpϕpuqqq ¨ ϕ 1puqγ1pϕpuqqdu “ ż d c ´f pγpϕpuqqq ¨ γ1pϕpuqq ¯ ϕ1puqdu “ ż b a ´ f pγptqq ¨ γ1ptq ¯ dt “ ż γ f psqd⃗s, by applying the change of variable formula t “ ϕpuq, dt “ ϕ1puqdu, since c “ ϕpaq and d “ ϕpbq. □ Because of this proposition, one speaks, for instance, of the line integral of a vector ﬁeld f along a circle, instead of ﬁxing a parameterization of the circle. But one should keep in mind that this means “going over the circle only once, without repetition”. Example 4.1.6. (1) Let n ě 1 and deﬁne γnptq “ pcosp2πt nq, sinp2πt nqq for 0 ď t ď 1. Then γn “ γ1 ˝ ϕn, where ϕnptq “ t n. Hence all γn are common reparameterizations of γ1. The curve described by γn is the circle of radius 1 centered at p0, 0q. Note that γ1 nptq “ p´2πnt n´1 sinp2πt nq, 2πnt n´1 cosp2πt nqq, and in particular, if n ě 2, we have γ1 np0q “ 0, which means that the trajectory de- scribed by γn starts from p1, 0q with very small speed, and then accelerates as t increases. Nevertheless, if f px, yq “ p´y, xq, we have always (for instance) ż γn f psq ¨ d⃗s “ ż γ1 f psq ¨ d⃗s “ 2π. (2) It is important that the reparameterizations that are used preserve the orienta- tion, in other words that the endpoints are not “switched”. For instance, suppose that γ : r0, 1s Ñ X is a parameterized curve. Let σpuq “ γp1 ´ uq; then σ is a parameterized curve, with the same image as γ, but it goes from σp0q “ γp1q, the endpoint of γ, to σp1q “ γp0q, the starting point of γ. 62 Let f be a continuous vector ﬁeld on X. Then we compute ż σ f psq ¨ d⃗s “ ż 1 0 f pγp1 ´ uqq ¨ p´γ1p1 ´ uqqdu and by substituting t “ 1 ´ u, this is ż 0 1 f pγptqq ¨ γ1ptqdt “ ´ ż γ f psq ¨ d⃗s. In other words: going along a parameterized curve “backwards” leads to the opposite value of the line integral. The following example is extremely important, as it gives a very fast way to compute certain line integrals. Example 4.1.7. Let X be an open set in Rn and g : X Ñ R a function of class C 1. Deﬁne f “ ∇g, which is a vector ﬁeld X Ñ Rn. Let γ : ra, bs Ñ X be a parameterized curve with image in X. We write γptq “ pγ1ptq, . . . , γnptqq. We then have by deﬁnition ż γ f psq ¨ d⃗s “ ż b a nÿ i“1 Bg Bxi pγptqqγ1 iptqdt. But, by the Chain Rule, the function nÿ i“1 Bg Bxi pγptqqγ1 iptq is the derivative of the C 1 function hptq “ gpγptqq. Hence, by the fundamental theorem of calculus from Analysis I ([1, §5.4]), we have ż γ ∇gpsq ¨ d⃗s “ gpγpbqq ´ gpγpaqq, the diﬀerence between the value of g at the “end point” γpbq of the curve, and the value at the “start point” γpaq. What is striking in this example is that the answer only depends on the extremities of the parameterized curve! It is irrelevant how complicated the path joining γpaq to γpbq may be. Definition 4.1.8. Let X Ă Rn and f : X Ñ Rn a continuous vector ﬁeld. If, for any x1, x2 in X, the line integral ż γ f psq ¨ d⃗s is independent of the choice of a parameterized curve γ in X from x1 to x2, then we say that the vector ﬁeld is conservative. Remark 4.1.9. (1) Equivalently, f is conservative if and only if ż γ f psq ¨ d⃗s “ 0 for any closed parameterized curve in X (where a curve is said to be closed if γpaq “ γpbq). 63 Indeed, if f is conservative, then the integral on a closed curve from x1 to x1 must be equal to the integral along the constant curve γptq “ x1, which is zero (the speed of γ being 0). Conversely, suppose that this condition holds. Let γ1, γ2 be two paths in X from x1 to x2. Then the parameterized curve γptq “ # γ1p2tq if 0 ď t ď 1{2 γ2p2p1 ´ tqq if 1{2 ď t ď 1 is a closed parameterized curve from x1 to x1, so that the integral of f along γ is zero by our assumption; but a simple computation shows that 0 “ ż γ f psq ¨ d⃗s “ ż γ1 f psq ¨ d⃗s ´ ż γ2 f psq ¨ d⃗s. Hence the vector ﬁeld is conservative. (2) In physics, to say that a force is represented by a conservative vector ﬁeld means that the work done by the force on a particle from one point to another is the same, whatever the trajectory between the two points. (3) The equation ∇g “ f is linear. It follows, for instance, that if f1 and f2 are both conservative, with respective potentials g1 and g2, then for any pa, bq P R2, the vector ﬁeld af1 ` bf2 is conservative, with potential ag1 ` bg2. The previous example shows if X is open then any gradient vector ﬁeld f on X is conservative, i.e., any vector ﬁeld of the form f “ ∇g, where g is of class C 1 on X, is conservative. The converse is true: Theorem 4.1.10. Let X be an open set and f a conservative vector ﬁeld. Then there exists a C 1 function g on X such that f “ ∇g. If any two points of X can be joined by a parameterized curve, then g is unique up to addition of a constant: if ∇g1 “ f , then g ´ g1 is constant on X. Remark 4.1.11. (1) To say that any two points of X can be joined by a parameterized curve means that, for all x and y in X, there exists a parameterized curve γ : ra, bs Ñ X such that γpaq “ x and γpbq “ y. When this is true, we say that X is path-connected. This is the case for instance when X is a disc in the plane, or a product of intervals. More generally, it is true whenever X is convex, which means that for any x and y in X, the line segment joining x to y is contained in X (this is because such a line segment is the image of a parameterized curve, as we saw in Example 4.1.2 (1)). On the other hand, let X be the union of two discs that are disjoint, for instance, the discs of radius 1 around p0, 0q and p3, 0q. Then X is not path-connected, since (by the intermediate value theorem) any curve γptq “ pγ1ptq, γ2ptqq joining p0, 0q to p3, 0q must be such that there exists t0 with γ1pt0q “ 3{2, which is impossible since the points of X have ﬁrst coordinate in r´1, 1s Y r2, 4s. (2) If f is a conservative vector ﬁeld on X, then a function g such that ∇g “ f is called a potential for f . Note that it is not unique, since at least it is possible to add a constant to g without changing the gradient. Idea of the proof. Write f pxq “ pf1pxq, . . . , fnpxqq for x P X. Assume that X is path-connected for simplicity. Then ﬁx a point x0 P X. For any x P X, select a parameterized curve γx from x0 to x, and deﬁne gpxq “ ż γx f psq ¨ d⃗s. 64 Figure 4.2. Vector ﬁeld along two curves This is a function of x, and gpxq doesn’t depend on the choice of γx, since the vector ﬁeld f is conservative. In particular, to compute the partial derivative Bx1g of g at x, we can compute gpx ` te1q for t small enough by selecting the curve γx`te1 to be the curve γx followed by the straight line segment ℓx,t from x to x ` te1 (which is contained in X, for t small enough, because X is open). Then we get easily gpx ` te1q ´ gpxq “ ż ℓx,t f psq ¨ d⃗s “ t ż 1 0 f1pp1 ´ uqx ` upx ` te1qqdu (since ℓ 1 x,tpuq “ te1 for all u; see again Example 4.1.2 (1))). By continuity of f , for t small enough, the function f1 is almost constant on the segment ℓx,t, equal to f1pxq. So gpx ` te1q ´ gpxq « tf1pxq which means that the partial derivative of g with respect to x1 exists and is equal to f1pxq. Doing the same for all partial derivatives, we conclude that ∇g “ f . □ Example 4.1.12. (1) Let n “ 3 and f px, y, zq “ py2, xz, 1q. We will show that f is not conservative by computing the line integrals along two curves joining the same points, and showing that they are diﬀerent. We let p1 “ p0, 0, 0q and p2 “ p1, 1, 1q. The parameterized curves γ1ptq “ pt, t, tq, γ2ptq “ pt, t 2, t 3q for 0 ď t ď 1 both join p1 to p2. We have ż γ1 f psq ¨ d⃗s “ ż 1 0 pt 2, t 2, 1q ¨ p1, 1, 1qdt “ ż 1 0 p2t 2 ` 1qdt “ ”2t 3 3 ` tı1 0 “ 5 3 . 65 On the other hand, we get ż γ2 f psq ¨ d⃗s “ ż 1 0 pt4, t 4, 1q ¨ p1, 2t, 3t 2qdt “ ż 1 0 p2t 5 ` t 4 ` 3t 2qdt “ ”t 6 3 ` t 5 5 ` t 3ı1 0 “ 1 3 ` 1 5 ` 1 “ 23 15. Note that the second integral is smaller. In a physics interpretation, this would mean that less work (and energy) is needed to move the particle subject to the force f from p1 to p2 using the second path than the ﬁrst path. (2) Suppose that we know that a concrete vector ﬁeld f is conservative. How does one ﬁnd a potential g such that ∇g “ f ? One way to do that ﬁnd g such that Bg Bx1 “ f1pxq, by integrating f1 with respect to x1, when other variables are ﬁxed. This gives an answer up to a function g1 that depends only on px2, . . . , xnq. We then solve for Bg Bx2 “ f2pxq, starting with the “partial” formula for g involving g1, obtaining a new unknown function depending only on px3, . . . , xnq, and we repeat. For instance, consider the vector ﬁeld f px, y, zq “ ¨ ˝ 9x2 cospyzq ` z sinpyq ´3x3z sinpyzq ` xz cospyq ` 2y ´3x3y sinpyzq ` x sinpyq ` 2z ˛ ‚. In order to have Bg Bx “ 9x2 cospyzq ` z sinpyq, by the fundamental theorem of calculus, applied for each value of py, zq separately, we must have gpx, y, zq “ 3x3 cospyzq ` xz sinpyq ` hpy, zq, for some function h : R2 Ñ R. Then, for g of this type to satisfy Bg By “ ´3x3z sinpyzq ` xz cospyq ` 2y, we must have ´3x3z sinpyzq ` xz cospyq ` Bh By “ ´3x3z sinpyzq ` xz cospyq ` 2y, which means that Byh “ 2y, or in other words that hpy, zq “ y2 ` kpzq, gpx, y, zq “ 3x3 cospyzq ` xz sinpyq ` y2 ` kpzq, for some function k. Finally, to have Bg Bz “ ´3x3y sinpyzq ` x sinpyq ` 2z, we need to have ´3x3y sinpyzq ` x sinpyq ` k1pzq “ ´3x3y sinpyzq ` x sinpyq ` 2z, 66 which means that kpzq “ z2 ` c for some real number c. We can pick c “ 0, which gives gpx, y, zq “ 3x3 cospyzq ` xz sinpyq ` y2 ` z2. The next general question is: how can one determine easily in practice if a concrete vector ﬁeld f is conservative? The characterization in terms of line integrals is not really convenient, since many integrals are very hard to compute. There is however a very simple necessary condition. Proposition 4.1.13. Let X Ă Rn be an open set and f : X Ñ Rn a vector ﬁeld of class C 1. Write f pxq “ pf1pxq, . . . , fnpxqq. If f is conservative, then we have Bfi Bxj “ Bfj Bxi for any integers with 1 ď i ­“ j ď n. Proof. Indeed, let g be a potential for f , which is then of class C 2. Then fi “ Bxig and hence Bfi Bxj “ B2g BxjBxi “ B2g BxiBxj “ Bfj Bxi by Proposition 3.5.4. □ Example 4.1.14. (1) Consider again the example f px, y, zq “ py2, xz, 1q. Since Bypy2q “ 2x ­“ z “ Bxpxzq, we can see that f is not conservative without having to search for two curves where the line integrals are diﬀerent. (2) If n “ 2, with f “ pf1, f2q, then the condition is simply that Bf2 Bx “ Bf1 By . It is natural to ask if this necessary criterion is also suﬃcient. This is not always true, and the answer depends on the set X where the vector ﬁeld is deﬁned. Definition 4.1.15. A subset X Ă Rn is star shaped if there exists x0 P X such that, for all x P X, the line segment joining x0 to x is contained in X. We then also say that X is star-shaped around x0. Example 4.1.16. (1) A ball X “ tx P Rn : }x ´ x0} ă ru, or a “rectangle” X “ ra1, b1s ˆ ¨ ¨ ¨ ˆ ran, bns, is a star-shaped subset. In fact, these are even convex sets, which means that for any x and y in X, the line segment between x and y is contained in X, or in other words, they are star-shaped around any point in X. On the other hand, let X1 and X2 be two diﬀerent discs in R2 that intersect in at least one point x0. Then X “ X1 Y X2 is star-shaped (since, for any x P X, the segment joining x0 to x is either contained in X1 or X2, and hence is contained in X) but in general not convex. 67 (2) The complement X “ tx P Rn : x ­“ 0u of the origin in Rn is not star-shaped: whatever value of x0 ­“ 0 we select in Rn, the segment between x0 and ´x0 is not contained in X, since it contains the origin 0 R X. (3) Let 0 ă a ă b be real numbers. The annulus X “ tppx, yq P R2 : a ď x2 ` y2 ď bu Ă R2 is not star-shaped, for the same reason as in (2): it does not contain p0, 0q, and the segment between px, yq and p´x, ´yq, which both belong to X if px, yq does, passes through p0, 0q. (4) If X is star-shaped, say around x0, then it is path-connected: indeed, for any x and y in X, there is a parameterized curve from x to y obtained by following ﬁrst the segment from x to x0, and then the segment from x0 to y, both of which are contained in X. Theorem 4.1.17. Let X be a star-shaped open subset of Rn. Let f be a C 1 vector ﬁeld such that (4.2) Bfi Bxj “ Bfj Bxi on X for all i ­“ j between 1 and n. Then the vector ﬁeld f is conservative. Remark 4.1.18. The requirement that X is star-shaped is not necessary. Intuitively, the correct hypothesis on X should be that there is no “hole” in the middle around which a circle can go without it being possible to contract it within X. Example 4.1.19. (1) Let X “ R2 ´ t0u. Deﬁne f px, yq “ ˆ´ y x2`y2 x x2`y2 ˙ on X. This is a C 1 vector ﬁeld. We have By´ y x2 ` y2 ¯ “ 1 x2 ` y2 ´ 2y2 px2 ` y2q2 “ x2 ´ y2 px2 ` y2q2 and Bx´ x x2 ` y2 ¯ “ 1 x2 ` y2 ´ 2x2 px2 ` y2q2 “ y2 ´ x2 px2 ` y2q2 , so that the condition (4.2) holds. However, consider the closed parameterized curve γptq “ pcosptq, sinptqq for 0 ď t ď 2π, which describes a circle of radius 1 around 0. Then we have ż γ f psq ¨ d⃗s “ ż 2π 0 psin 2ptq ` cos 2ptqqdt “ 2π ­“ 0, which implies that the vector ﬁeld f is not conservative. For this particular choice of X, one can in fact prove that a C 1 vector ﬁeld satisfy- ing (4.2) is conservative if and only if ż γ f psq ¨ d⃗s “ 0, for this particular curve γ. (2) If we consider the same vector ﬁeld as in (1), but deﬁned on the open set Y “ tpx, yq P R2 : x ą 0u (a half-plane), then since this half-plane is convex, and therefore star-shaped, it follows that there exists a potential g : Y Ñ R such that ∇g “ f . In 68 fact, one can check that gpx, yq “ arctanpy{xq has this property. Indeed, g is deﬁned for x ą 0, and since arctan 1pxq “ 1{p1 ` x2q, we get Bxg “ ´ y x2 1 1 ` py{xq2 “ ´ y x2 ` y2 , Byg “ 1 x 1 1 ` py{xq2 “ x x2 ` y2 on Y , which is the desired result. (3) Let pa, b, cq be real parameters. For which values of pa, b, cq b is the vector ﬁeld f px, yq “ ˆax3y ` bxy3 bx4 ` cx2y2 ˙ conservative? Since f is deﬁned on R 2, we need to check if Byf1 “ Bxf2, which becomes the equation ax3 ` 3bxy2 “ 4bx3 ` 2cxy2. Since x and y take arbitrary values, this is true if and only if # a “ 4b 3b “ 2c. This means that we can use b as an arbitrary parameter and put a “ 4b, c “ 3b 2 . For instance, this is the case when pa, b, cq “ p4, 1, 3{2q. If n “ 3, then there are three conditions (4.2). It is customary to view them as stating that an auxiliary vector ﬁeld attached to f , its curl, is zero. Definition 4.1.20. Let X Ă R3 be an open set and f : X Ñ R3 a C 1 vector ﬁeld. Then the curl of f , denoted curlpf q, is the continuous vector ﬁeld on X deﬁned by curlpf q “ ¨ ˝Byf3 ´ Bzf2 Bzf1 ´ Bxf3 Bxf2 ´ Byf1 ˛ ‚, where f px, y, zq “ pf1px, y, zq, f2px, y, zq, f3px, y, zqq. We see immediately that curlpf q “ 0 means precisely that the conditions (4.2) hold, for a 3-dimensional vector ﬁeld. Remark 4.1.21. To remember the deﬁnition, one can remember the (formal!) deter- minant curlpf q “ ˇ ˇ ˇ ˇ ˇ ˇ e1 e2 e3 Bx By Bz f1 f2 f3 ˇ ˇ ˇ ˇ ˇ ˇ , where pe1, e2, e3q is the canonical basis of R3, expanding it “as usual”, with the rule that Bx ¨ fi “ fi ¨ Bx “ Bxfi, etc. 4.2. The Riemann integral in Rn We will now describe the Riemann integral in Rn. The goal, for a closed bounded subset X Ă Rn and a continuous function f : X Ñ R, is to deﬁne its integral ż X f px1, . . . , xnqdx 69 so that it has analogue properties to the Riemann integral for n “ 1. The important diﬃculty, in comparison with the case n “ 1, is that there possibilities for X have many more diﬀerent shapes in higher dimension. Also, if X is a product of intervals X “ ra1, b1s ˆ ¨ ¨ ¨ ˆ ran, bns Ă Rn, (an n-dimensional “rectangle”) then it is fairly natural to attempt to partition into smaller rectangles, by considering partitions of each interval rai, bis. However, if X is even as simple as a disc X “ tpx, yq P R2 : x2 ` y2 ď 1u Ă R2, then it cannot be decomposed in a ﬁnite union of rectangles, or even of smaller discs. Because of this, the construction of the Riemann integral is much more involved. Since we will not be able to give the details of the proofs that this construction succeeds anyway, we will present its properties ﬁrst, and we will only discuss in a remark what is a precise limiting process that can be used as a deﬁnition (see Remark 4.2.7). For any bounded closed subset X Ă Rn and any continuous function f : X Ñ R, one can deﬁne the integral of f over X, denoted ż X f pxqdx, which is a real number, depending of course on X and on f . The integral satisﬁes the following properties: (1) (Compatibility) If n “ 1 and X “ ra, bs is an interval (with a ď b), then the integral of f over X is the Riemann integral of f : ż ra,bs f pxqdx “ ż b a f pxqdx. (2) (Linearity) If f and g are continuous on X and a, b are real numbers, then ż Xpaf1pxq ` bf2pxqqdx “ a ż X f1pxqdx ` b ż X f2pxqdx. (3) (Positivity) If f ď g, then ż X f pxqdx ď ż X gpxqdx and especially, if f ě 0, then ż X f pxqdx ě 0. Moreover, if Y Ă X is compact and f ě 0, then ż Y f pxqdx ď ż X f pxqdx. (4) (Upper bound and triangle inequality) In particular, since ´|f | ď f ď |f |, we have ˇ ˇ ˇż X f pxqdxˇ ˇ ˇ ď ż X |f pxq|dx, and since |f ` g| ď |f | ` |g|, we have ˇ ˇ ˇż Xpf pxq ` gpxqqdxˇ ˇ ˇ ď ż X |f pxq|dx ` ż X |gpxq|dx. 70 (5) (Volume) If f “ 1, then the integral of f is the “volume” in Rn of the set X, and if f ě 0 in general, the integral of f is the volume of the set tpx, yq P X ˆ R : 0 ď y ď f pxqu Ă Rn`1. In particular, if X is a bounded “rectangle”, say X “ ra1, b1s ˆ ¨ ¨ ¨ ˆ ran, bns Ă Rn and f “ 1, then (4.3) ż X dx “ pbn ´ anq ¨ ¨ ¨ pb1 ´ a1q. We write VolpXq or VolnpXq for the volume of X. (6) (Multiple integral, or Fubini’s Theorem) If n1 and n2 are integers ě 1 such that n “ n1 ` n2, then for x1 P Rn1, let (4.4) Yx1 “ tx2 P R n2 : px1, x2q P Xu Ă R n2. Let X1 be the set of x1 P Rn such that Yx1 is not empty. Then X1 is compact in Rn1 and Yx1 is compact in R n2 for all x1 P X1. If the function gpx1q “ ż Yx1 f px1, x2qdx2 on X1 is continuous, then ż X f px1, x2qdx “ ż X1 gpx1qdx1 “ ż X1 ´ż Yx1 f px1, x2qdx2¯ dx1. Similarly, exchanging the role of x1 and x2, we have ż X f px1, x2qdx “ ż X2 ´ż Zx2 f px1, x2qdx1¯dx2, where Zx2 “ tx1 : px1, x2q P Xu, if the integral over x1 is a continuous function. Remark 4.2.1. (1) The conditions we have stated are not independent, and are not the only properties of the integral that we will state. However, they are enough to get some intuition, and are suﬃcient to compute many concrete integrals. Moreover, they characterize the integral: there is at most one way to deﬁne an “integral” for all X and all f in order that all properties above are satisﬁed. (2) Property (5) is somewhat ambiguous, and could be replaced by the special case (4.3) (which is itself a special case of the formula (4.5) below); the fact that ş X dx is the volume of X would then be the deﬁnition of the volume VolpXq. (3) If the variables are x1, . . . , xn, we also write ż X f px1, . . . , xnqdx1 ¨ ¨ ¨ dxn. (4) There are at least two intuitive explanations of Fubini’s Theorem. First, if we think of integrals as generalizations of sums, then a two-variable integral is like a sum of real numbers ai,j with two indices; then Fubini’s formula amounts to ÿ i,j ai,j “ ÿ i ´ÿ j ai,j¯ “ ÿ j ´ÿ i ai,j¯ which are just diﬀerent ways of combining the sum of these numbers, and are equal because of the commutativity and associativity of addition. 71 Next, we think of volumes only, and take n “ 2 for simplicity. Consider a compact subset X Ă R 2 of the form X “ tpx, yq : a ď x ď b, f1pxq ď y ď f2pxqu, where f1 ď f2 are two continuous functions deﬁned on ra, bs. Then Fubini’s formula for the volume of X becomes VolpXq “ ż b a gpxqdx, where gpxq “ ż f2pxq f1pxq dy “ f2pxq ´ f1pxq. The function gpxq is the length of the vertical interval in X over the coordinate x (which can be thought of as a vertical slice of X), and so we say that the area of X is the integral of the length of vertical slices, which is intuitively reasonable. Note that for more complicated sets, the slices might not be just a single interval, but the same intuitive explanation applies. And similarly, the area is the integral of the length of horizontal slices of X. Example 4.2.2. (1) The simplest case of Fubini’s Theorem is when X is a “generalized rectangle”, namely X “ X1 ˆ X2, where X1 Ă Rn1 and X2 Ă R n2. Then X1 is the same set that was denoted X1 in Property (6). Moreover, for any x1 P X1, we have Yx1 “ tx2 P Rn2 : px1, x2q P X1 ˆ X2u “ X2 Ă Rn2, which is therefore independent of x1. If f is continuous on X, one can then prove that the function gpx1q “ ż Yx1 f px1, x2qdx2 “ ż X2 f px1, x2qdx2 is always continuous in that case. Hence Fubini’s Theorem takes the simple form ż X1ˆX2 f px1, x2qdx1dx2 “ ż X1 ´ż X2 f px1, x2qdx2¯dx1 “ ż X2 ´ż X1 f px1, x2qdx1¯ dx2 for any continuous function f on X. (2) Suppose now that X “ ra1, b1s ˆ ¨ ¨ ¨ ˆ ran, bns Ă Rn and that f is a function with separated variables given by f px1, . . . , xnq “ f1px1q ¨ ¨ ¨ fnpxnq, where each function fi is continuous (so f is also continuous). Then we claim that the integral takes the easy form (4.5) ż X f px1, . . . , xnqdx1 ¨ ¨ ¨ dxn “ ´ż b1 a1 f1pxqdx¯ ¨ ¨ ¨ ´ż bn an fnpxqdx¯ . Indeed, consider the case n “ 2 (the general case follows by induction): we have by Fubini’s Theorem ż X f px, yqdxdx “ ż b1 a1 gpxqdx 72 provided the function g, deﬁned by gpxq “ ż b2 a2 f1pxqf2pyqdy “ ´ż b2 a2 f2pyqdy¯ f1pxq is continuous – which is the case since f1 and f2 are continuous. Since g is a multiple of f1, we get ż X f px, yqdxdx “ ´ż b2 a2 f2pyqdy¯ ż b1 a1 f1pxqdx, which gives (4.5). (3) We want to compute the volume V of the ball of radius one in R3, namely X “ tpx, y, zq P R3 : x2 ` y2 ` z2 ď 1u. First approach. We use slices according to the z variable: since the z variable runs over r´1, 1s, according to Fubini’s Theorem, we have V “ ż 1 ´1 gpzqdz, where gpzq is the area of the subset Xz “ tpx, y, zq P Xu where the last coordinate is z. This is a disc (in the horizontal plane where this value of z is ﬁxed) of radius ? 1 ´ z2. So V “ ż 1 ´1 πp1 ´ z2qdz “ π´ 2 ´ 2 3 ¯ “ 4π 3 . Second approach. According to geometric intuition, the volume V is twice the volume of the subset X` where z ě 0, which is then X` “ tpx, y, zq P R3 : 0 ď x2 ` y2 ď 1, 0 ď z ď a1 ´ x2 ´ y2u. By Property (5), this means that V “ 2 ż D a 1 ´ x2 ´ y2dxdy where D “ tpx, yq P R 2 : x2 ` y2 ď 1u is the disc of radius 1 in R2. We use Fubini’s Theorem to compute this two-dimensional integral. Here the set X1 corresponding to the disc D is r´1, 1s (the set of possible ﬁrst coordinates of a point in D). For a given x P r´1, 1s, the possible set Yx of values of y is Yx “ r´ ? 1 ´ x2, ? 1 ´ x2s. So, according to Property (6) and Property (1), we have ż D a 1 ´ x2 ´ y2dxdy “ ż 1 ´1 gpxqdx where gpxq “ ż ? 1´x2 ´? 1´x2 a 1 ´ x2 ´ y2dy, if g is continuous. But this function gpxq is half of the area of a disc of radius 1 ´ x2, so we know that gpxq “ 1 2πp1 ´ x2q. In particular, it is indeed continuous, and as a consequence, we get ż D a 1 ´ x2 ´ y2dxdy “ π 2 ż 1 ´1p1 ´ x2qdx “ π 2 ´ 2 ´ 2 3 ¯ “ 2π 3 , 73 and ﬁnally V “ 4π{3. (4) In applying Fubini’s Theorem, it can indeed happen that the function gpxq is not continuous, although this creates no diﬃculty in practice, because of the possibility of decomposing the domain of integration, as we will discuss next. For instance, let X “ tpx, yq P R2 : 0 ď x ď 2 and 0 ď y ď 1 ` txuu (in other words, we have 0 ď y ď 1 if 0 ď x ă 1 and 0 ď y ď 2 if 1 ď x ď 2). If we take f “ 1 and therefore use the two-dimensional integral to compute the area of X using Fubini’s Theorem, we get X1 “ r0, 2s and Yx “ # r0, 1s if 0 ď x ă 1 r0, 2s if 1 ď x ď 2 for which gpxq “ ż Yx dy “ # 1 if 0 ď x ă 1 2 if 1 ď x ď 2. This function is not continuous at x “ 1. A useful tool to compute integrals in dimension ě 2 is to partition the domain of integration X. For this, we have the property that integrals “add up” over disjoint pieces, and more generally: (7) (Domain addivitity) If X1 and X2 are compact subsets of Rn, and f is con- tinuous on X1 Y X2, then (4.6) ż X1YX2 f pxqdx ` ż X1XX2 f pxqdx “ ż X1 f pxqdx ` ż X2 f pxqdx. Note that X1 X X2 is also compact, so all integrals exist. In particular, if X1 X X2 is empty, then ż X1YX2 f pxqdx “ ż X1 f pxqdx ` ż X2 f pxqdx, which is often very convenient. This simple formula holds also if the intersection X1 X X2 is “negligible”. For instance, in R2, the intersection might be a parameterized curve, and for such a set, the integral is 0 (intuitively, because it is a one-dimensional object and the integral in R2 measures area). We make the following deﬁnitions to deal with more general situations: Definition 4.2.3. (1) Let 1 ď m ď n be an integer. A parameterized m-set in Rn is a continuous map f : ra1, b1s ˆ ¨ ¨ ¨ ˆ ram, bms Ñ Rn which is C 1 on sa1, b1rˆ ¨ ¨ ¨ ˆsam, bmr. (2) A subset B Ă Rn is negligible if there exist an integer k ě 0 and parameterized mi-sets fi : Xi Ñ Rn, with 1 ď i ď k and mi ă n, such that X Ă f1pX1q Y ¨ ¨ ¨ Y fkpXkq. For instance, note that a parameterized 1-set in Rn is just a parameterized curve. Intuitively, we think of a parameterized m-set in Rn as a way to describe an m-dimensional subset of R n, but one should be aware that the image of a parameterized m-set f might 74 be of dimension smaller than m (for instance, it is possible that f is constant, in which case the image is a single point, which is an object of dimension 0). Example 4.2.4. (1) Any subset of the real axis R ˆ t0u Ă R 2 is negligible in R2. (2) More generally, if H Ă Rn is an aﬃne subspace of dimension m ă n, then any subset of Rn that is contained in H is negligible. (3) The image of a parameterized curve γ : ra, bs Ñ Rn is negligible, since γ is a 1-set in Rn, Proposition 4.2.5. Let X Ă Rn be a compact set. Assume that X is negligible. Then for any continuous function on X, we have ż X f pxqdx “ 0. We do not prove this, but illustrate this (fairly natural) property with examples. Example 4.2.6. (1) Consider the graph X “ tpt, γptqq : a ď t ď bu of a continuous function g : ra, bs Ñ R. This is the image of the parameterized curve t ÞÑ pt, γptqq, so it is negligible. Indeed, we can check the proposition in that case using Fubini’s Theorem: for any function f continuous on X, we have ż X gpx, yqdxdy “ ż b a ´ż f pxq f pxq gpx, yqdy¯dx “ 0, since an integral over a one-point interval is zero, and the integral of the zero function is zero by linearity. (2) The formula (4.6) also explains why the volume of the unit ball X Ă R3 in Example 4.2.2 is twice the volume of the hemisphere X` with z ě 0. Indeed, let X1 “ X` and X2 “ X´, the lower hemisphere. Since X “ X` Y X´, by Property (7), we have V “ ż X dxdydz “ ż X` dxdydz ` ż X´ dxdydz ´ ż X`XX´ dxdydz. The intersection X` X X´ is D ˆ t0u, where D Ă R 2 is the disc of radius 1. So it is negligible by Example 4.2.4, (2) (one can also see that this is the image in R 3 of the parameterized 2-set given by pr, θq ÞÑ pr cospθq, r sinpθqq on r0, 1s ˆ r0, 2πs). It follows by the proposition that ż D dxdydz “ 0, and hence V “ ż X dxdydz “ ż X` dxdydz ` ż X´ dxdydz. To show that the volume of X´ is the same as that of X`, one can use the same method as in Example 4.2.2 (later, we will see the change of variable formula that allows us to do this more directly). Remark 4.2.7. We will explain here one possible deﬁnition of the Riemann integral in Rn. It goes in the following steps: 75 (1) Deﬁnition of integrable functions on a closed bounded rectangle X “ ra1, b1s ˆ ¨ ¨ ¨ ˆ ran, bns. Namely, consider ﬁnite partitions of each interval ai “ ti,0 ă ti,1 ă ¨ ¨ ¨ ă ti,k “ bi which induce a partition of X into smaller rectangles Xj1,...,jn “ rt1,j1, t1,j1`1s ˆ ¨ ¨ ¨ ˆ rtn,jn, tn,jn`1s. Each such rectangle has n-dimensional volume mpj1, . . . , jnq “ pt1,j1`1 ´ t1,j1q ¨ ¨ ¨ ptn,jn`1 ´ tn,jnq. Each such partition deﬁnes an upper Riemann sum and a lower Riemann sum: S` “ k´1ÿ j1“0 ¨ ¨ ¨ k´1ÿ jn“0 ´ sup xPXj1,...,jn f pxq ¯mpj1, . . . , jnq S´ “ k´1ÿ j1“0 ¨ ¨ ¨ k´1ÿ jn“0 ´ inf xPXj1,...,jn f pxq ¯mpj1, . . . , jnq We say that f is Riemann-integrable over X if sup S´ “ inf S`, where we consider supremum and inﬁmum over all upper and lower Riemann sums computed for every possible partition. We then deﬁne ż X f pxqdx “ sup S´ “ inf S`. Such functions are not necessarily continuous, but all continuous functions on X are Riemann-integrable. (2) Deﬁnition of Jordan-measurable subsets X Ă Rn, which are necessarily bounded in Rn: we say that a bounded set X, contained in a closed rectangle B “ r´R, Rs n of “radius” R ą 0 around 0 is Jordan-measurable if the function deﬁned on B by ϕpxq “ # 1 if x P X 0 if x R X is integrable in the sense of (1). One then checks that this deﬁnition is indepen- dent of the choice of the radius R. (3) For a Jordan-measurable subset X Ă Rn, and a function f : X Ñ Rn, consider a closed bounded rectangle X 1 such that X Ă X 1. Then we say that f is integrable over X if the function rf px1, . . . , xnq “ # f px1, . . . , xnq if px1, . . . , xnq P X 0 otherwise, is integrable over the rectangle X 1, in the sense of the deﬁnition in Step (1), and we deﬁne ż X f px1, . . . , xnqdx “ ż X 1 rf px1, . . . , xnqdx. 76 Note that rf is, in general, not continuous, even if f is. One can show that if X is Jordan-measurable, then every continuous function f on X is integrable in this sense. To be precise, this deﬁnition leads to some restrictions on the compact sets X that are allowed, but all “usual” compact sets (such as rectangles, balls, etc) are Jordan- measurable, so this is not an issue in applications. The more general deﬁnition that leads to the integral over arbitrary compact subsets that we have discussed is that of the Lebesgue integral. With this restriction concerning X, the Riemann integral whose deﬁnition is sketched above satisﬁes, for continuous functions, all Properties described above. 4.3. Improper integrals As in the one-dimensional case, one is often interested in extending the integral to unbounded domains, or to open bounded regions with functions that are not bounded. This is done by taking appropriate limits of integrals over compact subsets of the region of interest. We indicate just some basic deﬁnitions in R2. Let I Ă R be a bounded interval and let J “ ra, `8r for some a P R. Let f be a continuous function on X “ J ˆ I. We say that it is Riemann-integrable on X if the limit lim xÑ`8 ż ra,xsˆI f px, yqdxdy “ lim xÑ`8 ż x a ´ż I f px, yqdy¯ dx “ lim xÑ`8 ż I ´ż x a f px, yqdx ¯dy exists (the equality being cases of Fubini’s Theorem). We then denote this limit by ż JˆI f px, yqdxdy. If f ě 0, or more generally if |f | is Riemann integrable on X, one can prove the Fubini formula ż JˆI f px, yqdxdy “ ż 8 a ´ż I f px, yqdy¯dx “ ż I ´ż `8 a f px, yqdx ¯ dy, where each improper integral is a one-variable integral (this formula is however not always true without some assumption). Similarly, let f be continuous on R2. Assume that f ě 0. We say that f is Riemann- integrable on R2, if the limit lim RÑ`8 ż r´R,Rs2 f px, yqdxdy exists, which is then called the integral of f over R2 and denoted ż R2 f px, yqdxdy. One can then show that this integral is also the limit of ż DR f px, yqdxdy where DR is the disc of radius R centered at 0 (or any increasing sequence of compact subsets of R 2 whose union is R2). There is also the Fubini formula ż R2 f px, yqdxdy “ ż 8 ´8 ´ż `8 ´8 f px, yqdy¯dx “ ż `8 ´8 ´ż `8 ´8 f px, yqdx¯ dy, 77 again with “ordinary” improper integrals in the last two formulas. Remark 4.3.1. In all these cases, we also often say that “the integral converges” to indicate that a function is Riemann-integrable on an unbounded set. The following comparison principle is the easiest way to prove that a certain improper integral exists: if |f | ď g (resp. 0 ď f ď g), and we know that ż JˆI gpx, yqdxdy or ż R2 gpx, yqdxdy exists, then so does ż JˆI f px, yqdxdy or ż R2 f px, yqdxdy, respectively. Example 4.3.2. (1) Consider the improper Riemann integral ż r0,`8rˆr1,2s xe´xydxdy. We have for any R ą 0 ż R 0 ´ż 2 1 xe´xydy¯ dx “ ż R 0 x”´ 1 xe´xyı2 1dx “ ż R 0 ´ e ´x ´ e ´2x¯dx. This can be evaluated and is equal to p1 ´ e ´Rq ´ 1 2 p1 ´ e ´2Rq “ 1 2 ´ e ´R ` e´2R Ñ 1 2. Hence the integral converges and is equal to 1{2. (2) In Example 4.4.3 (3) below, we will see that the improper integral ż R2 e ´px2`y2qdxdy exists and is equal to π. 4.4. The change of variable formula We now consider the analogue for the integral in Rn of the change of variable formula ż f pgpxqqg1pxqdx “ ż f pyqdy of one-variable calculus. Let ¯X Ă Rn and ¯Y Ă Rn be compact subsets. Let ϕ : ¯X Ñ ¯Y be a continuous map. We assume that we can write ¯X “ X Y B, ¯Y “ Y Y C where (1) the sets X and Y are open; (2) the sets B and C are negligible, in the sense of Deﬁnition 4.2.3; (3) the restriction of ϕ to the open set X is a C 1 bijective map from X to Y . In this situation, the Jacobian matrix Jϕpxq is invertible at all x P X; we assume that we can ﬁnd a continuous function on ¯X that restricts to detpJϕpxqq on X (this is usually obvious because we have a formula for the Jacobian, which makes sense and is clearly continuous on X). We abuse notation and still write detpJϕpxqq for this function, even if x P B. 78 Remark 4.4.1. (1) Note that there is no assumption concerning the image of B. (2) It is very frequent that ϕ is the restriction of a C 1 map Rn Ñ Rn, in which case the determinant of the Jacobian matrix is continuous everywhere, so that the last issue doesn’t require any argument. Theorem 4.4.2 (Change of variable formula). In the situation described above, for any continuous function f on ¯Y , we have ż ¯X f pϕpxqq| detpJϕpxqq|dx “ ż ¯Y f pyqdy. If one wants to remember this formula, the mnemonic is that when y “ ϕpxq, we have dy “ | detpJϕpxqq|dx. Example 4.4.3. (1) The simplest (but very important) case of the formula is when ϕpxq “ x ` x0 is a translation. Intuitively, this shouldn’t change the volume, or the integral. Indeed, since ϕ is aﬃne-linear, we have Jϕpxq “ 1n, the identity matrix, for all x. The change of variable formula becomes ż ¯X f px ` x0qdx “ ż x0` ¯X f pxqdx for any compact subset ¯X and any continuous function f on x0 ` ¯X. With f “ 1, we see that the volume of ¯X and that of x0 ` ¯X are the same. (2) The next most important special case is when ϕ is the restriction of a bijective linear map, namely ϕpxq “ Ax, where A is an invertible matrix of size n. Then Jϕpxq “ A for all x P Rn, with constant determinant detpAq. Let ¯X “ X YB be a compact set as above and ¯Y “ ϕp ¯Xq. Then ϕp ¯Xq “ ϕpXqYϕpBq. The change of variable formula becomes ż ¯X f pϕpxqqdx “ 1 | detpAq| ż ¯Y f pyqdy for any continuous function f on ¯Y . Take especially f to be the function equal to 1 on ¯Y , so that the integral of f over ¯Y is the n-dimensional volume of Y . Note that f pϕpxqq is the characteristic function of the set tx P Rn : Ax P Y u, in other words of A´1Y . We get VolpY q “ | detpAq| VolpA ´1Y q, which shows how the volume is transformed (dilated or contracted) under a linear map. If we replace A ´1Y by X, which means that Y “ AX, then we get equivalently VolpAXq “ | detpAq| VolpXq for any compact subset X Ă Rn. For instance, let X “ r0, 1s n be the unit cube in Rn. Its volume is 1, and therefore VolpAr0, 1s nq “ | detpAq|, which provides the geometric interpretation of the determinant of real matrices. It is actually possible to prove directly this last formula. For instance, observe that if A is diagonal, with diagonal entries a1, . . . , an, then Ar0, 1s n “ r0, a1s ˆ ¨ ¨ ¨ ˆ r0, ans, 79 which has volume |a1| ¨ ¨ ¨ |an| “ | detpAq|. One can also argue directly when A is an “elementary” matrix, for instance A “ ¨ ˝1 0 1 0 1 0 0 0 1 ˛ ‚ for n “ 3. Since Apx, y, zq “ px ` z, y, zq, one can check that Ar0, 1s 3 is the set Y “ tpx, y, zq : 0 ď y ď 1, 0 ď z ď 1, z ď x ď 1 ` zu. One can compute the volume of Y by applying Fubini’s Theorem (using the variable x in the inner integral). This gives VolpY q “ ż r0,1s2 ´ż 1`z z dx¯ dydz “ ż r0,1s2 dydz “ 1 “ detpAq. One can also intuitively observe that Y “ Y1 Y Y2, where Y1 “ tpx, y, zq : 0 ď y ď 1, 0 ď z ď 1, z ď x ď 1u, Y2 “ tpx, y, zq : 0 ď y ď 1, 0 ď z ď 1, 1 ď x ď 1 ` zu, and if translate Y2 by the vector p´1, 0, 0q, we obtain Y3 “ Y2 ´ p1, 0, 0q2 “ tpx, y, zq : 0 ď y ď 1, 0 ď z ď 1, 0 ď x ď zu, and then Y3 YY1 “ r0, 1s 3. Since Y3 XY1 is negligible, and the volume of Y2 is equal to that of Y3 (by (1), since Y3 is a translate of Y2), we get 1 “ Volpr0, 1s 3q “ VolpY1q ` VolpY2q “ VolpY q, again. (3) We consider the function f px, yq “ e ´px2`y2q and we want to compute its integral over the compact disc ¯YR “ tpx, yq P R2 : x2 ` y2 ď R2u where R ą 0 is a parameter. Note that ¯YR “ YR Y CR with YR “ tpx, yq P R2 : 0 ă x2 ` y2 ă R2, y ­“ 0 if x ă 0u, which is open, and CR is the union of the segment r´R, 0s ˆ t0u and of the circle of radius R, each of which is a parameterized curve, so that CR is negligible. Consider the polar coordinate change of variable ϕ : ¯XR Ñ ¯YR, where ¯XR “ r0, Rs ˆ r´π, πs and ϕpr, θq “ pr cospθq, r sinpθqq (see Example 3.6.2). Note that ϕ is continuous on ¯XR, and that the restriction of ϕ to a map from XR to YR, where XR “s0, Rrˆs ´ π, πr, is bijective and of class C 1 (see Figure 4.3). 80 Figure 4.3. Polar coordinates and boundaries The Jacobian matrix is Jϕpr, θq “ ˆcospθq ´r sinpθq sinpθq r cospθq ˙ , with determinant equal to detpJϕpr, θqq “ r. We have ¯XR “ XR Y BR where BR “ tpr, θq P XR : r “ 0 or r “ R or |θ| “ πu is negligible (it is the union of four line segments). Note that the Jacobian matrix is a function that makes sense and is continuous on the whole of ¯XR. The change of variable formula is applicable, and it means that ż ¯XR e ´r2rdrdθ “ ż ¯YR e ´px2`y2qdxdy. We can compute the integral in the left-hand side easily using Fubini’s Theorem: ż ¯XR e´r2rdrdθ “ ż R 0 re´r2´ż π ´pi dθ¯ dr “ 2π ż R 0 e ´r2rdr “ 2π”´1 2 e ´r2ıR 0 “ πp1 ´ e ´R2q. If we let R Ñ `8, we conclude that the improper Riemann integral of f over R2 converges and satisﬁes ż R2 e´px2`y2qdxdy “ π. We can go further and derive an interesting consequence of this computation. Consider instead the integral of f over a square, namely ż SR e´px2`y2qdxdy where SR “ r´R, Rs2. Since f is a function with separated variables, we can reduce this integral to a one-variable integral by Fubini’s Theorem (see (4.5)): we have ż SR e ´px2`y2qdxdy “ ´ż R ´R e ´x2dx¯2. But now observe that f ě 0 and that YR Ă SR Ă Y2R, 81 Figure 4.4. A sector so that by positivity (Property (3) of the integral), we know that ż YR e´px2`y2qdxdy ď ´ż R ´R e´x2dx ¯2 ď ż Y2R e ´px2`y2qdxdy, which means that πp1 ´ e´R2q ď ´ż R ´R e ´x2dx¯2 ď πp1 ´ e ´4R2q. If we let R Ñ `8, both the ﬁrst and the third quantities converge to π. We conclude that the improper Riemann integral of e´x2 over R exists and satisﬁes ż R e´x2dx “ ? π. There are standard examples of change of variable (in the sense also of Section 3.6) that are often used to perform integrals over speciﬁc domains that have particularly nice parameterizations in the new variables. (1) Polar coordinates pr, θq are useful for integrating over a disc in R2 centered at 0, or more generally over a disc sector ∆ “ ∆pa, b, Rq deﬁned by 0 ď r ď R, ´π ă a ď θ ď b ă π for some parameters pa, b, Rq. We computed the jacobian determinant in the previous example, and one gets the general formula (4.7) ż ∆ f px, yqdxdy “ ż R 0 ż b a f pr cos θ, r sin θqrdrdθ. Taking r to vary between 0 ă r0 ď r ď R, we obtain an annulus. (2) Spherical coordinates pr, θ, ϕq in R3 (Example 3.10.3 (2)) are useful for in- tegrating over balls centered at 0, or parts of them. We computed the jacobian and its determinant ´r2 sinpϕq in (3.5) and (3.6). So, for integrating a function f over a ball B of radius R in R3, we have the formula ż B f px, y, zqdxdydz “ ż R 0 ż 2π 0 ż π 0 f pr cos θ sin ϕ, r sin θ sin ϕ, r cos ϕqr2 sinpϕqdrdθdϕ (since it is easy to see that the boundary parts are neligible). Note that sinpϕq ě 0 for 0 ď ϕ ď π, so that the absolute value of the jacobian determinant is indeed r2 sinpϕq. 82 Example 4.4.4. (1) We compute the integral I of x2y over the sector given by ∆ “ t0 ď r ď 2, π{6 ď θ ď π{2u. In polar coordinates, this becomes I “ ż 2 0 ż π{2 π{6 r4 cos 2 θ sin θdrdθ “ ”r5 5 ı2 0 ż π{2 π{6 cos 2pθq sinpθqdθ. If we replace the trigonometric functions by their exponential versions, the function cos 2pθq sinpθq becomes cos 2pθq sinpθq “ 1 8i peiθ ` e´iθq 2pe iθ ´ e ´iθq “ 1 8i pe2iθ ` 2 ` e´2iθqpeiθ ´ e ´iθq “ 1 8i pe3iθ ´ e ´3iθ ` e iθ ´ e´iθq “ 1 4 psinp3θq ` sinpθqq.(4.8) Therefore I “ 32 5 ˆ 1 4 ż π{2 π{6 psinp3θq ` sinpθqqdθ “ 8 5 ”´1 3 cosp3θq ´ cospθq ıπ{2 π{6 “ 8 cospπ{6q 5 “ 4 ? 3 5 . (2) We compute the integral I of z2 over the spherical shell in R3 given by 1 ď r ď 2 in spherical coordinates. Since z “ r cospϕq, we get I “ ż 2 1 ż 2π 0 ż π 0 r4 cos 2pϕq sinpϕqdrdθdϕ We use the formula (4.8) to write this ﬁnally as “ 2π ˆ ”r5 5 ı2 1 ˆ 1 4 ż π 0 psinp3ϕq ` sinpϕqqdϕ “ 2π ˆ ´ 32 5 ´ 1 5 ¯ ˆ 2 3 “ 124π 15 . 4.5. Geometric applications of integrals Besides the fact that the integral can be used to deﬁne and compute volumes of subsets of Rn, there are quite a few other natural geometric quantities that can be expressed in terms of integrals. We present some of them in this section. (1) [Center of mass] Let X be a compact subset of R n, such that the volume of X is positive. The center of mass (or barycenter ) of X is the point ¯x P Rn such that ¯x “ p¯x1, . . . , ¯xnq with ¯xi “ 1 VolpXq ż X xidx. Intuitively, ¯xi is the average over X of the i-th coordinate, and ¯x is the point where X is “perfectly balanced”. Note that ¯x is not necessarily in X (for instance, for an annulus X “ tpx, yq P R2 : 1 ď x2 ` y2 ď 2u in R2, the center of mass is p0, 0q), but this is the case if X is convex. 83 (2) [Surface area] Consider a continuous function f : ra, bs ˆ rc, ds Ñ R which is C 1 on sa, brˆsc, dr. Let Γ “ tpx, y, zq P R3 : px, yq P ra, bs ˆ rc, ds, z “ f px, yqu Ă R3 be the graph of f . Intuitively, this is a surface, and it should have an area. This is in fact given by ż b a ż d c b 1 ` pBxf px, yqq2 ` pByf px, yqq2dxdy. Such a result also holds for the graphs of functions deﬁned on other sets, such as discs, provided they are C 1 in the “interior” of the domain. There is an analogue formula for the length of the graph of a function f : ra, bs Ñ R, namely it is equal to ż b a a 1 ` f 1pxq2dx. Example 4.5.1. (1) What is the center of mass of a cone X “ tpx, y, zq P R3 : 0 ď z ď 1, x 2 ` y2 ď p1 ´ zq 2u in R3? (This is a cone because for a given z, the “slice” of X where z is ﬁxed is a disc centered at 0 with radius 1 ´ z). For symmetry reasons, we have ¯x “ ¯y “ 0 (you should check that), so the question is to compute ¯z. First we compute the volume, using Fubini’s Theorem VolpXq “ ż X dxdydz “ ż 1 0 ´ πp1 ´ zq2¯ dz “ π 3 . Next we compute ż X zdxdydz “ ż 1 0 z´ πp1 ´ zq 2¯ dz “ π 12, so that the center of mass is p0, 0, 1{4q. (2) What is the surface S of the sphere X “ tpx, y, zq : x2 ` y2 ` z2 “ 1u of radius 1 in R3? Geometrically, this is twice the area of the graph of the function f px, yq “ a 1 ´ x2 ´ y2 deﬁned for px, yq such that x2 ` y2 ď 1. Although this is not deﬁned over a rectangle, an analogue of the formula above holds, and we have S “ 2 ż D b 1 ` pBxf q2 ` pByf q2dxdy where D is the disc of radius 1 centered at p0, 0q in R 2. We have Bxf “ ´ x a 1 ´ x2 ´ y2 , Byf “ ´ y a 1 ´ x2 ´ y2 , hence the surface is S “ 2 ż D ´ 1 ` x2 1 ´ x2 ´ y2 ` y2 1 ´ x2 ´ y2 ¯1{2dxdy “ 2 ż D 1 a 1 ´ x2 ´ y2 dxdy. 84 Using polar coordinates (4.7), this becomes S “ 4π ż 1 0 r ? 1 ´ r2 dr “ 4π” ´? 1 ´ r2ı1 0 “ 4π. 4.6. The Green formula In the last sections, we will discuss two important formulas which are of the form ż BX f “ ż X Df, where (1) f is a C 1 vector ﬁeld deﬁned on Rn; (2) X Ă Rn is a compact m-dimensional subset, with 1 ď m ă n; (3) BX is the “boundary” of X, which has dimension m ´ 1, and the integral on BX is a generalization of a line integral; (4) Df is some expression computed using the partial derivatives of ﬁrst order of f . In fact, there exist versions of these results in all dimensions, but we focus here on the cases n “ m “ 2 (Green’s formula) and, in the next section, on the case n “ m “ 3 (Gauss–Ostrogradski formula). 1 In all cases, the prototype is the Fundamental Theorem of Calculus, in the form (4.9) ż b a f 1pxqdx “ f pbq ´ f paq, where X “ ra, bs and the boundary is simply the set ta, bu with two elements. The Green formula concerns the case of relating an integral over a subset X of R 2 with a line integral over its boundary. The typical example is an integral over a compact disc of radius r ą 0 centered at x0, which is related to a line integral over the circle of radius r centered at x0. The diﬃculty in a rigorous formulation of this formula is mostly in precisely under- standing which subsets X are allowed, and what “boundary” means. Moreover there is an issue of orientation of the boundary (reﬂected in (4.9) in the fact that the sign of f pbq and f paq is not the same on the right-hand side). Definition 4.6.1. A simple closed parameterized curve γ : ra, bs Ñ R2 is a closed parameterized curve such that γptq ­“ γpsq unless t “ s or ts, tu “ ta, bu, and such that γ1ptq ­“ 0 for a ă t ă b. (If γ is only piecewise C 1 inside sa, br, this condition only applies where γ1ptq exists). Example 4.6.2. (1) A circle parameterized by γptq “ px0 ` r cosptq, y0 ` r sinptqq for 0 ď t ď 2π is a simple closed parameterized curve. But if we consider the circle twice over (i.e., for 0 ď t ď 4π), then it is not. (2) The lemniscate λ (Figure 4.1) deﬁned by (4.1) is not a simple closed curve, since λpπ{2q “ λp3π{2q “ 0. 1 The most general statement is known as the Stokes formula. 85 Figure 4.5. The set is on the left Figure 4.6. The set is on the left Theorem 4.6.3 (Green’s formula). Let X Ă R2 be a compact set with a boundary BX that is the union of ﬁnitely many simple closed parameterized curves γ1, . . . , γk. Assume that γi : rai, bis Ñ R2 has the property that X lies always “to the left” of the tangent vector γ1 iptq based at γiptq. Let f “ pf1, f2q be a vector ﬁeld of class C 1 deﬁned on some open set containing X. Then we have ż X ´Bf2 Bx ´ Bf1 By ¯ dxdy “ kÿ i“1 ż γi f ¨ d⃗s. The condition that X be on the left of the boundary is illustrated in Figure 4.5. We then say that the boundary is positively oriented by the corresponding parameterized curves γi. Note that if this condition is not met, it simply means that one must “reverse” the corresponding curve, e.g., replace γ : r0, 1s Ñ R2 by rγptq “ γp1 ´ tq for 0 ď t ď 1, which reverses the orientation of the tangent vector. Another case, where there are two boundary curves, shows again the way the boundary must be oriented possibly in diﬀerent directions depending on which part of the boundary is involved (see Figure 4.6). Example 4.6.4. (1) Suppose that the set X has only one boundary curve γ, and that f is a conservative vector ﬁeld. Then we see that the Green formula holds, since both sides are then zero (the right-hand side by Remark 4.1.9, and the left-hand side by Example 4.1.14 (2)). 86 (2) If X is a closed disc of radius r ą 0 around px0, y0q P R2, then the boundary is the circle which is the image of the parameterized curve γptq “ px0 ` r cosptq, x0 ` r sinptqq for 0 ď t ď 2π. Note that this is a simple closed curve; the tangent vector is γ1ptq “ p´r sinptq, r cosptqq and one sees on a picture that the disc is to the left of γ1ptq (e.g., γ1p0q “ p0, rq is a vertical vector based at γp0q “ pr, 0q, so the disc is located to the left). Hence the Green formula becomes ż X ´ Bf2 Bx ´ Bf1 By ¯dxdy “ ż γ f ¨ d⃗s. Let us specialize the vector ﬁeld to f px, yq “ p0, xq. Then the formula becomes ż X dxdy “ ż γ f ¨ d⃗s. Indeed, the left-hand side is the area πr2 of the disc, and we can check that the right-hand side is ż 2π 0 px0 ` r cosptqqpr cosptqqdt “ ż 2π 0 r2 cos 2ptqdt “ r2 ż 2π 0 1 2 ´ 1 ` cosp2tq¯ dt “ πr2. In this case, it is most likely the computation of the area of the disc that is the main interest. Many other vector ﬁelds have the property that Bf2 Bx ´ Bf1 By “ 1 (e.g. f px, yq “ pgpxq, xq where g is an arbitrary function) but it is of course best to choose a simple one to facilitate the computation of the line integral. (3) More generally, we can always use the Green formula to compute an integral over X. Indeed, for any function g, we can ﬁnd many vector ﬁelds f “ pf1, f2q such that g “ Bxf2 ´ Byf1. For instance, we can put f1 “ 0 and ﬁnd f2 by solving Bxf2 “ g (computing a primitive with respect to the x variable). As an example, let gpx, yq “ x2y2 and let X be the interior of an ellipse centered at 0 with axes lengths a ą 0 in the x-direction and b ą 0 in the y-direction. We want to compute ż X gpx, yqdxdy. We put f px, yq “ p0, 1 3x3y2q to have Bxf2 “ g, and we parameterize the boundary by γptq “ pa cosptq, b sinptqq, 0 ď t ď 2π, which is a simple closed parameterized curve. So ż X gpx, yqdxdy “ ż γ f ¨ d⃗s “ 1 3a3b2 ż 2π 0 cos 3ptq sin 2ptq ˆ b cosptqdt. 87 Using trigonometric computations as in Example 4.4.4, we ﬁnd that ż 2π 0 cos 4ptq sin2ptqdt “ π 8 , so the integral is πa 3b3{24. (4) Consider the square X “ r0, 1s 2 Ă R2 and the vector ﬁeld f px, yq “ pxy, x 2 ´ y2q. We want to compute the line integral over the boundary ż BX f ¨ d⃗s, where the boundary is taken counterclockwise (so that it satisﬁes the “set is on the left” condition). We do not even need to write a parameterization of the boundary square. By Green’s Formula and Fubini’s Formula, we get ż BX f ¨ d⃗s “ ż 1 0 ż 1 0 ´ 2x ´ x¯ dxdy “ 1 2 . (5) Green’s formula is equivalent with a variant where we integrate the divergence of a vector ﬁeld f “ pf1, f2q, which we recall is deﬁned by divpf q “ TrpJf q “ Bxf1 ` Byf2 (see Deﬁnition 3.3.11). Indeed, note that divpf q “ Bx rf2 ´ By rf1, where rf px, yq “ p´f2, f1q. So we have, under the assumptions that Green’s Formula is valid for X and its boundary, the relation ż X divpf qdxdy “ kÿ i“1 ż γi rf ¨ d⃗s. It is customary to note that the line integral for the boundary component γi is the integral of rf1pγiptqqγ1 i,1ptq ` rf2pγiptqqγ1 i,2ptq “ ´f2pγiptqqγ1 i,1ptq ` f1pγiptqqγ1 2,iptq “ f pγiptqq ¨ ⃗nptq where ⃗nptq “ pγ1 i,2ptq, ´γ1 i,1ptqq. For this reason, this variant of the Green formula is often written ż X divpf qdxdy “ kÿ i“1 ż γi f ¨ d⃗n. For each parameterized curve, note that ⃗nptq ¨ γ1ptq “ 0 for all t: in other words, ⃗nptq is a vector perpendicular (or normal ) to the tangent vector to the curve, and that it points “outwards” of X (i.e., it goes “to the right” since γ1 has the property that X is “to the left”). In fact, this vector is characterized by the conditions that (1) the length of ⃗nptq is the same as the length of γ1ptq; (2) it is perpendicular to γ1ptq; (3) ⃗nptq is directed “outwards”. One says that ⃗n is the “exterior normal vector”. As a further special case of the divergence form of the Green formula, when we apply it to the gradient ﬁeld ∇g of a function g, then we obtain ż X ∆pgqdxdy “ kÿ i“1 ż γi ∇pgq ¨ d⃗n 88 since divp∇gq “ ∆g is the Laplacian of g (Example 3.5.8). For instance, it follows that if ∇g is parallel to the boundary (i.e., orthogonal to ⃗n), then the integral of ∆g over X is zero, which is not at all obvious from the deﬁnition! We state separately the general example of using the Green formula for a suitable vector ﬁeld to compute the area of a region: Corollary 4.6.5. Let X Ă R2 be a compact set with a boundary BX that is the union of ﬁnitely many simple closed parameterized curves γ1, . . . , γk. Assume that γi “ pγi,1, γi,2q : rai, bis Ñ R2 has the property that X lies always “to the left” of the tangent vector γ1 iptq based at γiptq. Then we have VolpXq “ kÿ i“1 ż γi x ¨ d⃗s “ kÿ i“1 ż bi ai γi,1ptqγ1 i,2ptqdt. 4.7. The Gauss–Ostrogradski formula The Gauss–Ostrogradski formula is an analogue of the Green formula in R3. Thus it concerns a 3-dimensional compact set X Ă R3, with boundary S “ BX which is a surface (2-dimensional). Definition 4.7.1. A parameterized surface Σ : ra, bs ˆ rc, ds Ñ R3 is a 2-set in R3 such that the rank of the Jacobian matrix is 2 at all ps, tq Psa, brˆsc, dr. Note that since there are two variables, two is the maximal possible rank for the jacobian matrix. Example 4.7.2. (1) Consider a function g : ra, bsˆrc, ds Ñ R that is C 1 in sa, brˆsc, dr. Then the function Σps, tq “ ps, t, gps, tqq deﬁnes a parameterized surface in R 3, whose image is the graph of g. Indeed, the Jacobian matrix is JΣps, tq “ ¨ ˝ 1 0 0 1 Bsg Btg ˛ ‚ which has rank 2 for all ps, tq, since the ﬁrst two rows are linearly independent. (2) The sphere of radius r ą 0 centered at px0, y0, z0q is the image of the parameterized surface Σps, tq “ ¨ ˝x0 ` r cospsq sinptq y0 ` r sinpsq sinptq z0 ` r cosptq ˛ ‚ for ps, tq P r0, 2πs ˆ r0, πs. The Jacobian matrix is ¨ ˝ ´r sinpsq sinptq r cospsq cosptq r cospsq sinptq r sinpsq cosptq 0 ´r sinptq ˛ ‚. It has rank 2 if ps, tq Ps0, 2πrˆs0, πr (in that case, the second and third rows deﬁne an invertible 2 ˆ 2 matrix unless cospsq “ 0; but when that is the case, namely s “ π{2 or 3π{2, the ﬁrst and the third rows deﬁne an invertible 2 ˆ 2 matrix). 89 (3) Another parameterization of the same sphere is given by Σps, tq “ 1 p1 ` s2 ` t2q ¨ ˝ x0 ` 2rs y0 ` 2rt z0 ` rp1 ´ s2 ´ t 2q ˛ ‚ for ps, tq P R2 (although this is not a compact set in R2). Indeed, ﬁrst note that }Σps, tq ´ px0, y0, z0q} 2 “ 1 p1 ` s2 ` t2q2 ´ 4r2s2 ` 4r2t 2 ` r2p1 ´ s2 ´ t 2q 2¯ “ r2 for all ps, tq P R2, so that the image of Σ is contained in the sphere. It covers the whole sphere, except p0, 0, ´1q. Indeed, we may assume by translating that px0, y0, z0q “ 0. Then consider ps, tq with s2 ` t2 “ u2 ﬁxed (in other words, a circle of radius u). The image of this subset of R2 is the circle centered at p0, 0, p1´u 2q{p1`u2qq that is contained in the unit sphere. The function u ÞÑ p1 ´ u 2q{p1 ` u 2q “ ´1 ` 2{p1 ` u 2q is strictly decreasing for u ě 0, going from 1 to the limit ´1 as u Ñ `8. The Jacobian matrix is 1 1 ` s2 ` t2 ¨ ˝ 2r ´ 4rs2{p1 ` s2 ` t 2q ´4rst{p1 ` r2 ` s2q ´4rst{p1 ` s2 ` t2q 2r ´ 4rt2{p1 ` s2 ` t2q ´4rs{p1 ` s2 ` t 2q ´4rt{p1 ` s2 ` t 2q ˛ ‚. It is of rank 2 for all ps, tq (check that the second and third rows are independent unless s “ 0, in which case the ﬁrst and second rows are independent). We next recall a deﬁnition from linear algebra. Definition 4.7.3. Let x and y be two linearly independent vectors in R3. The vector product, or cross product z “ x ˆ y is the unique vector in R3 such that px, y, zq is a basis of R3 with detpx, y, zq ą 0, and }z} “ }x} }y} sinpθq, where θ is the angle between x and y. If x and y are not linearly independent, we just deﬁne x ˆ y “ 0, the zero vector. The formula for the length of the cross-product is still valid. We recall that there is in fact an elementary formula: if x “ px1, x2, x3q and y “ py1, y2, y3q, then x ˆ y “ ¨ ˝ x2y3 ´ x3y2 x3y1 ´ x1y3 x1y2 ´ x2y1 ˛ ‚“ det ˇ ˇ ˇ ˇ ˇ ˇ e1 e2 e3 x1 x2 x3 y1 y2 y3 ˇ ˇ ˇ ˇ ˇ ˇ , (with the same formal style of computation as in Remark 4.1.21, where pe1, e2, e3q is the canonical basis of R 3). Remark 4.7.4. In particular, note the useful formulas e1 ˆ e2 “ e3, e2 ˆ e3 “ e1, e3 ˆ e1 “ e2, and y ˆ x “ ´x ˆ y. If pf1, f2, f2q is a basis basis in R3, there are two possibilities: either detpf1, f2, f3q ą 0 or detpf1, f2, f3q ă 0. The ﬁrst type are called positively oriented. An example is the canonical basis pe1, e2, e3q, which has determinant 1. If the basis pf1, f2, f3q is orthogonal, it is possible to check that all positively oriented orthonormal bases, for instance pf1{}f1}, f2{}f2}, f3{}f3}q, are of the form pAe1, Ae2, Ae3q 90 where A is a rotation matrix (an element of SO3pRq). Intuitively, that means they can be obtained from the canonical basis by rotation. Let Σ : ra, bs ˆ rc, ds Ñ R3 be a parameterized surface such that Σ is injective on sa, brˆsc, dr. For all ps, tq, the vector ⃗n “ BsΣps, tq ˆ BtΣps, tq is orthogonal to the two vectors BsΣps, tq and BtΣps, tq, which are linearly independent since the Jacobian matrix of Σ has rank 2. Intuitively, the two vectors span the tangent plane to the surface, hence this vector ⃗n is perpendicular to the surface. Consider now a 3-dimensional compact subset X of R3 with boundary BX given by the image of the parameterized surface Σ : ra, bs ˆ rc, ds Ñ BX. (For instance, S could be a ball in R3 of some radius r ą 0, and the boundary BS would be the corresponding sphere sphere.) For the boundary surface Σ, the orientation condition that is the correct analogue of that concerning the boundary curves in Theorem 4.6.3 is now that the normal vector ⃗n based at any point of the boundary should point away from X: it should be an “exterior normal vector”. Example 4.7.5. Consider the parameterized sphere of Example 4.7.2. Then BsΣ “ ¨ ˝ ´r sinpsq sinptq r cospsq sinptq 0 ˛ ‚, BtΣ “ ¨ ˝ r cospsq cosptq r sinpsq cosptq ´r sinptq ˛ ‚. We compute the cross product BsΣ ˆ BtΣ “ ´r2 sinptq ¨ ˝ cospsq sinptq sinpsq sinptq cosptq ˛ ‚. One can check that this is an interior normal vector. For instance, let s “ π and t “ π{2, so that Σps, tq “ px0 ´ r, y0, z0q; then BsΣ “ ´re2 and BtΣ “ ´re3, so that the cross product is BsΣ ˆ BtΣ “ r2e2 ˆ e3 “ r2e1, which points inside the ball from the point px0 ´ r, y0, z0q. Here is the formula: Theorem 4.7.6 (Gauss–Ostrogradski formula). Let X Ă R 3 be a compact set with a boundary BX that is a parameterized surface Σ : ra, bs ˆ rc, ds Ñ R3. Assume that Σ is injective in sa, brˆsc, dr, and that Σ has the property that the normal vector ⃗n points away from Σ at all points. Let ⃗u “ ⃗n{}⃗n} be the unit exterior normal vector. Let f “ pf1, f2, f3q be a vector ﬁeld of class C 1 deﬁned on some open set containing X. Then we have ż X divpf qdxdydz “ ż Σpf ¨ ⃗uqdσ. In this case, both the left and right-hand sides require come explanation: (1) For a vector ﬁeld f “ pf1, f2, f3q on X Ă R 3, we denote divpf q “ Bxf `Byf `Bzf , which is called the divergence of the vector ﬁeld f (similarly to the case n “ 2). Hence the left-hand side of the formula is ż X divpf qdxdzdz “ ż X ´ Bf Bx ` Bf By ` Bf Bz ¯ dxdydz. 91 (2) For a parameterized surface Σ : ra, bs ˆ rc, ds Ñ R3 in R3 with exterior normal vector ﬁeld ⃗n “ pn1, n2, n3q “ BsΣ ˆ BtΣ, and a function g deﬁned on the image of Σ, we deﬁne the surface integral ż Σ g dσ “ ż b a ż d c gpΣps, tqqσps, tqdsdt where σps, tq “ }BsΣ ˆ BtΣ} “ }⃗nps, tq}. Like the line integral for a parameterized curve, the key property of the sur- face integral (and especially the explanation for the complicated-looking factor }BsΣˆBtΣ}) is that it is independent of the chosen parameterization of the surface (see Proposition 4.1.5). This can be proved by applying the change of variable formula, as in the case of line-integrals. Next, for a C 1 vector ﬁeld f “ pf1, f2, f3q on R3, we deﬁne ż Σpf ¨ ⃗nqdσ “ ż Σ g dσ, where gpΣps, tqq “ f pΣps, tqq ¨ ⃗ups, tq “ 3ÿ i“1 uips, tqfipΣps, tqq. This particular surface integral is called the ﬂux of the vector ﬁeld f through the surface Σ. Note that in the ﬂux, the expression ⃗ups, tqσps, tq simpliﬁes always to ⃗nps, tq since ⃗ups, tqσps, tq “ ⃗nps, tq }⃗nps, tq}σps, tq “ ⃗nps, tq. Example 4.7.7. (1) We illustrate ﬁrst the surface integral. Suppose Σ is a parame- terized surface given by Σps, tq “ ps, t, f ps, tqq for some function f : ra, bs ˆ rc, ds Ñ R (so the image is the graph of f ). We take gpx, y, zq “ 1, and we claim that ż Σ dσ “ the surface area of the graph of f , which is a natural result. Indeed, we have BsΣ “ ¨ ˝ 1 0 Bsf ˛ ‚, BtΣ “ ¨ ˝ 0 1 Btf ˛ ‚, hence BsΣ ˆ BtΣ “ ¨ ˝ ´Bsf ´Btf 1 ˛ ‚ so that }BsΣ ˆ BtΣ} “ ´ pBsf q 2 ` pBtf q 2 ` 1 ¯1{2, hence ż Σ dσ “ ż b a ż d c ´ pBsf q 2 ` pBtf q 2 ` 1 ¯1{2dsdt is the surface area of the graph according to Section 4.5. 92 (2) We can use the Gauss–Ostrogradski formula to compute volumes, similarly to the computation of areas using the Green formula. Consider the vector ﬁeld f px, y, zq “ px, 0, 0q, so that divpf q “ 1. Then if X Ă R3 has boundary Σ : ra, bs ˆ rc, ds Ñ R3 (an injective parameterized surface) with positive orientation, we have VolpXq “ ż Σpf ¨ ⃗nqdσ “ ż b a ż d c n1ps, tqxpx, tqσps, tqdsdt, where Σps, tq “ pxps, tq, yps, tq, zps, tqq. Consider the example of the volume of a ball B centered at 0 with radius r in R 3 again, where the boundary is parameterized as in Example 4.7.2. We computed BsΣˆBtΣ in Example 4.7.5. Since this normal vector is interior, and σps, tq “ }BsΣ ˆ BtΣ} “ r2 sinptq ´ cos 2psq sin 2ptq ` sin 2psq sin2ptq ` cos 2ptq¯1{2 “ r2 sinptq we get VolpBq “ ż 2π 0 ż π 0 r cospsq sinptq ˆ r2 cospsq sin 2ptqdsdt “ r3´ż 2π 0 cos 2psqds¯´ż π 0 sin 3ptqdt ¯ “ 4πr3 3 , using the formulas cos 2psq “ 1 2 p1 ` cosp2sqq sin 3ptq “ ´ 1 8i pe3it ´ 3eit ` 3e ´it ´ e´3itq “ 1 4p3 sinptq ´ sinp3tqq, which imply that ż 2π 0 cos 2psqds “ π and ż π 0 sin3ptqdt “ 1 4 ´ 3r´ cosptqsπ 0 ´ 1 3rcosp3tqs π 0 ¯ “ 1 4 ´ 3 ¨ 2 ´ 1 3 ¨ 2 ¯ “ 4 3. 93 Bibliography [1] M. Burger, Analysis II, script for FS 2019 Lecture. 94","libVersion":"0.5.0","langs":""}