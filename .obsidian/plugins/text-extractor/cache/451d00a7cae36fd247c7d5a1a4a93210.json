{"path":"sem4/W&S/VRL/extra/W&S-script-1.pdf","text":"Probability and Statistics V. Tassion D-INFK Spring 2022 (Updated: May 2, 2022) Introduction to probability Some questions you may ask What is probability? Ü A mathematical language describing systems involving randomness. Where are probabilities used? Ü Describe random experiments in the real world (coin ﬂip, dice rolling, arrival times of customers in a shop, weather in 1 year,...). Ü Express uncertainty. For example, when a machine performs a measurement, the value is rarely exact. One may use probability theory in this context by saying that the value obtained is equal to the real value plus a small random error. Ü Decision making. Probability theory can be used to describe a system when only part of the information is known. In such context, it may help to make a decision. Ü Randomized algorithms in computer science. Sometimes, it is more eﬃcient to add some randomness to perform an algorithm. Examples: Google web search, ants searching food. Ü Simplify complex systems. Examples: water molecules in water, cars on the highway, percolation processes. The notion of probabilistic model If one wants a precise physical description of a coin ﬂip one would need a lot (really!) of information: the exact position of the ﬁngers and the coins, the initial velocity, the initial angular velocity, imperfections of the coin, the surface characteristics of the table, air currents, the brain activity of the gambler... These parameters are almost impossible to measure precisely, and a tiny change in one of them may aﬀect completely the result. In practice, we rather use a probabilistic description, which here consists in a drastic simpliﬁcation of the system: we completely forget the physical description of the throw of the coin and we only focus on the possible outcomes of the experiment: head or tail. 1 2 Namely, the probabilistic model for the coin ﬂip is given by 2 possible outcomes (head and tail) and each outcome has probability phead = ptail = 1~2 to be realized. In other words, Coin ﬂip = {{head, tail}, phead = 1~2, ptail = 1~2} A surprising analysis: coin ﬂips are not fair! If one tosses a coin it has more chance to fall on the same face as its initial face! See the youtube video of Persi Diaconis: How random is a coin toss? - Numberphile Probability laws: randomness vs ordering If one performs a single random experiment (for example a coin ﬂip), the result is unpre- dictable. In contrast, when one performs many random experiments, then some general laws can be observed. For example if one tosses 10000 independent coins, one should generally observe 5000 heads and 5000 tails approximately: This is an instance of a fun- damental probability law, called the law of large numbers. One goal of probability theory is to describe how ordering can emerge out of many random experiments, and establish some general probability laws. Chapter 1 Mathematical framework Goals • Basic understanding of the notion of a probability space (Ω, F, P) Ü Generalization of discrete probability spaces (introduced in[LSW21]) Ü Notion of sigma-algebra. • Concept of independence, conditional probability. 3 CHAPTER 1. MATHEMATICAL FRAMEWORK 4 1 Probability space Sample space We want to model a random experiment. The ﬁrst mathematical object needed is the set of all possible outcomes of the experiment, denoted by Ω. Terminology: The set Ω is called the sample space. An element ω ∈ Ω is called an outcome (or elementary experiment). Example : Throw of a die Ω = {1, 2, 3, 4, 5, 6}. Events Reminder: The set P(Ω) denotes the collection of all subsets A ⊂ Ω. In the previous class [LSW21], the set of events was always P(Ω). In this class we will work with more general sets of events F ⊂ P(Ω), called sigma-algebras. Deﬁnition 1.1. A sigma-algebra is a subset F ⊂ P(Ω) satisfying the following properties. P1. Ω ∈ F P2. A ∈ F ⇒ Ac ∈ F if A is an event, “non A“ is also an event. P3. A1, A2, . . . ∈ F ⇒ ∞ ˜ i=1 Ai ∈ F. if A1, A2, . . . are events, then “A1 or A2 or ...” is an event Examples of sigma-algebras for Ω = {1, 2, 3, 4, 5, 6}: • F = {g, {1, 2, 3, 4, 5, 6}}. • F = P(Ω). (In this case SFS = 64). • F = {g, {1, 2}, {3, 4, 5, 6}, {1, 2, 3, 4, 5, 6}}. Non-examples of sigma-algebras for Ω = {1, 2, 3, 4, 5, 6}: • F = {{1, 2, 3, 4, 5, 6}} is not a sigma-algebra because P2 is not satisﬁed. • F = {g, {1, 2, 3}, {4, 5, 6}, {1}, {2, 3, 4, 5, 6}, Ω} is not a sigma-algebra because P3 is not satisﬁed. CHAPTER 1. MATHEMATICAL FRAMEWORK 5 Probability measure Deﬁnition 1.2. Let Ω be a sample space, let F be a sigma-algebra. A probability measure on (Ω, F) is a map P ∶ F → [0, 1] A ( P[A] that satisﬁes the following two properties P1. P[Ω] = 1. P2. (countable additivity) P[A] = ∑ ∞ i=1 P[Ai] if A = ˚ ∞ i=1 Ai (disjoint union). “A probability measure is a map that associates to each event a number in [0, 1].” Examples for Ω = {1, 2, 3, 4, 5, 6} and F = P({1, 2, 3, 4, 5, 6}): • The mapping P ∶ F ( [0, 1] deﬁned by ∀A ∈ F P[A] = SAS 6 is a probability measure on (Ω, F). • Given some numbers p1, . . . , p6 satisfying p1 + \u0005 + p6 = 1 , the mapping P ∶ F ( [0, 1] deﬁned by ∀A ∈ F P[A] = Q i∈A pi is a probability measure on (Ω, F). The case pi = 1 6 (for all i) corresponds to the ﬁrst example, modeling a fair die. The case p1 = \u0005 = p5 = 1 7, and p6 = 2 7 would correspond to a biased die, with twice more chance fall on 6 than on the other values. Notion of probability space Deﬁnition 1.3. Let Ω be a sample space, F a sigma-algebra, and P a probability measure. The triple (Ω, F, P) is called a probability space. To summarize, if one want to construct a probabilistic model, we give • a sample space Ω, “all the possible outcomes of the experiment” • a sigma-algebra F ⊂ P(Ω), “the set of events” • a probability measure P. “gives a number in [0, 1] to every event” CHAPTER 1. MATHEMATICAL FRAMEWORK 6 Terminology Let ω ∈ Ω (a possible outcome). Let A be an event. We say the event A occurs (for ω) if ω ∈ A. Aω We say that it does not occur if ω ∉ A. Aω Remark 1.4. The event A = g never occurs. “we never have ω ∈ g” The event A = Ω always occurs. “we always have ω ∈ Ω” 2 Examples of probability spaces Example with Ω ﬁnite We now discuss a particular type of probability spaces that appear in many concrete examples. The sample space Ω is an arbitrary ﬁnite set, and all the outcomes have the same probability pω = 1 SΩS. Deﬁnition 1.5. Let Ω be a ﬁnite sample space. The Laplace model on Ω is the triple (Ω, F, P), where • F = P(Ω), • P ∶ F → [0, 1] is deﬁned by ∀A ∈ F P[A] = SAS SΩS . One can easily check that the mapping P above deﬁnes a probability measure in the sense of the deﬁnition 1.2. In this context, estimating the probability P[A] boils down to counting the number of elements in A and in Ω. Example: We consider n ≥ 3 points on a circle, from which we select 2 at random. What is the probability that these two points selected are neighbors? CHAPTER 1. MATHEMATICAL FRAMEWORK 7 123456 Figure 1.1: A circle with n = 6 points, and the subset {1, 3} is selected. We consider the Laplace model on Ω = {E ⊂ {1, 2, \u0005, n} ∶ SES = 2}. The event “the two points of E are neighbors” is given by A = {{1, 2}, {2, 3}, \u0005, {n − 1, n}, {n, 1}}, and we have P[A] = SAS SΩS = n › n 2” = 2 n − 1 . Example with Ω inﬁnite countable We throw a biased coin multiple times, at each throw, the coin falls on head with prob- ability p, and it falls on tail with probability 1 − p (p is a ﬁxed parameter in [0, 1]). We stop at the ﬁrst time we see a tail. The probability that we stop exactly at time k is given by pk = pk−1(1 − p). (Indeed, we stop at time k, if we have seen exactly k − 1 heads and 1 tail.) For this experiment, one possible probability space is given by • Ω = N…{0} = {1, 2, 3, . . .}, • F = P(Ω), • for A ∈ F, P[A] = Q k∈A pk. Example with Ω uncountable (for the culture) A tempting approach to deﬁne a probability measure is to ﬁrst associate to every ω the probability pω that the output of the experiment is ω. Then, for an event A ⊂ Ω deﬁne the probability of A by the formula P[A] = Q ω∈A pω. (1.1) CHAPTER 1. MATHEMATICAL FRAMEWORK 8 This approach works perfectly well, when the sample space Ω is ﬁnite or countable (this is the case of the two examples above). But this approach does not work well if Ω is uncountable. For example, in the case of the droplet of water in a segment Ω = [0, 1]. In this case, the probability of landing a ﬁxed point is always 0 and the equation (1.1) does not make sense. This is for this reason that we use an axiomatic deﬁnition (in Deﬁnition 1.2) of probability measure. In this particular case, a natural choice of probability space is • Ω = [0, 1], • F = Borel σ-algebra1 • for A ∈ F, P(A) = Lebesgue measure of (A). 3 Properties of Events Operations on events and interpretation Since events are deﬁned as subsets of Ω, one can use operations from set theory (union, intersection, complement, symmetric diﬀerence,. . . ). From the deﬁnition, we know that we can take the complement of an event (by H2), or a countable union of events (by H3). The following proposition asserts that the other standard set operations are allowed. Proposition 1.6 (Consequences of the deﬁnition). Let F be a sigma-algebra on Ω. We have P4. g ∈ F, P5. A1, A2, . . . ∈ F ⇒ ∞ ˛ i=1 Ai ∈ F, P6. A, B ∈ F ⇒ A ∪ B ∈ F, P7. A, B ∈ F ⇒ A ∩ B ∈ F. Proof. We prove the items one after the other. i. By P1 in Deﬁnition 1.1, we have Ω ∈ F. Hence, by P3 in Deﬁnition 1.1, we have g = Ω c ∈ F. ii. Let A1, A2, . . . ∈ F. By P2, we also have A c 1, Ac 2, . . . ∈ F. Then, by P3, we have ˚∞ i=1(Ai)c ∈ F. Finally, using P2 again, we conclude that ∞ ˛ i=1 Ai = „ ∞ ˜ i=1(Ai) c‚ c ∈ F. 1the Borel σ-algebra F is deﬁned as follows: it contains all A = [x1, x2] × [y1, y2], with 0 ≤ x1 ≤ x2 ≤ 1, 0 ≤ y1 ≤ y2 ≤ 1, and it is the smallest collection of subsets of Ω which satisﬁes P1, P2 and P3 in Deﬁnition 1.1. CHAPTER 1. MATHEMATICAL FRAMEWORK 9 iii. Let A, B ∈ F. Deﬁne A1 = A, A2 = B, and for every i ≥ 3 Ai = g. By P3, we have A ∪ B = ∞ ˜ i=1 Ai ∈ F. iv. Let A, B ∈ F. By P2, Ac, Bc ∈ F. Then by iii. above, we have Ac ∪ Bc ∈ F. Finally, by P2, we deduce A ∩ B = (A c ∪ Bc) c ∈ F. On the following tabular we consider two events A and B, and we summarize the probabilistic interpretation of the most important set operation. Event Graphical representation Probab. interpretation Ac ΩAA c A does not occur A ∩ B AB A and B occur A ∪ B AB A or B occurs A∆B AB one and only one of A or B occurs Relations between events and interpretations Set relations (inclusion, distinctness, partition) also have probabilistic interpretations, summarized below. CHAPTER 1. MATHEMATICAL FRAMEWORK 10 Relation Graphical representation Probab. interpretation A ⊂ B AB If A occurs, then B occurs A ∩ B = g AB A and B cannot occur at the same time Ω = A1 ∪ A2 ∪ A3 with A1, A2, A3 pairwise disjoint ΩA1A3A2 for each outcome ω, one and only one of the events A1, A2, A3 is satisﬁed. Why not always working with F = P(Ω)? In the previous class [LSW21], the set of events was always taken to be P(Ω). It may seem useless to take more general sets of events, and consider the “complicated” notion of sigma-algebra. We give here two main motivations for that: • First motivation: partially observed experiment. Working with general set of event allows for natural decomposition of the probability spaces. This is particularly useful when we reveal the outcome of a random experiment algorithmically, as in the following simple example. We consider the throw of two independent dice. A possible outcome is a pair ω = (ω1, ω2) where ω1 and ω2 are the respective values of the ﬁrst and second dies. We choose the sample space Ω = {1, 2, 3, 4, 5, 6} 2. We can consider the following two sigma algebras F1 = {A × {1, 2, 3, 4, 5, 6}, A ⊂ {1, 2, 3, 4, 5, 6}} and F2 = P(Ω). The ﬁrst sigma-algebra F1 corresponds to all the events deﬁned in terms of the ﬁrst die, while the second sigma-algebra contains all the possible events in terms of the two dice. For example the event A = {2, 4, 6} × {1, 2, 3, 4, 5, 6} (“the ﬁrst die is even”) is in both F1 and F2, while the event B = {2, 4, 6}2 (“both dies are even”) belongs to F2 but not F1 because it requires the information of the second die. If one reveals the outcome of the experiment algorithmically, by ﬁrst revealing the ﬁrst die, and then the second die. After the ﬁrst step, we can say which of the events of F1 occur, and after the second step, we can say which of the events of F2 occur. CHAPTER 1. MATHEMATICAL FRAMEWORK 11 • Second motivation: theoretical. When the sample space is not countable (e.g. Ω = [0, 1] or Ω = {0, 1}N), one often needs to impose some conditions on the events. This is due to the fact that we want to be able to deﬁne a probability measure on the set of events. This is not always possible on F = P(Ω). For example, when deﬁning the uniform probability measure on Ω, one can construct set A ⊂ [0, 1] that are “strange enough” so that P[A] is not deﬁned. Therefore we have to restrict ourselves to F ø P(Ω), which excludes such strange sets (see [Wil01, 2.3, p. 43] for a short discussion on this issue). For this course, this theoretical obstacle is not crucial to understand in detail. Nevertheless it is of fundamental importance in measure theory, which is the theoretical support for probability theory. 4 Properties of probability measures Direct consequences of the deﬁnition Proposition 1.7. Let P be a probability measure on (Ω, F). P3. We have P[g] = 0. P4. (additivity) Let k ≥ 1, let A1, . . . , Ak be k pairwise disjoint events, then P[A1 ∪ \u0005 ∪ Ak] = P[A1] + \u0005 + P[Ak]. P5. Let A be an event, then P[A c] = 1 − P[A]. P6. If A and B are two events (not necessarily disjoint), then P[A ∪ B] = P[A] + P[B] − P[A ∩ B]. Proof. We prove the items one after the other P3. Deﬁne x = P[g]. We already know that x ∈ [0, 1] because x is the probability of some event. Deﬁning A1 = A2 = \u0005 = g, we have g = ∞ ˜ i=1 Ai. The events Ai are disjoint and countable additivity implies ∞ Q i=1 P [Ai] = P[g]. Since P[Ai] = x for every i and P [g] ≤ 1, we have ∞ Q i=1 x ≤ 1, CHAPTER 1. MATHEMATICAL FRAMEWORK 12 and therefore x = 0. P4. Deﬁne Ak+1 = Ak+2 = \u0005 = g. In this way we have A1 ∪ \u0005 ∪ Ak = A1 ∪ \u0005 ∪ Ak ∪ g ∪ g ∪ \u0005 = ∞ ˜ i=1 Ai. Since the events Ai are pairwise disjoint, one can apply countable additivity as follows: P[A1 ∪ \u0005 ∪ Ak] = P[ ∞ ˜ i=1 Ai] countable additivity = ∞ Q i=1 P[Ai] = P[A1] + \u0005 + P[Ak] + Q i>k P[Ai] ´¹¹¸¹¹¶ =0 . P5. By deﬁnition of the complement, we have Ω = A ∪ Ac, and therefore 1 = P[Ω] = P[A ∪ A c]. Since the two events A, Ac are disjoint, additivity ﬁnally gives 1 = P[A] + P[A c]. P6. A ∪ B is the disjoint union of A with B…A. Hence, by additivity, we have P[A ∪ B] = P[A] + P[B…A]. (1.2) Also B = (B ∩ A) ∪ (B ∩ Ac) = (B ∩ A) ∪ (B…A). Hence, by additivity, P[B] = P[B ∩ A] + P[B…A], which give P[B…A] = P[B] − P[A ∩ B]. Plugging this estimate in Eq. (1.2) we obtain the result. Useful Inequalities In applications, it happens often that the probability P[A] is diﬃcult to compute ex- actly: in such cases it is often useful to relate the event A to other events, and then use monotonicity (Proposition 1.8) and/or the union bound (Proposition 1.9) to obtain some bounds on P[A] in terms of probabilities of events that are easier to compute. CHAPTER 1. MATHEMATICAL FRAMEWORK 13 Proposition 1.8 (Monotonicity). Let A, B ∈ F, then A ⊂ B ⇒ P[A] ≤ P[B]. Proof. If A ⊂ B, then we have B = A ∪ (B…A) (disjoint union). Hence, by additivity, we have P[B] = P[A] + P[B…A] ≥ P[A]. Proposition 1.9 (Union bound). Let A1, A2, . . . be a sequence of events (not neces- sarily disjoint), then we have P[ ∞ ˜ i=1 Ai] ≤ ∞ Q i=1 P[Ai]. Remark 1.10. The union bound also applies to a ﬁnite collection of events. Proof. For i ≥ 1, deﬁne ̃Ai = Ai ∩ Ac i−1 ∩ \u0005 ∩ A c 1. One can check that ∞ ˜ i=1 Ai = ∞ ˜ i=1 ̃Ai. (To prove the direct inclusion, consider ω in the left hand side. Then deﬁne the smallest i such that ω ∈ Ai. For this i, we have ω ∈ ̃Ai, which implies that ω belongs to the right hand side. The other inclusion is clear because ̃Ai ⊂ Ai for every i.) Now, one can apply the countable additivity to the ̃Ai, because they are disjoint. We get P[ ∞ ˜ i=1 Ai] = P[ ∞ ˜ i=1 ̃Ai] = ∞ Q i=1 P[ ̃Ai] ≤ ∞ Q i=1 P[Ai]. Application. We throw a die n ≥ 2 times. We want to prove that the probability to see more than ℓ ∶= \u00147 log nˇ successive 1’s is small if n is large. We consider the probability space given by • Ω = {1, 2, 3, 4, 5, 6}n, an outcome is ω = ( ω1 ´¸¶ die 1 , \u0005, ωn ´¸¶ die n ), CHAPTER 1. MATHEMATICAL FRAMEWORK 14 • F = P(Ω), • for A ∈ F, P[A] = SAS SΩS . The event A that there exist ℓ successive 1’s can be deﬁned as follows. First, for a ﬁxed index k ∈ {1, . . . , n − ℓ}, we deﬁne the event that there ℓ successive 1’s between k + 1 and k + ℓ by Ak = {ω ∶ ωk+1 = ωk+2 = \u0005 = ωk+ℓ = 1}, This way, the event A is exactly the event that there exists k such that Ak occurs. Namely, A = n−ℓ ˜ k=0 Ak. Our goal is to prove that the probability of A is small. Notice that Ak ∩ Ak′ ≠ g for k ≠ k′, since the element ω = (1, 1, . . . , 1) always belongs to Ak for every index k. Hence, the event A is expressed as an non-disjoint union of events and we cannot directly Property P2 of the probability measure to estimate its probability. Nevertheless, one can use the union bound to show that P[A] ≤ n−ℓ Q k=0 P[Ak] ≤ n ⋅ ‰ 1 6’ ℓ ≤ n ⋅ n − log(7)~ log(6), and therefore we see that the probability of seeing more than 7 log n consecutive 1’s converge to 0 as n tends to inﬁnity. Continuity properties of probability measures Proposition 1.11. Let (An) be an increasing sequence of events (i.e. An ⊂ An+1 for every n). Then lim n→∞ P [An] = P[ ∞ ˜ n=1 An]. increasing limit Let (Bn) be a decreasing sequence of events (i.e. Bn ⊃ Bn+1 for every n). Then lim n→∞ P [Bn] = P[ ∞ ˛ n=1 Bn]. decreasing limit A1 ∞⋃ n=1 An A2A3 ∞⋂ n=1 Bn B1B2B3 Remark 1.12. By monotonicity, we have P[An] ≤ P[An+1] and P[Bn] ≥ P[Bn+1] for every n. Hence the limits in the proposition are well deﬁned as monotone limits. CHAPTER 1. MATHEMATICAL FRAMEWORK 15 Proof. Let (An)n≥1 be an increasing sequence of events. Deﬁne ̃A1 = A1 and for every n ≥ 2 ̃An = An…An−1. The events ̃An are disjoint and satisfy ∞ ˜ n=1 An = ∞ ˜ n=1 ̃An and AN = N ˜ n=1 ̃An. Using ﬁrst countable additivity and then additivity, we have P[ ∞ ˜ n=1 An] = P[ ∞ ˜ n=1 ̃An] = ∞ Q n=1 P[ ̃An] = lim N →∞ N Q n=1 P[ ̃An] = lim N →∞ P[AN ]. Now, let (Bn) be a decreasing sequence of events. Then (Bc n) is increasing, and we can apply the previous result in the following way: P[ ∞ ˛ n=1 Bn] = 1 − P[ ∞ ˜ n=1 Bc n] = 1 − lim n→∞ P[Bc n] = lim n→∞ P[Bn]. 5 Conditional probabilities Consider a random experiment represented by some probability space (Ω, F, P). We may sometimes possess incomplete information about the actual outcome of the experiment without knowing this outcome exactly. For example if we throw a die and a friend tells us that an even number is showing, then this information aﬀects all our calculation of probabilities. In general, if A and B are two events and we are given that B occurs, the new probability may no longer be P[A]. In this new circumstance, we know that A occurs if and only if A ∩ B occurs, suggesting that the new probability of A is proportional to P[A ∩ B]. Deﬁnition 1.13 (Conditional probability). Let (Ω, F, P) be some probability space. Let A, B be two events with P[B] > 0. The conditional probability of A given B CHAPTER 1. MATHEMATICAL FRAMEWORK 16 is deﬁned by P[A S B] = P[A ∩ B] P[B] . Remark 1.14. P[B S B] = 1. Condition on B, the event B always occurs. Example: We consider the probability space (Ω, F, P) corresponding to the throw of one die. Let A = {1, 2, 3} be the event that the die is smaller than or equal to 3, and let B = {2, 4, 6} be the event that the die is even. Then P[A S B] = P[A ∩ B] P[B] = 1~6 1~2 = 1~3. Proposition 1.15. Let (Ω, F, P) be some probability space. Let B be an event with positive probability. Then P[ . S B] is a probability measure on Ω. Proposition 1.16 (Formula of total probability). Let B1, \u0005, Bn be a partitiona of the sample space Ω with P[Bi] > 0 for every 1 ≤ i ≤ n. Then, one has ∀A ∈ F P[A] = n Q i=1 P[A S Bi] P[Bi]. ai.e. Ω = B1 ∪ \u0005 ∪ Bn and the events are pairwise disjoint. Proof. Using the distributivity of the intersection, we have A = A ∩ Ω = A ∩ (B1 ∪ \u0005 ∪ Bn) = (A ∩ B1) ∪ \u0005 ∪ (A ∩ Bn). Since the events A ∩ Bi are pairwise disjoint, we have P[A] = P[A ∩ B1] + \u0005 + P[A ∩ Bn]. By deﬁnition, we have P[A ∩ Bi] = P[A S Bi] P[Bi] for every i and using this expression in the equation above, we ﬁnally get P[A] = P[A S B1] P[B1] + \u0005 + P[A S Bn] P[Bn]. Proposition 1.17 (Bayes formula). Let B1, . . . , Bn ∈ F be a partition of Ω with P[Bi] > 0 for every i. For every event A with P[A] > 0, we have ∀i = 1, . . . , n P[Bi S A] = P[A S Bi] P[Bi] ∑n j=1 P[A S Bj] P[Bj] . CHAPTER 1. MATHEMATICAL FRAMEWORK 17 Typical application: A test is performed in order to diagnose a certain rare disease, which concerns 1~10000 of a population. This test is quite reliable and gives the right answer 99 percent of the times. If a patient has a “positive test” (i.e. the test indicates that he is sick), what is the probability that he is actually sick? The situation is modeled by setting Ω = {0, 1} × {0, 1}. and F = P(Ω). An outcome is a pair ω = (ω1, ω2) representing a patient, where ω1 = ¢¨¨ ¦ ¨¨¤ 0 if the patient is healthy, 1 if the patient is sick, ω2 = ¢¨¨ ¦ ¨¨¤ 0 if the test is negative, 1 if the test is positive. We consider the event S that the patient is sick, and the event T that the test is positive. The elements of S are all the outcomes ω = (ω1, ω2) such that ω1 = 1, ie S = {(1, 0), (1, 1)}. Equivalently, we have T = {(0, 1), (1, 1)}. From the hypotheses, the information that we have on the probability measure is P[S] = 1 10000, P[T S S] = 99 100, P[T S Sc] = 1 100. We are looking for the a posteriori probability P[S S T ] of being sick, given that the test is positive. By applying the Bayes formula to the partition Ω = S ∪ Sc, we obtain P[S S T ] = P[T S S] P[S] P[T S S] P[S] + P[T S Sc] P[Sc] = 0.99 × 0.0001 0.99 × 0.0001 + 0.01 × 0.9999 \b 0.0098. This result is quite surprising: the probability to be actually sick when the test is positive is very small! What is happening? If one looks at the whole population, there are two types of persons, who will have a positive test: - the healthy individuals with a (wrongly) positive test, which represent roughly one percent of the population. - the sick individuals with a (correctly) positive test, which represent roughly 1/10000 of the population. Given that the test is positive, a person has much more chances to be in the ﬁrst group of individuals. CHAPTER 1. MATHEMATICAL FRAMEWORK 18 6 Independence Independence of events Deﬁnition 1.18 (Independence of two events). Let (Ω, F, P) be a probability space. Two events A and B are said to be independent if P[A ∩ B] = P[A] P[B]. Remark 1.19. If P[A] ∈ {0, 1}, then A is independent of every event, i.e. ∀B ∈ F P[A ∩ B] = P[A] P[B]. If an event A is independent with itself (i.e. P[A ∩ A] = P[A]2), then P[A] ∈ {0, 1} A is independent of B if and only if A is independent of Bc. The concept of independence is fundamental in probability: it corresponds to the intuitive idea that two events do not inﬂuence each other, as illustrated in the following proposition. Proposition 1.20. Let A, B ∈ F be two events with P[A], P[B] > 0. Then the following are equivalent: (i) P[A ∩ B] = P[A] P[B], A and B are independent (ii) P[A S B] = P[A], the occurrence of B has no inﬂuence on A (iii) P[B S A] = P[B]. the occurrence of A has no inﬂuence on B Proof. Since P[B] > 0 we have (i) ⇔ „ P[A ∩ B] P[B] = P[A]‚ ⇔ (P[A S B] = P[A]) ⇔ (ii). Since (iii) is just the same as (ii) with the role of A and B reversed, the equivalence (i)⇔ (iii) can proved the same way. Typical examples of independent events occur when one performs successively a ran- dom experiment, as illustrated below with the throw of two dice. Example: Throw of two independent dice. We throw two dice independently. This is modeled by the Laplace model on the sample space Ω = {1, 2, 3, 4, 5, 6} 2. CHAPTER 1. MATHEMATICAL FRAMEWORK 19 Consider the events A = {ω ∶ ω1 ∈ 2Z}, “The ﬁrst die is even” B = {ω ∶ 1 + ω2 ∈ 2Z}, “The second die is odd” C = {ω ∶ ω1 + ω2 ≤ 3}, “The sum of the two dice is at most 3” D = {ω ∶ ω1 ≤ 2, ω2 ≤ 2}. “Both dice are smaller than or equal to 2” Check that: • A and B are independent, • A and C are not independent, • A and D are independent. Deﬁnition 1.21. Let I be an arbitrary set of indices. A collection of events (Ai)i∈I is said to be independent if ∀J ⊂ I ﬁnite P[˛ j∈J Aj] = M j∈J P[Aj]. Remark: Three events A, B and C are independent if the following 4 equations are satisﬁed (and not only the last one!): P[A ∩ B] = P[A] P[B], P[A ∩ C] = P[A] P[C], P[B ∩ C] = P[B] P[C], P[A ∩ B ∩ C] = P[A] P[B] P[C]. Example: We consider the same notation as in the example above Deﬁnition 1.21. The events A, B, and D are independent (Check that!). Chapter 2 Random variables and distribution functions Goals • Understand the deﬁnition of a random variable and its distribution function. • Use of abstract probability space (Ω, F, P). • Learn the notation allowing to deﬁne events in term of random variables. • Explicit construction of random variables from inﬁnite sequence of i.i.d. Bernoulli random variables. 20 CHAPTER 2. RANDOM VARIABLES AND DISTRIBUTION FUNCTIONS 21 1 Abstract deﬁnition Most often, the probabilistic model under consideration is rather complicated, and one is only interested in certain quantities in the model. For this reason, one introduces the notion of random variables Deﬁnition 2.1. Let (Ω, F, P) be a probability space. A random variable (r.v.) is a map X ∶ Ω → R such that for all a ∈ R, {ω ∈ Ω ∶ X(ω) ≤ a} ∈ F. Ü The condition {ω ∈ Ω ∶ X(ω) ≤ a} ∈ F is needed for P[{ω ∈ Ω ∶ X(ω) ≤ a}] to be well-deﬁned. Example 1: Gambling with one die We throw a fair die. The sample space is Ω = {1, 2, 3, 4, 5, 6} and we consider the Laplace model (Ω, F, P) as in Deﬁnition 1.5. Suppose that we gamble on the outcome in such a way that our proﬁt is −1 if the outcome is 1, 2 or 3, 0 if the outcome is 4, 2 if the outcome is 5 or 6, where a negative proﬁt correspond to a loss. Our proﬁt can be represented by the mapping X deﬁned by ∀ω ∈ Ω X(ω) = ¢¨¨¨¨ ¦ ¨¨¨¨¤ −1 if ω = 1, 2, 3, 0 if ω = 4, 2 if ω = 5, 6. (2.1) Since F = P(Ω), we have {ω ∶ X(ω) ≤ a} ∈ F for every a. Therefore, X is a random variable on (Ω, F, P). Example 2: Indicator function of an event Let A ∈ F. Consider the indicator function 1A of A, deﬁned by ∀ω ∈ Ω 1A(ω) = ¢¨¨ ¦ ¨¨¤ 0 if ω ∉ A, 1 if ω ∈ A. Then 1A is a random variable. Indeed, we have {ω ∶ 1A(ω) ≤ a} = ¢¨¨¨¨ ¦ ¨¨¨¨¤ g if a < 0, Ac if 0 ≤ a < 1, Ω if a ≥ 1, CHAPTER 2. RANDOM VARIABLES AND DISTRIBUTION FUNCTIONS 22 and g, Ac and Ω are three elements of F. Remark: Role of the sigma-algebra Consider the same notation as in Example 1. Additionally, we consider the following two sigma-algebras: F1 = {g, {1, 2, 3}, {4, 5, 6}, {1, 2, 3, 4, 5, 6}}, F2 = {g, {1, 2, 3}, {1, 2, 3, 4}, {4, 5, 6}, {5, 6}, {1, 2, 3, 5, 6}, {4}, {1, 2, 3, 4, 5, 6}}. Is X is random variable on (Ω, Fi, P}? To answer this question, one needs to examine the set {ω ∶ X(ω) ≤ a} in more details. Here we see that {ω ∶ X(ω) ≤ a} = ¢¨¨¨¨¨¨¨ ¦ ¨¨¨¨¨¨¨¤ g if a < −1, {1, 2, 3} if − 1 ≤ a < 0, {1, 2, 3, 4} if 0 ≤ a < 2, {1, 2, 3, 4, 5, 6} if a ≥ 2. In particular, we see that X is a random variable on (Ω, F2, P), but not on (Ω, F1, P). Notation: When events are deﬁned in terms of random variable, we will omit the dependence in ω. For example, for a ≤ b we write {X ≤ a} = {ω ∈ Ω ∶ X(ω) ≤ a}, {a < X ≤ b} = {ω ∈ Ω ∶ a < X(ω) < b}, {X ∈ Z} = {ω ∈ Ω ∶ X(ω) ∈ Z}. When consider the probability of events as above, we omit the brackets and for ex- ample simply write P[X ≤ a] = P[{X ≤ a}] = P[{ω ∈ Ω ∶ X(ω) ≤ a}]. 2 Distribution function Deﬁnition 2.2. Let X be a random variable on a probability space (Ω, F, P). The distribution function of X is the function FX ∶ R → [0, 1] deﬁned by ∀a ∈ R FX(a) = P[X ≤ a]. Idea: The distribution function FX encodes the probabilistic properties of the random variable X. Example 1: Gambling on a die CHAPTER 2. RANDOM VARIABLES AND DISTRIBUTION FUNCTIONS 23 Let X be the random variable deﬁned by Eq. (2.1). For a ∈ R, we have FX(a) = ¢¨¨¨¨¨¨¨ ¦ ¨¨¨¨¨¨¨¤ 0 if a < −1, 1~2 if −1 ≤ a < 0, 2~3 if 0 ≤ a < 2, 1 if a ≥ 2. −1021/22/31 Figure 2.1: Graph of the distribution function FX. Example 2: Indicator function of an event Let A be an event. Let X = 1A be the indicator function of the event A. Then FX(a) = ¢¨¨¨¨ ¦ ¨¨¨¨¤ 0 if a < 0, 1 − P[A] if 0 ≤ a < 1, 1 if a ≥ 1. Proposition 2.3 (Basic identity). Let a < b be two real numbers. Then P[a < X ≤ b] = F (b) − F (a). Proof. We have {X ≤ b} = {X ≤ a} ∪ {a < X ≤ b} (disjoint union). Hence P[X ≤ b] = P[X ≤ a] + P[a < X ≤ b], which directly implies the result. Theorem 2.4 (Properties of distribution functions). Let X be a random variable on some probability space (Ω, F, P). The distribution function F = FX ∶ R → [0, 1] of X satisﬁes the following properties. (i ) F is nondecreasing. (ii ) F is right continuousa. (iii ) lim a→−∞ F (a) = 0 and lim a→∞ F (a) = 1. CHAPTER 2. RANDOM VARIABLES AND DISTRIBUTION FUNCTIONS 24 ai.e. F (a) = lim h↓0 F (a + h) for every a ∈ R. Proof. We ﬁrst prove (i ), then (iii ) and ﬁnally (ii ). (i ) For a ≤ b, we have {X ≤ a} ⊂ {X ≤ b}. Hence, by monotonicity, we have P[X ≤ a] ≤ P[X ≤ b], i.e. F (a) ≤ F (b). (iii ) Let an ↑ ∞. For every ω ∈ Ω, there exists n large enough such that X(ω) ≤ an. Hence, Ω = ˜ n≥1 {X ≤ an}. Furthermore, we have {X ≤ an} ⊂ {X ≤ an+1} and the continuity properties of probability measures imply 1 = P[Ω] = P[ ˜ n≥1{X ≤ an}] = lim n→∞ P[X ≤ an] = lim n→∞ F (an). In the same way, one has lim a→−∞ F (a) = 0. Indeed, using that for every an ↓ −∞, g = ˛ n≥1 {X ≤ an} and {X ≤ an} ⊃ {X ≤ an+1}, it follows from the continuity properties of probability measures that 0 = P[g] = lim n→∞ P[X ≤ an] = lim n→∞ F (an). (ii ) Let a ∈ R, let hn ↓ 0. We have {X ≤ a} = ˛ n≥1 {X ≤ a + hn}, where {X ≤ a + hn} ⊃ {X ≤ a + hn+1}. Hence by the continuity properties of proba- bility measures, we have F (a) = P[X ≤ a] = P[ ˛ n≥1{X ≤ a + hn}] = lim n→∞ P[X ≤ a + hn] = lim n→∞ F [a + hn]. 3 Independence Independence of random variables CHAPTER 2. RANDOM VARIABLES AND DISTRIBUTION FUNCTIONS 25 Deﬁnition 2.5. Let X1, . . . , Xn be n random variables on some probability space (Ω, F, P). We say that X1, . . . , Xn are independent if ∀x1, . . . , xn ∈ R P[X1 ≤ x1, . . . , Xn ≤ xn] = P[X1 ≤ x1] . . . P[Xn ≤ xn]. (2.2) Remark: One can show that X1, . . . , Xn are independent if and only if ∀I1 ⊂ R, . . . , In ⊂ R intervals {X1 ∈ I1}, . . . , {Xn ∈ In} are independent. Example 1: Throw of two independent dices We consider the Laplace model (Ω, F, P) on Ω = {1, 2, 3, 4, 5, 6}2. An element ω ∈ Ω is a pair ω = (ω, ω2), where the ﬁrst coordinate represents the value of the ﬁrst die and the second coordinate represents the value of the second die. We deﬁne the random variables X, Y, Z ∶ Ω → R by X(ω) = ω1, Y (ω) = ω2, Z(ω) = ω1 + ω2. X and Y represent the values of the ﬁrst and second die, respectively. Z corresponds to the sum of the two dices. In this case, we have that X and Y are independent. To see this, observe that for every I, J ⊂ {1, . . . , 6}, we have P[X ∈ I, Y ∈ J] = P[I × J] = SI × JS SΩS = SIS 6 ⋅ SJS 6 = SI × {1, 2, 3, 4, 5, 6}S 36 ⋅ SI × {1, 2, 3, 4, 5, 6}S 36 = P[X ∈ I]P[Y ∈ J]. For every x, y ∈ R, there exist I, J ⊂ {1, 2, 3, 4, 5, 6} such that {X ≤ x} = {X ∈ I} and {X ≤ y} = {X ∈ J}. Therefore, P[X ≤ x, Y ≤ y] = P[X ≤ x]P[Y ≤ y]. On the other hand, X and Z are not independent since 1 62 = P[X ≤ 1, Z ≤ 2] ≠ P[X ≤ 1]P[Z ≤ 2] = 1 63 . Grouping If we have a set of independent random variables, and we make disjoint groups of such random variables, then these groups are also independent form each other. This idea is formalized by the following proposition. Proposition 2.6 (grouping). Let X1, . . . , Xn be n independent random variables. Let 1 ≤ i1 < i2 < \u0005 < ik ≤ n be some indices and φ1, . . . , φk some functions. Then Y1 = φ1(X1, . . . , Xi1), Y2 = φ2(Xi1+1, . . . , Xi2) . . . , Yk = φk(Xik−1+1, . . . , Xik) are independent. Proof. Admitted CHAPTER 2. RANDOM VARIABLES AND DISTRIBUTION FUNCTIONS 26 Sequences of i.i.d. random variables Deﬁnition 2.7. An inﬁnite sequence X1, X2, . . . of random variables is said to be • independent if X1, . . . , Xn are independent, for every n. • independent and identically distributed (iid) if they are independent and they have the same distribution function, i.e. ∀i, j FXi = FXj . 4 Transformation of random variables Once we have some random variables X1, X2, . . . on some probability space (Ω, F, P), we can create and consider many new random variables on the same probability space by using operations. For example, one can consider Z1 = exp(X1), Z2 = X1 + X2,. . . One should not completely forget that random variables are maps Ω → R. For example, the random variables Z1 and Z2 correspond to the maps deﬁned by for every ω ∈ Ω Z1(ω) = exp(X1(ω)), Z2(ω) = X1(ω) + X2(ω). Formally, we introduce the following notation, which allows us to work with random variables as if the were just real numbers. If X is a random variable, and φ ∶ R → R, the we write φ(X) ∶= φ ○ X. This way, φ(X) is a new mapping Ω → R as shown on the diagram. Ω X Ð→ R φ Ð→ R ω z→ X(ω) z→ φ(X(ω)). More generally, we can also consider functions of several variables. If X1, . . . , Xn are n random variables and φ ∶ Rn → R, then we write φ(X1, . . . , Xn) ∶= φ ○ (X1, . . . , Xn). 5 Construction of random variables In Section 1, we deﬁned random variables. In Section 2, we saw that we can associate to any random variable X a distribution function F = FX ∶ R → [0, 1], which encodes it probabilistic properties, and satisﬁes (i) F is nondecreasing, (ii) F is right continuous, CHAPTER 2. RANDOM VARIABLES AND DISTRIBUTION FUNCTIONS 27 (iii) lim a→−∞ F (a) = 0 and lim a→∞ F (a) = 1. Conversely, given a function F ∶ R → [0, 1] satisfying Items (i)–(iii), does there exist a random variable X such that FX = F ? The goal of this section is construct general random variables, and answer to the question above positively. A complete construction would require some tools from measure theory which are beyond the scope of this class: Our approach will rely on an abstract theorem of Kolmogorov, that guarantees existences of iid sequences. This Theorem will be admitted, but the rest of the construction will be rigorously detailed. Our motivation is twofold • On a theoretical level, the existence of random variables is fundamental: “it is more satisfying the objects we are talking about exist!” • On a practical level, the explicit construction provided here gives a general recipe to construct random variables. This can be used to simulate an arbitrary random variable, provided its distribution function. The construction proceeds in 4 steps. Step 1: Kolmogorov theorem and iid sequence of Bernoulli random variables Our construction start with Bernoulli random variables, that we now deﬁne. Deﬁnition 2.8. Let p ∈ [0, 1]. A random variable X is said to be a Bernoulli random variable with parameter p if P[X = 0] = 1 − p and P[X = 1] = p. In this case, we write X ∼ Ber(p). Example: Flipping n coins We wish to deﬁne a model for n successive independent coin ﬂips. Consider the sample space Ω = {0, 1}n equipped with the Laplace model (F, P). Deﬁne the random variables Xi ∶ Ω → {0, 1} (ω1, . . . , ωn) ( ωi . (Xi represents the result of the i-th coin ﬂip, Xi = 1 if the i-th coin ﬂip is a head, Xi = 0 if it is a tail.) Then the random variables X1, . . . , Xn are independent Bernoulli random variables with parameter 1~2. To prove that X1 ∼ Ber(1~2), we compute the probability of the events {X1 = 0} = {0} × {0, 1}n−1 and {X1 = 1} = {1} × {0, 1}n−1 using the deﬁnition of P for the Laplace model: P[X1 = 0] = S{0} × {0, 1}n−1S SΩS = 1 2 and P[X1 = 1] = S{1} × {0, 1}n−1S SΩS = 1 2. CHAPTER 2. RANDOM VARIABLES AND DISTRIBUTION FUNCTIONS 28 Equivalently, one can prove that each Xi, 1 ≤ i ≤ n is a Bernoulli random variable with parameter 1~2. To prove independence, it suﬃces to prove Equation (2.2) for x1, . . . , xn ∈ {0, 1}. For such numbers, using S{0, xi}S = 1 + xi, we have P[X1 ≤ x1, . . . , Xn ≤ xn] = P[{0, x1} × \u0005 × {0, xn}] = S{0, x1} × \u0005 × {0, xn}S SΩS = 1 + x1 2 \u0005 1 + xn 2 = P[X1 ≤ x1]\u0005P[Xn ≤ xn]. For every n ≥ 1 the example above constructs a probability space (Ω, F, P) and n independent Bernoulli random variables X1, . . . , Xn with parameter 1~2 . Similarly, it is natural to consider an inﬁnite sequence of independent Bernoulli random variables X1, X2, . . .. The construction of a suitable probability space is much more delicate and it is the content of the following theorem. Theorem 2.9 (Existence theorem of Kolmogorov). There exists a probability space (Ω, F, P) and an inﬁnite sequence of random variables X1, X2, . . . (on this probability space) that is an iid sequence of Bernoulli random variables with parameter 1~2. Proof. Admitted. Step 2: Construction of a uniform random variable in [0, 1] Here we use Bernoulli random variables to construct a uniform random variable in [0, 1]. Intuitively, one can imagine a droplet of water falling in the interval [0, 1]. We assume that the droplet falls on the interval homogeneously. For example, the probability to fall in [0, 1, 0.2] is the same as to fall in [0.8, 0.9]. A uniform random variable in [0, 1] represents the position at which such a droplet falls. Deﬁnition 2.10. A random variable U is said to be a uniform random variable in [0, 1] if its distribution function is equal to FU (x) = ¢¨¨¨¨ ¦ ¨¨¨¨¤ 0 x < 0 x 0 ≤ x ≤ 1 1 x > 1. In this case, we write U ∼ U([0, 1]). Let X1, X2, . . . be a sequence of independent Bernoulli random variables with param- eter 1~2. For every ﬁxed ω, we have X1(ω), X2(ω)\u0005 ∈ {0, 1}. Hence the inﬁnite series Y (ω) = ∞ Q n=1 2 −nXn(ω) (2.3) is absolutely convergent, and we have Y (ω) ∈ [0, 1]. CHAPTER 2. RANDOM VARIABLES AND DISTRIBUTION FUNCTIONS 29 101xFX(x)1 − p101xFX(x) Figure 2.2: Left: distribution function of a Bernoulli r.v. with parameter p . Right: distribution function of a uniform random variable in [0, 1]. Proposition 2.11. The mapping Y ∶ Ω → [0, 1] deﬁned by Equation (2.3) is a uniform random variable in [0, 1]. Step 3: Construction of a random variable with an arbitrary distribution F Let F ∶ R → [0, 1] satisfying Items (i)–(iii) at the beginning of the section. If F is strictly increasing and continuous then F is one to one and one can deﬁne its inverse F −1. For every α ∈ [0, 1], F −1(α) is the unique real number x such that F (x) = α. In such a case, this deﬁnes the inverse distribution function. More generally, we can deﬁne a generalized inverse for F . Deﬁnition 2.12 (Generalized inverse). The generalized inverse of F is the mapping F −1 ∶ (0, 1) → R deﬁned by ∀α ∈ (0, 1) F −1(α) = inf{x ∈ R ∶ F (x) ≥ α}. By deﬁnition of the inﬁmum and using right continuity of F , we have for every x ∈ R and α ∈ (0, 1) (F −1(α) ≤ x) ⇐⇒ (α ≤ F (x)). Relying on this general inverse function, the following theorem provides a way to a construct random variable with arbitrary distribution functions. Theorem 2.13 (inverse transform sampling). Let F ∶ R → [0, 1] satisfying Items (i )– (iii ) at the beginning of the section. Let U be a uniform random variable in [0, 1]. Then the random variable X = F −1(U ) (2.4) has distribution FX = F . Remark 2.14. Formally, there is an issue in the deﬁnition of X in Eq. (2.4). Indeed, we have U ∶ Ω → [0, 1] and F −1 ∶ (0, 1) → R. Nevertheless, we have P[U ∈ (0, 1)] = 1, and therefore X is well deﬁned on a set of probability 1, and we can easily ﬁx the issue by deﬁning X(ω) = ¢¨¨ ¦ ¨¨¤ F −1(U (ω)) if U (ω) ∈ (0, 1) 0 otherwise. CHAPTER 2. RANDOM VARIABLES AND DISTRIBUTION FUNCTIONS 30 (the value 0 in the second case plays no role and could be replaced by any real number). Proof. For every x ∈ R, we have P[X ≤ x] = P[F −1(U ) ≤ x] = P[U ≤ F (x)] = F (x). Step 4: General sequence of independent random variables Theorem 2.15. Let F1, F2 . . . be a sequence of functions R → [0, 1] satisfying Items (i )– (iii ) at the beginning of the section. Then there exist a probability space (Ω, F, P) and a sequence of independent random variables X1, X2, . . . on this probability space such that • for every i Xi has distribution function Fi (i.e. ∀x P[Xi ≤ x] = Fi(x)), and • X1, X2, . . . are independent. Proof. See exercise. The theorem above is important in the theory because it allows us to work with random variables directly without deﬁning precisely the probability space (Ω, F, P). For example, if F and G are two given distribution functions, it allows us for example to write: “Let X, Y be two independent random variables with distribution function F and G resp.”. Chapter 3 Discrete and continuous random variables Goals • Deﬁnition of discrete and continuous random variables. • Classical examples of discrete and continuous random variables: motivation, relation between them. • Probabilistic interpretation of the analytic properties of FX. • Density fX of a random variable: interpretation, relation with the distribution function FX. Framework We ﬁx some probability space (Ω, F, P). All the random variables consid- ered in this chapter will be deﬁned on this reference probability space. 31 CHAPTER 3. DISCRETE AND CONTINUOUS RANDOM VARIABLES 32 1 Discontinuity/continuity points of F We have seen that the distribution function F = FX of a random variable X is always right continuous. What about the left continuity? For a Bernoulli random variable X ∼ Ber(p) with p < 1, we have FX(−h) = 0 for every h > 0, but FX(0) = 1 − p ≠ 0. Therefore, FX is not left continuous at 0, i.e. lim h↓0 FX(−h) = 0 ≠ FX(0). One can see this on Fig. 5, which shows a jump of FX(x) at x = 0. In contrast, the distribution function of the Uniform random variable, represented on Fig. 5 is continuous on R, in particular, it is left continuous at every point: for a uniform random variable U , we have ∀a ∈ R lim h↓0 FU (a − h) = FU (a). The following proposition gives an interpretation of the left limit F (a−) ∶= lim h↓0 F (a − h) at a given point a for a general distribution function. Proposition 3.1 (probability of a given value). Let X ∶ Ω → R be a random variable with distribution function F . Then for every a in R we have P[X = a] = F (a) − F (a−) We omit the proof, which can easily be obtained using the basic identity of Proposi- tion 2.3 together with the continuity properties of probability measures (Prop. 1.11). We rather insist on the interpretation of this proposition. Fix a ∈ R. Ü If F is not continuous at a point a ∈ R, then the “jump size” F (a) − F (a−) is equal to the probability that X = a. Ü If F is continuous at a point a ∈ R, then P[X = a] = 0. 2 Almost sure events An important notion when working with random variables is the notion of almost sure occurrence for an event. CHAPTER 3. DISCRETE AND CONTINUOUS RANDOM VARIABLES 33 Deﬁnition 3.2. Let A ∈ F be an event. We say that A occurs almost surely (a.s.) if P[A] = 1. Remark 3.3. This notion can be extended to any set A ⊂ Ω (not necessarily an event): We say that A occurs almost surely if there exists an event A′ ∈ F such that A′ ⊂ A and P[A′] = 1. In other words, something occurs a.s. if it occurs with probability 1. For example, if X, Y are two random variables, we write X ≤ Y a.s. if P[X ≤ Y ] = 1, and X ≤ a a.s. if P[X ≤ a] = 1. 3 Discrete random variables Deﬁnition 3.4 (Discrete random variables). A random variable X ∶ Ω → R is said to be discrete if there exists some set W ⊂ R ﬁnite or countable such that X ∈ W a.s. Remark 3.5. If the sample space Ω is ﬁnite or countable, then every random variable X ∶ Ω → R is discrete. Indeed, the image X(Ω) = {x ∈ R ∶ ∃ω ∈ Ω X(ω) = x} is ﬁnite or countable and we have P[X ∈ W ] = 1, with W = X(Ω). Deﬁnition 3.6. Let X be a discrete random variable taking some values in some ﬁnite or countable set W ⊂ R. The distribution of X is the sequence of numbers (p(x))x∈W deﬁned by ∀x ∈ W p(x) ∶= P[X = x]. Proposition 3.7. The distribution (p(x))x∈W of a discrete random variable satisﬁes Q x∈W p(x) = 1. Proof. We have {X ∈ W } = ˜ x∈W{X = x}. Since the union is disjoint and the set W is at most countable, we have 1 = P[X ∈ W ] = P[ ˜ x∈W{X = x}] = Q x∈W P[X = x] = Q x∈W p(x). CHAPTER 3. DISCRETE AND CONTINUOUS RANDOM VARIABLES 34 Let us give 3 examples of discrete random variables deﬁned on (Ω, F, P), the Laplace model on Ω = {1, 2, 3, 4, 5, 6}. Example 1: Value of the die Consider the random variable X ∶ Ω → R deﬁned by ∀ω ∈ Ω X(ω) ∶= ω (X represents the value of the die). Then X takes values in W = {1, 2, 3, 4, 5, 6} almost surely. Hence it is discrete and its distribution is given by ∀x ∈ W p(x) = P[X = x] = 1 6 . Example 2: Gambling with one die Consider the random variable deﬁned by ∀ω ∈ Ω X(ω) ∶= ¢¨¨¨¨ ¦ ¨¨¨¨¤ −1 if ω = 1, 2, 3, 0 if ω = 4, 2 if ω = 5, 6. as in Example 1 Page 21. Then X takes values in W = {−1, 0, 2} almost surely and its distribution is given by p(−1) = 1 2, p(0) = 1 6 , p(2) = 1 3 . Example 3: Multiple of 3 Consider the random variable deﬁned by ∀ω ∈ Ω X(ω) ∶= ¢¨¨ ¦ ¨¨¤ 1 if ω ∈ {3, 6}, 0 otherwise. (X is the indicator function that the die is a multiple of 3). Then X takes values in W = {0, 1} almost surely and its distribution is given by p(0) = 2 3 , p(1) = 1 3. Following Deﬁnition 3.4, X is a Bernoulli random variable with parameter 1~3. CHAPTER 3. DISCRETE AND CONTINUOUS RANDOM VARIABLES 35 Remark 3.8. Conversely, if we are given a sequence of numbers (p(x))x∈W with values in [0, 1] and such that Q x∈W p(x) = 1, (3.1) then there exists a probability space (Ω, F, P) and a random variable X with associated distribution (p(x)). This is a consequence of the existence theorem 2.15 in Chapter 2. This observation is important in practice, it allows us to write: “Let X be a discrete random variable with distribution (p(x))x∈W .” Distribution p vs distribution function FX From p to FX Proposition 3.9. Let X be a discrete random variable with values in a ﬁnite or countable set W almost surely, and distribution p. Then the distribution function of X is given by ∀x ∈ R FX(x) = Q y≤x y∈W p(y) (3.2) Proof. For every x ∈ R we have P[X ≤ x] = P[X ∈ (−∞, x] ∩ W ]] + P[X ∈ (−∞, x] ∩ W c] ´¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¸¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¶ ≤P[X∈W c]=0 = P[ ˜ y≤x y∈W{X = y}] = Q y≤x y∈W P[X = y]. From FX to p Given a discrete random variable X, Equation (3.2) expresses the distri- bution function FX in terms of p as a piecewise constant function. Conversely, a random variable with a piecewise constant distribution function F is discrete and W and p are given by W = {positions of the jumps of FX}, p(x) = “height of the jump” at x ∈ W . 4 Examples of discrete random variables Bernoulli distribution The simplest (non constant) random variable is the Bernoulli random variable. It was deﬁned already in the previous chapter. We recall its deﬁnition here. CHAPTER 3. DISCRETE AND CONTINUOUS RANDOM VARIABLES 36 Deﬁnition 3.10 (Bernoulli). Let 0 ≤ p ≤ 1. A random variable X is said to be a Bernoulli random variable with parameter p if it takes values in W = {0, 1} and P[X = 0] = 1 − p and P[X = 1] = p. In this case, we write X ∼ Ber(p). Binomial distribution Another fundamental example is the binomial distribution, which appears in applications when we consider the number of successes in a repetition of Bernoulli experiments. Deﬁnition 3.11 (Binomial). Let 0 ≤ p ≤ 1, let n ∈ N. A random variable X is said to be a binomial random variable with parameters n and p if it takes values in W = {0, . . . , n} and ∀k ∈ {0, . . . , n} P[X = k] = ‰n k’ pk (1 − p)n−k . In this case, we write X ∼ Bin(n, p). Remark 3.12. If we deﬁne p(k) = › n k” pk (1 − p)n−k, we have n Q k=0 p(k) = n Q k=0 ‰n k’ pk (1 − p)n−k = (p + 1 − p) n = 1, hence the equation (3.1) is satisﬁed. This guarantees the existence of binomial random variables. Proposition 3.13 (Sum of independent Bernoulli and binomial). Let 0 ≤ p ≤ 1, Let n ∈ N. Let X1, . . . , Xn be independent Bernoulli random variables with parameter p. Then Sn ∶= X1 + \u0005 + Xn is a binomial random variable with parameter n and p. Proof. One can easily check that Sn is a random variable which takes values in {0, . . . , n}. Furthermore, for every k ∈ {0, . . . , n} we have {Sn = k} = ˜ x1,...,xn∈{0,1} x1+\u0005+xn=k {X1 = x1, \u0005, Xn = xn}. CHAPTER 3. DISCRETE AND CONTINUOUS RANDOM VARIABLES 37 Since the union is disjoint, we get P[Sn = k] = Q x1,...,xn∈{0,1} x1+\u0005+xn=k P[X1 = x1, \u0005, Xn = xn] = Q x1,...,xn∈{0,1} x1+\u0005+xn=k P[X1 = x1]\u0005P[Xn = xn] = Q x1,...,xn∈{0,1} x1+\u0005+xn=k pk(1 − p)n−k = ‰n k’pk(1 − p) n−k. Remark 3.14. In particular, the distribution Bin(1, p) is the same as the distribution Ber(p). One can also check that if X ∼ Bin(m, p) and Y ∼ Bin(n, p) and X, Y are independent, then X + Y ∼ Bin(m + n, p). Geometric distribution Deﬁnition 3.15 (Geometric). Let 0 < p ≤ 1. A random variable X is said to be a geometric random variable with parameter p if it takes values in W = N…{0} and ∀k ∈ N…{0} P[X = k] = (1 − p) k−1 ⋅ p . In this case, we write X ∼ Geom(p). Remark 3.16. For p = 1, and k = 1, a term 00 appears in the equation above, we use the convention 00 = 1 and therefore P[X = 1] = 1 in this case. Remark 3.17. If we deﬁne p(k) = (1 − p)k−1 p, we have ∞ Q k=1 p(k) = p ∞ Q k=1 (1 − p)k−1 = p ⋅ 1 p = 1, hence the equation (3.1) is satisﬁed. This guarantees the existence of geometric random variables. The geometric random variable appears naturally as the ﬁrst success in an inﬁnite sequence of Bernoulli experiments with parameter p. This is formalized by the following proposition. Proposition 3.18. Let X1, X2, . . . be a sequence of inﬁnitely many independent Bernoulli r.v.’s with parameter p. Then T ∶= min{n ≥ 1 ∶ Xn = 1} is a geometric random variable with parameter p. CHAPTER 3. DISCRETE AND CONTINUOUS RANDOM VARIABLES 38 Remark 3.19. When saying that T is a geometric random variable, we make a slight abuse: indeed, the random variable T may take the value +∞ if all the random variables Xi’s are equal to 0. Nevertheless, this is not a problem for the calculations because one can check that P[T = ∞] = 0. Proof. We have T = k if the ﬁrst k − 1 trials fail, and the k’s one is a success. Formally, we have {T = k} = {X1 = 0, . . . , Xk−1 = 0, Xk = 1}. Hence, by independence, P[T = k] = P[X1 = 0, . . . , Xk−1 = 0, Xk = 1] = P[X1 = 0]\u0005P[Xk−1 = 0] P[Xk = 1] = (1 − p) k−1p. The previous proposition gives us an easy way to remember the deﬁnition of the geometric r.v., and also some simple formulas related to the geometric distribution. For example, if T is a geometric distribution with parameter p, we have T > n if the n ﬁrst Bernoulli experiments fail, and therefore P[T > n] = (1 − p)n. (3.3) Also, it gives an important interpretation to the equation (3.4) in the proposition below: if we are waiting for a ﬁrst success in a sequence of experiments, and if we know that the ﬁrst n steps were a failure, then the remaining time to wait is again a geometric random variable with parameter p. Proposition 3.20 (Absence of memory of the geometric distribution). Let T ∼ Geom(p) for some 0 < p < 1. Then ∀n ≥ 0 ∀k ≥ 1 P[T ≥ n + k S T > n] = P[T ≥ k]. (3.4) Proof. It follows directly from the formula (3.3). Poisson distribution Deﬁnition 3.21. Let λ > 0 be a positive real number. A random variable X is said to be a Poisson random variable with parameter λ if it takes values in W = N and ∀k ∈ N P[X = k] = λk k! e−λ . In this case, we write X ∼ Poisson(λ). CHAPTER 3. DISCRETE AND CONTINUOUS RANDOM VARIABLES 39 Remark 3.22. If we deﬁne p(k) = λk k! e−λ, we have ∞ Q k=0 p(k) = e−λ ∞ Q k=0 λk k! = e−λ ⋅ e λ = 1, hence the equation (3.1) is satisﬁed. This guarantees the existence of Poisson random variables. The Poisson distribution appears naturally as an approximation of a binomial distri- bution when the parameter n is large and the parameter p is small, as stated formally in the following proposition. Proposition 3.23 (Poisson approximation of the binomial). Let λ > 0. For every n ≥ 1, consider a random variable Xn ∼ Bin(n, λ n ). Then ∀k ∈ N lim n→∞ P[Xn = k] = P[N = k], (3.5) where N is a Poisson random variable with parameter λ. Remark 3.24. The convergence (3.5) is called a convergence in distribution. Intuitively, it says that Xn and N have very similar probabilistic properties for n large. Proof. Fix k ∈ N. For every n ≥ 1, we have P[Xn = k] = ‰n k’ ‰ λ n’ k ‰1 − λ n’ n−k = λk k! ⋅ n ⋅ (n − 1)\u0005(n − k + 1) nk ⋅ ‰1 − λ n’ −k ´¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¸¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¶ Ð→ n→∞1 ⋅ ‰1 − λ n’ n ´¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¸¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¶ Ð→ n→∞e−λ , which concludes the proof. This approximation may be useful in practice. For example, consider a single page of the “Neue Zürcher Zeitung” containing, say, n = 104 characters, and suppose that the typesetter mis-sets approximately 1~1000 of the characters. In other words, each character has a probability p = 10~n to be mis-set. The number M of mistakes in the page corresponds to a binomial random variable with parameters n and p = 10~n. Hence by the Poisson approximation, for example we have P[M = 5] \b 105 5! e−10 \b 0, 0378. 5 Continuous random variables CHAPTER 3. DISCRETE AND CONTINUOUS RANDOM VARIABLES 40 Deﬁnition 3.25 (Continuous random variables). A random variable X ∶ Ω → R is said to be continuous if its distribution function FX can be written as FX(a) = S a −∞ f (x)dx for all a in R (3.6) for some nonnegative function f ∶ R → R+, called the density of X. Intuition: f (x)dx represent the probability that X takes a value in the inﬁnitesimal interval [x, x + dx]. To understand the terminology “continuous”, observe that the formula (3.6) implies that FX is a continuous function. In particular, by Proposition 3.1, the r.v. X satisﬁes ∀x ∈ R P[X = x] = 0. Proposition 3.26. The density f of a random variable satisﬁes S +∞ −∞ f (x)dx = 1 . Proof. We have S +∞ −∞ f (x)dx = lim y→∞ S y −∞ f (x)dx = lim y→∞ FX(y) = 1. Conversely, if we are given a nonnegative function f ∶ R → R+ such that S +∞ −∞ f (x)dx = 1. then there exists a probability space (Ω, F, P) and a random variable X with associated density f . This is a consequence of the existence theorem 2.15 in Chapter 2. Density f vs distribution function FX From f to FX Let X be a continuous random variable X with density f . By deﬁnition, the distribution function FX can be calculated as the integral FX(x) = S x −∞ f (y)dy. CHAPTER 3. DISCRETE AND CONTINUOUS RANDOM VARIABLES 41 From FX to f Since one goes from f to FX by integrating, it is natural to expect that the reverse operation is to take a derivative. This is in general the case, provided FX is regular enough. The following theorem will be useful in applications to calculate densities. Theorem 3.27. Let X be a random variable. Assume the distribution function FX is continuous and piecewise C1, i.e. that there exist x0 = −∞ < x1 < \u0005 < xn−1 < xn = +∞ such that FX is C1 on every interval (xi, xi+1). Then X is a continuous random variable and a density f can be constructed by deﬁning ∀x ∈ (xi, xi+1) f (x) = F ′ X(x) and setting arbitrary values at x1, . . . , xn−1. Proof. We simply write F = FX. Let 0 ≤ i < n. If xi < a < b < xi+1, the fundamental theorem of calculus implies that F (b) − F (a) = S b a F ′(y)dy = S b a f (y)dy Now, let x ∈ R and let i be such that x ∈ [xi, xi+1). Using the convention F (x0) = 0, we can write F (x) as a telescopic sum F (x) = F (x) − F (x0) = (F (x) − F (xi)) + \u0005 + (F (x1) − F (x0)). (3.7) By continuity of F , we have (F (x) − F (xi)) = lim a↓xi(F (x) − F (a)) = lim a↓xi S x a f (y)dy = S x xi f (y)dy, and equivalently F (xi) − F (xi−1) = S xi xi−1 f (y)dy Plugging these identities in (3.7), we get F (x) = S x xi f (y)dy + S xi xi−1 f (y)dy + \u0005 + S x1 x0 f (y)dy = S x −∞ f (y)dy. 6 Examples of continuous random variables Uniform distributions CHAPTER 3. DISCRETE AND CONTINUOUS RANDOM VARIABLES 42 Deﬁnition 3.28 (Uniform distribution in [a, b], a < b.). A continuous random vari- able X is said be uniform in [a, b] if its density is equal to fa,b(x) = ¢¨¨ ¦ ¨¨¤ 1 b−a x ∈ [a, b], 0 x ∉ [a, b]. In this case, we write X ∼ U([a, b]). 1 b−aabxfa,b(x)1abxFX(x) Figure 3.1: Density and distribution function of a uniform random variable in [a, b]. Intuition: X represents a uniformly chosen point in [a, b]. Properties of a uniform random variable X in [a, b]: • The probability to fall in a an interval [c, c + ℓ] ⊂ [a, b] depends only on its length ℓ: P[X ∈ [c, c + ℓ]] = ℓ b − a. • The distribution function of X is equal to FX(x) = ¢¨¨¨¨ ¦ ¨¨¨¨¤ 0 x < a, x−a b−a a ≤ x ≤ b, 1 x > b. Proof. Exponential distribution The exponential distribution is the continuous analogue of the geometric distribution. CHAPTER 3. DISCRETE AND CONTINUOUS RANDOM VARIABLES 43 Deﬁnition 3.29 (Exponential distribution with λ > 0). A continuous random variable T is said be exponential with parameter λ > 0 if its density is equal to fλ(x) = ¢¨¨ ¦ ¨¨¤ λe−λx x ≥ 0, 0 x < 0. In this case, we write T ∼ Exp(λ). λ/eλ1/λxfλ(x) = λe −λx 1xFX(x) = 1 − e−λx Figure 3.2: Density and distribution function of an exponential random variable with parameter λ. Intuition/application: T represents the time of a “clock ring”. For example, the time at which a ﬁrst customer arrives in a shop is well modeled by an exponential random variable. Properties of an exponential random variable T with parameter λ. • The waiting probability is exponentially small: ∀t ≥ 0 P[T > t] = e−λt . • It has the absence of memory property: ∀t, s ≥ 0 P[T > t + sST > t] = [T > s]. The ﬁrst item follows from the deﬁnition: P[T ≥ t] = S ∞ t λe −λxdx = e −λt. The second item is a direct computation of the conditional probability: P[T > t + sST > t] = P[T > t + s] P[T > t] = e−λ(t+s) e−λt = e−λs. Normal distribution CHAPTER 3. DISCRETE AND CONTINUOUS RANDOM VARIABLES 44 Deﬁnition 3.30. A continuous random variable X is said be normal with param- eters m and σ2 > 0 if its density is equal to fm,σ(x) = 1 √ 2πσ2 e− (x−m)2 2σ2 In this case, we write X ∼ N (m, σ2). fm,σ(x)m−σm+σm Figure 3.3: Density of a normal random variable with parameters m and σ2. Intuition/application: The normal distribution arises in many applications. For exam- ple, imagine that we measure a physical quantity: the real value is m and the measured value is in general very well modeled by a normal random variable X with parameters m and σ. The quantity σ, which represents the ﬂuctuations for X can also be inter- preted as the quality of the measurement in this context. A small σ corresponds to small ﬂuctuations of X, which means that X is typically closed to m. In contrary, a large σ corresponds to large ﬂuctuations and can be interpreted as inaccurate measurement. We will see later a mathematical justiﬁcation explaining why the normal random variable appears in many places. Properties of normal random variables • If X1, . . . , Xn are independent random variables with parameters (m1, σ2 1), . . . , (mn, σ2 n) respectively, then Z = m0 + λ1X1 + . . . + λnXn is a normal random variable with parameters m = m0 + λ1m1 + \u0005 + λnmn and σ2 = λ2 1σ2 1 + \u0005 + λ2 nσ2 n. • In particular, if X ∼ N (0, 1) (in this case we say that X is a standard normal random variable), then Z = m + σ ⋅ X is a normal random variable with parameters m and σ2. CHAPTER 3. DISCRETE AND CONTINUOUS RANDOM VARIABLES 45 • If X is a normal random variable with parameters m and σ2, then all the “probability mass” is mainly in the interval [m − 3σ, m + 3σ]. Namely, we have P[SX − mS ≥ 3σ] ≤ 0.0027. At a ﬁrst look, it may be surprising that the right hand side (0.0027) does not depend on the parameters σ and m... This is explained as follows: consider the random variable Z = X−m σ . The ﬁrst property above implies that Z = 1 σ X − m σ is a standard normal random variable. The left hand side can be rewritten as P[SX − mS ≥ 3σ] = P[SX − m σ S ≥ 3] = P[SZS ≥ 3] Then the inequality P[SZS ≥ 3] ≤ 0.0027 can be directly checked from a table. Chapter 4 Expectation Goals • Deﬁnition of the expectation and intuition. • Rules of calculus (sum/product of random variables). • Inequalities, relations between expectation and probability of events • Deﬁnition of the variance and intuition. Framework We ﬁx a probability space (Ω, F, P). All the random variables considered in this chapter will be deﬁned on this reference probability space. 46 CHAPTER 4. EXPECTATION 47 In this class, we focus on discrete and continuous random variables, and we will give the expectation for these two diﬀerent types of random variables by two diﬀerent formulas. There is a uniﬁed theory (based on measure theory) of expectation, that deﬁnes the expectation for general random variables. In this class, we will keep the focus on the important results from this theory and their applications, without giving the proofs. 1 Expectation for general random variables Deﬁnition 4.1. Let X ∶ Ω → R+ be a random variable with nonnegative values. The expectation of X is deﬁned as E[X] = S ∞ 0 (1 − FX(x))dx. (4.1) Remark 4.2. The expectation may be ﬁnite or inﬁnite. Proposition 4.3. Let X be a nonnegative random variable. Then we have E[X] ≥ 0, with equality if and only if X = 0 almost surely. Proof. The expectation E[X] is deﬁned as the integral of the nonnegative function G(x) = 1 − FX(x) ≥ 0. Hence E[X] ≥ 0. Now, assume that E[X] = 0. This implies that G(x) = 0 for every x > 0 (by contradiction, if G(x) = α > 0 for some x > 0, then G(y) ≥ G(x) = α for all y ∈ [0, x] by monotonicity, which implies that ∫ x 0 G(y)dy ≥ xα > 0). By continuity of probability measures, we have P[X > 0] = lim x↓0 P[X > x] = lim x↓0 G(x) = 0. Therefore, P[X ≤ 0] = 1 − P[X > 0] = 1. Hence X ≥ 0 and X ≤ 0 almost surely, which implies that X = 0 almost surely. For general random variables (not necessarily with a constant sign), we deﬁne the expectation by decomposing into positive and negative parts. The positive and negative parts of X are the random variables X−, X+ deﬁned by X+(ω) = ¢¨¨ ¦ ¨¨¤ X(ω) if X(ω) ≥ 0, 0 if X(ω) < 0, and X−(ω) = ¢¨¨ ¦ ¨¨¤ −X(ω) if X(ω) ≤ 0, 0 if X(ω) > 0. Notice that both X+ and X− take nonnegative values. Furthermore, we have X = X+ −X−, and SXS = X+ + X−. Deﬁnition 4.4. Let X be a random variable. If E[SXS] < ∞, then the expectation of X is deﬁned by E[X] = E[X+] − E[X−]. (4.2) CHAPTER 4. EXPECTATION 48 Remark 4.5. The condition E[SXS] < ∞ ensures that E[X−], E[X+] < ∞ (because SXS = X+ + X−), and therefore the diﬀerence in Eq. (4.2) makes sense. If X ≥ 0, the expectation of X is always deﬁned. It may be ﬁnite or inﬁnite. If X does not have a constant sign, the expectation of X is well deﬁned if E[SXS] < ∞. When this condition is not satisﬁed, we say that the expectation of X is undeﬁned. 2 Expectation of a discrete random variable Proposition 4.6. Let X ∶ Ω → R be a discrete random variable with values in W (ﬁnite or countable) almost surely. We have E[X] = Q x∈W x ⋅ P[X = x] , provided the sum is well deﬁned. Proof. We ﬁrst assume that X ≥ 0 almost surely. By Propositions 3.7 and 3.9 we have for every x ∈ R 1 − FX(x) = Q y>x y∈W p(y) = Q y∈W 1y>x ⋅ p(y). By using this identity in the deﬁnition of the expectation, we get E[X] = S ∞ 0 › Q y∈W 1y>x ⋅ p(y)”dx = Q y∈W › S ∞ 0 1y>xdx” ⋅ p(y) = Q y∈W y ⋅ p(y). Now if X is not of constant sign, we use the decomposition X = X+ − X− and we apply the formula above to X+and X−. We obtain E[X] = E[X+] − E[X−] = Q y∈W yP[X+ = y] − Q y∈W yP[X− = y]. The deﬁnitions of X+ and X− imply that {X = y} is equal to the disjoint union {X+ = y} ∪ {X− = −y} Example 1: Bernoulli r.v. Let X be a Bernoulli random variable with parameter p. We have E[X] = p . Indeed, E[X] = 0 ⋅ P[X = 0] + 1 ⋅ P[X = 1] = 0 ⋅ (1 − p) + 1 ⋅ p = p. Example 2: Bet on a die CHAPTER 4. EXPECTATION 49 Consider the random variable X ∶ Ω → {−1, 0, +2} deﬁned by Eq. (2.1) page 21. Then E[X] = −1 ⋅ 1 2 + 0 ⋅ 1 6 + 2 ⋅ 1 3 = 1 6 . Example 3: Poisson r.v. Let X be a Poisson random variable with parameter λ > 0, then E[X] = λ . Indeed, E[X] = ∞ Q k=0 k ⋅ λk k! ⋅ e−λ = λ ⋅ ( ∞ Q k=1 λk−1 (k − 1)!) ⋅ e −λ = λ. Example 4: Indicator of an event Let A ∈ F be an event. Consider the indicator function 1A of A, deﬁned by ∀ω ∈ Ω 1A(ω) = ¢¨¨ ¦ ¨¨¤ 0 if ω ∉ A, 1 if ω ∈ A. Then 1A is a random variable. Indeed, we have {1A ≤ a} = ¢¨¨¨¨ ¦ ¨¨¨¨¤ g if a < 0, Ac if 0 ≤ a < 1, Ω if a ≥ 1, and g, Ac and Ω are three elements of F. Furthermore, writing X = 1A, we have P[X = 0] = 1 − P[A] and P[X = 1] = P[A]. Therefore 1A is a Bernoulli r.v. with parameter P[A]. Hence, E[1A] = P[A] . Proposition 4.7. Let X ∶ Ω → R be a discrete random variable with values in W (ﬁnite or countable) almost surely. For every φ ∶ R → R, we have E[φ(X)] = Q x∈W φ(x) ⋅ P[X = x] , provided the sum is well deﬁned. Proof. Admitted. 3 Expectation of a continuous random variable CHAPTER 4. EXPECTATION 50 Proposition 4.8. Let X be a continuous random variable with density f . Then, we have E[X] = S ∞ −∞ x ⋅ f (x)dx, (4.3) provided the integral is well deﬁned. Proof. We assume that X ≥ 0 almost surely. The general case (without sign constraint) can be deduced from the positive case by using the decomposition X = X+ − X− (similarly as in the proof of Proposition 4.6). By deﬁnition of the density and Proposition 3.26, for every x ∈ R we have 1 − FX(x) = S +∞ x f (y)dy = S +∞ −∞ 1x<y ⋅ f (y)dy. By using this identity in the deﬁnition of the expectation, we get E[X] = S ∞ 0 − S +∞ −∞ 1x<y ⋅ f (y)dy‘dx = S +∞ −∞ − S ∞ 0 1y>x ⋅ dx‘ ⋅ f (y)dy = S ∞ −∞ y ⋅ f (y)dy. Example 1: Uniform random variable in [a, b], a < b We have E[X] = 1 b − a S b a xdx = 1 b − a ⋅ (1 2 b2 − 1 2 a2). Therefore, E[X] = a + b 2 . Example 2: Exponential random variable with parameter λ > 0 By integration by parts, we have E[X] = S ∞ 0 xλe−λxdx = \u0001−xe −λx\u0006∞ 0 + S ∞ 0 e −λxdx. Therefore, E[X] = 1 λ. Proposition 4.9. Let X be a continuous random variable with density f . Let φ ∶ R → R be such that φ(X) is a random variable. Then, we have E[φ(X)] = S ∞ −∞ φ(x)f (x)dx, (4.4) CHAPTER 4. EXPECTATION 51 provided the integral is well deﬁned. 4 Calculus One of the reasons why the expectation is a such a powerful tool in probability theory is that we can do calculations: for example, one can calculate the expectation of X + Y if one knows the expectations of X and Y . In this section we give the rules of calculus for the basic operations on random variables. Linearity Theorem 4.10 (Linearity of the expectation). Let X, Y ∶ Ω → R be random variables, let λ ∈ R. Provided the expectations are well deﬁned, we have 1. E[λ ⋅ X] = λ ⋅ E[X] . 2. E[X + Y ] = E[X] + E[Y ] . Remark 4.11. The random variables X and Y do not need to be independent. Remark 4.12. More generally, by induction we have: for every integer n ≥ 1 E[λ1X1 + λ2X2 + \u0005 + λnXn] = λ1E[X1] + λ2E[X2] + \u0005 + λnE[Xn], for any n random variables X1, X2, . . . , Xn ∶ Ω → E, and any λ1, λ2, \u0005, λn ∈ R, provided the expectations are well deﬁned. Proof. The proof of Item (i) follows from the deﬁnition. The proof of Item (ii) for general random variables belongs to the abstract framework of measure theory and we admit it. We give here the proof for a ﬁnite sample space, which illustrates well the key idea behind the linearity property. Let us assume that Ω is ﬁnite, and is equipped with the sigma-algebra F = P(Ω). In this case the two random variables X and Y are necessarily discrete (see Remark 3.5). For every x ∈ X, P[X = x] = P[ ˜ ω∈Ω ∶ X(ω)=x{ω}] = Q ω∈Ω 1X(ω)=x ⋅ P[ω], CHAPTER 4. EXPECTATION 52 where we make the abuse of notation P[ω] = P[{ω}]. Therefore, E[X] = Q x∈X(Ω) x ⋅ P[X = x] = Q x∈X(Ω) x ⋅ − Q ω∈Ω 1X(ω)=x ⋅ P[ω]‘ = Q x∈X(Ω) Q ω∈Ω x ⋅ 1X(ω)=x ⋅ P[ω] = Q ω∈Ω P[ω] ⋅ Q x∈X(Ω) x ⋅ 1X(ω)=x ´¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¸¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¶ =X(ω) = Q ω∈Ω X(ω) ⋅ P[ω]. Using this formula for X + Y and Y , we obtain E[X + Y ] = Q ω∈Ω ›X(ω) + Y (ω)” ⋅ P[ω] = Q ω∈Ω X(ω) ⋅ P[ω] + Q ω∈Ω Y (ω) ⋅ P[ω] = E[X] + E[Y ]. Application 1: Expectation of a Binomial random variable. Let n ≥ 1 and 0 ≤ p ≤ 1. Let S be a binomial random variable with parameters n and p. What is the expectation of S? By deﬁnition we have E[S] = n Q k=0 k ⋅ ‰n k’pk(1 − p)n−k and this sum does not look so nice... However, we can use that S has the same distribution as Sn = X1 + \u0005 + Xn, where X1, . . . , Xn are n i.i.d. Bernoulli random variables with parameter p. By linearity we have E[Sn] = E[X1] + \u0005 + E[Xn]. Using that E[Xi] = p for every i, we deduce directly E[S] = E[Sn] = np. Application 2: Expectation of a normal random variable If X is a normal distribution with parameters m and σ2, then it has the same distribution as m + σ ⋅ Y where Y is a standard normal random variable. By Proposition 4.9, we have E[X] = E[m + σ ⋅ Y ] = m + σE[Y ], hence it suﬃces to compute the expectation of Y . Writing f0,1 for the density of Y , we have E[Y ] = S ∞ −∞ x ⋅ f0,1(x)dx = 0 because x ⋅ f0,1(x) is an odd function. Finally, we obtain E[X] = m. CHAPTER 4. EXPECTATION 53 Theorem 4.13. Let X, Y be two random variables. If X and Y are independent, then E[XY ] = E[X]E[Y ]. Proof. Admitted. 5 Tailsum formulas Proposition 4.14 (Tailsum formula for nonnegative random variables). Let X be a ran- dom variable, such that X ≥ 0 almost surely. Then, we have E[X] = S ∞ 0 P[X > x]dx. Proof. It follows form the deﬁnition of the expectation (Eq. (4.1)) and the identity 1 − FX(x) = 1 − P[X ≤ x] = P[X > x]. Application: Alternative computation of the expectation of an exponential random variable. We proved in the previous paragraph that the expectation of an exponential random variable T with parameter λ ≥ 0 is equal to 1~λ. We here give an alternative derivation of this result. We have T ≥ 0 almost surely, and P[T > x] = e−λx. Hence, the proposition above gives E[T ] = S ∞ 0 e −λxdx = 1 λ. Proposition 4.15 (Tailsum formula for discrete random variables). Let X be a discrete r.v. taking values in N = {0, 1, 2, . . .}. Then E[X] = ∞ Q n=1 P[X ≥ n]. Proof. Since X ≥ 0 almost surely, we can apply proposition 4.14 to write E[X] = S ∞ 0 P[X > x]dx = ∞ Q n=1 S n n−1 P[X > x]dx. Now, for x ∈ [n − 1, n) we have P[X > x] = P[X ≥ n]. Therefore, S n n−1 P[X > x]dx = P[X ≥ n] S n n−1 dx = P[X ≥ n]. CHAPTER 4. EXPECTATION 54 Application: Computation of the expectation of a geometric random variable. Let T be a geometric random variable with parameter 0 < p ≤ 1. Then E[T ] = 1 p . Indeed, T takes values in N and the proposition above gives E[T ] = Q n≥1 P[T ≥ n] = Q n≥1 (1 − p) n−1 = 1 1 − (1 − p) = 1 p . 6 Characterizations via expectations Density If a random variable X has a density f , we can calculate the expectation of X via the formula (4.4). Notice that two random variables with diﬀerent densities may have the same expectation. For example a uniform random variable in [−1, 1] and a Gaussian random variable with parameters m = 0 and σ2 > 0 have the same expectation, but diﬀerent densities. In other words, the expectation of a random variable does not characterize the density. Nevertheless it is possible to characterize the density of a random variable X, by considering all the expectations of images φ(X) for a suﬃciently large class of functions φ. This is the content of the proposition below. For this course we consider functions φ that are piecewise continuous, bounded func- tions. Recall that a function φ ∶ R → R is piecewise continuous if there exists a1 < a2 < \u0005 < an such that φ is continuous on each interval (ai, ai+1). It is bounded if there exists C > 0 such that ∀x ∈ R Sφ(x)S ≤ C. Proposition 4.16. Let X be a random variable. Let f ∶ R → R+ such that ∫ +∞ −∞ f (x)dx = 1. Then the following are equivalent: (i) X is continuous with density f , (ii) For every function φ ∶ R → R piecewise continuous, bounded, E[φ(X)] = S ∞ −∞ φ(x)f (x)dx. (4.5) Proof. (i) ⇒ (ii) It follows from Prop. 4.9. (ii) ⇒ (i) Let a ∈ R, and consider the function φa deﬁned by φa(x) = 1x≤a. CHAPTER 4. EXPECTATION 55 By applying Eq. (4.5), we ﬁnd E[1X≤a] = E[φa(X)] = S ∞ −∞ φa(x)f (x)dx = S a −∞ f (x)dx. By applying the identity E[1A] = P[A] to the event A = {X ≤ a}, we ﬁnally get P[X ≤ a] = S a −∞ f (x)dx, which concludes the proof. Independence If two random variables X and Y are independent, then we have seen in Theorem 6.1 that E[XY ] = E[X]E[Y ]. (4.6) Conversely, the formula above does not imply that X and Y are independent. See for example Exercise 6.6. Nevertheless, if we consider a stronger form of Eq. (4.6), allowing to take arbitrary images of X and Y we obtain a characterization of independence, as stated below. Theorem 4.17. Let X, Y be 2 discrete random variables. Then the following are equivalent (i) X, Y are independent, (ii) For every φ ∶ R → R, ψ ∶ R → R piecewise continuous, bounded, E[φ(X)ψ(Y )] = E[φ(X)]E[ψ(Y )] . (4.7) Proof. (i)⇒ (ii) Admitted. (ii)⇒ (i) Let a, b ∈ R. By applying Eq. (4.7) to the two function deﬁned by φa(x) = 1x≤a and ψb(y) = 1y≤b, we get E[1X≤a,Y ≤b] = E[1X≤a]E[1X≤b]. Using E[1A] = P[A], this concludes that X and Y are independent. Above, we only considered a pair (X, Y ) of random variables, but the same ideas apply to n random variables X1, . . . , Xn, as in the following theorem. CHAPTER 4. EXPECTATION 56 Theorem 4.18. Let X1, . . . , Xn be n random variables. Then the following are equiv- alent (i) X1, . . . , Xn are independent, (ii) For every φ1 ∶ R → R, . . . , φn ∶ R → R piecewise continuous, bounded E[φ1(X1)\u0005φn(Xn)] = E[φ1(X1)]\u0005E[φn(Xn)]. 7 Inequalities Monotonicity Proposition 4.19. Let X, Y be two random variables such that X ≤ Y a.s. Then E[X] ≤ E[Y ], provided the two expectations are well deﬁned. Proof. Consider the random variable Z = Y − X. By hypothesis, we have Z ≥ 0, which implies that E[Z] ≥ 0 (by Prop. 4.3). By linearity, we have E[Y ] − E[X] = E[Z] ≥ 0. Markov’s inequality Theorem 4.20 (Markov’s inequality). Let X be a nonnegative random variable. Then for every a > 0, we have P[X ≥ a] ≤ E[X] a . (4.8) Proof. Let a > 0. By monotonicity of the expectation, we have E[X] ≥ E[X ⋅ 1X≥a] ≥ E[a ⋅ 1X≥a] = aP[X ≥ a]. CHAPTER 4. EXPECTATION 57 Jensen’s inequality Theorem 4.21 (Jensen’s inequality). Let X be a random variable. Let φ ∶ R → R be a convex function. If E[φ(X)] and E[X] are well deﬁned, then φ(E[X]) ≤ E[φ(X)]. The Jensen inequality has several important consequences. First, by applying it to φ(x) = SxS, we obtain the triangle inequality. For every integrable discrete random variable X, we have SE[X]S ≤ E[SXS]. Another important consequence relates the average of SXS with the average of X 2. By applying it to the convex function φ(x) = x2, we obtain that for every discrete random variable, we have E[SXS] ≤ »E[X 2]. (4.9) 8 Variance Deﬁnition 4.22. Let X be a variable such that E[X 2] < ∞. The variance of X is deﬁned by σ2 X = E[(X − m)2], where m = E[X]. The square root σX of the variance is called the standard deviation of X. Remark 4.23. If E[X 2] < ∞, then we also have E[SXS] < ∞ by Eq. (4.9) and therefore the average m = E[X] is well-deﬁned. The standard deviation is an indicator of how large the ﬂuctuations of X around m = E[X] are. We illustrate this fact on two simple examples. Example 1: Deterministic random variable Let a ∈ R. Consider the random variable deﬁned by X(ω) = a for every ω. Then m = E[X] = a and σ2 X = E[(X − m)2] = 0. Example 2: Uniform random variable on two points Let a < b be two real numbers. Consider a random variable X with distribution given by P[X = a] = P[X = b] = 1~2. Then m = E[X] = (a + b)~2 and σX = »E[(X − m)2] = a − b 2 . In general, a random variable X with a small variance is well concentrated on values close to its expectation m = E[X]. This concentration phenomena can be quantiﬁed using the Chebyshev’s inequality. CHAPTER 4. EXPECTATION 58 Theorem 4.24. Let X be a random variable such that E[X 2] < ∞. Then for every a ≥ 0 we have P[SX − mS ≥ a] ≤ σ2 X a2 , where m = E[X]. Proof. Consider the random variable Y = (X − m)2. By deﬁnition, we have σ2 X = E[Y ]. Furthermore, for every a ≥ 0 P[SX − mS ≥ a] = P[Y ≥ a2]. By applying Markov’s inequality to Y (which is nonnegative), we obtain P[SX − mS ≥ a] ≤ E[Y ] a2 ≤ σ2 X a2 . Proposition 4.25 (basic properties of the variance). 1. Let X be a random variable with E[X 2] < ∞. Then σ2 X = E[X 2] − E[X] 2. 2. Let X be a random variable with E[X 2] < ∞, let λ ∈ R. Then σ2 λX = λ2 ⋅ σ2 X. 3. Let X1, . . . , Xn be n pairwise independent random variables and S = X1 +\u0005+Xn. Then σ2 S = σ2 X1 + \u0005 + σ2 Xn. Proof. 1. Let m = E[X]. Using linearity of the expectation, we get E[(X − m)2] = [X 2 − 2mX + m 2] = E[X 2] − 2mE[X] + m 2 = E[X 2] − m 2. 2. Using the formula of Item 1 and linearity of the expectation, we obtain σ2 λX = E[(λX)2] − (E[λX]) 2 = λ2 ⋅ E[X 2] − λ 2 ⋅ E[X]2 = λ2 ⋅ σ2 X. 3. Writing mi = E[Xi], we have S − E[S] = n Q i=1(Xi − mi), CHAPTER 4. EXPECTATION 59 and therefore σ2 S = Q 1≤i,j≤n E[(Xi − mi)(Xj − mj)]. However, for i ≠ j, independence implies E[(Xi−mi)(Xj−mj)] = E[(Xi−mi)]E[(Xj− mj)] = 0. Hence only the diagonal terms (for which i = j) survive in the sum and we obtain σ2 S = n Q i=1 E[(Xi − mi) 2] = n Q i=1 σ2 Xi. Application: Let S be a binomial random variables with parameters n and p. What is the variance of S? Here, again, we can use that S has the same distribution as Sn = X1 + \u0005 + Xn where X1, . . . , Xn are i.i.d. Bernoulli random variables with parameter p. Hence σ2 S = σ2 Sn independence = σ2 X1 + \u0005 + σ2 Xn ident. distrib. = n ⋅ σ2 X1. One has σ2 X1 = E[X 2 i ] − p2 = p − p2 = p(1 − p). Hence σ2 S = n ⋅ p(1 − p) . Here, we have discovered an important eﬀect of summing i.i.d. random variables. One has E[S] = n ⋅ p and σS = √ n ⋅ »p(1 − p) so the expectation of S grows like n, while the ﬂuctuations of Sn grow like √ n thanks to cancellations in the sum Sn − np = (X1 − p) + \u0005 + (Xn − p). 9 Covariance We introduce the notion of covariance, which can be used in some cases as to quantify the dependence between two random variables. Deﬁnition 4.26. Let X, Y be two random variables. Assume that E[X 2] < ∞ and E[Y 2] < ∞ (ﬁnite second moment). We deﬁne the covariance between X and Y as Cov(X, Y ) = E[XY ] − E[X]E[Y ]. Remark: The condition that X and Y have ﬁnite second moment ensures that the covariance is well deﬁned. Indeed by the elementary inequality SXY S ≤ 1 2X 2 + 1 2Y 2 and monotonie and linearity of the expectation, we have E[SXY S] ≤ 1 2E[X 2] + 1 2E[Y 2] < ∞. CHAPTER 4. EXPECTATION 60 As we have seen in Section 4, the covariance between two independent random variables vanishes: X, Y independent Ô⇒ Cov(X, Y ) = 0. The reciprocal implication is not true in general (see Exercise 6.6). Nevertheless, as we have seen in Section 6, we can obtain a characterization by using a stronger property involving test functions. By Theorem 4.17, we have X, Y independent ⇐⇒ ∀φ, ψ piecewise continuous, bounded Cov(φ(X), ψ(Y )) = 0. Chapter 5 Joint distribution Goals • Deﬁnition of the joint distribution for discrete/continuous random variables. • Calculation of marginals • Interpretation of dependence/independence of random variables. Framework We ﬁx some probability space (Ω, F, P). All the random variables consid- ered in this chapter will be deﬁned on this reference probability space. 61 CHAPTER 5. JOINT DISTRIBUTION 62 1 Discrete joint distributions 1.1 Deﬁnition Deﬁnition 5.1. Let X1, . . . , Xn be n discrete random variables with Xi ∈ Wi almost surely, for some Wi ⊂ R ﬁnite or countable. The joint distribution of (X1, . . . , Xn) is the collection p = (p(x1, . . . , xn))x1∈W1,...,xn∈Wn deﬁned by p(x1, . . . , xn) = P[X1 = x1, . . . , Xn = xn] Example: Let X, Y be two independent Bernoulli random variables with parameter 1~2. The joint distribution of (X, Y ) is given by ∀x, y ∈ {0, 1} p(x, y) = 1 4. The joint distribution of (X, X) is equal to ∀x, y ∈ {0, 1} p(x, y) = ¢¨¨ ¦ ¨¨¤ 1 2 x = y, 0 x ≠ y. Let Z = X + Y , then the joint distribution p = (p(x, z))x∈{0,1},z∈{0,1,2} of (X, Z) is given by the following table: x z 0 1 2 0 1~4 1~4 0 1 0 1~4 1~4 Proposition 5.2. The joint distribution of some random variables X1, . . . , Xn satis- ﬁes Q x1∈W1,...,xn∈Wn p(x1, . . . , xn) = 1. (5.1) Proof. Consider the event A = {X1 ∈ W1, . . . , Xn ∈ Wn}. A is a ﬁnite intersection of almost sure events, hence P[A] = 1 (see Exercise 2.1, in Sheet 2). Furthermore, it can be written as the disjoint union A = ˜ x1∈W1,...,xn∈Wn{X1 = x1, . . . , Xn = xn}. Therefore, 1 = P[A] = Q x1∈W1,...,xn∈Wn P[X1 = x1, . . . , Xn = xn]. CHAPTER 5. JOINT DISTRIBUTION 63 Conversely, given some ﬁnite or countable sets W1, . . . , Wn and a function p ∶ W1 × \u0005 × Wn → [0, 1] satisfying (5.1), there exists a probability space and some discrete random variables with distribution p (see exercises). 1.2 Distribution of the image One of the main advantages of working with random variables is that we can “manipulate” them as numbers. For instance, if we are given n random variables X1, X2, . . . , Xn, we can think of them as n “random” numbers and we can make operations with them. The following proposition gives the distribution of such random variables as images of discrete random variables. Proposition 5.3. Let n ≥ 1 and φ ∶ Rn → R be an arbitrary function. Let X1, . . . , Xn be n discrete random variables on (Ω, F, P) with respective values in some ﬁnite or countable sets W1, . . . , Wn almost surely. Then Z = φ(X1, . . . , Xn) is a discrete random with values in W = φ(W1 × \u0005 × Wn) almost surely and with distribution given by ∀z ∈ W P[Z = z] = Q x1∈W1,...,xn∈Wn φ(x1,...,xn)=z P[X1 = x1, . . . Xn = xn]. Proof. First notice that the set W is ﬁnite or countable, as an image of a ﬁnite and countable set. To prove that Z is a discrete random variable, it suﬃces to show that • it takes values in W almost surely, and • for every z in W , we have {Z = z} ∈ F. Indeed, the second item implies that for every a ∈ R {Z ≤ a} = ˜ z∈W z≤a {Z = z} is also an event (as a countable union of events). Now, the ﬁrst item follows from the inclusion {X1 ∈ W1, . . . , Xn ∈ Wn} ⊂ {Z ∈ W }. For the second item, let z ∈ W and observe that {Z = z} = ˜ x1∈W1,...,xn∈Wn φ(x1,...,xn)=z {X1 = x1, . . . Xn = xn}. (5.2) Hence, {Z = z} ∈ F since it is a countable union of events. This implies that Z is a discrete random vartiable. To compute the distribution, observe that the union in (5.2) is disjoint and at most countable, hence, P[Z = z] = Q x1∈W1,...,xn∈Wn φ(x1,...,xn)=z P[X1 = x1, . . . Xn = xn]. CHAPTER 5. JOINT DISTRIBUTION 64 Example: Consider the random variable Z = X + Y deﬁned (as in Section 1.1) as the sum of two independent Bernoulli random variables with parameter 1~2. By applying the proposition above to φ(x, y) = x + y we get P[Z = 0] = Q x,y∈{0,1} x+y=0 P[X = x, Y = y] = P[X = 0, Y = 0] = 1~4 P[Z = 1] = Q x,y∈{0,1} x+y=1 P[X = x, Y = y] = P[X = 0, Y = 1] + P[X = 1, Y = 0] = 1~2 P[Z = 2] = Q x,y∈{0,1} x+y=2 P[X = x, Y = y] = P[X = 1, Y = 1] = 1~4. 1.3 Marginal distributions If one knows the joint distribution of X1, . . . , Xn, one can recover the distribution of each Xi separately. In this context the distribution of Xi is called the distribution of the i-th marginal. Proposition 5.4. Let X1, . . . , Xn be n discrete random variables with joint distribu- tion p = (p(x1, . . . , xn))x1∈W1,...,xn∈Wn. For every i, we have ∀z ∈ Wi P[Xi = z] = Q x1,...,xi−1,xi+1,...,xn p(x1, . . . , xi−1, z, xi+1, . . . , xn) . Proof. Apply Proposition 5.3 to φ(x1, . . . , xn) = xi. By the proposition above, if we know the joint distribution p of two random variables X, Y , then we can compute the distribution of X and the distribution of Y , but the converse is not true. Knowing the marginal distributions is not suﬃcient to compute the joint distribution. For example, let X, Y be two independent random variables. Then (X, Y ) and (X, X) have the same marginal distributions (both Bernoulli (1~2), but they have diﬀerent joint distributions. This notion of marginal distributions may help to understand joint distributions: Heuristically, the joint distribution of X1, . . . , Xn encodes the distribution of each Xi separately, as well as how the random variables depend on each other. 1.4 Expectation of the image Proposition 5.5. Let X1, . . . , Xn be n discrete random variables with joint distribu- CHAPTER 5. JOINT DISTRIBUTION 65 tion p = (p(x1, . . . , xn))x1∈W1,...,xn∈Wn. Let φ ∶ Rn → R, then E[φ(X1, . . . , Xn)] = Q x1,...,xn φ(x1, . . . , xn)p(x1, . . . , xn) , whenever the sum is well-deﬁned. Proof. Set W = φ(W1, . . . , Wn). Using the formula of Prop. 3.9, we have Q z∈F z ⋅ P[Z = z] = Q z∈F Q x1,...,xn∈E z ⋅ 1φ(x1,...,xn)=zP[X1 = x1, . . . , Xn = xn] = Q x1,...,xn∈E (Q z∈F z ⋅ 1φ(x1,...,xn)=z) ´¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¸¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¶ =φ(x1,...,xn) P[X1 = x1, . . . , Xn = xn]. (the permutation of the two sums can be justiﬁed by the Fubini’s theorem, provided the sum are well deﬁned). 1.5 Independence Proposition 5.6. Let X1, . . . , Xn be n discrete random variabless with joint distri- bution p = (p(x1, . . . , xn))x1∈W1,...,xn∈Wn. The following are equivalent (i) X1, . . . , Xn are independent, (ii) p(x1, . . . , xn) = P[X1 = x1]\u0005P[Xn = xn] for every x1 ∈ W1, . . . , xn ∈ Wn. Proof. (i)⇒ (ii) Consider the functions φ1, . . . , φn deﬁned by φi(z) = 1z=xi. We have P[X = x1, . . . , Xn = xn] = E[1X1=x1,...,Xn=xn] = E[φ1(X1)\u0005φn(Xn)] (i) = E[φ1(X1)]\u0005E[φn(Xn)] = P[X1 = x1]\u0005P[Xn = xn] (ii)⇒ (i) Let φ1 ∶ W1 → R, . . . , φn ∶ Wn → R. By Proposition 5.3 applied to φ(x1, . . . , xn) = CHAPTER 5. JOINT DISTRIBUTION 66 φ1(x1)\u0005φn(xn), we have E[φ1(X1)\u0005φn(Xn)] = Q x1,...,xn φ1(x1)\u0005φn(xn) ⋅ p(x1, . . . , xn) (ii) = Q x1,...,xn φ1(x1)\u0005φn(xn) ⋅ P[X1 = xx]\u0005P[Xn = xn] = › Q x1 φ1(x1) ⋅ P[X = x]”\u0005› Q xn φn(xn) ⋅ P[Xn = xn]” = E[φ1(X1)]\u0005E[φn(Xn)]. By Theorem 4.18, X1, \u0005, Xn are independent. 2 Continuous joint distribution 2.1 Deﬁnition Deﬁnition 5.7. Let n ≥ 1, some random variables X1, . . . , Xn ∶ Ω → R have a con- tinuous joint distribution if there exists a function f ∶ Rn → R+ such that P[X1 ≤ a1, . . . , Xn ≤ an] = S a1 −∞ \u0005 S an −∞ f (x1, \u0005, xn)dxn\u0005dx1 for every a1, . . . , an ∈ R. A function f as above is called a joint density of (X, Y ). Proposition 5.8. Let f be the joint density of n random variables X1, . . . , Xn. Then we have S ∞ −∞ \u0005 S ∞ −∞ f (x1, . . . , xn)dxn . . . dx1 = 1. (5.3) Conversely, given a non negative function f satisfying (5.3), one can always construct a probability space (Ω, F, P) and n random variables X1, . . . , Xn ∶ Ω → R with joint density f (admitted). Proof. By continuity of probability measures (Proposition 1.11) we have 1 = lim a1,...,an→∞ P[X1 ≤ a1, . . . , Xn ≤ an] = lim a1,...,an→∞ S a1 −∞ \u0005 S an −∞ f (x1, \u0005, xn)dxn\u0005dx1 = S ∞ −∞ \u0005 S ∞ −∞ f (x1, . . . , xn)dxn . . . dx1. CHAPTER 5. JOINT DISTRIBUTION 67 Interpretation: Informally, f (x1, . . . , xn)dx1\u0005dxn represents the probability that the random vector (X1, . . . , Xn) lies in the small region [x1, x1 + dx1] × \u0005 × [xn, xn + dxn]. Example 1: Uniform point in the square Consider two random variables X and Y with joint density f (x, y) = 10≤x,y≤1, i.e. f (x, y) = ¢¨¨ ¦ ¨¨¤ 1 (x, y) ∈ [0, 1]2 0 (x, y) ∉ [0, 1]2. Example 2: Uniform point in the disk Let D = {(x, y) ∶ x2 + y2 ≤ 1} be the disk of radius 1 around 0. Consider two random variables X and Y with joint density f (x, y) = 1 π 1x2+y2≤1, i.e. f (x, y) = ¢¨¨ ¦ ¨¨¤ 1 π x2 + y2 ≤ 1 0 x2 + y2 > 1. 2.2 Expectation of the image Proposition 5.9. Let φ ∶ Rn → R. If X1, . . . , Xn have joint density f , then the expectation of the random variable Z = φ(X1, . . . , Xn) can be calculated by the formula E[φ(X1, . . . , Xn)] = S ∞ −∞ \u0005 S ∞ −∞ φ(x1, . . . , xn) ⋅ f (x1, . . . , xn)dxn . . . dx1, (5.4) whenever the integral is well deﬁned. Proof. Admitted. Applications: Consider the pair (X, Y ) as in Example 1 above. By applying considering the function φ(x, y) = 1(x,y)∈R, we have for every rectangle R = (a, a′) × (b, b′) ⊂ [0, 1]2 P[(X, Y ) ∈ R] = E[φ(X, Y )] = S a′ a S b′ b dxdy = (a′ − a)(b′ − b) = Area(R), and (X, Y ) intuitively represents a uniform point in the square [0, 1]2. Equivalently, if we consider (X, Y ) as in example 2, we ﬁnd that for every rectangle R = (a, a′) × (b, b′) ⊂ D P[(X, Y ) ∈ R] = 1 π (a′ − a)(b′ − b) = Area(R) Area(D), and (X, Y ) intuitively represents a uniform point in the square [0, 1]2. CHAPTER 5. JOINT DISTRIBUTION 68 2.3 Marginal densities As in the discrete case, if X1, . . . , Xn have a joint density f , then each Xi taken individually is continuous, and the density of Xi can be calculated from f by integrating over all the variables xj, j ≠ i. Proposition 5.10. Let X1, \u0005, Xn be n random variables with a joint density f = fX1,...,Xn. Then for every i, Xi is a continuous random variable with density fi given by fi(z) = S (x1,...,xi−1,xi+1,...,xn)∈Rn−1 f (x1, . . . , xi−1, z, xi+1, . . . , xn)dx1 . . . dxi−1dxi+1dxn. for every z ∈ R Proof. We prove the result in the case two random variables X, Y (case n = 2). If X, Y possess a joint density fX,Y , then we have P[X ≤ a] = P[X ∈ [−∞, a], Y ∈ [−∞, ∞]] = S a −∞(S ∞ −∞ f (x, y)dy)dx, and therefore X is continuous with density fX(x) = S ∞ −∞ f (x, y)dy. Equivalently Y is continuous with density fY (y) = S ∞ −∞ f (x, y)dx. Let us calculate the marginal densities in the two examples of joint densities we have seen above. Example 1: Uniform point in the square If fX,Y (x, y) = 10≤x,y≤1, then X has density fX(x) = S0,1 10≤x≤110≤y≤1dy = 10≤x≤1. and equivalently fY (y) = 10≤y≤1. In other words, both X and Y are uniform random variables in [0, 1]. Example 2: Uniform point in the disk If fX,Y (x, y) = 1 π 1x2+y2≤1, then the density of X is fX(x) = S ∞ −∞ 1 π 1x2+y2≤1dy = S √1−x2 −√1−x2 1 π dy = 2 π √ 1 − x2, and equivalently fY (y) = 2 π »1 − y2. CHAPTER 5. JOINT DISTRIBUTION 69 2.4 Independence for continuous random variables The following theorem gives a useful characterization for the independence of continuous random variables. Theorem 5.11. Let X1, . . . , Xn be n continuous random variables with respective densities f1, . . . , fn. The following are equivalent (i) X1, . . . , Xn are independent, (ii) X1, . . . , Xn are jointly continuous with joint density f (x1, . . . , xn) = f1(x1) \u0005 fn(xn) . Remark 5.12. An important consequence is that two independent continuous random variables are automatically jointly continuous. Proof. We prove the result for two random variables (n = 2). The more general case is proved similarly. Let X, Y be two continuous random variables with density fX and fY respectively. (i) ⇒ (ii) If X and Y are independent, for every a, b ∈ R we have P[X ≤ a, Y ≤ b] = P[X ≤ a] ⋅ P[Y ≤ b] = S a −∞ fX(x)dx ⋅ S b −∞ fY (y)dy = S a −∞ S b −∞ fX(x)fY (y)dxdy. Therefore, X and Y have joint density fX,Y (x, y) = fX(x) ⋅ fY (y). (ii) ⇒ (i) Applying the formula (5.4), we ﬁnd, for every a, b ∈ R, E[φ(X)ψ(Y )] = S ∞ −∞ S ∞ −∞ φ(x)ψ(y)fX,Y (x, y)dxdy (ii) = S ∞ −∞ S ∞ −∞ φ(x)ψ(y)fX(x)fY (y)dxdy = S ∞ −∞ φ(x)fX(x)dx ⋅ S ∞ −∞ ψ(y)fY (y)dy = E[φ(X)] ⋅ E[ψ(Y )]. Hence, by the characterization of independence (Theorem 4.18), we conclude that X and Y are independent random variables. Example 1: Uniform point in the square If X and Y have joint density fX,Y (x, y) = 10≤x,y≤1, then fX,Y (x, y) = 10≤x≤110≤y≤1 = fX(x) ⋅ fY (y). CHAPTER 5. JOINT DISTRIBUTION 70 In other words, the two coordinates of a uniform random point in [0, 1]2 are independent. Example 2: Uniform point in the disk If X and Y have joint density fX,Y = 1 π 1D, then we have seen that fX(x) = 2 π √ 1 − x2 and fY (y) = 2 π »1 − y2, and therefore fX,Y (x, y) ≠ fX(x)fY (y). The two coordinates X and Y of a uniform point in D are not independent! This fact can easily be understood by looking at the event that X is larger than (say) √ 3~2. In this case, we have some information about Y which is constrainted to belong to [−1~2, 1~2]. Chapter 6 Asymptotic results In this chapter, we ﬁx a probability space (Ω, F, P) and an inﬁnite sequence of i.i.d. random variables X1, X2, . . . In other words, we are given some random variables Xi ∶ Ω → R such that ∀i1 < \u0005 < ik ∀x1, . . . xk ∈ R P[Xi1 ≤ x1, . . . , Xik ≤ xk] = F (x1)\u0005F (xk). where F is the common distribution function. For every n, we consider the partial sum Sn = X1 + \u0005 + Xn, and we are interested in the behavior (when n is large) of the random variable deﬁned by Sn n = X1 + \u0005 + Xn n . (6.1) and sometimes called the empirical average. 71 CHAPTER 6. ASYMPTOTIC RESULTS 72 1 Law of large numbers Theorem 6.1. Assume that E[SX1S] < ∞. Deﬁning m = E[X1] we have lim n→∞ X1 + \u0005 + Xn n = m a.s. (6.2) What does Eq. (6.2) mean? If we ﬁx ω ∈ Ω, the values X1(ω) 1 , X1(ω)+X2(ω) 2 , \u0005 simply deﬁne a sequence of real numbers. The properties of this sequence depends on the outcome ω. We are here interested on the ω’s for which the sequence X1(ω)+\u0005+Xn(ω) n converges to m. More precisely we consider E ⊂ Ω deﬁned by E = {ω ∶ lim n→∞ X1(ω) + \u0005 + Xn(ω) n = m} (6.3) One can check that E is an event and Eq. (6.2) says that P[E] = 1. Remark 6.2. In the statement of the theorem, it may be surprising that the assumption and the deﬁnition of m are in terms of X1 only. Actually, since the random variables are i.i.d. we also have E[SXiS] < ∞ and m = E[Xi] for every i. Example 1: Bernoulli random variables If X1, X2, . . . is an inﬁnite sequence of i.i.d. Bernoulli random variables with parameter p. Then we have lim n→∞ X1 + \u0005 + Xn n = p a.s. Example 2: Exponential random variables If T1, T2, . . . is an inﬁnite sequence of i.i.d. Exponential random variables with parameter λ. Then we have lim n→∞ T1 + \u0005 + Tn n = λ a.s. Proof. We prove the law of large numbers under a stronger moment assumption. We assume that C ∶= E[X 4 1 ] < ∞. Without loss of generality, we may assume that E[X1] = 0. (6.4) Indeed, if we have the result for m = 0, we can extend it to m ≠ 0 by considering the random variables Yi = Xi − m, i ≥ 1. Fix n ≥ 1 and consider the random variable Sn = X1 + \u0005 + Xn. CHAPTER 6. ASYMPTOTIC RESULTS 73 By expanding Z 4 and using linearity of the expectation, we have E[S4 n] = Q 1≤i,j,k,ℓ≤n E[XiXjXkXℓ]. As soon as one factor Xα appears a single time in the product XiXjXkXℓ , then indepen- dence and the hypothesis (6.4) imply that the expectation of the term vanishes. Hence, the only non vanishing terms are of the form E[X 4 i ] and E[X 2 i Y 2 j ], for i ≠ j. By indepen- dence and Jensen inequality, for i ≠ j we have E[X 2 i Y 2 j ] = E[X 2 1 ]2 ≤ C. Since there are at most n2 + n non-vanishing terms in the sum above and each such term is smaller than C, we obtain E[S4 n] ≤ C(n 2 + n) ≤ 2Cn 2. For every n, consider the event Fn = ıω ∈ Ω ∶ SSn(ω)S n < n−1~8\u0000 . By Markov inequality, we have P[F c n] = P[S4 n ≥ n7~2] ≤ E[S4 n] n7~2 ≤ 2C n3~2 . Now, for N ≥ 1, consider the event EN = ˛ n≥N Fn = {∀n ≥ N SSnS n ≤ n −1~8}. By the union bound, we have P[Ec N ] = P[ ˜ n≥N F c n] ≤ Q n≥N P[F c n] ≤ Q n≥N C n3~2 . Hence limN →∞ P[EN ] = 1. Furthermore, for every N , we have EN ⊂ E where E is the event deﬁned in Eq. (6.3) (with m = 0). Therefore, P[EN ] ≤ P[E], and the result follows by taking the limit as N tends to inﬁnity. 2 Application: Monte-Carlo integration The law of large number can be useful to approximate integrals, that may be diﬃcult to compute exactly. Let d ≥ 1 be an integer. Let g ∶ [0, 1] → R such that S 1 0 Sg(x)Sdx < ∞. How goal is to calculate I = S 1 0 g(x)dx. CHAPTER 6. ASYMPTOTIC RESULTS 74 Such an integral may be delicate to compute exactly, and we give a general method to obtain approximations of I. The key idea is to interpret I as an expectation. Let U be a uniform random variable in [0, 1]. Then, E[g(U )] = S 1 0 g(x)dx = I. Hence, approximating I is equivalent to approximating the expectation of g(U ), which can be achieved by the law of large numbers. Let U1, U2, . . . be an i.i.d. sequence of uniform random variables in [0, 1], and consider Xn = g(Un) for every n. The sequence X1, X2, . . . is i.i.d. and we have E[SX1S] = S 1 0 Sg(x)Sdx < ∞, and E[X1] = I. Hence, by the law of large numbers, we have lim n→∞ g(U1) + \u0005 + g(Un) n = I. Hence, we obtain an approximation of I by calculating g(U1)+\u0005+g(Un) n for some large n. Notice that this quantity is easy to compute in practice, once we have simulated some uniform random variables U1, . . . Un. This method generalizes in several ways, one can use diﬀerent densities to compute integrals over R, and we can use joint densities to approximate d-dimensional integrals, d ≥ 2. 3 Convergence in distribution When we have deterministic numbers in R, we can measure the distance between them: the distance between x and y is given by Sx − yS. This gives rise to a natural notion of convergence. A sequence of real numbers (xn)n∈N converges to x if lim n→∞ Sxn − xS = 0. For two random variables X and Y , one way to measure the “distance” between them is to look at their distribution functions. X and Y have similar probabilistic properties if their respective distribution functions FX and FY are close to each other. This gives rise to the following notion of convergence for random variables, called “convergence in distribution”. Deﬁnition 6.3. Let (Xn)n∈N and X be some random variables. We write Xn Approx ≈ X as n → ∞ if for every x ∈ R lim n→∞ P[Xn ≤ x] = P[X ≤ x]. CHAPTER 6. ASYMPTOTIC RESULTS 75 Example 1: Bernoulli random variables For every n, let Xn be a Bernoulli random variable with parameter pn ∈ [0, 1]. If limn→∞ pn = p. Then we have Xn Approx ≈ X as n → ∞, where X is a Bernoulli random variable with parameter p. Example 2: Approximation of the uniform In the ﬁrst example, we have discrete random variables converging towards another discrete random variable. It is also possible that a sequence of discrete random variables converge towards a continuous random variable, as in the following example. For every n, let Xn be a uniform random variable in {0, 1 n , 2 n, . . . , n−1 n , 1} (i.e. P[Xn = k n] = 1 n+1 for k = 0, 1, 2. . . . , n). Then we have Xn Approx ≈ X as n → ∞, where X is a uniform random variable in [0, 1]. Indeed, for every x ∈ [0, 1], we have P[Xn ≤ x] = xn\u000f n Ð→ n→∞ x = P[X ≤ x], and the convergence for x ∉ [0, 1] is trivial. 4 Central limit theorem A question of ﬂuctuation? The law of large numbers tells us that for large n, the empirical average (6.1) is closed to the expectation m = E[X1]. A second very natural question to ask is: How far is X1 + \u0005 + Xn n from m typically? The Gaussian case Let us ﬁrst look at the very instructive case when X1, X2, . . . is a sequence of i.i.d. normal random variables with parameters m and σ2. Then the results we have seen on normal random variables tell us that Z = X1 + \u0005 + Xn n − m is again a normal random variable with parameters ¯m = 0 and ¯σ2 = 1 nσ2. The standard deviation ¯σ = 1√ n σ represents the typical ﬂuctuations of Z. Roughly one can say that the typical distance between X1+\u0005+Xn n and m is of order σ√ n . CHAPTER 6. ASYMPTOTIC RESULTS 76 In this context, a more natural random variable to consider is to rescale Z by a factor √ n σ in order to get ﬂuctuations of order 1: using again the properties of normal distributions we see that √ n σ Z = X1 + \u0005 + Xn − n ⋅ m √ σ2n is a standard normal. As a conclusion, if we consider i.i.d. normal distributions with expectation m and variance σ2, then the random variable X1 + \u0005 + Xn − n ⋅ m √ σ2n corresponds to a rescaled version of the ﬂuctuations of X1+\u0005+Xn n and is a standard normal. General case: the central limit theorem If X1, X2, . . . are not normal, it is in general not easy to compute the law of X1 + \u0005 + Xn − n ⋅ m √ σ2n Nevertheless, the central limit theorem tells us that this random variable always get close to a standard normal if n is large. Theorem 6.4 (Central limit theorem). Assume that the expectation E[X 2 1 ] is well deﬁned and ﬁnite. Deﬁning m = E[X1] and σ2 = Var(X1), we have P[ Sn − n ⋅ m √ σ2n ≤ a] ÐÐ→ n→∞ Φ(a) = 1 √ 2π S a −∞ e −x2~2dx. (6.5) for every a ∈ R What does Eq. (6.5) mean? In words, the theorem above asserts that for n large, the distribution of the random variable Zn = Sn − n ⋅ m √ σ2n “looks like” the distribution of a normal random variables N (0, 1) . With the notation of Section 3, we have Zn Approx ≈ Z as n → ∞, where Z ∼ N (0, 1). Remark 1: For every n, we can use linearity properties of the expectation and variance to show that E[Zn] = 0 and Var(Zn) = 1. CHAPTER 6. ASYMPTOTIC RESULTS 77 Remark 2: The central limit theorem helps us predicting the behaviour of Sn for n large. For example consider p ∶= P[Z ∈ [−2, 2]], where Z is a standard normal random variable. It is known that p \b 0.95 (this correspond to the blue and brown area in the picture below). Figure 6.1: Quantiles of a normal random variable with parameters µ and σ2 By the central limit theorem, we know that lim n→∞ P[mn − 2 √ σ2n ≤ Sn ≤ mn + 2 √ σ2n] = p \b 95%. Bibliography [LSW21] J. Lengler, A. Steger, and E. Welzl, Algorithmen und Wahrscheinlichkeit, 2021. [Sch10] M. Schweizer, Wahrscheinlichkeit und Statistik, 2010. [Wil01] David Williams, Weighing the odds, Cambridge University Press, Cambridge, 2001, A course in probability and statistics. MR 1854128 78","libVersion":"0.3.2","langs":""}