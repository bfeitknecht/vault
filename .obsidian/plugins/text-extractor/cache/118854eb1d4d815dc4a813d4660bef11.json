{"path":"sem1/LinAlg/UE/s/LinAlg-u07-s.pdf","text":"D-INFK Linear Algebra HS 2023 Afonso Bandeira Bernd G¨artner Solution for Assignment 7 1. a) Observe that since both V and W are subspaces, we must have 0 ∈ V ∩ W and it remains to prove that this is the only element of V ∩ W . Since V and W are orthogonal, we must have v⊤w = 0 for all v ∈ V and w ∈ W . Now assume for a contradiction that there exists a vector u ∈ V ∩ W with u ̸= 0. From u ∈ V ∩ W we get that u belongs to both V and W and hence we must have u⊤u = 0. On the other hand, we get 0 < ∥ u ∥2 = u⊤u from u ̸= 0. In conclusion, we have shown 0 < u⊤u = 0, a contradiction. b) First of all, we observe that V ⊥ is not empty since 0 · v = 0 for all v ∈ V and hence 0 ∈ V ⊥. Thus, it remains to show that V ⊥ is closed under vector addition and scalar multiplication. Let u, w ∈ V ⊥ and c ∈ R be arbitrary. We want to prove u + w ∈ V ⊥ and cw ∈ V ⊥. Let v ∈ V be arbitrary. We first calculate (u + w) · v = u · v + w · v = 0 + 0 = 0 by using u ∈ V ⊥ and w ∈ V ⊥. Similarly, we also get (cw) · v = c(w · v) = c0 = 0 by using w ∈ V ⊥. Since this works for any vector v ∈ V , we conclude that the vectors u + w and cw are indeed orthogonal to all vectors from V and hence belong to V ⊥, i.e. u + w ∈ V ⊥ and cw ∈ V ⊥. We have shown that V ⊥ is non-empty and closed under vector addition and scalar multiplication. Hence, we conclude that it is a subspace of Rn. 2. Unless stated otherwise, references to the lecture notes refer to the notes of the second part of the course. a) In the proof of Proposition 4.2.4 it was shown that A and A⊤A have the same nullspace. From this, we concluded that A has independent columns if and only if A⊤A is invertible. But in fact, having N(A) = N(A⊤A) also implies rank(A) = rank(A⊤A). b) We know from the lecture that the rank of a matrix does not change if we transpose it. In particular, we have rank(A⊤) = rank(A) and hence rank(A⊤) = rank(A⊤A) by using a). For the second part, observe that the statement from a) holds for any matrix. In particular, it also holds for the matrix B := A⊤, i.e. we have rank(B) = rank(B⊤B). Plugging in B = A⊤ yields rank(A⊤) = rank(AA⊤). In conclusion, we have rank(A) = rank(A⊤) = rank(A⊤A) = rank(AA⊤). c) We will use the following three facts: 1. For any matrix B ∈ Rk×ℓ, we have R(B) = C(B⊤) = N(B)⊥. This was discussed in Section 4.1 of the lecture notes (first part). In particular, we can use this with B := A to get C(A⊤) = N(A)⊥ and for B := A⊤A to get C((A⊤A)⊤) = N(A⊤A)⊥. 2. The matrix A⊤A is symmetric. This is stated in Corollary 4.2.5 for the case where A has independent columns. But it holds in general since (A⊤A)⊤ = A⊤(A⊤)⊤ = A⊤A. 3. The matrices A and A⊤A have the same nullspace, i.e. N(A) = N(A⊤A). This was shown in the proof of Proposition 4.2.4 and we also used it in subtask a). We now prove C(A⊤) = C(A⊤A) by combining these three facts in C(A⊤) 1. = N(A) ⊥ 3. = N(A⊤A)⊥ 1. = C((A⊤A)⊤) 2. = C(A⊤A). 3. a) Observe (or recall from the lecture) that P 2 = A(A⊤A)−1A⊤A(A⊤A)−1A ⊤ = A(A⊤A) −1IA⊤ = A(A⊤A)−1A⊤ = P. Using this, we similarly get (I − P )2 = I 2 − 2IP + P 2 = I − 2P + P = I − P. As for the projection matrix P , the interpretation here is that applying (I − P ) twice does not do more than applying (I − P ) once. b) Since P projects vectors onto C(A) and w is already in C(A), we get that P w = w (i.e. nothing happens to w if we apply P ). We can derive this more formally from Theorem 4.2.6, which tells us that P w = arg min p∈C(A) ∥ w − p ∥ . We also have arg min p∈C(A) ∥ w − p ∥ = w by w ∈ C(A), and putting those two things together we get P w = w, as desired. From this we also get (I − P )w = w − P w = w − w = 0. In other words, we have w ∈ N(I − P ). c) Since v is orthogonal to all columns of A, we get A⊤v = 0. Hence, we also get P v = A(A⊤A)−1A⊤v = 0 and therefore (I − P )v = v. In words, applying (I − P ) to v does nothing. d) As is usual, there are many different ways to solve this. We present one way that heavily relies on Section 4.1 from the first part of the course. We need the following four observations: 1. In subtask b), we essentially proved C(A) ⊆ C(P ) and hence dim(C(A)) ≤ dim(C(P )). 2. Similarly, our arguments from c) imply C(A)⊥ ⊆ N(P ) and therefore dim(C(A)⊥) ≤ dim(N(P )). 3. From Section 4.1 (first part) we know that dim(C(A)) + dim(C(A)⊥) = m. 4. We also have dim(C(P )) + dim(N(P )) ≤ m. This follows from C(P ) ∩ N(P ) = {0} by Section 4.1 (first part). To see why C(P ) ∩ N(P ) = {0} holds, assume for a contradiction that there exists v ∈ C(P ) ∩ N(P ) with v ̸= 0. By v ∈ C(P ), there exists w ∈ Rm with P w = v. By v ∈ N(P ) we also get P (P w) = P v = 0. But since P is a projection matrix, we must have P 2w = P w which is in contradiction with P w = v ̸= 0 = P 2w. Finally, we plug together all the pieces to obtain dim(C(A)) 1. ≤ dim(C(P )) 4. ≤ m − dim(N(P )) 2. ≤ m − dim(C(A) ⊥) 3. = dim(C(A)) which implies rank(P ) = dim(C(P )) = dim(C(A)). e) Before we turn our attention to I − P , observe that from subtask d), we can also deduce C(A) = C(P ) since we have C(A) ⊆ C(P ) and dim(C(A)) = dim(C(P )). Analogously, we get C(A)⊥ = N(P ). We claim that C(I − P ) = C(A)⊥ and hence rank(I − P ) = dim(C(I − P )) = dim(C(A)⊥). From subtask c) we already know that C(A)⊥ ⊆ C(I −P ). Hence, it remains to prove C(I −P ) ⊆ C(A)⊥. To this end, let v ∈ C(I −P ) be arbitrary. Then there exists w ∈ Rm with (I −P )w = v. Hence, we get (I − P )v = (I − P )2w = (I − P )w = v and therefore P v = 0. This again implies v ∈ N(P ) = C(A)⊥, as desired. We conclude C(I − P ) = C(A)⊥ and hence rank(I − P ) = dim(C(I − P )) = dim(C(A)⊥). f) Given a point p ∈ Rm, we know from the lecture that the projection projC(A)⊥(p) of p onto C(A)⊥ is given by the unique point q ∈ C(A)⊥ satisfying v⊤(p − q) = 0 for all v ∈ C(A)⊥. Now let v ∈ C(A)⊥ be arbitrary. Choosing q = (I − P )p indeed yields v⊤(p − q) = v⊤(P p) = 0 since (P p) ∈ C(P ) = C(A) and v ∈ C(A)⊥. 4. a) For the first datapoint we get x1 = 1 and y1 = 2. Hence, we get the equation a + b = 2 from f (x1) = y1. We proceed analogously for all i ∈ [5] and obtain the system of linear equations       1 1 2 1 3 1 4 1 5 1       [a b ] =       2 3 5 6 8       . b) Since there are 5 equations and only 2 unknows, we generally don’t expect this system to have a solution. In particular, if our data comes from a real world process we cannot expect the data to exactly lie on a line (even if this is a good model for the real world process). c) Let A be the system matrix from the linear system above and let b be the right-hand side. We then get A⊤A = [ 1 2 3 4 5 1 1 1 1 1 ]       1 1 2 1 3 1 4 1 5 1       = [55 15 15 5 ] and A⊤b = [ 1 2 3 4 5 1 1 1 1 1 ]       2 3 5 6 8       = [87 24 ] . Hence, we get the normal equations [ 55 15 15 5 ] [ a b ] = [ 87 24 ] . We can now solve this for a and b using Gauss-elimination or other techniques. One way to do it in this particular case, is to subtract the second row 3 times from the first row to get [10 0 15 5 ] [ a b ] = [ 15 24 ] . We then conclude a = 3 2 and b = 3 10 . In particular, we conclude that the line f (x) = 3 2 x + 3 10 explains our data best.","libVersion":"0.5.0","langs":""}